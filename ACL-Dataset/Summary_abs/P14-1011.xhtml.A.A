<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 124" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings.
  </a>
  <a href="#1" id="1">
   The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously.
  </a>
  <a href="#2" id="2">
   After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other.
  </a>
  <a href="#3" id="3">
   We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates.
  </a>
  <a href="#4" id="4">
   Extensive experiments show that the BRAE is remarkably effective in these two tasks.
  </a>
 </body>
</html>