<html>
 <head>
  <meta content="SENT_NUM: 6, WORD_NUM: 108" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language.
  </a>
  <a href="#1" id="1">
   These features are problematic for standard language analysis tools and it can be desirable to convert them to canonical form.
  </a>
  <a href="#2" id="2">
   We propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings.
  </a>
  <a href="#3" id="3">
   The text embeddings are generated using an Simple Recurrent Network.
  </a>
  <a href="#4" id="4">
   We find that enriching the feature set with text embeddings substantially lowers word error rates on an English tweet normalization dataset.
  </a>
  <a href="#5" id="5">
   Our model improves on state-of-the-art with little training data and without any lexical resources.
  </a>
 </body>
</html>