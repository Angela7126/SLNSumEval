<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 105" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus; for example the recent effort in phrase-based MT [ 21 ] significantly outperforms mainstream methods that only train on small tuning sets.
  </a>
  <a href="#1" id="1">
   However, phrase-based MT suffers from limited reorderings, and thus its training can only utilize a small portion of the bitext due to the distortion limit.
  </a>
  <a href="#2" id="2">
   To address this problem, we extend \newcite yu+:2013 to syntax-based MT by generalizing their latent variable violation-fixing perceptron from graphs to hypergraphs.
  </a>
  <a href="#3" id="3">
   Experiments confirm that our method leads to up to +1.2 Bleu improvement over mainstream methods such as Mert and Pro.
  </a>
 </body>
</html>