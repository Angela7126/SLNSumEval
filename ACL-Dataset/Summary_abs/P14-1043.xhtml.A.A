<html>
 <head>
  <meta content="SENT_NUM: 8, WORD_NUM: 179" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training.
  </a>
  <a href="#1" id="1">
   Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data.
  </a>
  <a href="#2" id="2">
   With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.
  </a>
  <a href="#3" id="3">
   This framework offers two promising advantages.
  </a>
  <a href="#4" id="4">
   1) ambiguity encoded in parse forests compromises noise in 1-best parse trees.
  </a>
  <a href="#5" id="5">
   During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves.
  </a>
  <a href="#6" id="6">
   2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser.
  </a>
  <a href="#7" id="7">
   Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.
  </a>
 </body>
</html>