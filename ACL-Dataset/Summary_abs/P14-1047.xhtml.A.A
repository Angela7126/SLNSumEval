<html>
 <head>
  <meta content="SENT_NUM: 6, WORD_NUM: 134" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.
  </a>
  <a href="#1" id="1">
   Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from.
  </a>
  <a href="#2" id="2">
   In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.
  </a>
  <a href="#3" id="3">
   Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly.
  </a>
  <a href="#4" id="4">
   We also show that very high gradually decreasing exploration rates are required for convergence.
  </a>
  <a href="#5" id="5">
   We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.
  </a>
 </body>
</html>