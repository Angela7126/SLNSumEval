<html>
 <head>
  <meta content="SENT_NUM: 7, WORD_NUM: 176" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We present a method that learns word embedding for Twitter sentiment classification in this paper.
  </a>
  <a href="#1" id="1">
   Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.
  </a>
  <a href="#2" id="2">
   This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad , to neighboring word vectors.
  </a>
  <a href="#3" id="3">
   We address this issue by learning sentiment-specific word embedding ( SSWE ), which encodes sentiment information in the continuous representation of words.
  </a>
  <a href="#4" id="4">
   Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions.
  </a>
  <a href="#5" id="5">
   To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.
  </a>
  <a href="#6" id="6">
   Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.
  </a>
 </body>
</html>