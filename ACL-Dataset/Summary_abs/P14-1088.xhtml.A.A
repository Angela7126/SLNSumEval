<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 170" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.
  </a>
  <a href="#1" id="1">
   With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 (for phrase structure) and accuracy scores (for dependencies).
  </a>
  <a href="#2" id="2">
   In this work we present a chance-corrected metric based on Krippendorffs , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.
  </a>
  <a href="#3" id="3">
   To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.
  </a>
 </body>
</html>