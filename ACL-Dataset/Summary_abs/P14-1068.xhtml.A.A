<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 74" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper we address the problem of grounding distributional representations of lexical meaning.
  </a>
  <a href="#1" id="1">
   We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input.
  </a>
  <a href="#2" id="2">
   The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively.
  </a>
  <a href="#3" id="3">
   We evaluate our model on its ability to simulate similarity judgments and concept categorization.
  </a>
  <a href="#4" id="4">
   On both tasks, our approach outperforms baselines and related models.
  </a>
 </body>
</html>