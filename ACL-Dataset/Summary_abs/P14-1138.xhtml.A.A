<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 128" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers.
  </a>
  <a href="#1" id="1">
   We perform unsupervised learning using noise-contrastive estimation [ 15 , 26 ] , which utilizes artificially generated negative samples.
  </a>
  <a href="#2" id="2">
   Our alignment model is directional, similar to the generative IBM models [ 4 ].
  </a>
  <a href="#3" id="3">
   To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training.
  </a>
  <a href="#4" id="4">
   The RNN-based model outperforms the feed-forward neural network-based model [ 40 ] as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.
  </a>
 </body>
</html>