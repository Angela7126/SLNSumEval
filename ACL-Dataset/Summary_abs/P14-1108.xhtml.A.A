<html>
 <head>
  <meta content="SENT_NUM: 7, WORD_NUM: 158" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We introduce a novel approach for building language models based on a systematic, recursive exploration of skip n -gram models which are interpolated using modified Kneser-Ney smoothing.
  </a>
  <a href="#1" id="1">
   Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case.
  </a>
  <a href="#2" id="2">
   In this paper we motivate, formalize and present our approach.
  </a>
  <a href="#3" id="3">
   In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified Kneser-Ney smoothing.
  </a>
  <a href="#4" id="4">
   Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements.
  </a>
  <a href="#5" id="5">
   Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data.
  </a>
  <a href="#6" id="6">
   Using a very small training data set of only 736 KB text we yield improvements of even 25.7 % reduction of perplexity.
  </a>
 </body>
</html>