<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 116" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Sentence compression has been shown to benefit from joint inference involving both n-gram and dependency-factored objectives but this typically requires expensive integer programming.
  </a>
  <a href="#1" id="1">
   We explore instead the use of Lagrangian relaxation to decouple the two subproblems and solve them separately.
  </a>
  <a href="#2" id="2">
   While dynamic programming is viable for bigram-based sentence compression, finding optimal compressed trees within graphs is NP-hard.
  </a>
  <a href="#3" id="3">
   We recover approximate solutions to this problem using LP relaxation and maximum spanning tree algorithms, yielding techniques that can be combined with the efficient bigram-based inference approach using Lagrange multipliers.
  </a>
  <a href="#4" id="4">
   Experiments show that these approximation strategies produce results comparable to a state-of-the-art integer linear programming formulation for the same joint inference task along with a significant improvement in runtime.
  </a>
 </body>
</html>