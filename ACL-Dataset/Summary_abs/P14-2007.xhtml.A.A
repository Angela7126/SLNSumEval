<html>
 <head>
  <meta content="SENT_NUM: 8, WORD_NUM: 157" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   The effort required for a human annotator to detect sentiment is not uniform for all texts, irrespective of his/her expertise.
  </a>
  <a href="#1" id="1">
   We aim to predict a score that quantifies this effort, using linguistic properties of the text.
  </a>
  <a href="#2" id="2">
   Our proposed metric is called Sentiment Annotation Complexity (SAC.
  </a>
  <a href="#3" id="3">
   As for training data, since any direct judgment of complexity by a human annotator is fraught with subjectivity, we rely on cognitive evidence from eye-tracking.
  </a>
  <a href="#4" id="4">
   The sentences in our dataset are labeled with SAC scores derived from eye-fixation duration.
  </a>
  <a href="#5" id="5">
   Using linguistic features and annotated SACs, we train a regressor that predicts the SAC with a best mean error rate of 22.02% for five-fold cross-validation.
  </a>
  <a href="#6" id="6">
   We also study the correlation between a human annotator s perception of complexity and a machine s confidence in polarity determination.
  </a>
  <a href="#7" id="7">
   The merit of our work lies in (a) deciding the sentiment annotation cost in, for example, a crowdsourcing setting, (b) choosing the right classifier for sentiment prediction.
  </a>
 </body>
</html>