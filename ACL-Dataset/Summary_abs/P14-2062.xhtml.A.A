<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 84" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Crowdsourcing lets us collect multiple annotations for an item from several annotators.
  </a>
  <a href="#1" id="1">
   Typically, these are annotations for non-sequential classification tasks.
  </a>
  <a href="#2" id="2">
   While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.
  </a>
  <a href="#3" id="3">
   This paper shows that workers can actually annotate sequential data almost as well as experts.
  </a>
  <a href="#4" id="4">
   Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.
  </a>
 </body>
</html>