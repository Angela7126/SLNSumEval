<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 94" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition.
  </a>
  <a href="#1" id="1">
   However, experiments suggest that while the inclusion of perceptual input improves representations of certain concepts, it degrades the representations of others.
  </a>
  <a href="#2" id="2">
   We propose an unsupervised method to determine whether to include perceptual input for a concept, and show that it significantly improves the ability of multi-modal models to learn and represent word meanings.
  </a>
  <a href="#3" id="3">
   The method relies solely on image data, and can be applied to a variety of other NLP tasks.
  </a>
 </body>
</html>