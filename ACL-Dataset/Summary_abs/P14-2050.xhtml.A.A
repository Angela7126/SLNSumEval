<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 67" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   While continuous word embeddings are gaining popularity, current models are based solely on linear contexts.
  </a>
  <a href="#1" id="1">
   In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts.
  </a>
  <a href="#2" id="2">
   In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings.
  </a>
  <a href="#3" id="3">
   The dependency-based embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.
  </a>
 </body>
</html>