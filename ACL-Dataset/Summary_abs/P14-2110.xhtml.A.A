<html>
 <head>
  <meta content="SENT_NUM: 3, WORD_NUM: 71" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Code-switched documents are common in social media, providing evidence for polylingual topic models to infer aligned topics across languages.
  </a>
  <a href="#1" id="1">
   We present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.
  </a>
  <a href="#2" id="2">
   We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators.
  </a>
 </body>
</html>