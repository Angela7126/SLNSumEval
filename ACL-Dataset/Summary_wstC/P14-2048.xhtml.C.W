<html>
 <head>
  <meta content="SENT_NUM:5, WORD_NUM:129" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We presented a generalization of the SkipGram embedding model in which the linear bag-of-words contexts are replaced with arbitrary ones, and experimented with dependency-based contexts, showing that they produce markedly different kinds of similarities.
  </a>
  <a href="#1" id="1">
   These results are expected, and follow similar findings in the distributional semantics literature.
  </a>
  <a href="#2" id="2">
   We also demonstrated how the resulting embedding model can be queried for the discriminative contexts for a given word, and observed that the learning procedure seems to favor relatively local syntactic contexts, as well as conjunctions and objects of preposition.
  </a>
  <a href="#3" id="3">
   We hope these insights will facilitate further research into improved context modeling and better, possibly task-specific, embedded representations.
  </a>
  <a href="#4" id="4">
   Our software, allowing for experimentation with arbitrary contexts, together with the embeddings described in this paper, are available for download at the authors websites.
  </a>
 </body>
</html>