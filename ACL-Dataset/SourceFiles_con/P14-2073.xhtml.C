<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Particle Filter Rejuvenation and Latent Dirichlet Allocation.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_abstract">
      <h6 class="ltx_title ltx_title_abstract">
       Abstract
      </h6>
      <p class="ltx_p">
       Previous research has established several methods of
       online
       learning for latent Dirichlet allocation (LDA). However,
       streaming
       learning for LDA‚Äîallowing only one pass over the
data and constant storage complexity‚Äîis not as well explored. We
use reservoir sampling to reduce the storage complexity of a
previously-studied online algorithm, namely the particle filter, to
constant. We then show that a simpler particle filter
implementation performs just as well, and that the quality of the
initialization dominates other factors of performance.
      </p>
     </div>
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        We extend a popular model, latent Dirichlet allocation (LDA),
to unbounded streams of documents. In order for
inference to be practical in this setting it must use constant space
asymptotically and run in pseudo-linear time, perhaps
        O‚Å¢(n)
        or
        O‚Å¢(n‚Å¢log‚Å°n)
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        Canini et al. (2009)
        presented a method for LDA inference based on
particle filters, where a sample set of models is updated online with
each new token observed from a stream. In general, these models should be
regularly resampled and rejuvenated using Markov Chain
Monte Carlo (MCMC) steps over the history in order
to improve the efficiency of the particle filter
        [10]
        .
The particle filter of
        Canini et al. (2009)
        rejuvenates over
independent draws from the history by storing all past observations
and states. This algorithm thus has linear storage complexity
and is not an online learning algorithm in a strict
sense
        [6]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        In the current work we propose using reservoir sampling in the
rejuvenation step to reduce the storage complexity of the particle
filter to
        O‚Å¢(1)
        .
This improvement is practically useful in the large-data setting
and is also
scientifically interesting in that it recovers some of the
cognitive plausibility which originally motivated
        B√∂rschinger and Johnson (2012)
        .
However, in experiments on the dataset studied by
        Canini et al. (2009)
        ,
we show that rejuvenation does not benefit the particle filter‚Äôs
performance. Rather, performance is dominated by the effects of random
initialization (a problem for which we provide a correction while abiding
by the same constraints as
        Canini et al. (2009)
        ). This result
re-opens the
question of whether rejuvenation is of practical importance in
online learning for static Bayesian models.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Latent Dirichlet Allocation
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        For a sequence of
        N
        words collected into documents of varying
length, we denote the
        j
        -th word as
        wj
        , and the document it occurs in
as
        di
        . LDA
        [3]
        ‚Äúexplains‚Äù the occurrence of each word by postulating
that a document was generated by repeatedly: (1) sampling a topic
        z
        from
        Œ∏(d)
        , the document-specific mixture of
        T
        topics, and (2)
sampling a word
        w
        from
        œï(z)
        , the probability distribution
the
        z
        -th topic defines over the vocabulary.
       </p>
      </div>
      <div class="ltx_para" id="S2.p2">
       <p class="ltx_p">
        The goal is to infer
        Œ∏
        and
        œï
        , under the model:
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx1">
        <tr class="ltx_equation ltx_align_baseline" id="S2.Ex1">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          wi‚à£zi,œï(zi)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àºCategorical‚Å¢(œï(zi))
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
        <tr class="ltx_equation ltx_align_baseline" id="S2.Ex2">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          œï(z)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àºDirichlet‚Å¢(Œ≤)
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
        <tr class="ltx_equation ltx_align_baseline" id="S2.Ex3">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          zi‚à£Œ∏(di)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àºCategorical‚Å¢(Œ∏(di))
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
        <tr class="ltx_equation ltx_align_baseline" id="S2.Ex4">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          Œ∏(d)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àºDirichlet‚Å¢(Œ±)
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
      </div>
      <div class="ltx_para" id="S2.p3">
       <p class="ltx_p">
        Computing
        œï
        and
        Œ∏
        exactly is generally intractable,
motivating methods for approximate inference such as variational
Bayesian inference
        [3]
        , expectation propagation
        [17]
        , and collapsed Gibbs sampling
        [11]
        .
       </p>
      </div>
      <div class="ltx_para" id="S2.p4">
       <p class="ltx_p">
        A limitation of these techniques is they require multiple passes over
the data to obtain good samples of
        œï
        and
        Œ∏
        . This
requirement makes them impractical when the corpus is too large to fit
directly into memory and in particular when the corpus grows without
bound. This motivates online learning techniques, including
sampling-based methods
        [2, 7]
        and stochastic variational inference
        [12, 15, 13]
        .
However, where
these approaches generally assume the ability to draw independent samples
from the full
dataset, we consider the case when it is infeasible to access arbitrary
elements from the history.
The one existing algorithm that can be directly applied under this constraint,
to our knowledge,
is the streaming variational Bayes framework
        [4]
        in which the posterior is recursively updated as new data arrives
using a variational approximation.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Online LDA Using Particle Filters
      </h2>
      <div class="ltx_para" id="S3.p1">
       <p class="ltx_p">
        [t]
       </p>
       <span class="ltx_inline-block" style="border:1px solid #000000;padding-top:12pt;padding-bottom:12pt;">
        <p class="ltx_p">
         initialize weights
         œâ0(p)=1/P
         for
         p=1,‚Ä¶,P
        </p>
        <p class="ltx_p">
         i=1,‚Ä¶,N
         <span class="ltx_ERROR undefined">
          \For
         </span>
         <math alttext="p=1,\ldots,P" class="ltx_Math" display="inline" id="S3.p1.m4" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            p
           </mi>
           <mo>
            =
           </mo>
           <mrow>
            <mn>
             1
            </mn>
            <mo>
             ,
            </mo>
            <mi mathvariant="normal">
             ‚Ä¶
            </mi>
            <mo>
             ,
            </mo>
            <mi>
             P
            </mi>
           </mrow>
          </mrow>
         </math>
         set
         œâi(p)=œâi-1(p)ùêè(wi‚à£ùê≥i-1(p),ùê∞i-1)
        </p>
        <p class="ltx_p">
         sample
         zi(p)
         w.p.
         ùêè(zi(p)‚à£ùê≥i-1(p),ùê∞i)
         .
         <math alttext="\|\mathbf{\omega}\|_{2}^{-2}\leq" class="ltx_Math" display="inline" id="S3.p1.m8" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <msubsup>
            <mrow>
             <mo fence="true">
              ‚à•
             </mo>
             <mi>
              œâ
             </mi>
             <mo fence="true">
              ‚à•
             </mo>
            </mrow>
            <mn>
             2
            </mn>
            <mrow>
             <mo>
              -
             </mo>
             <mn>
              2
             </mn>
            </mrow>
           </msubsup>
           <mo>
            ‚â§
           </mo>
           <mi>
           </mi>
          </mrow>
         </math>
         ESS
         <span class="ltx_text ltx_font_small">
         </span>
         j‚àà‚Ñõ‚Å¢(i)
         <span class="ltx_ERROR undefined">
          \For
         </span>
         <math alttext="p=1,\ldots,P" class="ltx_Math" display="inline" id="S3.p1.m10" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            p
           </mi>
           <mo>
            =
           </mo>
           <mrow>
            <mn>
             1
            </mn>
            <mo>
             ,
            </mo>
            <mi mathvariant="normal">
             ‚Ä¶
            </mi>
            <mo>
             ,
            </mo>
            <mi>
             P
            </mi>
           </mrow>
          </mrow>
         </math>
         sample
         zj(p)
         w.p.
         ùêè(zj(p)‚à£ùê≥i\j(p),ùê∞i)
        </p>
        <p class="ltx_p">
         set
         œâi(p)=1/P
         for each particle
        </p>
        <p class="ltx_p">
         Particle filtering for LDA.
        </p>
       </span>
      </div>
      <div class="ltx_para" id="S3.p2">
       <p class="ltx_p">
        Particle filters are a family of sequential Monte Carlo (SMC) sampling
algorithms designed to estimate the posterior distribution of a system
with dynamic state
        [8]
        . A particle filter approximates the posterior by
a weighted
sample of points, or particles, from the state space.
The particle cloud is updated recursively
for each new observation using importance sampling (an approach
called
        sequential importance sampling
        ).
       </p>
      </div>
      <div class="ltx_para" id="S3.p3">
       <p class="ltx_p">
        Canini et al. (2009)
        apply this approach to
LDA after analytically integrating out
        œï
        and
        Œ∏
        , obtaining a
Rao-Blackwellized particle filter
        [9]
        that estimates
the collapsed posterior
        ùêè(ùê≥‚à£ùê∞)
        .
In this setting, the
        P
        particles are samples of the topic
assignment vector
        ùê≥(p)
        , and they are propagated forward
in state space one token at a time.
In general, the larger
        P
        is, the more accurately we
approximate the posterior; for small
        P
        ,
the approximation of the tails of the posterior will be particularly
poor
        [19]
        .
However, a larger value of
        P
        increases the runtime and storage
requirements of the algorithm.
       </p>
      </div>
      <div class="ltx_para" id="S3.p4">
       <p class="ltx_p">
        We now describe
the Rao-Blackwellized particle filter for LDA in detail (pseudocode
is given in Algorithm
        3
        ).
At the moment token
        i
        is observed, the particles form a
discrete approximation of the posterior up to the
        (i-1)
        -th word:
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx2">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex5">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          ùêè(ùê≥i-1‚à£ùê∞i-1)‚âà‚àëpœâi-1(p)Iùê≥i-1(ùê≥i-1(p))
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        where
        Iùê≥‚Å¢(ùê≥‚Ä≤)
        is the indicator function,
evaluating to 1 if
        ùê≥=ùê≥‚Ä≤
        and 0 otherwise.
Now each particle
        p
        is propagated forward by drawing a topic
        zi(p)
        from the conditional posterior distribution
        ùêè(zi(p)‚à£ùê≥i-1(p),ùê∞i)
        and scaling the particle weight by
        ùêè(wi‚à£ùê≥i-1(p),ùê∞i-1)
        . The particle
cloud now approximates the posterior up to the
        i
        -th word:
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx3">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex6">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          ùêè(ùê≥i‚à£ùê∞i)‚âà‚àëpœâi(p)Iùê≥i(ùê≥i(p)).
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        Dropping the superscript
        (p)
        for notational convenience,
the conditional posterior used in the propagation step is given by
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx4">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex7">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          ùêè(zi|ùê≥i-1,ùê∞i)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àùùêè(zi,wi‚à£ùê≥i-1,ùê∞i-1)
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex8">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
         </td>
         <td class="ltx_td ltx_align_left">
          =nzi,i\i(wi)+Œ≤nzi,i\i(‚ãÖ)+W‚Å¢Œ≤‚Å¢nzi,i\i(di)+Œ±n‚ãÖ,i\i(di)+T‚Å¢Œ±
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        where
        nzi,i\i(wi)
        is the number of times word
        wi
        has been assigned topic
        zi
        so far,
        nzi,i\i(‚ãÖ)
        is the number of times any word has been assigned
topic
        zi
        ,
        nzi,i\i(di)
        is the number of times
topic
        zi
        has been assigned to any word in document
        di
        , and
        n‚ãÖ,i\i(di)
        is the number of words observed in
document
        di
        .
The particle weights are scaled as
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx5">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex9">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          œâi(p)œâi-1(p)
         </td>
         <td class="ltx_td ltx_align_left">
          ‚àùùêè(wi‚à£ùê≥i(p),ùê∞i)ùêè(zi(p)‚à£ùê≥i-1(p))Q(zi(p)‚à£ùê≥i-1(p),ùê∞i)
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex10">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
         </td>
         <td class="ltx_td ltx_align_left">
          =ùêè(wi‚à£ùê≥i-1(p),ùê∞i-1)
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        where
        Q
        is the proposal distribution for the particle state transition; in our case,
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx6">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex11">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          Q(zi(p)‚à£ùê≥i-1(p),ùê∞i)=ùêè(zi(p)‚à£ùê≥i-1(p),ùê∞i),
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        minimizing the variance of the importance weights conditioned on
        ùê∞i
        and
        ùê≥i-1
        [9]
        .
       </p>
      </div>
      <div class="ltx_para" id="S3.p5">
       <p class="ltx_p">
        Over time the particle weights tend to diverge. To combat this
inefficiency, after every state transition we estimate the effective
sample size (ESS) of the particle weights as
        ‚à•œâi‚à•2-2
        [14]
        and resample the particles when that estimate
drops below a prespecified threshold. Several resampling strategies
have been proposed
        [9]
        ; we perform multinomial resampling
as in
        Pitt and Shephard (1999)
        and
        Ahmed et al. (2011)
        , treating the weights as
unnormalized probability masses on the particles.
       </p>
      </div>
      <div class="ltx_para" id="S3.p6">
       <p class="ltx_p">
        After resampling we are likely to have several copies of the same
particle, yielding a degenerate approximation to the posterior.
To reintroduce diversity to the particle cloud we take
MCMC steps over a sequence of states from the history
        [9, 10]
        . We call the indices of these states the
rejuvenation sequence, denoted
        ‚Ñõ‚Å¢(i)
        [7]
        .
The transition probability for a state
        j‚àà‚Ñõ‚Å¢(i)
        is given by
       </p>
       <table class="ltx_equationgroup ltx_eqn_align" id="S7.EGx7">
        <tr class="ltx_equation ltx_align_baseline" id="S3.Ex12">
         <td class="ltx_eqn_center_padleft">
         </td>
         <td class="ltx_td ltx_align_right">
          ùêè(zj‚à£ùê≥N\j,ùê∞N)‚àùnzj,N\j(wj)+Œ≤nzj,N\j(‚ãÖ)+W‚Å¢Œ≤nzj,N\j(dj)+Œ±n‚ãÖ,N\j(dj)+T‚Å¢Œ±
         </td>
         <td class="ltx_eqn_center_padright">
         </td>
        </tr>
       </table>
       <p class="ltx_p">
        where subscript
        N\j
        denotes counts up to token
        N
        , excluding those for token
        j
        .
       </p>
      </div>
      <div class="ltx_para" id="S3.p7">
       <p class="ltx_p">
        The rejuvenation sequence can be chosen by the practitioner.
Choosing a long sequence (large
        |‚Ñõ‚Å¢(i)|
        ) may result in
a more accurate posterior approximation but also increases runtime
and storage requirements. The tokens in
        ‚Ñõ‚Å¢(i)
        may be chosen uniformly
at random from the history or under a biased scheme that favors
recent observations. The particle filter studied empirically
by
        Canini et al. (2009)
        stored the entire history, incurring
linear storage complexity in the size of the stream.
        Ahmed et al. (2011)
        instead sampled ten documents from the most recent
1000, achieving constant storage complexity at the cost of a recency
bias. If we want to fit a model to a long non-i.i.d.¬†stream,
we require an unbiased rejuvenation sequence as well as sub-linear
storage complexity.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Reservoir Sampling
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        Reservoir sampling is a widely-used family of algorithms for choosing
an array (‚Äúreservoir‚Äù) of
        k
        items. The most common example,
presented in
        Vitter (1985)
        as Algorithm R, chooses
        k
        elements of a stream such that each possible subset of
        k
        elements is
equiprobable. This effects sampling
        k
        items uniformly
without replacement,
using runtime
        O‚Å¢(n)
        (constant per update) and storage
        O‚Å¢(k)
        .
       </p>
      </div>
      <div class="ltx_para" id="S4.p2">
       <span class="ltx_inline-block" style="border:1px solid #000000;padding-top:12pt;padding-bottom:12pt;">
        <p class="ltx_p">
         Initialize
         k
         -element array
         R
         Stream
         S
         <span class="ltx_ERROR undefined">
          \For
         </span>
         <math alttext="i=1,\ldots,k" class="ltx_Math" display="inline" id="S4.p2.m4" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            i
           </mi>
           <mo>
            =
           </mo>
           <mrow>
            <mn>
             1
            </mn>
            <mo>
             ,
            </mo>
            <mi mathvariant="normal">
             ‚Ä¶
            </mi>
            <mo>
             ,
            </mo>
            <mi>
             k
            </mi>
           </mrow>
          </mrow>
         </math>
         <math alttext="R[i]\leftarrow S[i]" class="ltx_Math" display="inline" id="S4.p2.m5" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mrow>
            <mi>
             R
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mrow>
             <mo>
              [
             </mo>
             <mi>
              i
             </mi>
             <mo>
              ]
             </mo>
            </mrow>
           </mrow>
           <mo>
            ‚Üê
           </mo>
           <mrow>
            <mi>
             S
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mrow>
             <mo>
              [
             </mo>
             <mi>
              i
             </mi>
             <mo>
              ]
             </mo>
            </mrow>
           </mrow>
          </mrow>
         </math>
         <span class="ltx_ERROR undefined">
          \For
         </span>
         <math alttext="i=k+1,\ldots,length(S)" class="ltx_Math" display="inline" id="S4.p2.m6" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            i
           </mi>
           <mo>
            =
           </mo>
           <mrow>
            <mrow>
             <mi>
              k
             </mi>
             <mo>
              +
             </mo>
             <mn>
              1
             </mn>
            </mrow>
            <mo>
             ,
            </mo>
            <mi mathvariant="normal">
             ‚Ä¶
            </mi>
            <mo>
             ,
            </mo>
            <mrow>
             <mi>
              l
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mi>
              e
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mi>
              n
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mi>
              g
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mi>
              t
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mi>
              h
             </mi>
             <mo>
              ‚Å¢
             </mo>
             <mrow>
              <mo>
               (
              </mo>
              <mi>
               S
              </mi>
              <mo>
               )
              </mo>
             </mrow>
            </mrow>
           </mrow>
          </mrow>
         </math>
         <math alttext="j\leftarrow random(1,i)" class="ltx_Math" display="inline" id="S4.p2.m7" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            j
           </mi>
           <mo>
            ‚Üê
           </mo>
           <mrow>
            <mi>
             r
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mi>
             a
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mi>
             n
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mi>
             d
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mi>
             o
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mi>
             m
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mrow>
             <mo>
              (
             </mo>
             <mrow>
              <mn>
               1
              </mn>
              <mo>
               ,
              </mo>
              <mi>
               i
              </mi>
             </mrow>
             <mo>
              )
             </mo>
            </mrow>
           </mrow>
          </mrow>
         </math>
         <span class="ltx_ERROR undefined">
          \If
         </span>
         <math alttext="j\leq k" class="ltx_Math" display="inline" id="S4.p2.m8" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mi>
            j
           </mi>
           <mo>
            ‚â§
           </mo>
           <mi>
            k
           </mi>
          </mrow>
         </math>
         <math alttext="R[j]\leftarrow S[i]" class="ltx_Math" display="inline" id="S4.p2.m9" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
           <mrow>
            <mi>
             R
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mrow>
             <mo>
              [
             </mo>
             <mi>
              j
             </mi>
             <mo>
              ]
             </mo>
            </mrow>
           </mrow>
           <mo>
            ‚Üê
           </mo>
           <mrow>
            <mi>
             S
            </mi>
            <mo>
             ‚Å¢
            </mo>
            <mrow>
             <mo>
              [
             </mo>
             <mi>
              i
             </mi>
             <mo>
              ]
             </mo>
            </mrow>
           </mrow>
          </mrow>
         </math>
        </p>
        <p class="ltx_p">
         Algorithm R for reservoir sampling
        </p>
       </span>
      </div>
      <div class="ltx_para" id="S4.p3">
       <p class="ltx_p">
        To ensure constant space over an unbounded stream,
we draw the rejuvenation sequence
        ‚Ñõ‚Å¢(i)
        uniformly from a reservoir.
As
each token of the training data is ingested by the particle filter, we
decide to insert that token into the reservoir, or not,
independent of the other tokens in the current document. Thus, at the
end of step
        i
        of the particle filter, each of the
        i
        tokens seen so
far in the training sequence has an equal probability of being in the
reservoir, hence being selected for rejuvenation.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Experiments
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        We evaluate our particle filter on three datasets studied in
        Canini et al. (2009)
        :
        diff3
        ,
        rel3
        , and
        sim3
        . Each of these datasets is a collection of posts under
three categories from the 20¬†Newsgroups
dataset.
        We use
a 60% training/40% testing split of this data that is available online.
       </p>
      </div>
      <div class="ltx_para" id="S5.p2">
       <p class="ltx_p">
        We preprocess the data by splitting each line on non-alphabet
characters, converting the resulting tokens to lower-case, and
filtering out any tokens that appear in a list of common English stop
words. In addition, we remove the header of every file and filter
every line that does not contain a non-trailing space (which removes
embedded ASCII-encoded attachments). Finally, we shuffle the order of
the documents. After these steps, we compute the vocabulary for each
dataset as the set of all non-singleton types in the training data
augmented with a special out-of-vocabulary symbol.
       </p>
      </div>
      <div class="ltx_para" id="S5.p3">
       <p class="ltx_p">
        During training we report the out-of-sample NMI, calculated by holding the word proportions
        œï
        fixed, running five sweeps of collapsed Gibbs sampling on the test set, and computing the topic for each document as the topic assigned to the most tokens in that document. Two Gibbs sweeps have been shown to yield good performance in practice
        [23]
        ; we increase the number of sweeps to five after inspecting the stability on our dataset.
The variance of the particle filter is often large, so for each experiment we perform 30 runs and plot the mean NMI inside bands spanning one sample standard deviation in either direction.
       </p>
      </div>
      <div class="ltx_paragraph" id="S5.SS0.SSS0.P1">
       <h3 class="ltx_title ltx_title_paragraph">
        Fixed Initialization.
       </h3>
       <div class="ltx_para" id="S5.SS0.SSS0.P1.p1">
        <p class="ltx_p">
         Our first set of experiments has a similar
parameterization
         to the experiments of
         Canini et al. (2009)
         except we draw the rejuvenation sequence from a reservoir.
We initialize the particle filter with 200 Gibbs sweeps on the first 10% of each dataset. Then, for each dataset, for rejuvenation disabled, rejuvenation based on a reservoir of size 1000, and rejuvenation based on the entire history (in turn), we perform 30 runs of the particle filter from that fixed initial model. Our results (Figure
         1
         ) resemble those of
         Canini et al. (2009)
         ; we believe the discrepancies are mostly attributable to differences in preprocessing.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS0.SSS0.P1.p2">
        <p class="ltx_p">
         In these experiments, the initial model was not chosen arbitrarily. Rather, an initial model that yielded out-of-sample NMI close to the initial out-of-sample NMI scores reported in the previous study was chosen from a set of 100 candidates.
        </p>
       </div>
      </div>
      <div class="ltx_paragraph" id="S5.SS0.SSS0.P2">
       <h3 class="ltx_title ltx_title_paragraph">
        Variable Initialization.
       </h3>
       <div class="ltx_para" id="S5.SS0.SSS0.P2.p1">
        <p class="ltx_p">
         We now investigate the significance of the initial model selection step used in the previous experiments.
We run a new set of experiments in which the reservoir size is held fixed at 1000 and the size of the initialization sample is varied. Specifically, we vary the size of the initialization sample, in documents, between zero (corresponding to no Gibbs initialization), 30, 100, and 300, and also perform a run of batch Gibbs sampling (with no particle filter). In each case, 2000 Gibbs sweeps are performed. In these experiments, the initial models are not held fixed; for each of the 30 runs for each dataset, the initial model was generated by a different Gibbs chain. The results for these experiments, depicted in Figure
         2
         , indicate that the size of the initialization sample improves mean NMI and reduces variance, and that the variance of the particle filter itself is dominated by the variance of the initial model.
        </p>
       </div>
      </div>
      <div class="ltx_paragraph" id="S5.SS0.SSS0.P3">
       <h3 class="ltx_title ltx_title_paragraph">
        Tuned Initialization.
       </h3>
       <div class="ltx_para" id="S5.SS0.SSS0.P3.p1">
        <p class="ltx_p">
         We observed previously that variance in the Gibbs initialization of the model contributes significantly to variance of the overall algorithm, as measured by NMI. With this in mind, we consider whether we can reduce variance in the initialization by tuning the initial model. Thus we perform a set of experiments in which we perform Gibbs initialization 20 times on the initialization set, setting the particle filter‚Äôs initial model to the model out of these 20 with the highest in-sample NMI. This procedure is performed independently for each run of the particle filter. We may not always have labeled data for initialization, so we also consider a variation in which Gibbs initialization is performed 20 times on the first 80% of the initialization sample, held-out perplexity (per word) is estimated on the remaining 20%, using a first-moment particle learning approximation
         [20]
         , and the particle filter is started from the model out of these 20 with the lowest held-out perplexity. The results, shown in Figure
         3
         , show that we can ameliorate the variance due to initialization by tuning the initial model to NMI or perplexity.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S6">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        6
       </span>
       Discussion
      </h2>
      <div class="ltx_para" id="S6.p1">
       <p class="ltx_p">
        Motivated by a desire for cognitive plausibility,
        B√∂rschinger and Johnson (2011)
        used a particle filter to learn Bayesian
word segmentation models, following the work of
        Canini et al. (2009)
        . They later showed that rejuvenation improved
performance
        [6]
        , but this impaired cognitive
plausibility by necessitating storage of all previous states and
observations. We attempted to correct this by drawing the
rejuvenation sequence from a reservoir, but our results indicate that
the particle filter for LDA on our dataset is highly sensitive to
initialization and not influenced by rejuvenation.
       </p>
      </div>
      <div class="ltx_para" id="S6.p2">
       <p class="ltx_p">
        In the experiments of
        B√∂rschinger and Johnson (2012)
        , the particle
cloud appears to be resampled once per utterance with a large
rejuvenation sequence;
        each particle takes many more rejuvenation MCMC steps than
new state transitions and thus resembles a batch MCMC sampler.
In our experiments resampling is done on the order of once
per document,
leading to less than one rejuvenation step per transition.
Future work should carefully note this ratio: sampling
history much more often than new states improves performance
but contradicts the intuition behind particle filters.
       </p>
      </div>
      <div class="ltx_para" id="S6.p3">
       <p class="ltx_p">
        We have also shown that tuning the initial model using in-sample NMI or held-out perplexity can improve mean NMI and reduce variance.
Perplexity (or likelihood) is often used to estimate model performance in LDA
        [3, 11, 22, 12]
        , and does not compare the inferred model against gold-standard labels,
yet it appears to be a good proxy for NMI in our experiment.
Thus, if initialization continues to be crucial to performance, at least we may have the flexibility of initializing without gold-standard labels.
       </p>
      </div>
      <div class="ltx_para" id="S6.p4">
       <p class="ltx_p">
        We have focused on NMI as our evaluation metric for comparison with
        Canini et al. (2009)
        . However, evaluation of topic models is a subject of considerable debate
        [22, 23, 18, 16]
        and it may be informative to investigate the effects of initialization and rejuvenation using other metrics such as perplexity or semantic coherence.
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
