<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:147">
</head>
<body bgcolor="white">
<a href="#0" id="0">Here the environment is the user.</a>
<a href="#1" id="1">These values were chosen with experimentation and the basic idea is that the agent should learn faster when “losing” and slower when “winning”.</a>
<a href="#2" id="2">The two agents have different goals.</a>
<a href="#3" id="3">Or alternatively concurrent learning could be used off-line to bootstrap the policies and then these policies could be improved via live interaction with human users (again using concurrent learning to address possible changes in user behavior).</a>
<a href="#4" id="4">With regard to using RL for learning negotiation policies, the amount of research that has been performed is very limited compared to slot-filling.</a>
<a href="#5" id="5">In the second case which from now on will be referred to as PHC-LF, we set δ to be equal to δL⁢F (also used for PHC-WoLF).</a>
<a href="#6" id="6">Gašić et al. (2013) showed that it is possible to learn “full” dialogue policies just via interaction with human users (without any bootstrapping using corpora or SUs).</a>
</body>
</html>