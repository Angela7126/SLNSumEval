<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:125">
</head>
<body bgcolor="white">
<a href="#0" id="0">As translation equivalents share the same semantic meaning, we employ high-quality phrase translation pairs as training corpus in this work.</a>
<a href="#1" id="1">In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error [22] , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs.</a>
<a href="#2" id="2">This section introduces the Bilingually-constrained Recursive Auto-encoders (BRAE), that is inspired by two observations.</a>
<a href="#3" id="3">Accordingly, our BRAE model is trained on Chinese and English.</a>
<a href="#4" id="4">In contrast, our BRAE model learns the semantic meaning for each phrase no matter whether it is short or relatively long.</a>
<a href="#5" id="5">There are three main perspectives handling this task in monolingual languages.</a>
</body>
</html>