<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:105">
</head>
<body bgcolor="white">
<a href="#0" id="0">•</a>
<a href="#1" id="1">Figure 1 shows example strings generated in this way: the network seems to prefer to output pseudo-tweets written consistently in a single script with words and pseudo-words mostly from a single language.</a>
<a href="#2" id="2">Many approaches to text normalization adopt the noisy channel setting, where the model normalizing source string s into target canonical form t is factored into two parts: t^=arg⁢maxtP(t)P(s|t) .</a>
<a href="#3" id="3">The training data for the model is generated by computing shortest edit scripts for pairs of original and normalized strings.</a>
<a href="#4" id="4">In comparison to our first-order linear-chain CRF, an MT model with reordering is more flexible but for this reason needs more training data.</a>
</body>
</html>