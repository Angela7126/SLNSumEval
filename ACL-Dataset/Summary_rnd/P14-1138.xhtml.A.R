<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:126">
</head>
<body bgcolor="white">
<a href="#0" id="0">In B⁢T⁢E⁢C , R⁢N⁢Nu and R⁢N⁢Nu+c significantly outperform R⁢N⁢Ns⁢(I) and R⁢N⁢Ns+c⁢(I) , respectively.</a>
<a href="#1" id="1">The model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure 1 ).</a>
<a href="#2" id="2">In a simple implementation, each 𝒆is generated by repeating a random sampling from a set of target words ( Ve ) |𝒆+| times and lining them up sequentially.</a>
<a href="#3" id="3">However, the FFNN-based model assumes a first-order Markov dependence for alignments.</a>
<a href="#4" id="4">Other weights were randomly initialized to [-0.1,0.1] .</a>
<a href="#5" id="5">The IBM Model 1 with l0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1.</a>
<a href="#6" id="6">These results indicate that our proposals contribute to improving translation performance .</a>
</body>
</html>