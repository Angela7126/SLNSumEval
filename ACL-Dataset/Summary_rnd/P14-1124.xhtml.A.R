<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:155">
</head>
<body bgcolor="white">
<a href="#0" id="0">αw =(1-e-DFw)⋅P^a⁢d⁢a⁢p⁢t⁢(w) (9)</a>
<a href="#1" id="1">In order to improve detection performance, and restricting ourselves to an existing ASR system or systems at our disposal, we focus on leveraging broad document context around detection hypotheses.</a>
<a href="#2" id="2">We see a somewhat different picture for Tagalog speech in Figure 7 .</a>
<a href="#3" id="3">This approach shows a significant improvement (0.7% absolute) over the baseline.</a>
<a href="#4" id="4">In the information retrieval community, clustering and latent topic models have yielded improvements over traditional vector space models.</a>
<a href="#5" id="5">Ew⁢[k⁢|k>⁢0] =fwDFw (3) as a function of fw .</a>
<a href="#6" id="6">Given a small vocabulary of interest (1000-2000 words or multi-word terms) the aim of the term detection task is to enumerate occurrences of the keywords within a target corpus.</a>
<a href="#7" id="7">For the Tagalog conversations, as with English newswire, we observe that the document frequency, DFw , of a word w is not a linear function of word frequency fw in the log domain, as would be expected under a naive Poisson generative assumption.</a>
</body>
</html>