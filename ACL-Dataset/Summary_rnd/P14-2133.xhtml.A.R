<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:113">
</head>
<body bgcolor="white">
<a href="#0" id="0">As β grows large words become more independent (and in the limit where β=∞ , each summand in Equation 1 is zero except where w′=w , and we recover the original direct-lookup model).</a>
<a href="#1" id="1">For each dimension i , we create an indicator feature corresponding to the linearly-bucketed value of the feature at that index.</a>
<a href="#2" id="2">But we don’t know how prevalent or important such “syntactic axes” are in practice.</a>
<a href="#3" id="3">We are interested in the question of whether a state-of-the-art discrete-variable constituency parser can be improved with word embeddings, and, more precisely, what aspect (or aspects) of the parser can be altered to make effective use of embeddings.</a>
<a href="#4" id="4">Next we consider the lexicon pooling model.</a>
</body>
</html>