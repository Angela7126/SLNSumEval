<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:165">
</head>
<body bgcolor="white">
<a href="#0" id="0">Figure 6 shows that our approaches consistently outperform the baseline and the state-of-the-art methods with diverse feature sparsity degrees.</a>
<a href="#1" id="1">We thus define ğ™=ğ™*+ğ„, where ğ™* as the underlying low-rank matrix ğ™*=[X*Y*]=[Xtâ¢râ¢aâ¢iâ¢n*Ytâ¢râ¢aâ¢iâ¢n*Xtâ¢eâ¢sâ¢t*Ytâ¢eâ¢sâ¢t*], and E is the error matrix ğ„=[EXtâ¢râ¢aâ¢iâ¢nEYtâ¢râ¢aâ¢iâ¢nEXtâ¢eâ¢sâ¢t0].</a>
<a href="#2" id="2">Shrinkage step:</a>
<a href="#3" id="3">We extract diverse textual features from all those relation mentions and combine them into a rich feature vector labeled by the relation names ( President-of and Born-in ) to produce a weak training corpus for relation classification.</a>
<a href="#4" id="4">C(u,v)=-logPr(u|v)=log(1+e-uâ¢v),</a>
<a href="#5" id="5">To tolerate the noise entries in the error matrix ğ„ , we minimize the cost functions Cx and Cy for features and labels respectively, rather than using the hard constraints in Formula (2).</a>
<a href="#6" id="6">We relax the feature filtering threshold ( Î¸=4,3,2 ) in Surdeanu et al.â€™s [24] open source program to generate more sparse features from NYTâ€™10 dataset.</a>
<a href="#7" id="7">We use the gradient descents ğ€=ğ™-Ï„zâ¢gâ¢(ğ™) and ğ›=ğ›-Ï„bâ¢gâ¢(ğ›) to gradually find the global minima of the cost function terms in Formula (3), where Ï„z and Ï„b are step sizes.</a>
</body>
</html>