<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:125">
</head>
<body bgcolor="white">
<a href="#0" id="0">The character embeddings are then stacked into a embedding matrix M∈ℝd×|D| .</a>
<a href="#1" id="1">In linear models, these kinds of interactions are usually modeled as features.</a>
<a href="#2" id="2">To model the tag dependency, previous neural network models [6, 35] introduce a transition score Ai⁢j for jumping from tag i∈T to tag j∈T .</a>
<a href="#3" id="3">Our tensor factorization is related to these work but uses a different tensor factorization approach.</a>
<a href="#4" id="4">We hypothesize that larger factor size results in too many parameters to train and hence perform worse.</a>
<a href="#5" id="5">Formally, we assume the extracted features form a feature dictionary Df .</a>
<a href="#6" id="6">In this way, the tag representation can be directly incorporated in the neural network so that the tag-tag interaction and tag-character interaction can be explicitly modeled in deeper layers (See Section 3.2).</a>
</body>
</html>