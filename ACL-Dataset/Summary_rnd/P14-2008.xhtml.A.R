<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:164">
</head>
<body bgcolor="white">
<a href="#0" id="0">This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences.</a>
<a href="#1" id="1">4.3 Discussion</a>
<a href="#2" id="2">The semi-supervised mSDA performs quite well with respect to the fully supervised approaches, obtaining the best results for books and electronics, which are also the highest scores overall.</a>
<a href="#3" id="3">Recently, the value of this information has been shown in practical applications such as information retrieval (IR) [25] , summarization [24] , and even identifying scientific breakthroughs [27] .</a>
<a href="#4" id="4">We train on each of the domains from the MDSD – books, dvd, electronics, and kitchen – and test on the citation data.</a>
<a href="#5" id="5">These experiments should help answer two questions: does a larger amount of training data, even if out of domain, improve citation classification; and how well do the different product domains generalize to citations (i.e., which domains are most similar to citations)?</a>
<a href="#6" id="6">In this paper we successfully use a large, out-of-domain, annotated corpus to improve the citation polarity classification.</a>
</body>
</html>