<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:162">
</head>
<body bgcolor="white">
<a href="#0" id="0">3 Generalized Language Models 3.1 Notation for Skip n -gram with k Skips</a>
<a href="#1" id="1">After explaining the evaluation methodology and introducing the data sets in Section 4 we will present the results of our evaluation in Section 5 .</a>
<a href="#2" id="2">Then we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data.</a>
<a href="#3" id="3">Appendix A</a>
<a href="#4" id="4">All data sets have been randomly split into a training and a test set on a sentence level.</a>
<a href="#5" id="5">Because of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly.</a>
<a href="#6" id="6">This indicates that the superior performance of the GLM on small training corpora and for higher order models indeed comes from its good performance properties with regard to sparse training data.</a>
<a href="#7" id="7">The term γm⁢i⁢d is again an interpolation factor which depends on the discounted probability mass D in the first term of the formula.</a>
</body>
</html>