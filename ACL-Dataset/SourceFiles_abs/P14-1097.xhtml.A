<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        A verb plays a primary role in conveying the meaning of a
sentence. Capturing the sense of a verb is essential for natural
language processing (NLP), and thus lexical resources for verbs play an
important role in NLP.
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        Verb classes are one such lexical resource. Manually-crafted verb
classes have been developed, such as Levin’s classes
        [16]
        and their extension, VerbNet
        [12]
        , in which verbs are
organized into classes on the basis of their syntactic and semantic
behavior. Such verb classes have been used in many NLP applications that
need to consider semantics in particular, such as word sense
disambiguation
        [4]
        , semantic parsing
        [41, 33]
        and discourse parsing
        [37]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        There have also been many attempts to automatically acquire verb classes
with the goal of either adding frequency information to
an existing resource or of inducing similar verb classes for other
languages. Most of these approaches assume that all target
verbs are monosemous
        [36, 32, 9, 18, 38, 39, 45, 26, 27, 7, 19, 29, 40]
        .
This monosemous assumption, however, is not realistic because many
frequent verbs actually have multiple senses.
Moreover, to the best of our knowledge,
none of the following approaches attempt to
quantitatively evaluate soft clusterings of verb classes induced by
polysemy-aware unsupervised approaches
        [14, 15, 17, 31]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        In this paper, we propose an unsupervised method for inducing verb
classes that is aware of verb polysemy. Our method consists of two
clustering steps: verb-specific semantic frames are first induced by
clustering verb uses in a corpus and then verb classes are induced by
clustering these frames. By taking this step-wise approach, we can not
only induce verb classes with frequency information from a massive
amount of verb uses in a scalable manner, but also deal with verb polysemy.
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        Our novel contributions are summarized as follows:
       </p>
      </div>
      <div class="ltx_para" id="S1.p6">
       <ul class="ltx_itemize" id="I1">
        <li class="ltx_item" id="I1.i1" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_itemize">
          •
         </span>
         <div class="ltx_para" id="I1.i1.p1">
          <p class="ltx_p">
           induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,
          </p>
         </div>
        </li>
        <li class="ltx_item" id="I1.i2" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_itemize">
          •
         </span>
         <div class="ltx_para" id="I1.i2.p1">
          <p class="ltx_p">
           explicitly deal with verb polysemy,
          </p>
         </div>
        </li>
        <li class="ltx_item" id="I1.i3" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_itemize">
          •
         </span>
         <div class="ltx_para" id="I1.i3.p1">
          <p class="ltx_p">
           discover effective features for each of the clustering steps, and
          </p>
         </div>
        </li>
        <li class="ltx_item" id="I1.i4" style="list-style-type:none;">
         <span class="ltx_tag ltx_tag_itemize">
          •
         </span>
         <div class="ltx_para" id="I1.i4.p1">
          <p class="ltx_p">
           quantitatively evaluate a soft clustering of verbs.
          </p>
         </div>
        </li>
       </ul>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Related Work
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        As stated in Section
        1
        , most of the previous
studies on verb clustering assume that verbs are monosemous. A typical
method in these studies is to represent each verb as a single data point and apply
classification (e.g.,
        Joanis et al. (2008)
        ) or clustering (e.g.,
        Sun and Korhonen (2009)
        ) to these data points. As a representation for a data
point, distributions of subcategorization frames are often used, and
other semantic features (e.g., selectional preferences) are sometimes added to
improve the performance.
       </p>
      </div>
      <div class="ltx_para" id="S2.p2">
       <p class="ltx_p">
        Among these studies on monosemous verb clustering (i.e., predominant
class induction), there have been several Bayesian methods.
        Vlachos et al. (2009)
        proposed a Dirichlet
process mixture model (DPMM;
        Neal (2000)
        ) to cluster verbs based
on subcategorization frame distributions. They evaluated their result
with a gold-standard test set, where a single class is assigned to a
verb.
        Parisien and Stevenson (2010)
        proposed a hierarchical Dirichlet process (HDP;
        Teh et al. (2006)
        ) model to jointly learn argument structures
(subcategorization frames) and verb classes by using syntactic
features.
        Parisien and Stevenson (2011)
        extended their model by adding semantic
features. They tried to account for verb learning by children and did
not evaluate the resultant verb classes.
        Modi et al. (2012)
        extended
the model of
        Titov and Klementiev (2012)
        , which is an unsupervised model for
inducing semantic roles, to jointly induce semantic roles and frames
across verbs using the Chinese Restaurant Process
        [1]
        .
All of the above methods considered verbs to be monosemous and did not
deal with verb polysemy. Our approach also uses Bayesian methods, but is
designed to capture verb polysemy.
       </p>
      </div>
      <div class="ltx_para" id="S2.p3">
       <p class="ltx_p">
        We summarize a few studies that consider polysemy of verbs in the rest
of this section.
       </p>
      </div>
      <div class="ltx_para" id="S2.p4">
       <p class="ltx_p">
        Miyao and Tsujii (2009)
        proposed a supervised method that can handle verb
polysemy. Their method represents a verb’s syntactic and semantic
features, and learns a log-linear model from the SemLink corpus
        [20]
        .
        Boleda et al. (2007)
        also proposed a supervised method
for Catalan adjectives considering the polysemy of adjectives.
       </p>
      </div>
      <div class="ltx_para" id="S2.p5">
       <p class="ltx_p">
        The most closely related work to our polysemy-aware task of unsupervised
verb class induction is the work of
        Korhonen et al. (2003)
        , who used distributions
of subcategorization frames to cluster verbs. They adopted the Nearest
Neighbor (NN) and Information Bottleneck (IB) methods for clustering. In
particular, they tried to consider verb polysemy by using the IB method,
which is a soft clustering method
        [43]
        . However, the verb
itself is still represented as a single data point.
After performing soft clustering, they noted that most verbs fell into a
single class, and they decided to assign a single class to each verb by
hardening the clustering. They considered multiple classes only in
the gold-standard data used for their evaluations. We also evaluate
our induced verb classes on this gold-standard data, which was created
on the basis of Levin’s classes
        [16]
        .
       </p>
      </div>
      <div class="ltx_para" id="S2.p6">
       <p class="ltx_p">
        Lapata and Brew (2004)
        and
        Li and Brew (2007)
        proposed probabilistic models
for calculating prior probabilities of verb classes for a verb. These
models are approximated to condition not on verbs but on
subcategorization frames. As mentioned in
        Li and Brew (2007)
        , it is
desirable to extend the model to depend on verbs to further improve
accuracy. They conducted several evaluations including predominant class
induction and token-level verb sense disambiguation, but did not
evaluate multiple classes output by their models.
        Schulte im Walde et al. (2008)
        also applied probabilistic soft clustering
to verbs by incorporating subcategorization frames and selectional
preferences based on WordNet. This model is based on the
Expectation-Maximization algorithm and the Minimum Description Length
principle. Since they focused on the incorporation of selectional
preferences, they did not evaluate verb classes but evaluated only
selectional preferences using a language model-based measure.
       </p>
      </div>
      <div class="ltx_para" id="S2.p7">
       <p class="ltx_p">
        Materna proposed LDA-frames, which are defined across verbs and can be
considered to be a kind of verb class
        [21, 22]
        .
LDA-frames are probabilistic semantic frames automatically induced from
a raw corpus. He used a model based on latent Dirichlet allocation (LDA;
        Blei et al. (2003)
        )
and the Dirichlet process to cluster verb instances of a triple
(subject, verb, object) to produce semantic frames and roles. Both of
these are represented as a probabilistic distribution of words across
verbs. He applied this method to the BNC and acquired 1,200 frames and
400 roles
        [21]
        . He did not evaluate the resulting frames
as verb classes.
       </p>
      </div>
      <div class="ltx_para" id="S2.p8">
       <p class="ltx_p">
        In sum, there have been no studies that quantitatively evaluate
polysemous verb classes automatically induced by unsupervised methods.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Our Approach
      </h2>
      <div class="ltx_subsection" id="S3.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.1
        </span>
        Overview
       </h3>
       <div class="ltx_para" id="S3.SS1.p1">
        <p class="ltx_p">
         Our objective is to automatically learn semantic frames and verb classes
from a massive amount of verb uses following usage-based
approaches. Although Bayesian approaches are a possible solution to
simultaneously induce frames and verb classes from a corpus as used in
previous studies, it has prohibitive computational cost. For
instance, Parisien and Stevenson applied HDP only to a
small-scale child speech corpus that contains 170K verb uses to jointly
induce subcategorization frames and verb classes
         [26, 27]
         . Materna applied an LDA-based method to
the BNC, which contains 1.4M verb uses, to induce semantic frames across
verbs that can be considered to be verb classes
         [21, 22]
         . However, it would take three months for
this experiment using this 100 million word corpus.
         Although it is best to use the largest possible corpus
for this kind of knowledge acquisition tasks
         [30]
         , it
is infeasible to scale to giga-word corpora using such joint
models.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p2">
        <p class="ltx_p">
         In this paper, we propose a two-step approach for inducing semantic
frames and verb classes. First, we make multiple data points for each
verb to deal with verb polysemy (cf. polysemy-aware previous studies still
represented a verb as one data point
         [14, 23]
         ). To
do that, we induce verb-specific semantic frames by clustering verb
uses. Then, we induce verb classes by clustering these verb-specific
semantic frames across verbs. An interesting point here is that we can use
exactly the same method for these two clustering steps.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p3">
        <p class="ltx_p">
         Our procedure to automatically induce verb classes from verb uses
is summarized as follows:
        </p>
        <ol class="ltx_enumerate" id="I2">
         <li class="ltx_item" id="I2.i1" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           1.
          </span>
          <div class="ltx_para" id="I2.i1.p1">
           <p class="ltx_p">
            induce verb-specific semantic frames by clustering predicate-argument
structures for each verb extracted from automatic parses as shown in the lower part of Figure
            1
            , and
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I2.i2" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           2.
          </span>
          <div class="ltx_para" id="I2.i2.p1">
           <p class="ltx_p">
            induce verb classes by clustering the induced semantic frames
across verbs as shown in the upper part of Figure
            1
            .
           </p>
          </div>
         </li>
        </ol>
        <p class="ltx_p">
         Each of these two steps is described in the following sections in detail.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.2
        </span>
        Inducing Verb-specific Semantic Frames
       </h3>
       <div class="ltx_para" id="S3.SS2.p1">
        <p class="ltx_p">
         We induce verb-specific semantic frames from verb uses based on the
method of
         Kawahara et al. (2014)
         . Our semantic frames consist of case
slots, each of which consists of word instances that can be filled. The
procedure for inducing these semantic frames is as follows:
        </p>
        <ol class="ltx_enumerate" id="I3">
         <li class="ltx_item" id="I3.i1" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           1.
          </span>
          <div class="ltx_para" id="I3.i1.p1">
           <p class="ltx_p">
            apply dependency parsing to a raw corpus and extract
predicate-argument structures for each verb from the automatic
parses,
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I3.i2" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           2.
          </span>
          <div class="ltx_para" id="I3.i2.p1">
           <p class="ltx_p">
            merge the predicate-argument structures that have presumably the same
meaning based on the assumption of one sense per collocation
            [46]
            to get a set of initial frames, and
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I3.i3" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           3.
          </span>
          <div class="ltx_para" id="I3.i3.p1">
           <p class="ltx_p">
            apply clustering to the initial frames based on the Chinese
Restaurant Process
            [1]
            to produce verb-specific semantic frames.
           </p>
          </div>
         </li>
        </ol>
        <p class="ltx_p">
         These three steps are briefly described below.
        </p>
       </div>
       <div class="ltx_subsubsection" id="S3.SS2.SSS1">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          3.2.1
         </span>
         Extracting Predicate-argument Structures from a Raw Corpus
        </h4>
        <div class="ltx_para" id="S3.SS2.SSS1.p1">
         <p class="ltx_p">
          We apply dependency parsing to a large raw corpus. We use the Stanford
parser with Stanford dependencies
          [5]
          .
          Collapsed dependencies are adopted to directly extract prepositional
phrases.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS1.p2">
         <p class="ltx_p">
          Then, we extract predicate-argument structures from the dependency
parses. Dependents that have the following dependency relations to a
verb are extracted as arguments:
         </p>
         <blockquote class="ltx_quote">
          <p class="ltx_p">
           nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep_
           *
          </p>
         </blockquote>
         <p class="ltx_p">
          In this process, the verb and arguments are lemmatized, and only the
head of an argument is preserved for compound nouns.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS1.p3">
         <p class="ltx_p">
          Predicate-argument structures are collected for each verb and
the subsequent processes are applied to the predicate-argument
structures of each verb.
         </p>
        </div>
       </div>
       <div class="ltx_subsubsection" id="S3.SS2.SSS2">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          3.2.2
         </span>
         Constructing Initial Frames from Predicate-argument Structures
        </h4>
        <div class="ltx_para" id="S3.SS2.SSS2.p1">
         <p class="ltx_p">
          To make the computation feasible, we merge the predicate-argument
structures that have the same or similar meaning to get initial
frames. These initial frames are the input of the subsequent clustering
process. For this merge, we assume one sense per collocation
          [46]
          for predicate-argument structures.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS2.p2">
         <p class="ltx_p">
          For each predicate-argument structure of a verb, we couple the verb and an
argument to make a unit for sense disambiguation. We select an argument in the following order by considering the degree
of effect on the verb sense:
         </p>
         <blockquote class="ltx_quote">
          <p class="ltx_p">
           dobj, ccomp, nsubj, prep_
           *
           , iobj.
          </p>
         </blockquote>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS2.p3">
         <p class="ltx_p">
          Then, the predicate-argument structures that have the same verb and argument
pair (slot and word, e.g., “dobj:effect”) are merged into an initial
frame. After this process, we discard minor initial frames that occur
fewer than 10 times.
         </p>
        </div>
       </div>
       <div class="ltx_subsubsection" id="S3.SS2.SSS3">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          3.2.3
         </span>
         Clustering Method
        </h4>
        <div class="ltx_para" id="S3.SS2.SSS3.p1">
         <p class="ltx_p">
          We cluster initial frames for each verb to produce semantic frames using
the Chinese Restaurant Process
          [1]
          , regarding each initial
frame as an instance.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS3.p2">
         <p class="ltx_p">
          We calculate the posterior probability of a cluster
          cj
          given an
initial frame
          fi
          as follows:
         </p>
         P(cj|fi)∝{n⁢(cj)N+α⋅P(fi|cj)cj≠n⁢e⁢wαN+α⋅P(fi|cj)cj=n⁢e⁢w,

(1)
         <p class="ltx_p">
          where
          N
          is the number of initial frames for the target verb and
          n⁢(cj)
          is the current number of initial frames assigned to the
cluster
          cj
          .
          α
          is a hyper-parameter that determines how
likely it is for a new cluster to be created. In this equation,
the first term is the Dirichlet process prior and the second term is the
likelihood of
          fi
          .
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS3.p3">
         <p class="ltx_p">
          P(fi|cj)
          is defined based on the Dirichlet-Multinomial distribution
as follows:
         </p>
         P(fi|cj)=∏w∈VP(w|cj)c⁢o⁢u⁢n⁢t⁢(fi,w),

(2)
         <p class="ltx_p">
          where
          V
          is the vocabulary in all case slots cooccurring with the verb
and
          c⁢o⁢u⁢n⁢t⁢(fi,w)
          is the number of
          w
          in the initial frame
          fi
          .
The original method in
          Kawahara et al. (2014)
          defined
          w
          as pairs of
slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not
consider slot-only features, e.g., “nsubj” and
“dobj,” which ignore lexical information. Here we experiment with both
representations and compare the results.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS3.p4">
         <p class="ltx_p">
          P(w|cj)
          is defined as follows:
         </p>
         P(w|cj)=c⁢o⁢u⁢n⁢t⁢(cj,w)+β∑t∈Vc⁢o⁢u⁢n⁢t⁢(cj,t)+|V|⋅β,

(3)
         <p class="ltx_p">
          where
          c⁢o⁢u⁢n⁢t⁢(cj,w)
          is the current number of
          w
          in the cluster
          cj
          ,
and
          β
          is a hyper-parameter of Dirichlet distribution. For a new
cluster, this probability is uniform (
          1/|V|
          ).
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS3.p5">
         <p class="ltx_p">
          We regard each output cluster as a semantic frame, by merging the initial
frames in a cluster into a semantic frame. In this way, semantic frames
for each verb are acquired.
         </p>
        </div>
        <div class="ltx_para" id="S3.SS2.SSS3.p6">
         <p class="ltx_p">
          We use Gibbs sampling to realize this clustering.
         </p>
        </div>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.3
        </span>
        Inducing Verb Classes from Semantic Frames
       </h3>
       <div class="ltx_para" id="S3.SS3.p1">
        <p class="ltx_p">
         To induce verb classes across verbs, we apply clustering to the induced
verb-specific semantic frames. We can use exactly the same clustering
method as described in Section
         3.2.3
         by using
semantic frames for multiple verbs as an input instead of initial frames
for a single verb. This is because an initial frame has the same structure as
a semantic frame, which is produced by merging initial frames. We
regard each output cluster as a verb class this time.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p2">
        <p class="ltx_p">
         For the features,
         w
         , in equation (
         2
         ), we
try the two representations again: slot-only features and slot-word pair
features. The representation using only slots corresponds to the
consideration of only syntactic argument patterns. The other
representation using the slot-word pairs means that semantic similarity
based on word overlap is naturally considered by looking at lexical
information. We will compare in our experiments four possible
combinations: two feature representations for each of the
two clustering steps.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Experiments and Evaluations
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        We first describe our experimental settings and define evaluation
metrics to evaluate induced soft clusterings of verb classes. Then, we
conduct type-level multi-class evaluations, type-level single-class
evaluations and token-level multi-class evaluations. These two levels of
evaluations are performed by considering the work of
        Reichart et al. (2010)
        on clustering evaluation. Finally, we discuss the
results of our full experiments.
       </p>
      </div>
      <div class="ltx_subsection" id="S4.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.1
        </span>
        Experimental Settings
       </h3>
       <div class="ltx_para" id="S4.SS1.p1">
        <p class="ltx_p">
         We use two kinds of large-scale corpora: a web corpus and the English
Gigaword corpus.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p2">
        <p class="ltx_p">
         To prepare a web corpus, we extracted sentences from crawled web pages
that are judged to be written in English based on the encoding
information. Then, we selected sentences that consist of at most
40 words, and removed duplicated sentences. From this process, we
obtained a corpus of one billion sentences, totaling
approximately 20 billion words. We focused on verbs whose frequency in
the web corpus was
more than 1,000. There were 19,649 verbs, including phrasal verbs, and
separating passive and active constructions. We extracted 2,032,774,982
predicate-argument structures.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p3">
        <p class="ltx_p">
         We also used the English Gigaword corpus (LDC2011T07; English Gigaword
Fifth Edition). This corpus consists of
approximately 180 million sentences, which totaling four billion
words. There were 7,356 verbs after applying the same frequency threshold as
the web corpus. We extracted 423,778,278 predicate-argument structures
from this corpus.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p4">
        <p class="ltx_p">
         We set the hyper-parameters
         α
         in (
         1
         ) and
         β
         in (
         3
         ) to 1.0. The cluster
assignments for all the components were initialized randomly. We took 100
samples for each input frame and selected the cluster assignment that has
the highest probability.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.2
        </span>
        Evaluation Metrics
       </h3>
       <div class="ltx_para" id="S4.SS2.p1">
        <p class="ltx_p">
         To measure the precision and recall of a clustering, modified
purity and inverse purity (also called collocation or weighted class
accuracy) are commonly used in previous studies on verb clustering
(e.g.,
         Sun and Korhonen (2009)
         ). However, since these measures are only
applicable to a hard clustering, it is necessary to extend them to be
applicable to a soft clustering, because in our task a verb can belong to multiple
clusters or classes.
         We propose a normalized version of
modified purity and inverse purity. This kind of normalization for soft
clusterings was performed for other evaluation metrics as in
         Springorum et al. (2013)
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p2">
        <p class="ltx_p">
         To measure the precision of a clustering, a normalized version of
modified purity is defined as follows. Suppose
         K
         is the set of automatically
induced clusters and
         G
         is the set of gold classes. Let
         Ki
         be the verb vector of
the
         i
         -th cluster and
         Gj
         be the verb vector of the
         j
         -th gold class. Each component of these vectors is
a normalized frequency, which equals a cluster/class attribute
probability given a verb. Where there is no frequency
information available for class distribution, such as the gold-standard data
described in Section
         4.3
         , we use a
uniform distribution across the verb’s classes.
The core idea of purity is that each cluster
         Ki
         is associated with
its most prevalent gold class. In addition, to penalize clusters that
consist of only one verb, such singleton clusters in
         K
         are considered
as errors, as is usual with modified purity. The normalized
modified purity (nmPU) can then be written as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p3">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="Sx1.EGx1">
         <tr class="ltx_equation ltx_align_baseline" id="S4.E4">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           nmPU=1N⁢∑i⁢s.t.⁢|Ki|&gt;1maxj⁡δKi⁢(Ki∩Gj),
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (4)
           </span>
          </td>
         </tr>
         <tr class="ltx_equation ltx_align_baseline" id="S4.E5">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           δKi⁢(Ki∩Gj)=∑v∈Ki∩Gjci⁢v,
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (5)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S4.SS2.p4">
        <p class="ltx_p">
         where
         N
         denotes the total number of verbs,
         |Ki|
         denotes the number of positive components in
         Ki
         , and
         ci⁢v
         denotes the
         v
         -th component of
         Ki
         .
         δKi⁢(Ki∩Gj)
         means the total mass of the set of verbs in
         Ki∩Gj
         , given by
summing up the values in
         Ki
         . In case of evaluating a hard clustering,
this is equal to
         |Ki∩Gj|
         because all the values of
         ci⁢v
         are
equal to 1.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p5">
        <p class="ltx_p">
         As usual, the following normalized inverse purity (niPU) is used to
measure the recall of a clustering:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p6">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="Sx1.EGx2">
         <tr class="ltx_equation ltx_align_baseline" id="S4.E6">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           niPU=1N⁢∑jmaxi⁡δGj⁢(Ki∩Gj).
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (6)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S4.SS2.p7">
        <p class="ltx_p">
         Finally, we use the harmonic mean (F
         1
         ) of nmPU and niPU as a single
measure of clustering quality.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.3
        </span>
        Type-level Multi-class Evaluations
       </h3>
       <div class="ltx_para" id="S4.SS3.p1">
        <p class="ltx_p">
         We first evaluate our induced verb classes on the test set created by
         Korhonen et al. (2003)
         (Table 1 of their paper) which was created by
considering verb polysemy on the basis of Levin’s classes and
the LCS database
         [6]
         . It consists of 62 classes and 110
verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous.
The average number of verb classes per verb is 2.24. An excerpt from
this data is shown in Table
         1
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p2">
        <p class="ltx_p">
         As our baselines, we adopt two previously proposed methods. We first
implemented a soft clustering method for verb class induction proposed
by
         Korhonen et al. (2003)
         . They used the information bottleneck (IB)
method for assigning probabilities of classes to each
verb. Note that
         Korhonen et al. (2003)
         actually hardened the clusterings
and left the evaluations of soft clusterings for their future work. For
input data, we employ VALEX
         [13]
         , which is a
publicly-available large-scale subcategorization
lexicon.
         By following
the method of
         Korhonen et al. (2003)
         , prepositional phrases (pp) are
parameterized for two frequent subcategorization frames (NP and NP_PP),
and the unfiltered raw frequencies of subcategorization frames are used
as features to represent a verb. It is necessary to specify the number
of clusters,
         k
         , for the IB method beforehand, and we adopt 35 and 42 clusters
according to their reported high accuracies. To output multiple classes
for each verb, we set a threshold,
         t
         , for class attribute probabilities.
That is, classes that have a higher class attribute probability than the
threshold are output for each verb. We report the results of the
following threshold values: 0.01, 0.02, 0.05 and 0.10.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p3">
        <p class="ltx_p">
         The other baseline is LDA-frames
         [21]
         . We use the
induced LDA-frames that are available on the web
site.
         This frame
data was induced from the BNC and consists of 1,200 frames and 400
semantic roles. Again, we set a threshold for frame attribute
probabilities.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p4">
        <p class="ltx_p">
         We report results using our methods with four feature combinations
(slot-only (S) and slot-word pair (SW) features each used for both
the frame-generation and verb-class clustering steps) for both the
Gigaword and web corpora. Table
         2
         lists evaluation results for the baseline methods and our methods.
         The
results of the IB baseline and our methods are obtained by averaging
five runs.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p5">
        <p class="ltx_p">
         We can see that “web/SW-S” achieved the best performance and obtained
a higher F
         1
         than the baselines by more than nine points. “Web/SW-S” uses the
combination of slot-word pair features for clustering verb-specific
frames and slot-only features for clustering across
verbs. Interestingly, this result indicates that slot distributions are
more effective than lexical information in slot-word pairs for inducing
verb classes similar to the gold standard. This result is consistent with
expectations, given a gold standard based on Levin’s verb classes,
which are organized according to the syntactic behavior of verbs.
The use of slot-word pairs for verb class induction generally merged too
many frames into each class, apparently due to accidental word overlaps across
verbs.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p6">
        <p class="ltx_p">
         The verb classes induced from the web corpus achieved a higher F
         1
         than those from the Gigaword corpus. This can be attributed to the
larger size of the web corpus. The employment of this kind of huge
corpus is enabled by our scalable method.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS4">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.4
        </span>
        Type-level Single-class Evaluations against Predominant/Multiple Classes
       </h3>
       <div class="ltx_para" id="S4.SS4.p1">
        <p class="ltx_p">
         Since we focus on the handling of verb polysemy, predominant class
induction for each verb is not our main objective. However, we wish to compare
our method with previous work on the induction of a predominant
(monosemous) class for each verb.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p2">
        <p class="ltx_p">
         To output a single class for each verb by using our proposed method, we
skip the induction of verb-specific semantic frames and instead create a
single frame for each verb by merging all predicate-argument structures
of the verb. Then, we apply clustering to these frames across verbs. For
clustering features, we again compare two representations:
slot-only features (S) and slot-word pair features (SW).
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p3">
        <p class="ltx_p">
         We evaluate the single-class output for
each verb based on the predominant gold-standard classes, which are
defined for each verb in the test set of
         Korhonen et al. (2003)
         . This
data contains 110 verbs and 33 classes. We evaluate these single-class
outputs in the same manner as
         Korhonen et al. (2003)
         , using the gold standard with multiple classes, which
we also use for our multi-class evaluations.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p4">
        <p class="ltx_p">
         As we did with the multi-class evaluations, we adopt modified purity
(mPU), inverse purity (iPU) and their harmonic mean (F
         1
         ) as the
metrics for the evaluation with predominant classes. It is not
necessary to normalize these metrics when we treat verbs as monosemous,
and evaluate against the predominant sense. When we evaluate
against the multiple classes in the gold standard, we do normalize
the inverse purity.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p5">
        <p class="ltx_p">
         For baselines, we once more adopt the Nearest Neighbor (NN) and Information
Bottleneck (IB) methods proposed by
         Korhonen et al. (2003)
         , and
LDA-frames proposed by
         Materna (2012)
         . The clusterings with the
NN and IB methods are obtained by using the VALEX subcategorization
lexicon. To harden the clusterings of the IB method and the LDA-frames,
the class with the highest probability is selected for each verb. This
hardening process is exactly the same as
         Korhonen et al. (2003)
         . Note
that our results of the NN and IB methods are different
from those reported in their paper since the data source is
different.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p6">
        <p class="ltx_p">
         Table
         3
         lists accuracies of baseline
methods and our methods. Our proposed method using the web corpus
achieved comparable performance with the baseline methods on the
predominant class evaluation and outperformed them on the multiple class
evaluation. More sophisticated methods for predominant class induction,
such as the method of
         Sun and Korhonen (2009)
         using selectional preferences,
could produce better single-class outputs, but have difficulty in
producing polysemy-aware verb classes.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p7">
        <p class="ltx_p">
         From the result, we can see that the induced verb classes based on
slot-only features did not achieve a higher F
         1
         than those
based on slot-word pair features in many cases. This result is different from that of
multi-class evaluations in Section
         4.3
         . We speculate that slot
distributions are not so different among verbs when all uses of a
verb are merged into one frame, and thus their discrimination power is lower than
that in the intermediate construction of semantic frames.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS5">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.5
        </span>
        Token-level Multi-class Evaluations
       </h3>
       <div class="ltx_para" id="S4.SS5.p1">
        <p class="ltx_p">
         We conduct token-level multi-class evaluations using 119 verbs, which
appear 100 or more times in sections 02-21 of the SemLink WSJ
corpus. These 119 verbs cover 102 VerbNet classes, and 48 of them are
polysemous in the sense of being in more than one VerbNet class. Each
instance of these 119 verbs in this corpus belongs to one of 102 VerbNet
classes. We first add these instances to the instances from a raw corpus
and apply the two-step clustering to these merged instances. Then, we
compare the induced verb classes of the SemLink instances with their
gold-standard VerbNet classes.
We report the values of modified purity (mPU), inverse purity (iPU) and
their harmonic mean (F
         1
         ). It is not necessary to normalize these
metrics because the clustering of these instances is hard.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS5.p2">
        <p class="ltx_p">
         For clustering features, we compare two feature combinations: “S-S”
and “SW-S,” which achieved high performance in the type-level
multi-class evaluations (Section
         4.3
         ).
The results of these methods are obtained by averaging five runs. For a
baseline, we use verb-specific semantic frames without clustering across
verbs (“S-NIL” and “SW-NIL”), where these frames are considered to
be verb classes but not shared across verbs. Table
         4
         lists accuracies of these methods for the two
corpora. We can see that “SW-S” achieved a higher F
         1
         than “S-S”
and the baselines without verb class induction (“S-NIL” and “SW-NIL”).
        </p>
       </div>
       <div class="ltx_para" id="S4.SS5.p3">
        <p class="ltx_p">
         Modi et al. (2012)
         induced semantic frames across verbs using the
monosemous assumption and reported an F
         1
         of 44.7% (77.9% PU and
31.4% iPU) for the assignment of FrameNet frames to
the FrameNet corpus. We also conducted the above evaluation against
FrameNet frames for 75 verbs.
         We achieved an F
         1
         of 62.79%
(66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F
         1
         of
60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is
difficult to directly compare these results with
         Modi et al. (2012)
         , but
our induced verb classes seem to have higher F
         1
         accuracy.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS6">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.6
        </span>
        Full Experiments and Discussions
       </h3>
       <div class="ltx_para" id="S4.SS6.p1">
        <p class="ltx_p">
         We finally induce verb classes from the semantic frames of 1,667 verbs,
which appear at least once in sections 02-21 of the WSJ corpus.
Based on the best results in the above evaluations, we induced semantic frames
using slot-word pair features, and then induced verb classes
using slot-only features. We ended with 38,481 semantic frames and 699
verb classes from the Gigaword corpus, and 61,903 semantic
frames and 840 verb classes from the web corpus. It took
two days to induce verb classes from the Gigaword corpus and three days
from the web corpus.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS6.p2">
        <p class="ltx_p">
         Examples of verb classes and semantic frames induced from the web corpus
are shown in Table
         5
         and Table
         6
         . While there are many classes with
consistent meanings, such as “Class 4” and “Class 16,” some
classes have mixed meanings. For instance, “Class 2” consists of the
semantic frames “need:2” and “say:2.” These frames were merged due
to the high syntactic similarity of constituting slot distributions, which are
comprised of a subject and a sentential complement. To improve the
quality of verb classes, it is necessary to develop a clustering model
that can consider syntactic and lexical similarity in a balanced way.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Conclusion
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        We presented a step-wise unsupervised method for inducing verb classes
from instances in giga-word corpora. This method first clusters
predicate-argument structures to induce verb-specific semantic frames
and then clusters these semantic frames across verbs to induce verb
classes. Both clustering steps are performed with exactly the same
method, which is based on the Chinese Restaurant Process. The resulting semantic
frames and verb classes are open to the public and also can be searched
via our web interface.
       </p>
      </div>
      <div class="ltx_para" id="S5.p2">
       <p class="ltx_p">
        From the results, we can see that the combination of the slot-word pair
features for clustering verb-specific frames and the slot-only features
for clustering across verbs is the most effective and outperforms the
baselines by approximately 10 points. This indicates that slot
distributions are more effective than lexical information in slot-word
pairs for the induction of verb classes, when Levin-style classes are
used for evaluation. This is consistent with Levin’s principle of
organizing verb classes according to the syntactic behavior of verbs.
       </p>
      </div>
      <div class="ltx_para" id="S5.p3">
       <p class="ltx_p">
        As applications of the resulting semantic frames and verb classes, we
plan to integrate them into syntactic parsing, semantic role labeling
and verb sense disambiguation. For instance,
        Kawahara and Kurohashi (2006)
        improved accuracy of dependency parsing based on Japanese semantic
frames automatically induced from a raw corpus. It is also valuable and
promising to apply the induced verb classes to NLP applications as used
in metaphor identification
        [34]
        and argumentative zoning
        [8]
        .
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
