<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Two Knives Cut Better Than One: Chinese Word Segmentation with Dual Decomposition.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by
        [15, 2, 3, 10]
        , the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing.
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-of-vocabulary (
        oov
        ) words, lead to difficulties for word- or dictionary-based approaches.
Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as æè½ (“talent”) and æ / è½ (“just able”)
        [8]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        There are two primary classes of models: character-based, where the foundational units for processing are individual Chinese characters
        [23, 19, 24, 20]
        , and word-based, where the units are full words based on some dictionary or training lexicon
        [1, 25]
        .
        Sun:2010:COLING details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new
        oov
        words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive.
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        In this work, we propose a simple and principled joint decoding method for combining character-based and word-based segmenters based on dual decomposition. This method has strong optimality guarantees and works very well empirically. It is easy to implement and does not require retraining of existing character- and word-based segmenters. Perhaps most importantly, this work presents a much more practical and usable form of classifier combination in the CWS context than existing methods offer.
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        Experimental results on standard SIGHAN 2003 and 2005 bake-off evaluations show that our model outperforms the character and word baselines by a significant margin.
In particular, out approach improves
        oov
        recall rates and segmentation consistency, and gives the best reported results to date on 6 out of 7 datasets.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Models for CWS
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        In this section, we describe the character-based and word-based models we use as baselines, review existing approaches to combination, and describe our algorithm for joint decoding with dual decomposition.
       </p>
      </div>
      <div class="ltx_subsection" id="S2.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.1
        </span>
        Character-based Models
       </h3>
       <div class="ltx_para" id="S2.SS1.p1">
        <p class="ltx_p">
         In the most commonly used contemporary approach to character-based segmentation, first proposed by
         [23]
         , CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF)
         [11]
         have been widely adopted for this task, and give state-of-the-art results
         [19]
         . In a first-order linear-chain CRF model, the conditional probability of a label sequence
         𝐲
         given a word sequence
         𝐱
         is defined as:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx1">
         <tr class="ltx_equation ltx_align_baseline" id="S2.Ex1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           P(𝐲|𝐱)=1Z∑t=1|𝐲|exp(θ⋅f(x,yt,yt+1))
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         f⁢(x,yt,yt-1)
         are feature functions that typically include surrounding character n-gram and morphological suffix/prefix features. These types of features capture the compositional properties of characters and are likely to generalize well to unknown words.
However, the Markov assumption in CRF limits the context of such features; it is difficult to capture long-range word features in this model.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.2
        </span>
        Word-based Models
       </h3>
       <div class="ltx_para" id="S2.SS2.p1">
        <p class="ltx_p">
         Word-based models search through lists of word candidates using scoring functions that directly assign scores to each.
Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching
         [4]
         .
More recently,
         Zhang:2007:ACL reported success using a linear model trained with the average perceptron algorithm
         [5]
         .
Formally, given input
         𝐱
         , their model seeks a segmentation
         F⁢(𝐱)
         such that:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx2">
         <tr class="ltx_equation ltx_align_baseline" id="S2.Ex2">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           F(𝐲|𝐱)=argmax𝐲∈GEN⁢(𝐱)(α⋅ϕ(𝐲))
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         Searching through the entire
         GEN⁢(𝐱)
         space is intractable even with a local model, so a beam-search algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new alternative hypotheses by either appending the new character to the last word in the beam, or starting a new word at the current position.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS2.p2">
        <p class="ltx_p">
         [!ht]
         Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms.

∀i∈{1⁢to⁢|𝐱|}:∀k∈{0,1}:ui⁢(k)=0
t←1 to T
𝐲𝐜⁣*=argmax𝐲P(𝐲𝑐|𝐱)+∑i∈|𝐱|ui(yi𝑐)
𝐲𝐰⁣*=argmax𝐲∈GEN⁢(𝐱)F(𝐲𝑤|𝐱)-∑j∈|𝐱|uj(yj𝑤)
 𝐲𝐜⁣*=𝐲𝐰⁣*
(𝐲𝑐⁣*,𝐲𝑤⁣*)
\FORALLi∈{1⁢to⁢|𝐱|}
∀k∈{0,1}:ui⁢(k)=ui⁢(k)+αt⁢(2⁢k-1)⁢(yiw⁣*-yic⁣*)
\ENDFOR(𝐲𝐜⁣*,𝐲𝐰⁣*)
\STATEViterbi:
V1⁢(1)=1,V1⁢(0)=0
i=2⁢to⁢|𝐱|
∀k∈{0,1}:Vi(k)=argmax𝐤′Pi(k|k′)Vi-1k′+ui(k)

\STATEBeam-Search:
i=1⁢to⁢|𝐱|
 item v={w0,⋯,wj} in beam⁢(i)
append xi to wj, score⁢(v)=+ui⁢(0)
v={w0,⋯,wj,xi}, score⁢(v)=+ui⁢(1)
\ENDFOR
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.3
        </span>
        Combining Models with Dual Decomposition
       </h3>
       <div class="ltx_para" id="S2.SS3.p1">
        <p class="ltx_p">
         Various mixing approaches have been proposed to combine the above two approaches
         [22, 12, 18, 17, 20]
         .
These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS3.p2">
        <p class="ltx_p">
         Dual decomposition (DD)
         [14]
         offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to
         [18]
         ) or decoding efficiency (in contrast to bagging in
         [22, 17]
         ). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing
         [9]
         , bilingual sequence tagging
         [21]
         and word alignment
         [6]
         .
        </p>
       </div>
       <div class="ltx_para" id="S2.SS3.p3">
        <p class="ltx_p">
         The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely to agree on.
Formally, the objective of DD is:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx3">
         <tr class="ltx_equation ltx_align_baseline" id="S2.E1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           max𝐲𝑐,𝐲𝑤P(𝐲𝑐|𝐱)+F(𝐲𝑤|𝐱)s.t.𝐲𝑐=𝐲𝑤
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (1)
           </span>
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         𝐲𝑐
         is the output of character-based CRF,
         𝐲𝑤
         is the output of word-based perceptron,
and the agreements are expressed as constraints.
         s.t.
         is a shorthand for “such that”.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS3.p4">
        <p class="ltx_p">
         Solving this constrained optimization problem directly is difficult.
Instead, we take the Lagrangian relaxation of this term as:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx4">
         <tr class="ltx_equation ltx_align_baseline" id="S2.E2">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
          </td>
          <td class="ltx_td ltx_align_left">
           L⁢(𝐲𝑐,𝐲𝑤,𝐔)=
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (2)
           </span>
          </td>
         </tr>
         <tr class="ltx_equation ltx_align_baseline" id="S2.Ex3">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
          </td>
          <td class="ltx_td ltx_align_left">
           P(𝐲𝑐|𝐱)+F(𝐲𝑤|𝐱)+∑i∈|𝐱|ui(yi𝑐-yi𝑤)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         𝐔
         is the set of Lagrangian multipliers that consists of a multiplier
         ui
         at each word position
         i
         .
        </p>
       </div>
       <div class="ltx_para" id="S2.SS3.p5">
        <p class="ltx_p">
         We can rewrite the original objective with the Lagrangian relaxation as:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx5">
         <tr class="ltx_equation ltx_align_baseline" id="S2.E3">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           max𝐲𝑐,𝐲𝑤⁡min𝐔⁡L⁢(𝐲𝑐,𝐲𝑤,𝐔)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (3)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S2.SS3.p6">
        <p class="ltx_p">
         We can then form the dual of this problem by taking the
         min
         outside of the
         max
         , which is an upper bound on the original problem.
The dual form can then be decomposed into two sub-components (the two
         max
         problems in Eq.
         4
         ), each of which is local with respect to the set of Lagrangian multipliers:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="S6.EGx6">
         <tr class="ltx_equation ltx_align_baseline" id="S2.E4">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           min𝐔(
          </td>
          <td class="ltx_td ltx_align_left">
           maxy𝑐[P(𝐲𝑐|𝐱)+∑i∈|𝐱|ui(yi𝑐)]
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (4)
           </span>
          </td>
         </tr>
         <tr class="ltx_equation ltx_align_baseline" id="S2.Ex4">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           +
          </td>
          <td class="ltx_td ltx_align_left">
           maxy𝑤[F(𝐲𝑤|𝐱)-∑j∈|𝐱|uj(yj𝑤)])
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         This method is called dual decomposition (DD)
         [14]
         .
Similar to previous work
         [13]
         , we solve this DD problem by iteratively updating the sub-gradient as depicted in Algorithm
         2.2
         .
         In each iteration, if the best segmentations provided by the two models do not agree, then the two models will receive penalties for the decisions they made that differ from the other. This penalty exchange is similar to message passing, and as the penalty accumulates over iterations, the two models are pushed towards agreeing with each other.
We also give an updated Viterbi decoding algorithm for CRF and a modified beam-search algorithm for perceptron in Algorithm
         2.2
         .
         T
         is the maximum number of iterations before early stopping, and
         αt
         is the learning rate at time
         t
         . We adopt a learning rate update rule from
         Koo:2010:EMNLP where
         αt
         is defined as
         1N
         , where
         N
         is the number of times we observed a consecutive dual value increase from iteration
         1
         to
         t
         .
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Experiments
      </h2>
      <div class="ltx_para" id="S3.p1">
       <p class="ltx_p">
        We conduct experiments on the SIGHAN 2003
        [16]
        and 2005
        [7]
        bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter
        [19]
        as our character-based baseline model, and reproduce the perceptron-based segmenter from
        Zhang:2007:ACL as our word-based baseline model.
       </p>
      </div>
      <div class="ltx_para" id="S3.p2">
       <p class="ltx_p">
        We adopted the development setting from
        [25]
        , and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. The optimized hyper-parameters used are:
        ℓ2
        regularization parameter
        λ
        in CRF is set to
        3
        ; the perceptron is trained for 10 iterations with beam size 200; dual decomposition is run to max iteration of 100 (
        T
        in Algo.
        2.2
        ) with step size 0.1 (
        αt
        in Algo.
        2.2
        ).
       </p>
      </div>
      <div class="ltx_para" id="S3.p3">
       <p class="ltx_p">
        Beyond standard precision (P), recall (R) and F
        1
        scores, we also evaluate segmentation consistency as proposed by
        [3]
        , who have shown that increased segmentation consistency is correlated with better machine translation performance. The consistency measure calculates the entropy of segmentation variations — the lower the score the better. We also report out-of-vocabulary recall (R
        oov
        ) as an estimation of the model’s generalizability to previously unseen words.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Results
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        Table
        1
        shows our empirical results on SIGHAN 2005 dataset. Our dual decomposition method outperforms both the word-based and character-based baselines consistently across all four subsets in both F
        1
        and
        oov
        recall (R
        oov
        ). Our method demonstrates a robustness across domains and segmentation standards regardless of which baseline model was stronger. Of particular note is DD’s is much more robust in R
        oov
        , where the two baselines swing a lot. This is an important property for downstream applications such as entity recognition. The DD algorithm is also more consistent, which would likely lead to improvements in applications such as machine translation
        [3]
        .
       </p>
      </div>
      <div class="ltx_para" id="S4.p2">
       <p class="ltx_p">
        The improvement over our word- and character-based baselines is also seen in our results on the earlier SIGHAN 2003 dataset.
Table
        2
        puts our method in the context of earlier systems for CWS. Our method achieves the best reported score on 6 out of 7 datasets.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Discussion and Error Analysis
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        On the whole, dual decomposition produces state-of-the-art segmentations that are more accurate, more consistent, and more successful at inducing
        oov
        words than the baseline systems that it combines.
On the SIGHAN 2005 test set, in over 99.1% of cases the DD algorithm converged within 100 iterations, which gives an optimality guarantee.
In 77.4% of the cases, DD converged in the first iteration. The number of iterations to convergence histogram is plotted in Figure
        1
        .
       </p>
      </div>
      <div class="ltx_paragraph" id="S5.SS3.SSS0.P1">
       <h4 class="ltx_title ltx_title_paragraph">
        Error analysis
       </h4>
       <div class="ltx_para" id="S5.SS3.SSS0.P1.p1">
        <p class="ltx_p">
         In many cases the relative confidence of each model means that dual decomposition is capable of using information from both sources to generate a series of correct segmentations better than either baseline model alone. The example below shows a difficult-to-segment proper name comprised of common characters, which results in undersegmentation by the character-based CRF and oversegmentation by the word-based perceptron, but our method achieves the correct middle ground.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS3.SSS0.P1.p2">
        <table class="ltx_tabular">
         <tr class="ltx_tr">
          <td class="ltx_td">
          </td>
          <td class="ltx_td ltx_align_left">
           Gloss
          </td>
          <td class="ltx_td ltx_align_left">
           Tian Yage / ’s / creations
          </td>
         </tr>
         <tr class="ltx_tr">
          <td class="ltx_td ltx_align_right">
           .
          </td>
          <td class="ltx_td ltx_align_left">
           Gold
           .
          </td>
          <td class="ltx_td ltx_align_left">
           ç°éå / ç / åä½
          </td>
         </tr>
         <tr class="ltx_tr">
          <td class="ltx_td ltx_align_right">
           .
          </td>
          <td class="ltx_td ltx_align_left">
           CRF
           .
          </td>
          <td class="ltx_td ltx_align_left">
           ç°éåç / åä½
          </td>
         </tr>
         <tr class="ltx_tr">
          <td class="ltx_td ltx_align_right">
           .
          </td>
          <td class="ltx_td ltx_align_left">
           PCPT
           .
          </td>
          <td class="ltx_td ltx_align_left">
           ç°é / å / ç / åä½
          </td>
         </tr>
         <tr class="ltx_tr">
          <td class="ltx_td ltx_align_right">
           .
          </td>
          <td class="ltx_td ltx_align_left">
           DD
           .
          </td>
          <td class="ltx_td ltx_align_left">
           ç°éå / ç / åä½
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S5.SS3.SSS0.P1.p3">
        <p class="ltx_p">
         A powerful feature of the dual decomposition approach is that it can generate correct segmentation decisions in cases where a voting or product-of-experts model could not, since joint decoding allows the sharing of information at decoding time. In the following example, both baseline models miss the contextually clear use of the word ç¹å¿ (“sweets / snack food”) and instead attach ç¹ to the prior word to produce the otherwise common compound ä¸ç¹ç¹ (“a little bit”); dual decomposition allows the model to generate the correct segmentation.
         Gloss
Enjoy / a bit of / snack food / , …

.
Gold .
äº«å / ä¸ç¹ / ç¹å¿ / ï¼

.
CRF .
äº«å / ä¸ç¹ç¹ / å¿ / ï¼

.
PCPT .
äº«å / ä¸ç¹ç¹ / å¿ / ï¼

.
DD .
äº«å / ä¸ç¹ / ç¹å¿ / ï¼
         We found more than 400 such surprisingly accurate instances in our dual decomposition output.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS3.SSS0.P1.p4">
        <p class="ltx_p">
         Finally, since dual decomposition is a method of joint decoding, it is still liable to reproduce errors made by the constituent systems.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S6">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        6
       </span>
       Conclusion
      </h2>
      <div class="ltx_para" id="S6.p1">
       <p class="ltx_p">
        In this paper we presented an approach to Chinese word segmentation using dual decomposition for system combination. We demonstrated that this method allows for joint decoding of existing CWS systems that is more accurate and consistent than either system alone, and further achieves the best performance reported to date on standard datasets for the task.
Perhaps most importantly, our approach is straightforward to implement and does not require retraining of the underlying segmentation models used. This suggests its potential for broader applicability in real-world settings than existing approaches to combining character-based and word-based models for Chinese word segmentation.
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
