<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Labelling Topics using Unsupervised Graph-based Methods.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        Topic models
        [11, 2]
        have proved to be a useful way to represent the content of document collections, e.g.
        [4, 7, 8, 10, 20]
        . In these interfaces, topics need to be presented to users in an easily interpretable way.
A common way to represent topics is as set of keywords generated from the
        n
        terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms:
        financial, bank, market, government, mortgage, bailout, billion, street, wall, crisis
        . But interpreting such lists is not always straightforward, particularly since background knowledge may be required
        [5]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        Textual labels could assist with the interpretations of topics and researchers have developed methods to generate these automatically
        [17, 15, 14]
        . For example, a topic which has keywords
        school, student, university, college, teacher, class, education, learn, high, program
        , could be labelled as
        Education
        and a suitable label for the topic shown above would be
        Global Financial Crisis
        . Approaches that make use of alternative modalities, such as images
        [1]
        , have also been proposed.
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        Mei et al. (2007)
        label topics using statistically significant bigrams identified in a reference collection.
        Magatti et al. (2009)
        introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while
        Lau et al. (2010)
        proposed selecting the most representative word from a topic as its label.
        Hulpus et al. (2013)
        make use of structured data from DBpedia to label topics.
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        Lau et al. (2011)
        proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set.
The labels are ranked using Support Vector Regression (SVR)
        [21]
        and features extracted using word association measures (i.e. PMI, t-test,
        χ2
        and Dice coefficient), lexical features and search engine ranking.
        Lau et al. (2011)
        report two versions of their approach, one unsupervised (which is used as a baseline) and another which is supervised. They reported that the supervised version achieves better performance than a previously reported approach
        [17]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        This paper introduces an alternative graph-based approach which is unsupervised and less computationally intensive than
        Lau et al. (2011)
        . Our method uses topic keywords to form a query. A graph is generated from the words contained in the search results and these are then ranked using the PageRank algorithm
        [19, 18]
        . Evaluation on a standard data set shows that our method consistently outperforms the best performing previously reported method, which is supervised
        [14]
        .
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Methodology
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        We use the topic keywords to query a search engine. We assume that the search results returned are relevant to the topic and can be used to identify and weigh relevant keywords. The most important keywords can be used to generate keyphrases for labelling the topic or weight pre-existing candidate labels.
       </p>
      </div>
      <div class="ltx_subsection" id="S2.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.1
        </span>
        Retrieving and Processing Text Information
       </h3>
       <div class="ltx_para" id="S2.SS1.p1">
        <p class="ltx_p">
         We use the approach described by
         Lau et al. (2011)
         to generate candidate labels from Wikipedia articles. The 10 terms with the highest marginal probabilities in the topic are used to query Wikipedia and the titles of the articles retrieved used as candidate labels. Further candidate labels are generated by processing the titles of these articles to identify noun chunks and n-grams within the noun chunks that are themselves the titles of Wikipedia articles. Outlier labels, identified using a similarity measure
         [9]
         , are removed. This method has been proved to produce labels which effectively summarise a topic’s main subject.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS1.p2">
        <p class="ltx_p">
         However, it should be noted that our method is flexible and could be applied to any set of candidate labels. We have experimented with various approaches to candidate label generation but chose to report results using the approach described by
         Lau et al. (2011)
         to allow direct comparison of approaches.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS1.p3">
        <p class="ltx_p">
         Information obtained from web searches is used to identify the best labels from the set of candidates. The top
         n
         keywords, i.e. those with highest marginal probability within the topic, are used to form a query which was submitted to the Bing
         search engine. Textual information included in the Title field
         of the search results metadata was extracted. Each title was tokenised using openNLP
         and stop words removed.
        </p>
       </div>
       <div class="ltx_para" id="S2.SS1.p4">
        <p class="ltx_p">
         Figure
         1
         shows a sample of the metadata associated with a search result for the topic:
         vmware, server, virtual, oracle, update, virtualization, application, infrastructure, management, microsoft
         .
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.2
        </span>
        Creating a Text Graph
       </h3>
       <div class="ltx_para" id="S2.SS2.p1">
        <p class="ltx_p">
         We consider any remaining words in the search result metadata as nodes,
         v∈V
         , in a graph
         G=(V,E)
         . Each node is connected to its neighbouring words in a context window of
         ±n
         words. In the previous example, the words added to the graph from the Title of the search result are
         microsoft, server, cloud, datacenter
         and
         virtualization
         .
        </p>
       </div>
       <div class="ltx_para" id="S2.SS2.p2">
        <p class="ltx_p">
         We consider both unweighted and weighted graphs. When the graph is unweighted we assume that all the edges have a weight
         e=1
         . In addition, we weight the edges of the graph by computing the relatedness between two nodes,
         vi
         and
         vj
         , as their normalised Pointwise Mutual Information (NPMI)
         [3]
         . Word co-occurrences are computed using Wikipedia as a a reference corpus. Pairs of words are connected with edges only if
         NPMI⁢(wi,wj)&gt;0.2
         avoiding connections between words co-occurring by chance and hence introducing noise.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.3
        </span>
        Identifying Important Terms
       </h3>
       <div class="ltx_para" id="S2.SS3.p1">
        <p class="ltx_p">
         Important terms are identified by applying the PageRank algorithm
         [19]
         in a similar way to the approach used by
         Mihalcea and Tarau (2004)
         for document keyphrase extraction. The PageRank score (
         P⁢r
         ) over
         G
         for a word (
         vi
         ) can be computed by the following equation:
        </p>
       </div>
       <div class="ltx_para" id="S2.SS3.p2">
        <table class="ltx_equationgroup ltx_eqn_align" id="Sx1.EGx1">
         <tr class="ltx_equation ltx_align_baseline" id="S2.Ex1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           P⁢r⁢(vi)
          </td>
          <td class="ltx_td ltx_align_left">
           =d⋅∑vj∈C⁢(vi)s⁢i⁢m⁢(vi,vj)∑vk∈C⁢(vj)s⁢i⁢m⁢(vj,vk)⁢P⁢r⁢(vj)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
         <tr class="ltx_equation ltx_align_baseline" id="S2.E1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
          </td>
          <td class="ltx_td ltx_align_left">
           +(1-d)⁢𝐯
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (1)
           </span>
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         C⁢(vi)
         denotes the set of vertices which are connected to the vertex
         vi
         .
         d
         is the damping factor which is set to the default value of
         d=0.85
         [19]
         . In standard PageRank all elements of the vector
         𝐯
         are the same,
         1N
         where
         N
         is the number of nodes in the graph.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS4">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.4
        </span>
        Ranking Labels
       </h3>
       <div class="ltx_para" id="S2.SS4.p1">
        <p class="ltx_p">
         Given a candidate label
         L={w1,…,wm}
         containing
         m
         keywords, we compute the score of
         L
         by simply adding the PageRank scores of its constituent keywords:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="Sx1.EGx2">
         <tr class="ltx_equation ltx_align_baseline" id="S2.E2">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           S⁢c⁢o⁢r⁢e⁢(L)=∑i=1mP⁢r⁢(wi)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (2)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S2.SS4.p2">
        <p class="ltx_p">
         The label with the highest score amongst the set of candidates is selected to represent the topic. We also experimented with normalised versions of the score, e.g. mean of the PageRank scores. However, this has a negative effect on performance since it favoured short labels of one or two words which were not sufficiently descriptive of the topics. In addition, we expect that candidate labels containing words that do not appear in the graph (with the exception of stop words) are unlikely to be good labels for the topic. In these cases the score of the candidate label is set to 0. We also experimented with removing this restriction but found that it lowered performance.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Experimental Evaluation
      </h2>
      <div class="ltx_subsection" id="S3.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.1
        </span>
        Data
       </h3>
       <div class="ltx_para" id="S3.SS1.p1">
        <p class="ltx_p">
         We evaluate our method on the publicly available data set published by
         Lau et al. (2011)
         . The data set consists of 228 topics generated using text documents from four domains, i.e. blog posts (
         BLOGS
         ), books (
         BOOKS
         ), news articles (
         NEWS
         ) and scientific articles from the biomedical domain (
         PUBMED
         ). Each topic is represented by its ten most probable keywords. It is also associated with candidate labels and human ratings denoting the appropriateness of a label given the topic. The full data set consists of approximately 6,000 candidate labels (27 labels per topic).
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.2
        </span>
        Evaluation Metrics
       </h3>
       <div class="ltx_para" id="S3.SS2.p1">
        <p class="ltx_p">
         Our evaluation follows the framework proposed by
         Lau et al. (2011)
         using two metrics, i.e.
         Top-1 average rating
         and
         nDCG
         , to compare various labelling methods.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p2">
        <p class="ltx_p">
         Top-1 average rating
         is the average human rating (between 0 and 3) assigned to the top-ranked label proposed by the system. This provides an indication of the overall quality of the label the system judges as the best one.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p3">
        <p class="ltx_p">
         Normalised discounted cumulative gain (
         nDCG
         )
         [13, 6]
         compares the label ranking proposed by the system to the ranking provided by human annotators. The discounted cumulative gain at position
         p
         ,
         D⁢C⁢Gp
         , is computed using the following equation:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="Sx1.EGx3">
         <tr class="ltx_equation ltx_align_baseline" id="S3.E3">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           D⁢C⁢Gp=r⁢e⁢l1+∑i=2pr⁢e⁢lil⁢o⁢g2⁢(i)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (3)
           </span>
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         r⁢e⁢li
         is the relevance of the label to the topic in position
         i
         . Then nDCG is computed as:
        </p>
        <table class="ltx_equationgroup ltx_eqn_align" id="Sx1.EGx4">
         <tr class="ltx_equation ltx_align_baseline" id="S3.E4">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           n⁢D⁢C⁢Gp=D⁢C⁢GpI⁢D⁢C⁢Gp
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (4)
           </span>
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         I⁢D⁢C⁢Gp
         is the superviseed ranking of the image labels, in our experiments this is the ranking provided by the scores in the human annotated data set.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.3
        </span>
        Model Parameters
       </h3>
       <div class="ltx_para" id="S3.SS3.p1">
        <p class="ltx_p">
         Our proposed model requires two parameters to be set: the context window size when connecting neighbouring words in the graph and the number of the search results considered when constructing the graph.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p2">
        <p class="ltx_p">
         We experimented with different sizes of context window,
         n
         , between
         ±
         1 words to the left and right and all words in the title. The best results were obtained when
         n=2
         for all of the domains. In addition, we experimented with varying the number of search results between 10 and 300. We observed no noticeable difference in the performance when the number of search results is equal or greater than 30 (see below). We choose to report results obtained using 30 search results for each topic. Including more results did not improve performance but required additional processing.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Results and Discussion
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        Results are shown in Table
        1
        . Performance when PageRank is applied to the unweighted (
        PR
        ) and NPMI-weighted graphs (
        PR-NPMI
        ) (see Section
        2.2
        ) is shown. Performance of the best unsupervised (
        Lau et al. (2011)
        -U
        ) and supervised (
        Lau et al. (2011)
        -S
        ) methods reported by
        Lau et al. (2011)
        are shown.
        Lau et al. (2011)
        -U uses the average
        χ2
        scores between the topic keywords and the label keywords while
        Lau et al. (2011)
        -S uses SVR to combine evidence from all features. In addition, upper bound figures, the maximum possible value given the scores assigned by the annotators, are also shown.
       </p>
      </div>
      <div class="ltx_para" id="S4.p2">
       <p class="ltx_p">
        The results obtained by applying PageRank over the unweighted graph (2.05, 1.98, 2.04 and 1.88) are consistently better than the supervised and unsupervised methods reported by
        Lau et al. (2011)
        for the Top-1 Average scores and this improvement is observed in all domains. The difference is significant (t-test,
        p&lt;0.05
        ) for the unsupervised method. A slight improvement in performance is observed when the weighted graph is used (2.08, 2.01, 2.05 and 1.90). This is expected since the weighted graph contains additional information about word relatedness. For example, the word
        hardware
        is more related and, therefore, closer in the graph to the word
        virtualization
        than to the word
        investments
        .
       </p>
      </div>
      <div class="ltx_para" id="S4.p3">
       <p class="ltx_p">
        Results from the nDCG metric imply that our methods provide better rankings of the candidate labels in the majority of the cases. It is outperformed by the best supervised approach in two domains, NEWS and PUBMED, using the nDCG-3 and nDCG-5 metrics. However, the best label proposed by our methods is judged to be better (as shown by the nDCG-1 and Top-1 Av. Rating scores), demonstrating that it is only the lower ranked labels in our approach that are not as good as the supervised approach.
       </p>
      </div>
      <div class="ltx_para" id="S4.p4">
       <p class="ltx_p">
        An interesting finding is that, although limited in length, the textual information in the search result’s metadata contain enough salient terms relevant to the topic to provide reliable estimates of term importance. Consequently, it is not necessary to measure semantic similarity between topic keywords and candidate labels as previous approaches have done.
In addition, performance improvement gained from using the weighted graph is modest, suggesting that the computation of association scores over a large reference corpus could be omitted if resources are limited.
       </p>
      </div>
      <div class="ltx_para" id="S4.p5">
       <p class="ltx_p">
        In Figure
        6
        , we show the scores of Top-1 average rating obtained in the different domains by experimenting with the number of search results used to generate the text graph. The most interesting finding is that performance is stable when 30 or more search results are considered. In addition, we observe that quality of the topic labels in the four domains remains stable, and higher than the supervised method, when the number of search results used is between 150 and 200. The only domain in which performance of the supervised method is sometimes better than the approach proposed here is NEWS. The main reason is that news topics are more fine grained and the candidate labels of better quality
        [14]
        which has direct impact in good performance of ranking methods.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Conclusion
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        We described an unsupervised graph-based method to associate textual labels with automatically generated topics. Our approach uses results retrieved from a search engine using the topic keywords as a query. A graph is generated from the words contained in the search results metadata and candidate labels ranked using the PageRank algorithm. Evaluation on a standard data set shows that our method consistently outperforms the supervised state-of-the-art method for the task.
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
