<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Mutual Disambiguation for Entity Linking.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        The Entity Linking (EL) task consists in linking name
        mentions
        of named entities (NEs) found in a document to their corresponding entities in a reference Knowledge Base (KB).
These NEs can be of type person (PER), organization (ORG), etc., and they are usually represented in the KB by a Uniform Resource Identifier (URI).
Dealing with ambiguity is one of the key difficulties in this task, since mentions are often highly polysemous, and potentially related to many different KB entries.
Various approaches have been proposed to solve the named entity disambiguation (NED) problem.
Most of them involve the use of surface forms extracted from Wikipedia.
Surface forms consist of a word or a group of words that match lexical units like
        Paris
        or
        New York City
        .
They are used as matching sequences to locate corresponding candidate entries in the KB, and then to disambiguate those candidates using similarity measures.
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        The NED problem is related to the
        Word Sense Disambiguation (WSD)
        problem
        [16]
        , and is often more challenging since mentions of NEs can be highly ambiguous.
For instance, names of places can be very common as is Paris, which refers to 26 different places in Wikipedia.
Hence, systems that attempt to address the NED problem must include disambiguation resources.
In the context of the Named Entity Recognition (NER) task, such resources can be generic and generative.
This generative approach does not apply to the EL task where each entity to be linked to a semantic description has a specific word context, marker of its exact identity.
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        One of the classical approach to conduct the disambiguation process in NED applications is to consider the context of the mention to be mapped, and compare this context with contextual information about the potential target entities (see for instance the KIM system
        [18]
        ).
This is usually done using similarity measures (such as cosine similarity, weighted Jaccard distance, KL divergence‚Ä¶)
that evaluate the distance between a bag of words related to a candidate annotation, and the words surrounding the entity to annotate in the text.
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        In more recent approaches, it is suggested that annotation processes based on similarity distance measures can be improved by making use of other annotations present in the same document.
Such techniques are referred to as
        semantic relatedness
        [19]
        ,
        collective disambiguation
        [12]
        , or
        joint disambiguation
        [8]
        .
The idea is to evaluate in a set of candidate links which one is the most likely to be correct by taking the other links contained in the document into account.
For example, if a NE describes a city name like
        Paris
        ,
it is more probable that the correct link for this city name designates
        Paris (France)
        rather than
        Paris (Texas)
        if a neighbor entity offers candidate links semantically related to
        Paris (France)
        like the
        Seine river
        or
the
        Champs-Elys√©es
        .
Such techniques mostly involve exploration of graphs resulting of all the candidate annotations proposed for a given document, and try to rank the best candidates for each annotation using an ontology.
The ontology (like YAGO or DBPedia) provides a pre-existing set of potential relations between the entities to link (like for instance, in our previous example,
        Paris (France)
        has_river
        Seine
        ) that will be used to rank the best candidates according to their mutual presence in the document.
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        In this paper we explore the capabilities of a disambiguation algorithm using all the available annotation layers of NEs to improve their links.
The paper makes the following novel propositions:
1) the ontology used to evaluate the relatedness of candidates is replaced by internal links and categories from the Wikipedia corpus;
2) the coherence of entities is improved prior to the calculation of semantic relatedness using a co-reference resolution algorithm, and a NE label correction method;
3) the proposed method is robust enough to improve the performance of existing entity linking annotation engines, which are capable of providing a set of ranked candidates for each annotation in a document.
       </p>
      </div>
      <div class="ltx_para" id="S1.p6">
       <p class="ltx_p">
        This paper is organized as follows.
Section
        2
        describes related works.
The proposed method is presented in Section
        3
        where we explain how our SemLinker system prepares documents that contain mentions to disambiguate,
then we detail the disambiguation algorithm.
The evaluation of the complete system is provided in Section
        4
        .
Finally, we discuss the obtained results, and conclude.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Related Work
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        Entity annotation and linking in natural language text has been extensively studied in NLP research.
A strong effort has been conducted recently by the TAC-KBP evaluation task
        [13]
        to create standardized corpus, and annotation standards based on Wikipedia for evaluation and comparison of EL systems.
In this paper, we consider the TAC-KBP framework.
We describe below some recent approaches proposed for solving the EL task.
       </p>
      </div>
      <div class="ltx_subsection" id="S2.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.1
        </span>
        Wikipedia-based Disambiguation Methods
       </h3>
       <div class="ltx_para" id="S2.SS1.p1">
        <p class="ltx_p">
         The use of Wikipedia for explicit disambiguation dates back to
         [2]
         who built a system that compared the context of a mention to the Wikipedia categories of an entity candidate.
Lately,
         [6, 15, 17]
         extended this framework by using richer features for similarity comparison.
Some authors like Milne and Witten (2008) utilized machine learning methods rather than a similarity function to map mentions to entities.
They also introduced the notion of semantic relatedness.
Alternative propositions were suggested in other works like
         [10]
         that considered the relatedness of common noun phrases in a mention context with Wikipedia article names.
While all these approaches focus on semantic relation between entities, their potential is limited by the separate mapping of candidate links for each mention.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S2.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         2.2
        </span>
        Semantic Web Compliant Methods
       </h3>
       <div class="ltx_para" id="S2.SS2.p1">
        <p class="ltx_p">
         More recently, several systems have been launched as web services dedicated to EL tasks.
Most of them are compliant with new emergent semantic web standards like LinkedData network.
DBPedia Spotlight
         [14]
         is a system that finds mentions of DBpedia
         [1]
         resources in a textual document.
Wikimeta
         [3]
         is another system relying on DBpedia.
It uses bags of words to disambiguate semantic entities according to a cosine similarity algorithm.
Those systems have been compared with commercial ones like AlchemyAPI, Zemanta, or Open Calais in
         [9]
         .
The study showed that they perform differently on various essential aspects of EL tasks (mention detection, linking, disambiguation).
This suggests a wide range of potential improvements on many aspects of the EL task.
Only some of these systems introduce the semantic relatedness in their methods like the AIDA
         [12]
         system.
It proposes a disambiguation method that combines popularity-based priors, similarity measures, and coherence. It relies on the Wikipedia-derived YAGO2
         [11]
         knowledge base.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Proposed Algorithm
      </h2>
      <div class="ltx_para" id="S3.p1">
       <p class="ltx_p">
        We propose a mutual disambiguation algorithm that improves the accuracy of entity links in a document by using successive corrections applied to an
        annotation object
        representing this document.
The annotation object is composed of information extracted from the document along with linguistic and semantic annotations as described hereafter.
       </p>
      </div>
      <div class="ltx_subsection" id="S3.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.1
        </span>
        Annotation Object
       </h3>
       <div class="ltx_para" id="S3.SS1.p1">
        <p class="ltx_p">
         Documents are processed by an annotator capable of producing POS tags for each word,
as well as spans, NE surface forms, NE labels and ranked candidate Wikipedia URIs for each candidate NE.
For each document
         ùíü
         , this knowledge is gathered in an array called
         annotation object
         , which has initially one row per document lexical unit.
Since the system focuses on NEs, rows with lexical units that do not belong to a NE SF are dropped from the annotation object, and NE SF are refined as described in
         [5]
         .
When NE SF are spanned over several rows, these rows are merged into a single one.
Thus, we consider an annotation object
         ùíúùíü
         , which is an array with a row for each NE, and columns storing related knowledge.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p2">
        <p class="ltx_p">
         If
         n
         NEs were annotated in
         ùíü
         , then
         ùíúùíü
         has
         n
         rows.
If
         l
         candidate URIs are provided for each NE, then
         ùíúùíü
         has
         (l+4)
         columns
         cu,u‚àà{1,l+4}
         .
Columns
         c1
         to
         cl
         store Wikipedia URIs associated with NEs, ordered by decreasing values of likelihood.
Column
         cl+1
         stores the offset of the NEs,
         cl+2
         stores their surface forms,
         cl+3
         stores the NE labels (PER, ORG, ‚Ä¶), and
         cl+4
         stores the (vectors of) POS tags associated with the NE surface forms.
         ùíúùíü
         contains all the available knowledge about the NEs found in
         ùíü
         .
Before being processed by the disambiguation module,
         ùíúùíü
         is dynamically updated by correction processes.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.2
        </span>
        Named Entity Label Correction
       </h3>
       <div class="ltx_para" id="S3.SS2.p1">
        <p class="ltx_p">
         To support the correction process based on co-reference chains, the system tries to correct NE labels for all the NEs listed in the
         annotation object
         .
The NE label correction process assigns the same NE label to all the NEs associated with the same first rank URI.
For all the rows in
         ùíúùíü
         , sets of rows with identical first rank URIs are considered.
Then, for each set, NE labels are counted per type, and all the rows in a same set are updated with the most frequent NE label found in the set, i.e. all the NEs in this set are tagged with this label.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.3
        </span>
        Correction Based on Co-reference Chains
       </h3>
       <div class="ltx_para" id="S3.SS3.p1">
        <p class="ltx_p">
         First rank candidate URIs are corrected by a process that relies on co-reference chains found in the document.
The co-reference detection is conducted using the information recorded in the annotation object.
Among the NEs present in the document, the ones that co-refer are identified and clustered by logical rules
applied to the content of the annotation object.
When a co-reference chain of NEs is detected, the system assigns the same URI to all the members of the chain.
This URI is selected through a decision process that gives more weight to longer surface forms and frequent URIs.
The following example illustrates an application of this correction process:
         Three sentences are extracted from a document about Paris, the French capital.
NEs are indicated in brackets, first rank URIs and surface forms are added below the content of each sentence.
         - [Paris] is famous around the world.
         <span class="ltx_text ltx_font_small">
          URI
          <math alttext="{}_{1}" class="ltx_Math" display="inline" id="S3.SS3.p1.m1" xmlns="http://www.w3.org/1998/Math/MathML">
           <msub>
            <mi>
            </mi>
            <mn mathsize="normal" stretchy="false">
             1
            </mn>
           </msub>
          </math>
          :
          <span class="ltx_text ltx_font_typewriter ltx_font_script">
           http://en.wikipedia.org/wiki
           <span class="ltx_text ltx_font_small">
            /Paris_Hilton
            <br class="ltx_break"/>
           </span>
          </span>
         </span>
         NE surface form:
         Paris
         - The [city of Paris] attracts millions of tourists.
         <span class="ltx_text ltx_font_small">
          URI
          <math alttext="{}_{1}" class="ltx_Math" display="inline" id="S3.SS3.p1.m2" xmlns="http://www.w3.org/1998/Math/MathML">
           <msub>
            <mi>
            </mi>
            <mn mathsize="normal" stretchy="false">
             1
            </mn>
           </msub>
          </math>
          :
          <span class="ltx_text ltx_font_typewriter ltx_font_script">
           http://en.wikipedia.org/wiki
           <span class="ltx_text ltx_font_small">
            /Paris
            <br class="ltx_break"/>
           </span>
          </span>
         </span>
         NE surface form:
         city of Paris
         - The [capital of France] is easy to reach by train.
         <span class="ltx_text ltx_font_small">
          URI
          <math alttext="{}_{1}" class="ltx_Math" display="inline" id="S3.SS3.p1.m3" xmlns="http://www.w3.org/1998/Math/MathML">
           <msub>
            <mi>
            </mi>
            <mn mathsize="normal" stretchy="false">
             1
            </mn>
           </msub>
          </math>
          :
          <span class="ltx_text ltx_font_typewriter ltx_font_script">
           http://en.wikipedia.org/wiki
           <span class="ltx_text ltx_font_small">
            /Paris
            <br class="ltx_break"/>
           </span>
          </span>
         </span>
         NE surface form:
         capital of France
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p2">
        <p class="ltx_p">
         The three NEs found in these sentences compose a co-reference chain.
The second NE has a longer surface form than the first one, and its associated first rank URI is the most frequent.
Hence, the co-reference correction process will assign the right URI to the first NE (
         URI1: http://en.wikipedia.org/wiki/Paris
         ), which was wrongly linked to the actress Paris Hilton.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS4">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.4
        </span>
        Mutual Disambiguation Process
       </h3>
       <div class="ltx_para" id="S3.SS4.p1">
        <p class="ltx_p">
         The extraction of an accurate link is a process occurring after the URI annotation of NEs in the whole document. The system makes use of all the semantic content stored in
         ùíúùíü
         to locally improve the precision of each URI annotation in the document.
The Mutual Disambiguation Process (MDP) relies on the graph of all the relations (internal links, categories) between Wikipedia content related to the document annotations.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p2">
        <p class="ltx_p">
         A basic example of semantic relatedness that should be captured is explained hereafter.
Let us consider the mention
         IBM
         in a given document.
Candidate NE annotations for this mention can be
         International Business Machine
         or
         International Brotherhood of Magicians
         .
But if the
         IBM
         mention co-occurs with a
         Thomas Watson, Jr
         mention in the document, there will probably be more links between the
         International Business Machine
         and
         Thomas Watson, Jr
         related Wikipedia pages than between the
         International Brotherhood of Magicians
         and
         Thomas Watson, Jr
         related Wikipedia pages.
The purpose of the MDP is to capture this semantic relatedness information contained in the graph of links extracted from Wikipedia pages related to each candidate annotation.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p3">
        <p class="ltx_p">
         In MDP, for each Wikipedia URI candidate annotation, all the internal links and categories contained in the source Wikipedia document related to this URI are collected.
This information will be used to calculate a weight for each of the
         l
         candidate URI annotations of each mention.
For a given NE, this weight is expected to measure the mutual relations of a candidate annotation with all the other candidate annotations of NEs in the document.
The input of the MDP is an annotation object
         ùíúùíü
         with
         n
         rows, obtained as explained in Section
         3.1
         .
For all
         i‚àà[[1,n]]
         ,
         k‚àà[[1,l]]
         , we build the set
         Sik
         , composed of the Wikipedia URIs and categories contained in the source Wikipedia document related to the URI stored in
         ùíúùíü
         [i]‚Å¢[k]
         that we will refer to as URI
         ki
         to ease the reading.
         <span class="ltx_text ltx_font_bold">
          Scoring
         </span>
         :
         For all
         i,‚ÄÖj‚àà[[1,n]]
         ,
         k‚àà[[1,l]]
         , we want to calculate the weight of mutual relations between the candidate URI
         ki
         and all the first rank candidates URI
         1j
         for
         j‚â†i
         .
The calculation combines two scores that we called
         direct semantic relation
         score (dsr_score) and
         common semantic relation
         score (csr_score):
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p4">
        <p class="ltx_p">
         - the dsr_score for
URI
         ki
         sums up the number of occurrences of
URI
         ki
         in
         Sj1
         for all
         j‚àà[[1,n]]-{i}
         .
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p5">
        <p class="ltx_p">
         - the csr_score for
URI
         ki
         sums up the number of common URIs and categories between
         Sik
         and
         Sj1
         for all
         j‚àà[[1,n]]-{i}
         .
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p6">
        <p class="ltx_p">
         We assumed the dsr_score was much more semantically significant than the csr_score, and translated this assumption in the weight calculation by introducing two correction parameters
         Œ±
         and
         Œ≤
         used in the final scoring calculation.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS4.p7">
        <p class="ltx_p">
         Re-ranking
         :
         For all
         i‚àà[[1,n]]
         , for each set of URIs
         {URIik,‚ÄÖk‚àà[[1,l]]}
         ,
the re-ranking process is conducted according to the following steps:
         For all
         i‚ààI
         ,
        </p>
        <ol class="ltx_enumerate" id="I1">
         <li class="ltx_item" id="I1.i1" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           1.
          </span>
          <div class="ltx_para" id="I1.i1.p1">
           <p class="ltx_p">
            ‚àÄk‚àà[[1,l]]
            , calculate dsr_score(URI
            ki
            )
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I1.i2" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           2.
          </span>
          <div class="ltx_para" id="I1.i2.p1">
           <p class="ltx_p">
            ‚àÄk‚àà[[1,l]]
            , calculate csr_score(URI
            ki
            )
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I1.i3" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           3.
          </span>
          <div class="ltx_para" id="I1.i3.p1">
           <p class="ltx_p">
            ‚àÄk‚àà[[1,l]]
            , calculate
            mutual_relation_score(URI
            ki
            ) =
            <math alttext="\alpha." class="ltx_Math" display="inline" id="I1.i3.p1.m3" xmlns="http://www.w3.org/1998/Math/MathML">
             <mrow>
              <mi>
               Œ±
              </mi>
              <mo>
               .
              </mo>
             </mrow>
            </math>
            dsr_score(URI
            ki
            )+
            Œ≤.
            csr_score(URI
            ki
            )
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I1.i4" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_enumerate">
           4.
          </span>
          <div class="ltx_para" id="I1.i4.p1">
           <p class="ltx_p">
            re-order
            {URIik,‚ÄÖk‚àà[[1,l]]}
            , by
            decreasing order of mutual relation score.
           </p>
          </div>
         </li>
        </ol>
        <p class="ltx_p">
         In the following, we detail the MDP in the context of a toy example to illustrate how it works.
The document contains two sentences, NE mentions are in bold:
        </p>
        <p class="ltx_p" style="border:1px solid #000000;padding-top:12pt;padding-bottom:12pt;">
         IBM has 12 research laboratories worldwide. Thomas J. Watson, Jr. became president of the company.
        </p>
        <p class="ltx_p">
         For the first NE mention [
         IBM
         ],
         ùíúùíü
         contains two candidate URIs identifying two different resources:
        </p>
        <span class="ltx_inline-block" style="border:1px solid #000000;padding-top:12pt;padding-bottom:12pt;">
         <p class="ltx_p">
          [IBM]
URI11‚â°International Brotherhood of Magicians
         </p>
         <p class="ltx_p">
          URI21‚â°International Business Machines Corporation
         </p>
        </span>
        <p class="ltx_p">
         For the second NE mention [
         Thomas J. Watson, Jr.
         ],
         ùíúùíü
         contains the following candidate URI, which is ranked first:
        </p>
        <p class="ltx_p" style="border:1px solid #000000;padding-top:12pt;padding-bottom:12pt;">
         [Thomas J. Watson, Jr.]
URI12‚â°Thomas Watson, Jr.
        </p>
        <p class="ltx_p">
         S11
         gathers URIs and categories contained in the
         International Brotherhood of Magicians
         Wikipedia page.
         S12
         is associated to the
         International Business Machines Corporation
         ,
and
         S21
         to the
         Thomas Watson, Jr.
         page. dsr_score(URI
         11
         ) sums up the number of occurrences of URI
         11
         in
         Sj1
         for all
         j‚àà[[1,n]]-{1}
         .
Hence, in the current example, dsr_score(URI
         11
         ) is the number of occurrences of URI
         11
         in
         S21
         ,
namely the number of times the
         International Brotherhood of Magicians
         are cited in the
         Thomas Watson, Jr.
         page.
Similarly, dsr_score(URI
         21
         ) is equal to the number of times the
         International Business Machines Corporation
         is cited in the
         Thomas Watson, Jr.
         page.
         csr_score(URI
         11
         ) sums up the number of common URIs and categories between
         S11
         and
         S21
         ,
i.e. the number of URIs and categories appearing in both
         International Brotherhood of Magicians
         and
         Thomas Watson, Jr.
         pages.
csr_score(URI
         21
         ) counts the number of URIs and categories appearing in both
         International Business Machines Corporation
         and
         Thomas Watson, Jr.
         pages.
         After calculation, we have:
         <span class="ltx_text ltx_font_small">
          mutual_relation_score(URI
          <math alttext="{}_{1}^{1}" class="ltx_Math" display="inline" id="S3.SS4.p7.m24" xmlns="http://www.w3.org/1998/Math/MathML">
           <mmultiscripts>
            <msup>
             <mi>
             </mi>
             <mn mathsize="normal" stretchy="false">
              1
             </mn>
            </msup>
            <mprescripts>
            </mprescripts>
            <mn mathsize="normal" stretchy="false">
             1
            </mn>
            <none>
            </none>
           </mmultiscripts>
          </math>
          )
          <math alttext="&lt;" class="ltx_Math" display="inline" id="S3.SS4.p7.m25" xmlns="http://www.w3.org/1998/Math/MathML">
           <mo mathsize="normal" stretchy="false">
            &lt;
           </mo>
          </math>
          mutual_relation_score(URI
          <math alttext="{}_{1}^{2}" class="ltx_Math" display="inline" id="S3.SS4.p7.m26" xmlns="http://www.w3.org/1998/Math/MathML">
           <mmultiscripts>
            <msup>
             <mi>
             </mi>
             <mn mathsize="normal" stretchy="false">
              2
             </mn>
            </msup>
            <mprescripts>
            </mprescripts>
            <mn mathsize="normal" stretchy="false">
             1
            </mn>
            <none>
            </none>
           </mmultiscripts>
          </math>
          )
         </span>
         The candidate URIs for [
         IBM
         ] are re-ranked accordingly, and
         International Business Machines Corporation
         becomes its first rank candidate.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Experiments and Results
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        SemLinker has been evaluated on the TAC-KBP 2012 EL task
        [4]
        .
In this task, mentions of entities found in a document collection must be linked to entities in a reference KB, or to new named entities discovered in the collection.
The document collection built for KBP 2012 contains a combination of newswire articles (News), posts to blogs and newsgroups (Web).
Given a query that consists of a document with a specified name mention of an entity, the task is to determine the correct node in the reference KB for the entity, adding a new node for the entity if it is not already in the reference KB.
Entities can be of type person (PER), organization (ORG), or geopolitical entity (GPE).
The reference knowledge base is derived from an October 2008 dump of English Wikipedia, which includes 818,741 nodes.
Table
        2
        provides a breakdown of the queries per categories of entities, and per type of documents.
       </p>
      </div>
      <div class="ltx_para" id="S4.p2">
       <p class="ltx_p">
        A complete description of these linguistic resources can be found in
        [7]
        .
For the sake of reproducibility, we applied the KBP scoring metric (
        B3+F
        ) described in
        [20]
        ,
and we used the KBP scorer
        .
       </p>
      </div>
      <div class="ltx_para" id="S4.p3">
       <p class="ltx_p">
        The evaluated system makes use of the Wikimeta annotation engine.
The maximum number of candidate URIs is
        l=15
        .
The MDP correction parameters
        Œ±
        and
        Œ≤
        described in Section
        3.4
        have been experimentally set to
        Œ±=10
        ,
        Œ≤=2
        .
Table
        1
        presents the results obtained by the system in three configurations.
In the first column, the system is evaluated without the disambiguation module.
In the second column, we applied the MDP without correction processes.
The system with the complete disambiguation module obtained the results provided in the third column.
The three best results and the median from TAC-KBP 2012 systems are shown in the remaining columns for the sake of comparison.
       </p>
      </div>
      <div class="ltx_para" id="S4.p4">
       <p class="ltx_p">
        We observe that the complete algorithm (co-references, named entity labels and MDP) provides the best results on PER NE links.
On GPE and ORG entities, the simple application of MDP without prior corrections obtains the best results.
A slight loss of accuracy is observed on ORG NEs when the MDP is applied with corrections.
For those three categories of entities, we show that the complete system improves the performance of a simple algorithm using distance measures.
Results on categories News and Web show that the best performance on the whole KBP corpus (without distinction of NE categories) is obtained with the complete algorithm.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Conclusion
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        The presented system provides a robust semantic disambiguation method, based on mutual relation of entities inside a document, using a standard annotation engine.
It uses co-reference, NE normalization methods, and Wikipedia internal links as mutual disambiguation resource to improve the annotations.
We show that our proposition improves the performance of a standard annotation engine applied to the TAC-KBP evaluation framework.
SemLinker is fully implemented, and publicly released as an open source toolkit (
        http://code.google.com/p/semlinker
        ).
It has been deployed in the TAC-KBP 2013 evaluation campaign.
Our future work will integrate other annotation engines in the system architecture in a collaborative approach.
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
