<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   Opinion Mining on YouTube.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        Social media such as Twitter, Facebook or YouTube contain rapidly changing information generated by millions of users that can dramatically affect the reputation of a person or an organization. This raises the importance of automatic extraction of sentiments and opinions expressed in social media.
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between people sharing an interest.
Hence, doing sentiment research in such an environment is highly relevant for the community.
While the linguistic conventions used on Twitter and YouTube indeed show similarities
        [2]
        , focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research.
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by the classification system of YouTube comments proposed by
        Siersdorfer et al. (2010)
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        While previous state-of-the-art models for opinion classification have been successfully applied to traditional corpora
        [15]
        ,
YouTube comments pose additional challenges: (i) polarity words can refer to either video or product while expressing contrasting sentiments; (ii) many comments are unrelated or contain spam; and (iii) learning supervised models requires training data for each different YouTube domain, e.g.,
        tablets
        ,
        automobiles
        , etc.
For example, consider a typical comment on a YouTube review video about a
        Motorola Xoom
        tablet:
       </p>
       <blockquote class="ltx_quote">
        <p class="ltx_p">
         this guy really puts a  negative spin on this , and I ’m not sure why , this seems  crazy fast , and I ’m not entirely sure why his pinch to zoom his  laggy all the other xoom reviews
        </p>
       </blockquote>
       <p class="ltx_p">
        The comment contains a product name
        xoom
        and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video.
Similarly, the following comment:
       </p>
       <blockquote class="ltx_quote">
        <p class="ltx_p">
         iPad 2 is better. the superior apps just  destroy the xoom.
        </p>
       </blockquote>
       <p class="ltx_p">
        contains two positive and one negative word, yet the sentiment towards the product is negative (the negative word
        destroy
        refers to
        Xoom
        ). Clearly, the bag-of-words lacks the structural information linking the sentiment with the target product.
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        In this paper, we carry out a systematic study on OM targeting YouTube comments; its contribution is three-fold: firstly, to
solve the problems outlined above, we define a classification schema, which separates spam and not related comments from the informative ones, which are, in turn, further categorized into video- or product-related comments (type classification). At the final stage, different classifiers assign polarity (positive, negative or neutral) to each type of a meaningful comment. This allows us to filter out irrelevant comments, providing accurate OM distinguishing comments about the video and the target product.
       </p>
      </div>
      <div class="ltx_para" id="S1.p6">
       <p class="ltx_p">
        The second contribution of the paper is the creation and annotation (by an expert coder) of a comment corpus containing 35k manually labeled comments for two product YouTube domains:
        tablets
        and
        automobiles
        .
        It is the first manually annotated corpus that enables researchers to use supervised methods on YouTube for comment classification and opinion analysis. The comments from different product domains exhibit different properties (cf. Sec.
        5.2
        ), which give the possibility to study the domain adaptability of the supervised models by training on one category and testing on the other (and vice versa).
       </p>
      </div>
      <div class="ltx_para" id="S1.p7">
       <p class="ltx_p">
        The third contribution of the paper is a novel structural representation, based on shallow syntactic trees enriched with conceptual information, i.e., tags generalizing the specific topic of the video, e.g.,
        iPad
        ,
        Kindle
        ,
        Toyota Camry
        .
Given the complexity and the novelty of the task, we exploit structural kernels to automatically engineer novel features. In particular, we define an efficient tree kernel derived from the Partial Tree Kernel,
        [10]
        , suitable for encoding structural representation of comments into Support Vector Machines (SVMs).
Finally, our results show that our models are adaptable, especially when the structural information is used. Structural models generally improve on both tasks – polarity and type classification – yielding up to 30% of relative improvement, when little data is available.
Hence, the impractical task of annotating data for each YouTube category can be mitigated by the use of models that adapt better across domains.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Related work
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        Most prior work on more general OM has been carried out on more
standardized forms of text, such as consumer reviews or newswire. The
most commonly used datasets include: the MPQA corpus of news documents
        [29]
        , web customer review data
        [6]
        , Amazon review
data
        [3]
        , the JDPA corpus of blogs
        [8]
        , etc. The aforementioned corpora are, however, only
partially suitable for developing models on social media, since the
informal text poses additional challenges for Information Extraction
and Natural Language Processing. Similar to Twitter, most YouTube
comments are very short, the language is informal with numerous
accidental and deliberate errors and grammatical inconsistencies,
which makes previous corpora less suitable to train models for OM on
YouTube. A recent study focuses on sentiment analysis for
Twitter
        [14]
        , however, their corpus was compiled automatically by
searching for emoticons expressing positive and negative sentiment
only.
       </p>
      </div>
      <div class="ltx_para" id="S2.p2">
       <p class="ltx_p">
        Siersdorfer et al. (2010)
        focus on exploiting user ratings
(counts of ‘thumbs up/down’ as flagged by other users) of
YouTube video comments to train classifiers to predict the community
acceptance of new comments. Hence, their goal is different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting
the information from user ratings is a feature that we have not
exploited thus far, but we believe that it is a valuable feature to
use in future work.
       </p>
      </div>
      <div class="ltx_para" id="S2.p3">
       <p class="ltx_p">
        Most of the previous work on supervised sentiment analysis use feature
vectors to encode documents. While a few successful attempts have been
made to use more involved linguistic analysis for opinion mining,
such as dependency trees with latent nodes
        [26]
        and
syntactic parse trees with vectorized nodes
        [24]
        ,
recently, a comprehensive study by
        Wang and Manning (2012)
        showed that a
simple model using bigrams and SVMs performs on par with more complex
models.
       </p>
      </div>
      <div class="ltx_para" id="S2.p4">
       <p class="ltx_p">
        In contrast, we show that adding structural features from syntactic
trees is particularly useful for the cross-domain setting.
They help to build a system that is more robust across domains.
Therefore, rather than trying to build a specialized system for every
new target domain, as it has been done in most prior work on domain
adaptation
        [3, 4]
        , the domain
adaptation problem boils down to finding a more robust
system
        [25, 17]
        .
This is in line with recent advances in parsing the
web
        [16]
        , where participants where asked to build
a single system able to cope with different yet related domains.
       </p>
      </div>
      <div class="ltx_para" id="S2.p5">
       <p class="ltx_p">
        Our approach relies on robust syntactic structures to automatically
generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering
        [12, 19, 20]
        and Semantic Textual Similarity
        [21]
        .
Moreover, we introduce additional tags,
e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Representations and models
      </h2>
      <div class="ltx_para" id="S3.p1">
       <p class="ltx_p">
        Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods.
       </p>
      </div>
      <div class="ltx_subsection" id="S3.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.1
        </span>
        Feature Set
       </h3>
       <div class="ltx_para" id="S3.SS1.p1">
        <p class="ltx_p">
         We enrich the traditional bag-of-word representation with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (
         FVEC
         ) encodes each document using the following feature groups:
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p2">
        <p class="ltx_p">
         -
         word n-grams
         : we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the presence/absence of a given item.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p3">
        <p class="ltx_p">
         -
         lexicon
         : a sentiment lexicon is a collection of words associated with a positive or negative sentiment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lexicon
         [29]
         and the lexicon of
         Hu and Liu (2004)
         . For each of the lexicons, we use the number of words found in the comment that have
         positive
         and
         negative
         sentiment as a feature.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p4">
        <p class="ltx_p">
         -
         negation
         : the count of negation words, e.g., {
         don’t, never, not, etc.
         }, found in a comment.
         Our structural representation (defined next) enables a more involved treatment of negation.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p5">
        <p class="ltx_p">
         -
         video concept
         : cosine similarity between a comment and the title/description of the video. Most of the videos come with a title and a short description, which can be used to encode the topicality of each comment by looking at their overlap.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.2
        </span>
        Structural model
       </h3>
       <div class="ltx_para" id="S3.SS2.p1">
        <p class="ltx_p">
         We go beyond traditional feature vectors by employing structural models (
         STRUCT
         ), which encode each comment into a shallow syntactic tree. These trees are input to tree kernel functions for generating structural features. Our structures are specifically adapted to the noisy user-generated texts and encode important aspects of the comments, e.g., words from the sentiment lexicons, product concepts and negation words, which specifically targets the sentiment and comment type classification tasks.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p2">
        <p class="ltx_p">
         In particular, our shallow tree structure is a two-level syntactic hierarchy built from word lemmas (leaves) and part-of-speech tags that are further grouped into chunks (Fig.
         1
         ). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models
         [18, 5]
         trained specifically to process noisy texts showing significant reductions in the error rate on user-generated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger
         [5, 13]
         to obtain the part-of-speech tags.
Our second component – chunker – is taken from
         [18]
         , which also comes with a model trained on Twitter data
         and shown to perform better on noisy data such as user comments.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p3">
        <p class="ltx_p">
         To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts of the video, (ii) sentiment-bearing words expressing
         positive
         or
         negative
         sentiment and (iii) negation words. To automatically identify concept words of the video we use context words (tokens detected as nouns by the part-of-speech tagger) from the video title and video description and match them in the tree. For the matched words, we enrich labels of their parent nodes (part-of-speech and chunk) with the
         PRODUCT
         tag. Similarly, the nodes associated with words found in the sentiment lexicon are enriched with a polarity tag (either
         positive
         or
         negative
         ), while negation words are labeled with the
         NEG
         tag. It should be noted that vector-based (
         FVEC
         ) model relies only on feature counts whereas the proposed tree encodes powerful contextual syntactic features in terms of tree fragments. The latter are automatically generated and learned by SVMs with expressive tree kernels.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p4">
        <p class="ltx_p">
         For example, the comment in Figure
         1
         shows two
         positive
         and one
         negative
         word from the sentiment lexicon. This would strongly bias the
         FVEC
         sentiment classifier to assign a
         positive
         label to the comment. In contrast, the
         STRUCT
         model relies on the fact that the negative word,
         destroy
         , refers to the PRODUCT (
         xoom
         ) since they form a verbal phase (VP). In other words, the tree fragment:
         [S [negative-VP [negative-V [destroy]] [PRODUCT-NP [PRODUCT-N [xoom]]]]
         is a strong feature (induced by tree kernels) to help the classifier to discriminate such hard cases. Moreover, tree kernels generate all possible subtrees, thus producing generalized (back-off) features, e.g.,
         [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]]
         or
         [S [negative-VP [PRODUCT-NP]]]]
         .
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.3
        </span>
        Learning
       </h3>
       <div class="ltx_para" id="S3.SS3.p1">
        <p class="ltx_p">
         We perform OM on YouTube using supervised methods, e.g., SVM. Our goal is to learn a model to automatically detect the sentiment and type of each comment. For this purpose, we build a multi-class classifier using the one-vs-all scheme. A binary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVM-light-TK
         , which encodes structural kernels in the SVM-light
         [7]
         solver. We define a novel and efficient tree kernel function, namely,
         Sh
         allow syntactic
         T
         ree
         K
         ernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK)
         [10]
         to handle feature engineering over the structural representations of the
         STRUCT
         model. A polynomial kernel of degree 3 is applied to feature vectors (
         FVEC
         ).
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p2">
        <p class="ltx_p">
         Combining structural and vector models.
         A typical kernel machine, e.g., SVM, classifies a test input
         𝒙
         using the following prediction function:
         h⁢(𝒙)=∑iαi⁢yi⁢K⁢(𝒙,𝒙i)
         , where
         αi
         are the model parameters estimated from the training data,
         yi
         are target variables,
         𝒙i
         are support vectors, and
         K(⋅,⋅)
         is a kernel function. The latter computes the
         similarity
         between two comments.
The
         STRUCT
         model treats each comment as a tuple
         𝒙=⟨𝑻,𝒗⟩
         composed of a shallow syntactic tree
         𝑻
         and a feature vector
         𝒗
         . Hence, for each pair of comments
         𝒙1
         and
         𝒙2
         , we define the following comment similarity kernel:
        </p>
        K⁢(𝒙1,𝒙2)=KTK⁢(𝑻1,𝑻2)+Kv⁢(𝒗1,𝒗2),

(1)
        <p class="ltx_p">
         where
         KTK
         computes SHTK (defined next), and
         Kv
         is a kernel over feature vectors, e.g., linear, polynomial, Gaussian, etc.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p3">
        <p class="ltx_p">
         Shallow syntactic tree kernel.
         Following the convolution kernel framework, we define the new SHTK function from Eq.
         1
         to compute the similarity between tree structures. It counts the number of common substructures between two trees
         T1
         and
         T2
         without explicitly considering the whole fragment space. The general equations for Convolution Tree Kernels is:
        </p>
        T⁢K⁢(T1,T2)=∑n1∈NT1∑n2∈NT2Δ⁢(n1,n2),

(2)
        <p class="ltx_p">
         where
         NT1
         and
         NT2
         are the sets of the
         T1
         ’s and
         T2
         ’s nodes, respectively and
         Δ⁢(n1,n2)
         is equal to the number of common fragments rooted in the
         n1
         and
         n2
         nodes, according to several possible definition of the atomic fragments.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p4">
        <p class="ltx_p">
         To improve the speed computation of
         T⁢K
         , we consider pairs of nodes
         (n1,n2)
         belonging to the same tree level. Thus, given
         H
         , the height of the
         STRUCT
         trees, where each level
         h
         contains nodes of the same type, i.e., chunk, POS, and lexical nodes, we define SHTK as the following
         :
        </p>
        S⁢H⁢T⁢K⁢(T1,T2)=∑h=1H∑n1∈NT1h∑n2∈NT2hΔ⁢(n1,n2),

(3)
        <p class="ltx_p">
         where
         NT1h
         and
         NT2h
         are sets of nodes at height
         h
         .
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p5">
        <p class="ltx_p">
         The above equation can be applied with any
         Δ
         function. To have a more general and expressive kernel, we use
         Δ
         previously defined for PTK. More formally: if
         n1
         and
         n2
         are leaves then
         Δ⁢(n1,n2)=μ⁢λ⁢(n1,n2)
         ;
else
         Δ⁢(n1,n2)=
        </p>
        μ⁢(λ2+∑I→1,I→2,|I→1|=|I→2|λd⁢(I→1)+d⁢(I→2)⁢∏j=1|I→1|Δ⁢(cn1⁢(I→1⁢j),cn2⁢(I→2⁢j))),
        <p class="ltx_p">
         where
         λ,μ∈[0,1]
         are decay factors; the large sum is adopted from a definition of the subsequence kernel
         [22]
         to generate children subsets with gaps, which are then used in a recursive call to
         Δ
         .
Here,
         cn1⁢(i)
         is the
         it⁢h
         child of the node
         n1
         ;
         I→1
         and
         I→2
         are two sequences of indexes that enumerate subsets of children with gaps, i.e.,
         I→=(i1,i2,..,|I|)
         , with
         1≤i1&lt;i2&lt;..&lt;i|I|
         ; and
         d⁢(I→1)=I→1⁢l⁢(I→1)-I→11+1
         and
         d⁢(I→2)=I→2⁢l⁢(I→2)-I→21+1
         , which penalizes subsequences with larger gaps.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p6">
        <p class="ltx_p">
         It should be noted that:
firstly, the use of a subsequence kernel makes it possible to generate child subsets of the two nodes, i.e., it allows for gaps, which makes matching of syntactic patterns less rigid. Secondly, the resulting SHTK is essentially a special case of PTK
         [10]
         , adapted to the shallow structural representation
         STRUCT
         (see Sec.
         3.2
         ). When applied to
         STRUCT
         trees, SHTK exactly computes the same feature space as PTK, but in faster time (on average). Indeed, SHTK required to be only applied to node pairs from the same level (see Eq.
         3
         ), where the node labels can match – chunk, POS or lexicals. This reduces the time for selecting the matching-node pairs carried out in PTK
         [10, 11]
         . The fragment space is obviously the same, as the node labels of different levels in
         STRUCT
         are different and will not be matched by PTK either.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p7">
        <p class="ltx_p">
         Finally, given its recursive definition in Eq.
         3
         and the use of subsequence (with gaps), SHTK can derive useful dependencies between its elements. For example, it will generate the following subtree fragments:
         [positive-NP [positive-A N]], [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]]
         and so on.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       YouTube comments corpus
      </h2>
      <div class="ltx_para" id="S4.p1">
       <p class="ltx_p">
        To build a corpus of YouTube comments, we focus on a particular set of videos (technical reviews and advertisings) featuring commercial products. In particular, we chose two product categories: automobiles (
        AUTO
        ) and tablets (
        TABLETS
        ). To collect the videos, we compiled a list of products and queried the YouTube gData API
        to retrieve the videos. We then manually excluded irrelevant videos. For each video, we extracted all available comments (limited to maximum 1k comments per video) and manually annotated each comment with its type and polarity. We distinguish between the following types:
       </p>
      </div>
      <div class="ltx_para" id="S4.p2">
       <p class="ltx_p">
        product
        : discuss the topic product in general or some features of the product;
       </p>
      </div>
      <div class="ltx_para" id="S4.p3">
       <p class="ltx_p">
        video
        : discuss the video or some of its details;
       </p>
      </div>
      <div class="ltx_para" id="S4.p4">
       <p class="ltx_p">
        spam
        : provide advertising and malicious links; and
       </p>
      </div>
      <div class="ltx_para" id="S4.p5">
       <p class="ltx_p">
        off-topic
        : comments that have almost no content (“lmao”) or content that is not related to the video (“Thank you!”).
       </p>
      </div>
      <div class="ltx_para" id="S4.p6">
       <p class="ltx_p">
        Regarding the polarity, we distinguish between {
        positive, negative, neutral
        } sentiments with respect to the product and the video. If the comment contains several statements of different polarities, it is annotated as both
        positive
        and
        negative
        :
“Love the video but waiting for iPad 4”.
In total we have annotated 208 videos with around 35k comments (128 videos
        TABLETS
        and 80 for
        AUTO
        ).
       </p>
      </div>
      <div class="ltx_para" id="S4.p7">
       <p class="ltx_p">
        To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement
        α
        value
        [9, 1]
        scores are 60.6 (
        AUTO
        ), 72.1 (
        TABLETS
        ) for the sentiment task and 64.1 (
        AUTO
        ), 79.3 (
        TABLETS
        ) for the
        type
        classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in
        Uryupina et al. (2014)
        .
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Experiments
      </h2>
      <div class="ltx_para" id="S5.p1">
       <p class="ltx_p">
        This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalability to other domains.
       </p>
      </div>
      <div class="ltx_subsection" id="S5.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.1
        </span>
        Task description
       </h3>
       <div class="ltx_para" id="S5.SS1.p1">
        <p class="ltx_p">
         Sentiment classification.
         We treat each comment as expressing
         positive
         ,
         negative
         or
         neutral
         sentiment. Hence, the task is a three-way classification.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS1.p2">
        <p class="ltx_p">
         Type classification.
         One of the challenging aspects of sentiment analysis of YouTube data is that the comments may express the sentiment not only towards the
         product
         shown in the video, but also the
         video
         itself, i.e., users may post positive comments to the video while being generally negative about the product and vice versa. Hence, it is of crucial importance to distinguish between these two types of comments. Additionally, many comments are irrelevant for both the product and the video (
         off-topic
         ) or may even contain
         spam
         .
Given that the main goal of sentiment analysis is to select sentiment-bearing comments and identify their polarity, distinguishing between
         off-topic
         and
         spam
         categories is not critical.
Thus, we merge the
         spam
         and
         off-topic
         into a single
         uninformative
         category. Similar to the opinion classification task, comment type classification is a multi-class classification with three classes:
         video
         ,
         product
         and
         uninform
         .
        </p>
       </div>
       <div class="ltx_para" id="S5.SS1.p3">
        <p class="ltx_p">
         Full task.
         While the previously discussed sentiment and type identification tasks are useful to model and study in their own right, our end goal is: given a stream of comments, to jointly predict both the type and the sentiment of each comment.
We cast this problem as a single multi-class classification task with seven classes: the Cartesian product between {
         product, video
         } type labels and {
         positive, neutral, negative
         } sentiment labels plus the
         uninformative
         category (
         spam
         and
         off-topic
         ). Considering a real-life application, it is important not only to detect the polarity of the comment, but to also identify if it is expressed towards the product or the video.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S5.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.2
        </span>
        Data
       </h3>
       <div class="ltx_para" id="S5.SS2.p1">
        <p class="ltx_p">
         We split all the videos 50% between training set (TRAIN) and test set (TEST), where each video contains all its comments. This ensures that all comments from the same video appear either in TRAIN or in TEST. Since the number of comments per video varies, the resulting sizes of each set are different (we use the larger split for TRAIN). Table
         1
         shows the data distribution across the task-specific classes –
         sentiment
         and
         type
         classification. For the
         sentiment
         task we exclude
         off-topic
         and
         spam
         comments as well as comments with ambiguous sentiment, i.e., annotated as both
         positive
         and
         negative
         .
        </p>
       </div>
       <div class="ltx_para" id="S5.SS2.p2">
        <p class="ltx_p">
         For the
         sentiment
         task about 50% of the comments have
         neutral
         polarity, while the
         negative
         class is much less frequent. Interestingly, the ratios between polarities expressed in comments from
         AUTO
         and
         TABLETS
         are very similar across both TRAIN and TEST.
Conversely, for the
         type
         task, we observe that comments from
         AUTO
         are uniformly distributed among the three classes, while for the
         TABLETS
         the majority of comments are
         product
         related. It is likely due to the nature of the
         TABLETS
         videos, that are more geek-oriented, where users are more prone to share their opinions and enter involved discussions about a product. Additionally, videos from the
         AUTO
         category (both commercials and user reviews) are more visually captivating and, being generally oriented towards a larger audience, generate more video-related comments.
Regarding the
         full
         setting, where the goal is to have a joint prediction of the comment sentiment and type, we observe that
         video-negative
         and
         video-positive
         are the most scarce classes, which makes them the most difficult to predict.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S5.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.3
        </span>
        Results
       </h3>
       <div class="ltx_para" id="S5.SS3.p1">
        <p class="ltx_p">
         We start off by presenting the results for the traditional in-domain setting, where both TRAIN and TEST come from the same domain, e.g.,
         AUTO
         or
         TABLETS
         . Next, we show the learning curves to analyze the behavior of
         FVEC
         and
         STRUCT
         models according to the training size. Finally, we perform a set of cross-domain experiments that describe the enhanced adaptability of the patterns generated by the
         STRUCT
         model.
        </p>
       </div>
       <div class="ltx_subsubsection" id="S5.SS3.SSS1">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          5.3.1
         </span>
         In-domain experiments
        </h4>
        <div class="ltx_para" id="S5.SS3.SSS1.p1">
         <p class="ltx_p">
          We compare
          FVEC
          and
          STRUCT
          models on three tasks described in Sec.
          5.1
          : sentiment, type and full. Table
          2
          reports the per-class performance and the overall accuracy of the multi-class classifier.
Firstly, we note that the performance on
          TABLETS
          is much higher than on
          AUTO
          across all tasks. This can be explained by the following: (i)
          TABLETS
          contains more training data and (ii) videos from
          AUTO
          and
          TABLETS
          categories draw different types of audiences – well-informed users and geeks expressing better-motivated opinions about a product for the former vs. more general audience for the latter. This results in the different quality of comments with the
          AUTO
          being more challenging to analyze.
Secondly, we observe that the
          STRUCT
          model provides 1-3% of absolute improvement in accuracy over
          FVEC
          for every task. For individual categories the F1 scores are also improved by the
          STRUCT
          model (except for the
          negative
          classes for
          AUTO
          , where we see a small drop). We conjecture that sentiment prediction for
          AUTO
          category is largely driven by one-shot phrases and statements where it is hard to improve upon the bag-of-words and sentiment lexicon features. In contrast, comments from
          TABLETS
          category tend to be more elaborated and well-argumented, thus, benefiting from the expressiveness of the structural representations.
         </p>
        </div>
        <div class="ltx_para" id="S5.SS3.SSS1.p2">
         <p class="ltx_p">
          Considering per-class performance, correctly predicting
          negative
          sentiment is most difficult for both
          AUTO
          and
          TABLETS
          , which is probably caused by the smaller proportion of the negative comments in the training set. For the
          type
          task, video-related class is substantially more difficult than product-related for both categories. For the
          full
          task, the class
          video-negative
          accounts for the largest error. This is confirmed by the results from the previous sentiment and type tasks, where we saw that handling negative sentiment and detecting video-related comments are most difficult.
         </p>
        </div>
       </div>
       <div class="ltx_subsubsection" id="S5.SS3.SSS2">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          5.3.2
         </span>
         Learning curves
        </h4>
        <div class="ltx_para" id="S5.SS3.SSS2.p1">
         <p class="ltx_p">
          The learning curves depict the behavior of
          FVEC
          and
          STRUCT
          models as we increase the size of the training set. Intuitively, the
          STRUCT
          model relies on more general syntactic patterns and may overcome the sparseness problems incurred by the
          FVEC
          model when little training data is available.
         </p>
        </div>
        <div class="ltx_para" id="S5.SS3.SSS2.p2">
         <p class="ltx_p">
          Nevertheless, as we see in Figure
          2
          , the learning curves for sentiment and type classification tasks across both product categories do not confirm this intuition. The
          STRUCT
          model consistently outperforms the
          FVEC
          across all training sizes, but the gap in the performance does not increase when we move to smaller training sets. As we will see next, this picture changes when we perform the cross-domain study.
         </p>
        </div>
       </div>
       <div class="ltx_subsubsection" id="S5.SS3.SSS3">
        <h4 class="ltx_title ltx_title_subsubsection">
         <span class="ltx_tag ltx_tag_subsubsection">
          5.3.3
         </span>
         Cross-domain experiments
        </h4>
        <div class="ltx_para" id="S5.SS3.SSS3.p1">
         <p class="ltx_p">
          To understand the performance of our classifiers on other YouTube domains, we perform a set of cross-domain experiments by training on the data from one product category and testing on the other.
         </p>
        </div>
        <div class="ltx_para" id="S5.SS3.SSS3.p2">
         <p class="ltx_p">
          Table
          3
          reports the accuracy for three tasks when we use all comments (TRAIN + TEST) from
          AUTO
          to predict on the TEST from
          TABLETS
          and in the opposite direction (
          TABLETS→AUTO
          ). When using
          AUTO
          as a source domain,
          STRUCT
          model provides additional 1-3% of absolute improvement, except for the sentiment task.
         </p>
        </div>
        <div class="ltx_para" id="S5.SS3.SSS3.p3">
         <p class="ltx_p">
          Similar to the in-domain experiments, we studied the effect of the source domain size on the target test performance. This is useful to assess the adaptability of features exploited by the
          FVEC
          and
          STRUCT
          models with the change in the number of labeled examples available for training. Additionally, we considered a setting including a small amount of training data from the target data (i.e., supervised domain adaptation).
         </p>
        </div>
        <div class="ltx_para" id="S5.SS3.SSS3.p4">
         <p class="ltx_p">
          For this purpose, we drew the learning curves of the
          FVEC
          and
          STRUCT
          models applied to the
          sentiment
          and
          type
          tasks (Figure
          3
          ):
          AUTO
          is used as the source domain to train models, which are tested on
          TABLETS
          .
          The plot shows that when little training data is available, the features generated by the
          STRUCT
          model exhibit better adaptability (up to 10% of improvement over FVEC). The bag-of-words model seems to be affected by the data sparsity problem which becomes a crucial issue when only a small training set is available. This difference becomes smaller as we add data from the same domain. This is an important advantage of our structural approach, since we cannot realistically expect to obtain manual annotations for 10k+ comments for each (of many thousands) product domains present on YouTube.
         </p>
        </div>
       </div>
      </div>
      <div class="ltx_subsection" id="S5.SS4">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.4
        </span>
        Discussion
       </h3>
       <div class="ltx_para" id="S5.SS4.p1">
        <p class="ltx_p">
         Our
         STRUCT
         model is more accurate since it is able to induce structural patterns of sentiment. Consider the following comment:
         optimus pad is better. this xoom is just to bulky but optimus pad offers better functionality
         . The
         FVEC
         bag-of-words model misclassifies it to be
         positive
         , since it contains two positive expressions (
         better
         ,
         better functionality
         ) that outweigh a single negative expression (
         bulky
         ). The structural model, in contrast, is able to identify the product of interest (
         xoom
         ) and associate it with the negative expression through a structural feature and thus correctly classify the comment as
         negative
         .
        </p>
       </div>
       <div class="ltx_para" id="S5.SS4.p2">
        <p class="ltx_p">
         Some issues remain problematic even for the structural model. The largest group of errors are implicit sentiments. Thus, some comments do not contain any explicit positive or negative opinions, but provide detailed and well-argumented criticism, for example,
         this phone is heavy
         . Such comments might also include irony. To account for these cases, a deep understanding of the product domain is necessary.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S6">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        6
       </span>
       Conclusions and Future Work
      </h2>
      <div class="ltx_para" id="S6.p1">
       <p class="ltx_p">
        We carried out a systematic study on OM from YouTube comments by training a set of supervised multi-class classifiers distinguishing between video and product related opinions.
We use standard feature vectors augmented by shallow syntactic trees enriched with additional conceptual information.
       </p>
      </div>
      <div class="ltx_para" id="S6.p2">
       <p class="ltx_p">
        This paper makes several contributions: (i) it shows that effective OM can be carried out with supervised models trained on high quality annotations; (ii) it introduces a novel annotated corpus of YouTube comments, which we make available for the research community; (iii) it defines novel structural models and kernels, which can improve on feature vectors, e.g., up to 30% of relative improvement in type classification, when little data is available, and demonstrates that the structural model scales well to other domains.
       </p>
      </div>
      <div class="ltx_para" id="S6.p3">
       <p class="ltx_p">
        In the future, we plan to work on a joint model to classify all the comments of a given video, s.t. it is possible to exploit latent dependencies between entities and the sentiments of the comment thread. Additionally, we plan to experiment with hierarchical multi-label classifiers for the full task (in place of a flat multi-class learner).
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
