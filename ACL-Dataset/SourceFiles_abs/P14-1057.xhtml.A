<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <title>
   A Study of Concept-based Weighting Regularization for Medical Records Search.
  </title>
 </head>
 <body>
  <div class="ltx_page_main">
   <div class="ltx_page_content">
    <div class="ltx_document ltx_authors_1line">
     <div class="ltx_section" id="S1">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        1
       </span>
       Introduction
      </h2>
      <div class="ltx_para" id="S1.p1">
       <p class="ltx_p">
        With the increasing use of electronic health records,
it becomes urgent to leverage this rich information
resource about patients’ health conditions to transform
research in health and medicine. As an example, when
developing a cohort for a clinical trial,
researchers need to identify patients matching a set of clinical
criteria based on their medical records during their
hospital visits
        [22, 8]
        . This selection
process is clearly a domain-specific retrieval problem,
which searches for
        relevant medical records
        that contain
useful information about their corresponding patients’
qualification to the criteria specified in a query,
e.g., “female patient with breast cancer with mastectomies during
admission”.
       </p>
      </div>
      <div class="ltx_para" id="S1.p2">
       <p class="ltx_p">
        Intuitively, to better solve this domain-specific retrieval problem,
we need to understand the requirements specified in a query
and identify the documents satisfying these requirements based on
their semantic meanings. In the past decades, significant efforts
have been put on constructing biomedical knowledge bases
        [2, 17, 4]
        and developing natural language processing (NLP) tools, such as MetaMap,
to utilize the information from the knowledge bases
        [3, 19]
        .
These efforts make it possible to map free text to concepts and use
these concepts to represent queries and documents.
       </p>
      </div>
      <div class="ltx_para" id="S1.p3">
       <p class="ltx_p">
        Indeed,
        concept-based representation
        is one of the commonly used approaches
that leverage knowledge bases to improve the retrieval performance
        [12, 14]
        .
The basic idea is to represent both queries and documents as
“bags of concepts”, where the concepts are identified based on the information
from the knowledge bases. This method has been shown to be more effective
than traditional term-based representation in the medical record retrieval
because of its ability to handle the ambiguity in the medical terminology.
However, this method also suffers the limitation that its effectiveness depends
on the accuracy of the concept mapping results. As a result, directly
applying existing weighting strategies might lead to non-optimal retrieval
performance.
       </p>
      </div>
      <div class="ltx_para" id="S1.p4">
       <p class="ltx_p">
        In this paper, to address the limitation caused by the inaccurate concept mapping results,
we propose to regularize the weighting strategies in the concept-based
representation methods. Specifically, by applying the axiomatic approaches
        [7]
        ,
we analyze the retrieval functions with concept-based representation and find that
they may violate some reasonable retrieval constraints. We then propose two concept-based
weighting regularization methods so that the regularized retrieval functions would satisfy
the retrieval constraints and achieve better retrieval performance.
Experimental results over two TREC collections show that both proposed concept-based weighting
regularization methods can improve the retrieval performance,
and their performance is comparable with the best systems of the TREC Medical
Records tracks
        [25, 24]
        .
       </p>
      </div>
      <div class="ltx_para" id="S1.p5">
       <p class="ltx_p">
        Many NLP techniques have been developed to understand the semantic meaning
of textual information, and are often applied to improve the search accuracy.
However, due to the inherent ambiguity of natural languages, the results of
NLP tools are not perfect.
One of our contributions is to present a
general methodology that can be used
to adjust existing IR techniques based on the inaccurate NLP results.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S2">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        2
       </span>
       Related Work
      </h2>
      <div class="ltx_para" id="S2.p1">
       <p class="ltx_p">
        The Medical Records track of the Text REtrieval Conference (TREC)
provides a common platform to study the medical records retrieval problem
and evaluate the proposed methods
        [25, 24]
        .
       </p>
      </div>
      <div class="ltx_para" id="S2.p2">
       <p class="ltx_p">
        Concept-based representation has been studied for the medical record retrieval
problem
        [12, 14, 13, 20, 9, 10]
        .
For example, Qi and Laquerre used MetaMap to generate the concept-based
representation and then apply a vector space retrieval model for ranking, and their
results are one of the top ranked runs in the TREC 2012 Medical Records track
        [20]
        .
To further improve the performance, Limsopatham et al. proposed a task-specific
representation, i.e., using only four types of concepts (symptom, diagnostic test,
diagnosis and treatment) in the concept-based representation and a query expansion
method based on the relationships among the medical concepts
        [12, 13]
        .
Moreover, they also proposed a learning approach to combine
both term-based and concept-based representation to further improve the
performance
        [14]
        .
       </p>
      </div>
      <div class="ltx_para" id="S2.p3">
       <p class="ltx_p">
        Our work is also related to domain-specific IR
        [26, 16, 29]
        .
For example, Yan et al. proposed a granularity-based document ranking
model that utilizes ontologies to identify document concepts.
However, none of the previous work has studied how to regularize
the weight of concepts based on their relations.
       </p>
      </div>
      <div class="ltx_para" id="S2.p4">
       <p class="ltx_p">
        It is well known that the effectiveness of a retrieval function is closely related to
the weighting strategies
        [7, 23]
        . Various term weighting strategies have been
proposed and studied for the term-based representation
        [1, 23, 21]
        .
However, existing studies on concept-based representation still used weighting
strategies developed for term-based representation such as vector space models
        [20]
        and divergence from randomness (DFR)
        [13]
        and did not
take the inaccurate concept mapping results into consideration.
Compared with previous work, we focus on addressing the limitation caused by the inaccurate
concept mapping. Note that our efforts are orthogonal to existing work, and it is expected to
bring additional improvement to the retrieval performance.
       </p>
      </div>
     </div>
     <div class="ltx_section" id="S3">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        3
       </span>
       Concept-based Representation for Medical Records Retrieval
      </h2>
      <div class="ltx_subsection" id="S3.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.1
        </span>
        Problem Formulation
       </h3>
       <div class="ltx_para" id="S3.SS1.p1">
        <p class="ltx_p">
         We follow the problem setup used in the TREC medical record track
         [25, 24]
         .
The task is to retrieve relevant patient visits with respect to a query. Since each visit can be
associated with multiple medical records, the relevance of a visit is related to the relevance
of individual associated medical records. Existing studies computed the relevance
scores at either visit-level, where all the medical records of a visit are merged into
a visit document
         [5, 15]
         , or record-level, where we can
first compute the relevance score of individual records and then aggregate their scores
as the relevance score of a visit
         [15, 30, 12]
         .
In this paper, we focus on the visit-level relevance because of its simplicity.
In particular, given a patient’s visit, all the medical records generated from this visit are
merged as a document. Note that
our proposed concept-weighting strategies can also be easily applied to record-level relevance modeling.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS1.p2">
        <p class="ltx_p">
         Since the goal is to retrieve medical records of patients that satisfying requirements
specified in a query, the relevance of medical records should be modeled based on
how well they match all the requirements (i.e., aspects) specified in the queries.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.2
        </span>
        Background: UMLS and MetaMap
       </h3>
       <div class="ltx_para" id="S3.SS2.p1">
        <p class="ltx_p">
         Unified Medical Language System (UMLS) is a metathesaurus containing
information from more than 100 controlled medical terminologies
such as the Systematized Nomenclature of Medicine Clinical Terms (SNOMED-CT)
and Medical Subject Headings (MeSH). Specifically, it contains the information
about over 2.8 million biomedical concepts. Each concept is labeled with a
Concept Unique Identifier (CUI) and has a preferred name and a semantic type.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p2">
        <p class="ltx_p">
         Moreover, NLP tools for utilizing the information from
UMLS have been developed. In particular, MetaMap
         [3]
         can
take a text string as the input, segment it into phrases,
and then map each phrase to
         multiple
         UMLS CUIs with confidence scores.
The confidence score is an indicator of the quality of the phrase-to-concept
mapping by MetaMap. It is computed by four metrics: centrality, variation, coverage
and cohesiveness
         [3]
         . These four measures try to evaluate the mapping
from different angles, such as the involvement of the central part, the distance of the
concept to the original phrase, and how well the concept matches the phrase.
The maximum confidence in MetaMap is 1000.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS2.p3">
        <p class="ltx_p">
         Figure
         1
         shows the MetaMap results for an example query
“children with dental caries”.
Two
         query aspects
         , i.e., “children” and “dental caries”, are identified.
Each of them is mapped to multiple concepts, and each concept is associated with
the confidence score as well as more detailed information about this concept.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S3.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         3.3
        </span>
        Concept-based Representation
       </h3>
       <div class="ltx_para" id="S3.SS3.p1">
        <p class="ltx_p">
         Traditional retrieval models are based on “bag of terms” representation. One limitation
of this representation is that relevance scores are computed based on the matching of terms
rather than the meanings. As a result, the system may fail to retrieve the relevant documents
that do not contain any query terms.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p2">
        <p class="ltx_p">
         To overcome this limitation, concept-based representation has been proposed to bridge the
vocabulary gap between documents and queries
         [20, 14, 10]
         .
In particular, MetaMap is used to map terms from queries and documents (e.g., medical records)
to the semantic concepts from biomedical knowledge bases such as UMLS.
Within the concept-based representation, the query can then
be represented as a bag of all the generated CUIs in the MetaMap results.
For example, the query from Figure
         1
         can be represented as
         {C⁢0008059,C⁢0680063,C⁢0011334,C⁢0333519,C⁢0226984}
         . Documents can
be represented in a similar way.
        </p>
       </div>
       <div class="ltx_para" id="S3.SS3.p3">
        <p class="ltx_p">
         After converting both queries and documents to concept-based representations using
MetaMap, previous work applied existing retrieval functions such as vector
space models
         [23]
         to rank the documents. Note that when referring to
existing retrieval functions in the paper, they include traditional keyword matching
based functions such as pivoted normalization
         [23]
         , Okapi
         [21]
         ,
Dirichlet prior
         [27]
         and basic axiomatic functions
         [7]
         .
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S4">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        4
       </span>
       Weighting Strategies for Concept-based Representation
      </h2>
      <div class="ltx_subsection" id="S4.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.1
        </span>
        Motivation
       </h3>
       <div class="ltx_para" id="S4.SS1.p1">
        <p class="ltx_p">
         Although existing retrieval functions can be directly applied to concept-based
representation, they may lead to non-optimal performance. This is mainly caused
by the fact that MetaMap may generate more than one mapped concepts for an
aspect, i.e., a semantic unit in the text.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p2">
        <p class="ltx_p">
         Ideally, an aspect will be mapped to only
         one
         concept, and
different concepts would represent different semantic meanings. Under such
a situation, traditional retrieval functions would likely work well and generate
satisfying retrieval performance since the relations among concepts are
independent which is consistent with the assumptions made in traditional IR
         [18]
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p3">
        <p class="ltx_p">
         However, the mapping results generated by MetaMap are not perfect. Although
MetaMap is able to rank all the candidate concepts with the confidence score
and pick the most likely one, the accuracy is not very high. In particular,
our preliminary results show that turning on the disambiguation functionality
provided by MetaMap (i.e., returning only the most likely concept for each query)
could lead to worse retrieval performance than using all the candidate mappings.
Thus, we use the one-to-many mapping results generated by MetaMap, in which
each aspect can be mapped to multiple concepts.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p4">
        <p class="ltx_p">
         Unfortunately, such one-to-many concept mappings could hinder the retrieval
performance in the following two ways.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p5">
        <ul class="ltx_itemize" id="I1">
         <li class="ltx_item" id="I1.i1" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_itemize">
           •
          </span>
          <div class="ltx_para" id="I1.i1.p1">
           <p class="ltx_p">
            The multiple concepts generated from the same aspect are related, which
is inconsistent with the independence assumption made in the existing retrieval
functions
            [18]
            . For example, as shown in Figure
            1
            ,
“dental caries” is mapped to three concepts. It is clear that the concepts
are related, but existing retrieval functions are unable to capture their
relations and would compute the weight of each concept independently.
           </p>
          </div>
         </li>
         <li class="ltx_item" id="I1.i2" style="list-style-type:none;">
          <span class="ltx_tag ltx_tag_itemize">
           •
          </span>
          <div class="ltx_para" id="I1.i2.p1">
           <p class="ltx_p">
            The one-to-many mapping results generated by MetaMap could arbitrarily
inflate the weights of some query aspects.
For example, as shown in Figure
            1
            , query aspect “children” is mapped
to 2 concepts while “dental caries” is mapped to 3 concepts.
In the existing retrieval functions,
term occurrences are important relevance signals. However,
when converting the text to concepts representation using MetaMap, the
occurrences of the concepts are determined by not only the original term occurrences,
a good indicator of relevance, but also the number of mapped concepts,
which is determined by MetaMap and has nothing to do with the relevance status.
As a result, the occurrences of concepts might
not be a very accurate indicator of importance of the corresponding query aspect.
           </p>
          </div>
         </li>
        </ul>
       </div>
       <div class="ltx_para" id="S4.SS1.p6">
        <p class="ltx_p">
         To address the limitations caused by the inaccurate mapping results, we propose to
apply axiomatic approaches
         [7]
         to regularize the weighting strategies for
concept-based representation methods. In particular, we first formalize retrieval
constraints that any reasonable concept-based representation methods should satisfy
and then discuss how to regularize the existing weighting strategies to satisfy
the constraints and improve the retrieval performance.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS1.p7">
        <p class="ltx_p">
         We first explain the notations used in this section.
         Q
         and
         D
         denote a query
and a document with the concept-based representation.
         S⁢(Q,D)
         is the relevance score of
         D
         with respect to
         Q
         .
         ei
         denotes
a concept, and
         𝒜⁢(e)
         denotes the query aspect associated with
         e
         , i.e., a set of concepts that are mapped to the same phrases as
         e
         by using
MetaMap.
         i⁢(e)
         is the normalized confidence score of the mapping for concept
         e
         generated
by MetaMap.
         c⁢(e,D)
         denotes the occurrences of concept
         e
         in document
         D
         ,
         d⁢f⁢(e)
         denotes the number of documents containing
         e
         .
         |D|
         is the document length of
         D
         .
         I⁢m⁢pc⁢(e)
         is the importance of the concept
such as the concept IDF value, and
         I⁢m⁢pA⁢(𝒜)
         is the importance
of the aspect.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.2
        </span>
        Unified concept weighting regularization
       </h3>
       <div class="ltx_para" id="S4.SS2.p1">
        <p class="ltx_p">
         We now discuss how to address the first challenge, i.e,. how to regularize
the weighting strategy so that we can take into consideration the fact that
concepts associated with the same query aspect are not independent.
We call a concept is a
         variant
         of another one if both of them are
associated with the same aspect.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p2">
        <p class="ltx_p">
         Intuitively, given a query with two aspects, a document covering
both aspects should be ranked higher than those covering only one
aspect. We can formalize the intuition in the concept-based
representation as the following constraint.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p3">
        <p class="ltx_p">
         Unified Constraint:
         Let query be
         Q={e1,e2,e3}
         ,
and we know that
         e2
         is a variant of
         e3
         .
Assume we have two documents
         D1
         and
         D2
         with the same
document length, i.e.,
         |D1|=|D2|
         . If we know that
         c⁢(e1,D1)=c⁢(e3,D2)&gt;0
         ,
         c⁢(e1,D2)=c⁢(e3,D1)=0
         and
         c⁢(e2,D1)=c⁢(e2,D2)&gt;0
         , then
         S⁢(Q,D1)&gt;S⁢(Q,D2)
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p4">
        <p class="ltx_p">
         It is clear that existing retrieval functions would violate
this constraint since they ignore the relations among concepts.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p5">
        <p class="ltx_p">
         One simple strategy to fix this problem is to merge all the concept
variants as a single concept and select one representative concept to
replace all occurrences of other variants in both queries and documents.
By merging the concepts together, we are aiming to purify the concepts and
make the similar concepts centralized so that the assumption that all
the concepts are independent would hold.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p6">
        <p class="ltx_p">
         Formally, the adjusted occurrences of a concept
         e
         in a document
         D
         is shown as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p7">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="S6.EGx1">
         <tr class="ltx_equation ltx_align_baseline" id="S4.E1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           cm⁢o⁢d⁢(e,D)={∑e′∈E⁢C⁢(e)c⁢(e′,D)e=R⁢e⁢p⁢(E⁢C⁢(e))0e≠R⁢e⁢p⁢(E⁢C⁢(e))
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (1)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S4.SS2.p8">
        <p class="ltx_p">
         where
         c⁢(e,D)
         is the original occurrence of concept
         e
         in document
         D
         ,
         E⁢C⁢(e)
         denotes a set of all the variants of
         e
         including itself
(i.e., all the concepts with the same preferred name as
         e
         ), and
         R⁢e⁢p⁢(E⁢C⁢(e))
         denotes the representative concept from
         E⁢C⁢(e)
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p9">
        <p class="ltx_p">
         It is trivial to prove that, with such changes, existing retrieval
functions would satisfy the above constraint since the constraint
implies TFC2 constraint defined in the previous study
         [6]
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS2.p10">
        <p class="ltx_p">
         Now the remaining question is how to select the representative concept
from all the variants. There are three options: select the concept
with the maximum IDF, average IDF, or minimum IDF. We conduct
exploratory data analysis on these three options. In particular,
for each option, we generate a plot indicating the correlation
between the IDF value of a concept and the relevance probability
of the concept (i.e., the probability that a document containing
the concept is relevant). Note that both original and replaced
IDF values are shown in the plot for each option. Figure
         2
         shows the
results. It is clear that the right plot (i.e., selecting the concept
with the maximum IDF as the representative concept) is the best
choice since the changes make the points less scattered.
In fact, this can also be confirmed by experimental results as reported
in Table
         5
         . Thus, we use the concept with the maximum
IDF value as the representative concept of all the variants.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.3
        </span>
        Balanced concept weighting regularization
       </h3>
       <div class="ltx_para" id="S4.SS3.p1">
        <p class="ltx_p">
         We now discuss how to address the second challenge, i.e., how to
regularize the weighting strategy to deal with the arbitrarily
inflated statistics caused by the one-to-many mappings.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p2">
        <p class="ltx_p">
         The arbitrary inflation could impact the importance of the query aspects.
For example, as shown in Figure
         1
         , one aspect is mapped
to two concepts while the other is mapped to three.
Moreover, it could also impact the accuracy of the concept IDF values.
Consider “colonoscopies” and “adult”, it is clear that the first term
is more important than the second one, which is consistent with their
term IDF values, i.e., 7.52 and 2.92, respectively. However, with the
concept-based representation, the IDF value of the concept “colonoscopies”(C0009378) is
2.72, which is even smaller than that of concept “adult” (C1706450), i.e., 2.92.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p3">
        <p class="ltx_p">
         To fix the negative impact on query aspects, we could leverage the findings in the
previous study
         [28]
         and regularize the weighting strategy based on the length
of query aspects to favor documents covering more query aspects. Since each concept
mapping is associated with a confidence score, we can incorporate them into the
regularization function as follows:
        </p>
        f⁢(e,Q)=(1-α)+α⋅(∑e′∈Qi⁢(e′)∑e′′∈𝒜⁢(e)i⁢(e′′)),

(2)
        <p class="ltx_p">
         where
         i⁢(e)
         is the normalized confidence score of concept
         e
         generated by MetaMap,
and
         α
         is a parameter between 0 and 1 to control the effect of the regularization.
When
         α
         is set to 0, there is no regularization. This regularization function
aims to penalize the weight of concept
         e
         based on its variants as well as the
concepts from other aspects. In particular, a concept would receive more penalty
(i.e., its weight will be decreased more) when it has more variants and the mappings
of these variants are more accurate.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p4">
        <p class="ltx_p">
         To fix the negative impact on the concept IDF values, we propose to regularize
the weighting based on the importance of the query aspect. This regularization
can be formalized as the following constraint.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p5">
        <p class="ltx_p">
         Balanced Constraint:
         Let
         Q
         be a query with two concepts and the
concepts are associated with different aspects, i.e.,
         Q={e1,e2}
         ,
and
         𝒜⁢(e1)≠𝒜⁢(e2)
         .
Assume
         D1
         and
         D2
         are two documents with the same length, i.e.,
         |D1|=|D2|
         ,
and they cover different concepts with the same occurrences, i.e.,
         c⁢(e1,D1)=c⁢(e2,D2)&gt;0
         and
         c⁢(e2,D1)=c⁢(e1,D2)=0
         .
If we know
         I⁢m⁢pc⁢(e1)=I⁢m⁢pc⁢(e2)
         and
         I⁢m⁢pA⁢(𝒜⁢(e1))&lt;I⁢m⁢pA⁢(𝒜⁢(e2))
         ,
then we have
         S⁢(Q,D1)&lt;S⁢(Q,D2)
         .
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p6">
        <p class="ltx_p">
         This constraint requires that the relevance score of a document should be
affected by not only the importance of the concepts but also the importance
of the associated query aspect. In a way, the constraint aims to counteract
the arbitrary statistics inflation caused by MetaMap results and balance
the weight among concepts based on the importance of the associated query aspects.
And it is not difficult to show that existing retrieval functions violate this constraint.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p7">
        <p class="ltx_p">
         Now the question is how to revise the retrieval functions to make them satisfy
this constraint. We propose to incorporate the importance of query aspect
into the previous regularization function in Equation (
         2
         ) as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS3.p8">
        f⁢(e,Q)=(1-α)+α⋅(∑e′∈Qi⁢(e′)∑e′′∈𝒜⁢(e)i⁢(e′′))⋅I⁢m⁢pA⁢(𝒜⁢(e)).

(3)
       </div>
       <div class="ltx_para" id="S4.SS3.p9">
        <p class="ltx_p">
         Note that
         I⁢m⁢pA⁢(𝒜⁢(e))
         is the importance of a query aspect and can
be estimated based on the terms from the query aspect. In this paper, we
use the maximum term IDF value from the aspect to estimate the importance,
which performs better than using minimum and average IDF values as shown
in the experiments (i.e., Table
         6
         ).
We plan to study other options in the future work.
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S4.SS4">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         4.4
        </span>
        Discussions
       </h3>
       <div class="ltx_para" id="S4.SS4.p1">
        <p class="ltx_p">
         Both proposed regularization methods can be combined with any existing
retrieval functions. In this paper, we focus on one of the state of the art
weighting strategies, i.e., F2-EXP function derived from axiomatic
retrieval model
         [7]
         , and
explain how to incorporate the regularization methods into the function.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p2">
        <p class="ltx_p">
         The original F2-EXP retrieval function is shown as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p3">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="S6.EGx2">
         <tr class="ltx_equation ltx_align_baseline" id="S4.E4">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           S⁢(Q,D)=∑e∈Q∩Dc⁢(e,Q)⋅(Nd⁢f⁢(e))0.35⋅c⁢(e,D)c⁢(e,D)+b+b×|D|a⁢v⁢d⁢l
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (4)
           </span>
          </td>
         </tr>
        </table>
        <p class="ltx_p">
         where
         b
         is a parameter control the weight of the document length normalization.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p4">
        <p class="ltx_p">
         With the unified concept weighting regularization, the revised function
based on F2-EXP function, i.e.,
         Unified
         , is shown as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p5">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="S6.EGx3">
         <tr class="ltx_equation ltx_align_baseline" id="S4.E5">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           S⁢(Q,D)=∑e∈Q∩Dcm⁢o⁢d⁢(e,Q)⋅(Nd⁢f⁢(t))0.35⋅cm⁢o⁢d⁢(e,D)cm⁢o⁢d⁢(e,D)+b+b×|D|a⁢v⁢d⁢l
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="1">
           <span class="ltx_tag ltx_tag_equation">
            (5)
           </span>
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S4.SS4.p6">
        <p class="ltx_p">
         where
         cm⁢o⁢d⁢(e,D)
         and
         cm⁢o⁢d⁢(e,Q)
         denote the modified occurrences
as shown in Equation (
         1
         ). It can be shown that this function
satisfies the unified constraint but violates the balanced constraint.
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p7">
        <p class="ltx_p">
         Following the similar strategy used in the previous study
         [28]
         ,
we can further incorporate the regularization function proposed in
Equation (
         3
         ) to the above function to make it satisfy
the balanced constraint as follows:
        </p>
       </div>
       <div class="ltx_para" id="S4.SS4.p8">
        <table class="ltx_equationgroup ltx_eqn_eqnarray" id="S6.EGx4">
         <tr class="ltx_equation ltx_align_baseline" id="S4.Ex1">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
           S⁢(Q,D)
          </td>
          <td class="ltx_td ltx_align_center">
           =
          </td>
          <td class="ltx_td ltx_align_left">
           ∑e∈Q∩Dcm⁢o⁢d⁢(e,Q)⋅(Nd⁢f⁢(t))0.35⋅f⁢(e,Q)
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
          <td class="ltx_align_middle ltx_align_right" rowspan="2">
           <span class="ltx_tag ltx_tag_equation">
            (6)
           </span>
          </td>
         </tr>
         <tr class="ltx_align_baseline">
          <td class="ltx_eqn_center_padleft">
          </td>
          <td class="ltx_td ltx_align_right">
          </td>
          <td class="ltx_td ltx_align_center">
          </td>
          <td class="ltx_td ltx_align_left">
           ⋅cm⁢o⁢d⁢(e,D)cm⁢o⁢d⁢(e,D)+b+b×|D|a⁢v⁢d⁢l
          </td>
          <td class="ltx_eqn_center_padright">
          </td>
         </tr>
        </table>
       </div>
       <div class="ltx_para" id="S4.SS4.p9">
        <p class="ltx_p">
         where
         f⁢(e,Q)
         is the newly proposed regularization function as shown in Equation (
         3
         ).
This method is denoted as
         Balanced
         , and can be shown that it satisfies both constraints.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S5">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        5
       </span>
       Experiments
      </h2>
      <div class="ltx_subsection" id="S5.SS1">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.1
        </span>
        Experiment Setup
       </h3>
       <div class="ltx_para" id="S5.SS1.p1">
        <p class="ltx_p">
         We conduct experiments using two data sets from the TREC
Medical Records track 2011 and 2012. The data sets are denoted as
         Med11
         and
         Med12
         . Both data sets used the same document collection
with 100,866 medical records, each of which is associated with a unique
patient visit to the hospital or emergency department. Since the task is
to retrieve relevant visits, we merged all the records from a visit to form
a single document for the visit, which leads to 17,198 documents in the collection.
There are 34 queries in
         Med11
         and 47 in
         Med12
         .
These queries were developed by domain experts based on the “inclusion
criteria” of a clinical study
         [25, 24]
         .
        </p>
       </div>
       <div class="ltx_para" id="S5.SS1.p2">
        <p class="ltx_p">
         After applying MetaMap to both documents and queries, we can construct a concept-based
collection.
Since documents are often much longer, we can first segment them into sentences,
get the mapping results for each sentence, and then merge
them together to generate the concept-based representation for
the documents.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS1.p3">
        <p class="ltx_p">
         Table
         1
         compares the statistics of the term-based and the concept-based collections,
including the number of unique tokens in the collection (i.e., the number of terms for term-based representation
and the number of concepts for concept-based representation), the average number of tokens in the documents (AvgDL)
and the average number of tokens in the queries for these two collections (AvgQL11 and
AvgQL12).
It is interesting to see that the number of unique tokens is much smaller when using the concept-based indexing.
This is expected since terms are semantically related and a group of related terms would be
mapped to one semantic concept. Moreover, we observe that the document length
and query length are similar for both collections. This is caused by the
fact that concepts are related and the MetaMap would map an aspect
to multiple related concepts.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS1.p4">
        <p class="ltx_p">
         Table
         2
         summarizes the methods that we compare in
the experiments.
Following the evaluation methodology used in the medical record track,
we use MAP@1000 as the primary measure for
         Med11
         and also report bpref. For
         Med12
         , we take infNDCG@100 as the primary
measure and also report infAP@100. Different measures were
chosen for these two sets mainly because
different pooling strategies were used to create the judgment pools
         [24]
         .
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S5.SS2">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.2
        </span>
        Performance Comparison
       </h3>
       <div class="ltx_para" id="S5.SS2.p1">
        <p class="ltx_p">
         Table
         3
         shows the performance under optimized parameter settings
for all the methods over both data sets. The performance is optimized in terms
of MAP in
         Med11
         , and infNDCG in
         Med12
         , respectively.
         α
         and
         b
         are tuned from 0 to 1 with the step 0.1.
Note that
         T
         ,
         C
         and
         T⁢S
         indicate improvement over Term-BL, Concept-BL
and TSConcept-BL is statistically significant at 0.05 level based on Wilcoxon
signed-rank test, respectively.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS2.p2">
        <p class="ltx_p">
         Results show that
         Balanced
         method can significantly improve the
retrieval performance over both collections.
         Unified
         method
outperforms the baseline methods in terms of the primary measure on both
collections, although it fails to improve the infAP on
         Med12
         for
one baseline method. It is not surprising to see that
         Balanced
         method
is more effective than
         Unified
         since the former satisfies both of the
proposed retrieval constraints while the latter satisfies only one.
Finally, we noticed that the performance difference between
TSConcept-BL and Concept-BL is not as significant as the ones reported in the
previous study
         [12]
         , which is probably caused by
the difference of problem set up (i.e., record-level vs. visit-level as discussed
in Section
         3.1
         ).
        </p>
       </div>
       <div class="ltx_para" id="S5.SS2.p3">
        <p class="ltx_p">
         We also conduct experiments to train parameters on one collection and compare
the testing performance on the other collection. The results are summarized
in Table
         4
         . Clearly,
         Balanced
         is still the most
effective regularization method. The testing performance is very close
to the optimal performance, which indicates that the proposed methods are robust
with respect to the parameter setting.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS2.p4">
        <p class="ltx_p">
         Moreover, we would like to point out that the testing performance of
         Balanced
         is comparable to the top ranked runs from the TREC Medical records track.
For example, the performance of the best automatic system in
         Med11
         (e.g., CengageM11R3) is 0.552 in terms of bpref, while the performance of the best
automatic system in
         Med12
         (e.g., udelSUM) is 0.578 in terms
of infNDCG. Note that the top system of
         Med12
         used multiple external resources such
as Wikipedia and Web, while we did not use such resources.
Moreover, our performance might be
further improved if we apply the result filtering methods used by many TREC participants
         [11]
         .
        </p>
       </div>
      </div>
      <div class="ltx_subsection" id="S5.SS3">
       <h3 class="ltx_title ltx_title_subsection">
        <span class="ltx_tag ltx_tag_subsection">
         5.3
        </span>
        More Analysis
       </h3>
       <div class="ltx_para" id="S5.SS3.p1">
        <p class="ltx_p">
         In the
         Unified
         method, we chose the concept with the maximum IDF as the representative
concept among all the variants. We now conduct experiments on
         Med11
         to compare its performance
with those of using average IDF and minimum IDF ones as the representative concept.
The results are shown in Table
         5
         . It is clear that using maximum IDF is
the best choice, which is consistent with our observation from the data exploratory
analysis shown in Figure
         2
         .
        </p>
       </div>
       <div class="ltx_para" id="S5.SS3.p2">
        <p class="ltx_p">
         In the
         Balanced
         method, we used the maximum IDF value to estimate the query importance.
We also conduct experiments to compare its performance with those using the minimum and
average IDF values. Table
         6
         summarizes the results, and shows that using
the maximum IDF value performs better than the other choices.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS3.p3">
        <p class="ltx_p">
         As shown in Equation (
         3
         ), the
         Balanced
         method regularizes the
weights through two components: (1) normalized confidence score of each aspect,
i.e.,
         ∑e′∈Qi⁢(e′)∑e′′∈𝒜⁢(e)i⁢(e′′)
         ;
and (2) the importance of the query aspect, i.e.,
         I⁢m⁢pA⁢(𝒜⁢(e))
         .
To examine the effectiveness of each component, we conduct experiments using
the modified
         Balanced
         method with only one of the components.
The results are shown in Table
         7
         .
It is clear that both components are essential to improve the retrieval
performance.
        </p>
       </div>
       <div class="ltx_para" id="S5.SS3.p4">
        <p class="ltx_p">
         Finally, we report the performance improvement of the proposed methods
over the
         Concept-BL
         for each query in Figure
         3
         .
Clearly, both of the proposed methods can improve the effectiveness of
most queries, and the
         Balanced
         method is more robust than
the
         Unified
         method.
        </p>
       </div>
      </div>
     </div>
     <div class="ltx_section" id="S6">
      <h2 class="ltx_title ltx_title_section">
       <span class="ltx_tag ltx_tag_section">
        6
       </span>
       Conclusions and Future Work
      </h2>
      <div class="ltx_para" id="S6.p1">
       <p class="ltx_p">
        Medical record retrieval is an important domain-specific IR problem.
Concept-based representation is an effective approach to dealing with
ambiguity terminology in medical domain. However, the results of the
NLP tools used to generate the concept-based representation are often
not perfect. In this paper, we present a general methodology that
can use axiomatic approaches as guidance to regularize the concept
weighting strategies to address the limitations caused by the inaccurate
concept mapping and improve the retrieval performance.
In particular, we proposed two weighting regularization methods based
on the relations among concepts. Experimental results show that the proposed
methods can significantly outperform existing retrieval functions.
       </p>
      </div>
      <div class="ltx_para" id="S6.p2">
       <p class="ltx_p">
        There are many interesting directions for our future work.
First, we plan to study how to automatically predict whether to use concept-based
indexing based on the quality of MetaMap results, and explore whether
the proposed methods are applicable for other entity linking methods.
Second, we will study how to leverage other information from knowledge bases to
further improve the performance.
Third, more experiments could be conducted to examine the effectiveness of the
proposed methods when using other ranking strategies.
Finally, it would be interesting to study how to
follow the proposed methodology to study other domain-specific IR problems.
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </body>
</html>
