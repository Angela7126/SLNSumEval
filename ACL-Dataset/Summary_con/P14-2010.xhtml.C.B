<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:204" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper we propose a novel algorithm that classifies documents based on class labels over few topics.
  </a>
  <a href="#1" id="1">
   This reduces the need to label a large collection of documents.
  </a>
  <a href="#2" id="2">
   We have used the idea of sprinkling originally proposed in the context of supervised Latent Semantic Analysis, but the setting here is quite different.
  </a>
  <a href="#3" id="3">
   Unlike the work in (Chakraborti et al., 2007), we do not assume that we have class labels over the set of training documents.
  </a>
  <a href="#4" id="4">
   Instead, to realize our goal of reducing knowledge acquisition overhead, we propose a way of propagating knowledge of few topic labels to the words and inducing a new topic distribution that has its topics more closely aligned to the class labels.
  </a>
  <a href="#5" id="5">
   The results show that the approach can yield performance comparable to entirely supervised settings.
  </a>
  <a href="#6" id="6">
   In future work, we also envision the possibility of sprinkling knowledge from background knowledge sources like Wikipedia (Gabrilovich and Markovitch, 2007) to realize an alignment of topics to Wikipedia concepts.
  </a>
  <a href="#7" id="7">
   We would like to study effect of change in number of topics on the text classification performance.
  </a>
  <a href="#8" id="8">
   We will also explore techniques which will help annotators to encode their domain knowledge efficiently when the topics are not well aligned to the class labels.
  </a>
 </body>
</html>