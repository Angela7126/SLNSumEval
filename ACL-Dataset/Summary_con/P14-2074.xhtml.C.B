<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:184" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper we performed a sentence-level correlation analysis of automatic evaluation measures against expert human judgements for the automatic image description task.
  </a>
  <a href="#1" id="1">
   We found that sentence-level unigram bleu is only weakly correlated with human judgements, even though it has extensively reported in the literature for this task.
  </a>
  <a href="#2" id="2">
   Meteor was found to have the highest correlation with human judgements, but it requires Wordnet and paraphrase resources that are not available for all languages.
  </a>
  <a href="#3" id="3">
   Our findings held when judgements were made on human-written or computer-generated descriptions.
  </a>
  <a href="#4" id="4">
   The variability in what and how people describe images will cause problems for all of the measures compared in this paper.
  </a>
  <a href="#5" id="5">
   Nevertheless, we propose that unigram bleu should no longer be used as an objective function for automatic image description because it has a weak correlation with human accuracy judgements.
  </a>
  <a href="#6" id="6">
   We recommend adopting either Meteor, Smoothed bleu , or rouge-su4 because they show stronger correlations with human judgements.
  </a>
  <a href="#7" id="7">
   We believe these suggestions are also applicable to the ranking tasks proposed in Hodosh et al.
  </a>
  <a href="#8" id="8">
   2013 ) , where automatic evaluation scores could act as features to a ranking function.
  </a>
 </body>
</html>