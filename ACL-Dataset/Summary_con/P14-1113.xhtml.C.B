<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:169" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   This paper proposes a novel method for semantic hierarchy construction based on word embeddings, which are trained using a large-scale corpus.
  </a>
  <a href="#1" id="1">
   Using the word embeddings, we learn the hypernym hyponym relationship by estimating projection matrices which map words to their hypernyms.
  </a>
  <a href="#2" id="2">
   Further improvements are made using a cluster-based approach in order to model the more fine-grained relations.
  </a>
  <a href="#3" id="3">
   Then we propose a few simple criteria to identity whether a new word pair is a hypernym hyponym relation.
  </a>
  <a href="#4" id="4">
   Based on the pairwise hypernym hyponym relations, we build semantic hierarchies automatically.
  </a>
  <a href="#5" id="5">
   In our experiments, the proposed method significantly outperforms state-of-the-art methods and achieves the best F1-score of 73.74% on a manually labeled test dataset.
  </a>
  <a href="#6" id="6">
   Further experiments show that our method is complementary to the previous manually-built hierarchy extension methods.
  </a>
  <a href="#7" id="7">
   For future work, we aim to improve word embedding learning under the guidance of hypernym hyponym relations.
  </a>
  <a href="#8" id="8">
   By including the hypernym hyponym relation constraints while training word embeddings, we expect to improve the embeddings such that they become more suitable for this task.
  </a>
 </body>
</html>