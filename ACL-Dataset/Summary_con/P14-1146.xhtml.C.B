<html>
 <head>
  <meta content="SENT_NUM:7, WORD_NUM:155" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper, we propose learning continuous word representations as features for Twitter sentiment classification under a supervised learning framework.
  </a>
  <a href="#1" id="1">
   We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.
  </a>
  <a href="#2" id="2">
   These methods typically only model the context information of words so that they cannot distinguish words with similar context but opposite sentiment polarity (e.g., good and bad.
  </a>
  <a href="#3" id="3">
   We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks.
  </a>
  <a href="#4" id="4">
   We train SSWE with massive distant-supervised tweets selected by positive and negative emoticons.
  </a>
  <a href="#5" id="5">
   The effectiveness of SSWE has been implicitly evaluated by using it as features in sentiment classification on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similarity in the embedding space for sentiment lexicons.
  </a>
  <a href="#6" id="6">
   Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.
  </a>
 </body>
</html>