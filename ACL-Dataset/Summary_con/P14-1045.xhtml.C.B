<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:275" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We proposed a method to reliably evaluate distributional semantic similarity in a broad sense by considering the validation of lexical pairs in contexts where they both appear.
  </a>
  <a href="#1" id="1">
   This helps cover non classical semantic relations which are hard to evaluate with classical resources.
  </a>
  <a href="#2" id="2">
   We also presented a supervised learning model which combines global features from the corpus used to built a distributional thesaurus and local features from the text where similarities are to be judged as relevant or not to the coherence of a document.
  </a>
  <a href="#3" id="3">
   It seems from these experiments that the quality of distributional relations depends on the contextualizing of the related lexical items, beyond just the similarity score and the ranks of items as neighbours of other items.
  </a>
  <a href="#4" id="4">
   This can hopefully help filter out lexical pairs when word lexical similarity is used as an information source where context is important lexical disambiguation [] , topic segmentation [].
  </a>
  <a href="#5" id="5">
   This can also be a preprocessing step when looking for similarities at higher levels, for instance at the sentence level [] or other macro-textual level [] , since these are always aggregation functions of word similarities.
  </a>
  <a href="#6" id="6">
   There are limits to what is presented here we need to evaluate the importance of the level of noise in the distributional neighbours database, or at least the quantity of non-semantic relations present, and this depends on the way the database is built.
  </a>
  <a href="#7" id="7">
   Our starting corpus is relatively small compared to current efforts in this framework.
  </a>
  <a href="#8" id="8">
   We are confident that the same methodology can be followed, even though the quantitative results may vary, since it is independent of the particular distributional thesaurus we used, and the way the similarities are computed.
  </a>
 </body>
</html>