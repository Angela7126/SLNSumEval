<html>
 <head>
  <meta content="SENT_NUM:7, WORD_NUM:140" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   We have presented a new learning objective for neural language models that incorporates prior knowledge contained in resources to improve learned word embeddings.
  </a>
  <a href="#1" id="1">
   We demonstrated that the Relation Constrained Model can lead to better semantic embeddings by incorporating resources like PPDB, leading to better language modeling, semantic similarity metrics, and predicting human semantic judgements.
  </a>
  <a href="#2" id="2">
   Our implementation is based on the word2vec package and we made it available for general use 2 2 https://github.com/Gorov/JointRCM.
  </a>
  <a href="#3" id="3">
   We believe that our techniques have implications beyond those considered in this work.
  </a>
  <a href="#4" id="4">
   We plan to explore the embeddings suitability for other semantics tasks, including the use of resources with both typed and scored relations.
  </a>
  <a href="#5" id="5">
   Additionally, we see opportunities for jointly learning embeddings across many tasks with many resources, and plan to extend our model accordingly.
  </a>
  <a href="#6" id="6">
   Yu is supported by China Scholarship Council and by NSFC 61173073.
  </a>
 </body>
</html>