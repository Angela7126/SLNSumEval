<html>
 <head>
  <meta content="SENT_NUM:10, WORD_NUM:195" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   This paper has explored the bilingually-constrained recursive auto-encoders in learning phrase embeddings, which can distinguish phrases with different semantic meanings.
  </a>
  <a href="#1" id="1">
   With the objective to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-translation pairs simultaneously, the learned model can semantically embed any phrase in two languages and can transform the semantic space in one language to the other.
  </a>
  <a href="#2" id="2">
   Two end-to-end SMT tasks are involved to test the power of the proposed model at learning the semantic phrase embeddings.
  </a>
  <a href="#3" id="3">
   The experimental results show that the BRAE model is remarkably effective in phrase table pruning and decoding with phrasal semantic similarities.
  </a>
  <a href="#4" id="4">
   We have also discussed many other potential applications and extensions of our BRAE model.
  </a>
  <a href="#5" id="5">
   In the future work, we will explore four directions.
  </a>
  <a href="#6" id="6">
   1) we will try to model the decoding process with DNN based on our semantic embeddings of the basic translation units.
  </a>
  <a href="#7" id="7">
   2) we are going to learn semantic phrase embeddings with the paraphrase corpus.
  </a>
  <a href="#8" id="8">
   3) we will apply the BRAE model in other monolingual and cross-lingual tasks.
  </a>
  <a href="#9" id="9">
   4) we plan to learn semantic sentence embeddings by automatically learning different weight matrices for different nodes in the BRAE model.
  </a>
 </body>
</html>