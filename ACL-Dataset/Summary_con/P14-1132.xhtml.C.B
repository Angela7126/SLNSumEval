<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:283" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   At the outset of this work, we considered the problem of linking purely language-based distributional semantic spaces with objects in the visual world by means of cross-modal mapping.
  </a>
  <a href="#1" id="1">
   We compared recent models for this task both on a benchmark object recognition dataset and on a more realistic and noisier dataset covering a wide range of concepts.
  </a>
  <a href="#2" id="2">
   The neural network architecture emerged as the best performing approach, and our qualitative analysis revealed that it induced a categorical organization of concepts.
  </a>
  <a href="#3" id="3">
   Most importantly, our results suggest the viability of cross-modal mapping for grounded word-meaning acquisition in a simulation of fast mapping.
  </a>
  <a href="#4" id="4">
   Given the success of NN , we plan to experiment in the future with more sophisticated neural network architectures inspired by recent work in machine translation [ 19 ] and multimodal deep learning [ 51 ].
  </a>
  <a href="#5" id="5">
   Furthermore, we intend to adopt visual attributes [ 14 , 44 ] as visual representations, since they should allow a better understanding of how cross-modal mapping works, thanks to their linguistic interpretability.
  </a>
  <a href="#6" id="6">
   The error analysis in Section 5.3 suggests that automated localization techniques [ 54 ] , distinguishing an object from its surroundings, might drastically improve mapping accuracy.
  </a>
  <a href="#7" id="7">
   Similarly, in the textual domain, models that extract collocates of a word that are more likely to denote conceptual properties [ 28 ] might lead to more informative and discriminative linguistic vectors.
  </a>
  <a href="#8" id="8">
   Finally, the lack of large child-directed speech corpora constrained the experimental design of fast mapping simulations; we plan to run more realistic experiments with true nonce words and using source corpora (e.g.,, the Simple Wikipedia, child stories, portions of CHILDES) that contain sentences more akin to those a child might effectively hear or read in her word-learning years.
  </a>
 </body>
</html>