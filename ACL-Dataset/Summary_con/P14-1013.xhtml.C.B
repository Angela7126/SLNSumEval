<html>
 <head>
  <meta content="SENT_NUM:9, WORD_NUM:184" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper, we propose a neural network based approach to learning bilingual topic representation for SMT.
  </a>
  <a href="#1" id="1">
   We enrich contexts of parallel sentence pairs with topic related monolingual data and obtain a set of documents to represent sentences.
  </a>
  <a href="#2" id="2">
   These documents are converted to a bag-of-words input and fed into neural networks.
  </a>
  <a href="#3" id="3">
   The learned low-dimensional vector is used to obtain the topic representations of synchronous rules.
  </a>
  <a href="#4" id="4">
   In SMT decoding, appropriate rules are selected to best match source texts according to their similarity in the topic space.
  </a>
  <a href="#5" id="5">
   Experimental results show that our approach is promising for SMT systems to learn a better translation model.
  </a>
  <a href="#6" id="6">
   It is a significant improvement over the state-of-the-art Hiero system, as well as a conventional LDA-based method.
  </a>
  <a href="#7" id="7">
   In the future research, we will extend our neural network methods to address document-level translation, where topic transition between sentences is a crucial problem to be solved.
  </a>
  <a href="#8" id="8">
   Since the translation of the current sentence is usually influenced by the topic of previous sentences, we plan to leverage recurrent neural networks to model this phenomenon, where the history translation information is naturally combined in the model.
  </a>
 </body>
</html>