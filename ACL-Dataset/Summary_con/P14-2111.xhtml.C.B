<html>
 <head>
  <meta content="SENT_NUM:6, WORD_NUM:124" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Learning sequences of edit operations from examples while incorporating unlabeled data via neural text embeddings constitutes a compelling approach to tweet normalization.
  </a>
  <a href="#1" id="1">
   Our results are especially interesting considering that we trained on only a small annotated data set and did not use any other manually created resources such as dictionaries.
  </a>
  <a href="#2" id="2">
   We want to push performance further by expanding the training data and incorporating existing lexical resources.
  </a>
  <a href="#3" id="3">
   It will also be important to check how our method generalizes to other language and datasets (e.g., 5 ; 1 ).
  </a>
  <a href="#4" id="4">
   The general form of our model can be used in settings where normalization is not limited to word-to-word transformations.
  </a>
  <a href="#5" id="5">
   We are planning to find or create data with such characteristics and evaluate our approach under these conditions.
  </a>
 </body>
</html>