<html>
 <head>
  <meta content="SENT_NUM:8, WORD_NUM:173" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   This paper is devoted to large-scale Chinese UWS for SMT.
  </a>
  <a href="#1" id="1">
   An efficient unified monolingual and bilingual UWS method is proposed and applied to large-scale bilingual corpora.
  </a>
  <a href="#2" id="2">
   Complexity analysis shows that our method is capable of scaling to large-scale corpora.
  </a>
  <a href="#3" id="3">
   This was verified by experiments on a corpus of 1-million sentence pairs on which traditional MCMC approaches would struggle [].
  </a>
  <a href="#4" id="4">
   The proposed method does not require any annotated data, but the SMT system with it can achieve comparable performance compared to state-of-the-art supervised word segmenters trained on precious annotated data.
  </a>
  <a href="#5" id="5">
   Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus.
  </a>
  <a href="#6" id="6">
   Thus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained.
  </a>
  <a href="#7" id="7">
   In future research, we plan to improve the bilingual UWS through applying VB and integrating more accurate alignment models such as HMM models and IBM model 4.
  </a>
 </body>
</html>