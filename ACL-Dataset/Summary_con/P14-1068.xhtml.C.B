<html>
 <head>
  <meta content="SENT_NUM:8, WORD_NUM:181" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining textual and visual modalities.
  </a>
  <a href="#1" id="1">
   The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data.
  </a>
  <a href="#2" id="2">
   To the best of our knowledge, our model is novel in its use of attribute-based input in a deep neural network.
  </a>
  <a href="#3" id="3">
   Experimental results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and related models trained on the same attribute-based input.
  </a>
  <a href="#4" id="4">
   Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input.
  </a>
  <a href="#5" id="5">
   In the future, we would like to apply our model to other tasks, such as image and text retrieval () , zero-shot learning () , and word learning ().
  </a>
  <a href="#6" id="6">
   We would like to thank Vittorio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valuable feedback.
  </a>
  <a href="#7" id="7">
   We acknowledge the support of EPSRC through project grant EP/I037415/1.
  </a>
 </body>
</html>