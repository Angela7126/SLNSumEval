<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</title>
<!--Generated on Tue Jun 10 19:13:24 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaodong Zeng<math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math>   Lidia S. Chao<math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math>   Derek F. Wong<math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math>   Isabel Trancoso<math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="{}^{\ddagger}" display="inline"><msup><mi/><mo>‡</mo></msup></math>   Liang Tian<math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math>
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math>NLP<math xmlns="http://www.w3.org/1998/Math/MathML" id="m7" class="ltx_Math" alttext="{}^{2}" display="inline"><msup><mi/><mn>2</mn></msup></math>CT Lab / Department of Computer and Information Science, University of Macau 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m8" class="ltx_Math" alttext="{}^{\ddagger}" display="inline"><msup><mi/><mo>‡</mo></msup></math>INESC-ID / Instituto Superior Ténico, Lisboa, Portugal
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">nlp2ct.samuel@gmail.com, </span>{<span class="ltx_text ltx_font_typewriter">lidiasc, derekfw</span>}<span class="ltx_text ltx_font_typewriter">@umac.mo,</span> 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com
<br class="ltx_break"/></span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">This study investigates on building a better Chinese word segmentation model for statistical machine translation. It aims at leveraging word boundary information, automatically learned by bilingual character-based alignments, to induce a preferable segmentation model. We propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised CRFs model, trained by the treebank data (labeled), on the bilingual data (unlabeled). The induced word boundary information is encoded as a graph propagation constraint. The constrained model induction is accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) <cite class="ltx_cite">[<a href="#bib.bib28" title="Integrated Chinese word segmentation in statistical machine translation" class="ltx_ref">23</a>, <a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>, <a href="#bib.bib16" title="An empirical study on word segmentation for Chinese machine translation" class="ltx_ref">31</a>]</cite>. In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” <cite class="ltx_cite">[<a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>]</cite>. The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the hand-annotated treebank data, e.g., Chinese treebank (CTB) <cite class="ltx_cite">[<a href="#bib.bib6" title="The Penn Chinese TreeBank: phrase structure annotation of a large corpus" class="ltx_ref">25</a>]</cite>. These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency <cite class="ltx_cite">[<a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>]</cite>. But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In recent years, a number of works <cite class="ltx_cite">[<a href="#bib.bib28" title="Integrated Chinese word segmentation in statistical machine translation" class="ltx_ref">23</a>, <a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>, <a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>, <a href="#bib.bib18" title="Enhancing statistical machine translation with character alignment" class="ltx_ref">22</a>]</cite> attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation <cite class="ltx_cite">[<a href="#bib.bib19" title="Do we need Chinese word segmentation for statistical machine translation?" class="ltx_ref">24</a>]</cite>, or form labeled data for training a sequence labeling model <cite class="ltx_cite">[<a href="#bib.bib20" title="Integration of multiple bilingually-trained segmentation schemes into statistical machine translation" class="ltx_ref">18</a>]</cite>. The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment <cite class="ltx_cite">[<a href="#bib.bib21" title="A systematic comparison of various statistical alignment models" class="ltx_ref">15</a>]</cite>. However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmentation nature. We propose leveraging the bilingual knowledge to form learning constraints that guide a supervised segmentation model toward a better solution for SMT. Besides the bilingual motivated models, character-based alignment is also employed to achieve the mappings of the successive Chinese characters and the target language words. Instead of directly merging the characters into concrete segmentations, this work attempts to extract word boundary distributions for character-level trigrams (types) from the “chars-to-word” mappings. Furthermore, these word boundaries are encoded into a graph propagation (GP) expression, in order to widen the influence of the induced bilingual knowledge among Chinese texts. The GP expression constrains similar types having approximated word boundary distributions. Crucially, the GP expression with the bilingual knowledge is then used as side information to regularize a CRFs (conditional random fields) model’s learning over treebank and bitext data, based on the posterior regularization (PR) framework <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite>. This constrained learning amounts to a jointly coupling of GP and CRFs, i.e., integrating GP into the estimation of a parametric structural model.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">This paper is structured as follows: Section 2 points out the main differences with the related works of this study. Section 3 presents the details of the proposed segmentation model. Section 4 reports the experimental results of the proposed model for a Chinese-to-English MT task. The conclusion is drawn in Section 5.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In the literature, many approaches have been proposed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. Chang et al. <cite class="ltx_cite">[<a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>]</cite> enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. <cite class="ltx_cite">[<a href="#bib.bib23" title="Improved statistical machine translation by multiple Chinese word segmentation" class="ltx_ref">29</a>]</cite> produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. <cite class="ltx_cite">[<a href="#bib.bib19" title="Do we need Chinese word segmentation for statistical machine translation?" class="ltx_ref">24</a>]</cite> proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum matching segmentation in SMT task. The works in <cite class="ltx_cite">[<a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>, <a href="#bib.bib16" title="An empirical study on word segmentation for Chinese machine translation" class="ltx_ref">31</a>]</cite> extended the dictionary extraction strategy. Ma and Way <cite class="ltx_cite">[<a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>]</cite> adopted co-occurrence frequency metric to iteratively optimize “candidate words” extract from the alignments. Zhao et al. <cite class="ltx_cite">[<a href="#bib.bib16" title="An empirical study on word segmentation for Chinese machine translation" class="ltx_ref">31</a>]</cite> attempted to find an optimal subset of the dictionary learned by the character-based alignment to maximize the MT performance. Paul et al. <cite class="ltx_cite">[<a href="#bib.bib20" title="Integration of multiple bilingually-trained segmentation schemes into statistical machine translation" class="ltx_ref">18</a>]</cite> used the words learned from “chars-to-word” alignments to train a maximum entropy segmentation model. Rather than playing the “hard” uses of the bilingual segmentation knowledge, i.e., directly merging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. <cite class="ltx_cite">[<a href="#bib.bib5" title="Efficient graph-based semi-supervised learning of structured tagging models" class="ltx_ref">20</a>]</cite>, Das and Petrov <cite class="ltx_cite">[<a href="#bib.bib8" title="Unsupervised part-of-speech tagging with bilingual graph-based projections." class="ltx_ref">6</a>]</cite>, Zeng et al. <cite class="ltx_cite">[<a href="#bib.bib24" title="Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging" class="ltx_ref">27</a>, <a href="#bib.bib25" title="Lexicon expansion for latent variable grammars" class="ltx_ref">26</a>]</cite> and Zhu et al. <cite class="ltx_cite">[<a href="#bib.bib41" title="Unsupervised chunking based on graph propagation from bilingual corpus" class="ltx_ref">32</a>]</cite>, proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">One of our main objectives is to bias CRFs model’s learning on unlabeled data, under a non-linear GP constraint encoding the bilingual knowledge. This is accomplished by the posterior regularization (PR) framework <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite>. PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. <cite class="ltx_cite">[<a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>]</cite> described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum <cite class="ltx_cite">[<a href="#bib.bib26" title="Generalized expectation criteria for semi-supervised learning of conditional random fields" class="ltx_ref">13</a>]</cite> and McCallum et al. <cite class="ltx_cite">[<a href="#bib.bib27" title="Generalized expectation criteria" class="ltx_ref">14</a>]</cite> proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented Chinese sentences from treebank <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="\mathcal{D}_{l}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup></math> and parallel unsegmented sentences of Chinese and foreign language <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="\mathcal{D}_{u}^{f}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>f</mi></msubsup></math>. The first step is to conduct character-based alignment over bitexts <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m4" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m5" class="ltx_Math" alttext="\mathcal{D}_{u}^{f}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>f</mi></msubsup></math>, where every Chinese character is an alignment target. Here, we are interested on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-to-1 alignment patterns, i.e., one target word is aligned to one or more source Chinese characters. The second step aims to collect word boundary distributions for all types, i.e., character-level trigrams, according to the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m7" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-to-1 mappings (Section 3.1). The third step is to encode the induced word boundary information into a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m8" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-nearest-neighbors (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m9" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-NN) similarity graph constructed over the entire set of types from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m10" class="ltx_Math" alttext="\mathcal{D}_{l}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m11" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math> (Section 3.2). The final step trains a discriminative sequential labeling model, conditional random fields, on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m12" class="ltx_Math" alttext="\mathcal{D}_{l}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m13" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math> under bilingual constraints in a graph propagation expression (Section 3.3). This constrained learning is carried out based on posterior regularization (PR) framework <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite>.
<span class="ltx_ERROR undefined">{algorithm}</span>
<span class="ltx_text ltx_caption">CWS model induction with bilingual constraints</span>

<span class="ltx_ERROR undefined">{algorithmic}</span>[1]
<span class="ltx_ERROR undefined">\REQUIRE</span>  
<br class="ltx_break"/>Segmented Chinese sentences from treebank <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m14" class="ltx_Math" alttext="\mathcal{D}_{l}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup></math>; Parallel sentences of Chinese and foreign language <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m15" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m16" class="ltx_Math" alttext="\mathcal{D}_{u}^{f}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>f</mi></msubsup></math>
<span class="ltx_ERROR undefined">\ENSURE</span>  
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m17" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>: the CRFs model parameters
<span class="ltx_ERROR undefined">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m18" class="ltx_Math" alttext="\mathcal{D}^{c\leftrightarrow f}\leftarrow" display="inline"><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>c</mi><mo>↔</mo><mi>f</mi></mrow></msup><mo>←</mo><mi/></mrow></math> <span class="ltx_text ltx_font_sansserif ltx_font_bold ltx_font_small">char_align_bitext</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m19" class="ltx_Math" alttext="(\mathcal{D}^{c}_{u},\mathcal{D}^{f}_{u})" display="inline"><mrow><mo>(</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>f</mi></msubsup></mrow><mo>)</mo></mrow></math>
<span class="ltx_ERROR undefined">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m20" class="ltx_Math" alttext="r\leftarrow" display="inline"><mrow><mi>r</mi><mo>←</mo><mi/></mrow></math> <span class="ltx_text ltx_font_sansserif ltx_font_bold ltx_font_small">learn_word_bound</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m21" class="ltx_Math" alttext="(\mathcal{D}^{c\leftrightarrow f})" display="inline"><mrow><mo>(</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>c</mi><mo>↔</mo><mi>f</mi></mrow></msup><mo>)</mo></mrow></math>
<span class="ltx_ERROR undefined">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m22" class="ltx_Math" alttext="\mathcal{G}\leftarrow{\textbf{\small encode\_graph\_constraint}}~{}(\mathcal{D%&#10;}^{c}_{l},\mathcal{D}^{c}_{u},r)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒢</mi><mo>←</mo><mrow><mpadded width="+3.3pt"><mtext mathsize="small" mathvariant="bold" stretchy="false">encode_graph_constraint</mtext></mpadded><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup><mo>,</mo><mi>r</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m23" class="ltx_Math" alttext="\theta\leftarrow{\textbf{\small pr\_crf\_graph}}~{}(\mathcal{D}^{c}_{l},%&#10;\mathcal{D}^{c}_{u},\mathcal{G})" display="inline"><mrow><mi>θ</mi><mo>←</mo><mrow><mpadded width="+3.3pt"><mtext mathsize="small" mathvariant="bold" stretchy="false">pr_crf_graph</mtext></mpadded><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒢</mi></mrow><mo>)</mo></mrow></mrow></mrow></math></p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Word Boundaries Learned from Character-based Alignments</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in <cite class="ltx_cite">[<a href="#bib.bib19" title="Do we need Chinese word segmentation for statistical machine translation?" class="ltx_ref">24</a>, <a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>, <a href="#bib.bib29" title="Unsupervised tokenization for machine translation" class="ltx_ref">5</a>]</cite> to learn bilingual segmentation knowledge. This relies on statistical character-based alignment: first, every Chinese character in the bitexts is divided by a white space so that individual characters are regarded as special “words” or alignment targets, and second, they are connected with English words by using a statistical word aligner, e.g., GIZA++ <cite class="ltx_cite">[<a href="#bib.bib21" title="A systematic comparison of various statistical alignment models" class="ltx_ref">15</a>]</cite>. Note that the aligner is restricted to use an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-to-1 alignment pattern. The primary idea is that consecutive Chinese characters are grouped to a candidate word, if they are aligned to the same foreign word. It is worth mentioning that prior works presented a straightforward usage for candidate words, treating them as golden segmentations, either dictionary units or labeled resources. But this study treats the induced candidate words in a different way. We propose to extract the word boundary distributions<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>The distribution is on four word boundary labels indicating the character positions in a word, i.e., <span class="ltx_text ltx_font_bold">B</span> (begin), <span class="ltx_text ltx_font_bold">M</span> (middle), <span class="ltx_text ltx_font_bold">E</span> (end) and <span class="ltx_text ltx_font_bold">S</span> (single character).</span></span></span> for character-level trigrams (<span class="ltx_text ltx_font_italic">type</span>)<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>A word boundary distribution corresponds to the center character of a type. In fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters <cite class="ltx_cite">[<a href="#bib.bib30" title="Maximum margin semi-supervised learning for structured variables" class="ltx_ref">1</a>]</cite>.</span></span></span>, as shown in Figure 1, instead of the very specific words. There are two main reasons to do so. First, it is a more general expression which can reduce the impact amplification of erroneous character alignments. Second, boundary distributions can play more flexible roles as constraints over labelings to bias the model learning.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">The type-level word boundary extraction is formally described as follows. Given the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>th sentence pair <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m2" class="ltx_Math" alttext="\langle x_{i}^{c},x_{i}^{f},\mathcal{A}_{i}^{c\to f}\rangle" display="inline"><mrow><mo>⟨</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mi>c</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mi>f</mi></msubsup><mo>,</mo><msubsup><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>i</mi><mrow><mi>c</mi><mo>→</mo><mi>f</mi></mrow></msubsup></mrow><mo>⟩</mo></mrow></math> of the aligned bilingual corpus <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m3" class="ltx_Math" alttext="\mathcal{D}^{c\leftrightarrow f}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>c</mi><mo>↔</mo><mi>f</mi></mrow></msup></math>, the Chinese sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m4" class="ltx_Math" alttext="x_{i}^{c}" display="inline"><msubsup><mi>x</mi><mi>i</mi><mi>c</mi></msubsup></math> consisting of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m5" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> characters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m6" class="ltx_Math" alttext="\{x_{i,1}^{c},x_{i,2}^{c},...,x_{i,m}^{c}\}" display="inline"><mrow><mo>{</mo><mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow><mi>c</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow><mi>c</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>m</mi></mrow><mi>c</mi></msubsup></mrow><mo>}</mo></mrow></math>, and the foreign language sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m7" class="ltx_Math" alttext="x_{i}^{f}" display="inline"><msubsup><mi>x</mi><mi>i</mi><mi>f</mi></msubsup></math>, consisting of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m8" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> words <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m9" class="ltx_Math" alttext="\{x_{i,1}^{f},x_{i,2}^{f},...,x_{i,n}^{f}\}" display="inline"><mrow><mo>{</mo><mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow><mi>f</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow><mi>f</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>n</mi></mrow><mi>f</mi></msubsup></mrow><mo>}</mo></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m10" class="ltx_Math" alttext="\mathcal{A}_{i}^{c\to f}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>i</mi><mrow><mi>c</mi><mo>→</mo><mi>f</mi></mrow></msubsup></math> represents a set of alignment pairs <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m11" class="ltx_Math" alttext="a_{j}=\langle C_{j},x_{i,j}^{f}\rangle" display="inline"><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>=</mo><mrow><mo>⟨</mo><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>f</mi></msubsup></mrow><mo>⟩</mo></mrow></mrow></math> that defines connections between a few Chinese characters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m12" class="ltx_Math" alttext="C_{j}=\{x_{i,j_{1}}^{c},x_{i,j_{2}}^{c},...,x_{i,j_{k}}^{c}\}" display="inline"><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mn>1</mn></msub></mrow><mi>c</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mn>2</mn></msub></mrow><mi>c</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub></mrow><mi>c</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math> and a single foreign word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m13" class="ltx_Math" alttext="x_{i,j}^{f}" display="inline"><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>f</mi></msubsup></math>. For an alignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m14" class="ltx_Math" alttext="a_{j}=\langle C_{j},x_{i,j}^{f}\rangle" display="inline"><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>=</mo><mrow><mo>⟨</mo><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>f</mi></msubsup></mrow><mo>⟩</mo></mrow></mrow></math>, only the sequence of characters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m15" class="ltx_Math" alttext="C_{j}=\{x_{i,j_{1}}^{c},x_{i,j_{2}}^{c},...,x_{i,j_{k}}^{c}\}\ \forall d\in[1,%&#10;k-1],j_{d+1}-j_{d}=1" display="inline"><mrow><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><mrow><mrow><mo>{</mo><mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mn>1</mn></msub></mrow><mi>c</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mn>2</mn></msub></mrow><mi>c</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><msub><mi>j</mi><mi>k</mi></msub></mrow><mi>c</mi></msubsup></mrow><mo>}</mo></mrow><mo>⁢</mo><mrow><mo>∀</mo><mi>d</mi></mrow></mrow><mo>∈</mo><mrow><mo>[</mo><mrow><mn>1</mn><mo>,</mo><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></mrow><mo>]</mo></mrow></mrow><mo>,</mo><mrow><mrow><msub><mi>j</mi><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><msub><mi>j</mi><mi>d</mi></msub></mrow><mo>=</mo><mn>1</mn></mrow></mrow></math> constitutes a valid candidate word. For the whole bilingual corpus, we assign each character in the candidate words with a word boundary tag <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m16" class="ltx_Math" alttext="T\in\{B,M,E,S\}" display="inline"><mrow><mi>T</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>B</mi><mo>,</mo><mi>M</mi><mo>,</mo><mi>E</mi><mo>,</mo><mi>S</mi></mrow><mo>}</mo></mrow></mrow></math>, and then count across the entire corpus to collect the tag distributions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m17" class="ltx_Math" alttext="r_{i}=\{r_{i,t};t\in T\}" display="inline"><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>;</mo><mi>t</mi><mo>∈</mo><mi>T</mi><mo>}</mo></mrow></mrow></math> for each type <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m18" class="ltx_Math" alttext="x_{i,j-1}^{c}x_{i,j}^{c}x_{i,j+1}^{c}" display="inline"><mrow><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></mrow><mi>c</mi></msubsup><mo>⁢</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>c</mi></msubsup><mo>⁢</mo><msubsup><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>c</mi></msubsup></mrow></math>.</p>
</div>
<div id="S3.F1" class="ltx_figure"><img src="P14-1128/image001.png" id="S3.F1.g1" class="ltx_graphics" width="354" height="250" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of similarity graph over character-level trigrams (types).</div>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Constraints Encoded by Graph Propagation Expression</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">The previous step contributes to generate bilingual segmentation supervisions, i.e., type-level word boundary distributions. An intuitive manner is to directly leverage the induced boundary distributions as label constraints to regularize segmentation model learning, based on a constrained learning algorithm. This study, however, makes further efforts to elevate the positive effects of the bilingual knowledge via the graph propagation technique. We adopt a similarity graph to encode the learned type-level word boundary distributions. The GP expression will be defined as a PR constraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches <cite class="ltx_cite">[<a href="#bib.bib5" title="Efficient graph-based semi-supervised learning of structured tagging models" class="ltx_ref">20</a>, <a href="#bib.bib8" title="Unsupervised part-of-speech tagging with bilingual graph-based projections." class="ltx_ref">6</a>, <a href="#bib.bib24" title="Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging" class="ltx_ref">27</a>]</cite>, where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples <cite class="ltx_cite">[<a href="#bib.bib7" title="Graph-based lexicon expansion with sparsity-inducing penalties" class="ltx_ref">7</a>]</cite>, a similarity graph <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="\mathcal{G}=(V,E)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒢</mi><mo>=</mo><mrow><mo>(</mo><mrow><mi>V</mi><mo>,</mo><mi>E</mi></mrow><mo>)</mo></mrow></mrow></math> is constructed over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> types extracted from Chinese training data, including treebank <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m3" class="ltx_Math" alttext="\mathcal{D}_{l}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>l</mi><mi>c</mi></msubsup></math> and bitexts <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m4" class="ltx_Math" alttext="\mathcal{D}_{u}^{c}" display="inline"><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>u</mi><mi>c</mi></msubsup></math>. Each vertex <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m5" class="ltx_Math" alttext="V_{i}" display="inline"><msub><mi>V</mi><mi>i</mi></msub></math> has a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m6" class="ltx_Math" alttext="|T|" display="inline"><mrow><mo fence="true">|</mo><mi>T</mi><mo fence="true">|</mo></mrow></math>-dimensional estimated measure <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m7" class="ltx_Math" alttext="v_{i}=\{v_{i,t};t\in T\}" display="inline"><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>;</mo><mi>t</mi><mo>∈</mo><mi>T</mi><mo>}</mo></mrow></mrow></math> representing a probability distribution on word boundary tags. The induced type-level word boundary distributions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m8" class="ltx_Math" alttext="r_{i}=\{r_{i,t};t\in T\}" display="inline"><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>;</mo><mi>t</mi><mo>∈</mo><mi>T</mi><mo>}</mo></mrow></mrow></math> are empirical measures for the corresponding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m9" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> graph vertices. The edges <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m10" class="ltx_Math" alttext="E\in V_{i}\times V_{j}" display="inline"><mrow><mi>E</mi><mo>∈</mo><mrow><msub><mi>V</mi><mi>i</mi></msub><mo>×</mo><msub><mi>V</mi><mi>j</mi></msub></mrow></mrow></math> connect all the vertices. Scores between pairs of graph vertices (types), <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m11" class="ltx_Math" alttext="w_{ij}" display="inline"><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>, refer to the similarities of their syntactic environment, which are computed following the method in <cite class="ltx_cite">[<a href="#bib.bib5" title="Efficient graph-based semi-supervised learning of structured tagging models" class="ltx_ref">20</a>, <a href="#bib.bib8" title="Unsupervised part-of-speech tagging with bilingual graph-based projections." class="ltx_ref">6</a>, <a href="#bib.bib24" title="Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging" class="ltx_ref">27</a>]</cite>. The similarities are measured based on co-occurrence statistics over a set of predefined features (introduced in Section 4.1). Specifically, the point-wise mutual information (PMI) values, between vertices and each feature instantiation that they have in common, are summed to sparse vectors, and their cosine distances are computed as the similarities. The nature of this similarity graph enforces that the connected types with high weights appearing in different texts should have similar word boundary distributions.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">The quality (smoothness) of the similarity graph can be estimated by using a standard propagation function, as shown in Equation 1. The square-loss criterion <cite class="ltx_cite">[<a href="#bib.bib2" title="Semi-supervised learning using gaussian fields and harmonic functions" class="ltx_ref">33</a>, <a href="#bib.bib10" title="Label propagation and quadratic criterion" class="ltx_ref">3</a>]</cite> is used to formulate this function:</p>
<table id="S3.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle\mathcal{P}(v)=\sum_{t=1}^{T}\Bigg(\sum_{i=1}^{%&#10;M}(v_{i,t}-r_{i,t})^{2}\\&#10;\displaystyle+\mu\sum_{j=1}^{N}\sum_{i=1}^{N}w_{ij}(v_{i,t}-v_{j,t})^{2}+\rho%&#10;\sum_{i=1}^{N}(v_{i,t})^{2}\Bigg)\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mo>(</mo><mi>v</mi><mo>)</mo></mrow><mo>=</mo><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mrow><mo mathsize="2.5em" stretchy="false">(</mo><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msup><mrow><mo>(</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>-</mo><msub><mi>r</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>+</mo><mi>μ</mi><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><msup><mrow><mo>(</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>-</mo><msub><mi>v</mi><mrow><mi>j</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>)</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>ρ</mi><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mrow><mo>(</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub><mo>)</mo></mrow><mn>2</mn></msup><mo mathsize="2.5em" stretchy="false">)</mo></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">The first term in this equation refers to seed matches that compute the distances between the estimated measure <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m1" class="ltx_Math" alttext="v_{i}" display="inline"><msub><mi>v</mi><mi>i</mi></msub></math> and the empirical probabilities <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m2" class="ltx_Math" alttext="r_{i}" display="inline"><msub><mi>r</mi><mi>i</mi></msub></math>. The second term refers to edge smoothness that measures how vertices <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m3" class="ltx_Math" alttext="v_{i}" display="inline"><msub><mi>v</mi><mi>i</mi></msub></math> are smoothed with respect to the graph. Two types connected by an edge with high weight should be assigned similar word boundary distributions. The third term, a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m4" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></math> norm, evaluates the distribution sparsity <cite class="ltx_cite">[<a href="#bib.bib7" title="Graph-based lexicon expansion with sparsity-inducing penalties" class="ltx_ref">7</a>]</cite> per vertex. Typically, the GP process amounts to an optimization process with respect to parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m5" class="ltx_Math" alttext="v" display="inline"><mi>v</mi></math> such that Equation 1 is minimized. This propagation function can be used to reflect the graph smoothness, where the higher the score, the lower the smoothness.</p>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>PR Learning with GP Constraint</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">Our learning problem belongs to semi-supervised learning (SSL), as the training is done on treebank labeled data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m1" class="ltx_Math" alttext="(\mathrm{X}_{L},\mathrm{Y}_{L})=\{(\mathrm{x}_{1},\mathrm{y}_{1}),...,(\mathrm%&#10;{x}_{l},\mathrm{y}_{l})\}" display="inline"><mrow><mrow><mo>(</mo><mrow><msub><mi mathvariant="normal">X</mi><mi>L</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Y</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mrow><mo>(</mo><mrow><msub><mi mathvariant="normal">x</mi><mn>1</mn></msub><mo>,</mo><msub><mi mathvariant="normal">y</mi><mn>1</mn></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo>(</mo><mrow><msub><mi mathvariant="normal">x</mi><mi>l</mi></msub><mo>,</mo><msub><mi mathvariant="normal">y</mi><mi>l</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow></math>, and bilingual unlabeled data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m2" class="ltx_Math" alttext="(\mathrm{X}_{U})=\{\mathrm{x}_{1},...,\mathrm{x}_{u}\}" display="inline"><mrow><mrow><mo>(</mo><msub><mi mathvariant="normal">X</mi><mi>U</mi></msub><mo>)</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><msub><mi mathvariant="normal">x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi mathvariant="normal">x</mi><mi>u</mi></msub></mrow><mo>}</mo></mrow></mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m3" class="ltx_Math" alttext="\mathrm{x}_{i}=\{x^{1},...,x^{m}\}" display="inline"><mrow><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><msup><mi>x</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>x</mi><mi>m</mi></msup></mrow><mo>}</mo></mrow></mrow></math> is an input word sequence and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m4" class="ltx_Math" alttext="\mathrm{y}_{i}=\{y^{1},...,y^{m}\},y\in T" display="inline"><mrow><mrow><msub><mi mathvariant="normal">y</mi><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><msup><mi>y</mi><mn>1</mn></msup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>y</mi><mi>m</mi></msup></mrow><mo>}</mo></mrow></mrow><mo>,</mo><mrow><mi>y</mi><mo>∈</mo><mi>T</mi></mrow></mrow></math> is its corresponding label sequence. Supervised linear-chain CRFs can be modeled in a standard conditional log-likelihood objective with a Gaussian prior:</p>
<table id="S3.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle\mathcal{L}(\theta)=p_{\theta}(\mathrm{y}_{i}|%&#10;\mathrm{x}_{i})-\frac{\Arrowvert\theta\Arrowvert^{2}}{2\sigma}\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><mo>=</mo><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo>(</mo><msub><mi mathvariant="normal">y</mi><mi>i</mi></msub><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>-</mo><mfrac><msup><mrow><mo fence="true">∥</mo><mi>θ</mi><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mrow><mn>2</mn><mo>⁢</mo><mi>σ</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">The conditional probabilities <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m5" class="ltx_Math" alttext="p_{\theta}" display="inline"><msub><mi>p</mi><mi>θ</mi></msub></math> are expressed as a log-linear form:</p>
<table id="S3.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle p_{\theta}(\mathrm{y}_{i}|\mathrm{x}_{i})=%&#10;\frac{\displaystyle\mathrm{exp}(\sum_{k=1}^{m}\theta^{\mathrm{T}}f(y_{i}^{k-1}%&#10;,y_{i}^{k},\mathrm{x}_{i}))}{Z_{\theta}(\mathrm{x}_{i})}\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo>(</mo><msub><mi mathvariant="normal">y</mi><mi>i</mi></msub><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></mstyle><mrow><msup><mi>θ</mi><mi mathvariant="normal">T</mi></msup><mo>⁢</mo><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>k</mi></msubsup><mo>,</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mrow><msub><mi>Z</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mfrac></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">Where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m6" class="ltx_Math" alttext="Z_{\theta}(\mathrm{x}_{i})" display="inline"><mrow><msub><mi>Z</mi><mi>θ</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> is a partition function that normalizes the exponential form to be a probability distribution, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m7" class="ltx_Math" alttext="f(y_{i}^{k-1},y_{i}^{k},\mathrm{x}_{i})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi>y</mi><mi>i</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>y</mi><mi>i</mi><mi>k</mi></msubsup><mo>,</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math> are arbitrary feature functions.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">In our setting, the CRFs model is required to learn from unlabeled data. This work employs the posterior regularization (PR) framework<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>The readers are refered to the original paper of Ganchev et al. <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite>.</span></span></span>  <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite> to bias the CRFs model’s learning on unlabeled data, under a constraint encoded by the graph propagation expression. It is expected that similar types in the graph should have approximated expected taggings under the CRFs model. We follow the approach introduced by <cite class="ltx_cite">[<a href="#bib.bib31" title="Graph-based posterior regularization for semi-supervised structured prediction" class="ltx_ref">10</a>]</cite> to set up a penalty-based PR objective with GP: the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints:</p>
<table id="S3.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle\mathcal{R}_{U}(\theta,q)=\mathrm{KL}(q||p_{%&#10;\theta})+\lambda\mathcal{P}(v)\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>U</mi></msub><mrow><mo>(</mo><mi>θ</mi><mo>,</mo><mi>q</mi><mo>)</mo></mrow><mo>=</mo><mi>KL</mi><mrow><mo>(</mo><mi>q</mi><mo>|</mo><mo>|</mo><msub><mi>p</mi><mi>θ</mi></msub><mo>)</mo></mrow><mo>+</mo><mi>λ</mi><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><mo>(</mo><mi>v</mi><mo>)</mo></mrow></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">Rather than regularize CRFs model’s posteriors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m1" class="ltx_Math" alttext="p_{\theta}(\mathcal{Y}|\mathrm{x}_{i})" display="inline"><mrow><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo>(</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> directly, our model uses an auxiliary distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m2" class="ltx_Math" alttext="q(\mathcal{Y}|\mathrm{x}_{i})" display="inline"><mrow><mi>q</mi><mrow><mo>(</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> over the possible labelings <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m3" class="ltx_Math" alttext="\mathcal{Y}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒴</mi></math> for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m4" class="ltx_Math" alttext="\mathrm{x}_{i}" display="inline"><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub></math>, and penalizes the CRFs marginal log-likelihood by a <span class="ltx_text ltx_font_bold">KL-divergence</span> term<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>The form of KL term: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m5" class="ltx_Math" alttext="\mathrm{KL}(q||p)=\sum_{q\in\mathcal{Y}}q(\mathrm{y})\log\frac{q(\mathrm{y})}{%&#10;p(\mathrm{y})}" display="inline"><mrow><mi>KL</mi><mrow><mo>(</mo><mi>q</mi><mo>|</mo><mo>|</mo><mi>p</mi><mo>)</mo></mrow><mo>=</mo><msub><mo largeop="true" symmetric="true">∑</mo><mrow><mi>q</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi></mrow></msub><mi>q</mi><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>)</mo></mrow><mi>log</mi><mfrac><mrow><mi>q</mi><mo>⁢</mo><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>)</mo></mrow></mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>.</span></span></span>, representing the distance between the estimated posteriors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m6" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> and the desired posteriors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m7" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math>, as well as a <span class="ltx_text ltx_font_bold">penalty</span> term, formed by the GP function. The hyperparameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m8" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> is used to control the impacts of the penalty term. Note that the penalty is fired if the graph score computed based on the expected taggings given by the current CRFs model is increased vis-a-vis the previous training iteration. This nature requires that the penalty term <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m9" class="ltx_Math" alttext="\mathcal{P}(v)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>⁢</mo><mrow><mo>(</mo><mi>v</mi><mo>)</mo></mrow></mrow></math> should be formed as a function of posteriors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m10" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math> over CRFs model predictions<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>The original PR setting also requires that the penalty term should be a linear (Ganchev et al., 2010) or non-linear <cite class="ltx_cite">[<a href="#bib.bib31" title="Graph-based posterior regularization for semi-supervised structured prediction" class="ltx_ref">10</a>]</cite> function on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m11" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math>.</span></span></span>, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m12" class="ltx_Math" alttext="\mathcal{P}(q)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>⁢</mo><mrow><mo>(</mo><mi>q</mi><mo>)</mo></mrow></mrow></math>. To state this, a mapping <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m13" class="ltx_Math" alttext="\mathcal{M}:(\{1,...,u\},\{1,...,m\})\rightarrow V" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>:</mo><mrow><mrow><mo>(</mo><mrow><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>u</mi></mrow><mo>}</mo></mrow><mo>,</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>m</mi></mrow><mo>}</mo></mrow></mrow><mo>)</mo></mrow><mo>→</mo><mi>V</mi></mrow></mrow></math> from words in the corpus to vertices in the graph is defined. We can thus decompose <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m14" class="ltx_Math" alttext="v_{i,t}" display="inline"><msub><mi>v</mi><mrow><mi>i</mi><mo>,</mo><mi>t</mi></mrow></msub></math> into a function of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p2.m15" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math> as follows:</p>
<table id="S3.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="\small\begin{array}[]{l}\displaystyle v_{i,t}=\frac{\displaystyle\sum_{a=1}^{u%&#10;}\sum_{\substack{b=1;\\&#10;\mathcal{M}(a,b)=V_{i}}}^{m}\sum_{c=1}^{T}\sum_{\mathrm{y}\in\mathcal{Y}}%&#10;\textbf{1}(y^{b}=t,y^{b-1}=c)q(\mathrm{y}|\mathrm{x}_{a})}{\displaystyle\sum_{%&#10;a=1}^{u}\sum_{b=1}^{m}\textbf{1}(\mathcal{M}(a,b)=V_{i})}\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><msub><mi mathsize="small" stretchy="false">v</mi><mrow><mi mathsize="small" stretchy="false">i</mi><mo>,</mo><mi mathsize="small" stretchy="false">t</mi></mrow></msub><mo mathsize="small" stretchy="false">=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mrow><mi mathsize="small" stretchy="false">a</mi><mo mathsize="small" stretchy="false">=</mo><mn mathsize="small" stretchy="false">1</mn></mrow><mi mathsize="small" stretchy="false">u</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mstyle scriptlevel="+1"><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mrow><mi mathsize="small" stretchy="false">b</mi><mo mathsize="small" stretchy="false">=</mo><mn mathsize="small" stretchy="false">1</mn><mo mathsize="small" stretchy="false">;</mo></mrow></mtd></mtr><mtr><mtd><mrow><mrow><mi class="ltx_font_mathcaligraphic" mathsize="small" stretchy="false">ℳ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi mathsize="small" stretchy="false">a</mi><mo>,</mo><mi mathsize="small" stretchy="false">b</mi></mrow><mo>)</mo></mrow></mrow><mo mathsize="small" stretchy="false">=</mo><msub><mi mathsize="small" stretchy="false">V</mi><mi mathsize="small" stretchy="false">i</mi></msub></mrow></mtd></mtr></mtable></mstyle><mi mathsize="small" stretchy="false">m</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mrow><mi mathsize="small" stretchy="false">c</mi><mo mathsize="small" stretchy="false">=</mo><mn mathsize="small" stretchy="false">1</mn></mrow><mi mathsize="small" stretchy="false">T</mi></munderover></mstyle><mstyle displaystyle="true"><munder><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mrow><mi mathsize="small" mathvariant="normal" stretchy="false">y</mi><mo mathsize="small" stretchy="false">∈</mo><mi class="ltx_font_mathcaligraphic" mathsize="small" stretchy="false">𝒴</mi></mrow></munder></mstyle><mtext>𝟏</mtext><mrow><mo mathsize="small" stretchy="false">(</mo><msup><mi mathsize="small" stretchy="false">y</mi><mi mathsize="small" stretchy="false">b</mi></msup><mo mathsize="small" stretchy="false">=</mo><mi mathsize="small" stretchy="false">t</mi><mo mathsize="small" stretchy="false">,</mo><msup><mi mathsize="small" stretchy="false">y</mi><mrow><mi mathsize="small" stretchy="false">b</mi><mo mathsize="small" stretchy="false">-</mo><mn mathsize="small" stretchy="false">1</mn></mrow></msup><mo mathsize="small" stretchy="false">=</mo><mi mathsize="small" stretchy="false">c</mi><mo mathsize="small" stretchy="false">)</mo></mrow><mi mathsize="small" stretchy="false">q</mi><mrow><mo mathsize="small" stretchy="false">(</mo><mi mathsize="small" mathvariant="normal" stretchy="false">y</mi><mo mathsize="small" stretchy="false">|</mo><msub><mi mathsize="small" mathvariant="normal" stretchy="false">x</mi><mi mathsize="small" stretchy="false">a</mi></msub><mo mathsize="small" stretchy="false">)</mo></mrow></mrow><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mrow><mi mathsize="small" stretchy="false">a</mi><mo mathsize="small" stretchy="false">=</mo><mn mathsize="small" stretchy="false">1</mn></mrow><mi mathsize="small" stretchy="false">u</mi></munderover></mstyle><mstyle displaystyle="true"><munderover><mo largeop="true" mathsize="small" movablelimits="false" stretchy="false" symmetric="true">∑</mo><mrow><mi mathsize="small" stretchy="false">b</mi><mo mathsize="small" stretchy="false">=</mo><mn mathsize="small" stretchy="false">1</mn></mrow><mi mathsize="small" stretchy="false">m</mi></munderover></mstyle><mtext>𝟏</mtext><mrow><mo mathsize="small" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" mathsize="small" stretchy="false">ℳ</mi><mrow><mo mathsize="small" stretchy="false">(</mo><mi mathsize="small" stretchy="false">a</mi><mo mathsize="small" stretchy="false">,</mo><mi mathsize="small" stretchy="false">b</mi><mo mathsize="small" stretchy="false">)</mo></mrow><mo mathsize="small" stretchy="false">=</mo><msub><mi mathsize="small" stretchy="false">V</mi><mi mathsize="small" stretchy="false">i</mi></msub><mo mathsize="small" stretchy="false">)</mo></mrow></mrow></mfrac></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">The final learning objective combines the CRFs likelihood with the PR regularization term: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m1" class="ltx_Math" alttext="\mathcal{J}(\theta,q)=\mathcal{L}(\theta)+\mathcal{R}_{U}(\theta,q)" display="inline"><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>θ</mi><mo>,</mo><mi>q</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>U</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>θ</mi><mo>,</mo><mi>q</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>. This joint objective, over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m2" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m3" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math>, can be optimized by an expectation maximization (EM) style algorithm as reported in <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite>. We start from initial parameters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m4" class="ltx_Math" alttext="\theta^{0}" display="inline"><msup><mi>θ</mi><mn>0</mn></msup></math>, estimated by supervised CRFs model training on treebank data. The E-step is to minimize <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m5" class="ltx_Math" alttext="\mathcal{R}_{U}(\theta,q)" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>U</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>θ</mi><mo>,</mo><mi>q</mi></mrow><mo>)</mo></mrow></mrow></math> over the posteriors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m6" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math> that are constrained to the probability simplex. Since the penalty term <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m7" class="ltx_Math" alttext="\mathcal{P}(v)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>⁢</mo><mrow><mo>(</mo><mi>v</mi><mo>)</mo></mrow></mrow></math> is a non-linear form, the optimization method in <cite class="ltx_cite">[<a href="#bib.bib22" title="Posterior regularization for structured latent variable models" class="ltx_ref">9</a>]</cite> via projected gradient descent on the dual is inefficient<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>According to <cite class="ltx_cite">[<a href="#bib.bib31" title="Graph-based posterior regularization for semi-supervised structured prediction" class="ltx_ref">10</a>]</cite>, the dual of quadratic program implies an expensive matrix inverse.</span></span></span>. This study follows the optimization method <cite class="ltx_cite">[<a href="#bib.bib31" title="Graph-based posterior regularization for semi-supervised structured prediction" class="ltx_ref">10</a>]</cite> that uses exponentiated gradient descent (EGD) algorithm. It allows that the variable update expression, as shown in Equation 6, takes a multiplicative rather than an additive form.</p>
<table id="S3.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle q^{(w+1)}(\mathrm{y}|\mathrm{x}_{i})=q^{(w)}(%&#10;\mathrm{y}|\mathrm{x}_{i})\exp(-\eta\frac{\partial\mathcal{R}}{\partial q^{(w)%&#10;}(\mathrm{y}|\mathrm{x}_{i})})\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><msup><mi>q</mi><mrow><mo>(</mo><mrow><mi>w</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow><mo>=</mo><msup><mi>q</mi><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></msup><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>exp</mi><mrow><mo>(</mo><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mo>⁡</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi></mrow><mrow><mo>∂</mo><msup><mi>q</mi><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></msup><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mfrac><mo>)</mo></mrow></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
<p class="ltx_p">where the parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m8" class="ltx_Math" alttext="\eta" display="inline"><mi>η</mi></math> controls the optimization rate in the E-step. With the contributions from the E-step that further encourage <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m9" class="ltx_Math" alttext="q" display="inline"><mi>q</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m10" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> to agree, the M-step aims to optimize the objective <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m11" class="ltx_Math" alttext="\mathcal{J}(\theta,q)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>θ</mi><mo>,</mo><mi>q</mi></mrow><mo>)</mo></mrow></mrow></math> with respect to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m12" class="ltx_Math" alttext="\theta" display="inline"><mi>θ</mi></math>. The M-step is similar to the standard CRFs parameter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m13" class="ltx_Math" alttext="\mathcal{J}(\theta,q)" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>θ</mi><mo>,</mo><mi>q</mi></mrow><mo>)</mo></mrow></mrow></math> and thus is guaranteed to converge to a local optimum.</p>
<table id="S3.E7" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m1" class="ltx_Math" alttext="\begin{array}[]{l}\displaystyle\text{\bf E-step:}\quad q^{(t+1)}=\underset{q}{%&#10;\operatorname{arg\,min}}\mathcal{R}_{U}(\theta^{(t)},q^{(t)})\\&#10;\displaystyle\text{\bf M-step:}\quad\theta^{(t+1)}=\underset{\theta}{%&#10;\operatorname{arg\,max}}\mathcal{L}(\theta)\\&#10;\displaystyle\quad\quad\quad\quad\quad\ \ +\delta\sum_{i=1}^{u}\sum_{\mathrm{y%&#10;}\in\mathcal{Y}}q^{(t+1)}(\mathrm{y}|\mathrm{x}_{i})\log p_{\theta}(\mathrm{y}%&#10;|\mathrm{x}_{i})\end{array}" display="block"><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><mrow><mtext mathvariant="bold">E-step:</mtext><mo separator="true"> </mo><msup><mi>q</mi><mrow><mo>(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup></mrow><mo>=</mo><mrow><munder accentunder="true"><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>min</mi></mrow><mo>𝑞</mo></munder><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℛ</mi><mi>U</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>θ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>q</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msup></mrow><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><mtext mathvariant="bold">M-step:</mtext><mo separator="true"> </mo><msup><mi>θ</mi><mrow><mo>(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup></mrow><mo>=</mo><mrow><munder accentunder="true"><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mo>𝜃</mo></munder><mo>⁢</mo><mi class="ltx_font_mathcaligraphic">ℒ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo lspace="72.5pt">+</mo><mi>δ</mi><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>u</mi></munderover><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi mathvariant="normal">y</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒴</mi></mrow></munder><msup><mi>q</mi><mrow><mo>(</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo>(</mo><mi mathvariant="normal">y</mi><mo>|</mo><msub><mi mathvariant="normal">x</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Data and Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">The experiments in this study evaluated the performances of various CWS models in a Chinese-to-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU <cite class="ltx_cite">[<a href="#bib.bib32" title="BLEU: a method for automatic evaluation of machine translation" class="ltx_ref">17</a>]</cite>, NIST <cite class="ltx_cite">[<a href="#bib.bib33" title="The nist speaker recognition evaluation–overview, methodology, systems, results, perspective" class="ltx_ref">8</a>]</cite> and METEOR <cite class="ltx_cite">[<a href="#bib.bib34" title="METEOR: an automatic metric for mt evaluation with improved correlation with human judgments" class="ltx_ref">2</a>]</cite>, to evaluate the translation quality.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">The monolingual segmented data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m1" class="ltx_Math" alttext="\mathrm{train_{TB}}" display="inline"><msub><mi>train</mi><mi>TB</mi></msub></math>, is extracted from the Penn Chinese Treebank (CTB-7) <cite class="ltx_cite">[<a href="#bib.bib6" title="The Penn Chinese TreeBank: phrase structure annotation of a large corpus" class="ltx_ref">25</a>]</cite>, containing 51,447 sentences. The bilingual training data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m2" class="ltx_Math" alttext="\mathrm{train_{MT}}" display="inline"><msub><mi>train</mi><mi>MT</mi></msub></math>, is formed by a large in-house Chinese-English parallel corpus <cite class="ltx_cite">[<a href="#bib.bib40" title="UM-Corpus: a large English-Chinese parallel corpus for statistical machine translation." class="ltx_ref">21</a>]</cite>. There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including <span class="ltx_text ltx_font_italic">laws</span>, <span class="ltx_text ltx_font_italic">novels</span>, <span class="ltx_text ltx_font_italic">spoken</span>, <span class="ltx_text ltx_font_italic">news</span> and <span class="ltx_text ltx_font_italic">miscellaneous<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_text ltx_font_upright">The in-house corpus has been manually validated, in a long process that exceeded 500 hours.</span></span></span></span></span>. This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m3" class="ltx_Math" alttext="\mathrm{train_{LM}}" display="inline"><msub><mi>train</mi><mi>LM</mi></msub></math>, crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m4" class="ltx_Math" alttext="\mathrm{dev_{MT}}" display="inline"><msub><mi>dev</mi><mi>MT</mi></msub></math>, and testing data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m5" class="ltx_Math" alttext="\mathrm{test_{MT}}" display="inline"><msub><mi>test</mi><mi>MT</mi></msub></math>, respectively.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. <cite class="ltx_cite">[<a href="#bib.bib35" title="An improved Chinese word segmentation system with conditional random field" class="ltx_ref">30</a>]</cite> for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner <cite class="ltx_cite">[<a href="#bib.bib21" title="A systematic comparison of various statistical alignment models" class="ltx_ref">15</a>]</cite>. For the GP, a 10-NNs similarity graph was constructed<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup>We evaluated graphs with top <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> (from 3 to 20) nearest neighbors on development data, and found that the performance converged beyond 10-NNs.</span></span></span>. Following <cite class="ltx_cite">[<a href="#bib.bib5" title="Efficient graph-based semi-supervised learning of structured tagging models" class="ltx_ref">20</a>, <a href="#bib.bib24" title="Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging" class="ltx_ref">27</a>]</cite>, the features used to compute similarities between vertices were (Suppose given a type “ <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m2" class="ltx_Math" alttext="w_{2}w_{3}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math>” surrounding contexts “<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m3" class="ltx_Math" alttext="w_{1}w_{2}w_{3}w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math>”): <span class="ltx_text ltx_font_sansserif">unigram</span> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m4" class="ltx_Math" alttext="w_{3}" display="inline"><msub><mi>w</mi><mn>3</mn></msub></math>), <span class="ltx_text ltx_font_sansserif">bigram</span> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m5" class="ltx_Math" alttext="w_{1}w_{2}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m6" class="ltx_Math" alttext="w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m7" class="ltx_Math" alttext="w_{2}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math>), <span class="ltx_text ltx_font_sansserif">trigram</span> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m8" class="ltx_Math" alttext="w_{2}w_{3}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m9" class="ltx_Math" alttext="w_{2}w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m10" class="ltx_Math" alttext="w_{1}w_{2}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math>), <span class="ltx_text ltx_font_sansserif">trigram+context</span> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m11" class="ltx_Math" alttext="w_{1}w_{2}w_{3}w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math>) and <span class="ltx_text ltx_font_sansserif">character classes</span> in number, punctuation, alphabetic letter and other (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m12" class="ltx_Math" alttext="t(w_{2})t(w_{3})t(w_{4})" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>)</mo></mrow><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mn>3</mn></msub><mo>)</mo></mrow><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mn>4</mn></msub><mo>)</mo></mrow></mrow></math>). There are four hyperparameters in our model to be tuned by using the development data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m13" class="ltx_Math" alttext="\mathrm{dev_{MT}}" display="inline"><msub><mi>dev</mi><mi>MT</mi></msub></math>) among the following settings: for the graph propagation, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m14" class="ltx_Math" alttext="\mu\in\{0.2,0.5,0.8\}" display="inline"><mrow><mi>μ</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0.2</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.8</mn></mrow><mo>}</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m15" class="ltx_Math" alttext="\rho\in\{0.1,0.3,0.5,0.8\}" display="inline"><mrow><mi>ρ</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.8</mn></mrow><mo>}</mo></mrow></mrow></math>; for the PR learning, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m16" class="ltx_Math" alttext="\lambda\in\{0\leq\lambda_{i}\leq 1\}" display="inline"><mrow><mi>λ</mi><mo>∈</mo><mrow><mo>{</mo><mn>0</mn><mo>≤</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>≤</mo><mn>1</mn><mo>}</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m17" class="ltx_Math" alttext="\sigma\in\{0\leq\sigma_{i}\leq 1\}" display="inline"><mrow><mi>σ</mi><mo>∈</mo><mrow><mo>{</mo><mn>0</mn><mo>≤</mo><msub><mi>σ</mi><mi>i</mi></msub><mo>≤</mo><mn>1</mn><mo>}</mo></mrow></mrow></math> where the step is 0.1. The best performed joint settings, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m18" class="ltx_Math" alttext="\mu=0.5,\rho=0.5,\lambda=0.9" display="inline"><mrow><mrow><mi>μ</mi><mo>=</mo><mn>0.5</mn></mrow><mo>,</mo><mrow><mrow><mi>ρ</mi><mo>=</mo><mn>0.5</mn></mrow><mo>,</mo><mrow><mi>λ</mi><mo>=</mo><mn>0.9</mn></mrow></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m19" class="ltx_Math" alttext="\sigma=0.8" display="inline"><mrow><mi>σ</mi><mo>=</mo><mn>0.8</mn></mrow></math>, were used to measure the final performance.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments <cite class="ltx_cite">[<a href="#bib.bib21" title="A systematic comparison of various statistical alignment models" class="ltx_ref">15</a>]</cite> over the segmented bitexts. The heuristic strategy of <span class="ltx_text ltx_font_italic">grow-diag-final-and</span> <cite class="ltx_cite">[<a href="#bib.bib36" title="Moses: open source toolkit for statistical machine translation" class="ltx_ref">11</a>]</cite> was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM <cite class="ltx_cite">[<a href="#bib.bib37" title="SRILM-an extensible language modeling toolkit." class="ltx_ref">19</a>]</cite> on monolingual English data. Moses <cite class="ltx_cite">[<a href="#bib.bib36" title="Moses: open source toolkit for statistical machine translation" class="ltx_ref">11</a>]</cite> was used as decoder. The Minimum Error Rate Training (MERT) <cite class="ltx_cite">[<a href="#bib.bib38" title="Minimum error rate training in statistical machine translation" class="ltx_ref">16</a>]</cite> was used to tune the feature parameters on development data.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Various Segmentation Models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models:</p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Character Segmenter (CS)</span>: this model simply divides Chinese sentences into sequences of characters.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Supervised Monolingual Segmenter (SMS)</span>: this model is trained by CRFs on treebank training data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i2.p1.m1" class="ltx_Math" alttext="\mathrm{train_{TB}}" display="inline"><msub><mi>train</mi><mi>TB</mi></msub></math>). The same feature templates <cite class="ltx_cite">[<a href="#bib.bib35" title="An improved Chinese word segmentation system with conditional random field" class="ltx_ref">30</a>]</cite> are used. The standard four-tags (<span class="ltx_text ltx_font_bold">B</span>, <span class="ltx_text ltx_font_bold">M</span>, <span class="ltx_text ltx_font_bold">E</span> and <span class="ltx_text ltx_font_bold">S</span>) were used as the labels. The stochastic gradient descent is adopted to optimize the parameters.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Unsupervised Bilingual Segmenter (UBS)</span>: this model is trained on the bitexts (trainMT) following the approach introduced in <cite class="ltx_cite">[<a href="#bib.bib17" title="Bilingually motivated domain-adapted word segmentation for statistical machine translation" class="ltx_ref">12</a>]</cite>. The optimal set of the model parameter values was found on <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p1.m1" class="ltx_Math" alttext="\mathrm{dev_{MT}}" display="inline"><msub><mi>dev</mi><mi>MT</mi></msub></math> to be <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p1.m2" class="ltx_Math" alttext="k=3,t_{AC}=0.0" display="inline"><mrow><mrow><mi>k</mi><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>t</mi><mrow><mi>A</mi><mo>⁢</mo><mi>C</mi></mrow></msub><mo>=</mo><mn>0.0</mn></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p1.m3" class="ltx_Math" alttext="t_{COOC}=15" display="inline"><mrow><msub><mi>t</mi><mrow><mi>C</mi><mo>⁢</mo><mi>O</mi><mo>⁢</mo><mi>O</mi><mo>⁢</mo><mi>C</mi></mrow></msub><mo>=</mo><mn>15</mn></mrow></math>.</p>
</div></li>
</ul>
<p class="ltx_p">The comparison candidates also involve two popular off-the-shelf segmentation models:</p>
<ul id="I2" class="ltx_itemize">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Stanford Segmenter</span>: this model, trained by Chang et al. <cite class="ltx_cite">[<a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>]</cite>, treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ICTCLAS Segmenter</span>: this model, trained by Zhang et al. <cite class="ltx_cite">[<a href="#bib.bib39" title="HHMM-based Chinese lexical analyzer ICTCLAS" class="ltx_ref">28</a>]</cite>, is a hierarchical HMM segmenter that incorporates parts-of-speech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities.</p>
</div></li>
</ul>
<p class="ltx_p">This work also evaluated four variant models<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>Note that there are two variant models working with GP. To be fair, the same similarity graph settings introduced in this paper were used.</span></span></span> that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches.</p>
<ul id="I3" class="ltx_itemize">
<li id="I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Self-training Segmenters (STS)</span>: two variant models were defined by the approach reported in <cite class="ltx_cite">[<a href="#bib.bib5" title="Efficient graph-based semi-supervised learning of structured tagging models" class="ltx_ref">20</a>]</cite> that uses the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examples as additional labeled data to retrain a CRFs model. One variant (STS-NO-GP) skips the GP step, directly decoding with type-level word boundary probabilities induced from bitexts, while the other (STS-GP-PL) runs the GP at first and then decodes with GP outcomes. The optimal hyperparameter values were found to be: STS-NO-GP (<math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i1.p1.m1" class="ltx_Math" alttext="\alpha=0.8" display="inline"><mrow><mi>α</mi><mo>=</mo><mn>0.8</mn></mrow></math>) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i1.p1.m2" class="ltx_Math" alttext="\eta=0.6" display="inline"><mrow><mi>η</mi><mo>=</mo><mn>0.6</mn></mrow></math>) and STS-GP-PL (<math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i1.p1.m3" class="ltx_Math" alttext="\mu=0.5,\rho=0.3,\alpha=0.8" display="inline"><mrow><mrow><mi>μ</mi><mo>=</mo><mn>0.5</mn></mrow><mo>,</mo><mrow><mrow><mi>ρ</mi><mo>=</mo><mn>0.3</mn></mrow><mo>,</mo><mrow><mi>α</mi><mo>=</mo><mn>0.8</mn></mrow></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i1.p1.m4" class="ltx_Math" alttext="\eta=0.6" display="inline"><mrow><mi>η</mi><mo>=</mo><mn>0.6</mn></mrow></math>).</p>
</div></li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Virtual Evidences Segmenters (VES)</span>: Two variant models based on the approach in <cite class="ltx_cite">[<a href="#bib.bib24" title="Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging" class="ltx_ref">27</a>]</cite> were defined. The type-level word boundary distributions, induced by the character-based alignment (VES-NO-GP), and the graph propagation (VES-GP-PL), are regarded as virtual evidences to bias CRFs model’s learning on the unlabeled data. The optimal hyperparameter values were found to be: VES-NO-GP (<math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i2.p1.m1" class="ltx_Math" alttext="\alpha=0.7" display="inline"><mrow><mi>α</mi><mo>=</mo><mn>0.7</mn></mrow></math>) and VES-GP-PL (<math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i2.p1.m2" class="ltx_Math" alttext="\mu=0.5,\rho=0.3" display="inline"><mrow><mrow><mi>μ</mi><mo>=</mo><mn>0.5</mn></mrow><mo>,</mo><mrow><mi>ρ</mi><mo>=</mo><mn>0.3</mn></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i2.p1.m3" class="ltx_Math" alttext="\alpha=0.7" display="inline"><mrow><mi>α</mi><mo>=</mo><mn>0.7</mn></mrow></math>).</p>
</div></li>
</ul>
</div>
</div>
<div id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">Table 1 summarizes the final MT performance on the MT-05 test data, evaluated with ten different CWS models. In what follows, we summarized four major observations from the results. Firstly, as expected, having word segmentation does help Chinese-to-English MT. All other nine CWS models outperforms the CS baseline which does not try to identify Chinese words at all. Secondly, the other two baselines, SMS and UBS, are on a par with each other, showing less than 0.36 average performance differences on the three evaluation metrics. This outcome validated that the models, trained by either the treebank or the bilingual data, performed reasonably well. But they only capture partial segmentation features so that less gains for SMT are achieved when comparing to other sophisticated models. Thirdly, we notice that the two off-the-shelf models, Stanford and ICTCLAS, just brought minor improvements over the SMS baseline, although they are trained using richer supervisions. This behaviour illustrates that the conventional optimizations to the monolingual supervised model, e.g., accumulating more supervised data or predefined segmentation properties, are insufficient to help model for achieving better segmentations for SMT. Finally, highlighting the five models working with the bilingual constraints, most of them can achieve significant gains over the other ones without using the bilingual constraints. This strongly demonstrates that bilingually-learned segmentation knowledge does helps CWS for SMT. The models working with GP, STS-GP-PL, VES-GP-PL and ours outperform all others. We attribute this to the role of GP in assisting the spread of bilingual knowledge on the Chinese side. Importantly, it can be observed that our model outperforms STS-GP, VES-GP, which greatly supports that joint learning of CRFs and GP can alleviate the error transfer by the pipelined models. This is one of the most crucial findings in this study. Overall, the boldface numbers in the last row illustrate that our model obtains average improvements of 1.89, 1.76 and 1.61 on BLEU, NIST and METEOR over others.</p>
</div>
<div id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">NIST</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">METEOR</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CS</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.38</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.85</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">54.07</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">SMS</th>
<td class="ltx_td ltx_align_center ltx_border_r">30.05</td>
<td class="ltx_td ltx_align_center ltx_border_r">61.33</td>
<td class="ltx_td ltx_align_center ltx_border_r">55.95</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">UBS</th>
<td class="ltx_td ltx_align_center ltx_border_r">30.15</td>
<td class="ltx_td ltx_align_center ltx_border_r">61.56</td>
<td class="ltx_td ltx_align_center ltx_border_r">55.39</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Stanford</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.40</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.01</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">ICTCLAS</th>
<td class="ltx_td ltx_align_center ltx_border_r">30.29</td>
<td class="ltx_td ltx_align_center ltx_border_r">61.26</td>
<td class="ltx_td ltx_align_center ltx_border_r">55.72</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">STS-NO-GP</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.47</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">56.12</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">STS-GP-PL</th>
<td class="ltx_td ltx_align_center ltx_border_r">31.94</td>
<td class="ltx_td ltx_align_center ltx_border_r">63.20</td>
<td class="ltx_td ltx_align_center ltx_border_r">57.09</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">VES-NO-GP</th>
<td class="ltx_td ltx_align_center ltx_border_r">31.98</td>
<td class="ltx_td ltx_align_center ltx_border_r">62.63</td>
<td class="ltx_td ltx_align_center ltx_border_r">56.59</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">VES-GP-PL</th>
<td class="ltx_td ltx_align_center ltx_border_r">32.04</td>
<td class="ltx_td ltx_align_center ltx_border_r">63.49</td>
<td class="ltx_td ltx_align_center ltx_border_r">57.34</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Our Model</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">32.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">63.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">57.64</span></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Translation performances (%) on MT-05 testing data by using ten different CWS models.</div>
</div>
</div>
<div id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis &amp; Discussion </h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">This section aims to further analyze the three primary observations concluded in Section 4.3: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>) word segmentation is useful to SMT; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m2" class="ltx_Math" alttext="ii" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>i</mi></mrow></math>) the treebank and the bilingual segmentation knowledge are helpful, performing segmentation of different nature; and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m3" class="ltx_Math" alttext="iii" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>i</mi></mrow></math>) the bilingual constraints lead to learn segmentations better tailored for SMT.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p">The first observation derives from the comparisons between the CS baseline and other models. Our results, showing the significant CWS benefits to SMT, are consistent with the works reported in the literature <cite class="ltx_cite">[<a href="#bib.bib19" title="Do we need Chinese word segmentation for statistical machine translation?" class="ltx_ref">24</a>, <a href="#bib.bib15" title="Optimizing Chinese word segmentation for machine translation performance" class="ltx_ref">4</a>]</cite>. In our experiment, two additional evidences found in the translation model are provided to further support that NO tokenization of Chinese (i.e., the CS model’s output) could harm the MT system. First, the SMT phrase extraction, i.e., building “phrases” on top of the character sequences, cannot fully capture all meaningful segmentations produced by the CS model. The character based model leads to missing some useful longer phrases, and to generate many meaningless or redundant translations in the phrase table. Moreover, it is affected by translation ambiguities, caused by the cases where a Chinese character has very different meanings in different contextual environments.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p class="ltx_p">The second observation shifts the emphasis to SMS and UBS, based on the treebank and the bilingual segmentation, respectively. Our results show that both segmentation patterns can bring positive effects to MT. Through analyzing both models’ segmentations for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m1" class="ltx_Math" alttext="\mathrm{train_{MT}}" display="inline"><msub><mi>train</mi><mi>MT</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m2" class="ltx_Math" alttext="\mathrm{test_{MT}}" display="inline"><msub><mi>test</mi><mi>MT</mi></msub></math>, we attempted to get a closer inspection on the segmentation preferences and their influence on MT. Our first finding is that the segmentation consensuses between SMS and UBS are positive to MT. There have about 35% identical segmentations produced by the two models. If these identical segmentations are removed, and the experiments are rerun, the translation scores decrease (on average) by 0.50, 0.85 and 0.70 on BLEU, NIST and METEOR, respectively. Our second finding is that SMS exhibits better segmentation consistency than UBS. One representative example is the segmentations for “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå­¤é¶é¶ (lonely)”. All the outputs of SMS were “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå­¤é¶é¶”, while UBS generated three ambiguous segmentations, “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå­¤(alone)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné¶é¶(double zero)”, “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå­¤é¶(lonely)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné¶(zero)” and “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå­¤(alone)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné¶(zero)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné¶(zero)”. The segmentation consistency of SMS rests on the high-quality treebank data and the robust CRFs tagging model. On the other hand, the advantage of UBS is to capture the segmentations matching the aligned target words. For example, UBS grouped “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå½(country)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné(border)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné´(between)” to a word “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå½éé´(international)”, rather than two words “<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsnå½é(international)_<span class="ltx_ERROR undefined">{CJK}</span>UTF8gbsné´(between)” (as given by SMS), since these three characters are aligned to a single English word “international”. The above analysis shows that SMS and UBS have their own merits and combining the knowledge derived from both segmentations is highly encouraged.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p class="ltx_p">The third observation concerns the great impact of the bilingual constraints to the segmentation models in the MT task. The use of the bilingual constraints is the prime objective of this study. Our first contribution for this purpose is on using the word boundary distributions to capture the bilingual segmentation supervisions. This representation contributes to reduce the negative impacts of erroneous “chars-to-word” alignments. The ambiguous types (having relatively uniform boundary distribution), caused by alignment errors, cannot directly bias the model tagging preferences. Furthermore, the word boundary distributions are convenient to make up the learning constraints over the labelings among various constrained learning approaches. They have successfully played in three types of constraints for our experiments: PR penalty (Our model), decoding constraints in self-training (STS) and virtual evidences (VES). The second contribution is the use of GP, illustrated by STS-GP-PL, VES-GP-PL and Our model. The major effect is to multiply the impacts of the bilingual knowledge through the similarity graph. The graph vertices (types)<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>This experiment yielded a similarity graph that consists of 11,909,620 types from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p4.m1" class="ltx_Math" alttext="\mathrm{train_{TB}}" display="inline"><msub><mi>train</mi><mi>TB</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p4.m2" class="ltx_Math" alttext="\mathrm{train_{MT}}" display="inline"><msub><mi>train</mi><mi>MT</mi></msub></math>, where there have 8,593,220 (72.15%) types without any empirical boundary distributions.</span></span></span>, without any supervisions, can learn the word boundary information from their similar types (neighborhoods) having the empirical boundary probabilities. The segmentations given by the three GP models show about 70% positive segmentation changes, affected by the unlabeled graph vertices, with respect to the ones given by the NO-GP models, STS-NO-GP and VES-NO-GP. In our opinion, the learning mechanism of our approach, joint coupling of GP and CRFs, rather than the pipelined one as the other two models, contributes to maximizing the graph smoothness effects to the CRFs estimation so that the error propagation of the pipelined approaches is alleviated.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">This paper proposed a novel CWS model for the SMT task. This model aims to maintain the linguistic segmentation supervisions from treebank data and simultaneously integrate useful bilingual segmentations induced from the bitexts. This objective is accomplished by three main steps: 1) learn word boundaries from character-based alignments; 2) encode the learned word boundaries into a GP constraint; and 3) training a CRFs model, under the GP constraint, by using the PR framework. The empirical results indicate that the proposed model can yield better segmentations for SMT.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">The authors are grateful to the Science and Technology Development Fund of Macau and the Research Committee of the University of Macau (Grant No. MYRG076 (Y1-L2)-FST13-WF and MYRG070 (Y1-L2)-FST12-CS) for the funding support for our research. The work of Isabel Trancoso was supported by national funds through FCT-Fundação para a Ciêcia e a Tecnologia, under project PEst-OE/EEI/LA0021/2013. The authors also wish to thank the anonymous reviewers for many helpful comments.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib30" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Altun, D. McAllester and M. Belkin</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Maximum margin semi-supervised learning for structured variables</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">18</span>, <span class="ltx_text ltx_bib_pages"> pp. 33</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Word Boundaries Learned from Character-based Alignments ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Banerjee and A. Lavie</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">METEOR: an automatic metric for mt evaluation with improved correlation with human judgments</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 65–72</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Bengio, O. Delalleau and N. Le Roux</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Label propagation and quadratic criterion</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Semi-Supervised Learning</span>, <span class="ltx_text ltx_bib_pages"> pp. 193–216</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p3" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Chang, M. Galley and C. D. Manning</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Optimizing Chinese word segmentation for machine translation performance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 224–232</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.i1.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S1.p1" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S4.SS4.p2" title="4.4 Analysis &amp; Discussion ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Chung and D. Gildea</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised tokenization for machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 718–726</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Word Boundaries Learned from Character-based Alignments ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Das and S. Petrov</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised part-of-speech tagging with bilingual graph-based projections.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 600–609</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS2.p1" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S3.SS2.p2" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Das and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Graph-based lexicon expansion with sparsity-inducing penalties</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 677–687</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p2" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S3.SS2.p3" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
</span></li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. R. Doddington, M. A. Przybocki, A. F. Martin and D. A. Reynolds</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The nist speaker recognition evaluation–overview, methodology, systems, results, perspective</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Speech Communication</span> <span class="ltx_text ltx_bib_volume">31</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 225–254</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Ganchev, J. Graça, J. Gillenwater and B. Taskar</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Posterior regularization for structured latent variable models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">11</span>, <span class="ltx_text ltx_bib_pages"> pp. 2001–2049</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS3.p2" title="3.3 PR Learning with GP Constraint ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
<a href="#S3.SS3.p3" title="3.3 PR Learning with GP Constraint ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
<a href="#S3.p1" title="3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. He, J. Gillenwater and B. Taskar</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Graph-based posterior regularization for semi-supervised structured prediction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 38</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 PR Learning with GP Constraint ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>,
<a href="#S3.SS3.p3" title="3.3 PR Learning with GP Constraint ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran and R. Zens</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Moses: open source toolkit for statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 177–180</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p4" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Ma and A. Way</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bilingually motivated domain-adapted word segmentation for statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 549–557</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i3.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S1.p1" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Word Boundaries Learned from Character-based Alignments ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. S. Mann and A. McCallum</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generalized expectation criteria for semi-supervised learning of conditional random fields</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 870–878</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. McCallum, G. Mann and G. Druck</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generalized expectation criteria</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Science Technical Note, University of Massachusetts, Amherst, MA</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. J. Och and H. Ney</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A systematic comparison of various statistical alignment models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">29</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 19–51</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Word Boundaries Learned from Character-based Alignments ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS1.p3" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS1.p4" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. J. Och</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minimum error rate training in statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 160–167</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p4" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Papineni, S. Roukos, T. Ward and W. Zhu</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BLEU: a method for automatic evaluation of machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 311–318</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Paul, F. Andrew and S. Eiichiro</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Integration of multiple bilingually-trained segmentation schemes into statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEICE Transactions on Information and Systems</span> <span class="ltx_text ltx_bib_volume">94</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 690–697</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Stolcke</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SRILM-an extensible language modeling toolkit.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p4" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Subramanya, S. Petrov and F. Pereira</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient graph-based semi-supervised learning of structured tagging models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 167–176</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I3.i1.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS2.p1" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S3.SS2.p2" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S4.SS1.p3" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Tian, D. F. Wong, L. S. Chao, P. Quaresma, F. Oliveira, S. Li, Y. Wang and Y. Lu</span><span class="ltx_text ltx_bib_year">(2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">UM-Corpus: a large English-Chinese parallel corpus for statistical machine translation.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p2" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Xi, G. Tang, X. Dai, S. Huang and J. Chen</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Enhancing statistical machine translation with character alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 285–290</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Xu, E. Matusov, R. Zens and H. Ney</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Integrated Chinese word segmentation in statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 141–147</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Xu, R. Zens and H. Ney</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Do we need Chinese word segmentation for statistical machine translation?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 122–128</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Word Boundaries Learned from Character-based Alignments ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS4.p2" title="4.4 Analysis &amp; Discussion ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Xue, F. Xia, F. Chiou and M. Palmer</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Penn Chinese TreeBank: phrase structure annotation of a large corpus</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Natural Language Engineering</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 207–238</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS1.p2" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zeng, D. F. Wong, L. S. Chao, I. Trancoso, L. He and Q. Huang</span><span class="ltx_text ltx_bib_year">(2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Lexicon expansion for latent variable grammars</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Pattern Recognition Letters</span> <span class="ltx_text ltx_bib_volume">42</span>, <span class="ltx_text ltx_bib_pages"> pp. 47–55</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zeng, D. F. Wong, L. S. Chao and I. Trancoso</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Graph-based semi-supervised model for joint Chinese word segmentation and part-of-speech tagging</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 770–779</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I3.i2.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS2.p1" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S3.SS2.p2" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S4.SS1.p3" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhang, H. Yu, D. Xiong and Q. Liu</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HHMM-based Chinese lexical analyzer ICTCLAS</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 184–187</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.i2.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Zhang, K. Yasuda and E. Sumita</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved statistical machine translation by multiple Chinese word segmentation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 216–223</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhao, C. Huang and M. Li</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An improved Chinese word segmentation system with conditional random field</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i2.p1" title="4.2 Various Segmentation Models ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS1.p3" title="4.1 Data and Setup ‣ 4 Experiments ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhao, M. Utiyama, E. Sumita and B. Lu</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An empirical study on word segmentation for Chinese machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_inbook">Computational Linguistics and Intelligent Text Processing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 248–263</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Zhu, D. F. Wong and L. S. Chao</span><span class="ltx_text ltx_bib_year">(2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised chunking based on graph propagation from bilingual corpus</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Scientific World Journal</span> <span class="ltx_text ltx_bib_volume">2014</span> (<span class="ltx_text ltx_bib_number">401943</span>), <span class="ltx_text ltx_bib_pages"> pp. 10</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zhu, Z. Ghahramani and J. Lafferty</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semi-supervised learning using gaussian fields and harmonic functions</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp. 912–919</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p3" title="3.2 Constraints Encoded by Graph Propagation Expression ‣ 3 Methodology ‣ Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 19:13:24 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
