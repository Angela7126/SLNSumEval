<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Modelling function words improves unsupervised word segmentation</title>
<!--Generated on Tue Jun 10 17:25:11 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Modelling function words improves unsupervised word segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mark Johnson<math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="{}^{1,2}" display="inline"><msup><mi/><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msup></math>, Anne Christophe<math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="{}^{3,4}" display="inline"><msup><mi/><mrow><mn>3</mn><mo>,</mo><mn>4</mn></mrow></msup></math>, Katherine Demuth<math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="{}^{2,6}" display="inline"><msup><mi/><mrow><mn>2</mn><mo>,</mo><mn>6</mn></mrow></msup></math> 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emmanuel Dupoux<math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="{}^{3,5}" display="inline"><msup><mi/><mrow><mn>3</mn><mo>,</mo><mn>5</mn></mrow></msup></math> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="{}^{1}" display="inline"><msup><mi/><mn>1</mn></msup></math><span class="ltx_text ltx_font_small"> Department of Computing, Macquarie University, Sydney, Australia</span> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="{}^{2}" display="inline"><msup><mi/><mn>2</mn></msup></math><span class="ltx_text ltx_font_small"> Santa Fe Institute, Santa Fe, New Mexico, USA</span> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m7" class="ltx_Math" alttext="{}^{3}" display="inline"><msup><mi/><mn>3</mn></msup></math><span class="ltx_text ltx_font_small"> Ecole Normale Supérieure, Paris, France</span> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m8" class="ltx_Math" alttext="{}^{4}" display="inline"><msup><mi/><mn>4</mn></msup></math><span class="ltx_text ltx_font_small"> Centre National de la Recherche Scientifique, Paris, France</span> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m9" class="ltx_Math" alttext="{}^{5}" display="inline"><msup><mi/><mn>5</mn></msup></math><span class="ltx_text ltx_font_small"> Ecole des Hautes Etudes en Sciences Sociales, Paris, France</span> 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m10" class="ltx_Math" alttext="{}^{6}" display="inline"><msup><mi/><mn>6</mn></msup></math><span class="ltx_text ltx_font_small"> Department of Linguistics, Macquarie University, Sydney, Australia</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Inspired by experimental psychological findings suggesting that
function words play a special role in word learning, we make a
simple modification to an Adaptor Grammar based Bayesian word
segmentation model to allow it to learn sequences of monosyllabic
“function words” at the beginnings and endings of collocations of
(possibly multi-syllabic) words. This modification improves
unsupervised word segmentation on the standard
<cite class="ltx_cite"/> corpus of child-directed English by more
than 4% token f-score compared to a model identical except that it
does not special-case “function words”, setting a new
state-of-the-art of 92.4% token f-score.
Our function word model assumes that function words appear at the
left periphery, and while this is true of languages such as
English, it is not true universally. We show that a learner can
use Bayesian model selection to determine the location of function
words in their language, even though the input to the model only
consists of unsegmented sequences of phones. Thus our computational
models support the hypothesis that function words play a special
role in word learning.</p>
</div>
<div id="p1" class="ltx_para">
<p class="ltx_p">SIL/I”</p>
</div>
<div id="p2" class="ltx_para"><svg xmlns="http://www.w3.org/2000/svg" height="50" version="1.1" viewBox="192 -236 65 50" width="65"><g transform="matrix(1 0 0 -1 0 -422)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g><path d="M225-206L206-213" style="fill:none"/></g><g><path d="M225-206L244-213" style="fill:none"/></g><g><g><g transform="matrix(1 0 0 1 220 -200)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="p2.pic1.m1" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math></p></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 197 -225)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="p2.pic1.m2" class="ltx_Math" alttext="t_{1}" display="inline"><msub><mi>t</mi><mn>1</mn></msub></math></p></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 234 -225)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="p2.pic1.m3" class="ltx_Math" alttext="t_{n}" display="inline"><msub><mi>t</mi><mi>n</mi></msub></math></p></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 225 -225)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="p2.pic1.m4" class="ltx_Math" alttext="\ldots" display="inline"><mi mathvariant="normal">…</mi></math></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></svg>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Over the past two decades psychologists have investigated the role
that function words might play in human language acquisition. Their
experiments suggest that function words play a special role in the
acquisition process: children learn function words before they learn
the vast bulk of the associated content words, and they use function
words to help identify context words.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The goal of this paper is to determine whether computational models of
human language acquisition can provide support for the hypothesis that
function words are treated specially in human language acquisition.
We do this by comparing two computational models of word segmentation
which differ solely in the way that they model function words.
Following <cite class="ltx_cite"/> and <cite class="ltx_cite"/> our word segmentation
models identify word boundaries from unsegmented sequences of phonemes
corresponding to utterances, effectively performing unsupervised
learning of a lexicon. For example, given input consisting of unsegmented
utterances such as the following:</p>
<p class="ltx_p ltx_align_center">j  u  w  É  n  t  t  u  s  i  Ã°  É  b  Ê  k</p>
<p class="ltx_p">a word segmentation model should segment this as ju wÉnt tu si
Ã°É bÊk, which is the IPA representation of “you want to see the book”.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">We show that a model equipped with the ability to learn some rudimentary
properties of the target language’s function words is able to learn the
vocabulary of that language more accurately than a model that is identical
except that it is incapable of learning these generalisations about
function words.

This suggests that there are
acquisition advantages to treating function words specially that human
learners could take advantage of (at least to the extent that they are
learning similar generalisations as our models), and thus supports the
hypothesis that function words are treated specially in human lexical
acquisition.

As a reviewer points out, we present no evidence that children use function
words in the way that our model does, and we want to emphasise we make no such claim.

While absolute accuracy is not directly relevant to the
main point of the paper, we note that the models that learn
generalisations about function words perform unsupervised word
segmentation at 92.5% token f-score on the standard
<cite class="ltx_cite"/> corpus, which improves the previous
state-of-the-art by more than 4%.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">As a reviewer points out, the changes we make to our models to incorporate
function words can be viewed as “building in” substantive information
about possible human languages.

The model that achieves the best token f-score expects function words
to appear at the left edge of phrases. While this is true for
languages such as English, it is not true universally. By comparing
the posterior probability of two models — one in which function
words appear at the left edges of phrases, and another in which
function words appear at the right edges of phrases — we show that a
learner could use Bayesian posterior probabilities to determine that
function words appear at the left edges of phrases in English, even
though they are not told the locations of word boundaries or which
words are function words.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">This paper is structured as follows. Section <a href="#S2" title="2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the
specific word segmentation models studied in this paper, and the way we
extended them to capture certain properties of function words. The
word segmentation experiments are presented in section <a href="#S3" title="3 Word segmentation results ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
and section <a href="#S4" title="4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses how a learner could determine
whether function words occur on the left-periphery or the
right-periphery in the language they are learning.
Section <a href="#S5" title="5 Conclusions and future work ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes and describes possible future work.
The rest of this introduction provides background on function words,
the Adaptor Grammar models we use to describe lexical acquisition and
the Bayesian inference procedures we use to infer these models.</p>
</div>
<div id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">1.1 </span>Psychological evidence for the role of function words in word learning</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p class="ltx_p">Traditional descriptive linguistics distinguishes <em class="ltx_emph">function words</em>,
such as determiners and prepositions, from <em class="ltx_emph">content words</em>, such
as nouns and verbs, corresponding roughly to the distinction between
functional categories and lexical categories of modern generative
linguistics <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p class="ltx_p">Function words differ from content words in at least the following
ways:</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">there are usually far fewer function word types than content
word types in a language</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">function word types typically have much higher token frequency
than content word types</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">function words are typically morphologically and phonologically
simple (e.g., they are typically monosyllabic)</p>
</div></li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">4.</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p">function words typically appear in peripheral positions of
phrases (e.g., prepositions typically appear at the beginning of
prepositional phrases)</p>
</div></li>
<li id="I1.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">5.</span> 
<div id="I1.i5.p1" class="ltx_para">
<p class="ltx_p">each function word class is associated with specific content
word classes (e.g., determiners and prepositions are associated with
nouns, auxiliary verbs and complementisers are associated with main
verbs)</p>
</div></li>
<li id="I1.i6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">6.</span> 
<div id="I1.i6.p1" class="ltx_para">
<p class="ltx_p">semantically, content words denote sets of objects or events, while
function words denote more complex relationships over the entities denoted
by content words</p>
</div></li>
<li id="I1.i7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">7.</span> 
<div id="I1.i7.p1" class="ltx_para">
<p class="ltx_p">historically, the rate of innovation of function words is much
lower than the rate of innovation of content words (i.e., function
words are typically “closed class”, while content words are “open
class”)</p>
</div></li>
</ol>
<p class="ltx_p">Properties 1–4 suggest that function words might play a special role
in language acquisition because they are especially easy to identify,
while property 5 suggests that they might be useful for identifying
lexical categories. The models we study here focus on properties 3
and 4, in that they are capable of learning specific sequences of
monosyllabic words in peripheral (i.e., initial or final) positions of
phrase-like units.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p class="ltx_p">A number of psychological experiments have shown that infants are
sensitive to the function words of their language within their first
year of life <cite class="ltx_cite">[]</cite>, often before they have
experienced the “word learning spurt”.

Crucially for our purpose, infants of this age were shown to
exploit frequent function words to segment neighboring content words
<cite class="ltx_cite">[]</cite>.

In addition, 14 to 18-month-old children
were shown to exploit function words to constrain lexical access to
known words â- for instance, they expect a noun after a determiner
<cite class="ltx_cite">[]</cite>.

In addition, it is plausible that function words play a crucial role
in children’s acquisition of more complex syntactic phenomena
<cite class="ltx_cite">[]</cite>, so it is interesting to investigate
the roles they might play in computational models of language
acquisition.</p>
</div>
</div>
<div id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">1.2 </span>Adaptor grammars</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p class="ltx_p">Adaptor grammars are a framework for Bayesian inference of a certain
class of hierarchical non-parametric models <cite class="ltx_cite">[]</cite>. They
define distributions over the trees specified by a context-free
grammar, but unlike probabilistic context-free grammars, they
“learn” distributions over the possible subtrees of a user-specified
set of “adapted” nonterminals. (Adaptor grammars are
non-parametric, i.e., not characterisable by a finite set of
parameters, if the set of possible subtrees of the adapted
nonterminals is infinite). Adaptor grammars are useful when the goal
is to learn a potentially unbounded set of entities that need to
satisfy hierarchical constraints. As section <a href="#S2" title="2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> explains in
more detail, word segmentation is such a case: words are composed of
syllables and belong to phrases or collocations, and modelling this
structure improves word segmentation accuracy.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p class="ltx_p">Adaptor Grammars are formally defined in <cite class="ltx_cite"/>, which
should be consulted for technical details. Adaptor Grammars (AGs) are
an extension of Probabilistic Context-Free Grammars (PCFGs), which we
describe first. A <span class="ltx_text ltx_font_italic">Context-Free Grammar</span> (CFG) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m1" class="ltx_Math" alttext="G=(N,W,R,S)" display="inline"><mrow><mi>G</mi><mo>=</mo><mrow><mo>(</mo><mrow><mi>N</mi><mo>,</mo><mi>W</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>S</mi></mrow><mo>)</mo></mrow></mrow></math>
consists of disjoint finite sets of <span class="ltx_text ltx_font_italic">nonterminal symbols</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> and
<span class="ltx_text ltx_font_italic">terminal symbols</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m3" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>, a finite set of <span class="ltx_text ltx_font_italic">rules</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m4" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> of the
form <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m5" class="ltx_Math" alttext="A\operatorname{\mathop{\rightarrow}}\alpha" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m6" class="ltx_Math" alttext="A\in N" display="inline"><mrow><mi>A</mi><mo>∈</mo><mi>N</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m7" class="ltx_Math" alttext="\alpha\in(N\cup W)^{\star}" display="inline"><mrow><mi>α</mi><mo>∈</mo><msup><mrow><mo>(</mo><mrow><mi>N</mi><mo>∪</mo><mi>W</mi></mrow><mo>)</mo></mrow><mo>⋆</mo></msup></mrow></math>, and
a <span class="ltx_text ltx_font_italic">start symbol</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m8" class="ltx_Math" alttext="S\in N" display="inline"><mrow><mi>S</mi><mo>∈</mo><mi>N</mi></mrow></math>. (We assume there are no
“<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m9" class="ltx_Math" alttext="\epsilon" display="inline"><mi>ϵ</mi></math>-rules” in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m10" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math>, i.e., we require that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m11" class="ltx_Math" alttext="|\alpha|\geq 1" display="inline"><mrow><mrow><mo fence="true">|</mo><mi>α</mi><mo fence="true">|</mo></mrow><mo>≥</mo><mn>1</mn></mrow></math>
for each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p2.m12" class="ltx_Math" alttext="A\operatorname{\mathop{\rightarrow}}\alpha\in R" display="inline"><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow><mo>∈</mo><mi>R</mi></mrow></math>).</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p class="ltx_p">A <span class="ltx_text ltx_font_italic">Probabilistic Context-Free Grammar</span> (PCFG) is a quintuple
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m1" class="ltx_Math" alttext="(N,W,R,S,\boldsymbol{\theta})" display="inline"><mrow><mo>(</mo><mrow><mi>N</mi><mo>,</mo><mi>W</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>S</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m3" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m4" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m5" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> are the
nonterminals, terminals, rules and start symbol of a CFG respectively,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m6" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> is a vector of non-negative reals indexed by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m7" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> that
satisfy <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m8" class="ltx_Math" alttext="\sum_{\alpha\in R_{A}}\theta_{A\operatorname{\mathop{\rightarrow}}\alpha}=1" display="inline"><mrow><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mrow><mi>α</mi><mo>∈</mo><msub><mi>R</mi><mi>A</mi></msub></mrow></msub><msub><mi>θ</mi><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow></msub></mrow><mo>=</mo><mn>1</mn></mrow></math> for each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m9" class="ltx_Math" alttext="A\in N" display="inline"><mrow><mi>A</mi><mo>∈</mo><mi>N</mi></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m10" class="ltx_Math" alttext="R_{A}=\{A\operatorname{\mathop{\rightarrow}}\alpha:A\operatorname{\mathop{%&#10;\rightarrow}}\alpha\in R\}" display="inline"><mrow><msub><mi>R</mi><mi>A</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow><mo separator="true">:</mo><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow><mo>∈</mo><mi>R</mi></mrow></mrow><mo>}</mo></mrow></mrow></math> is the set of rules
expanding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p3.m11" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>.</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p class="ltx_p">Informally, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m1" class="ltx_Math" alttext="\theta_{A\operatorname{\mathop{\rightarrow}}\alpha}" display="inline"><msub><mi>θ</mi><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow></msub></math> is the probability of a node
labelled <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m2" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> expanding to a sequence of nodes labelled <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m3" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>, and
the probability of a tree is the product of the probabilities of the
rules used to construct each non-leaf node in it. More precisely, for
each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m4" class="ltx_Math" alttext="X\in N\cup W" display="inline"><mrow><mi>X</mi><mo>∈</mo><mrow><mi>N</mi><mo>∪</mo><mi>W</mi></mrow></mrow></math> a PCFG associates distributions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m5" class="ltx_Math" alttext="G_{X}" display="inline"><msub><mi>G</mi><mi>X</mi></msub></math> over the set of
trees <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m6" class="ltx_Math" alttext="\mathcal{T}_{X}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>X</mi></msub></math> generated by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p4.m7" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> as follows:</p>
</div>
<div id="S1.SS2.p5" class="ltx_para">
<p class="ltx_p">If <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p5.m1" class="ltx_Math" alttext="X\in W" display="inline"><mrow><mi>X</mi><mo>∈</mo><mi>W</mi></mrow></math> (i.e., if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p5.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> is a terminal) then <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p5.m3" class="ltx_Math" alttext="G_{X}" display="inline"><msub><mi>G</mi><mi>X</mi></msub></math> is the
distribution that puts probability 1 on the single-node tree labelled
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p5.m4" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>.</p>
</div>
<div id="S1.SS2.p6" class="ltx_para">
<p class="ltx_p">If <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m1" class="ltx_Math" alttext="X\in N" display="inline"><mrow><mi>X</mi><mo>∈</mo><mi>N</mi></mrow></math> (i.e., if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> is a nonterminal) then:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S1.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.E1.m1" class="ltx_Math" alttext="\displaystyle G_{X}" display="inline"><msub><mi>G</mi><mi>X</mi></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.E1.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.E1.m3" class="ltx_Math" alttext="\displaystyle\sum_{{X\operatorname{\mathop{\rightarrow}}B_{1}\ldots B_{n}\in R%&#10;_{X}}\hskip{-25.0pt}}\!\!\!\!\!\theta_{X\operatorname{\mathop{\rightarrow}}B_{%&#10;1}\ldots B_{n}}\hbox{\sc TD}_{X}(G_{B_{1}},\ldots,G_{B_{n}})" display="inline"><mrow><mpadded width="-8.5pt"><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mrow><mi>X</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>B</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>∈</mo><mpadded width="-25.0pt"><msub><mi>R</mi><mi>X</mi></msub></mpadded></mrow></munder></mstyle></mpadded><mrow><msub><mi>θ</mi><mrow><mi>X</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>B</mi><mi>n</mi></msub></mrow></mrow></mrow></msub><mo>⁢</mo><msub><mtext mathvariant="normal">TD</mtext><mi>X</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>G</mi><msub><mi>B</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>G</mi><msub><mi>B</mi><mi>n</mi></msub></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m3" class="ltx_Math" alttext="R_{X}" display="inline"><msub><mi>R</mi><mi>X</mi></msub></math> is the subset of rules in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m4" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> expanding nonterminal <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m5" class="ltx_Math" alttext="X\in N" display="inline"><mrow><mi>X</mi><mo>∈</mo><mi>N</mi></mrow></math>, and:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S1.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex1.m1" class="ltx_Math" alttext="\displaystyle\hbox{\sc TD}_{X}(G_{1},\ldots,G_{n})\left(11\right)" display="inline"><mrow><msub><mtext mathvariant="normal">TD</mtext><mi>X</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>G</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>G</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>⁢</mo><mrow><mo>(</mo><mn>11</mn><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex1.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex1.m3" class="ltx_Math" alttext="\displaystyle\displaystyle\prod_{i=1}^{n}G_{i}(t_{i})." display="inline"><mrow><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mrow><msub><mi>G</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">That is, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m6" class="ltx_Math" alttext="\hbox{\sc TD}_{X}(G_{1},\ldots,G_{n})" display="inline"><mrow><msub><mtext mathvariant="normal">TD</mtext><mi>X</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>G</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>G</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></math> is a distribution over the
set of trees <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m7" class="ltx_Math" alttext="\mathcal{T}_{X}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>X</mi></msub></math> generated by nonterminal <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m8" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>, where each subtree
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m9" class="ltx_Math" alttext="t_{i}" display="inline"><msub><mi>t</mi><mi>i</mi></msub></math> is generated independently from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m10" class="ltx_Math" alttext="G_{i}" display="inline"><msub><mi>G</mi><mi>i</mi></msub></math>. The PCFG generates the
distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m11" class="ltx_Math" alttext="G_{S}" display="inline"><msub><mi>G</mi><mi>S</mi></msub></math> over the set of trees <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m12" class="ltx_Math" alttext="\mathcal{T}_{S}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>S</mi></msub></math> generated by the start
symbol <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p6.m13" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>; the distribution over the strings it generates is obtained
by marginalising over the trees.</p>
</div>
<div id="S1.SS2.p7" class="ltx_para">
<p class="ltx_p">In a Bayesian PCFG one puts Dirichlet priors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p7.m1" class="ltx_Math" alttext="\mathrm{Dir}(\boldsymbol{\alpha})" display="inline"><mrow><mi>Dir</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝜶</mi><mo>)</mo></mrow></mrow></math> on the
rule probability vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p7.m2" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>, such that there is one Dirichlet
parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p7.m3" class="ltx_Math" alttext="\alpha_{A\operatorname{\mathop{\rightarrow}}\alpha}" display="inline"><msub><mi>α</mi><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow></msub></math> for each rule <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p7.m4" class="ltx_Math" alttext="A\operatorname{\mathop{\rightarrow}}\alpha\in R" display="inline"><mrow><mrow><mi>A</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mi>α</mi></mrow></mrow><mo>∈</mo><mi>R</mi></mrow></math>.
There are Markov Chain Monte Carlo (MCMC) and Variational Bayes procedures for estimating the posterior
distribution over rule probabilities <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p7.m5" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> and parse trees
given data consisting of terminal strings alone <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.SS2.p8" class="ltx_para">
<p class="ltx_p">PCFGs can be viewed as recursive mixture models over trees.
While PCFGs are expressive enough to describe a range of
linguistically-interesting phenomena, PCFGs are <em class="ltx_emph">parametric
models</em>, which limits their ability to describe phenomena where the
set of basic units, as well as their properties, are the target of
learning. Lexical acqusition is an example of a phenomenon that is
naturally viewed as <em class="ltx_emph">non-parametric inference</em>, where the number
of lexical entries (i.e., words) as well as their properties must be
learnt from the data.</p>
</div>
<div id="S1.SS2.p9" class="ltx_para">
<p class="ltx_p">It turns out there is a straight-forward modification to
the PCFG distribution (<a href="#S1.E1" title="(1) ‣ 1.2 Adaptor grammars ‣ 1 Introduction ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) that makes it suitably non-parametric. As
<cite class="ltx_cite"/> explain, by inserting a Dirichlet Process (DP) or
Pitman-Yor Process (PYP) into the generative mechanism (<a href="#S1.E1" title="(1) ‣ 1.2 Adaptor grammars ‣ 1 Introduction ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)
the model “concentrates” mass on a subset of trees <cite class="ltx_cite">[]</cite>.
Specifically, an Adaptor Grammar identifies a subset <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m1" class="ltx_Math" alttext="A\subseteq N" display="inline"><mrow><mi>A</mi><mo>⊆</mo><mi>N</mi></mrow></math> of
<em class="ltx_emph">adapted nonterminals</em>. In an Adaptor Grammar the unadapted
nonterminals <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m2" class="ltx_Math" alttext="N\setminus A" display="inline"><mrow><mi>N</mi><mo>∖</mo><mi>A</mi></mrow></math> expand via (<a href="#S1.E1" title="(1) ‣ 1.2 Adaptor grammars ‣ 1 Introduction ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), just as in a
PCFG, but the distributions of the adapted nonterminals <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m3" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> are
“concentrated” by passing them through a DP or PYP:</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S1.Ex2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex2.m1" class="ltx_Math" alttext="\displaystyle H_{X}" display="inline"><msub><mi>H</mi><mi>X</mi></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex2.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex2.m3" class="ltx_Math" alttext="\displaystyle\sum_{{X\operatorname{\mathop{\rightarrow}}B_{1}\ldots B_{n}\in R%&#10;_{X}}\hskip{-25.0pt}}\!\!\!\!\!\theta_{X\operatorname{\mathop{\rightarrow}}B_{%&#10;1}\ldots B_{n}}\hbox{\sc TD}_{X}(G_{B_{1}},\ldots,G_{B_{n}})" display="inline"><mrow><mpadded width="-8.5pt"><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mrow><mi>X</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>B</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>∈</mo><mpadded width="-25.0pt"><msub><mi>R</mi><mi>X</mi></msub></mpadded></mrow></munder></mstyle></mpadded><mrow><msub><mi>θ</mi><mrow><mi>X</mi><mo>⁢</mo><mrow><mo>→</mo><mo>⁡</mo><mrow><msub><mi>B</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>B</mi><mi>n</mi></msub></mrow></mrow></mrow></msub><mo>⁢</mo><msub><mtext mathvariant="normal">TD</mtext><mi>X</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>G</mi><msub><mi>B</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>G</mi><msub><mi>B</mi><mi>n</mi></msub></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S1.Ex3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex3.m1" class="ltx_Math" alttext="\displaystyle G_{X}" display="inline"><msub><mi>G</mi><mi>X</mi></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex3.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex3.m3" class="ltx_Math" alttext="\displaystyle\mathrm{PYP}(H_{X},a_{X},b_{X})" display="inline"><mrow><mi>PYP</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>H</mi><mi>X</mi></msub><mo>,</mo><msub><mi>a</mi><mi>X</mi></msub><mo>,</mo><msub><mi>b</mi><mi>X</mi></msub></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Here <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m4" class="ltx_Math" alttext="a_{X}" display="inline"><msub><mi>a</mi><mi>X</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m5" class="ltx_Math" alttext="b_{X}" display="inline"><msub><mi>b</mi><mi>X</mi></msub></math> are parameters of the PYP associated with the
adapted nonterminal <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p9.m6" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>. As <cite class="ltx_cite"/> explain, such
Pitman-Yor Processes naturally generate power-law distributed data.</p>
</div>
<div id="S1.SS2.p10" class="ltx_para">
<p class="ltx_p">Informally, Adaptor Grammars can be viewed as caching entire subtrees
of the adapted nonterminals. Roughly speaking, the probability of
generating a particular subtree of an adapted nonterminal is
proportional to the number of times that subtree has been generated
before. This “rich get richer” behaviour causes the distribution of
subtrees to follow a power-law (the power is specified by the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p10.m1" class="ltx_Math" alttext="a_{X}" display="inline"><msub><mi>a</mi><mi>X</mi></msub></math>
parameter of the PYP). The PCFG rules expanding an adapted nonterminal <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p10.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>
define the “base distribution” of the associated DP or PYP, and the
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p10.m3" class="ltx_Math" alttext="a_{X}" display="inline"><msub><mi>a</mi><mi>X</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.SS2.p10.m4" class="ltx_Math" alttext="b_{X}" display="inline"><msub><mi>b</mi><mi>X</mi></msub></math> parameters determine how much mass is reserved for
“new” trees.</p>
</div>
<div id="S1.SS2.p11" class="ltx_para">
<p class="ltx_p">There are several different procedures for inferring the parse trees
and the rule probabilities given a corpus of strings:
<cite class="ltx_cite"/> describe a MCMC sampler and <cite class="ltx_cite"/>
describe a Variational Bayes procedure. We use the MCMC procedure
here since this has been successfully applied to word segmentation
problems in previous work <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Word segmentation with Adaptor Grammars</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Perhaps the simplest word segmentation model is the <em class="ltx_emph">unigram
model</em>, where utterances are modeled as sequences of words, and
where each word is a sequence of segments <cite class="ltx_cite">[]</cite>.
A unigram model can be expressed as an Adaptor Grammar with one
adapted non-terminal <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{\mathsf{Word}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi><mo mathcolor="#000073">¯</mo></munder></math> (we indicate adapted nonterminals
by underlining them in grammars here; regular expressions are expanded
into right-branching productions).</p>
<table id="Sx1.EGx4" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m1" class="ltx_Math" alttext="\displaystyle\mathsf{Sentence}" display="inline"><mi>𝖲𝖾𝗇𝗍𝖾𝗇𝖼𝖾</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Word}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖶𝗈𝗋𝖽</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
<tr id="S2.E3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Word}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Phone}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖯𝗁𝗈𝗇𝖾</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">The first rule (<a href="#S2.E2" title="(2) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) says that a sentence consists of one
or more <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m2" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math>s, while the second rule (<a href="#S2.E3" title="(3) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) states
that a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m3" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> consists of a sequence of one or more <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m4" class="ltx_Math" alttext="\mathsf{Phone}" display="inline"><mi>𝖯𝗁𝗈𝗇𝖾</mi></math>s;
we assume that there are rules expanding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m5" class="ltx_Math" alttext="\mathsf{Phone}" display="inline"><mi>𝖯𝗁𝗈𝗇𝖾</mi></math> into all possible
phones. Because <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m6" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> is an adapted nonterminal, the adaptor
grammar memoises <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m7" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> subtrees, which corresponds to learning
the phone sequences for the words of the language.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">The more sophisticated Adaptor Grammars discussed below can be
understood as specialising either the first or the second of the rules
in (<a href="#S2.E2" title="(2) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>–<a href="#S2.E3" title="(3) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The next two subsections
review the Adaptor Grammar word segmentation models presented in
<cite class="ltx_cite"/> and <cite class="ltx_cite"/>: section <a href="#S2.SS1" title="2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>
reviews how phonotactic syllable-structure constraints can be
expressed with Adaptor Grammars, while section <a href="#S2.SS2" title="2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> reviews
how phrase-like units called “collocations” capture inter-word
dependencies. Section <a href="#S2.SS3" title="2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> presents the major novel
contribution of this paper by explaining how we modify these adaptor
grammars to capture some of the special properties of function words.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Syllable structure and phonotactics</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">The rule (<a href="#S2.E3" title="(3) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) models words as sequences of independently
generated phones: this is what <cite class="ltx_cite"/> called the
“monkey model” of word generation (it instantiates the metaphor that
word types are generated by a monkey randomly banging on the keys of a
typewriter). However, the words of a language are typically composed
of one or more syllables, and explicitly modelling the internal
structure of words typically improves word segmentation considerably.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> suggested replacing (<a href="#S2.E3" title="(3) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) with the
following model of word structure:</p>
<table id="Sx1.EGx5" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E4.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Word}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E4.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Syllable}^{1:4}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾</mi><mrow><mn>1</mn><mo>:</mo><mn>4</mn></mrow></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
<tr id="S2.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E5.m1" class="ltx_Math" alttext="\displaystyle\mathsf{Syllable}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E5.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{Onset})\,\mathsf{Rhyme}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖮𝗇𝗌𝖾𝗍</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><mi>𝖱𝗁𝗒𝗆𝖾</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
<tr id="S2.E6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E6.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Onset}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖮𝗇𝗌𝖾𝗍</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E6.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Consonant}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
<tr id="S2.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m1" class="ltx_Math" alttext="\displaystyle\mathsf{Rhyme}" display="inline"><mi>𝖱𝗁𝗒𝗆𝖾</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Nucleus}\,(\mathsf{%&#10;Coda})" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mpadded width="+1.7pt"><mi>𝖭𝗎𝖼𝗅𝖾𝗎𝗌</mi></mpadded></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>𝖢𝗈𝖽𝖺</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
<tr id="S2.E8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E8.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Nucleus}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖭𝗎𝖼𝗅𝖾𝗎𝗌</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E8.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Vowel}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖵𝗈𝗐𝖾𝗅</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
<tr id="S2.E9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E9.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Coda}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝖽𝖺</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E9.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Consonant}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
</table>
<p class="ltx_p">Here and below superscripts indicate iteration (e.g., a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math>
consists of 1 to 4 <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="\mathsf{Syllable}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾</mi></math>s), while an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m3" class="ltx_Math" alttext="\mathsf{Onset}" display="inline"><mi>𝖮𝗇𝗌𝖾𝗍</mi></math> consists of
an unbounded number of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m4" class="ltx_Math" alttext="\mathsf{Consonant}" display="inline"><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi></math>s), while parentheses indicate
optionality (e.g., a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m5" class="ltx_Math" alttext="\mathsf{Rhyme}" display="inline"><mi>𝖱𝗁𝗒𝗆𝖾</mi></math> consists of an obligatory <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m6" class="ltx_Math" alttext="\mathsf{Nucleus}" display="inline"><mi>𝖭𝗎𝖼𝗅𝖾𝗎𝗌</mi></math>
followed by an optional <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m7" class="ltx_Math" alttext="\mathsf{Coda}" display="inline"><mi>𝖢𝗈𝖽𝖺</mi></math>). We assume that there are rules
expanding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m8" class="ltx_Math" alttext="\mathsf{Consonant}" display="inline"><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m9" class="ltx_Math" alttext="\mathsf{Vowel}" display="inline"><mi>𝖵𝗈𝗐𝖾𝗅</mi></math> to the set of all consonants
and vowels respectively (this amounts to assuming that the learner
can distinguish consonants from vowels). Because <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m10" class="ltx_Math" alttext="\mathsf{Onset}" display="inline"><mi>𝖮𝗇𝗌𝖾𝗍</mi></math>,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m11" class="ltx_Math" alttext="\mathsf{Nucleus}" display="inline"><mi>𝖭𝗎𝖼𝗅𝖾𝗎𝗌</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m12" class="ltx_Math" alttext="\mathsf{Coda}" display="inline"><mi>𝖢𝗈𝖽𝖺</mi></math> are adapted, this model learns the
possible syllable onsets, nucleii and coda of the language, even
though neither syllable structure nor word boundaries are explicitly
indicated in the input to the model.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">The model just described assumes that word-internal syllables have the
same structure as word-peripheral syllables, but in languages such as
English word-peripheral onsets and codas can be more complex than the
corresponding word-internal onsets and codas. For example, the word
“string” begins with the onset cluster str, which is relatively rare
word-internally. <cite class="ltx_cite"/> showed that word
segmentation accuracy improves if the model can learn different
consonant sequences for word-inital onsets and word-final codas. It
is easy to express this as an Adaptor Grammar: (<a href="#S2.E4" title="(4) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) is
replaced with (<a href="#S2.E10" title="(10) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>–<a href="#S2.E11" title="(11) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) and
(<a href="#S2.E12" title="(12) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>–<a href="#S2.E17" title="(17) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>) are added to the grammar.</p>
<table id="Sx1.EGx6" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E10.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Word}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E10.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{SyllableIF}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
<tr id="S2.E11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Word}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{SyllableI}\,\mathsf{%&#10;Syllable}^{0:2}\,\mathsf{SyllableF}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><mrow><mpadded width="+1.7pt"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨</mi></mpadded><mo>⁢</mo><mpadded width="+1.7pt"><msup><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾</mi><mrow><mn>0</mn><mo>:</mo><mn>2</mn></mrow></msup></mpadded><mo>⁢</mo><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖥</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
<tr id="S2.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E12.m1" class="ltx_Math" alttext="\displaystyle\mathsf{SyllableIF}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E12.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{OnsetI})\,\mathsf{RhymeF}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖮𝗇𝗌𝖾𝗍𝖨</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><mi>𝖱𝗁𝗒𝗆𝖾𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
<tr id="S2.E13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E13.m1" class="ltx_Math" alttext="\displaystyle\mathsf{SyllableI}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E13.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{OnsetI})\,\mathsf{Rhyme}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖮𝗇𝗌𝖾𝗍𝖨</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><mi>𝖱𝗁𝗒𝗆𝖾</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(13)</span></td></tr>
<tr id="S2.E14" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E14.m1" class="ltx_Math" alttext="\displaystyle\mathsf{SyllableF}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖥</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E14.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{Onset})\,\mathsf{RhymeF}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖮𝗇𝗌𝖾𝗍</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><mi>𝖱𝗁𝗒𝗆𝖾𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(14)</span></td></tr>
<tr id="S2.E15" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E15.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{OnsetI}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖮𝗇𝗌𝖾𝗍𝖨</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E15.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Consonant}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(15)</span></td></tr>
<tr id="S2.E16" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E16.m1" class="ltx_Math" alttext="\displaystyle\mathsf{RhymeF}" display="inline"><mi>𝖱𝗁𝗒𝗆𝖾𝖥</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E16.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Nucleus}\,(\mathsf{%&#10;CodaF})" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mpadded width="+1.7pt"><mi>𝖭𝗎𝖼𝗅𝖾𝗎𝗌</mi></mpadded></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>𝖢𝗈𝖽𝖺𝖥</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(16)</span></td></tr>
<tr id="S2.E17" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E17.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{CodaF}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝖽𝖺𝖥</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E17.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Consonant}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗇𝗌𝗈𝗇𝖺𝗇𝗍</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(17)</span></td></tr>
</table>
<p class="ltx_p">In this grammar the suffix “<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m1" class="ltx_Math" alttext="\mathsf{I}" display="inline"><mi>𝖨</mi></math>” indicates a word-initial
element, and “<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m2" class="ltx_Math" alttext="\mathsf{F}" display="inline"><mi>𝖥</mi></math>” indicates a word-final element. Note that
the model simply has the ability to learn that different clusters can
occur word-peripherally and word-internally; it is not given any
information about the relative complexity of these clusters.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Collocation models of inter-word dependencies</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> point out the detrimental effect that inter-word
dependencies can have on word segmentation models that assume that the
words of an utterance are independently generated. Informally, a
model that generates words independently is likely to incorrectly
segment multi-word expressions such as “the doggie” as single words
because the model has no way to capture word-to-word dependencies,
e.g., that “doggie” is typically preceded by “the”. Goldwater et
al show that word segmentation accuracy improves when the model is
extended to capture bigram dependencies.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">Adaptor grammar models cannot express bigram dependencies, but they
can capture similiar inter-word dependencies using phrase-like units
that <cite class="ltx_cite"/> calls collocations. <cite class="ltx_cite"/> showed
that word segmentation accuracy improves further if the model learns a
nested hierarchy of collocations. This can be achieved by replacing
(<a href="#S2.E2" title="(2) ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) with (<a href="#S2.E18" title="(18) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>–<a href="#S2.E21" title="(21) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>).</p>
<table id="Sx1.EGx7" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E18" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E18.m1" class="ltx_Math" alttext="\displaystyle\mathsf{Sentence}" display="inline"><mi>𝖲𝖾𝗇𝗍𝖾𝗇𝖼𝖾</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E18.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Colloc3}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(18)</span></td></tr>
<tr id="S2.E19" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E19.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc3}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E19.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Colloc2}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(19)</span></td></tr>
<tr id="S2.E20" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E20.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc2}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E20.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Colloc1}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(20)</span></td></tr>
<tr id="S2.E21" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E21.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc1}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E21.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{Word}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖶𝗈𝗋𝖽</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(21)</span></td></tr>
</table>
<p class="ltx_p">Informally, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m1" class="ltx_Math" alttext="\mathsf{Colloc1}" display="inline"><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m2" class="ltx_Math" alttext="\mathsf{Colloc2}" display="inline"><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m3" class="ltx_Math" alttext="\mathsf{Colloc3}" display="inline"><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi></math> define a
nested hierarchy of phrase-like units. While not designed to
correspond to syntactic phrases, by examining the sample parses induced
by the Adaptor Grammar we noticed that the collocations often
correspond to noun phrases, prepositional phrases or verb phrases.
This motivates the extension to the Adaptor Grammar discussed below.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Incorporating “function words” into collocation models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">The starting point and baseline for our extension is the adaptor
grammar with syllable structure phonotactic constraints and three
levels of collocational structure (<a href="#S2.E5" title="(5) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>-<a href="#S2.E21" title="(21) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>), as
prior work has found that this yields the highest word segmentation
token f-score <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">Our extension assumes that the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m1" class="ltx_Math" alttext="\mathsf{Colloc1}-\mathsf{Colloc3}" display="inline"><mrow><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo>-</mo><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi></mrow></math> constituents
are in fact phrase-like, so we extend the rules
(<a href="#S2.E19" title="(19) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>–<a href="#S2.E21" title="(21) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>) to permit an optional sequence of
monosyllabic words at the left edge of each of these constituents.
Our model thus captures two of the properties of function words
discussed in section <a href="#S1.SS1" title="1.1 Psychological evidence for the role of function words in word learning ‣ 1 Introduction ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.1</span></a>: they are monosyllabic (and
thus phonologically simple), and they appear on the periphery of
phrases. (We put “function words” in scare quotes below because
our model only approximately captures the linguistic
properties of function words).</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p">Specifically, we replace rules (<a href="#S2.E19" title="(19) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>–<a href="#S2.E21" title="(21) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>)
with the following sequence of rules:</p>
<table id="Sx1.EGx8" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E22" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E22.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc3}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E22.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{FuncWords3})\,\mathsf%&#10;{Colloc2}^{+}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟥</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><msup><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(22)</span></td></tr>
<tr id="S2.E23" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E23.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc2}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E23.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{FuncWords2})\,\mathsf%&#10;{Colloc1}^{+}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟤</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><msup><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(23)</span></td></tr>
<tr id="S2.E24" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E24.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{Colloc1}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E24.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}(\mathsf{FuncWords1})\,\mathsf%&#10;{Word}^{+}" display="inline"><mrow><mrow><mo>→</mo><mo>⁡</mo><mrow><mo>(</mo><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟣</mi><mo>)</mo></mrow></mrow><mo>⁢</mo><msup><mi>𝖶𝗈𝗋𝖽</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(24)</span></td></tr>
<tr id="S2.E25" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E25.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWords3}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟥</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E25.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{FuncWord3}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(25)</span></td></tr>
<tr id="S2.E26" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E26.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWord3}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E26.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{SyllableIF}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(26)</span></td></tr>
<tr id="S2.E27" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E27.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWords2}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟤</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E27.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{FuncWord2}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟤</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(27)</span></td></tr>
<tr id="S2.E28" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E28.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWord2}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟤</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E28.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{SyllableIF}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(28)</span></td></tr>
<tr id="S2.E29" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E29.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWords1}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟣</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E29.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{FuncWord1}^{+}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><msup><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣</mi><mo>+</mo></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(29)</span></td></tr>
<tr id="S2.E30" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E30.m1" class="ltx_Math" alttext="\displaystyle{\color{blue!45!black}\colorlet{pgfstrokecolor}{.}\underline{%&#10;\mathsf{FuncWord1}}}" display="inline"><munder accentunder="true"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣</mi><mo mathcolor="#000073">¯</mo></munder></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E30.m2" class="ltx_Math" alttext="\displaystyle\operatorname{\mathop{\rightarrow}}\mathsf{SyllableIF}" display="inline"><mrow><mo>→</mo><mo>⁡</mo><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(30)</span></td></tr>
</table>
<p class="ltx_p">This model memoises (i.e., learns) both the individual “function
words” and the sequences of “function words” that modify the
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m1" class="ltx_Math" alttext="\mathsf{Colloc1}-\mathsf{Colloc3}" display="inline"><mrow><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi><mo>-</mo><mi>𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi></mrow></math> constituents. Note also that “function
words” expand directly to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m2" class="ltx_Math" alttext="\mathsf{SyllableIF}" display="inline"><mi>𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥</mi></math>, which in turn expands to
a monosyllable with a word-initial onset and word-final coda. This
means that “function words” are memoised independently of the
“content words” that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m3" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> expands to; i.e., the model learns
distinct “function word” and “content word” vocabularies.
Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts a sample parse generated by
this grammar.</p>
</div>
<div id="S2.F1" class="ltx_figure">
<span class="ltx_inline-block" style="width:498.7pt;height:0px;vertical-align:-0.0pt;"><svg xmlns="http://www.w3.org/2000/svg" height="159" version="1.1" viewBox="-19 -162 371 159" width="371"><g transform="matrix(1 0 0 -1 0 -165)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g><g><g transform="matrix(1 0 0 1 136 -15)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="83">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m1" class="ltx_Math" alttext="\mathsf{Sentence}" display="inline"><mi>𝖲𝖾𝗇𝗍𝖾𝗇𝖼𝖾</mi></math></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 177 -18 L 177 -25" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 141 -39)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="73">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m2" class="ltx_Math" alttext="\mathsf{Colloc3}" display="inline"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟥</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 177 -42 L 97 -48" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 45 -62)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="104">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m3" class="ltx_Math" alttext="\mathsf{FuncWords3}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟥</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 97 -65 L 32 -72" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 -15 -86)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="93">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m4" class="ltx_Math" alttext="\mathsf{FuncWord3}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 32 -89 L 32 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 16 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="31">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">you</span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 97 -65 L 97 -72" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 50 -86)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="93">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m5" class="ltx_Math" alttext="\mathsf{FuncWord3}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 97 -89 L 97 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 76 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="42">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">want</span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 97 -65 L 162 -72" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 115 -86)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="93">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m6" class="ltx_Math" alttext="\mathsf{FuncWord3}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 162 -89 L 162 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 152 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="21">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">to</span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 177 -42 L 259 -48" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 222 -62)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="73">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m7" class="ltx_Math" alttext="\mathsf{Colloc2}" display="inline"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟤</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 259 -65 L 219 -72" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 182 -86)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="73">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m8" class="ltx_Math" alttext="\mathsf{Colloc1}" display="inline"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 219 -89 L 219 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 198 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="42">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m9" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 219 -112 L 219 -119" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 203 -133)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="31">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">see</span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 259 -65 L 299 -72" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 263 -86)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="73">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m10" class="ltx_Math" alttext="\mathsf{Colloc1}" display="inline"><mi mathcolor="#000073">𝖢𝗈𝗅𝗅𝗈𝖼𝟣</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 299 -89 L 273 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 221 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="104">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m11" class="ltx_Math" alttext="\mathsf{FuncWords1}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟣</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 273 -112 L 273 -119" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 226 -133)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="93">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m12" class="ltx_Math" alttext="\mathsf{FuncWord1}" display="inline"><mi mathcolor="#000073">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 273 -136 L 273 -143" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 257 -156)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="31">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">the</span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 299 -89 L 327 -95" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 306 -109)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="42">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text" style="color:#000073;text-decoration:underline;"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.pic1.m13" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi mathcolor="#000073">𝖶𝗈𝗋𝖽</mi></math></span></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.6pt"><path d="M 327 -112 L 327 -119" style="fill:none"/></g></g><g><g><g transform="matrix(1 0 0 1 306 -133)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 7)"><switch><foreignObject color="#000000" height="8" overflow="visible" width="42">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_italic">book</span></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></svg>
</span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> A sample parse generated by the
“function word” Adaptor Grammar with rules
(<a href="#S2.E10" title="(10) ‣ 2.1 Syllable structure and phonotactics ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>–<a href="#S2.E18" title="(18) ‣ 2.2 Collocation models of inter-word dependencies ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>) and
(<a href="#S2.E22" title="(22) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>–<a href="#S2.E30" title="(30) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">30</span></a>). To simplify the parse we
only show the root node and the adapted nonterminals, and replace
word-internal structure by the word’s orthographic form.</div>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p class="ltx_p">This grammar builds in the fact that function words appear on the
left periphery of phrases. This is true of languages such as
English, but is not true cross-linguistically. For comparison
purposes we also include results for a mirror-image model that
permits “function words” on the right periphery, a model which
permits “function words” on both the left and right periphery
(achieved by changing rules <a href="#S2.E22" title="(22) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>–<a href="#S2.E24" title="(24) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>),
as well as a model that analyses all words as monosyllabic.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p class="ltx_p">Section <a href="#S4" title="4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> explains how a learner could use Bayesian
model selection to determine that function words appear on the left
periphery in English by comparing the posterior probability of the
data under our “function word” Adaptor Grammar to that obtained
using a grammar which is identical except that rules
(<a href="#S2.E22" title="(22) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>–<a href="#S2.E24" title="(24) ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>) are replaced with the
mirror-image rules in which “function words” are attached to the
right periphery.</p>
</div>
<div id="S2.F2" class="ltx_figure">
<p class="ltx_p"><img src="P14-1027/image001.png" id="S2.F2.g1" class="ltx_graphics" width="195" height="345" alt=""/>
 
<img src="P14-1027/image002.png" id="S2.F2.g2" class="ltx_graphics" width="344" height="344" alt=""/></p>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>

Token and lexicon (i.e., type) f-score on the
<cite class="ltx_cite"/> corpus as a function of training data
size for the baseline model, the model where “function words” can
appear on the left periphery, a model where “function words” can
appear on the right periphery, and a model where “function words”
can appear on both the left and the right periphery. For comparison
purposes we also include results for a model that assumes that all
words are monosyllabic.

</div>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Word segmentation results</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">This section presents results of running our Adaptor Grammar models on
subsets of the <cite class="ltx_cite"/> corpus of child-directed
English. We use the Adaptor Grammar software available from
http://web.science.mq.edu.au/~mjohnson/ with the same settings as
described in <cite class="ltx_cite"/>, i.e., we perform Bayesian inference
with “vague” priors for all hyperparameters (so there are no
adjustable parameters in our models), and perform 8 different MCMC
runs of each condition with table-label resampling for 2,000 sweeps of the training data.
At every 10th sweep of the last 1,000 sweeps we use the model to segment
the entire corpus (even if it is only trained on a subset of it),
so we collect 800 sample segmentations of each utterance. The
most frequent segmentation in these 800 sample segmentations is the
one we score in the evaluations below.</p>
</div>
<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_bold">Model</span></th>
<th class="ltx_td ltx_align_center ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Token</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">f-score</span></td></tr>
</table><span class="ltx_text ltx_font_bold"></span></th>
<th class="ltx_td ltx_align_center ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Boundary</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">precision</span></td></tr>
</table><span class="ltx_text ltx_font_bold"></span></th>
<th class="ltx_td ltx_align_center ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Boundary</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">recall</span></td></tr>
</table><span class="ltx_text ltx_font_bold"></span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.872</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.918</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.956</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">+ left FWs</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.924</span></td>
<td class="ltx_td ltx_align_center">0.935</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.990</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b">+ left + right FWs</th>
<td class="ltx_td ltx_align_center ltx_border_b">0.912</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">0.957</span></td>
<td class="ltx_td ltx_align_center ltx_border_b">0.953</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Mean token f-scores and boundary precision and recall results
averaged over 8 trials, each consisting of 8 MCMC runs of models
trained and tested on the full <cite class="ltx_cite"/> corpus
(the standard deviations of all values are less than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m3" class="ltx_Math" alttext="0.006" display="inline"><mn>0.006</mn></math>; Wilcox
sign tests show the means of all token f-scores differ <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m4" class="ltx_Math" alttext="p&lt;\mbox{2e-4}" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mtext>2e-4</mtext></mrow></math>).
</div>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Word segmentation with “function word” models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Here we evaluate the word segmentations found by the “function word”
Adaptor Grammar model described in section <a href="#S2.SS3" title="2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> and
compare it to the baseline grammar with collocations and phonotactics
from <cite class="ltx_cite"/>.

Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3 Incorporating “function words” into collocation models ‣ 2 Word segmentation with Adaptor Grammars ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the standard token and
lexicon (i.e., type) f-score evaluations for word segmentations proposed
by these models <cite class="ltx_cite">[]</cite>, and
Table <a href="#S3.T1" title="Table 1 ‣ 3 Word segmentation results ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarises
the token and lexicon f-scores for the major models discussed in this
paper. It is interesting to note that adding “function words”
improves token f-score by more than 4%, corresponding to a 40%
reduction in overall error rate.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">When the training data is very small the Monosyllabic grammar produces
the highest accuracy results, presumably because a large proportion of
the words in child-directed speech are monosyllabic. However, at
around 25 sentences the more complex models that are capable of
finding multisyllabic words start to become more accurate.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">It’s interesting that after about 1,000 sentences the model that
allows “function words” only on the right periphery is considerably
less accurate than the baseline model. Presumably this is because it
tends to misanalyse multi-syllabic words on the right periphery as
sequences of monosyllabic words.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">The model that allows “function words” only on the left periphery is
more accurate than the model that allows them on both the left and
right periphery when the input data ranges from about 100 to
about 1,000 sentences, but when the training data is larger than
about 1,000 sentences both models are equally accurate.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Content and function words found by “function word” model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">As noted earlier, the “function word” model generates function words
via adapted nonterminals other than the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> category.

In order
to better understand just how the model works, we
give the 5 most frequent words in each word category found during
8 MCMC runs of the left-peripheral “function word” grammar above:</p>
<dl id="I2" class="ltx_description">
<dt id="I2.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_description"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix1.m1" class="ltx_Math" alttext="\mathsf{Word}:" display="inline"><mrow><mi>𝖶𝗈𝗋𝖽</mi><mo>:</mo><mi/></mrow></math></span></dt>
<dd class="ltx_item">
<div id="I2.ix1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_slanted">book, doggy, house, want, I</span></p>
</div></dd>
<dt id="I2.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_description"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix2.m1" class="ltx_Math" alttext="\mathsf{FuncWord1}:" display="inline"><mrow><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣</mi><mo>:</mo><mi/></mrow></math></span></dt>
<dd class="ltx_item">
<div id="I2.ix2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_slanted">a, the, your, little<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_text ltx_font_upright">
The phone ‘l’ is generated by both </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix2.p1.m1" class="ltx_Math" alttext="\mathsf{Consonant}" display="inline"><mi mathvariant="normal">Consonant</mi></math><span class="ltx_text ltx_font_upright"> and </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix2.p1.m2" class="ltx_Math" alttext="\mathsf{Vowel}" display="inline"><mi mathvariant="normal">Vowel</mi></math><span class="ltx_text ltx_font_upright">,
so “little” can be (incorrectly) analysed as one syllable.</span></span></span></span></span>, <span class="ltx_text ltx_font_slanted">in</span></p>
</div></dd>
<dt id="I2.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_description"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix3.m1" class="ltx_Math" alttext="\mathsf{FuncWord2}:" display="inline"><mrow><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟤</mi><mo>:</mo><mi/></mrow></math></span></dt>
<dd class="ltx_item">
<div id="I2.ix3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_slanted">to, in, you, what, put</span></p>
</div></dd>
<dt id="I2.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_description"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix4.m1" class="ltx_Math" alttext="\mathsf{FuncWord3}:" display="inline"><mrow><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi><mo>:</mo><mi/></mrow></math></span></dt>
<dd class="ltx_item">
<div id="I2.ix4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_slanted">you, a, what, no, can</span></p>
</div></dd>
</dl>
<p class="ltx_p">Interestingly, these categories seem fairly reasonable. The
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="\mathsf{Word}" display="inline"><mi>𝖶𝗈𝗋𝖽</mi></math> category includes open-class nouns and verbs, the
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m3" class="ltx_Math" alttext="\mathsf{FuncWord1}" display="inline"><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣</mi></math> category includes noun modifiers such as determiners,
while the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m4" class="ltx_Math" alttext="\mathsf{FuncWord2}" display="inline"><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟤</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m5" class="ltx_Math" alttext="\mathsf{FuncWord3}" display="inline"><mi>𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥</mi></math> categories include
prepositions, pronouns and auxiliary verbs.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Thus, the present model, initially aimed at segmenting words from
continuous speech, shows three interesting characteristics that are
also exhibited by human infants: it distinguishes between function
words and content words <cite class="ltx_cite">[]</cite>, it allows learners to
acquire at least some of the function words of their language (e.g. <cite class="ltx_cite">[]</cite>);
and furthermore, it may also allow them to start grouping together
function words according to their category <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Are “function words” on the left or right periphery?</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We have shown that a model that expects function words on the left
periphery performs more accurate word segmentation on English, where
function words do indeed typically occur on the left periphery, leaving
open the question: how could a learner determine whether
function words generally appear on the left or the right periphery of
phrases in the language they are learning?
This question is important because
knowing the side where function words preferentially occur is related
to the question of the direction of syntactic headedness in the
language, and an accurate method for identifying the location of
function words might be useful for initialising a syntactic learner.
Experimental evidence suggests that infants as young
as 8 months of age already expect function words on the correct side
for their language — left-periphery for Italian infants and
right-periphery for Japanese infants <cite class="ltx_cite">[]</cite> — so it
is interesting to see whether purely distributional learners such as
the ones studied here can identify the correct location of function
words in phrases.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">We experimented with a variety of approaches that use a single adaptor
grammar inference process, but none of these were successful. For
example, we hoped that given an Adaptor Grammar that permits
“function words” on both the left and right periphery, the inference
procedure would decide that the right-periphery rules simply are not
used in a language like English. Unfortunately we did not find this
in our experiments; the right-periphery rules were used almost as often
as the left-periphery rules (recall that a large fraction of the words
in English child-directed speech are monosyllabic).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">In this section, we show that learners could
use Bayesian model selection to determine that function words appear
on the left periphery in English by comparing the marginal probability
of the data for the left-periphery and the right-periphery models.</p>
</div>
<div id="S4.F3" class="ltx_figure"><img src="P14-1027/image003.png" id="S4.F3.g1" class="ltx_graphics" width="677" height="677" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>  Bayes factor in favour of left-peripheral
“function word” attachment as a function of the number of
sentences in the training corpus, calculated using the Harmonic Mean
estimator (see warning in text).</div>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">Instead, we used Bayesian model
selection techniques to determine whether left-peripheral or a
right-peripheral model better fits the unsegmented utterances that
constitute the training data.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>
Note that neither the left-peripheral nor the right-peripheral model
is correct: even strongly left-headed languages like
English typically contain a few right-headed constructions. For
example, “ago” is arguably the head of the phrase “ten years ago”.
</span></span></span>
While Bayesian model selection is in
principle straight-forward, it turns out to require the ratio of two
integrals (for the “evidence” or marginal likelihood) that are often
intractable to compute.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">Specifically, given a training corpus <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> of unsegmented sentences and
model families <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m2" class="ltx_Math" alttext="G_{1}" display="inline"><msub><mi>G</mi><mn>1</mn></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m3" class="ltx_Math" alttext="G_{2}" display="inline"><msub><mi>G</mi><mn>2</mn></msub></math> (here the “function word” adaptor
grammars with left-peripheral and right-peripheral attachment
respectively), the Bayes factor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m4" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> is the ratio of the marginal
likelihoods of the data:</p>
<table id="Sx1.EGx9" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex4.m1" class="ltx_Math" alttext="\displaystyle K" display="inline"><mi>K</mi></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex4.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex4.m3" class="ltx_Math" alttext="\displaystyle\frac{\mathrm{P}(D\mid G_{1})}{\mathrm{P}(D\mid G_{2})}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>∣</mo><msub><mi>G</mi><mn>1</mn></msub><mo>)</mo></mrow></mrow><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>∣</mo><msub><mi>G</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where the marginal likelihood or “evidence” for a model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m5" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> is
obtained by integrating over all of the hidden or latent structure
and parameters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m6" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>:</p>
<table id="Sx1.EGx10" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.E31" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E31.m1" class="ltx_Math" alttext="\displaystyle\mathrm{P}(D\mid G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>∣</mo><mi>G</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E31.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E31.m3" class="ltx_Math" alttext="\displaystyle\int_{\Delta}\mathrm{P}(D,\boldsymbol{\theta}\mid G)\,d%&#10;\boldsymbol{\theta}" display="inline"><mrow><mstyle displaystyle="true"><msub><mo largeop="true" symmetric="true">∫</mo><mi mathvariant="normal">Δ</mi></msub></mstyle><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>,</mo><mi>𝜽</mi><mo>∣</mo><mi>G</mi><mo rspace="4.2pt">)</mo></mrow><mi>d</mi><mi>𝜽</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(31)</span></td></tr>
</table>
<p class="ltx_p">Here the variable <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m7" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math> ranges over the space <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m8" class="ltx_Math" alttext="\Delta" display="inline"><mi mathvariant="normal">Δ</mi></math> of all
possible parses for the utterances in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m9" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> and all possible
configurations of the Pitman-Yor processes and their parameters that
constitute the “state” of the Adaptor Grammar <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m10" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math>. While the
probability of any specific Adaptor Grammar configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m11" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>
is not too hard to calculate (the MCMC sampler for Adaptor Grammars
can print this after each sweep through <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m12" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>), the integral in
(<a href="#S4.E31" title="(31) ‣ 4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">31</span></a>) is in general intractable.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">Textbooks such as <cite class="ltx_cite"/> describe a number of methods for
calculating <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m1" class="ltx_Math" alttext="\mathrm{P}(D\mid G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>∣</mo><mi>G</mi><mo>)</mo></mrow></mrow></math>, but most of them assume that the parameter
space <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m2" class="ltx_Math" alttext="\Delta" display="inline"><mi mathvariant="normal">Δ</mi></math> is continuous and so cannot be directly applied here.
The Harmonic Mean estimator (<a href="#S4.Ex5" title="4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) for (<a href="#S4.E31" title="(31) ‣ 4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">31</span></a>), which
we used here, is a popular estimator for (<a href="#S4.E31" title="(31) ‣ 4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">31</span></a>) because it
only requires the ability to calculate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m3" class="ltx_Math" alttext="\mathrm{P}(D,\boldsymbol{\theta}\mid G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>,</mo><mi>𝜽</mi><mo>∣</mo><mi>G</mi><mo>)</mo></mrow></mrow></math> for
samples from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m4" class="ltx_Math" alttext="\mathrm{P}(\boldsymbol{\theta}\mid D,G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>𝜽</mi><mo>∣</mo><mi>D</mi><mo>,</mo><mi>G</mi><mo>)</mo></mrow></mrow></math>:</p>
<table id="Sx1.EGx11" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex5.m1" class="ltx_Math" alttext="\displaystyle\mathrm{P}(D\mid G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>∣</mo><mi>G</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex5.m2" class="ltx_Math" alttext="\displaystyle\approx" display="inline"><mo>≈</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex5.m3" class="ltx_Math" alttext="\displaystyle\left(\frac{1}{n}\sum_{i=1}^{n}\frac{1}{\displaystyle\mathrm{P}(D%&#10;,\boldsymbol{\theta}_{i}\mid G)}\right)^{-1}" display="inline"><msup><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>n</mi></mfrac></mstyle><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>D</mi><mo>,</mo><msub><mi>𝜽</mi><mi>i</mi></msub><mo>∣</mo><mi>G</mi><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo>)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m5" class="ltx_Math" alttext="\boldsymbol{\theta}_{i},\ldots,\boldsymbol{\theta}_{n}" display="inline"><mrow><msub><mi>𝜽</mi><mi>i</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>𝜽</mi><mi>n</mi></msub></mrow></math> are <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> samples from
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m7" class="ltx_Math" alttext="\mathrm{P}(\boldsymbol{\theta}\mid D,G)" display="inline"><mrow><mi mathvariant="normal">P</mi><mrow><mo>(</mo><mi>𝜽</mi><mo>∣</mo><mi>D</mi><mo>,</mo><mi>G</mi><mo>)</mo></mrow></mrow></math>, which can be generated by the MCMC
procedure.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Are “function words” on the left or right periphery? ‣ Modelling function words improves unsupervised word segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts how the Bayes factor in favour of
left-peripheral attachment of “function words” varies as a function
of the number of utterances in the training data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p7.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> (calculated from
the last 1000 sweeps of 8 MCMC runs of the corresponding adaptor
grammars). As that figure shows, once the training data contains more
than about 1,000 sentences the evidence for the left-peripheral
grammar becomes very strong. On the full training data the estimated
log Bayes factor is over 6,000, which would constitute overwhelming
evidence in favour of left-peripheral attachment.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p class="ltx_p">Unfortunately, as Murphy and others warn, the Harmonic Mean estimator
is extremely unstable (Radford Neal calls it “the worst MCMC method
ever” in his blog), so we think it is important to confirm these results
using a more stable estimator. However, given the magnitude of the differences
and the fact that the two models being compared are of similar complexity,
we believe that these results suggest
that Bayesian model selection can be used to determine properties
of the language being learned.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusions and future work</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">This paper showed that the word segmentation accuracy of a state-of-the-art
Adaptor Grammar model is significantly improved by extending it so that
it explicitly models some properties of function words.
We also showed how Bayesian model selection can be used
to identify that function words appear on the left periphery of
phrases in English, even though the input to the model only consists
of an unsegmented sequence of phones.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">Of course this work only scratches the surface in terms of
investigating the role of function words in language acquisition. It
would clearly be very interesting to examine the performance of these
models on other corpora of child-directed English, as well as on
corpora of child-directed speech in other languages. Our evaluation
focused on word-segmentation, but we could also evaluate the effect
that modelling “function words” has on other aspects of the model,
such as its ability to learn syllable structure.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">The models of “function words” we investigated here only capture two
of the 7 linguistic properties of function words identified in
section 1 (i.e., that function words tend to be monosyllabic,
and that they tend to appear phrase-peripherally), so it would be
interesting to develop and explore models that capture other
linguistic properties of function words.

For example, following the suggestion by <cite class="ltx_cite"/>
that human learners use frequency cues
to identify function words, it might be interesting to develop
computational models that do the same thing.
In an Adaptor Grammar the frequency distribution of function words might be modelled
by specifying the prior for the Pitman-Yor Process parameters
associated with the function words’ adapted nonterminals
so that it prefers to generate a small number of high-frequency items.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">It should also be possible to develop models which capture the fact
that function words tend not to be topic-specific.

<cite class="ltx_cite"/> and <cite class="ltx_cite"/> show how Adaptor Grammars
can model the association between words and non-linguistic “topics”;
perhaps these models could be extended to capture some of the semantic
properties of function words.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p">It would also be interesting to further explore the extent to which
Bayesian model selection is a useful approach to linguistic
“parameter setting”. In order to do this it is imperative to
develop better methods than the problematic “Harmonic Mean”
estimator used here for calculating the evidence (i.e., the marginal
probability of the data) that can handle the combination of discrete
and continuous hidden structure that occur in computational linguistic
models.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p class="ltx_p">As well as substantially improving the accuracy of unsupervised word
segmentation, this work is interesting because it suggests a connection
between unsupervised word segmentation and the induction of syntactic
structure. It is reasonable to expect that hierarchical non-parametric
Bayesian models such as Adaptor Grammars may be useful tools for
exploring such a connection.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work was supported in part by the Australian Research Council’s
Discovery Projects funding scheme (project numbers DP110102506 and
DP110102593), the European Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la Recherche
(ANR-10-LABX-0087 IEC, and ANR-10-IDEX-0001-02 PSL*), and the Mairie
de Paris, Ecole des Hautes Etudes en Sciences Sociales, the Ecole Normale
Supérieure, and the Fondation Pierre Gilles de Gennes.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 17:25:11 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
