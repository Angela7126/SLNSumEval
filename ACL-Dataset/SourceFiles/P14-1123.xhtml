<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring</title>
<!--Generated on Tue Jun 10 19:03:55 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Suma Bhat
<br class="ltx_break"/>Beckman Institute,
<br class="ltx_break"/>University of Illinois,
<br class="ltx_break"/>Urbana, IL 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">spbhat2@illinois.edu</span> 
<br class="ltx_break"/>&amp;Huichao Xue
<br class="ltx_break"/>Dept. of Computer Science
<br class="ltx_break"/>University of Pittsburgh
<br class="ltx_break"/>Pittsburgh, PA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">hux10@cs.pitt.edu</span> 
<br class="ltx_break"/>&amp;Su-Youn Yoon
<br class="ltx_break"/> Educational Testing Service
<br class="ltx_break"/>Princeton, NJ 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">syoon@ets.org</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Designing measures that capture various aspects of language ability is a
central task in the design of systems for automatic scoring of spontaneous speech. In this study, we address a key aspect of language
proficiency assessment – syntactic complexity. We propose a novel measure of
syntactic complexity for spontaneous speech that shows optimum empirical
performance on real world data in multiple ways. First, it is both <span class="ltx_text ltx_font_italic">robust</span>
and <span class="ltx_text ltx_font_italic">reliable</span>, producing automatic scores that agree well with human
rating compared to the state-of-the-art. Second, the measure makes sense
theoretically, both from algorithmic and native language acquisition points of
view.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Assessment of a speaker’s proficiency in a second language is the main task in the domain of automatic evaluation of spontaneous speech <cite class="ltx_cite">[<a href="#bib.bib197" title="Automatic scoring of non-native spontaneous speech in tests of spoken english" class="ltx_ref">38</a>]</cite>. Prior studies in language acquisition and second language research have conclusively shown that proficiency in a second language is characterized by several factors, some of which are, fluency in language production, pronunciation accuracy, choice of vocabulary, grammatical sophistication and accuracy. The design of automated scoring systems for non-native speaker speaking proficiency is guided by these studies in the choice of pertinent objective measures of these key aspects of language proficiency.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The focus of this study is the design and performance analysis of a measure of the syntactic complexity of non-native English responses for use in automatic scoring systems.
The state-of-the art automated scoring system for spontaneous speech <cite class="ltx_cite">[<a href="#bib.bib197" title="Automatic scoring of non-native spontaneous speech in tests of spoken english" class="ltx_ref">38</a>, <a href="#bib.bib316" title="A three-stage approach to the automated scoring of spontaneous spoken responses" class="ltx_ref">15</a>]</cite> currently uses measures of fluency and pronunciation (acoustic aspects) to produce scores that are in reasonable agreement with human-rated scores of proficiency. Despite its good performance, there is a need to extend its coverage to higher order aspects of language ability. Fluency and pronunciation may, by themselves, already be good indicators of proficiency in non-native speakers, but from a construct validity perspective<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>Construct validity is the degree to which a test measures what it claims, or purports, to be measuring and an important criterion in the development and use of assessments or tests.</span></span></span>, it is necessary that an automatic assessment model measure higher-order aspects of language proficiency. Syntactic complexity is one such aspect of proficiency. By “syntactic complexity”, we mean a learner’s ability to use a wide range of sophisticated grammatical structures.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">This study is different from studies that focus on capturing grammatical errors in non-native speakers <cite class="ltx_cite">[<a href="#bib.bib304" title="The influence of planning and task type on second language performance" class="ltx_ref">12</a>, <a href="#bib.bib296" title="Assessed levels of second language speaking proficiency: how distinct?" class="ltx_ref">17</a>]</cite>. Instead of focusing on grammatical errors that are found to be highly representative of language proficiency, our interest is in capturing the <em class="ltx_emph">range</em> of forms that surface in language production and the degree of <em class="ltx_emph">sophistication</em> of such forms, collectively referred to as <span class="ltx_text ltx_font_italic">syntactic complexity</span> in <cite class="ltx_cite">[<a href="#bib.bib303" title="Syntactic complexity measures and their relationship to L2 proficiency: a research synthesis of college–level L2 writing" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">The choice and design of objective measures of language proficiency is governed by two crucial constraints:</p>
<ol id="I1" class="ltx_enumerate">[noitemsep,nolistsep]

<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">Validity: a measure should show high discriminative ability
between various levels of language proficiency, and the scores produced by the use of this measure should show high agreement with human-assigned scores.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">Robustness: a measures should
be derived automatically and should be robust to errors in the measure generation process.</p>
</div></li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">A critical impediment to the robustness constraint in the state-of-the-art is the multi-stage automated process, where errors in the speech recognition stage (the very first stage) affect subsequent stages. Guided by studies in second language development, we design a measure of syntactic complexity that captures patterns indicative of proficient and non-proficient grammatical structures by a shallow-analysis of spoken language, as opposed to a deep syntactic analysis, and analyze the performance of the automatic scoring model with its inclusion. We compare and contrast the proposed measure with that found to be optimum in <cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">Our primary contributions in this study are:</p>
<ul id="I2" class="ltx_itemize">[noitemsep,nolistsep]

<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p">We show that the measure of syntactic complexity derived from a shallow-analysis of spoken utterances satisfies the design constraint of high discriminative ability between proficiency levels. In addition, including our proposed measure of syntactic complexity in an automatic scoring model results in a statistically significant performance gain over the state-of-the-art.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p">The proposed measure, derived through a completely automated process, satisfies the robustness criterion reasonably well.</p>
</div></li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p">In the domain of native language acquisition, the presence or absence of a grammatical structure indicates grammatical development. We observe that the proposed approach elegantly and effectively captures this presence-based criterion of grammatical development, since the feature indicative of presence or absence of a grammatical structure is optimal from an algorithmic point of view.</p>
</div></li>
</ul>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Speaking in a non-native language requires diverse abilities, including fluency, pronunciation, intonation,
grammar, vocabulary, and discourse. Informed by studies in second language acquisition and language testing that regard these factors as key determiners of spoken language proficiency, some researchers have focused on the objective measurement of these aspects of spoken language in the context of automatic assessment of language ability. Notable are studies that have focused on assessment of fluency <cite class="ltx_cite">[<a href="#bib.bib287" title="Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology" class="ltx_ref">9</a>, <a href="#bib.bib288" title="Quantitative assessment of second language learners’ fluency: comparisons between read and spontaneous speech" class="ltx_ref">10</a>]</cite>,
pronunciation <cite class="ltx_cite">[<a href="#bib.bib289" title="Performance measures for phone-level pronunciation teaching in CALL" class="ltx_ref">33</a>, <a href="#bib.bib290" title="Use of the speech recognition in computer-assisted language learning" class="ltx_ref">34</a>, <a href="#bib.bib292" title="Automatic pronunciation scoring for language instruction" class="ltx_ref">13</a>, <a href="#bib.bib293" title="Automatic scoring of pronunciation quality" class="ltx_ref">23</a>]</cite>, and intonation
<cite class="ltx_cite">[<a href="#bib.bib197" title="Automatic scoring of non-native spontaneous speech in tests of spoken english" class="ltx_ref">38</a>]</cite>. The relative success of these studies has yielded objective measures of acoustic aspects of speaking ability, resulting in a shift in focus to more complex aspects of
assessment of grammar <cite class="ltx_cite">[<a href="#bib.bib294" title="Fluency and structural complexity as predictors of L2 oral proficiency" class="ltx_ref">2</a>, <a href="#bib.bib31" title="Detecting structural events for assessing non-native speech" class="ltx_ref">5</a>, <a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">6</a>]</cite>, topic development <cite class="ltx_cite">[<a href="#bib.bib291" title="Exploring content features for automated speech scoring" class="ltx_ref">36</a>]</cite>, and coherence
<cite class="ltx_cite">[<a href="#bib.bib298" title="Coherence modeling for the automated assessment of spontaneous spoken responses" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">In an effort to assess grammar and usage in a second language learning
environment, numerous studies have focused on identifying relevant
quantitative measures. These measures have been used to estimate
proficiency levels in English as a second language (ESL) writing with
reasonable success. <cite class="ltx_cite">Wolf-Quintero<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib302" title="Second language development in writing: measures of fluency, accuracy, and complexity" class="ltx_ref">1998</a>)</cite>, <cite class="ltx_cite">Ortega (<a href="#bib.bib303" title="Syntactic complexity measures and their relationship to L2 proficiency: a research synthesis of college–level L2 writing" class="ltx_ref">2003</a>)</cite>, and <cite class="ltx_cite">Lu (<a href="#bib.bib300" title="Automatic analysis of syntactic complexity in second language writing" class="ltx_ref">2010</a>)</cite> found that
measures such as mean length of T-unit<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>T-units are defined as
“the shortest grammatically allowable sentences into which writing
can be split.” <cite class="ltx_cite">[<a href="#bib.bib301" title="Grammatical structures written at three grade levels (ncte research report no. 3)" class="ltx_ref">16</a>]</cite></span></span></span> and dependent clauses
per clause (henceforth termed as length-based measures) are well
correlated with holistic proficiency scores suggesting that these
quantitative measures can be used as objective indices of grammatical
development.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">In the context of spoken ESL, these measures have been studied as well but the results have been inconclusive. The measures could only broadly discriminate between students’ proficiency levels, rated on a scale with moderate to weak correlations, and strong data dependencies on the participant groups were observed <cite class="ltx_cite">[<a href="#bib.bib295" title="Assessing oral proficiency: a comparison of holistic and objective measures" class="ltx_ref">14</a>, <a href="#bib.bib296" title="Assessed levels of second language speaking proficiency: how distinct?" class="ltx_ref">17</a>, <a href="#bib.bib297" title="Features of oral proficiency in task performance by efl and jfl learners" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">With the recent interest in the area of automatic assessment of speech, there is a concurrent need to assess the grammatical development of ESL students automatically. Studies that explored the applicability of length-based measures in an automated scoring system <cite class="ltx_cite">[<a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">6</a>, <a href="#bib.bib31" title="Detecting structural events for assessing non-native speech" class="ltx_ref">5</a>]</cite> observed another important drawback of these measures in that setting. Length-based measures do not meet the constraints of the design, that, in order for measures to be effectively incorporated in the automated speech scoring system, they must be generated in a fully automated manner, via a multi-stage automated process that includes speech recognition, part of speech (POS) tagging, and parsing.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">A major bottleneck in the multi-stage process of an automated speech scoring system for second language is the stage of automated speech recognition (ASR). Automatic recognition of non-native speakers’ spontaneous speech is a challenging task as evidenced by the error rate of the state-of-the-art speech recognizer. For instance, <cite class="ltx_cite">Chen and Zechner (<a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">2011</a>)</cite> reported a 50.5% word error rate (WER) and <cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite> reported a 30% WER in the recognition of ESL students’ spoken responses. These high error rates at the recognition stage negatively affect the subsequent stages of the speech scoring system in general, and in particular, during a deep syntactic analysis, which operates on a long sequence of words as its context. As a result, measures of grammatical complexity that are closely tied to a correct syntactic analysis are rendered unreliable. Not surprisingly, <cite class="ltx_cite">Chen and Zechner (<a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">2011</a>)</cite> studied measures of grammatical complexity via syntactic parsing and found that a Pearson’s correlation coefficient of 0.49 between syntactic complexity measures (derived from manual transcriptions) and
proficiency scores, was drastically reduced to near non-existence
when the measures were applied to ASR word
hypotheses.
This suggests that measures
that rely on deep syntactic analysis are unreliable in current ASR-based scoring
systems for spontaneous speech.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">In order to avoid the problems encountered with deep
analysis-based measures, <cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite> explored a shallow
analysis-based approach, based on the assumption that the level of
grammar sophistication at each proficiency level is reflected in the distribution of
part-of-speech (POS) tag bigrams. The idea of capturing differences in POS tag distributions for classification has been explored in several previous studies. In the area of text-genre classification, POS tag distributions have been found to capture genre differences in text <cite class="ltx_cite">[<a href="#bib.bib319" title="Part-of-speech histograms for genre classification of text" class="ltx_ref">11</a>, <a href="#bib.bib321" title="Filtering web text to match target genres" class="ltx_ref">21</a>]</cite>; in a language testing context, it has been used in grammatical error detection and essay scoring <cite class="ltx_cite">[<a href="#bib.bib318" title="An unsupervised method for detecting grammatical errors" class="ltx_ref">7</a>, <a href="#bib.bib322" title="The ups and downs of preposition error detection in ESL writing" class="ltx_ref">31</a>]</cite>. We will see next what aspects of syntactic complexity are captured by such a shallow-analysis.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Shallow-analysis approach to measuring syntactic complexity</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">The measures of syntactic complexity in this approach are POS bigrams and are not obtained by a deep analysis (syntactic
parsing) of the structure of the sentence. Hence we will refer to this
approach as ‘shallow analysis’. In a shallow-analysis approach to measuring syntactic complexity, we rely on the distribution of POS bigrams at every proficiency level to be representative of the range and sophistication of grammatical constructions at that level. At the outset, POS-bigrams may seem too simplistic to represent any aspect of true syntactic complexity. We illustrate to the contrary, that they are indeed able to capture certain grammatical errors and sophisticated constructions by means of the following instances. Consider the two sentence fragments below taken from actual responses (the bigrams of interest and their associated POS tags are bold-faced).</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="I3" class="ltx_enumerate">[noitemsep,nolistsep]

<li id="I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p">They <em class="ltx_emph ltx_font_bold">can/MD to/TO</em> survive …</p>
</div></li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p">They created <em class="ltx_emph ltx_font_bold">the culture/NN that/WDT now/RB</em> is common in the US.</p>
</div></li>
</ol>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">We notice that Example <a href="#I3.i1" title="1. ‣ 3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is not only less grammatically sophisticated than Example <a href="#I3.i2" title="2. ‣ 3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> but also has a grammatical error. The error stems from the fact that it has a modal verb followed by the word “to”. On the other hand, Example <a href="#I3.i2" title="2. ‣ 3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> contains a relative clause composed of a noun introduced by “that”. Notice how these grammatical expressions (one erroneous and the other sophisticated) can be detected by the POS bigrams “MD-TO” and “NN-WDT”, respectively.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">The idea that the level of syntactic complexity (in terms of its range and sophistication) can be assessed based
on the distribution of POS-tags is informed by prior studies in second
language acquisition. It has been shown that the usage of certain
grammatical constructions (such as that of the embedded relative
clause in the second sentence above) are indicators of specific
milestones in grammar development <cite class="ltx_cite">[<a href="#bib.bib185" title="How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale" class="ltx_ref">8</a>]</cite>. In addition,
studies such as <cite class="ltx_cite">Foster and Skehan (<a href="#bib.bib304" title="The influence of planning and task type on second language performance" class="ltx_ref">1996</a>)</cite> have successfully explored the utility
of frequency of grammatical errors as objective measures of
grammatical development.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">Based on this idea, <cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite> developed a set of features of syntactic complexity based on POS
sequences extracted from a large corpus of ESL learners’ spoken responses, grouped by human-assigned scores of proficiency level. Unlike previous studies, it did not rely on the occurrence of <span class="ltx_text ltx_font_italic">normative</span> grammatical constructions. The main assumption was that each score level is characterized by different types of prominent grammatical structures. These representative constructions are gathered from a collection of ESL learners’ spoken responses rated for overall proficiency. The syntactic complexity of a test spoken response was estimated based on its similarity to the proficiency groups in the
reference corpus with respect to the score-specific constructions. A score was assigned to the response based on
how similar it was to the high score group. In Section <a href="#S4.SS1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we go over the approach in further detail.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">Our current work is inspired by the shallow analysis-based approach of
<cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite> and operates under the same assumptions of capturing the range and sophistication of grammatical constructions at each score level. However, the approaches differ in the way in which a spoken response is assigned to a score group. We first analyze
the limitations of the model studied in <cite class="ltx_cite">[<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">37</a>]</cite> and
then describe how our model can address those
limitations. The result is a new measure based on POS bigrams
to assess ESL learners’ mastery of syntactic complexity.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Models for Measuring Grammatical Competence</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We mentioned that the measure proposed in this study is derived from assumptions similar to those studied in <cite class="ltx_cite">[<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">37</a>]</cite>. Accordingly, we will summarize the previously studied model, outline its limitations, show how our proposed measure addresses those limitations and compare the two measures for the task of automatic scoring of speech.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Vector-Space Model based approach</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite> explored an approach inspired by information retrieval. They treat the concatenated collection of responses from a particular score-class as a ‘super’ document. Then, regarding POS bigrams as terms, they construct POS-based vector space models for each score-class (there are four score classes denoting levels of proficiency as will be explained in Section <a href="#S5.SS2" title="5.2 Data ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>), thus yielding four score-specific vector-space models (VSMs). The terms of the VSM are weighted by the term frequency-inverse document frequency (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m1" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m2" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math>) weighting scheme <cite class="ltx_cite">[<a href="#bib.bib306" title="A vector space model for automatic indexing" class="ltx_ref">29</a>]</cite>. The intuition behind the approach is that responses in the
same proficiency level often share similar grammar and usage
patterns. The similarity between a test response and
a score-specific vector is then calculated by a cosine similarity
metric.
Although a total of 4 cosine similarity scores
(one per score group) were generated, only
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m3" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>from among the four similarity scores, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m4" class="ltx_Math" alttext="cosmax" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></math>, were selected as features.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ul id="I4" class="ltx_itemize">[noitemsep,nolistsep]

<li id="I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I4.i1.p1" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I4.i1.p1.m1" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>: the cosine similarity score between the test response and the vector of POS bigrams for the highest score class (level 4); and,</p>
</div></li>
<li id="I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I4.i2.p1" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I4.i2.p1.m1" class="ltx_Math" alttext="cosmax" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></math>: the score level of the VSM with which the given response shows maximum similarity.</p>
</div></li>
</ul>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">Of these, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m1" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>was selected based on its empirical performance (it showed the
strongest correlation with human-assigned scores of proficiency among the distance-based measures). In
addition, an intuitive justification for the choice is that the
score-4 vector is a grammatical “norm” representing the <span class="ltx_text ltx_font_italic">average</span>
grammar usage distribution of the most proficient ESL students. The measure of syntactic complexity of a response, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m2" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>, is its similarity to the highest score class.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">The study found that the measures showed reasonable discriminative ability across proficiency levels. Despite its encouraging empirical performance, the VSM method of capturing grammatical sophistication
has the following limitations.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p class="ltx_p">First, the VSM-based method is likely to over-estimate the contribution of the POS bigrams when highly correlated bigrams occur as terms in the VSM. Consider the presence of a grammar pattern represented by more than one POS bigram. For example, both “NN-WDT” and “WDT-RB” in Sentence <a href="#I3.i2" title="2. ‣ 3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reflect the learner’s usage of a relative
clause. However, we note that the two bigrams are correlated and including them both results in an over-estimation of their contribution. The VSM set-up has no mechanism to handle correlated features.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p class="ltx_p">Second, the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m1" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m2" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> weighting scheme for relatively rare POS bigrams does not adequately capture their underlying distribution
with respect to score groups. Grammatical expressions that occur frequently in one score level but rarely in other levels can be assumed to be characteristic of a specific score level. Therefore, the more uneven the distribution of a grammatical expression across score classes, the more important that grammatical expression should be as an indicator of a particular score class. However, the simple <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m3" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> scheme cannot capture this uneven
distribution. A pattern that occurs rarely but uniformly across
different score groups can get the same weight as a pattern which is
unevenly distributed to one score group. <cite class="ltx_cite">Martineau and Finin (<a href="#bib.bib299" title="Delta tfidf: an improved feature space for sentiment analysis." class="ltx_ref">2009</a>)</cite>
observed this weakness of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m4" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m5" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> weighting in the domain of sentiment analysis. When using <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m6" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m7" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> weighting to extract words that were strongly associated with positive sentiment in a movie review corpus (they considered each review as a document and a word as a term), it was found that a substantial proportion of words with the highest <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m8" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p6.m9" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> were rare words (e.g., proper nouns) which were not directly associated with the sentiment.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p class="ltx_p">We propose to address these important limitations of the VSM approach by the use of a method that accounts for each of the deficiencies. This is done by resorting to a maximum entropy model based approach, to which we turn next.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Maximum Entropy-Based model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">In order to address the limitations discussed in <a href="#S4.SS1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we propose a classification-based approach. Taking an approach different from previous studies, we formulate the task of assigning a score of syntactic complexity to a spoken response as a classification problem: given a spoken response, assign the response to a proficiency class. A classifier is trained in an inductive fashion, using a large corpus of learner responses that is divided into proficiency scores as the training data and then used to test data that is similar to the training data. A distinguishing feature of the current study is that the measure is based on a comparison of characteristics of the test response to models trained on large amounts of data from each score point, as opposed to measures that are simply characteristics of the responses themselves (which is how measures have been considered in prior studies).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">The inductive classifier we use here is the maximum-entropy model (MaxEnt) which has been used to solve several statistical natural language processing problems with much success <cite class="ltx_cite">[<a href="#bib.bib305" title="A maximum entropy approach to natural language processing" class="ltx_ref">1</a>, <a href="#bib.bib314" title="Exploiting diverse knowledge sources via maximum entropy in named entity recognition" class="ltx_ref">3</a>, <a href="#bib.bib312" title="A maximum entropy approach to named entity recognition" class="ltx_ref">4</a>, <a href="#bib.bib310" title="Thumbs up?: sentiment classification using machine learning techniques" class="ltx_ref">25</a>, <a href="#bib.bib313" title="Named entity recognition with character-level models" class="ltx_ref">19</a>, <a href="#bib.bib311" title="Adaptive statistical language modeling: a maximum entropy approach" class="ltx_ref">28</a>]</cite>. The productive feature engineering aspects of incorporating features into the discriminative MaxEnt classifier motivate the model choice for the problem at hand. In particular, the ability of the MaxEnt model’s estimation routine to handle overlapping (correlated) features makes it directly applicable to address the first limitation of the VSM model. The second limitation, related to the ineffective weighting of terms via the the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="tf" display="inline"><mrow><mi>t</mi><mo>⁢</mo><mi>f</mi></mrow></math>-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m2" class="ltx_Math" alttext="idf" display="inline"><mrow><mi>i</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>f</mi></mrow></math> scheme, seems to be addressed by the fact that the MaxEnt model assigns a weight to each feature (in our case, POS bigrams) on a per-class basis (in our case, score group), by taking every instance into consideration. Therefore, a MaxEnt model has an advantage over the model described in <a href="#S4.SS1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> in that it uses four different weight schemes (one per score level) and each scheme is optimized for each score level. This is beneficial in situations where the features are not evenly important across all score levels.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">Our experiments seek answers to the following questions.</p>
<ol id="I5" class="ltx_enumerate">[noitemsep,nolistsep]

<li id="I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I5.i1.p1" class="ltx_para">
<p class="ltx_p">To what extent does a MaxEnt-score of syntactic complexity discriminate between levels of proficiency?</p>
</div></li>
<li id="I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I5.i2.p1" class="ltx_para">
<p class="ltx_p">What is the effect of including the proposed measure of syntactic complexity in the state-of-the-art automatic scoring model?</p>
</div></li>
<li id="I5.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I5.i3.p1" class="ltx_para">
<p class="ltx_p">How robust is the measure to errors in the various stages of automatic generation?</p>
</div></li>
</ol>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Tasks</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">In order to answer the motivating questions of the study, we set-up two tasks. In the first task, we compare the extent to which the VSM-based measure and the MaxEnt-based measure (outlined in <a href="#S4.SS1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a href="#S4.SS2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> above) discriminate between levels of syntactic complexity. Additionally, we compare the performance of an automatic scoring model of overall proficiency that includes the measures of syntactic complexity from each of the two models being compared and analyze the gains with respect to the state-of-the-art. In the second task, we study the measures’ robustness to errors incurred by ASR.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Data</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">In this study, we used a collection of responses from an international English language assessment. The assessment consisted of questions to which speakers were prompted to provide spontaneous spoken responses lasting approximately 45-60 seconds per question. Test takers read and/or listened to stimulus materials and then responded to questions based on the stimuli. All questions solicited spontaneous, unconstrained natural speech.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">A small portion of the available data with inadequate audio quality and lack of student response was excluded from the study. The remaining responses were partitioned into two datasets: the ASR set
and the scoring model training/test (SM) set. The ASR set, with
47,227 responses, was used for ASR training and POS similarity
model training. The SM set, with 2,950 responses, was used
for feature evaluation and automated scoring model evaluation. There
was no overlap in speakers between the ASR set and the SM set.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">Each response was rated for overall proficiency by trained human scorers using
a 4-point scoring scale, where 1 indicates low speaking proficiency
and 4 indicated high speaking proficiency. The distribution of proficiency scores, along with other details of the data sets, are presented in
Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Data ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S5.T1" class="ltx_table">
<span class="ltx_inline-block ltx_align_center" style="width:433.6pt;height:0px;vertical-align:-0.0pt;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Data set</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">No. of</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">No. of</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span class="ltx_text ltx_font_tiny">Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span class="ltx_text ltx_font_tiny">Score distribution</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">responses</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">speakers</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Mean</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">SD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">ASR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">47,227</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">7,872</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">1,953</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">16,834</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">23,106</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">5,334</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">36%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">49%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">11%</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">SM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2,950</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">500</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">0.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">166</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">1,103</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">1,385</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">296</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">37%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">47%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_tiny">10%</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
</span>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data size and score distribution</div>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">As seen in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Data ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, there is a strong bias towards the middle scores (score 2 and 3) with approximately 84-85% of the responses belonging to these two score levels. Although the skewed distribution limits the number of score-specific instances for the highest and lowest scores available for model training, we used the data without modifying the distribution since it is representative of responses in a large-scale language assessment scenario.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p class="ltx_p">Human raters’ extent of agreement in the subjective task of rating responses for language proficiency constrains the extent to which we can expect a machine’s score to agree with that of humans. An estimate of the extent to which human raters agree on the subjective task of proficiency assessment, is obtained by two raters scoring approximately 5% of data (2,388 responses from ASR set and 140 responses from SM set). Pearson correlation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p5.m1" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math> between the scores assigned by the two raters was 0.62 in ASR set and 0.58 in SM set. This level of agreement will guide the evaluation of the human-machine agreement on scores.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Stages of Automatic Grammatical Competence Assessment</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">Here we outline the multiple stages involved in the automatic syntactic complexity assessment.
The first stage, ASR, yields an automatic transcription, which is followed by the POS tagging stage. Subsequently, the feature extraction stage (a VSM or a MaxEnt model as the case may be) generates the syntactic complexity feature which is then incorporated in a multiple linear regression model to generate a score.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">The steps for automatic assessment of overall proficiency follow an analogous process (either including the POS tagger or not), depending on the objective measure being evaluated. The various objective measures are then combined in the multiple regression scoring model to generate an overall score of proficiency.</p>
</div>
<div id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Automatic Speech Recognizer</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">An HMM recognizer was trained using ASR set (approximately 733 hours of non-native speech collected from 7,872 speakers). A gender independent triphone acoustic model and combination of bigram, trigram, and four-gram language models were used. A word error rate (WER) of 31% on the SM dataset was observed.</p>
</div>
</div>
<div id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>POS tagger</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">POS tags were generated using the POS tagger implemented in the
Open-NLP toolkit<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>http://opennlp.apache.org</span></span></span>. It was trained
on the Switchboard (SWBD) corpus. This POS tagger was trained on about
528K word/tag pairs. A combination of 36 tags from the Penn Treebank
tag set and 6 tags generated for spoken languages were used in the
tagger.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p">The tagger achieved a tagging accuracy of 96.3% on a
Switchboard evaluation set composed of 379K words, suggesting high accuracy of the tagger. However, due to substantial amount of speech recognition errors in our data, the POS error rate (resulting from the combined errors of ASR and automated POS tagger) is expected to be higher.</p>
</div>
</div>
<div id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>VSM-based Model</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p">We used the ASR data set to train a POS-bigram VSM for the highest score class and generated <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.SSS3.p1.m1" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>  and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.SSS3.p1.m2" class="ltx_Math" alttext="cosmax" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></math>  reported in <cite class="ltx_cite">Yoon and Bhat (<a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">2012</a>)</cite>, for the SM data set as outlined in Section <a href="#S4.SS1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
</div>
<div id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.3.4 </span>Maximum Entropy Model Classifier</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p class="ltx_p">The input to the classifier is a set of POS bigrams (1366 bigrams in all) obtained from the POS-tagged output of the data. We considered binary-valued features (whether a POS bigram occurred or not), occurrence frequency, and relative frequency as input for the purpose of experimentation. We used the maximum entropy classifier implementation in the MaxEnt toolkit<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html</span></a>.</span></span></span>. The classifier was trained using the LBFGS algorithm for parameter estimation and used equal-scale gaussian priors for smoothing. The results that follow are based on MaxEnt classifier’s parameter settings initialized to zero. Since a preliminary analysis of the effect of varying the feature (binary or frequency) revealed that the binary-valued feature was optimal (in terms of yielding the best agreement between human and machine scores), we only report our results for this case. The ASR data set was used to train the MaxEnt classifier and the features generated from the SM data set were used for evaluation.</p>
</div>
<div id="S5.SS3.SSS4.p2" class="ltx_para">
<p class="ltx_p">One straightforward way of using the maximum entropy classifier’s
prediction for our case is to directly use its predicted score-level – 1, 2, 3 or 4.
However, this forces the classifier to make a coarse-grained choice and may over-penalize the classifier’s scoring errors.
To illustrate this, consider a scenario where the
classifier assigns two responses A and B to score level 2 (based on the maximum <em class="ltx_emph">a posteriori</em> condition). Suppose that, for response A, the score class with the second highest probability corresponds to score level 1 and that, for response B, it corresponds to score level 3. It is apparent that the classifier
has an overall tendency to assign a higher score to
B, but looking at its top preference alone (2 for both responses),
masks this tendency.</p>
</div>
<div id="S5.SS3.SSS4.p3" class="ltx_para">
<p class="ltx_p">We thus capture the classifier’s finer-grained scoring tendency by
calculating the <span class="ltx_text ltx_font_italic">expected value</span> of the classifier output. For a given response, the MaxEnt classifier
calculates the conditional probability of a score-class given the response, in turn yielding conditional probabilities of each score group given the observation – <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.SSS4.p3.m1" class="ltx_Math" alttext="p_{i}" display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> for score group
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.SSS4.p3.m2" class="ltx_Math" alttext="i\in\{1,2,3,4\}" display="inline"><mrow><mi>i</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn></mrow><mo>}</mo></mrow></mrow></math>. In our case, we consider the predicted score of syntactic complexity to be the expected value of the class label given the observation as,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.SSS4.p3.m3" class="ltx_Math" alttext="{mescore}=1\times p_{1}+2\times p_{2}+3\times p_{3}+4\times p_{4}." display="inline"><mrow><mrow><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow><mo>=</mo><mrow><mrow><mn>1</mn><mo>×</mo><msub><mi>p</mi><mn>1</mn></msub></mrow><mo>+</mo><mrow><mn>2</mn><mo>×</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>+</mo><mrow><mn>3</mn><mo>×</mo><msub><mi>p</mi><mn>3</mn></msub></mrow><mo>+</mo><mrow><mn>4</mn><mo>×</mo><msub><mi>p</mi><mn>4</mn></msub></mrow></mrow></mrow><mo>.</mo></mrow></math>
This permits us to better represent the score assigned by the MaxEnt
classifier as a relative preference over score
assignments.</p>
</div>
</div>
<div id="S5.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">5.3.5 </span>Automatic Scoring System</h4>

<div id="S5.SS3.SSS5.p1" class="ltx_para">
<p class="ltx_p">We consider a multiple regression automatic scoring model as studied in <cite class="ltx_cite">Zechner<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib197" title="Automatic scoring of non-native spontaneous speech in tests of spoken english" class="ltx_ref">2009</a>), Chen and Zechner (<a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">2011</a>), Higgins<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib316" title="A three-stage approach to the automated scoring of spontaneous spoken responses" class="ltx_ref">2011</a>)</cite>. In its state-of-the-art set-up, the following
model uses the features – HMM acoustic model score (global normalized), speaking rate, word types per second, average
chunk length in words and language model
score (global normalized). We use these features by themselves (<span class="ltx_text ltx_font_bold">Base</span>), and also in conjunction with the VSM-based feature (<span class="ltx_text ltx_font_bold">cva4</span>) and the MaxEnt-based feature
(<span class="ltx_text ltx_font_bold">mescore</span>).</p>
</div>
</div>
</div>
<div id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.4 </span>Evaluation Metric</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">We evaluate the measures using the metrics chosen in previous studies <cite class="ltx_cite">[<a href="#bib.bib197" title="Automatic scoring of non-native spontaneous speech in tests of spoken english" class="ltx_ref">38</a>, <a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">6</a>, <a href="#bib.bib28" title="Assessment of esl learners’ syntactic competence based on similarity measures" class="ltx_ref">37</a>]</cite>. A measure’s utility has been evaluated according to its ability to discriminate between levels of proficiency assigned by human raters. This is done by considering the Pearson correlation coefficient between the feature and the human scores. In an ideal situation, we would have compared machine score with scores of grammatical skill assigned by human raters. In our case, however, with only access to the overall proficiency scores, we use scores of language proficiency as those of grammatical skill.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p class="ltx_p">A criterion for evaluating the performance of the scoring model is the extent to which the automatic scores of overall proficiency agree with the human scores. As in prior studies, here too the level of agreement is evaluated by means of the weighted
kappa measure as well as unrounded and rounded Pearson’s correlations between machine
and human scores (since the output of the regression model can either
be rounded or regarded as is). The feature that maximizes this degree of
agreement will be preferred.</p>
</div>
<div id="S5.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Features</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Manual Transcriptions</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">ASR</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m1" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.52</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m2" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.43</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m3" class="ltx_Math" alttext="cosmax" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">0.31</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 2: </span>Pearson correlation coefficients between measures and holistic proficiency scores. All values are significant at level 0.01. Only the measures <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m6" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi mathsize="normal" stretchy="false">c</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">o</mi><mo mathsize="small" stretchy="false">⁢</mo><msub><mi mathsize="normal" stretchy="false">s</mi><mn mathsize="normal" stretchy="false">4</mn></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m7" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi mathsize="normal" stretchy="false">m</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">e</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">s</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">c</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">o</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">r</mi><mo mathsize="small" stretchy="false">⁢</mo><mi mathsize="normal" stretchy="false">e</mi></mrow></math> were compared for robustness using manual and ASR transcriptions. </div>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Experimental Results</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">First, we compare the discriminative ability of measures of syntactic complexity (VSM-model based measure with that of the MaxEnt-based measure) across proficiency levels. Table <a href="#S5.T2" title="Table 2 ‣ 5.4 Evaluation Metric ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes our experimental results for this task. We notice that of the measures compared, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m1" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></math> shows the highest correlation with scores of syntactic complexity. The correlation was approximately 0.1 higher in absolute value than that of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m2" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>, which was the best performing feature in the VSM-based model and the difference is statistically significant.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">Seeking to study the robustness of the measures derived using a shallow analysis, we next compare the two measures studied here, with respect to the impact of speech recognition errors on their correlation with scores of syntactic complexity. Towards this end, we compare <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m1" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m2" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>when POS
bigrams are extracted from manual transcriptions (ideal ASR) and ASR transcriptions.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">In Table <a href="#S5.T2" title="Table 2 ‣ 5.4 Evaluation Metric ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, noticing that the correlations decrease going along a row, we can say that the errors in the ASR system caused both <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m1" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m2" class="ltx_Math" alttext="cos_{4}" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>s</mi><mn>4</mn></msub></mrow></math>to under-perform. However, the performance drop (around 0.05) resulting from a shallow analysis
is relatively small compared to the drop observed while employing
a deep syntactic analysis.
<cite class="ltx_cite">Chen and Zechner (<a href="#bib.bib32" title="Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech" class="ltx_ref">2011</a>)</cite> found that while using measures of syntactic complexity obtained from transcriptions,
errors in ASR transcripts caused over 0.40 drop in correlation from that found with manual transcriptions<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>Due to differences in the dataset and ASR system, a direct comparison between the current study and the cited prior study was not possible.</span></span></span>. This comparison suggests that the
current POS-based shallow analysis approach is more robust to ASR
errors compared to a syntactic analysis-based approach.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p">The effect of the measure of syntactic complexity is best studied by including it in an automatic scoring model of overall proficiency.
We compare the performance gains over the state-of-the-art with the inclusion of additional features (VSM-based and MaxEnt-based, in turn).
Table <a href="#S6.T3" title="Table 3 ‣ 6 Experimental Results ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the system performance with different grammar sophistication measures. The results reported are averaged over a 5-fold cross validation of the multiple regression model, where 80% of the SM data set is used to train the model and the evaluation is done using 20% of the data in every fold.</p>
</div>
<div id="S6.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Evaluation method</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Base</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Base+cos4</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Base+mescore</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Weighted </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T3.m1" class="ltx_Math" alttext="kappa" display="inline"><mrow><mi>k</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>p</mi><mo>⁢</mo><mi>p</mi><mo>⁢</mo><mi>a</mi></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.503</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.524</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">0.546</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Correlation (unrounded)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.548</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.562</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">0.592</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Correlation (rounded)</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">0.482</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">0.492</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">0.519</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of scoring model performances using features of
syntactic complexity studied in this paper along with those
available in the state-of-the-art. Here, <span class="ltx_text ltx_font_bold">Base</span> is the scoring model
without the measures of syntactic complexity. All correlations are
significant at level 0.01.</div>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p">As seen in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experimental Results ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, using the proposed measure, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m1" class="ltx_Math" alttext="mescore" display="inline"><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></math>,
leads to an improved agreement between human and machine scores of proficiency.
Comparing the unrounded correlation results in Table <a href="#S6.T3" title="Table 3 ‣ 6 Experimental Results ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we notice that
the model <span class="ltx_text ltx_font_bold">Base+mescore</span> shows the highest correlation of predicted
scores with human scores. In addition, we test the significance of the difference between two dependent correlations using Steiger’s Z-test (via the <span class="ltx_text ltx_font_typewriter">paired.r</span> function in the R statistical package <cite class="ltx_cite">[<a href="#bib.bib317" title="Psych: procedures for psychological, psychometric, and personality research" class="ltx_ref">26</a>]</cite>). We note that the performance gain of <span class="ltx_text ltx_font_bold">Base+mescore</span> over <span class="ltx_text ltx_font_bold">Base</span> as well as over <span class="ltx_text ltx_font_bold">Base + cos4</span> is
statistically significant at level = 0.01. The performance gain of <span class="ltx_text ltx_font_bold">Base+cos4</span> over <span class="ltx_text ltx_font_bold">Base</span>, however, is not statistically
significant at level = 0.01. Thus, the inclusion of the
MaxEnt-based measure of syntactic complexity results in improved agreement
between machine and human scores compared to the state-of-the-art
model (here, <span class="ltx_text ltx_font_bold">Base</span>).</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Discussions</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">We now discuss some of the observations and results of our study with respect to the following items.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Improved performance</span>: We sought to verify empirically that the MaxEnt model really outperforms the VSM in the case of correlated POS bigrams.
To see this, we separate the test set into three subsets <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m1" class="ltx_Math" alttext="A,B,C" display="inline"><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow></math>. Set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m2" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> contains responses where MaxEnt outperforms VSM; set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m3" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math> contains responses where VSM outperforms MaxEnt; set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m4" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> contains responses where their predictions are comparable. For each group of responses <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m5" class="ltx_Math" alttext="s\in\{A,B,C\}" display="inline"><mrow><mi>s</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow><mo>}</mo></mrow></mrow></math>, we calculate the percentage of responses <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m6" class="ltx_Math" alttext="P_{s}" display="inline"><msub><mi>P</mi><mi>s</mi></msub></math> where two highly correlated POS bigrams occur<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>We consider two POS bigrams to be highly correlated, when the their pointwise-mutual information is higher than 4.</span></span></span>. We found that the percentages follow the order: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m7" class="ltx_Math" alttext="P_{A}=12.93\%&gt;P_{C}=7.29\%&gt;P_{B}=4.41\%" display="inline"><mrow><msub><mi>P</mi><mi>A</mi></msub><mo>=</mo><mrow><mn>12.93</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow><mo>&gt;</mo><msub><mi>P</mi><mi>C</mi></msub><mo>=</mo><mrow><mn>7.29</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow><mo>&gt;</mo><msub><mi>P</mi><mi>B</mi></msub><mo>=</mo><mrow><mn>4.41</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></mrow></math>. This suggests that when correlated POS bigrams occur, MaxEnt is more likely to provide better score predictions than VSM does.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Feature design</span>: In the case of MaxEnt, the observation that binary-valued features (presence/absence of POS bigrams) yield better performance than features indicative of the occurrence frequency of the bigram has interesting implications. This was also observed in <cite class="ltx_cite">Pang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib310" title="Thumbs up?: sentiment classification using machine learning techniques" class="ltx_ref">2002</a>)</cite> where it was interpreted to mean that overall sentiment is indicated by the presence/absence of keywords, as opposed to topic of a text, which is indicated by the repeated use of the same or similar terms. An analogous explanation is applicable here.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">At first glance, the use of the presence/absence of grammatical structures may raise concerns about a potential loss of information (e.g. the distinction between an expression that is used once and another that is used multiple times is lost). However, when considered in the context of language acquisition studies, this approach seems to be justified. Studies in native language acquisition, have considered multiple grammatical developmental indices that represent the grammatical levels reached at various stages of language acquisition. For instance, <cite class="ltx_cite">Covington<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib185" title="How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale" class="ltx_ref">2006</a>)</cite> proposed the revised D-level scale which was originally studied by <cite class="ltx_cite">Rosenberg and Abbeduto (<a href="#bib.bib309" title="Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults" class="ltx_ref">1987</a>)</cite>. The D-Level Scale categorizes grammatical development into 8 levels according to the presence of a set of diverse grammatical expressions varying in difficulty (for example, level 0 consists of simple sentences, while level 5 consists of sentences joined by a subordinating conjunction). Similarly, <cite class="ltx_cite">Scarborough (<a href="#bib.bib308" title="Index of productive syntax" class="ltx_ref">1990</a>)</cite> proposed the Index of Productive Syntax (IPSyn), according to which, the presence of particular grammatical structures, from a list of 60 structures (ranging from simple ones such as including only subjects and verbs, to more complex constructions such as conjoined sentences) is evidence of language acquisition milestones.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p">Despite the functional differences between the indices, there is a fundamental operational similarity - that they both use the presence or absence of grammatical structures, rather than their occurrence count, as evidence of acquisition of certain grammatical levels.
The assumption that a presence-based view of grammatical level acquisition is also applicable to second language assessment helps validate our observation that binary-valued features yield a better performance when compared with frequency-valued features.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generalizability</span>: The training and test sets used in this study had similar underlying distributions – they both sought unconstrained responses to a set of items with some minor differences in item type. Looking ahead, an important question is the extent to which our measure is sensitive to a mismatch between training and test data.</p>
</div>
</div>
<div id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">8 </span>Conclusions</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">Seeking alternatives to measuring syntactic complexity of spoken responses via syntactic parsers, we study a shallow-analysis based approach for use in automatic scoring.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p class="ltx_p">Empirically, we show that the proposed measure, based on a maximum entropy classification, satisfied the constraints of the design of an objective measure to a high degree. In addition, the proposed measure was found to be relatively robust to ASR errors. The measure <span class="ltx_text ltx_font_italic">outperformed</span> a related measure of syntactic complexity (also based on shallow-analysis of spoken response) previously found to be well-suited for automatic scoring. Including the measure of syntactic complexity in an automatic scoring model resulted in statistically significant performance gains over the state-of-the-art. We also make an interesting observation that the impressionistic evaluation of syntactic complexity is better approximated by the presence or absence of grammar and usage patterns (and not by their frequency of occurrence), an idea supported by studies in native language acquisition.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib305" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. L. Berger, V. J. D. Pietra and S. A. D. Pietra</span><span class="ltx_text ltx_bib_year">(1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A maximum entropy approach to natural language processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational linguistics</span> <span class="ltx_text ltx_bib_volume">22</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 39–71</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib294" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Bernstein, J. Cheng and M. Suzuki</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fluency and structural complexity as predictors of L2 oral proficiency</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1241–1244</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib314" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Borthwick, J. Sterling, E. Agichtein and R. Grishman</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exploiting diverse knowledge sources via maximum entropy in named entity recognition</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib312" class="ltx_bibitem ltx_bib_thesis"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Borthwick</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A maximum entropy approach to named entity recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Ph.D. Thesis</span>, <span class="ltx_text ltx_bib_publisher">New York University</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Chen and S. Yoon</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Detecting structural events for assessing non-native speech</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">IUNLPBEA ’11</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 38–45</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781937284039</span>,
<a href="http://dl.acm.org/citation.cfm?id=2043132.2043137" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Chen and K. Zechner</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 722–731</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S5.SS3.SSS5.p1" title="5.3.5 Automatic Scoring System ‣ 5.3 Stages of Automatic Grammatical Competence Assessment ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.5</span></a>,
<a href="#S5.SS4.p1" title="5.4 Evaluation Metric ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>,
<a href="#S6.p3" title="6 Experimental Results ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span></li>
<li id="bib.bib318" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Chodorow and C. Leacock</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An unsupervised method for detecting grammatical errors</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 140–147</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib185" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. A. Covington, C. He, C. Brown, L. Naci and J. Brown</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How complex is that sentence? a proposed revision of the rosenberg and abbeduto d-level scale</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ReVision. Washington, DC http://www. ai. uga. edu/caspr/2006-01-Covington. pdf.(Accessed May 10, 2010.)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p4" title="3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S7.p4" title="7 Discussions ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib287" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Cucchiarini, H. Strik and L. Boves</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quantitative assessment of second language learners’ fluency by means of automatic speech recognition technology</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Journal of the Acoustical Society of America</span> <span class="ltx_text ltx_bib_volume">107</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 989–999</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib288" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Cucchiarini, H. Strik and L. Boves</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quantitative assessment of second language learners’ fluency: comparisons between read and spontaneous speech</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Journal of the Acoustical Society of America</span> <span class="ltx_text ltx_bib_volume">111</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 2862–2873</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib319" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Feldman, M.A. Marin, M. Ostendorf and M. R. Gupta</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Part-of-speech histograms for genre classification of text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 4781 –4784</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib304" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Foster and P. Skehan</span><span class="ltx_text ltx_bib_year">(1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The influence of planning and task type on second language performance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Studies in Second Language Acquisition</span> <span class="ltx_text ltx_bib_volume">18</span>, <span class="ltx_text ltx_bib_pages"> pp. 299–324</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.p4" title="3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib292" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Franco, L. Neumeyer, Y. Kim and O. Ronen</span><span class="ltx_text ltx_bib_year">(1997)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic pronunciation scoring for language instruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1471–1474</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib295" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. B. Halleck</span><span class="ltx_text ltx_bib_year">(1995)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Assessing oral proficiency: a comparison of holistic and objective measures</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Modern Language Journal</span> <span class="ltx_text ltx_bib_volume">79</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 223–234</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib316" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Higgins, X. Xi, K. Zechner and D. Williamson</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A three-stage approach to the automated scoring of spontaneous spoken responses</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Speech &amp; Language</span> <span class="ltx_text ltx_bib_volume">25</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 282–306</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S5.SS3.SSS5.p1" title="5.3.5 Automatic Scoring System ‣ 5.3 Stages of Automatic Grammatical Competence Assessment ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.5</span></a>.
</span></li>
<li id="bib.bib301" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. W. Hunt</span><span class="ltx_text ltx_bib_year">(1965)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Grammatical structures written at three grade levels (ncte research report no. 3)</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Champaign, IL: National Council of Teachers of English</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib296" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Iwashita, A. Brown, T. McNamara and S. O’Hagan</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Assessed levels of second language speaking proficiency: how distinct?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Applied Linguistics</span> <span class="ltx_text ltx_bib_volume">29</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 24–49</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p3" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib297" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Iwashita</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Features of oral proficiency in task performance by efl and jfl learners</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 32–47</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib313" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Klein, J. Smarr, H. Nguyen and C. D. Manning</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Named entity recognition with character-level models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 180–183</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib300" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Lu</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic analysis of syntactic complexity in second language writing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Corpus Linguistics</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 474–496</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib321" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Marin, S. Feldman, M. Ostendorf and M. R. Gupta</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Filtering web text to match target genres</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3705–3708</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib299" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Martineau and T. Finin</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Delta tfidf: an improved feature space for sentiment analysis.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p6" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib293" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Neumeyer, H. Franco, V. Digalakis and M. Weintraub</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic scoring of pronunciation quality</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Speech Communication</span>, <span class="ltx_text ltx_bib_pages"> pp. 88–93</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib303" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Ortega</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Syntactic complexity measures and their relationship to L2 proficiency: a research synthesis of college–level L2 writing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Applied Linguistics</span> <span class="ltx_text ltx_bib_volume">24</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 492–518</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p2" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib310" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Pang, L. Lee and S. Vaithyanathan</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Thumbs up?: sentiment classification using machine learning techniques</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 79–86</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S7.p3" title="7 Discussions ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib317" class="ltx_bibitem ltx_bib_manual"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Revelle</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Psych: procedures for psychological, psychometric, and personality research</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Northwestern University</span>, <span class="ltx_text ltx_bib_place">Evanston, Illinois</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">R package version 1.2.1</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://personality-project.org/r/psych.manual.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.p5" title="6 Experimental Results ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span></li>
<li id="bib.bib309" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Rosenberg and L. Abbeduto</span><span class="ltx_text ltx_bib_year">(1987)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Indicators of linguistic competence in the peer group conversational behavior of mildly retarded adults</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Applied Psycholinguistics</span> <span class="ltx_text ltx_bib_volume">8</span>, <span class="ltx_text ltx_bib_pages"> pp. 19–32</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p4" title="7 Discussions ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib311" class="ltx_bibitem ltx_bib_thesis"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Rosenfeld</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adaptive statistical language modeling: a maximum entropy approach</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Ph.D. Thesis</span>, <span class="ltx_text ltx_bib_publisher">IBM</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 Maximum Entropy-Based model ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib306" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Salton, A. Wong and C. Yang</span><span class="ltx_text ltx_bib_year">(1975)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A vector space model for automatic indexing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Communications of the ACM</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp. 613–620</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib308" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. S. Scarborough</span><span class="ltx_text ltx_bib_year">(1990)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Index of productive syntax</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Applied Psycholinguistics</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 1–22</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p4" title="7 Discussions ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib322" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. R. Tetreault and M. Chodorow</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The ups and downs of preposition error detection in ESL writing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 865–872</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p6" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib298" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wang, K. Evanini and K. Zechner</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Coherence modeling for the automated assessment of spontaneous spoken responses</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 814–819</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib289" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Witt and S. Young</span><span class="ltx_text ltx_bib_year">(1997)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Performance measures for phone-level pronunciation teaching in CALL</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 99–102</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib290" class="ltx_bibitem ltx_bib_thesis"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Witt</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Use of the speech recognition in computer-assisted language learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Unpublished dissertation</span>, <span class="ltx_text ltx_bib_publisher">Cambridge University Engineering department</span>, <span class="ltx_text ltx_bib_place">Cambridge, U.K.</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib302" class="ltx_bibitem ltx_bib_report"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Wolf-Quintero, S. Inagaki and H. Kim</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Second language development in writing: measures of fluency, accuracy, and complexity</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock">Technical Report <span class="ltx_text ltx_bib_number">17</span>,  <span class="ltx_text ltx_bib_publisher">Second Language Teaching and curriculum Center, The University of Hawai’i</span>,  <span class="ltx_text ltx_bib_place">Honolulu, HI</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib291" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Xie, K. Evanini and K. Zechner</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exploring content features for automated speech scoring</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 103–111</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Yoon and S. Bhat</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Assessment of esl learners’ syntactic competence based on similarity measures</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 600–608</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p5" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p5" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p6" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.p5" title="3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S3.p6" title="3 Shallow-analysis approach to measuring syntactic complexity ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S4.SS1.p1" title="4.1 Vector-Space Model based approach ‣ 4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.p1" title="4 Models for Measuring Grammatical Competence ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>,
<a href="#S5.SS3.SSS3.p1" title="5.3.3 VSM-based Model ‣ 5.3 Stages of Automatic Grammatical Competence Assessment ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.3</span></a>,
<a href="#S5.SS4.p1" title="5.4 Evaluation Metric ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
</span></li>
<li id="bib.bib197" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Zechner, D. Higgins, X. Xi and D. M. Williamson</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic scoring of non-native spontaneous speech in tests of spoken english</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Speech Communication</span> <span class="ltx_text ltx_bib_volume">51</span> (<span class="ltx_text ltx_bib_number">10</span>), <span class="ltx_text ltx_bib_pages"> pp. 883–895</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S5.SS3.SSS5.p1" title="5.3.5 Automatic Scoring System ‣ 5.3 Stages of Automatic Grammatical Competence Assessment ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3.5</span></a>,
<a href="#S5.SS4.p1" title="5.4 Evaluation Metric ‣ 5 Experimental Setup ‣ Shallow Analysis Based Assessment of Syntactic Complexity for Automated Speech Scoring" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 19:03:55 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
