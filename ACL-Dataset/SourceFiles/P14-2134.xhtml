<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Distributed Representations of Geographically Situated Language</title>
<!--Generated on Wed Jun 11 18:37:48 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Distributed Representations of Geographically Situated Language</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Bamman â€Chris Dyer â€Noah A. Smith 
<br class="ltx_break"/>School of Computer Science 
<br class="ltx_break"/>Carnegie Mellon University

<br class="ltx_break"/>Pittsburgh, PA 15213, USA 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">dbamman,cdyer,nasmith</span>}<span class="ltx_text ltx_font_typewriter">@cs.cmu.edu</span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We introduce a model for incorporating contextual information (such as geography) in learning vector-space representations of <em class="ltx_emph">situated</em> language.
In contrast to approaches to multimodal representation learning that have used properties of the <em class="ltx_emph">object</em> being described (such as its color), our model includes information about the <em class="ltx_emph">subject</em> (i.e., the speaker), allowing us to learn the contours of a wordâ€™s meaning that are shaped by the context in which it is uttered.
In a quantitative evaluation on the task of judging geographically informed semantic similarity
between representations learned from 1.1 billion words of geo-located tweets,
our joint model outperforms comparable independent models that learn meaning in isolation.</p>
</div><span class="ltx_ERROR undefined">\setitemize</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt<span class="ltx_ERROR undefined">\setenumerate</span>noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt</p>
</div>
<div id="S0.F1" class="ltx_figure"><span class="ltx_ERROR undefined ltx_centering">\includegraphics</span>
<p class="ltx_p ltx_align_center">[scale=1]images/geomodel.pdf</p>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">FigureÂ 1: </span>Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item #2) spoken in Alaska, along with a single output. Parameter matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S0.F1.m2" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> consists of the learned low-dimensional embeddings. 
</div>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The vast textual resources used in NLP â€“ newswire, web text, parliamentary proceedings â€“ can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word
is uttered by a particular person at a particular place and time.
In short: language is <em class="ltx_emph">situated</em>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions <cite class="ltx_cite">[]</cite>, learning correlations between words and socioeconomic variables <cite class="ltx_cite">[]</cite>;
and charting how new terms spread geographicallyÂ <cite class="ltx_cite">[]</cite>. These models can tell us that <em class="ltx_emph">hella</em> was (at one time) used most often by a particular demographic group in northern California, echoing earlier linguistic studies <cite class="ltx_cite">[]</cite>, and that <em class="ltx_emph">wicked</em> is used most often in New England <cite class="ltx_cite">[]</cite>;
and they have practical applications, facilitating tasks like text-based geolocation <cite class="ltx_cite">[]</cite>.
One desideratum that remains, however, is how the <em class="ltx_emph">meaning</em> of these terms is shaped by geographical influences â€“ while <em class="ltx_emph">wicked</em> is used throughout the United States to mean <em class="ltx_emph">bad</em> or <em class="ltx_emph">evil</em> (â€œhe is a wicked manâ€),
in New England it is used as an adverbial intensifier (â€œmy boyâ€™s wicked smartâ€).
In leveraging grounded social media to uncover linguistic variation, what we want to learn is how a wordâ€™s meaning is shaped by its geography.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaningÂ <cite class="ltx_cite">[, <em class="ltx_emph">inter alia</em>]</cite>. In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning;
while other work has used visual information to improve distributed representationsÂ <cite class="ltx_cite">[]</cite>, this work generally exploits information about the object being described (e.g., <em class="ltx_emph">strawberry</em> and a picture of a strawberry);
in contrast, we use information about the <em class="ltx_emph">speaker</em> to learn representations that vary according to contextual variables from the speakerâ€™s perspective.
Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a userÂ <cite class="ltx_cite">[]</cite>, our primary input is textual data, supplemented with metadata about the author and the moment of authorship.
This information enables learning models of word meaning that are sensitive to such factors, allowing us to distinguish, for example, between the usage of <em class="ltx_emph">wicked</em> in Massachusetts from the usage of that word elsewhere, and letting us better associate geographically grounded named entities (e.g, <em class="ltx_emph">Boston</em>) with their hypernyms (<em class="ltx_emph">city</em>) in their respective regions.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Model</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The model we introduce is grounded in the distributional hypothesisÂ <cite class="ltx_cite">[]</cite>, that two words are similar by appearing in the same kinds of contexts (where â€œcontextâ€ itself can be variously defined as the bag or sequence of tokens around a target word, either by linear distance or dependency path).
We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts.
For example:</p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">â€¢</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">my boyâ€™s <em class="ltx_emph">wicked</em> smart</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">â€¢</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">my boyâ€™s <em class="ltx_emph">hella</em> smart</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">â€¢</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">my boyâ€™s <em class="ltx_emph">very</em> smart</p>
</div></li>
</ul>
<p class="ltx_p">Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasksÂ <cite class="ltx_cite">[]</cite>, we use a simple, but state-of-the-art neural architectureÂ <cite class="ltx_cite">[]</cite> to learn low-dimensional real-valued representations of words.
The graphical form of this model is illustrated in figure <a href="#S0.F1" title="FigureÂ 1 â€£ Distributed Representations of Geographically Situated Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">This model corresponds to an extension of the â€œskip-gramâ€ language modelÂ <cite class="ltx_cite">[]</cite> (hereafter SGLM).
Given an input sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m1" class="ltx_Math" alttext="\bm{s}" display="inline"><mi>ğ’”</mi></math> and a context window of size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>, each word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m3" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is conditioned on in turn to predict the identities of all of the tokens within <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m4" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> words around it.
For a vocabulary <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m5" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math>, each input word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m6" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is represented as a one-hot vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m7" class="ltx_Math" alttext="\bm{w}_{i}" display="inline"><msub><mi>ğ’˜</mi><mi>i</mi></msub></math> of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m8" class="ltx_Math" alttext="|V|" display="inline"><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow></math>.
The SGLM has two sets of parameters.
The first is the representation matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m9" class="ltx_Math" alttext="W\in\mathbb{R}^{|V|\times k}" display="inline"><mrow><mi>W</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow><mo>Ã—</mo><mi>k</mi></mrow></msup></mrow></math>, which encodes the real-valued embeddings for each word in the vocabulary.
A matrix multiply <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m10" class="ltx_Math" alttext="\bm{h}=\bm{w}^{\top}W,\in\mathbb{R}^{k}" display="inline"><mrow><mi>ğ’‰</mi><mo>=</mo><msup><mi>ğ’˜</mi><mo>âŠ¤</mo></msup><mi>W</mi><mo>,</mo><mo>âˆˆ</mo><msup><mi>â„</mi><mi>k</mi></msup></mrow></math> serves to index the particular embedding for word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m11" class="ltx_Math" alttext="\bm{w}" display="inline"><mi>ğ’˜</mi></math>, which constitutes the modelâ€™s hidden layer.
To predict the value of the context word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m12" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> (again, a one-hot vector of dimensionality <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m13" class="ltx_Math" alttext="|V|" display="inline"><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow></math>), this hidden representation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m14" class="ltx_Math" alttext="\bm{h}" display="inline"><mi>ğ’‰</mi></math> is then multiplied by a second parameter matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m15" class="ltx_Math" alttext="X\in\mathbb{R}^{|V|\times k}" display="inline"><mrow><mi>X</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow><mo>Ã—</mo><mi>k</mi></mrow></msup></mrow></math>.
The final prediction over the output vocabulary is then found by passing this resulting vector through the softmax function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m16" class="ltx_Math" alttext="\bm{o}=\textrm{softmax}(X\bm{h})" display="inline"><mrow><mi>ğ’</mi><mo>=</mo><mrow><mtext>softmax</mtext><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>â¢</mo><mi>ğ’‰</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>, giving a vector in the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m17" class="ltx_Math" alttext="|V|" display="inline"><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow></math>-dimensional unit simplex.
Backpropagation using (input <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m18" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math>, output <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m19" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>) word tuples learns the values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m20" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> (the embeddings) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m21" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> (the output parameter matrix) that maximize the likelihood of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m22" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> (i.e., the context words) conditioned on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m23" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> (i.e., the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m24" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math>â€™s).
During backpropagation, the errors propagated are the difference between <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m25" class="ltx_Math" alttext="\bm{o}" display="inline"><mi>ğ’</mi></math> (a probability distribution with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m26" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> outcomes) and the true (one-hot) output <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m27" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Let us define a set of contextual variables <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’</mi></math>; in the experiments that follow, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m2" class="ltx_Math" alttext="\mathcal{C}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’</mi></math> is comprised solely of geographical state <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m3" class="ltx_Math" alttext="\mathcal{C}_{\mathit{state}}=\{\text{AK},\text{AL},\ldots,\text{WY}\}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ğ’</mi><mi>ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtext>AK</mtext><mo>,</mo><mtext>AL</mtext><mo>,</mo><mi mathvariant="normal">â€¦</mi><mo>,</mo><mtext>WY</mtext></mrow><mo>}</mo></mrow></mrow></math>) but could in principle include any number of features, such as calendar month, day of week, or other demographic variables of the speaker.
Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m4" class="ltx_Math" alttext="|\mathcal{C}|" display="inline"><mrow><mo fence="true">|</mo><mi class="ltx_font_mathcaligraphic">ğ’</mi><mo fence="true">|</mo></mrow></math> denote the sum of the cardinalities of all variables in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m5" class="ltx_Math" alttext="\mathcal{C}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’</mi></math> (i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m6" class="ltx_Math" alttext="51" display="inline"><mn>51</mn></math> states, including the District of Columbia).
Rather than using a single embedding matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m7" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> that contains low-dimensional representations for every word in the vocabulary, we define a global embedding matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m8" class="ltx_Math" alttext="W_{\mathit{main}}\in\mathbb{R}^{|V|\times k}" display="inline"><mrow><msub><mi>W</mi><mi>ğ‘šğ‘ğ‘–ğ‘›</mi></msub><mo>âˆˆ</mo><msup><mi>â„</mi><mrow><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow><mo>Ã—</mo><mi>k</mi></mrow></msup></mrow></math> and an additional <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m9" class="ltx_Math" alttext="|\mathcal{C}|" display="inline"><mrow><mo fence="true">|</mo><mi class="ltx_font_mathcaligraphic">ğ’</mi><mo fence="true">|</mo></mrow></math> such matrices (each again of size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m10" class="ltx_Math" alttext="|V|\times k" display="inline"><mrow><mrow><mo fence="true">|</mo><mi>V</mi><mo fence="true">|</mo></mrow><mo>Ã—</mo><mi>k</mi></mrow></math>, which capture the effect that each variable value has on each word in the vocabulary.
Given an input word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m11" class="ltx_Math" alttext="\bm{w}" display="inline"><mi>ğ’˜</mi></math> and set of active variable values <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m12" class="ltx_Math" alttext="\mathcal{A}" display="inline"><mi class="ltx_font_mathcaligraphic">ğ’œ</mi></math> (e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m13" class="ltx_Math" alttext="\mathcal{A}=\{\mathit{state}=\text{MA}\}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">ğ’œ</mi><mo>=</mo><mrow><mo>{</mo><mi>ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’</mi><mo>=</mo><mtext>MA</mtext><mo>}</mo></mrow></mrow></math>), we calculate the hidden layer <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m14" class="ltx_Math" alttext="\bm{h}" display="inline"><mi>ğ’‰</mi></math> as the sum of these independent embeddings: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m15" class="ltx_Math" alttext="\bm{h}=\bm{w}^{\top}W_{\mathit{main}}+\sum_{a\in\mathcal{A}}\bm{w}^{\top}W_{a}" display="inline"><mrow><mi>ğ’‰</mi><mo>=</mo><mrow><mrow><msup><mi>ğ’˜</mi><mo>âŠ¤</mo></msup><mo>â¢</mo><msub><mi>W</mi><mi>ğ‘šğ‘ğ‘–ğ‘›</mi></msub></mrow><mo>+</mo><mrow><msub><mo largeop="true" symmetric="true">âˆ‘</mo><mrow><mi>a</mi><mo>âˆˆ</mo><mi class="ltx_font_mathcaligraphic">ğ’œ</mi></mrow></msub><mrow><msup><mi>ğ’˜</mi><mo>âŠ¤</mo></msup><mo>â¢</mo><msub><mi>W</mi><mi>a</mi></msub></mrow></mrow></mrow></mrow></math>.
While the word <em class="ltx_emph">wicked</em> has a common low-dimensional representation in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m16" class="ltx_Math" alttext="W_{\mathit{main},\mathit{wicked}}" display="inline"><msub><mi>W</mi><mrow><mi>ğ‘šğ‘ğ‘–ğ‘›</mi><mo>,</mo><mi>ğ‘¤ğ‘–ğ‘ğ‘˜ğ‘’ğ‘‘</mi></mrow></msub></math> that is invoked for every instance of its use (regardless of the place), the corresponding vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m17" class="ltx_Math" alttext="W_{\text{MA},\mathit{wicked}}" display="inline"><msub><mi>W</mi><mrow><mtext>MA</mtext><mo>,</mo><mi>ğ‘¤ğ‘–ğ‘ğ‘˜ğ‘’ğ‘‘</mi></mrow></msub></math> indicates how that common representation should shift in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m18" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-dimensional space when used in Massachusetts.
Backpropagation functions as in standard SGLM, with gradient updates for each training example <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m19" class="ltx_Math" alttext="\{x,y\}" display="inline"><mrow><mo>{</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>}</mo></mrow></math> touching not only <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m20" class="ltx_Math" alttext="W_{\mathit{main}}" display="inline"><msub><mi>W</mi><mi>ğ‘šğ‘ğ‘–ğ‘›</mi></msub></math> (as in SGLM), but all active <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m21" class="ltx_Math" alttext="W_{\mathcal{A}}" display="inline"><msub><mi>W</mi><mi class="ltx_font_mathcaligraphic">ğ’œ</mi></msub></math> as well.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">The additional <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m1" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> embeddings we add lead to an increase in the number of total parameters by a factor of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m2" class="ltx_Math" alttext="|\mathcal{C}|" display="inline"><mrow><mo fence="true">|</mo><mi class="ltx_font_mathcaligraphic">ğ’</mi><mo fence="true">|</mo></mrow></math>.
To control for the extra degrees of freedom this entails, we add squared <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m3" class="ltx_Math" alttext="\ell_{2}" display="inline"><msub><mi mathvariant="normal">â„“</mi><mn>2</mn></msub></math> regularization to all parameters, using stochastic gradient descent for backpropagation with minibatch updates for the regularization term.
As in Mikolov et al.Â <cite class="ltx_cite">[]</cite>, we speed up computation using the hierarchical softmaxÂ <cite class="ltx_cite">[]</cite> on the output matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m4" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">This model defines a joint parameterization over all variable values in the data, where information from data originating in California, for instance, can influence the representations learned for Wisconsin;
a naive alternative would be to simply train individual models on each variable value (a â€œCaliforniaâ€ model using data only from California, etc.).
A joint model has three <em class="ltx_emph">a priori</em> advantages over independent models: (i) sharing data across variable values encourages representations across those values to be similar;
e.g., while <em class="ltx_emph">city</em> may be closer to <em class="ltx_emph">Boston</em> in Massachusetts and <em class="ltx_emph">Chicago</em> in Illinois, in both places it still generally connotes a <em class="ltx_emph">municipality</em>;
(ii) such sharing can mitigate data sparseness for less-witnessed areas;
and (iii) with a joint model, all representations are guaranteed to be in the same vector space and can therefore be compared to each other;
with individual models (each with different initializations), word vectors across different states may not be directly compared.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We evaluate our model by confirming its face validity in a qualitative analysis and estimating its accuracy at the quantitative task of judging geographically-informed semantic similarity.
We use 1.1 billion tokens from 93 million geolocated tweets gathered between September 1, 2011 and August 30, 2013 (approximately 127,000 tweets per day evenly sampled over those two years).
This data only includes tweets that have been geolocated to state-level granularity in the United States using high-precision pattern matching on the user-specified location field (e.g., â€œnew york nyâ€ <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>â†’</mo></math> NY, â€œchicagoâ€ <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>â†’</mo></math> IL, etc.).
As a preprocessing step, we identify a set of target multiword expressions in this corpus as the maximal sequence of adjectives + nouns with the highest pointwise mutual information;
in all experiments described below, we define the vocabulary <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> as the most frequent 100,000 terms (either unigrams or multiword expressions) in the total data, and set the dimensionality of the embedding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m4" class="ltx_Math" alttext="k=100" display="inline"><mrow><mi>k</mi><mo>=</mo><mn>100</mn></mrow></math>.
In all experiments, the contextual variable is the observed US state (including DC),
so that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m5" class="ltx_Math" alttext="|\mathcal{C}|=51" display="inline"><mrow><mrow><mo fence="true">|</mo><mi class="ltx_font_mathcaligraphic">ğ’</mi><mo fence="true">|</mo></mrow><mo>=</mo><mn>51</mn></mrow></math>;
the vector space representation of word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m6" class="ltx_Math" alttext="\bm{w}" display="inline"><mi>ğ’˜</mi></math> in state <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m7" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m8" class="ltx_Math" alttext="\bm{w}^{\top}W_{\mathit{main}}+\bm{w}^{\top}W_{s}" display="inline"><mrow><mrow><msup><mi>ğ’˜</mi><mo>âŠ¤</mo></msup><mo>â¢</mo><msub><mi>W</mi><mi>ğ‘šğ‘ğ‘–ğ‘›</mi></msub></mrow><mo>+</mo><mrow><msup><mi>ğ’˜</mi><mo>âŠ¤</mo></msup><mo>â¢</mo><msub><mi>W</mi><mi>s</mi></msub></mrow></mrow></math>.</p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Qualitative Evaluation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">To illustrate how the model described above can learn geographically-informed semantic representations of words, table <a href="#S3.T1" title="TableÂ 1 â€£ 3.1 Qualitative Evaluation â€£ 3 Evaluation â€£ Distributed Representations of Geographically Situated Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the terms with the highest cosine similarity to <em class="ltx_emph">wicked</em> in Kansas and Massachusetts after running our joint model on the full 1.1 billion words of Twitter data;
while <em class="ltx_emph">wicked</em> in Kansas is close to other evaluative terms like <em class="ltx_emph">evil</em> and <em class="ltx_emph">pure</em> and religious terms like <em class="ltx_emph">gods</em> and <em class="ltx_emph">spirit</em>, in Massachusetts it is most similar to other intensifiers like <em class="ltx_emph">super</em>, <em class="ltx_emph">ridiculously</em> and <em class="ltx_emph">insanely</em>.</p>
</div>
<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" colspan="2"><span class="ltx_text ltx_font_small">Kansas</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span class="ltx_text ltx_font_small">Massachusetts</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">term</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">cosine</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">term</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">cosine</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">wicked</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_tt"><span class="ltx_text ltx_font_small">1.000</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">wicked</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">1.000</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">evil</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.884</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">super</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.855</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">pure</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.841</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">ridiculously</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.851</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">gods</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.841</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">insanely</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.820</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">mystery</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.830</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">extremely</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.793</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">spirit</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.830</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">goddamn</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.781</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">king</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.828</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">surprisingly</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.774</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">above</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.825</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">kinda</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.772</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">righteous</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.823</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">#sarcasm</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.772</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">magic</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.822</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">sooooooo</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.770</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">TableÂ 1: </span> Terms with the highest cosine similarity to <em class="ltx_emph">wicked</em> in Kansas and Massachusetts.</div>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T2" title="TableÂ 2 â€£ 3.1 Qualitative Evaluation â€£ 3 Evaluation â€£ Distributed Representations of Geographically Situated Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> likewise presents the terms with the highest cosine similarity to <em class="ltx_emph">city</em> in both California and New York;
while the terms most evoked by <em class="ltx_emph">city</em> in California include regional locations like Chinatown, Los Angelesâ€™ South Bay and San Franciscoâ€™s East Bay, in New York the most similar terms include <em class="ltx_emph">hamptons</em>, <em class="ltx_emph">upstate</em> and <em class="ltx_emph">borough</em> (New York Cityâ€™s term of administrative division).</p>
</div>
<div id="S3.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" colspan="2"><span class="ltx_text ltx_font_small">California</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span class="ltx_text ltx_font_small">New York</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">term</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">cosine</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">term</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">cosine</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">city</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_tt"><span class="ltx_text ltx_font_small">1.000</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">city</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_small">1.000</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">valley</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.880</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">suburbs</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.866</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">bay</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.874</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">town</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.855</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">downtown</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.873</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">hamptons</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.852</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">chinatown</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.854</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">big city</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.842</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">south bay</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.854</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">borough</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.837</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">area</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.851</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">neighborhood</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.835</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">east bay</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.845</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">downtown</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.827</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">neighborhood</span></td>
<td class="ltx_td ltx_align_right ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.843</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">upstate</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.826</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">peninsula</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_rr ltx_border_t"><span class="ltx_text ltx_font_small">0.840</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">big apple</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.825</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">TableÂ 2: </span> Terms with the highest cosine similarity to <em class="ltx_emph">city</em> in California and New York.</div>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Quantitative Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">As a quantitative measure of our modelâ€™s performance, we consider the task of judging semantic similarity among words whose meanings are likely to evoke strong geographical correlations.
In the absence of a sizable number of linguistically interesting terms (like <em class="ltx_emph">wicked</em>) that are known to be geographically variable, we consider the proxy of estimating the named entities evoked by specific terms in different geographical regions.
As noted above, geographic terms like <em class="ltx_emph">city</em> provide one such example: in Massachusetts we expect the term <em class="ltx_emph">city</em> to be more strongly connected to grounded named entities like <em class="ltx_emph">Boston</em> than to other US cities.
We consider seven categories for which we can reasonably expect the connotations of each term to vary by geography; in each case, we calculate the distance between two terms <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> using representations learned for a given state (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m3" class="ltx_Math" alttext="\delta_{\mathit{state}}(x,y)" display="inline"><mrow><msub><mi>Î´</mi><mi>ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></math>).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="I2" class="ltx_enumerate">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">city</em>. For each state, we measure the distance between the word <em class="ltx_emph">city</em> and the stateâ€™s most populous city; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i1.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{AZ}}(\mathit{city},\mathit{phoenix})" display="inline"><mrow><msub><mi>Î´</mi><mi>AZ</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ğ‘–ğ‘¡ğ‘¦</mi><mo>,</mo><mi>ğ‘â„ğ‘œğ‘’ğ‘›ğ‘–ğ‘¥</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">state</em>. For each state, the distance between the word <em class="ltx_emph">state</em> and the stateâ€™s name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i2.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{WI}}(\mathit{state},\mathit{wisconsin})" display="inline"><mrow><msub><mi>Î´</mi><mi>WI</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’</mi><mo>,</mo><mi>ğ‘¤ğ‘–ğ‘ ğ‘ğ‘œğ‘›ğ‘ ğ‘–ğ‘›</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">football</em>. For all NFL teams, the distance between the word <em class="ltx_emph">football</em> and the team name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i3.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{IL}}(\mathit{football},\mathit{bears})" display="inline"><mrow><msub><mi>Î´</mi><mi>IL</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘“ğ‘œğ‘œğ‘¡ğ‘ğ‘ğ‘™ğ‘™</mi><mo>,</mo><mi>ğ‘ğ‘’ğ‘ğ‘Ÿğ‘ </mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">4.</span> 
<div id="I2.i4.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">basketball</em>. For all NBA teams from a US state, the distance between the word <em class="ltx_emph">basketball</em> and the team name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i4.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{FL}}(\mathit{basketball},\mathit{heat})" display="inline"><mrow><msub><mi>Î´</mi><mi>FL</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ğ‘ğ‘ ğ‘˜ğ‘’ğ‘¡ğ‘ğ‘ğ‘™ğ‘™</mi><mo>,</mo><mi>â„ğ‘’ğ‘ğ‘¡</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">5.</span> 
<div id="I2.i5.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">baseball</em>. For all MLB teams from a US state, the distance between the word <em class="ltx_emph">baseball</em> and the team name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i5.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{IL}}(\mathit{baseball},\mathit{cubs})" display="inline"><mrow><msub><mi>Î´</mi><mi>IL</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ğ‘ğ‘ ğ‘’ğ‘ğ‘ğ‘™ğ‘™</mi><mo>,</mo><mi>ğ‘ğ‘¢ğ‘ğ‘ </mi></mrow><mo>)</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i5.p1.m2" class="ltx_Math" alttext="\delta_{\mathrm{IL}}(\mathit{baseball},\mathit{white\ sox})" display="inline"><mrow><msub><mi>Î´</mi><mi>IL</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ğ‘ğ‘ ğ‘’ğ‘ğ‘ğ‘™ğ‘™</mi><mo>,</mo><mrow><mpadded width="+5.0pt"><mi>ğ‘¤â„ğ‘–ğ‘¡ğ‘’</mi></mpadded><mo>â¢</mo><mi>ğ‘ ğ‘œğ‘¥</mi></mrow></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">6.</span> 
<div id="I2.i6.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">hockey</em>. For all NHL teams from a US state, the distance between the word <em class="ltx_emph">hockey</em> and the team name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i6.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{PA}}(\mathit{hockey},\mathit{penguins})" display="inline"><mrow><msub><mi>Î´</mi><mi>PA</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>â„ğ‘œğ‘ğ‘˜ğ‘’ğ‘¦</mi><mo>,</mo><mi>ğ‘ğ‘’ğ‘›ğ‘”ğ‘¢ğ‘–ğ‘›ğ‘ </mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I2.i7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">7.</span> 
<div id="I2.i7.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">park</em>. For all US national parks, the distance between the word <em class="ltx_emph">park</em> and the park name; e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i7.p1.m1" class="ltx_Math" alttext="\delta_{\mathrm{AK}}(\mathit{park},\mathit{denali})" display="inline"><mrow><msub><mi>Î´</mi><mi>AK</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>ğ‘ğ‘ğ‘Ÿğ‘˜</mi><mo>,</mo><mi>ğ‘‘ğ‘’ğ‘›ğ‘ğ‘™ğ‘–</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">Each of these questions asks the following: what words are evoked for a given target word (like <em class="ltx_emph">football</em>)? While <em class="ltx_emph">football</em> may everywhere evoke similar sports like <em class="ltx_emph">baseball</em> or <em class="ltx_emph">soccer</em> or more specific football-related terms like <em class="ltx_emph">touchdown</em> or <em class="ltx_emph">field goal</em>, we expect that particular sports teams will be evoked more strongly by the word <em class="ltx_emph">football</em> in their particular geographical region: in Wisconsin, <em class="ltx_emph">football</em> should evoke <em class="ltx_emph">packers</em>, while in Pennsylvania, <em class="ltx_emph">football</em> evokes <em class="ltx_emph">steelers</em>.
Note that this is not the same as simply asking which sports team is most frequently (or most characteristically) mentioned in a given area;
by measuring the distance to a target word (<em class="ltx_emph">football</em>), we are attempting to estimate the varying strengths of association between concepts in different regions.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p class="ltx_p">For each category, we measure similarity as the average cosine similarity between the vector for the target word for that category (e.g., <em class="ltx_emph">city</em>) and the corresponding vector for each state-specific answer (e.g., <em class="ltx_emph">chicago</em> for IL; <em class="ltx_emph">boston</em> for MA). We compare three different models:</p>
</div>
<div id="S3.F2" class="ltx_figure"><span class="ltx_ERROR undefined ltx_centering">\includegraphics</span>
<p class="ltx_p ltx_align_center">[width=]images/barplot.pdf</p>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">FigureÂ 2: </span>Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean. </div>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<ol id="I3" class="ltx_enumerate">
<li id="I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">Joint</span>. The full model described in section <a href="#S2" title="2 Model â€£ Distributed Representations of Geographically Situated Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, in which we learn a global representation for each word along with deviations from that common representation for each state.</p>
</div></li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">Individual</span>. For comparison, we also partition the data among all 51 states, and train a single model for each state using only data from that state. In this model, there is no sharing among states; California has the most data with 11,604,637 tweets; Wyoming has the least with 47,503 tweets.</p>
</div></li>
<li id="I3.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I3.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">â€“Geo</span>. We also train a single model on all of the training data, but ignore any state metadata. In this case the distance <math xmlns="http://www.w3.org/1998/Math/MathML" id="I3.i3.p1.m1" class="ltx_Math" alttext="\delta" display="inline"><mi>Î´</mi></math> between two terms is their overall distance within the entire United States.</p>
</div></li>
</ol>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p class="ltx_p">As one concrete example of these differences between individual data points, the cosine similarity between <em class="ltx_emph">city</em> and <em class="ltx_emph">seattle</em> in the <span class="ltx_text ltx_font_smallcaps">â€“Geo</span> model is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p6.m1" class="ltx_Math" alttext="0.728" display="inline"><mn>0.728</mn></math> (<em class="ltx_emph">seattle</em> is ranked as the 188th most similar term to <em class="ltx_emph">city</em> overall); in the <span class="ltx_text ltx_font_smallcaps">Individual</span> model using only tweets from Washington state, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p6.m2" class="ltx_Math" alttext="\delta_{WA}(city,seattle)=0.780" display="inline"><mrow><mrow><msub><mi>Î´</mi><mrow><mi>W</mi><mo>â¢</mo><mi>A</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mrow><mi>c</mi><mo>â¢</mo><mi>i</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>y</mi></mrow><mo>,</mo><mrow><mi>s</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>e</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mn>0.780</mn></mrow></math> (rank #32); and in the <span class="ltx_text ltx_font_smallcaps">Joint</span> model, using information from the entire United States with deviations for Washington, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p6.m3" class="ltx_Math" alttext="\delta_{WA}(city,seattle)=0.858" display="inline"><mrow><mrow><msub><mi>Î´</mi><mrow><mi>W</mi><mo>â¢</mo><mi>A</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mrow><mi>c</mi><mo>â¢</mo><mi>i</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>y</mi></mrow><mo>,</mo><mrow><mi>s</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>e</mi></mrow></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mn>0.858</mn></mrow></math> (rank #6). The overall similarity for the city category of each model is the average of 51 such tests (one for each city).</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F2" title="FigureÂ 2 â€£ 3.2 Quantitative Evaluation â€£ 3 Evaluation â€£ Distributed Representations of Geographically Situated Language" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> present the results of the full evaluation, including 95% confidence intervals for each mean.
While the two models that include geographical information naturally outperform the model that does not, the <span class="ltx_text ltx_font_smallcaps">Joint</span> model generally far outperforms the <span class="ltx_text ltx_font_smallcaps">Individual</span> models trained on state-specific subsets of the data.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>This result is robust to the choice of distance metric;
an evaluation measuring the Euclidean distance between vectors shows the <span class="ltx_text ltx_font_smallcaps">Joint</span> model to outperform the <span class="ltx_text ltx_font_smallcaps">Individual</span> and <span class="ltx_text ltx_font_smallcaps">â€“Geo</span> models across all seven categories.</span></span></span>
A model that can exploit all of the information in the data, learning core vector-space representations for all words along with deviations for each contextual variable, is able to learn more geographically-informed representations for this task than strict geographical models alone.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We introduced a model for leveraging situational information in learning vector-space representations of words that are sensitive to the speakerâ€™s social context.
While our results use geographical information in learning low-dimensional representations, other contextual variables are straightforward to include as well; incorporating effects for time â€“ such as time of day, month of year and absolute year â€“ may be a powerful tool for revealing periodic and historical influences on lexical semantics.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">Our approach explores the degree to which geography, and other contextual factors, influence word <em class="ltx_emph">meaning</em> in addition to frequency of usage.
By allowing all words in different regions (or more generally, with different metadata factors) to exist in the same vector space, we are able compare different points in that space â€“ for example, to ask what terms used in Chicago are most similar to <em class="ltx_emph">hot dog</em> in New York, or what word groups shift together in the same region in comparison to the background (indicating the shift of an entire semantic field). All datasets and software to support these geographically-informed representations can be found at: <a href="http://www.ark.cs.cmu.edu/geoSGLM" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.ark.cs.cmu.edu/geoSGLM</span></a>.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Acknowledgments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">The research reported in this article was supported by US NSF grants IIS-1251131 and CAREER IIS-1054319, and by an ARCS scholarship to D.B. This work was made possible through the use of computing resources made available by the Open Cloud Consortium, Yahoo and the Pittsburgh Supercomputing Center.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 18:37:48 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
