<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Convolutional Neural Network for Modelling Sentences</title>
<!--Generated on Tue Jun 10 18:02:14 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Convolutional Neural Network for Modelling Sentences</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nal Kalchbrenner 
<br class="ltx_break"/>&amp;Edward Grefenstette
<br class="ltx_break"/> 
<br class="ltx_break"/><span class="ltx_text ltx_font_small">{<span class="ltx_text ltx_font_typewriter">nal.kalchbrenner, edward.grefenstette, phil.blunsom</span>}<span class="ltx_text ltx_font_typewriter">@cs.ox.ac.uk
<br class="ltx_break"/></span></span>Department of Computer Science 
<br class="ltx_break"/>University of Oxford
<br class="ltx_break"/>&amp;Phil Blunsom 
<br class="ltx_break"/>
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than <math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="25\%" display="inline"><mrow><mn>25</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> error reduction in the last task with respect to the strongest baseline.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation. The sentence modelling problem is at the core of many tasks involving a degree of natural language comprehension. These tasks include sentiment analysis, paraphrase detection, entailment recognition, summarisation, discourse analysis, machine translation, grounded language learning and image retrieval.
Since individual sentences are rarely observed or not observed at all, one must represent a sentence in terms of features that depend on the words and short <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams in the sentence that are frequently observed. The core of a sentence model involves a feature function that defines the process by which the features of the sentence are extracted from the features of the words or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.</p>
</div>
<div id="S1.F1" class="ltx_figure"><img src="P14-1062/image001.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="325" height="182" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Subgraph of a feature graph induced over an input sentence in a Dynamic Convolutional Neural Network. The full induced graph has multiple subgraphs of this kind with a distinct set of edges; subgraphs may merge at different layers. The left diagram emphasises the pooled nodes. The width of the convolutional filters is 3 and 2 respectively. With dynamic pooling, a filter with small width at the higher layers can relate phrases far apart in the input sentence.</div>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Various types of models of meaning have been proposed. Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases. In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors <cite class="ltx_cite">[]</cite>. In other cases, a composition function is learned and either tied to particular syntactic relations <cite class="ltx_cite">[]</cite> or to particular word types <cite class="ltx_cite">[]</cite>. Another approach represents the meaning of sentences by way of automatically extracted logical forms <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">A central class of models are those based on neural networks. These range from basic neural bag-of-words or bag-of-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations <cite class="ltx_cite">[]</cite>. Neural sentence models have a number of advantages. They can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur. Through supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task. Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We define a convolutional neural network architecture and apply it to the semantic modelling of sentences. The network handles input sequences of varying length. The layers in the network interleave one-dimensional convolutional layers and dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling layers. Dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling is a generalisation of the max pooling operator. The max pooling operator is a non-linear subsampling function that returns the maximum of a set of values <cite class="ltx_cite">[]</cite>. The operator is generalised in two respects. First, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling over a linear sequence of values returns the subsequence of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> maximum values in the sequence, instead of the single maximum value. Secondly, the pooling parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m5" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> can be dynamically chosen by making <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> a function of other aspects of the network or the input.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">The convolutional layers apply one-dimensional filters across each row of features in the sentence matrix. Convolving the same filter with the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p5.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence. A convolutional layer followed by a dynamic pooling layer and a non-linearity form a feature map. Like in the convolutional networks for object recognition <cite class="ltx_cite">[]</cite>, we enrich the representation in the first layer by computing multiple feature maps with different filters applied to the input sentence. Subsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below. The weights at these layers form an order-4 tensor. The resulting architecture is dubbed a Dynamic Convolutional Neural Network.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">Multiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence. Figure 1 illustrates such a graph. Small filters at higher layers can capture syntactic or semantic relations between non-continuous phrases that are far apart in the input sentence. The feature graph induces a hierarchical structure somewhat akin to that in a syntactic parse tree. The structure is not tied to purely syntactic relations and is internal to the neural network.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p">We experiment with the network in four settings. The first two experiments involve predicting the sentiment of movie reviews <cite class="ltx_cite">[]</cite>. The network outperforms other approaches in both the binary and the multi-class experiments. The third experiment involves the categorisation of questions in six question types in the TREC dataset <cite class="ltx_cite">[]</cite>. The network matches the accuracy of other state-of-the-art methods that are based on large sets of engineered features and hand-coded knowledge resources.
The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision <cite class="ltx_cite">[]</cite>. The network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs in them. On the hand-labelled test set, the network achieves a greater than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p7.m1" class="ltx_Math" alttext="25\%" display="inline"><mrow><mn>25</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> reduction in the prediction error with respect to the strongest unigram and bigram baseline reported in <cite class="ltx_cite"/>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p class="ltx_p">The outline of the paper is as follows. Section 2 describes the background to the DCNN including central concepts and related neural sentence models. Section 3 defines the relevant operators and the layers of the network. Section 4 treats of the induced feature graph and other properties of the network. Section 5 discusses the experiments and inspects the learnt feature detectors.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>Code available at <a href="www.nal.co" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">www.nal.co</span></a></span></span></span></p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The layers of the DCNN are formed by a convolution operation followed by a pooling operation. We begin with a review of related neural sentence models. Then we describe the operation of <em class="ltx_emph">one-dimensional convolution</em> and the classical Time-Delay Neural Network (TDNN) <cite class="ltx_cite">[]</cite>. By adding a max pooling layer to the network, the TDNN can be adopted as a sentence model <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Related Neural Sentence Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">Various neural sentence models have been described. A general class of basic sentence models is that of Neural Bag-of-Words (NBoW) models. These generally consist of a projection layer that maps words, sub-word units or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams to high dimensional embeddings; the latter are then combined component-wise with an operation such as summation. The resulting combined vector is classified through one or more fully connected layers.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) <cite class="ltx_cite">[]</cite>. At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence.
The Recurrent Neural Network (RNN) is a special case of the recursive network where the structure that is followed is a simple linear chain <cite class="ltx_cite">[]</cite>. The RNN is primarily used as a language model, but may also be viewed as a sentence model with a linear structure. The layer computed at the last word represents the sentence.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">Finally, a further class of neural sentence models is based on the convolution operation and the TDNN architecture <cite class="ltx_cite">[]</cite>. Certain concepts used in these models are central to the DCNN and we describe them next.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Convolution</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">The <em class="ltx_emph">one-dimensional convolution</em> is an operation between a vector of weights <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m1" class="ltx_Math" alttext="\mathbf{m}\in\mathbb{R}^{m}" display="inline"><mrow><mi>𝐦</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow></math> and a vector of inputs viewed as a sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m2" class="ltx_Math" alttext="\mathbf{s}\in\mathbb{R}^{s}" display="inline"><mrow><mi>𝐬</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>s</mi></msup></mrow></math>. The vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m3" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> is the <em class="ltx_emph">filter</em> of the convolution. Concretely, we think of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m4" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> as the input sentence and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m5" class="ltx_Math" alttext="\mathbf{s}_{i}\in\mathbb{R}" display="inline"><mrow><msub><mi>𝐬</mi><mi>i</mi></msub><mo>∈</mo><mi>ℝ</mi></mrow></math> is a single feature value associated with the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th word in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m7" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> with each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m8" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math>-gram in the sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m9" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> to obtain another sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m10" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math>:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m1" class="ltx_Math" alttext="\mathbf{c}_{j}=\mathbf{m}^{\intercal}\mathbf{s}_{j-m+1:j}" display="block"><mrow><msub><mi>𝐜</mi><mi>j</mi></msub><mo>=</mo><mrow><msup><mi>𝐦</mi><mo>⊺</mo></msup><mo>⁢</mo><msub><mi>𝐬</mi><mrow><mrow><mi>j</mi><mo>-</mo><mi>m</mi><mo>+</mo><mn>1</mn></mrow><mo>:</mo><mi>j</mi></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">Equation <a href="#S2.E1" title="(1) ‣ 2.2 Convolution ‣ 2 Background ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives rise to two types of convolution depending on the range of the index <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math>. The <em class="ltx_emph">narrow</em> type of convolution requires that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m2" class="ltx_Math" alttext="s\geq m" display="inline"><mrow><mi>s</mi><mo>≥</mo><mi>m</mi></mrow></math> and yields a sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m3" class="ltx_Math" alttext="\mathbf{c}\in\mathbb{R}^{s-m+1}" display="inline"><mrow><mi>𝐜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>s</mi><mo>-</mo><mi>m</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow></math> with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m4" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math> ranging from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m5" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m6" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>. The <em class="ltx_emph">wide</em> type of convolution does not have requirements on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m7" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m8" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> and yields a sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m9" class="ltx_Math" alttext="\mathbf{c}\in\mathbb{R}^{s+m-1}" display="inline"><mrow><mi>𝐜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>s</mi><mo>+</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></math> where the index <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m10" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math> ranges from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m11" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m12" class="ltx_Math" alttext="s+m-1" display="inline"><mrow><mi>s</mi><mo>+</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow></math>. Out-of-range input values <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m13" class="ltx_Math" alttext="\mathbf{s}_{i}" display="inline"><msub><mi>𝐬</mi><mi>i</mi></msub></math> where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m14" class="ltx_Math" alttext="i&lt;1" display="inline"><mrow><mi>i</mi><mo>&lt;</mo><mn>1</mn></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m15" class="ltx_Math" alttext="i&gt;s" display="inline"><mrow><mi>i</mi><mo>&gt;</mo><mi>s</mi></mrow></math> are taken to be zero. The result of the narrow convolution is a subsequence of the result of the wide convolution. The two types of one-dimensional convolution are illustrated in Fig. 2.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">The trained weights in the filter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> correspond to a linguistic feature detector that learns to recognise a specific class of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams. These <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams have size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m4" class="ltx_Math" alttext="n\leq m" display="inline"><mrow><mi>n</mi><mo>≤</mo><mi>m</mi></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m5" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> is the width of the filter. Applying the weights <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m6" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> in a wide convolution has some advantages over applying them in a narrow one. A wide convolution ensures that all weights in the filter reach the entire sentence, including the words at the margins. This is particularly significant when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m7" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> is set to a relatively large value such as 8 or 10. In addition, a wide convolution guarantees that the application of the filter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m8" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> to the input sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m9" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> always produces a valid non-empty result <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m10" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math>, independently of the width <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m11" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> and the sentence length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m12" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>. We next describe the classical convolutional layer of a TDNN.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Time-Delay Neural Networks</h3>

<div id="S2.F2" class="ltx_figure"><img src="P14-1062/image002.png" id="S2.F2.g1" class="ltx_graphics ltx_centering" width="324" height="67" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Narrow and wide types of convolution. The filter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.m3" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> has size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.m4" class="ltx_Math" alttext="m=5" display="inline"><mrow><mi>m</mi><mo>=</mo><mn>5</mn></mrow></math>. </div>
</div>
<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">A TDNN convolves a sequence of inputs <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> with a set of weights <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m2" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math>. As in the TDNN for phoneme recognition <cite class="ltx_cite">[]</cite>, the sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m3" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> is viewed as having a time dimension and the convolution is applied over the time dimension. Each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m4" class="ltx_Math" alttext="\mathbf{s}_{j}" display="inline"><msub><mi>𝐬</mi><mi>j</mi></msub></math> is often not just a single value, but a vector of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m5" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> values so that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m6" class="ltx_Math" alttext="\mathbf{s}\in\mathbb{R}^{d\times s}" display="inline"><mrow><mi>𝐬</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>s</mi></mrow></msup></mrow></math>. Likewise, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m7" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> is a matrix of weights of size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m8" class="ltx_Math" alttext="{d\times m}" display="inline"><mrow><mi>d</mi><mo>×</mo><mi>m</mi></mrow></math>. Each row of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m9" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> is convolved with the corresponding row of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m10" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> and the convolution is usually of the narrow type. Multiple convolutional layers may be stacked by taking the resulting sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m11" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math> as input to the next layer.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">The Max-TDNN sentence model is based on the architecture of a TDNN <cite class="ltx_cite">[]</cite>. In the model, a convolutional layer of the narrow type is applied to the sentence matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math>, where each column corresponds to the feature vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m2" class="ltx_Math" alttext="\mathbf{w}_{i}\in\mathbb{R}^{d}" display="inline"><mrow><msub><mi>𝐰</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></math> of a word in the sentence:</p>
<table id="S2.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m1" class="ltx_Math" alttext="\mathbf{s}=\begin{bmatrix}\kern 6.0pt\vline&amp;\kern 2.0pt\vline&amp;\vline\\&#10;\mathbf{w}_{1}&amp;\ldots&amp;\mathbf{w}_{s}\\&#10;\kern 6.0pt\vline&amp;\kern 2.0pt\vline&amp;\vline\\&#10;\end{bmatrix}" display="block"><mrow><mi>𝐬</mi><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="center"><mi/></mtd><mtd columnalign="center"><mi/></mtd><mtd columnalign="center"><mi/></mtd></mtr><mtr><mtd columnalign="center"><msub><mi>𝐰</mi><mn>1</mn></msub></mtd><mtd columnalign="center"><mi mathvariant="normal">…</mi></mtd><mtd columnalign="center"><msub><mi>𝐰</mi><mi>s</mi></msub></mtd></mtr><mtr><mtd columnalign="center"><mi/></mtd><mtd columnalign="center"><mi/></mtd><mtd columnalign="center"><mi/></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">To address the problem of varying sentence lengths, the Max-TDNN takes the maximum of each row in the resulting matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m3" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math> yielding a vector of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m4" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> values:</p>
<table id="S2.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m1" class="ltx_Math" alttext="\mathbf{c}_{max}=\begin{bmatrix}\max(\mathbf{c}_{1,:})\\&#10;\vdots\\&#10;\max(\mathbf{c}_{d,:})\\&#10;\end{bmatrix}" display="block"><mrow><msub><mi>𝐜</mi><mrow><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></msub><mo>=</mo><mrow><mo>[</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="center"><mrow><mo movablelimits="false">max</mo><mo>⁡</mo><mrow><mo>(</mo><msub><mi>𝐜</mi><mrow><mn>1</mn><mo>,</mo><mo>:</mo></mrow></msub><mo>)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd></mtr><mtr><mtd columnalign="center"><mrow><mo movablelimits="false">max</mo><mo>⁡</mo><mrow><mo>(</mo><msub><mi>𝐜</mi><mrow><mi>d</mi><mo>,</mo><mo>:</mo></mrow></msub><mo>)</mo></mrow></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">The aim is to capture the most relevant feature, i.e. the one with the highest value, for each of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m5" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> rows of the resulting matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m6" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math>. The fixed-sized vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m7" class="ltx_Math" alttext="\mathbf{c}_{max}" display="inline"><msub><mi>𝐜</mi><mrow><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow></msub></math> is then used as input to a fully connected layer for classification.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p">The Max-TDNN model has many desirable properties. It is sensitive to the order of the words in the sentence and it does not depend on external language-specific features such as dependency or constituency parse trees. It also gives largely uniform importance to the signal coming from each of the words in the sentence, with the exception of words at the margins that are considered fewer times in the computation of the narrow convolution. But the model also has some limiting aspects. The range of the feature detectors is limited to the span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m1" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> of the weights. Increasing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m2" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> or stacking multiple convolutional layers of the narrow type makes the range of the feature detectors larger; at the same time it also exacerbates the neglect of the margins of the sentence and increases the minimum size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m3" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> of the input sentence required by the convolution. For this reason higher-order and long-range feature detectors cannot be easily incorporated into the model.</p>
</div>
<div id="S2.F3" class="ltx_figure"><img src="P14-1062/image003.png" id="S2.F3.g1" class="ltx_graphics" width="339" height="428" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A DCNN for the seven word input sentence. Word embeddings have size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F3.m5" class="ltx_Math" alttext="d=4" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>4</mn></mrow></math>. The network has two convolutional layers with two feature maps each. The widths of the filters at the two layers are respectively 3 and 2. The (dynamic) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F3.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling layers have values <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F3.m7" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F3.m8" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math> and 3. </div>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p class="ltx_p">The max pooling operation has some disadvantages too. It cannot distinguish whether a relevant feature in one of the rows occurs just one or multiple times and it forgets the order in which the features occur. More generally, the pooling factor by which the signal of the matrix is reduced at once corresponds to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m1" class="ltx_Math" alttext="s-m+1" display="inline"><mrow><mi>s</mi><mo>-</mo><mi>m</mi><mo>+</mo><mn>1</mn></mrow></math>; even for moderate values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m2" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> the pooling factor can be excessive.
The aim of the next section is to address these limitations while preserving the advantages.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Convolutional Neural Networks with Dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-Max Pooling</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We model sentences using a convolutional architecture that alternates wide convolutional layers with dynamic pooling layers given by <em class="ltx_emph">dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling</em>. In the network the width of a feature map at an intermediate layer varies depending on the length of the input sentence; the resulting architecture is the Dynamic Convolutional Neural Network. Figure 3 represents a DCNN. We proceed to describe the network in detail.</p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Wide Convolution</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Given an input sentence, to obtain the first layer of the DCNN we take the embedding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="\mathbf{w}_{i}\in\mathbb{R}^{d}" display="inline"><mrow><msub><mi>𝐰</mi><mi>i</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></math> for each word in the sentence and construct the sentence matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m2" class="ltx_Math" alttext="\mathbf{s}\in\mathbb{R}^{d\times s}" display="inline"><mrow><mi>𝐬</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>s</mi></mrow></msup></mrow></math> as in Eq. <a href="#S2.E2" title="(2) ‣ 2.3 Time-Delay Neural Networks ‣ 2 Background ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The values in the embeddings <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m3" class="ltx_Math" alttext="\mathbf{w}_{i}" display="inline"><msub><mi>𝐰</mi><mi>i</mi></msub></math> are parameters that are optimised during training.
A convolutional layer in the network is obtained by convolving a matrix of weights <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m4" class="ltx_Math" alttext="\mathbf{m}\in\mathbb{R}^{d\times m}" display="inline"><mrow><mi>𝐦</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo>×</mo><mi>m</mi></mrow></msup></mrow></math> with the matrix of activations at the layer below. For example, the second layer is obtained by applying a convolution to the sentence matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m5" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> itself. Dimension <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m6" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> and filter width <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m7" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> are hyper-parameters of the network. We let the operations be <em class="ltx_emph">wide</em> one-dimensional convolutions as described in Sect. <a href="#S2.SS2" title="2.2 Convolution ‣ 2 Background ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>. The resulting matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m8" class="ltx_Math" alttext="\mathbf{c}" display="inline"><mi>𝐜</mi></math> has dimensions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m9" class="ltx_Math" alttext="d\times(s+m-1)" display="inline"><mrow><mi>d</mi><mo>×</mo><mrow><mo>(</mo><mrow><mi>s</mi><mo>+</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-Max Pooling</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">We next describe a pooling operation that is a generalisation of the max pooling over the time dimension used in the Max-TDNN sentence model and different from the local max pooling operations applied in a convolutional network for object recognition <cite class="ltx_cite">[]</cite>. Given a value <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> and a sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="\mathbf{p}\in\mathbb{R}^{p}" display="inline"><mrow><mi>𝐩</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>p</mi></msup></mrow></math> of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m3" class="ltx_Math" alttext="p\geq k" display="inline"><mrow><mi>p</mi><mo>≥</mo><mi>k</mi></mrow></math>, <em class="ltx_emph"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling</em> selects the subsequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m5" class="ltx_Math" alttext="\mathbf{p}^{k}_{max}" display="inline"><msubsup><mi>𝐩</mi><mrow><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow><mi>k</mi></msubsup></math> of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> highest values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m7" class="ltx_Math" alttext="\mathbf{p}" display="inline"><mi>𝐩</mi></math>. The order of the values in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m8" class="ltx_Math" alttext="\mathbf{p}^{k}_{max}" display="inline"><msubsup><mi>𝐩</mi><mrow><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>x</mi></mrow><mi>k</mi></msubsup></math> corresponds to their original order in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m9" class="ltx_Math" alttext="\mathbf{p}" display="inline"><mi>𝐩</mi></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling operation makes it possible to pool the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> most active features in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m3" class="ltx_Math" alttext="\mathbf{p}" display="inline"><mi>𝐩</mi></math> that may be a number of positions apart; it preserves the order of the features, but is insensitive to their specific positions. It can also discern more finely the number of times the feature is highly activated in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m4" class="ltx_Math" alttext="\mathbf{p}" display="inline"><mi>𝐩</mi></math> and the progression by which the high activations of the feature change across <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m5" class="ltx_Math" alttext="\mathbf{p}" display="inline"><mi>𝐩</mi></math>.
The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling operator is applied in the network after the topmost convolutional layer. This guarantees that the input to the fully connected layers is independent of the length of the input sentence.
But, as we see next, at intermediate convolutional layers the pooling parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m7" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is not fixed, but is dynamically selected in order to allow for a smooth extraction of higher-order and longer-range features.</p>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-Max Pooling</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">A <em class="ltx_emph">dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling</em> operation is a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling operation where we let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> be a function of the length of the sentence and the depth of the network. Although many functions are possible, we simply model the pooling parameter as follows:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="k_{l}=\max(\ k_{top},\ \lceil\frac{L-l}{L}s\rceil\ )" display="block"><mrow><msub><mi>k</mi><mi>l</mi></msub><mo>=</mo><mrow><mo movablelimits="false">max</mo><mo>⁡</mo><mrow><mo>(</mo><mrow><msub><mi>k</mi><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>p</mi></mrow></msub><mo separator="true">,  </mo><mrow><mo>⌈</mo><mrow><mfrac><mrow><mi>L</mi><mo>-</mo><mi>l</mi></mrow><mi>L</mi></mfrac><mo>⁢</mo><mi>s</mi></mrow><mo>⌉</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m1" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math> is the number of the current convolutional layer to which the pooling is applied and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> is the total number of convolutional layers in the network; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m3" class="ltx_Math" alttext="k_{top}" display="inline"><msub><mi>k</mi><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>p</mi></mrow></msub></math> is the fixed pooling parameter for the topmost convolutional layer (Sect. <a href="#S3.SS2" title="3.2 k-Max Pooling ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). For instance, in a network with three convolutional layers and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m4" class="ltx_Math" alttext="k_{top}=3" display="inline"><mrow><msub><mi>k</mi><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>p</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow></math>, for an input sentence of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m5" class="ltx_Math" alttext="s=18" display="inline"><mrow><mi>s</mi><mo>=</mo><mn>18</mn></mrow></math>, the pooling parameter at the first layer is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m6" class="ltx_Math" alttext="k_{1}=12" display="inline"><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>=</mo><mn>12</mn></mrow></math> and the pooling parameter at the second layer is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m7" class="ltx_Math" alttext="k_{2}=6" display="inline"><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>=</mo><mn>6</mn></mrow></math>; the third layer has the fixed pooling parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m8" class="ltx_Math" alttext="k_{3}=k_{top}=3" display="inline"><mrow><msub><mi>k</mi><mn>3</mn></msub><mo>=</mo><msub><mi>k</mi><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>p</mi></mrow></msub><mo>=</mo><mn>3</mn></mrow></math>. Equation <a href="#S3.E4" title="(4) ‣ 3.3 Dynamic k-Max Pooling ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is a model of the number of values needed to describe the relevant parts of the progression of an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m9" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math>-th order feature over a sentence of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m10" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>. For an example in sentiment prediction, according to the equation a first order feature such as a positive word occurs <em class="ltx_emph">at most</em> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m11" class="ltx_Math" alttext="k_{1}" display="inline"><msub><mi>k</mi><mn>1</mn></msub></math> times in a sentence of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m12" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>, whereas a second order feature such as a negated phrase or clause occurs at most <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m13" class="ltx_Math" alttext="k_{2}" display="inline"><msub><mi>k</mi><mn>2</mn></msub></math> times.</p>
</div>
</div>
<div id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.4 </span>Non-linear Feature Function</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p">After (dynamic) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling is applied to the result of a convolution, a bias <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p1.m2" class="ltx_Math" alttext="\mathbf{b}\in\mathbb{R}^{d}" display="inline"><mrow><mi>𝐛</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></math> and a non-linear function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p1.m3" class="ltx_Math" alttext="g" display="inline"><mi>g</mi></math> are applied component-wise to the pooled matrix. There is a single bias value for each row of the pooled matrix.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p">If we temporarily ignore the pooling layer, we may state how one computes each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p2.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>-dimensional column <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p2.m2" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math> in the matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p2.m3" class="ltx_Math" alttext="\mathbf{a}" display="inline"><mi>𝐚</mi></math> resulting after the convolutional and non-linear layers. Define <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p2.m4" class="ltx_Math" alttext="\mathbf{M}" display="inline"><mi>𝐌</mi></math> to be the matrix of diagonals:</p>
<table id="S3.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="\mathbf{M}=\left[\mbox{diag}(\mathbf{m}_{:,1}),\ldots,\mbox{diag}(\mathbf{m}_{%&#10;:,m})\right]" display="block"><mrow><mi>𝐌</mi><mo>=</mo><mrow><mo>[</mo><mrow><mrow><mtext>diag</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>𝐦</mi><mrow><mo>:</mo><mo>,</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mtext>diag</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>𝐦</mi><mrow><mo>:</mo><mo>,</mo><mi>m</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p3.m1" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> are the weights of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p3.m2" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> filters of the wide convolution.
Then after the first pair of a convolutional and a non-linear layer, each column <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p3.m3" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math> in the matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p3.m4" class="ltx_Math" alttext="\mathbf{a}" display="inline"><mi>𝐚</mi></math> is obtained as follows, for some index <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p3.m5" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math>:</p>
<table id="S3.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="a=g\left(\mathbf{M}\left[\begin{array}[]{ccc}\mathbf{w}_{j}\\&#10;\vdots\\&#10;\mathbf{w}_{j+m-1}\end{array}\right]+\mathbf{b}\right)" display="block"><mrow><mi>a</mi><mo>=</mo><mrow><mi>g</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝐌</mi><mo>⁢</mo><mrow><mo>[</mo><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="center"><msub><mi>𝐰</mi><mi>j</mi></msub></mtd><mtd/><mtd/></mtr><mtr><mtd columnalign="center"><mi mathvariant="normal">⋮</mi></mtd><mtd/><mtd/></mtr><mtr><mtd columnalign="center"><msub><mi>𝐰</mi><mrow><mi>j</mi><mo>+</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub></mtd><mtd/><mtd/></mtr></mtable><mo>]</mo></mrow></mrow><mo>+</mo><mi>𝐛</mi></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p class="ltx_p">Here <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m1" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math> is a column of first order features. Second order features are similarly obtained by applying Eq. <a href="#S3.E6" title="(6) ‣ 3.4 Non-linear Feature Function ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> to a sequence of first order features <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m2" class="ltx_Math" alttext="a_{j},...,a_{j+m^{\prime}-1}" display="inline"><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>a</mi><mrow><mi>j</mi><mo>+</mo><msup><mi>m</mi><mo>′</mo></msup><mo>-</mo><mn>1</mn></mrow></msub></mrow></math> with another weight matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m3" class="ltx_Math" alttext="\mathbf{M}^{\prime}" display="inline"><msup><mi>𝐌</mi><mo>′</mo></msup></math>. Barring pooling, Eq. <a href="#S3.E6" title="(6) ‣ 3.4 Non-linear Feature Function ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> represents a core aspect of the feature extraction function and has a rather general form that we return to below. Together with pooling, the feature function induces position invariance and makes the range of higher-order features variable.</p>
</div>
</div>
<div id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.5 </span>Multiple Feature Maps</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p class="ltx_p">So far we have described how one applies a wide convolution, a (dynamic) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling layer and a non-linear function to the input sentence matrix to obtain a first order <em class="ltx_emph">feature map</em>. The three operations can be repeated to yield feature maps of increasing order and a network of increasing depth. We denote a feature map of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m2" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th order by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m3" class="ltx_Math" alttext="\mathbf{F}^{i}" display="inline"><msup><mi>𝐅</mi><mi>i</mi></msup></math>. As in convolutional networks for object recognition, to increase the number of learnt feature detectors of a certain order, multiple feature maps <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m4" class="ltx_Math" alttext="\mathbf{F}^{i}_{1},\ldots,\mathbf{F}^{i}_{n}" display="inline"><mrow><msubsup><mi>𝐅</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝐅</mi><mi>n</mi><mi>i</mi></msubsup></mrow></math> may be computed in parallel at the same layer. Each feature map <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m5" class="ltx_Math" alttext="\mathbf{F}^{i}_{j}" display="inline"><msubsup><mi>𝐅</mi><mi>j</mi><mi>i</mi></msubsup></math> is computed by convolving a distinct set of filters arranged in a matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m6" class="ltx_Math" alttext="\mathbf{m}^{i}_{j,k}" display="inline"><msubsup><mi>𝐦</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mi>i</mi></msubsup></math> with each feature map <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m7" class="ltx_Math" alttext="\mathbf{F}^{i-1}_{k}" display="inline"><msubsup><mi>𝐅</mi><mi>k</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math> of the lower order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p1.m8" class="ltx_Math" alttext="i-1" display="inline"><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></math> and summing the results:</p>
<table id="S3.E7" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m1" class="ltx_Math" alttext="\mathbf{F}^{i}_{j}=\sum_{k=1}^{n}\mathbf{m}^{i}_{j,k}*\mathbf{F}^{i-1}_{k}" display="block"><mrow><msubsup><mi>𝐅</mi><mi>j</mi><mi>i</mi></msubsup><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><msubsup><mi>𝐦</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mi>i</mi></msubsup><mo>*</mo><msubsup><mi>𝐅</mi><mi>k</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p2.m1" class="ltx_Math" alttext="*" display="inline"><mo>*</mo></math> indicates the wide convolution. The weights <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p2.m2" class="ltx_Math" alttext="\mathbf{m}^{i}_{j,k}" display="inline"><msubsup><mi>𝐦</mi><mrow><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mi>i</mi></msubsup></math> form an order-4 tensor. After the wide convolution, first dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS5.p2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling and then the non-linear function are applied individually to each map.</p>
</div>
</div>
<div id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.6 </span>Folding</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p class="ltx_p">In the formulation of the network so far, feature detectors applied to an individual row of the sentence matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m1" class="ltx_Math" alttext="\mathbf{s}" display="inline"><mi>𝐬</mi></math> can have many orders and create complex dependencies across the same rows in multiple feature maps. Feature detectors in different rows, however, are independent of each other until the top fully connected layer. Full dependence between different rows could be achieved by making <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m2" class="ltx_Math" alttext="\mathbf{M}" display="inline"><mi>𝐌</mi></math> in Eq. <a href="#S3.E5" title="(5) ‣ 3.4 Non-linear Feature Function ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> a full matrix instead of a sparse matrix of diagonals. Here we explore a simpler method called <em class="ltx_emph">folding</em> that does not introduce any additional parameters. After a convolutional layer and before (dynamic) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling, one just sums every two rows in a feature map component-wise. For a map of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m4" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> rows, folding returns a map of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m5" class="ltx_Math" alttext="d/2" display="inline"><mrow><mi>d</mi><mo>/</mo><mn>2</mn></mrow></math> rows, thus halving the size of the representation. With a folding layer, a feature detector of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th order depends now on two rows of feature values in the lower maps of order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS6.p1.m7" class="ltx_Math" alttext="i-1" display="inline"><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></math>. This ends the description of the DCNN.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Properties of the Sentence Model</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We describe some of the properties of the sentence model based on the DCNN. We describe the notion of the <em class="ltx_emph">feature graph</em> induced over a sentence by the succession of convolutional and pooling layers. We briefly relate the properties to those of other neural sentence models.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Word and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-Gram Order</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">One of the basic properties is sensitivity to the order of the words in the input sentence. For most applications and in order to learn fine-grained feature detectors, it is beneficial for a model to be able to discriminate whether a specific <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram occurs in the input. Likewise, it is beneficial for a model to be able to tell the <em class="ltx_emph">relative</em> position of the most relevant <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams. The network is designed to capture these two aspects. The filters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m3" class="ltx_Math" alttext="\mathbf{m}" display="inline"><mi>𝐦</mi></math> of the wide convolution in the first layer can learn to recognise specific <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams that have size less or equal to the filter width <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m5" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math>; as we see in the experiments, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m6" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> in the first layer is often set to a relatively large value such as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m7" class="ltx_Math" alttext="10" display="inline"><mn>10</mn></math>. The subsequence of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m8" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams extracted by the generalised pooling operation induces invariance to absolute positions, but maintains their order and relative positions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">As regards the other neural sentence models, the class of NBoW models is by definition insensitive to word order. A sentence model based on a recurrent neural network is sensitive to word order, but it has a bias towards the latest words that it takes as input <cite class="ltx_cite">[]</cite>. This gives the RNN excellent performance at language modelling, but it is suboptimal for remembering at once the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams further back in the input sentence. Similarly, a recursive neural network is sensitive to word order but has a bias towards the topmost nodes in the tree; shallower trees mitigate this effect to some extent <cite class="ltx_cite">[]</cite>. As seen in Sect. <a href="#S2.SS3" title="2.3 Time-Delay Neural Networks ‣ 2 Background ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the Max-TDNN is sensitive to word order, but max pooling only picks out a single <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram feature in each row of the sentence matrix.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Induced Feature Graph</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Some sentence models use internal or external structure to compute the representation for the input sentence. In a DCNN, the convolution and pooling layers induce an internal feature graph over the input. A node from a layer is connected to a node from the next higher layer if the lower node is involved in the convolution that computes the value of the higher node. Nodes that are not selected by the pooling operation at a layer are dropped from the graph. After the last pooling layer, the remaining nodes connect to a single topmost root. The induced graph is a connected, directed acyclic graph with weighted edges and a root node; two equivalent representations of an induced graph are given in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In a DCNN without folding layers, each of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> rows of the sentence matrix induces a subgraph that joins the other subgraphs only at the root node. Each subgraph may have a different shape that reflects the kind of relations that are detected in that subgraph. The effect of folding layers is to join pairs of subgraphs at lower layers before the top root node.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">Convolutional networks for object recognition also induce a feature graph over the input image. What makes the feature graph of a DCNN peculiar is the global range of the pooling operations. The (dynamic) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling operator can draw together features that correspond to words that are many positions apart in the sentence. Higher-order features have highly variable ranges that can be either short and focused or global and long as the input sentence. Likewise, the edges of a subgraph in the induced graph reflect these varying ranges. The subgraphs can either be localised to one or more parts of the sentence or spread more widely across the sentence. This structure is internal to the network and is defined by the forward propagation of the input through the network.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">Of the other sentence models, the NBoW is a shallow model and the RNN has a linear chain structure. The subgraphs induced in the Max-TDNN model have a single fixed-range feature obtained through max pooling. The recursive neural network follows the structure of an external parse tree. Features of variable range are computed at each node of the tree combining one or more of the children of the tree. Unlike in a DCNN, where one learns a clear hierarchy of feature orders, in a RecNN low order features like those of single words can be directly combined with higher order features computed from entire clauses. A DCNN generalises many of the structural aspects of a RecNN. The feature extraction function as stated in Eq. <a href="#S3.E6" title="(6) ‣ 3.4 Non-linear Feature Function ‣ 3 Convolutional Neural Networks with Dynamic k-Max Pooling ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> has a more general form than that in a RecNN, where the value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m1" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> is generally 2. Likewise, the induced graph structure in a DCNN is more general than a parse tree in that it is not limited to syntactically dictated phrases; the graph structure can capture short or long-range semantic relations between words that do not necessarily correspond to the syntactic relations in a parse tree. The DCNN has internal input-dependent structure and does not rely on externally provided parse trees, which makes the DCNN directly applicable to hard-to-parse sentences such as tweets and to sentences from any language.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_small">Classifier</span></th>
<th class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_small">Fine-grained (%)</span></th>
<th class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_small">Binary (%)</span></th>
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">NB</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">41.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">81.8</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">BiNB</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">41.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">83.1</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">SVM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">40.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">79.4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">RecNTN</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">45.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">85.4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">Max-TDNN</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">37.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">77.1</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">NBoW</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">42.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">80.5</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">DCNN</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">48.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">86.8</span></td>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy of sentiment prediction in the movie reviews dataset. The first four results are reported from <cite class="ltx_cite"/>. The baselines <span class="ltx_text ltx_font_smallcaps">NB</span> and <span class="ltx_text ltx_font_smallcaps">BiNB</span> are Naive Bayes classifiers with, respectively, unigram features and unigram and bigram features. <span class="ltx_text ltx_font_smallcaps">SVM</span> is a support vector machine with unigram and bigram features. <span class="ltx_text ltx_font_smallcaps">RecNTN</span> is a recursive neural network with a tensor-based feature function, which relies on external structural features given by a parse tree and performs best among the RecNNs. </div>
</div>
<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We test the network on four different experiments. We begin by specifying aspects of the implementation and the training of the network. We then relate the results of the experiments and we inspect the learnt feature detectors.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Training</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">In each of the experiments, the top layer of the network has a fully connected layer followed by a softmax non-linearity that predicts the probability distribution over classes given the input sentence.
The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="L_{2}" display="inline"><msub><mi>L</mi><mn>2</mn></msub></math> regularisation term over the parameters.
The set of parameters comprises the word embeddings, the filter weights and the weights from the fully connected layers. The network is trained with mini-batches by backpropagation and the gradient-based optimisation is performed using the Adagrad update rule <cite class="ltx_cite">[]</cite>.
Using the well-known convolution theorem, we can compute fast one-dimensional linear convolutions at all rows of an input matrix by using Fast Fourier Transforms. To exploit the parallelism of the operations, we train the network on a GPU. A Matlab implementation processes multiple millions of input sentences per hour on one GPU, depending primarily on the number of layers used in the network.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Sentiment Prediction in Movie Reviews</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank <cite class="ltx_cite">[]</cite>. The output variable is binary in one experiment and can have five possible outcomes in the other: negative, somewhat negative, neutral, somewhat positive, positive. In the binary case, we use the given splits of 6920 training, 872 development and 1821 test sentences. Likewise, in the fine-grained case, we use the standard 8544/1101/2210 splits. Labelled phrases that occur as subparts of the training sentences are treated as independent training instances. The size of the vocabulary is 15448.</p>
</div>
<div id="S5.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_small">Classifier</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_small">Features</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_small">Acc. (%)</span></td>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_smallcaps ltx_font_small">Hier</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unigram, POS, head chunks</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">91.0</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">NE, semantic relations</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span class="ltx_text ltx_font_smallcaps ltx_font_small">MaxEnt</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unigram, bigram, trigram</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">92.6</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">POS, chunks, NE, supertags</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">CCG parser, WordNet</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span class="ltx_text ltx_font_smallcaps ltx_font_small">MaxEnt</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unigram, bigram, trigram</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">93.6</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">POS, wh-word, head word</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">word shape, parser</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">hypernyms, WordNet</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span class="ltx_text ltx_font_smallcaps ltx_font_small">SVM</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unigram, POS, wh-word</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">95.0</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">head word, parser</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">hypernyms, WordNet</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">60 hand-coded rules</span></td>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">Max-TDNN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unsupervised vectors</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">84.4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">NBoW</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">unsupervised vectors</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">88.2</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">DCNN</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">unsupervised vectors</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">93.0</span></td>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy of six-way question classification on the TREC questions dataset. The second column details the external features used in the various approaches. The first four results are respectively from <cite class="ltx_cite"/>, <cite class="ltx_cite"/>, <cite class="ltx_cite"/> and <cite class="ltx_cite"/>. </div>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">Table 1 details the results of the experiments. In the three neural sentence models—the Max-TDNN, the NBoW and the DCNN—the word vectors are parameters of the models that are randomly initialised; their dimension <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> is set to 48. The Max-TDNN has a filter of width <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="6" display="inline"><mn>6</mn></math> in its narrow convolution at the first layer; shorter phrases are padded with zero vectors. The convolutional layer is followed by a non-linearity, a max-pooling layer and a softmax classification layer. The NBoW sums the word vectors and applies a non-linearity followed by a softmax classification layer. The adopted non-linearity is the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="\tanh" display="inline"><mi>tanh</mi></math> function.
The hyper parameters of the DCNN are as follows. The binary result is based on a DCNN that has a wide convolutional layer followed by a folding layer, a dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling layer and a non-linearity; it has a second wide convolutional layer followed by a folding layer, a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m5" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling layer and a non-linearity. The width of the convolutional filters is 7 and 5, respectively. The value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m6" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> for the top <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m7" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling is 4. The number of feature maps at the first convolutional layer is 6; the number of maps at the second convolutional layer is 14. The network is topped by a softmax classification layer. The DCNN for the fine-grained result has the same architecture, but the filters have size 10 and 7, the top pooling parameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m8" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is 5 and the number of maps is, respectively, 6 and 12. The networks use the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m9" class="ltx_Math" alttext="\tanh" display="inline"><mi>tanh</mi></math> non-linear function. At training time we apply dropout to the penultimate layer after the last <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m10" class="ltx_Math" alttext="\tanh" display="inline"><mi>tanh</mi></math> non-linearity <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S5.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_small">Classifier</span></th>
<th class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_small">Accuracy (%)</span></th>
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">SVM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">81.6</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">BiNB</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">82.7</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">MaxEnt</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">83.0</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">Max-TDNN</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">78.8</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">NBoW</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">80.9</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_smallcaps ltx_font_small">DCNN</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">87.4</span></td>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_bb ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy on the Twitter sentiment dataset. The three non-neural classifiers are based on unigram and bigram features; the results are reported from <cite class="ltx_cite">[]</cite>. </div>
</div>
<div id="S5.F4" class="ltx_figure"><img src="P14-1062/image004.png" id="S5.F4.g1" class="ltx_graphics ltx_centering" width="676" height="130" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top five <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.F4.m2" class="ltx_Math" alttext="7" display="inline"><mn>7</mn></math>-grams at four feature detectors in the first layer of the network.</div>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">We see that the DCNN significantly outperforms the other neural and non-neural models. The NBoW performs similarly to the non-neural <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram based classifiers. The Max-TDNN performs worse than the NBoW likely due to the excessive pooling of the max pooling operation; the latter discards most of the sentiment features of the words in the input sentence.
Besides the RecNN that uses an external parser to produce structural features for the model, the other models use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram based or neural features that do not require external resources or additional annotations. In the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Question Type Classification</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information <cite class="ltx_cite">[]</cite>. The training dataset consists of 5452 labelled questions whereas the test dataset consists of 500 questions.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">The results are reported in Tab. 2. The non-neural approaches use a classifier over a large number of manually engineered features and hand-coded resources. For instance, <cite class="ltx_cite"/> present a Maximum Entropy model that relies on 26 sets of syntactic and semantic features including unigrams, bigrams, trigrams, POS tags, named entity tags, structural relations from a CCG parse and WordNet synsets.
We evaluate the three neural models on this dataset with mostly the same hyper-parameters as in the binary sentiment experiment of Sect. <a href="#S5.SS2" title="5.2 Sentiment Prediction in Movie Reviews ‣ 5 Experiments ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. As the dataset is rather small, we use lower-dimensional word vectors with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p2.m1" class="ltx_Math" alttext="d=32" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>32</mn></mrow></math> that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence <cite class="ltx_cite">[]</cite>. The DCNN uses a single convolutional layer with filters of size 8 and 5 feature maps. The difference between the performance of the DCNN and that of the other high-performing methods in Tab. 2 is not significant (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p2.m2" class="ltx_Math" alttext="p&lt;0.09" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.09</mn></mrow></math>). Given that the only labelled information used to train the network is the training set itself, it is notable that the network matches the performance of state-of-the-art classifiers that rely on large amounts of engineered features and rules and hand-coded resources.</p>
</div>
</div>
<div id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.4 </span>Twitter Sentiment Prediction with Distant Supervision</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">In our final experiment, we train the models on a large dataset of tweets, where a tweet is automatically labelled as positive or negative depending on the emoticon that occurs in it. The training set consists of 1.6 million tweets with emoticon-based labels and the test set of about 400 hand-annotated tweets. We preprocess the tweets minimally following the procedure described in <cite class="ltx_cite"/>; in addition, we also lowercase all the tokens. This results in a vocabulary of 76643 word types. The architecture of the DCNN and of the other neural models is the same as the one used in the binary experiment of Sect. <a href="#S5.SS2" title="5.2 Sentiment Prediction in Movie Reviews ‣ 5 Experiments ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. The randomly initialised word embeddings are increased in length to a dimension of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m1" class="ltx_Math" alttext="d=60" display="inline"><mrow><mi>d</mi><mo>=</mo><mn>60</mn></mrow></math>. Table 3 reports the results of the experiments. We see a significant increase in the performance of the DCNN with respect to the non-neural <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram based classifiers; in the presence of large amounts of training data these classifiers constitute particularly strong baselines. We see that the ability to train a sentiment classifier on automatically extracted emoticon-based labels extends to the DCNN and results in highly accurate performance. The difference in performance between the DCNN and the NBoW further suggests that the ability of the DCNN to both capture features based on long <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and to hierarchically combine these features is highly beneficial.</p>
</div>
</div>
<div id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.5 </span>Visualising Feature Detectors</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p class="ltx_p">A filter in the DCNN is associated with a feature detector or neuron that learns during training to be particularly active when presented with a specific sequence of input words. In the first layer, the sequence is a continuous <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram from the input sentence; in higher layers, sequences can be made of multiple separate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams. We visualise the feature detectors in the first layer of the network trained on the binary sentiment task (Sect. <a href="#S5.SS2" title="5.2 Sentiment Prediction in Movie Reviews ‣ 5 Experiments ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>). Since the filters have width 7, for each of the 288 feature detectors we rank all <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m3" class="ltx_Math" alttext="7" display="inline"><mn>7</mn></math>-grams occurring in the validation and test sets according to their activation of the detector. Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Sentiment Prediction in Movie Reviews ‣ 5 Experiments ‣ A Convolutional Neural Network for Modelling Sentences" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the top five <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m4" class="ltx_Math" alttext="7" display="inline"><mn>7</mn></math>-grams for four feature detectors. Besides the expected detectors for positive and negative sentiment, we find detectors for particles such as ‘not’ that negate sentiment and such as ‘too’ that potentiate sentiment. We find detectors for multiple other notable constructs including ‘all’, ‘or’, ‘with…that’, ‘as…as’. The feature detectors learn to recognise not just single <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams, but patterns within <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS5.p1.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams that have syntactic, semantic or structural significance.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We have described a dynamic convolutional neural network that uses the dynamic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-max pooling operator as a non-linear subsampling function. The feature graph induced by the network is able to capture word relations of varying size. The network achieves high performance on question and sentiment classification without requiring external features as provided by parsers or other resources.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We thank Nando de Freitas and Yee Whye Teh for great discussions on the paper. This work was supported by a Xerox Foundation Award, EPSRC grant number EP/F042728/1, and EPSRC grant number EP/K036580/1.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:02:14 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
