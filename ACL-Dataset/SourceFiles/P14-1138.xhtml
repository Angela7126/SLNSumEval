<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Recurrent Neural Networks for Word Alignment Model</title>
<!--Generated on Tue Jun 10 19:27:01 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Recurrent Neural Networks for Word Alignment Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Akihiro Tamura, Taro Watanabe, Eiichiro Sumita
<br class="ltx_break"/>National Institute of Information and Communications Technology 
<br class="ltx_break"/>3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">a-tamura@ah.jp.nec.com,</span> 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">taro.watanabe, eiichiro.sumita</span>}<span class="ltx_text ltx_font_typewriter">@nict.go.jp
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes"><span>Â The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. </span></span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers.
We perform unsupervised learning using noise-contrastive estimation <cite class="ltx_cite">[<a href="#bib.bib2" title="Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models" class="ltx_ref">15</a>, <a href="#bib.bib3" title="A Fast and Simple Algorithm for Training Neural Probabilistic Language Models" class="ltx_ref">26</a>]</cite>, which utilizes artificially generated negative samples.
Our alignment model is directional, similar to the generative IBM models <cite class="ltx_cite">[<a href="#bib.bib4" title="The Mathematics of Statistical Machine Translation: Parameter Estimation" class="ltx_ref">4</a>]</cite>.
To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training.
The RNN-based model outperforms the feed-forward neural network-based model <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Automatic word alignment is an important task for statistical machine translation.
The most classical approaches are the probabilistic IBM models 1-5 <cite class="ltx_cite">[<a href="#bib.bib4" title="The Mathematics of Statistical Machine Translation: Parameter Estimation" class="ltx_ref">4</a>]</cite> and the HMM model <cite class="ltx_cite">[<a href="#bib.bib5" title="Hmm-based Word Alignment in Statistical Translation" class="ltx_ref">39</a>]</cite>.
Various studies have extended those models.
Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> adapted the Context-Dependent Deep Neural Network for HMM (CD-DNN-HMM) <cite class="ltx_cite">[<a href="#bib.bib9" title="Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition" class="ltx_ref">7</a>]</cite>, a type of feed-forward neural network (FFNN)-based model, to the HMM alignment model and achieved state-of-the-art performance.
However, the FFNN-based model assumes a first-order Markov dependence for alignments.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks <cite class="ltx_cite">[<a href="#bib.bib11" title="Recurrent Neural Network based Language Model" class="ltx_ref">24</a>, <a href="#bib.bib10" title="Context Dependent Recurrent Neural Network Language Model" class="ltx_ref">25</a>, <a href="#bib.bib13" title="Joint Language and Translation Modeling with Recurrent Neural Networks" class="ltx_ref">1</a>, <a href="#bib.bib12" title="Recurrent Continuous Translation Models" class="ltx_ref">16</a>, <a href="#bib.bib14" title="Comparison of Feedforward and Recurrent Neural Network Language Models" class="ltx_ref">34</a>]</cite>.
An RNN has a hidden layer with recurrent connections that propagates its own previous signals.
Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data.
We assume that this property would fit with a word alignment task, and
we propose an RNN-based word alignment model.
Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">The NN-based alignment models are supervised models.
Unfortunately, it is usually difficult to prepare word-by-word aligned bilingual data.
Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> trained their model from word alignments produced by traditional unsupervised probabilistic models.
However, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited.
To solve this problem, we apply noise-contrastive estimation (NCE) <cite class="ltx_cite">[<a href="#bib.bib2" title="Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models" class="ltx_ref">15</a>, <a href="#bib.bib3" title="A Fast and Simple Algorithm for Training Neural Probabilistic Language Models" class="ltx_ref">26</a>]</cite> for unsupervised training of our RNN-based model without gold standard alignments or pseudo-oracle alignments.
NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Our RNN-based alignment model has a direction, such as other alignment models, i.e., from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m1" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> (source language) to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m2" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> (target language) and from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m3" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m4" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>.
It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently <cite class="ltx_cite">[<a href="#bib.bib15" title="Symmetric Word Alignments for Statistical Machine Translation" class="ltx_ref">22</a>, <a href="#bib.bib16" title="Alignment by Agreement" class="ltx_ref">21</a>, <a href="#bib.bib17" title="Expectation Maximization and Posterior Constraints" class="ltx_ref">14</a>, <a href="#bib.bib18" title="Better Alignments = Better Translations?" class="ltx_ref">11</a>]</cite>.
The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other.
Based on this motivation, our directional models are also simultaneously trained.
Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function.
This constraint prevents each model from overfitting to a particular direction and leads to global optimization across alignment directions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">This paper presents evaluations of Japanese-English and French-English word alignment tasks and Japanese-to-English and Chinese-to-English translation tasks.
The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks.
For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Various word alignment models have been proposed.
These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. <cite class="ltx_cite">[<a href="#bib.bib4" title="The Mathematics of Statistical Machine Translation: Parameter Estimation" class="ltx_ref">4</a>]</cite>, Vogel et al. <cite class="ltx_cite">[<a href="#bib.bib5" title="Hmm-based Word Alignment in Statistical Translation" class="ltx_ref">39</a>]</cite>, and Och and Ney <cite class="ltx_cite">[<a href="#bib.bib8" title="A Systematic Comparison of Various Statistical Alignment Models" class="ltx_ref">28</a>]</cite>, and discriminative models, such as those proposed by Taskar et al. <cite class="ltx_cite">[<a href="#bib.bib38" title="A Discriminative Matching Approach to Word Alignment" class="ltx_ref">36</a>]</cite>, Moore <cite class="ltx_cite">[<a href="#bib.bib37" title="A Discriminative Framework for Bilingual Word Alignment" class="ltx_ref">27</a>]</cite>, and Blunsom and Cohn <cite class="ltx_cite">[<a href="#bib.bib39" title="Discriminative Word Alignment with Conditional Random Fields" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Generative Alignment Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">Given a source language sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="f_{1}^{J}=f_{1},...,f_{J}" display="inline"><mrow><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">â¦</mi><mo>,</mo><msub><mi>f</mi><mi>J</mi></msub></mrow></mrow></math> and a target language sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="e_{1}^{I}=e_{1},...,e_{I}" display="inline"><mrow><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>=</mo><mrow><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">â¦</mi><mo>,</mo><msub><mi>e</mi><mi>I</mi></msub></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="f_{1}^{J}" display="inline"><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup></math> is generated by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="e_{1}^{I}" display="inline"><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup></math> via the alignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m5" class="ltx_Math" alttext="a_{1}^{J}=a_{1},...,a_{J}" display="inline"><mrow><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>=</mo><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">â¦</mi><mo>,</mo><msub><mi>a</mi><mi>J</mi></msub></mrow></mrow></math>.
Each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m6" class="ltx_Math" alttext="a_{j}" display="inline"><msub><mi>a</mi><mi>j</mi></msub></math> is a hidden variable indicating that the source word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m7" class="ltx_Math" alttext="f_{j}" display="inline"><msub><mi>f</mi><mi>j</mi></msub></math> is aligned to the target word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m8" class="ltx_Math" alttext="e_{a_{j}}" display="inline"><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub></math>.
Usually, a ânullâ word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m9" class="ltx_Math" alttext="e_{0}" display="inline"><msub><mi>e</mi><mn>0</mn></msub></math> is added to the target language sentence and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m10" class="ltx_Math" alttext="a_{1}^{J}" display="inline"><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup></math> may contain <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m11" class="ltx_Math" alttext="a_{j}=0" display="inline"><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow></math>, which indicates that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m12" class="ltx_Math" alttext="f_{j}" display="inline"><msub><mi>f</mi><mi>j</mi></msub></math> is not aligned to any target word.
The probability of generating the sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m13" class="ltx_Math" alttext="f_{1}^{J}" display="inline"><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup></math> from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m14" class="ltx_Math" alttext="e_{1}^{I}" display="inline"><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup></math> is defined as</p>
<table id="S2.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m1" class="ltx_Math" alttext="p(f_{1}^{J}|e_{1}^{I})=\sum_{a_{1}^{J}}p(f_{1}^{J},a_{1}^{J}|e_{1}^{I})." display="block"><mrow><mi>p</mi><mrow><mo>(</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>=</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup></munder><mi>p</mi><mrow><mo>(</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">The IBM Models 1 and 2 and the HMM model decompose it into an alignment probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m15" class="ltx_Math" alttext="p_{a}" display="inline"><msub><mi>p</mi><mi>a</mi></msub></math> and a lexical translation probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m16" class="ltx_Math" alttext="p_{t}" display="inline"><msub><mi>p</mi><mi>t</mi></msub></math> as</p>
<table id="S2.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center">
<span class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:263.4pt;height:31.8055555555556px;vertical-align:-9.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.7pt,-0.2pt) scale(0.98,0.98) ;-webkit-transform:translate(-2.7pt,-0.2pt) scale(0.98,0.98) ;-ms-transform:translate(-2.7pt,-0.2pt) scale(0.98,0.98) ;">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m1.m1" class="ltx_Math" alttext="\displaystyle p(f_{1}^{J},a_{1}^{J}|e_{1}^{I})=\prod_{j=1}^{J}p_{a}(a_{j}|a_{j%&#10;-1},j)p_{t}(f_{j}|e_{a_{j}})." display="inline"><mrow><mi>p</mi><mrow><mo>(</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><msub><mi>p</mi><mi>a</mi></msub><mrow><mo>(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>|</mo><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>j</mi><mo>)</mo></mrow><msub><mi>p</mi><mi>t</mi></msub><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>|</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow><mo>.</mo></mrow></math></p>
</span></span></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="0" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">The three models differ in their definition of alignment probability.
For example, the HMM model uses an alignment probability with a first-order Markov property: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m17" class="ltx_Math" alttext="p_{a}(a_{j}|a_{j}-a_{j-1})" display="inline"><mrow><msub><mi>p</mi><mi>a</mi></msub><mrow><mo>(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>|</mo><msub><mi>a</mi><mi>j</mi></msub><mo>-</mo><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></math>.
In addition, the IBM models 3-5 are extensions of these, which consider the fertility and distortion of each translated word.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">These models are trained using the expectation-maximization algorithm <cite class="ltx_cite">[<a href="#bib.bib19" title="Maximum Likelihood from Incomplete Data via the EM Algorithm" class="ltx_ref">8</a>]</cite> from bilingual sentences without word-level alignments (unlabeled training data).
Given a specific model, the best alignment (Viterbi alignment) of the sentence pair (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="f_{1}^{J}" display="inline"><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="e_{1}^{I}" display="inline"><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup></math>) can be found as</p>
<table id="S2.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m1" class="ltx_Math" alttext="\hat{a}_{1}^{J}=\argmax_{a_{1}^{J}}p(f_{1}^{J},a_{1}^{J}|e_{1}^{I})." display="block"><mrow><msubsup><mover accent="true"><mi>a</mi><mo stretchy="false">^</mo></mover><mn>1</mn><mi>J</mi></msubsup><mo>=</mo><msub><merror class="ltx_ERROR undefined undefined"><mtext>\argmax</mtext></merror><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup></msub><mi>p</mi><mrow><mo>(</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>,</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>FFNN-based Alignment Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">As an instance of discriminative models, we describe an FFNN-based word alignment model <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite>, which is our baseline.
An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data.
Recently, FFNNs have been applied successfully to several tasks, such as speech recognition <cite class="ltx_cite">[<a href="#bib.bib9" title="Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition" class="ltx_ref">7</a>]</cite>, statistical machine translation <cite class="ltx_cite">[<a href="#bib.bib21" title="Continuous Space Translation Models with Neural Networks" class="ltx_ref">20</a>, <a href="#bib.bib20" title="Decoding with Large-Scale Neural Language Models Improves Translation" class="ltx_ref">38</a>]</cite>, and other popular natural language processing tasks <cite class="ltx_cite">[<a href="#bib.bib23" title="A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning" class="ltx_ref">6</a>, <a href="#bib.bib22" title="Natural Language Processing (Almost) from Scratch" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> have adapted a type of FFNN, i.e., CD-DNN-HMM <cite class="ltx_cite">[<a href="#bib.bib9" title="Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition" class="ltx_ref">7</a>]</cite>, to the HMM alignment model.
Specifically, the lexical translation and alignment probability in Eq. <a href="#S2.E2" title="(2) â£ 2.1 Generative Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are computed using FFNNs as</p>
<table id="S2.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E4.m1" class="ltx_Math" alttext="\scalebox{1.0}{$\displaystyle s_{NN}(a_{1}^{J}|f_{1}^{J},e_{1}^{I})=\prod_{j=1%&#10;}^{J}t_{a}(a_{j}-a_{j-1}|c(e_{a_{j-1}}))$}\\&#10;\scalebox{1.0}{$\displaystyle\cdot t_{lex}(f_{j},e_{a_{j}}|c(f_{j}),c(e_{a_{j}%&#10;})),$}" display="block"><mrow><mrow><msub><mi>s</mi><mrow><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub><mrow><mo>(</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>,</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>=</mo><munderover><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><msub><mi>t</mi><mi>a</mi></msub><mrow><mo>(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>-</mo><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>|</mo><mi>c</mi><mrow><mo>(</mo><msub><mi>e</mi><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></msub><mo>)</mo></mrow><mo>)</mo></mrow></mrow><mo>â¢</mo><mrow><mo>â</mo><msub><mi>t</mi><mrow><mi>l</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>x</mi></mrow></msub><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>,</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>|</mo><mi>c</mi><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>)</mo></mrow><mo>,</mo><mi>c</mi><mrow><mo>(</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow><mo>)</mo></mrow><mo>,</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m1" class="ltx_Math" alttext="t_{a}" display="inline"><msub><mi>t</mi><mi>a</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m2" class="ltx_Math" alttext="t_{lex}" display="inline"><msub><mi>t</mi><mrow><mi>l</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>x</mi></mrow></msub></math> are an alignment score and a lexical translation score, respectively, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m3" class="ltx_Math" alttext="s_{NN}" display="inline"><msub><mi>s</mi><mrow><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub></math> is a score of alignments <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m4" class="ltx_Math" alttext="a_{1}^{J}" display="inline"><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup></math>, and â<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m5" class="ltx_Math" alttext="c(\text{a word }w)" display="inline"><mrow><mi>c</mi><mo>â¢</mo><mrow><mo>(</mo><mrow><mtext>a wordÂ </mtext><mo>â¢</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></math>â denotes a context of word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m6" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math>.
Note that the model uses nonprobabilistic scores rather than probabilities because normalization over all words is computationally expensive.
The model finds the Viterbi alignment using the Viterbi algorithm, similar to the classic HMM model.
Note that alignments in the FFNN-based model are also governed by first-order Markov dynamics because an alignment score depends on the previous alignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m7" class="ltx_Math" alttext="a_{j-1}" display="inline"><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></math>.</p>
</div>
<div id="S2.F1" class="ltx_figure"><img src="P14-1138/image001.png" id="S2.F1.g1" class="ltx_graphics ltx_centering" width="474" height="673" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">FigureÂ 1: </span>FFNN-based model for computing a lexical translation score of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m2" class="ltx_Math" alttext="(f_{j},e_{a_{j}})" display="inline"><mrow><mo>(</mo><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>,</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub></mrow><mo>)</mo></mrow></math></div>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">Figure <a href="#S2.F1" title="FigureÂ 1 â£ 2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the network structure with one hidden layer for computing a lexical translation probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="t_{lex}(f_{j},e_{a_{j}}|c(f_{j}),c(e_{a_{j}}))" display="inline"><mrow><msub><mi>t</mi><mrow><mi>l</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>x</mi></mrow></msub><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>,</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>|</mo><mi>c</mi><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>)</mo></mrow><mo>,</mo><mi>c</mi><mrow><mo>(</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow><mo>)</mo></mrow></mrow></math>.
The model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices.
The model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure <a href="#S2.F1" title="FigureÂ 1 â£ 2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
First, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>), and then concatenates them.
Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m3" class="ltx_Math" alttext="V_{f}" display="inline"><msub><mi>V</mi><mi>f</mi></msub></math> (or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m4" class="ltx_Math" alttext="V_{e}" display="inline"><msub><mi>V</mi><mi>e</mi></msub></math>) be a set of source words (or target words) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m5" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> be a predetermined embedding length.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m6" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> is a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m7" class="ltx_Math" alttext="M\times(|V_{f}|+|V_{e}|)" display="inline"><mrow><mi>M</mi><mo>Ã</mo><mrow><mo>(</mo><mrow><mrow><mo fence="true">|</mo><msub><mi>V</mi><mi>f</mi></msub><mo fence="true">|</mo></mrow><mo>+</mo><mrow><mo fence="true">|</mo><msub><mi>V</mi><mi>e</mi></msub><mo fence="true">|</mo></mrow></mrow><mo>)</mo></mrow></mrow></math> matrix<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>We add a special token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m8" class="ltx_Math" alttext="\langle unk\rangle" display="inline"><mrow><mo>â¨</mo><mrow><mi>u</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>k</mi></mrow><mo>â©</mo></mrow></math> to handle unknown words and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m9" class="ltx_Math" alttext="\langle null\rangle" display="inline"><mrow><mo>â¨</mo><mrow><mi>n</mi><mo>â¢</mo><mi>u</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>l</mi></mrow><mo>â©</mo></mrow></math> to handle null alignments to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m10" class="ltx_Math" alttext="V_{f}" display="inline"><msub><mi>V</mi><mi>f</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m11" class="ltx_Math" alttext="V_{e}" display="inline"><msub><mi>V</mi><mi>e</mi></msub></math></span></span></span>.
Word embeddings are dense, low dimensional, and real-valued vectors that can capture syntactic and semantic properties of the words <cite class="ltx_cite">[<a href="#bib.bib24" title="A Neural Probabilistic Language Model" class="ltx_ref">2</a>]</cite>.
The concatenation (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m12" class="ltx_Math" alttext="z_{0}" display="inline"><msub><mi>z</mi><mn>0</mn></msub></math>) is then fed to the hidden layer to capture nonlinear relations.
Finally, the output layer receives the output of the hidden layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m13" class="ltx_Math" alttext="z_{1}" display="inline"><msub><mi>z</mi><mn>1</mn></msub></math>) and computes a lexical translation score.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">The computations in the hidden and output layer are as follows<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Consecutive <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m1" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math> hidden layers can be used: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m2" class="ltx_Math" alttext="z_{l}=f(H_{l}\times z_{l-1}+B_{H_{l}})" display="inline"><mrow><msub><mi>z</mi><mi>l</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>H</mi><mi>l</mi></msub><mo>Ã</mo><msub><mi>z</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msub><mi>B</mi><msub><mi>H</mi><mi>l</mi></msub></msub></mrow><mo>)</mo></mrow></mrow></mrow></math>. For simplicity, this paper describes the model with 1 hidden layer.</span></span></span>:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E5.m3" class="ltx_Math" alttext="\displaystyle z_{1}=f(H\times z_{0}+B_{H})," display="inline"><mrow><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>=</mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mrow><mrow><mi>H</mi><mo>Ã</mo><msub><mi>z</mi><mn>0</mn></msub></mrow><mo>+</mo><msub><mi>B</mi><mi>H</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
<tr id="S2.E6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E6.m3" class="ltx_Math" alttext="\displaystyle t_{lex}=O\times z_{1}+B_{O}," display="inline"><mrow><mrow><msub><mi>t</mi><mrow><mi>l</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>x</mi></mrow></msub><mo>=</mo><mrow><mrow><mi>O</mi><mo>Ã</mo><msub><mi>z</mi><mn>1</mn></msub></mrow><mo>+</mo><msub><mi>B</mi><mi>O</mi></msub></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m4" class="ltx_Math" alttext="B_{H}" display="inline"><msub><mi>B</mi><mi>H</mi></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m5" class="ltx_Math" alttext="O" display="inline"><mi>O</mi></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m6" class="ltx_Math" alttext="B_{O}" display="inline"><msub><mi>B</mi><mi>O</mi></msub></math> are <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m7" class="ltx_Math" alttext="|z_{1}|\times|z_{0}|" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>1</mn></msub><mo fence="true">|</mo></mrow><mo>Ã</mo><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>0</mn></msub><mo fence="true">|</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m8" class="ltx_Math" alttext="|z_{1}|\times 1" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>1</mn></msub><mo fence="true">|</mo></mrow><mo>Ã</mo><mn>1</mn></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m9" class="ltx_Math" alttext="1\times|z_{1}|" display="inline"><mrow><mn>1</mn><mo>Ã</mo><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>1</mn></msub><mo fence="true">|</mo></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m10" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo>Ã</mo><mn>1</mn></mrow></math> matrices, respectively, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m11" class="ltx_Math" alttext="f(x)" display="inline"><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math> is an activation function.
Following Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite>, a âhardâ version of the hyperbolic tangent, htanh<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m12" class="ltx_Math" alttext="(x)" display="inline"><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>htanh<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m13" class="ltx_Math" alttext="(x)=-1" display="inline"><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></math> for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m14" class="ltx_Math" alttext="x&lt;-1" display="inline"><mrow><mi>x</mi><mo>&lt;</mo><mrow><mo>-</mo><mn>1</mn></mrow></mrow></math>, htanh<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m15" class="ltx_Math" alttext="(x)=1" display="inline"><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mn>1</mn></mrow></math> for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m16" class="ltx_Math" alttext="x&gt;1" display="inline"><mrow><mi>x</mi><mo>&gt;</mo><mn>1</mn></mrow></math>, and htanh<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m17" class="ltx_Math" alttext="(x)=x" display="inline"><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mi>x</mi></mrow></math> for others.</span></span></span>, is used as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m18" class="ltx_Math" alttext="f(x)" display="inline"><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math> in this study.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p class="ltx_p">The alignment model based on an FFNN is formed in the same manner as the lexical translation model.
Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD)<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>In our experiments, we used a mini-batch SGD instead of a plain SGD.</span></span></span>, where gradients are computed by the back-propagation algorithm <cite class="ltx_cite">[<a href="#bib.bib25" title="Learning Internal Representations by Error Propagation" class="ltx_ref">31</a>]</cite>:</p>
<table id="S2.E7" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m1" class="ltx_Math" alttext="\hbox to 0.0pt{$\displaystyle\scalebox{1.0}{$\displaystyle loss(\theta)=\sum_{%&#10;(\bm{f},\bm{e})\in T}\text{max}\{0,1-s_{\theta}(\bm{a^{+}}|\bm{f},\bm{e})$}$}%&#10;\\&#10;\scalebox{1.0}{$\displaystyle+s_{\theta}(\bm{a^{-}}|\bm{f},\bm{e})\},$}" display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo>(</mo><mi>Î¸</mi><mo>)</mo></mrow><mo>=</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mrow><mo>(</mo><mrow><mi>ð</mi><mo>,</mo><mi>ð</mi></mrow><mo>)</mo></mrow><mo>â</mo><mi>T</mi></mrow></munder><mtext>max</mtext><mrow><mo>{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>-</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>|</mo><mi>ð</mi><mo>,</mo><mi>ð</mi><mo>)</mo></mrow></mrow></mrow><mo>â¢</mo><mrow><mo>+</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup><mo>|</mo><mi>ð</mi><mo>,</mo><mi>ð</mi><mo>)</mo></mrow><mo>}</mo><mo>,</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m1" class="ltx_Math" alttext="\theta" display="inline"><mi>Î¸</mi></math> denotes the weights of layers in the model, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m2" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is a set of training data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m3" class="ltx_Math" alttext="\bm{a^{+}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></math> is the gold standard alignment, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m4" class="ltx_Math" alttext="\bm{a^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math> is the incorrect alignment with the highest score under <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m5" class="ltx_Math" alttext="\theta" display="inline"><mi>Î¸</mi></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m6" class="ltx_Math" alttext="s_{\theta}" display="inline"><msub><mi>s</mi><mi>Î¸</mi></msub></math> denotes the score defined by Eq. <a href="#S2.E4" title="(4) â£ 2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> as computed by the model under <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m7" class="ltx_Math" alttext="\theta" display="inline"><mi>Î¸</mi></math>.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>RNN-based Alignment Model</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">This section proposes an RNN-based alignment model, which computes a score for alignments <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="a_{1}^{J}" display="inline"><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup></math> using an RNN:</p>
<table id="S3.E8" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center">
<span class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:227.7pt;height:30.2777777777777px;vertical-align:-8.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.6pt,-0.8pt) scale(0.93,0.93) ;-webkit-transform:translate(-8.6pt,-0.8pt) scale(0.93,0.93) ;-ms-transform:translate(-8.6pt,-0.8pt) scale(0.93,0.93) ;">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E8.m1.m1" class="ltx_Math" alttext="\displaystyle s_{NN}(a_{1}^{J}|f_{1}^{J},e_{1}^{I})=\prod_{j=1}^{J}t_{RNN}(a_{%&#10;j}|a_{1}^{j-1},f_{j},e_{a_{j}})," display="inline"><mrow><msub><mi>s</mi><mrow><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub><mrow><mo>(</mo><msubsup><mi>a</mi><mn>1</mn><mi>J</mi></msubsup><mo>|</mo><msubsup><mi>f</mi><mn>1</mn><mi>J</mi></msubsup><mo>,</mo><msubsup><mi>e</mi><mn>1</mn><mi>I</mi></msubsup><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover></mstyle><msub><mi>t</mi><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub><mrow><mo>(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>|</mo><msubsup><mi>a</mi><mn>1</mn><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msub><mi>f</mi><mi>j</mi></msub><mo>,</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow><mo>,</mo></mrow></math></p>
</span></span></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="0" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="t_{RNN}" display="inline"><msub><mi>t</mi><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub></math> is the score of an alignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="a_{j}" display="inline"><msub><mi>a</mi><mi>j</mi></msub></math>.
The prediction of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m4" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math>-th alignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m5" class="ltx_Math" alttext="a_{j}" display="inline"><msub><mi>a</mi><mi>j</mi></msub></math> depends on all preceding alignments <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m6" class="ltx_Math" alttext="a_{1}^{j-1}" display="inline"><msubsup><mi>a</mi><mn>1</mn><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math>.
Note that the proposed model also uses nonprobabilistic scores, similar to the FFNN-based model.</p>
</div>
<div id="S3.F2" class="ltx_figure"><img src="P14-1138/image002.png" id="S3.F2.g1" class="ltx_graphics ltx_centering" width="591" height="837" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">FigureÂ 2: </span>RNN-based alignment model</div>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The RNN-based model is illustrated in Figure <a href="#S3.F2" title="FigureÂ 2 â£ 3 RNN-based Alignment Model â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The model consists of a lookup layer, a hidden layer, and an output layer, which have weight matrices <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="\{H^{d},R^{d},B_{H}^{d}\}" display="inline"><mrow><mo>{</mo><mrow><msup><mi>H</mi><mi>d</mi></msup><mo>,</mo><msup><mi>R</mi><mi>d</mi></msup><mo>,</mo><msubsup><mi>B</mi><mi>H</mi><mi>d</mi></msubsup></mrow><mo>}</mo></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m3" class="ltx_Math" alttext="\{O,B_{O}\}" display="inline"><mrow><mo>{</mo><mrow><mi>O</mi><mo>,</mo><msub><mi>B</mi><mi>O</mi></msub></mrow><mo>}</mo></mrow></math>, respectively.
Each matrix in the hidden layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m4" class="ltx_Math" alttext="H^{d}" display="inline"><msup><mi>H</mi><mi>d</mi></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m5" class="ltx_Math" alttext="R^{d}" display="inline"><msup><mi>R</mi><mi>d</mi></msup></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m6" class="ltx_Math" alttext="B_{H}^{d}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mi>d</mi></msubsup></math>) depends on alignment, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m7" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> denotes the jump distance from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m8" class="ltx_Math" alttext="a_{j-1}" display="inline"><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m9" class="ltx_Math" alttext="a_{j}" display="inline"><msub><mi>a</mi><mi>j</mi></msub></math>: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m10" class="ltx_Math" alttext="d=a_{j}-a_{j-1}" display="inline"><mrow><mi>d</mi><mo>=</mo><mrow><msub><mi>a</mi><mi>j</mi></msub><mo>-</mo><msub><mi>a</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow></math>.
In our experiments, we merge distances that are greater than 8 and less than -8 into the special â<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m11" class="ltx_Math" alttext="\geq" display="inline"><mo>â¥</mo></math>8â and â<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m12" class="ltx_Math" alttext="\leq" display="inline"><mo>â¤</mo></math>-8â distances, respectively.
Specifically, the hidden layer has weight matrices {<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m13" class="ltx_Math" alttext="H^{\leq-8}" display="inline"><msup><mi>H</mi><mrow><mi/><mo>â¤</mo><mrow><mo>-</mo><mn>8</mn></mrow></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m14" class="ltx_Math" alttext="H^{-7}" display="inline"><msup><mi>H</mi><mrow><mo>-</mo><mn>7</mn></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m15" class="ltx_Math" alttext="\cdots" display="inline"><mi mathvariant="normal">â¯</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m16" class="ltx_Math" alttext="H^{7}" display="inline"><msup><mi>H</mi><mn>7</mn></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m17" class="ltx_Math" alttext="H^{\geq 8}" display="inline"><msup><mi>H</mi><mrow><mi/><mo>â¥</mo><mn>8</mn></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m18" class="ltx_Math" alttext="R^{\leq-8}" display="inline"><msup><mi>R</mi><mrow><mi/><mo>â¤</mo><mrow><mo>-</mo><mn>8</mn></mrow></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m19" class="ltx_Math" alttext="R^{-7}" display="inline"><msup><mi>R</mi><mrow><mo>-</mo><mn>7</mn></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m20" class="ltx_Math" alttext="\cdots" display="inline"><mi mathvariant="normal">â¯</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m21" class="ltx_Math" alttext="R^{7}" display="inline"><msup><mi>R</mi><mn>7</mn></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m22" class="ltx_Math" alttext="R^{\geq 8}" display="inline"><msup><mi>R</mi><mrow><mi/><mo>â¥</mo><mn>8</mn></mrow></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m23" class="ltx_Math" alttext="B_{H}^{\leq-8}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mrow><mi/><mo>â¤</mo><mrow><mo>-</mo><mn>8</mn></mrow></mrow></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m24" class="ltx_Math" alttext="B_{H}^{-7}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mrow><mo>-</mo><mn>7</mn></mrow></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m25" class="ltx_Math" alttext="\cdots" display="inline"><mi mathvariant="normal">â¯</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m26" class="ltx_Math" alttext="B_{H}^{7}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mn>7</mn></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m27" class="ltx_Math" alttext="B_{H}^{\geq 8}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mrow><mi/><mo>â¥</mo><mn>8</mn></mrow></msubsup></math>} and computes <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m28" class="ltx_Math" alttext="y_{j}" display="inline"><msub><mi>y</mi><mi>j</mi></msub></math> using the corresponding matrices of the jump distance <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m29" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The Viterbi alignment is determined using the Viterbi algorithm, similar to the FFNN-based model, where the model is sequentially applied from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m1" class="ltx_Math" alttext="f_{1}" display="inline"><msub><mi>f</mi><mn>1</mn></msub></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m2" class="ltx_Math" alttext="f_{J}" display="inline"><msub><mi>f</mi><mi>J</mi></msub></math><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>Strictly speaking, we cannot apply the dynamic programming forward-backward algorithm (i.e., the Viterbi algorithm) due to the long alignment history of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m3" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
Thus, the Viterbi alignment is computed approximately using heuristic beam search.
</span></span></span>.
When computing the score of the alignment between <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m4" class="ltx_Math" alttext="f_{j}" display="inline"><msub><mi>f</mi><mi>j</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m5" class="ltx_Math" alttext="e_{a_{j}}" display="inline"><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub></math>, the two words are input to the lookup layer.
In the lookup layer, each of these words is converted to its word embedding, and then the concatenation of the two embeddings (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m6" class="ltx_Math" alttext="x_{j}" display="inline"><msub><mi>x</mi><mi>j</mi></msub></math>) is fed to the hidden layer in the same manner as the FFNN-based model.
Next, the hidden layer receives the output of the lookup layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m7" class="ltx_Math" alttext="x_{j}" display="inline"><msub><mi>x</mi><mi>j</mi></msub></math>) and that of the previous hidden layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m8" class="ltx_Math" alttext="y_{j-1}" display="inline"><msub><mi>y</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></math>).
The hidden layer then computes and outputs the nonlinear relations between them.
Note that the weight matrices used in this computation are embodied by the specific jump distance <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m9" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>.
The output of the hidden layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m10" class="ltx_Math" alttext="y_{j}" display="inline"><msub><mi>y</mi><mi>j</mi></msub></math>) is copied and fed to the output layer and the next hidden layer.
Finally, the output layer computes the score of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m11" class="ltx_Math" alttext="a_{j}" display="inline"><msub><mi>a</mi><mi>j</mi></msub></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m12" class="ltx_Math" alttext="t_{RNN}(a_{j}|a_{1}^{j-1},f_{j},e_{a_{j}})" display="inline"><mrow><msub><mi>t</mi><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub><mrow><mo>(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>|</mo><msubsup><mi>a</mi><mn>1</mn><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msub><mi>f</mi><mi>j</mi></msub><mo>,</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow></mrow></math>) from the output of the hidden layer (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m13" class="ltx_Math" alttext="y_{j}" display="inline"><msub><mi>y</mi><mi>j</mi></msub></math>).
Note that the FFNN-based model consists of two components: one is for lexical translation and the other is for alignment.
The proposed RNN produces a single score that is constructed in the hidden layer by employing the distance-dependent weight matrices.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">Specifically, the computations in the hidden and output layer are as follows:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S3.E9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E9.m3" class="ltx_Math" alttext="\displaystyle y_{j}=f(H^{d}\times x_{j}+R^{d}\times y_{j-1}+B^{d}_{H})," display="inline"><mrow><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mrow><mrow><msup><mi>H</mi><mi>d</mi></msup><mo>Ã</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>+</mo><mrow><msup><mi>R</mi><mi>d</mi></msup><mo>Ã</mo><msub><mi>y</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>+</mo><msubsup><mi>B</mi><mi>H</mi><mi>d</mi></msubsup></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
<tr id="S3.E10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E10.m3" class="ltx_Math" alttext="\displaystyle t_{RNN}=O\times y_{j}+B_{O}," display="inline"><mrow><mrow><msub><mi>t</mi><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><mi>N</mi></mrow></msub><mo>=</mo><mrow><mrow><mi>O</mi><mo>Ã</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><mo>+</mo><msub><mi>B</mi><mi>O</mi></msub></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m1" class="ltx_Math" alttext="H^{d}" display="inline"><msup><mi>H</mi><mi>d</mi></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m2" class="ltx_Math" alttext="R^{d}" display="inline"><msup><mi>R</mi><mi>d</mi></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m3" class="ltx_Math" alttext="B^{d}_{H}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mi>d</mi></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m4" class="ltx_Math" alttext="O" display="inline"><mi>O</mi></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m5" class="ltx_Math" alttext="B_{O}" display="inline"><msub><mi>B</mi><mi>O</mi></msub></math> are <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m6" class="ltx_Math" alttext="|y_{j}|\times|x_{j}|" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow><mo>Ã</mo><mrow><mo fence="true">|</mo><msub><mi>x</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m7" class="ltx_Math" alttext="|y_{j}|\times|y_{j-1}|" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow><mo>Ã</mo><mrow><mo fence="true">|</mo><msub><mi>y</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo fence="true">|</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m8" class="ltx_Math" alttext="|y_{j}|\times 1" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow><mo>Ã</mo><mn>1</mn></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m9" class="ltx_Math" alttext="1\times|y_{j}|" display="inline"><mrow><mn>1</mn><mo>Ã</mo><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m10" class="ltx_Math" alttext="1\times 1" display="inline"><mrow><mn>1</mn><mo>Ã</mo><mn>1</mn></mrow></math> matrices, respectively.
Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m11" class="ltx_Math" alttext="|y_{j-1}|=|y_{j}|" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>y</mi><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo fence="true">|</mo></mrow><mo>=</mo><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow></mrow></math>.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m12" class="ltx_Math" alttext="f(x)" display="inline"><mrow><mi>f</mi><mo>â¢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math> is an activation function, which is a hard hyperbolic tangent, i.e., htanh<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m13" class="ltx_Math" alttext="(x)" display="inline"><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></math>, in this study.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">As described above, the RNN-based model has a hidden layer with recurrent connections.
Under the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p5.m1" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math>.
Therefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Training</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">During training, we optimize the weight matrices of each layer (i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m2" class="ltx_Math" alttext="H^{d}" display="inline"><msup><mi>H</mi><mi>d</mi></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m3" class="ltx_Math" alttext="R^{d}" display="inline"><msup><mi>R</mi><mi>d</mi></msup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m4" class="ltx_Math" alttext="B^{d}_{H}" display="inline"><msubsup><mi>B</mi><mi>H</mi><mi>d</mi></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m5" class="ltx_Math" alttext="O" display="inline"><mi>O</mi></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m6" class="ltx_Math" alttext="B_{O}" display="inline"><msub><mi>B</mi><mi>O</mi></msub></math>) following a given objective using a mini-batch SGD with batch size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m7" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>, which converges faster than a plain SGD (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m8" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> = 1).
Gradients are computed by the back-propagation through time algorithm <cite class="ltx_cite">[<a href="#bib.bib25" title="Learning Internal Representations by Error Propagation" class="ltx_ref">31</a>]</cite>, which unfolds the network in time (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m9" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math>) and computes gradients over time steps.
In addition, an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m10" class="ltx_Math" alttext="l2" display="inline"><mrow><mi>l</mi><mo>â¢</mo><mn>2</mn></mrow></math> regularization term is added to the objective to prevent the model from overfitting the training data.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. <a href="#S2.E7" title="(7) â£ 2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> (Section <a href="#S2.SS2" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>).
However, this approach requires gold standard alignments.
To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Unsupervised Learning</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">Dyer et al. <cite class="ltx_cite">[<a href="#bib.bib6" title="Unsupervised Word Alignment with Arbitrary Features" class="ltx_ref">9</a>]</cite> presented an unsupervised alignment model based on contrastive estimation (CE) <cite class="ltx_cite">[<a href="#bib.bib40" title="Contrastive Estimation: Training Log-Linear Models on Unlabeled Data" class="ltx_ref">32</a>]</cite>.
CE seeks to discriminate observed data from its neighborhood, which can be viewed as pseudo-negative samples.
Dyer et al. <cite class="ltx_cite">[<a href="#bib.bib6" title="Unsupervised Word Alignment with Arbitrary Features" class="ltx_ref">9</a>]</cite> regarded all possible alignments of the bilingual sentences, which are given as training data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m1" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>), and those of the full translation search space (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m2" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Î©</mi></math>) as the observed data and its neighborhood, respectively.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">We introduce this idea to a ranking loss with margin as</p>
<table id="S4.E11" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E11.m1" class="ltx_Math" alttext="\hbox to 0.0pt{$\displaystyle\scalebox{0.9}{$\displaystyle loss(\theta)=\text{%&#10;max}\biggl\{0,1-\sum_{\bm{(f^{+},e^{+})}\in T}\text{E}_{\Phi}[s_{\theta}(\bm{a%&#10;}|\bm{f^{+}},\bm{e^{+}})]$}$}\\&#10;\scalebox{0.9}{$\displaystyle+\sum_{(\bm{f^{+}},\bm{e^{-}})\in\Omega}\text{E}_%&#10;{\Phi}[s_{\theta}(\bm{a}|\bm{f^{+}},\bm{e^{-}})]\biggr\},$}" display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo>(</mo><mi>Î¸</mi><mo>)</mo></mrow><mo>=</mo><mtext>max</mtext><mrow><mo mathsize="2.0em" stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>-</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></mrow><mo>)</mo></mrow><mo>â</mo><mi>T</mi></mrow></munder><msub><mtext>E</mtext><mi mathvariant="normal">Î¦</mi></msub><mrow><mo>[</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><mi>ð</mi><mo>|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>)</mo></mrow><mo>]</mo></mrow></mrow></mrow><mo>â¢</mo><mrow><mo>+</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></mrow><mo>)</mo></mrow><mo>â</mo><mi mathvariant="normal">Î©</mi></mrow></munder><msub><mtext>E</mtext><mi mathvariant="normal">Î¦</mi></msub><mrow><mo>[</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><mi>ð</mi><mo>|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup><mo>)</mo></mrow><mo>]</mo></mrow><mo mathsize="2.0em" stretchy="false">}</mo><mo>,</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m1" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Î¦</mi></math> is a set of all possible alignments given <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m2" class="ltx_Math" alttext="(\bm{f},\bm{e})" display="inline"><mrow><mo>(</mo><mrow><mi>ð</mi><mo>,</mo><mi>ð</mi></mrow><mo>)</mo></mrow></math>, E<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m3" class="ltx_Math" alttext="{}_{\Phi}[s_{\theta}]" display="inline"><mmultiscripts><mrow><mo>[</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mo>]</mo></mrow><mprescripts/><mi mathvariant="normal">Î¦</mi><none/></mmultiscripts></math> is the expected value of the scores <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m4" class="ltx_Math" alttext="s_{\theta}" display="inline"><msub><mi>s</mi><mi>Î¸</mi></msub></math> on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m5" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Î¦</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m6" class="ltx_Math" alttext="\bm{e^{+}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></math> denotes a target language sentence in the training data, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m7" class="ltx_Math" alttext="\bm{e^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math> denotes a pseudo-target language sentence.
The first expectation term is for the observed data, and the second is for the neighborhood.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">However, the computation for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m1" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Î©</mi></math> is prohibitively expensive.
To reduce computation, we employ NCE, which uses randomly sampled sentences from all target language sentences in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m2" class="ltx_Math" alttext="\Omega" display="inline"><mi mathvariant="normal">Î©</mi></math> as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m3" class="ltx_Math" alttext="\bm{e^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math>, and calculate the expected values by a beam search with beam width <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m4" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> to truncate alignments with low scores.
In our experiments, we set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m5" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> to 100.
In addition, the above criterion is converted to an online fashion as</p>
<table id="S4.E12" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E12.m1" class="ltx_Math" alttext="\hbox to 0.0pt{$\displaystyle\scalebox{0.95}{$\displaystyle loss(\theta)=\sum_%&#10;{\bm{f^{+}}\in T}\text{max}\biggr\{0,1-\text{E}_{\text{GEN}}[s_{\theta}(\bm{a}%&#10;|\bm{f^{+}},\bm{e^{+}})]$}$}\\&#10;\scalebox{0.95}{$\displaystyle+\frac{1}{N}\sum_{\bm{e^{-}}}\text{E}_{\text{GEN%&#10;}}[s_{\theta}(\bm{a}|\bm{f^{+}},\bm{e^{-}})]\biggl\},$}" display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mrow><mo>(</mo><mi>Î¸</mi><mo>)</mo></mrow><mo>=</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><mrow><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>â</mo><mi>T</mi></mrow></munder><mtext>max</mtext><mo mathsize="2.0em" stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>-</mo><mtext>E</mtext><msub><mi/><mtext>GEN</mtext></msub><mo>[</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><mi>ð</mi><mo>|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>)</mo></mrow><mo>]</mo></mrow><mo>â¢</mo><mrow><mo>+</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munder><mo largeop="true" movablelimits="false" symmetric="true">â</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></munder><msub><mtext>E</mtext><mtext>GEN</mtext></msub><mrow><mo>[</mo><msub><mi>s</mi><mi>Î¸</mi></msub><mrow><mo>(</mo><mi>ð</mi><mo>|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup><mo>)</mo></mrow><mo>]</mo></mrow><mrow><mo mathsize="2.0em" stretchy="false">}</mo><mo>,</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m6" class="ltx_Math" alttext="\bm{e^{+}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></math> is a target language sentence aligned to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m7" class="ltx_Math" alttext="\bm{f^{+}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></math> in the training data, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m8" class="ltx_Math" alttext="\bm{(f^{+},e^{+})}\in T" display="inline"><mrow><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></mrow><mo>)</mo></mrow><mo>â</mo><mi>T</mi></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m9" class="ltx_Math" alttext="\bm{e^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math> is a randomly sampled pseudo-target language sentence with length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m10" class="ltx_Math" alttext="|\bm{e^{+}}|" display="inline"><mrow><mo fence="true">|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo fence="true">|</mo></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m11" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> denotes the number of pseudo-target language sentences per source sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m12" class="ltx_Math" alttext="\bm{f^{+}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></math>.
Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m13" class="ltx_Math" alttext="|\bm{e^{+}}|=|\bm{e^{-}}|" display="inline"><mrow><mrow><mo fence="true">|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo fence="true">|</mo></mrow><mo>=</mo><mrow><mo fence="true">|</mo><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup><mo fence="true">|</mo></mrow></mrow></math>.
GEN is a subset of all possible word alignments <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m14" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Î¦</mi></math>, which is generated by beam search.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">In a simple implementation, each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m1" class="ltx_Math" alttext="\bm{e^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math> is generated by repeating a random sampling from a set of target words (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m2" class="ltx_Math" alttext="V_{e}" display="inline"><msub><mi>V</mi><mi>e</mi></msub></math>) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m3" class="ltx_Math" alttext="|\bm{e^{+}}|" display="inline"><mrow><mo fence="true">|</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup><mo fence="true">|</mo></mrow></math> times and lining them up sequentially.
To employ more discriminative negative samples, our implementation samples each word of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m4" class="ltx_Math" alttext="\bm{e^{-}}" display="inline"><msup><mi>ð</mi><mo mathvariant="bold">-</mo></msup></math> from a set of the target words that co-occur with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m5" class="ltx_Math" alttext="f_{i}\in\bm{f^{+}}" display="inline"><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>â</mo><msup><mi>ð</mi><mo mathvariant="bold">+</mo></msup></mrow></math> whose probability is above a threshold <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m6" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> under the IBM Model 1 incorporating <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m7" class="ltx_Math" alttext="l_{0}" display="inline"><msub><mi>l</mi><mn>0</mn></msub></math> prior <cite class="ltx_cite">[<a href="#bib.bib7" title="Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the l0-norm" class="ltx_ref">37</a>]</cite>.
The IBM Model 1 with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m8" class="ltx_Math" alttext="l_{0}" display="inline"><msub><mi>l</mi><mn>0</mn></msub></math> prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Agreement Constraints</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side.
Asymmetric models are usually trained in each alignment direction.
The model proposed by Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> is no exception.
However, it has been demonstrated that encouraging directional models to agree improves alignment performance <cite class="ltx_cite">[<a href="#bib.bib15" title="Symmetric Word Alignments for Statistical Machine Translation" class="ltx_ref">22</a>, <a href="#bib.bib16" title="Alignment by Agreement" class="ltx_ref">21</a>, <a href="#bib.bib17" title="Expectation Maximization and Posterior Constraints" class="ltx_ref">14</a>, <a href="#bib.bib18" title="Better Alignments = Better Translations?" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">Inspired by their work, we introduce an agreement constraint to our learning.
The constraint concretely enforces agreement in word embeddings of both directions.
The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings:</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.E13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E13.m1" class="ltx_Math" alttext="\displaystyle\argmin_{\theta_{FE}}\bigl\{loss(\theta_{FE})+\alpha\lVert\theta_%&#10;{L_{EF}}-\theta_{L_{FE}}\rVert\bigr\}," display="inline"><mrow><mrow><msub><merror class="ltx_ERROR undefined undefined"><mtext>\argmin</mtext></merror><msub><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow></msub></msub><mo>â¢</mo><mrow><mo>{</mo><mrow><mrow><mi>l</mi><mo>â¢</mo><mi>o</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mrow><mo>(</mo><msub><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>Î±</mi><mo>â¢</mo><mrow><mo fence="true">â¥</mo><mrow><msub><mi>Î¸</mi><msub><mi>L</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow></msub></msub><mo>-</mo><msub><mi>Î¸</mi><msub><mi>L</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow></msub></msub></mrow><mo fence="true">â¥</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(13)</span></td></tr>
<tr id="S4.E14" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E14.m1" class="ltx_Math" alttext="\displaystyle\argmin_{\theta_{EF}}\bigl\{loss(\theta_{EF})+\alpha\lVert\theta_%&#10;{L_{FE}}-\theta_{L_{EF}}\rVert\bigr\}," display="inline"><mrow><mrow><msub><merror class="ltx_ERROR undefined undefined"><mtext>\argmin</mtext></merror><msub><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow></msub></msub><mo>â¢</mo><mrow><mo>{</mo><mrow><mrow><mi>l</mi><mo>â¢</mo><mi>o</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mrow><mo>(</mo><msub><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow></msub><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>Î±</mi><mo>â¢</mo><mrow><mo fence="true">â¥</mo><mrow><msub><mi>Î¸</mi><msub><mi>L</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow></msub></msub><mo>-</mo><msub><mi>Î¸</mi><msub><mi>L</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow></msub></msub></mrow><mo fence="true">â¥</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(14)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="\theta_{FE}" display="inline"><msub><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow></msub></math> (or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m2" class="ltx_Math" alttext="\theta_{EF}" display="inline"><msub><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow></msub></math>) denotes the weights of layers in a source-to-target (or target-to-source) alignment model, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m3" class="ltx_Math" alttext="\theta_{L}" display="inline"><msub><mi>Î¸</mi><mi>L</mi></msub></math> denotes weights of a lookup layer, i.e., word embeddings, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m4" class="ltx_Math" alttext="\alpha" display="inline"><mi>Î±</mi></math> is a parameter that controls the strength of the agreement constraint.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m5" class="ltx_Math" alttext="\lVert\theta\rVert" display="inline"><mrow><mo fence="true">â¥</mo><mi>Î¸</mi><mo fence="true">â¥</mo></mrow></math> indicates the norm of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m6" class="ltx_Math" alttext="\theta" display="inline"><mi>Î¸</mi></math>. 2-norm is used in our experiments.
Equations <a href="#S4.E13" title="(13) â£ 4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> and <a href="#S4.E14" title="(14) â£ 4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> can be applied to both supervised and unsupervised approaches.
Equations <a href="#S2.E7" title="(7) â£ 2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.E12" title="(12) â£ 4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> are substituted into <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m7" class="ltx_Math" alttext="loss(\theta)" display="inline"><mrow><mi>l</mi><mo>â¢</mo><mi>o</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mrow><mo>(</mo><mi>Î¸</mi><mo>)</mo></mrow></mrow></math> in supervised and unsupervised learning, respectively.
The proposed constraint penalizes overfitting to a particular direction and enables two directional models to optimize across alignment directions globally.</p>
</div>
<div id="S4.SS2.tab1" class="ltx_table">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:265.7pt;height:257.083333333333px;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.2pt,-20.3pt) scale(0.82,0.82) ;-webkit-transform:translate(-29.2pt,-20.3pt) scale(0.82,0.82) ;-ms-transform:translate(-29.2pt,-20.3pt) scale(0.82,0.82) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_bold">Algorithm 1</span>Â  Training Algorithm</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_bold">Input:</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m1" class="ltx_Math" alttext="\theta_{FE}^{1}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mn>1</mn></msubsup></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m2" class="ltx_Math" alttext="\theta_{EF}^{1}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mn>1</mn></msubsup></math>, training data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m3" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m4" class="ltx_Math" alttext="MaxIter" display="inline"><mrow><mi>M</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>x</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>r</mi></mrow></math>,</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">batch size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m5" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m6" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m7" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m8" class="ltx_Math" alttext="IBM1" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>1</mn></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m9" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m10" class="ltx_Math" alttext="\alpha" display="inline"><mi>Î±</mi></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">1: <span class="ltx_text ltx_font_bold">for all</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m11" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> such that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m12" class="ltx_Math" alttext="1\leq t\leq MaxIter" display="inline"><mrow><mn>1</mn><mo>â¤</mo><mi>t</mi><mo>â¤</mo><mrow><mi>M</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>x</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>r</mi></mrow></mrow></math> <span class="ltx_text ltx_font_bold">do</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">2: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m13" class="ltx_Math" alttext="\{({\bf f^{+}},{\bf e^{+}})^{D}\}" display="inline"><mrow><mo>{</mo><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>}</mo></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m14" class="ltx_Math" alttext="\leftarrow" display="inline"><mo>â</mo></math>sample(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m15" class="ltx_Math" alttext="D,T" display="inline"><mrow><mi>D</mi><mo>,</mo><mi>T</mi></mrow></math>)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">3-1: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m16" class="ltx_Math" alttext="\{({\bf f^{+}},{\bf\{e^{-}\}}^{N})^{D}\}" display="inline"><mrow><mo>{</mo><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mrow><mo>{</mo><msup><mi>ð</mi><mo>-</mo></msup><mo>}</mo></mrow><mi>N</mi></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>}</mo></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m17" class="ltx_Math" alttext="\leftarrow" display="inline"><mo>â</mo></math>neg<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m18" class="ltx_Math" alttext="{}_{\text{e}}" display="inline"><msub><mi/><mtext>e</mtext></msub></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m19" class="ltx_Math" alttext="(\{({\bf f^{+}},{\bf e^{+}})^{D}\},N,C,IBM1)" display="inline"><mrow><mo>(</mo><mrow><mrow><mo>{</mo><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>}</mo></mrow><mo>,</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>1</mn></mrow></mrow><mo>)</mo></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">3-2: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m20" class="ltx_Math" alttext="\{({\bf e^{+}},{\bf\{f^{-}\}}^{N})^{D}\}" display="inline"><mrow><mo>{</mo><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mrow><mo>{</mo><msup><mi>ð</mi><mo>-</mo></msup><mo>}</mo></mrow><mi>N</mi></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>}</mo></mrow></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m21" class="ltx_Math" alttext="\leftarrow" display="inline"><mo>â</mo></math>neg<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m22" class="ltx_Math" alttext="{}_{\text{f}}" display="inline"><msub><mi/><mtext>f</mtext></msub></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m23" class="ltx_Math" alttext="(\{({\bf f^{+}},{\bf e^{+}})^{D}\},N,C,IBM1)" display="inline"><mrow><mo>(</mo><mrow><mrow><mo>{</mo><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>}</mo></mrow><mo>,</mo><mi>N</mi><mo>,</mo><mi>C</mi><mo>,</mo><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>1</mn></mrow></mrow><mo>)</mo></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">4-1: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m24" class="ltx_Math" alttext="\theta_{FE}^{t+1}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m25" class="ltx_Math" alttext="\leftarrow" display="inline"><mo>â</mo></math>update<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m26" class="ltx_Math" alttext="(({\bf f^{+}},{\bf e^{+}},{\bf\{e^{-}\}}^{N})^{D},\theta_{FE}^{t},\theta_{EF}^%&#10;{t},W,\alpha)" display="inline"><mrow><mo>(</mo><mrow><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mrow><mo>{</mo><msup><mi>ð</mi><mo>-</mo></msup><mo>}</mo></mrow><mi>N</mi></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>,</mo><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mi>W</mi><mo>,</mo><mi>Î±</mi></mrow><mo>)</mo></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">4-2: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m27" class="ltx_Math" alttext="\theta_{EF}^{t+1}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m28" class="ltx_Math" alttext="\leftarrow" display="inline"><mo>â</mo></math>update<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m29" class="ltx_Math" alttext="(({\bf e^{+}},{\bf f^{+}},{\bf\{f^{-}\}}^{N})^{D},\theta_{EF}^{t},\theta_{FE}^%&#10;{t},W,\alpha)" display="inline"><mrow><mo>(</mo><mrow><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mrow><mo>{</mo><msup><mi>ð</mi><mo>-</mo></msup><mo>}</mo></mrow><mi>N</mi></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup><mo>,</mo><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mi>t</mi></msubsup><mo>,</mo><mi>W</mi><mo>,</mo><mi>Î±</mi></mrow><mo>)</mo></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">5: <span class="ltx_text ltx_font_bold">end for</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b"><span class="ltx_text ltx_font_bold">Output:</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.m30" class="ltx_Math" alttext="\theta_{EF}^{MaxIter+1},\theta_{FE}^{MaxIter+1}" display="inline"><mrow><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mrow><mrow><mi>M</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>x</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>r</mi></mrow><mo>+</mo><mn>1</mn></mrow></msubsup><mo>,</mo><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mrow><mrow><mi>M</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>x</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>t</mi><mo>â¢</mo><mi>e</mi><mo>â¢</mo><mi>r</mi></mrow><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></td></tr>
</tbody>
</table>
</span></span>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">Our unsupervised learning procedure is summarized in Algorithm 1.
In Algorithm 1, line 2 randomly samples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> bilingual sentences <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m2" class="ltx_Math" alttext="({\bf f^{+}},{\bf e^{+}})^{D}" display="inline"><msup><mrow><mo>(</mo><mrow><msup><mi>ð</mi><mo>+</mo></msup><mo>,</mo><msup><mi>ð</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow><mi>D</mi></msup></math> from training data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m3" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>.
Lines 3-1 and 3-2 generate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m4" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> pseudo-negative samples for each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m5" class="ltx_Math" alttext="{\bf f^{+}}" display="inline"><msup><mi>ð</mi><mo>+</mo></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m6" class="ltx_Math" alttext="{\bf e^{+}}" display="inline"><msup><mi>ð</mi><mo>+</mo></msup></math> based on the translation candidates of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m7" class="ltx_Math" alttext="{\bf f^{+}}" display="inline"><msup><mi>ð</mi><mo>+</mo></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m8" class="ltx_Math" alttext="{\bf e^{+}}" display="inline"><msup><mi>ð</mi><mo>+</mo></msup></math> found by the IBM Model 1 with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m9" class="ltx_Math" alttext="l_{0}" display="inline"><msub><mi>l</mi><mn>0</mn></msub></math> prior, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m10" class="ltx_Math" alttext="IBM1" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>1</mn></mrow></math> (Section <a href="#S4.SS1" title="4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).
Lines 4-1 and 4-2 update the weights in each layer following a given objective (Sections <a href="#S4.SS1" title="4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a href="#S4.SS2" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).
Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m11" class="ltx_Math" alttext="\theta_{FE}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mi>t</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m12" class="ltx_Math" alttext="\theta_{EF}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mi>t</mi></msubsup></math> are concurrently updated in each iteration, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m13" class="ltx_Math" alttext="\theta_{EF}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mi>t</mi></msubsup></math> (or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m14" class="ltx_Math" alttext="\theta_{FE}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mi>t</mi></msubsup></math>) is employed to enforce agreement between word embeddings when updating <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m15" class="ltx_Math" alttext="\theta_{FE}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>F</mi><mo>â¢</mo><mi>E</mi></mrow><mi>t</mi></msubsup></math> (or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m16" class="ltx_Math" alttext="\theta_{EF}^{t}" display="inline"><msubsup><mi>Î¸</mi><mrow><mi>E</mi><mo>â¢</mo><mi>F</mi></mrow><mi>t</mi></msubsup></math>).</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiment</h2>

<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Data</h3>

<div id="S5.T1" class="ltx_table">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:202.5pt;height:177.777777777778px;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-webkit-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-ms-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_l ltx_border_r ltx_border_t" colspan="2"/>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Train</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Dev</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Test</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">9 K</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">960</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m2" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.1 M</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">37</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">447</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m3" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m4" class="ltx_Math" alttext="NIST03" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>03</mn></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" rowspan="2">240 K</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" rowspan="2">878</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">919</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m5" class="ltx_Math" alttext="NIST04" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>04</mn></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_r">1,597</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m6" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">40 K</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2,501</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">489</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m7" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math></th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">3.2 M</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">2,000</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">2,000</td></tr>
</tbody>
</table>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">TableÂ 1: </span>Size of experimental datasets</div>
</div>
<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">We evaluated the alignment performance of the proposed models with two tasks: Japanese-English word alignment with the Basic Travel Expression Corpus (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math>) <cite class="ltx_cite">[<a href="#bib.bib27" title="Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World" class="ltx_ref">35</a>]</cite> and French-English word alignment with the Hansard dataset (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m2" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>) from the 2003 NAACL shared task <cite class="ltx_cite">[<a href="#bib.bib26" title="An Evaluation Exercise for Word Alignment" class="ltx_ref">23</a>]</cite>.
In addition, we evaluated the end-to-end translation performance of three tasks: a Chinese-to-English translation task with the FBIS corpus (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m3" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math>), the IWSLT 2007 Japanese-to-English translation task (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m4" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math>) <cite class="ltx_cite">[<a href="#bib.bib28" title="Overview of the IWSLT 2007 Evaluation Campaign" class="ltx_ref">10</a>]</cite>, and the NTCIR-9 Japanese-to-English patent translation task (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m5" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math>) <cite class="ltx_cite">[<a href="#bib.bib34" title="Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop" class="ltx_ref">13</a>]</cite><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable.</span></span></span>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T1" title="TableÂ 1 â£ 5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the sizes of our experimental datasets.
Note that the development data was not used in the alignment tasks, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m2" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>, because the hyperparameters of the alignment models were set by preliminary small-scale experiments.
The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m3" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> data is the first 9,960 sentence pairs in the training data for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m4" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math>, which were annotated with word alignment <cite class="ltx_cite">[<a href="#bib.bib29" title="Constraining a Generative Word Alignment Model with Discriminative Output" class="ltx_ref">12</a>]</cite>.
We split these pairs into the first 9,000 for training data and the remaining 960 as test data.
All the data in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m5" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> is word-aligned, and the training data in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m6" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math> is unlabeled data.
In <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m7" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math>, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m8" class="ltx_Math" alttext="NIST03" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>03</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m9" class="ltx_Math" alttext="NIST04" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>04</mn></mrow></math>).</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparing Methods</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer.
The IBM Model 4 was trained by previously presented model sequence schemes <cite class="ltx_cite">[<a href="#bib.bib8" title="A Systematic Comparison of Various Statistical Alignment Models" class="ltx_ref">28</a>]</cite>: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m1" class="ltx_Math" alttext="1^{5}H^{5}3^{5}4^{5}" display="inline"><mrow><msup><mn>1</mn><mn>5</mn></msup><mo>â¢</mo><msup><mi>H</mi><mn>5</mn></msup><mo>â¢</mo><msup><mn>3</mn><mn>5</mn></msup><mo>â¢</mo><msup><mn>4</mn><mn>5</mn></msup></mrow></math>, i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m2" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math>).
For the FFNN-based model, we set the word embedding length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m3" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> to 30, the number of units of a hidden layer <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m4" class="ltx_Math" alttext="|z_{1}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>1</mn></msub><mo fence="true">|</mo></mrow></math> to 100, and the window size of contexts to 5.
Hence, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m5" class="ltx_Math" alttext="|z_{0}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>z</mi><mn>0</mn></msub><mo fence="true">|</mo></mrow></math> is 300 (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m6" class="ltx_Math" alttext="30\times 5\times 2" display="inline"><mrow><mn>30</mn><mo>Ã</mo><mn>5</mn><mo>Ã</mo><mn>2</mn></mrow></math>).
Following Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite>, the FFNN-based model was trained by the supervised approach described in Section <a href="#S2.SS2" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m7" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">For the RNN-based models, we set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> to 30 and the number of units of each recurrent hidden layer <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="|y_{j}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>y</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow></math> to 100.
Thus, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="|x_{j}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>x</mi><mi>j</mi></msub><mo fence="true">|</mo></mrow></math> is 60 (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m4" class="ltx_Math" alttext="30\times 2" display="inline"><mrow><mn>30</mn><mo>Ã</mo><mn>2</mn></mrow></math>).
The number of units of each layer of the FFNN-based and RNN-based models and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m5" class="ltx_Math" alttext="M" display="inline"><mi>M</mi></math> were set through preliminary experiments.
To demonstrate the effectiveness of the proposed learning methods, we evaluated four types of RNN-based models: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m6" class="ltx_Math" alttext="RNN_{s}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m7" class="ltx_Math" alttext="RNN_{s+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m8" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m9" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>, where â<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m10" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>/<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m11" class="ltx_Math" alttext="u" display="inline"><mi>u</mi></math>â denotes a supervised/unsupervised model and â<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m12" class="ltx_Math" alttext="+c" display="inline"><mrow><mo>+</mo><mi>c</mi></mrow></math>â indicates that the agreement constraint was used.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">In training all the models except <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m1" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math>, the weights of each layer were initialized first.
For the weights of a lookup layer <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>, we preliminarily trained word embeddings for the source and target language from each side of the training data.
We then set the word embeddings to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m3" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> to avoid falling into local minima.
Other weights were randomly initialized to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m4" class="ltx_Math" alttext="[-0.1,0.1]" display="inline"><mrow><mo>[</mo><mrow><mrow><mo>-</mo><mn>0.1</mn></mrow><mo>,</mo><mn>0.1</mn></mrow><mo>]</mo></mrow></math>.
For the pretraining, we used the RNNLM Toolkit <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.fit.vutbr.cz/~imikolov/rnnlm/</span></a></span></span></span> <cite class="ltx_cite">[<a href="#bib.bib11" title="Recurrent Neural Network based Language Model" class="ltx_ref">24</a>]</cite> with the default options.
We mapped all words that occurred less than five times to the special token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m5" class="ltx_Math" alttext="\langle unk\rangle" display="inline"><mrow><mo>â¨</mo><mrow><mi>u</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>k</mi></mrow><mo>â©</mo></mrow></math>.
Next, each weight was optimized using the mini-batch SGD, where batch size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m6" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> was 100, learning rate was 0.01, and an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m7" class="ltx_Math" alttext="l_{2}" display="inline"><msub><mi>l</mi><mn>2</mn></msub></math> regularization parameter was 0.1.
The training stopped after 50 epochs.
The other parameters were set as follows: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m8" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m9" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m10" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> in the unsupervised learning were 100, 50, and 0.001, respectively, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m11" class="ltx_Math" alttext="\alpha" display="inline"><mi>Î±</mi></math> for the agreement constraint was 0.1.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">In the translation tasks, we used the Moses phrase-based SMT systems <cite class="ltx_cite">[<a href="#bib.bib32" title="Moses: Open Source Toolkit for Statistical Machine Translation" class="ltx_ref">17</a>]</cite>.
All Japanese and Chinese sentences were segmented by ChaSen<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><a href="http://chasen-legacy.sourceforge.jp/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://chasen-legacy.sourceforge.jp/</span></a></span></span></span> and the Stanford Chinese segmenter<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><a href="http://nlp.stanford.edu/software/segmenter.shtml" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://nlp.stanford.edu/software/segmenter.shtml</span></a></span></span></span>, respectively.
In the training, long sentences with over 40 words were filtered out.
Using the SRILM Toolkits <cite class="ltx_cite">[<a href="#bib.bib30" title="SRILM - An Extensible Language Modeling Toolkit" class="ltx_ref">33</a>]</cite> with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m1" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m2" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math>, and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m3" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math>.
The SMT weighting parameters were tuned by MERT <cite class="ltx_cite">[<a href="#bib.bib31" title="Minimum Error Rate Training in Statistical Machine Translation" class="ltx_ref">29</a>]</cite> in the development data.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Word Alignment Results</h3>

<div id="S5.T2" class="ltx_table">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:193.5pt;height:273.333333333334px;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-webkit-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-ms-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Alignment</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m2" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m3" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4859</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9029</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m4" class="ltx_Math" alttext="FFNN_{s}(" display="inline"><mrow><mi>F</mi><mi>F</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m5" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4770</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9020</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m6" class="ltx_Math" alttext="RNN_{s}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m7" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r">0.5053<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m8" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">0.9068</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m9" class="ltx_Math" alttext="RNN_{s+c}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m10" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r">0.5174<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m11" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">0.9202<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m12" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m13" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5307<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m14" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9037</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m15" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math></th>
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_bold">0.5562<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m16" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo mathvariant="normal">+</mo></msup></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_bold">0.9275<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m17" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo mathvariant="normal">+</mo></msup></math></span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m18" class="ltx_Math" alttext="FFNN_{s}(" display="inline"><mrow><mi>F</mi><mi>F</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>R<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m19" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.8224</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">-</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m20" class="ltx_Math" alttext="RNN_{s}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>R<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m21" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r">0.8798<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m22" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">-</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m23" class="ltx_Math" alttext="RNN_{s+c}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>(</mo></mrow></math>R<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m24" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">0.8921<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m25" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo mathvariant="normal">+</mo></msup></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">-</td></tr>
</tbody>
</table>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">TableÂ 2: </span>Word alignment performance (F1-measure)</div>
</div>
<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the alignment performance by the F1-measure.
Hereafter, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m1" class="ltx_Math" alttext="MODEL(R)" display="inline"><mrow><mi>M</mi><mo>â¢</mo><mi>O</mi><mo>â¢</mo><mi>D</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mrow><mo>(</mo><mi>R</mi><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m2" class="ltx_Math" alttext="MODEL(I)" display="inline"><mrow><mi>M</mi><mo>â¢</mo><mi>O</mi><mo>â¢</mo><mi>D</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mrow><mo>(</mo><mi>I</mi><mo>)</mo></mrow></mrow></math> denote the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m3" class="ltx_Math" alttext="MODEL" display="inline"><mrow><mi>M</mi><mo>â¢</mo><mi>O</mi><mo>â¢</mo><mi>D</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>L</mi></mrow></math> trained from gold standard alignments and word alignments found by the IBM Model 4, respectively.
In <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m4" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>, all models were trained from randomly sampled 100 K data<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>Due to high computational cost, we did not use all the training data. Scaling up to larger datasets will be addressed in future work.</span></span></span>.
We evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the âgrow-diag-final-andâ heuristic <cite class="ltx_cite">[<a href="#bib.bib35" title="Statistical Phrase-Based Translation" class="ltx_ref">18</a>]</cite>.
The significance test on word alignment performance was performed by the sign test with a 5% significance level.
â+â in Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicates that the comparisons are significant over corresponding baselines, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m5" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m6" class="ltx_Math" alttext="FFNN_{s}(R/I)" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>/</mo><mi>I</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">In Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p2.m1" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>, which includes all our proposals, i.e., the RNN-based model, the unsupervised learning, and the agreement constraint, achieves the best performance for both <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p2.m2" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p2.m3" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>.
The differences from the baselines are statistically significant.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m1" class="ltx_Math" alttext="RNN_{s}(R/I)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>/</mo><mi>I</mi></mrow><mo>)</mo></mrow></mrow></math> outperforms <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m2" class="ltx_Math" alttext="FFNN_{s}(R/I)" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>/</mo><mi>I</mi></mrow><mo>)</mo></mrow></mrow></math>, which is statistically significant in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m3" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math>.
These results demonstrate that capturing the long alignment history in the RNN-based model improves the alignment performance.
We discuss the difference of the RNN-based modelâs effectiveness between language pairs in Section <a href="#S6.SS1" title="6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.
Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> also shows that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m4" class="ltx_Math" alttext="RNN_{s+c}(R/I)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>/</mo><mi>I</mi></mrow><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m5" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> achieve significantly better performance than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m6" class="ltx_Math" alttext="RNN_{s}(R/I)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mrow><mi>R</mi><mo>/</mo><mi>I</mi></mrow><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m7" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math> in both tasks, respectively.
This indicates that the proposed agreement constraint is effective in training better models in both the supervised and unsupervised approaches.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p class="ltx_p">In <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m2" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m3" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> significantly outperform <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m4" class="ltx_Math" alttext="RNN_{s}(I)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mi>I</mi><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m5" class="ltx_Math" alttext="RNN_{s+c}(I)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>I</mi><mo>)</mo></mrow></mrow></math>, respectively.
The performance of these models is comparable with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m6" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>.
This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data.
This is especially true when the quality of training data, i.e., the performance of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p4.m7" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math>, is low.</p>
</div>
</div>
<div id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.4 </span>Machine Translation Results</h3>

<div id="S5.T3" class="ltx_table">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:202.4pt;height:185.972222222222px;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.5pt,-18.9pt) scale(0.78,0.78) ;-webkit-transform:translate(-28.5pt,-18.9pt) scale(0.78,0.78) ;-ms-transform:translate(-28.5pt,-18.9pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2">Alignment</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m1" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m2" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m3" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m4" class="ltx_Math" alttext="NIST03" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>03</mn></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m5" class="ltx_Math" alttext="NIST04" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mn>04</mn></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m6" class="ltx_Math" alttext="IBM4_{all}" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><msub><mn>4</mn><mrow><mi>a</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>l</mi></mrow></msub></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="2">46.47</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">27.91</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25.90</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">28.34</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m7" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">27.25</td>
<td class="ltx_td ltx_align_left ltx_border_r">25.41</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.65</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m8" class="ltx_Math" alttext="FFNN_{s}(" display="inline"><mrow><mi>F</mi><mi>F</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m9" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">46.38</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">27.05</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25.45</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">27.61</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m10" class="ltx_Math" alttext="RNN_{s}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mi>s</mi></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m11" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">46.43</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.24</td>
<td class="ltx_td ltx_align_left ltx_border_r">25.47</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.56</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m12" class="ltx_Math" alttext="RNN_{s+c}(" display="inline"><mrow><mi>R</mi><mi>N</mi><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>(</mo></mrow></math>I<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m13" class="ltx_Math" alttext=")" display="inline"><mo>)</mo></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">46.51</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.12</td>
<td class="ltx_td ltx_align_left ltx_border_r">25.55</td>
<td class="ltx_td ltx_align_left ltx_border_r">27.73</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m14" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">47.05<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m15" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">27.79<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m16" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25.76<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m17" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">27.91<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m18" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m19" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">46.97<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m20" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">27.76<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m21" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">25.84<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m22" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">28.20<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m23" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
</tbody>
</table>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">TableÂ 3: </span>Translation performance (BLEU4(%))</div>
</div>
<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the translation performance by the case sensitive BLEU4 metric<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>We used mteval-v13a.pl as the evaluation tool (<a href="http://www.itl.nist.gov/iad/mig/tests/mt/2009/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.itl.nist.gov/iad/mig/tests/mt/2009/</span></a>).</span></span></span> <cite class="ltx_cite">[<a href="#bib.bib33" title="BLEU: a Method for Automatic Evaluation of Machine Translation" class="ltx_ref">30</a>]</cite>.
Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the average BLEU of three different MERT runs.
In <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m1" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m2" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math>, each alignment model was trained from the randomly sampled 100 K data, and then a translation model was trained from all the training data that was word-aligned by the alignment model.
In addition, for a detailed comparison, we evaluated the SMT system where the IBM Model 4 was trained from all the training data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m3" class="ltx_Math" alttext="IBM4_{all}" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><msub><mn>4</mn><mrow><mi>a</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>l</mi></mrow></msub></mrow></math>).
The significance test on translation performance was performed by the bootstrap method <cite class="ltx_cite">[<a href="#bib.bib36" title="Statistical Significance Tests for Machine Translation Evaluation" class="ltx_ref">19</a>]</cite> with a 5% significance level.
â*â in Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> indicates that the comparisons are significant over both baselines, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m4" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p1.m5" class="ltx_Math" alttext="FFNN_{s}(I)" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mi>I</mi><mo>)</mo></mrow></mrow></math>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> also shows that better word alignment does not always result in better translation, which has been discussed previously <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite>.
However, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m1" class="ltx_Math" alttext="RNN_{u}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m2" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> outperform <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m3" class="ltx_Math" alttext="FFNN_{s}(I)" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub><mo>â¢</mo><mrow><mo>(</mo><mi>I</mi><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m4" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math> in all tasks.
These results indicate that our proposals contribute to improving translation performance<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>We also confirmed the effectiveness of our models on the NIST05 and NTCIR-10 evaluation data.</span></span></span>.
In addition, Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that these proposed models are comparable to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m5" class="ltx_Math" alttext="IBM4_{all}" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><msub><mn>4</mn><mrow><mi>a</mi><mo>â¢</mo><mi>l</mi><mo>â¢</mo><mi>l</mi></mrow></msub></mrow></math> in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m6" class="ltx_Math" alttext="NTCIR" display="inline"><mrow><mi>N</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>C</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>R</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS4.p2.m7" class="ltx_Math" alttext="FBIS" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>I</mi><mo>â¢</mo><mi>S</mi></mrow></math> even though the proposed models are trained from only a small part of the training data.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.1 </span>Effectiveness of RNN-based Alignment Model</h3>

<div id="S6.F3" class="ltx_figure"><img src="P14-1138/image003.png" id="S6.F3.g1" class="ltx_graphics ltx_centering" width="583" height="828" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">FigureÂ 3: </span>Word alignment examples</div>
</div>
<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#S6.F3" title="FigureÂ 3 â£ 6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows word alignment examples from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m1" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m2" class="ltx_Math" alttext="RNN_{s}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>, where solid squares indicate the gold standard alignments.
Figure <a href="#S6.F3" title="FigureÂ 3 â£ 6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) shows that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m3" class="ltx_Math" alttext="RRN_{s}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>R</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> adequately identifies complicated alignments with long distances compared to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m4" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> (e.g., jaggy alignments of âhave you been learningâ in Fig <a href="#S6.F3" title="FigureÂ 3 â£ 6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a)) because <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m5" class="ltx_Math" alttext="RNN_{s}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> captures alignment paths based on long alignment history, which can be viewed as phrase-level alignments, while <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p1.m6" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> employs only the last alignment.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p">In French-English word alignment, the most valuable clues are located locally because English and French have similar word orders and their alignment has more one-to-one mappings than Japanese-English word alignment (Figure <a href="#S6.F3" title="FigureÂ 3 â£ 6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Figure <a href="#S6.F3" title="FigureÂ 3 â£ 6.1 Effectiveness of RNN-based Alignment Model â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) shows that both <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p2.m1" class="ltx_Math" alttext="RRN_{s}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>R</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS1.p2.m2" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math> work for such simpler alignments.
Therefore, the RNN-based model has less effect on French-English word alignment than Japanese-English word alignment, as indicated in Table <a href="#S5.T2" title="TableÂ 2 â£ 5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</div>
<div id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.2 </span>Impact of Training Data Size</h3>

<div id="S6.T4" class="ltx_table">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:217.5pt;height:105px;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-webkit-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;-ms-transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Alignment</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40 K</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9 K</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1 K</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T4.m1" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5467</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.4859</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.4128</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T4.m2" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r">0.6004</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.5562</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.4842</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T4.m3" class="ltx_Math" alttext="RNN_{s+c}(R)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>R</mi><mo>)</mo></mrow></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.8921</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.6063</td></tr>
</tbody>
</table>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">TableÂ 4: </span>Word alignment performance on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T4.m5" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> with various sized training data</div>
</div>
<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S6.T4" title="TableÂ 4 â£ 6.2 Impact of Training Data Size â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the alignment performance on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> with various training data sizes, i.e., training data for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m2" class="ltx_Math" alttext="IWSLT" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>W</mi><mo>â¢</mo><mi>S</mi><mo>â¢</mo><mi>L</mi><mo>â¢</mo><mi>T</mi></mrow></math> (40 K), training data for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m3" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> (9 K), and the randomly sampled 1 K data from the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m4" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> training data.
Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m5" class="ltx_Math" alttext="RNN_{s+c}(R)" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub><mo>â¢</mo><mrow><mo>(</mo><mi>R</mi><mo>)</mo></mrow></mrow></math> cannot be trained from the 40 K data because the 40 K data does not have gold standard word alignments.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S6.T4" title="TableÂ 4 â£ 6.2 Impact of Training Data Size â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates that the proposed RNN-based model outperforms <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p2.m1" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math> trained from the unlabeled 40 K data by employing either the 1 K labeled data or the 9 K unlabeled data, which is less than 25% of the training data for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p2.m2" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math>.
Consequently, the SMT system using <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p2.m3" class="ltx_Math" alttext="RNN_{u+c}" display="inline"><mrow><mi>R</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> trained from a small part of training data can achieve comparable performance to that using <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p2.m4" class="ltx_Math" alttext="IBM4" display="inline"><mrow><mi>I</mi><mo>â¢</mo><mi>B</mi><mo>â¢</mo><mi>M</mi><mo>â¢</mo><mn>4</mn></mrow></math> trained from all training data, which is shown in Table <a href="#S5.T3" title="TableÂ 3 â£ 5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</div>
<div id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.3 </span>Effectiveness of Unsupervised Learning/Agreement Constraints</h3>

<div id="S6.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Alignment</th>
<td class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m1" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m2" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m3" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>(I)</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.4770</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9020</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m4" class="ltx_Math" alttext="FFNN_{s+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>(I)</th>
<td class="ltx_td ltx_align_left ltx_border_r">0.4854<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m5" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">0.9085<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m6" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m7" class="ltx_Math" alttext="FFNN_{u}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.5105<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m8" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.9026</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m9" class="ltx_Math" alttext="FFNN_{u+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math></th>
<td class="ltx_td ltx_align_left ltx_border_r">0.5313<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m10" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_left ltx_border_r">0.9144<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m11" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m12" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>(R)</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.8224</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m13" class="ltx_Math" alttext="FFNN_{s+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>(R)</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">0.8367<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T5.m14" class="ltx_Math" alttext="{}^{+}" display="inline"><msup><mi/><mo>+</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">-</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">TableÂ 5: </span>Word alignment performance of various FFNN-based models (F1-measure)</div>
</div>
<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p">The proposed unsupervised learning and agreement constraints can be applied to any NN-based alignment model.
Table <a href="#S6.T5" title="TableÂ 5 â£ 6.3 Effectiveness of Unsupervised Learning/Agreement Constraints â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the alignment performance of the FFNN-based models trained by our supervised/unsupervised approaches (s/u) with and without our agreement constraints.
In Table <a href="#S6.T5" title="TableÂ 5 â£ 6.3 Effectiveness of Unsupervised Learning/Agreement Constraints â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, â+câ denotes that the agreement constraint was used, and â+â indicates that the comparison with its corresponding baseline, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p1.m1" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>(I/R), is significant in the sign test with a 5% significance level.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S6.T5" title="TableÂ 5 â£ 6.3 Effectiveness of Unsupervised Learning/Agreement Constraints â£ 6 Discussion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m1" class="ltx_Math" alttext="FFNN_{s+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>(R/I) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m2" class="ltx_Math" alttext="FFNN_{u+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> achieve significantly better performance than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m3" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>(R/I) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m4" class="ltx_Math" alttext="FFNN_{u}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math>, respectively, in both <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m5" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m6" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>.
In addition, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m7" class="ltx_Math" alttext="FFNN_{u}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>u</mi></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m8" class="ltx_Math" alttext="FFNN_{u+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math> significantly outperform <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m9" class="ltx_Math" alttext="FFNN_{s}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mi>s</mi></msub></mrow></math>(I) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m10" class="ltx_Math" alttext="FFNN_{s+c}" display="inline"><mrow><mi>F</mi><mo>â¢</mo><mi>F</mi><mo>â¢</mo><mi>N</mi><mo>â¢</mo><msub><mi>N</mi><mrow><mi>s</mi><mo>+</mo><mi>c</mi></mrow></msub></mrow></math>(I), respectively, in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m11" class="ltx_Math" alttext="BTEC" display="inline"><mrow><mi>B</mi><mo>â¢</mo><mi>T</mi><mo>â¢</mo><mi>E</mi><mo>â¢</mo><mi>C</mi></mrow></math>.
The performance of these models is comparable in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p2.m12" class="ltx_Math" alttext="Hansards" display="inline"><mrow><mi>H</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>n</mi><mo>â¢</mo><mi>s</mi><mo>â¢</mo><mi>a</mi><mo>â¢</mo><mi>r</mi><mo>â¢</mo><mi>d</mi><mo>â¢</mo><mi>s</mi></mrow></math>.
These results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model.</p>
</div>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">We have proposed a word alignment model based on an RNN, which captures long alignment history through recurrent architectures.
Furthermore, we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions.
Our experiments have shown that the proposed model outperforms the FFNN-based model <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite> for word alignment and machine translation, and that the agreement constraint improves alignment performance.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">In future, we plan to employ contexts composed of surrounding words (e.g., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m1" class="ltx_Math" alttext="c(f_{j})" display="inline"><mrow><mi>c</mi><mo>â¢</mo><mrow><mo>(</mo><msub><mi>f</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p2.m2" class="ltx_Math" alttext="c(e_{a_{j}})" display="inline"><mrow><mi>c</mi><mo>â¢</mo><mrow><mo>(</mo><msub><mi>e</mi><msub><mi>a</mi><mi>j</mi></msub></msub><mo>)</mo></mrow></mrow></math> in the FFNN-based model) in our model, even though our model implicitly encodes such contexts in the alignment history.
We also plan to enrich each hidden layer in our model with multiple layers following the success of Yang et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Word Alignment Modeling with Context Dependent Deep Neural Network" class="ltx_ref">40</a>]</cite>, in which multiple hidden layers improved the performance of the FFNN-based model.
In addition, we would like to prove the effectiveness of the proposed method for other datasets.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We thank the anonymous reviewers for their helpful suggestions and valuable comments on the first version of this paper.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Auli, M. Galley, C. Quirk and G. Zweig</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Joint Language and Translation Modeling with Recurrent Neural Networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1044â1054</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Bengio, R. Ducharme, P. Vincent and C. Janvin</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Neural Probabilistic Language Model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp.Â 1137â1155</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p3" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Blunsom and T. Cohn</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discriminative Word Alignment with Conditional Random Fields</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 65â72</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. F. Brown, S. A. D. Pietra, V. J. D. Pietra and R. L. Mercer</span><span class="ltx_text ltx_bib_year">(1993)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Mathematics of Statistical Machine Translation: Parameter Estimation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">19</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.Â 263â311</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Recurrent Neural Networks for Word Alignment Model</span></span>,
<a href="#S1.p1" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Natural Language Processing (Almost) from Scratch</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">12</span>, <span class="ltx_text ltx_bib_pages"> pp.Â 2493â2537</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Collobert and J. Weston</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 160â167</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. E. Dahl, D. Yu, L. Deng and A. Acero</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Audio, Speech, and Language Processing, IEEE Transactions on</span> <span class="ltx_text ltx_bib_volume">20</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.Â 30â42</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p2" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. P. Dempster, N. M. Laird and D. B. Rubin</span><span class="ltx_text ltx_bib_year">(1977)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Maximum Likelihood from Incomplete Data via the EM Algorithm</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of the Royal Statistical Society, Series B</span> <span class="ltx_text ltx_bib_volume">39</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp.Â 1â38</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p2" title="2.1 Generative Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Dyer, J. Clark, A. Lavie and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised Word Alignment with Arbitrary Features</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 409â419</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. S. Fordyce</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Overview of the IWSLT 2007 Evaluation Campaign</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1â12</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p1" title="5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Ganchev, J. V. GraÃ§a and B. Taskar</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Better Alignments = Better Translations?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 986â993</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Goh, T. Watanabe, H. Yamamoto and E. Sumita</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constraining a Generative Word Alignment Model with Discriminative Output</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEICE Transactions</span> <span class="ltx_text ltx_bib_volume">93-D</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp.Â 1976â1983</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p2" title="5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Goto, B. Lu, K. P. Chow, E. Sumita and B. K. Tsou</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 559â578</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p1" title="5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. V. GraÃ§a, K. Ganchev and B. Taskar</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Expectation Maximization and Posterior Constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 569â576</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Gutmann and A. HyvÃ¤rinen</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 297â304</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Recurrent Neural Networks for Word Alignment Model</span></span>,
<a href="#S1.p3" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Kalchbrenner and P. Blunsom</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Recurrent Continuous Translation Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1700â1709</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constrantin and E. Herbst</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Moses: Open Source Toolkit for Statistical Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 177â180</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p4" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Koehn, F. J. Och and D. Marcu</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical Phrase-Based Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 48â54</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS3.p1" title="5.3 Word Alignment Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.
</span></li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Koehn</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical Significance Tests for Machine Translation Evaluation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 388â395</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS4.p1" title="5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Le, A. Allauzen and F. Yvon</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Continuous Space Translation Models with Neural Networks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 39â48</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Liang, B. Taskar and D. Klein</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Alignment by Agreement</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 104â111</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Matusov, R. Zens and H. Ney</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Symmetric Word Alignments for Statistical Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 219â225</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Mihalcea and T. Pedersen</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An Evaluation Exercise for Word Alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1â10</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p1" title="5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Mikolov, M. KarafiÃ¡t, L. Burget, J. CernockÃ½ and S. Khudanpur</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Recurrent Neural Network based Language Model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1045â1048</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S5.SS2.p3" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Mikolov and G. Zweig</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Context Dependent Recurrent Neural Network Language Model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 234â239</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Mnih and Y. W. Teh</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Fast and Simple Algorithm for Training Neural Probabilistic Language Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1751â1758</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Recurrent Neural Networks for Word Alignment Model</span></span>,
<a href="#S1.p3" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. C. Moore</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Discriminative Framework for Bilingual Word Alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 81â88</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. J. Och and H. Ney</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Systematic Comparison of Various Statistical Alignment Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">29</span>, <span class="ltx_text ltx_bib_pages"> pp.Â 19â51</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S5.SS2.p1" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. J. Och</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minimum Error Rate Training in Statistical Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 160â167</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p4" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Papineni, S. Roukos, T. Ward and W. Zhu</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BLEU: a Method for Automatic Evaluation of Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 311â318</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS4.p1" title="5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. E. Rumelhart, G. E. Hinton and R. J. Williams</span><span class="ltx_text ltx_bib_year">(1986)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning Internal Representations by Error Propagation</span>.
</span>
<span class="ltx_bibblock">in <span class="ltx_text ltx_bib_editor">D. E. Rumelhart and J. L. McClelland (Eds.)</span>, <span class="ltx_text ltx_bib_inbook">Parallel Distributed Processing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 318â362</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p5" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S4.p1" title="4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. A. Smith and J. Eisner</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contrastive Estimation: Training Log-Linear Models on Unlabeled Data</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 354â362</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p1" title="4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Stolcke</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SRILM - An Extensible Language Modeling Toolkit</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 901â904</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p4" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Sundermeyer, I. Oparin, J. Gauvain, B. Freiberg, R. SchlÃ¼ter and H. Ney</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Comparison of Feedforward and Recurrent Neural Network Language Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 8430â8434</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto and S. Yamamoto</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 147â152</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p1" title="5.1 Experimental Data â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Taskar, S. Lacoste-Julien and D. Klein</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Discriminative Matching Approach to Word Alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 73â80</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Vaswani, L. Huang and D. Chiang</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the <math xmlns="http://www.w3.org/1998/Math/MathML" id="bib.bib7.m1a" class="ltx_Math" alttext="l_{0}" display="inline"><msub><mi>l</mi><mn>0</mn></msub></math>-norm</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 311â319</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p4" title="4.1 Unsupervised Learning â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Vaswani, Y. Zhao, V. Fossum and D. Chiang</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Decoding with Large-Scale Neural Language Models Improves Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 1387â1392</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Vogel, H. Ney and C. Tillmann</span><span class="ltx_text ltx_bib_year">(1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hmm-based Word Alignment in Statistical Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 836â841</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Yang, S. Liu, M. Li, M. Zhou and N. Yu</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word Alignment Modeling with Context Dependent Deep Neural Network</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.Â 166â175</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Recurrent Neural Networks for Word Alignment Model</span></span>,
<a href="#S1.p1" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p1" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p2" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p4" title="2.2 FFNN-based Alignment Model â£ 2 Related Work â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S4.SS2.p1" title="4.2 Agreement Constraints â£ 4 Training â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S5.SS2.p1" title="5.2 Comparing Methods â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>,
<a href="#S5.SS4.p2" title="5.4 Machine Translation Results â£ 5 Experiment â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>,
<a href="#S7.p1" title="7 Conclusion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S7.p2" title="7 Conclusion â£ Recurrent Neural Networks for Word Alignment Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 19:27:01 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
