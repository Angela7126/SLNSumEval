<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony</title>
<!--Generated on Wed Jun 11 17:46:47 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Florent Perek 
<br class="ltx_break"/>Princeton University 
<br class="ltx_break"/>Princeton, NJ, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">fperek@princeton.edu</span> 
<br class="ltx_break"/>
</span></span></div>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">This paper describes an application of distributional semantics to the study of syntactic productivity in diachrony, i.e., the property of grammatical constructions to attract new lexical items over time. By providing an empirical measure of semantic similarity between words derived from lexical co-occurrences, distributional semantics not only reliably captures how the verbs in the distribution of a construction are related, but also enables the use of visualization techniques and statistical modeling to analyze the semantic development of a construction over time and identify the semantic determinants of syntactic productivity in naturally occurring data.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Language change does not exclusively consist of drastic shifts in ‘core’ aspects of grammar, such as changes in word order. Variation in usage, which can occur in no more than a few decades, is much more common, and to many linguists constitutes linguistic change in the making. Among these aspects of language use that are subject to diachronic change, this paper is concerned with the productivity of syntactic constructions, i.e., the range of lexical items with which a construction can be used. A given construction might occur with very different distributions at different points in time, even when the function it conveys remains the same. This is what Israel <cite class="ltx_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Israel1996</a>]</cite> finds for the pattern “Verb <span class="ltx_text ltx_font_italic">one’s way</span> Path”, commonly called the <span class="ltx_text ltx_font_italic">way</span>-construction <cite class="ltx_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Goldberg1995</a>]</cite>, exemplified by (1) and (2) below.</p>
</div><span class="ltx_ERROR undefined">{itemize*}</span>
<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_section">(1)</span> 
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">They hacked their way through the jungle.

<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">(2)</span> 
<div id="S1.p2.p1" class="ltx_para">
<p class="ltx_p">She typed her way to a promotion.</p>
</div>
<div id="S1.p2.p2" class="ltx_para">
<p class="ltx_p">As reported by Israel, examples like (1), in which the main verb describes the physical means whereby motion towards a goal is enabled, are attested as early as the 16<sup class="ltx_sup">th</sup> century, but it was not until the 19<sup class="ltx_sup">th</sup> century that examples like (2) started to appear, in which the action depicted by the verb provides a more indirect (and abstract) way of attaining the agent’s goal.</p>
</div>
<div id="S1.p2.p3" class="ltx_para">
<p class="ltx_p">The productivity of a construction may appear partly arbitrary, but a growing body of evidence suggests that it is tied to the previous experience of speakers with that construction <cite class="ltx_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Bar<math xmlns="http://www.w3.org/1998/Math/MathML" class="ltx_Math" alttext="\eth" display="inline"><mi>ð</mi></math>dal2008</a>, <a href="#bib.bibx4" title="" class="ltx_ref">Bybee and Eddington2006</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Suttle and Goldberg2011</a>]</cite>. More specifically, previous research points to a strong semantic component, in that the possibility of a novel use depends on how it semantically relates to prior usage. Along these lines, Suttle and Goldberg <cite class="ltx_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Suttle and Goldberg2011</a>, 1254]</cite> posit a criterion of coverage, defined as “the degree to which attested instances ‘cover’ the category determined jointly by attested instances together with the target coinage”. Coverage relates to how the semantic domain of a construction is populated in the vicinity of a given target coinage, and in particular to the density of the semantic space.</p>
</div></li></p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">The importance of semantics for syntactic productivity implies that the meaning of lexical items must be appropriately taken into account when studying the distribution of constructions, which calls for an empirical operationalization of semantics. Most existing studies rely either on the semantic intuitions of the analyst, or on semantic norming studies <cite class="ltx_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Bybee and Eddington2006</a>]</cite>. In this paper, I present a third alternative that takes advantage of advances in computational linguistics and draws on a distributionally-based measure of semantic similarity. On the basis of a case study of the construction “V <span class="ltx_text ltx_font_italic">the hell out of</span> NP”, I show how distributional semantics can profitably be applied to the study of syntactic productivity.</p>
</div></li>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>The <span class="ltx_text ltx_font_italic">hell</span>-construction</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The case study presented in this paper considers the syntactic pattern “V <span class="ltx_text ltx_font_italic">the hell out of</span> NP”, as exemplified by the following sentences from the Corpus of Contemporary American English (COCA; Davies, 2008):</p>
</div><span class="ltx_ERROR undefined">{itemize*}</span>
<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_section">(3)</span> 
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Snakes just scare the hell out of me.

<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">(4)</span> 
<div id="S2.p2.p1" class="ltx_para">
<p class="ltx_p">It surprised the hell out of me when I heard what he’s been accused of.

<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">(5)</span> 
<div id="S2.p2.p1.p1" class="ltx_para">
<p class="ltx_p">You might kick the hell out of me like you did that doctor.</p>
</div>
<div id="S2.p2.p1.p2" class="ltx_para">
<p class="ltx_p">The construction generally conveys an intensifying function (very broadly defined). Thus, <span class="ltx_text ltx_font_italic">scare/surprise the hell out of</span> means “scare/surprise very much”, and <span class="ltx_text ltx_font_italic">kick the hell out of</span> means “kick very hard”. The particular aspect that is intensified may be highly specific to the verb and depend to some extent on the context. <span class="ltx_text ltx_font_italic">Scare</span> and <span class="ltx_text ltx_font_italic">beat</span> are the most typical verbs in that construction (and arguably the two that first come to mind), but a wide and diverse range of other verbs can also be found, such that <span class="ltx_text ltx_font_italic">avoid</span> in (6), <span class="ltx_text ltx_font_italic">drive</span> (a car) in (7) and even an intransitive verb (<span class="ltx_text ltx_font_italic">listen</span>) in (8):</p>
</div><span class="ltx_ERROR undefined">{itemize*}</span>
<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="S2.p2.p1.p3" class="ltx_para">
<p class="ltx_p">I […] avoided the hell out of his presence.

<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">(7)</span> 
<div id="S2.p2.p1.p3.p1" class="ltx_para">
<p class="ltx_p">But you drove the hell out of it!

<li class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">(8)</span> 
<div id="S2.p2.p1.p3.p1.p1" class="ltx_para">
<p class="ltx_p">I’ve been listening the hell out of your tape.</p>
</div>
<div id="S2.p2.p1.p3.p1.p2" class="ltx_para">
<p class="ltx_p">To examine how the construction evolved over time, I used diachronic data from the Corpus of Historical American English (COHA; Davies 2010), which contains about 20 million words of written American English for each decade between 1810 and 2009 roughly balanced for genre (fiction, magazines, newspapers, non-fiction). Instances of the <span class="ltx_text ltx_font_italic">hell</span>-construction were filtered out manually from the results of the query “[v*] the hell out of”, mostly ruling out locative constructions like <span class="ltx_text ltx_font_italic">get the hell out of here</span>. The diachronic evolution of the verb slot in terms of token and type frequency is plotted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 The hell-construction ‣ Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Since the corpus size varies slightly in each decade, the token frequencies are normalized per million words.</p>
</div>
<div id="S2.F1" class="ltx_figure"><img src="" id="S2.F1.g1" class="ltx_graphics ltx_centering" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diachronic development of the <span class="ltx_text ltx_font_italic">hell</span>-construction in terms of normalized token frequency and type frequency</div>
</div></li></p>
</div>
<div id="S2.p2.p1.p3.p2" class="ltx_para">
<p class="ltx_p">The construction is first attested in the corpus in the 1930s. Since then, it has been steadily increasing in token frequency (to the exception of a sudden decrease in the 1990s). Also, more and more different verbs are attested in the construction, as shown by the increase in type frequency. This reflects a general expansion of the productivity of the construction, but it does not show what this productivity consists of. For instance, it does not say what kinds of verbs joined the distribution and to what extent the distribution becomes semantically more diverse over time. To answer these questions, I will analyze the distribution of the construction from a semantic point of view by using a measure of semantic similarity derived from distributional information.</p>
</div></li></p>
</div></li></li></p>
</div></li></p>
</div></li>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Distributional measure of semantic similarity</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Drawing on the observation that words occurring in similar contexts tend to have related meanings <cite class="ltx_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Miller and Charles1991</a>]</cite>, distributional approaches to semantics seek to capture the meaning of words through their distribution in large text corpora <cite class="ltx_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Lenci2008</a>, <a href="#bib.bibx18" title="" class="ltx_ref">Turney and Pantel2010</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Erk2012</a>]</cite>. One benefit of the distributional semantics approach is that it allows semantic similarity between words to be quantified by measuring the similarity in their distribution. This is achieved by means of a vector-space model that assigns an array of numerical values (i.e., a vector) derived from distributional information to each word. A wide range of distributional information can be employed in vector-based models; the present study uses the ‘bag of words’ approach, which is based on the frequency of co-occurrence of words within a given context window. According to Sahlgren <cite class="ltx_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Sahlgren2008</a>]</cite>, this kind of model captures to what extent words can be substituted for each other, which is a good measure of semantic similarity between verbs. As it turns out, even this relatively coarse model captures semantic distinctions in the distribution of the <span class="ltx_text ltx_font_italic">hell</span>-construction that make intuitive sense.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">All instances of the relevant verbs were extracted from the COCA<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>The COCA contains 464 million words of American English consisting of the same amount of spoken, fiction, magazine, newspaper, and academic prose data for each year between 1990 and 2012. Admittedly, a more ecologically valid choice would have been to use data from a particular time frame to build a vector-space model for the same time frame, but even the twenty-odd million words per decade of the COHA did not prove sufficient to achieve that purpose. This is, however, not as problematic as it might sound, since the meaning of the verbs under consideration are not likely to have changed considerably within the time frame of this study. Besides, using the same data presents the advantage that the distribution is modeled with the same semantic space in all time periods, which makes it easier to visualize changes.</span></span></span> with their context of occurrence. In order to make sure that enough distributional information is available to reliably assess semantic similarity, verbs with less than 2,000 occurrences were excluded, which left 92 usable items (out of 105). The words in the sentence contexts extracted from the COCA were lemmatized and annotated for part-of-speech using TreeTagger <cite class="ltx_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Schmid1994</a>]</cite>. The part-of-speech annotated lemma of each collocate within a 5-word window was extracted from the COCA data to build the co-occurrence matrix recording the frequency of co-occurrence of each verb with its collocates. Only the nouns, verbs, adjectives, and adverbs listed among the 5,000 most frequent words in the corpus were considered (to the exclusion of <span class="ltx_text ltx_font_italic">be</span>, <span class="ltx_text ltx_font_italic">have</span>, and <span class="ltx_text ltx_font_italic">do</span>), thus ignoring function words (articles, prepositions, conjunctions, etc.) and all words that did not make the top 5,000.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The co-occurrence matrix was transformed by applying a Point-wise Mutual Information weighting scheme, using the DISSECT toolkit <cite class="ltx_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Dinu et al.2013</a>]</cite>, to turn the raw frequencies into weights that reflect how distinctive a collocate is for a given target word with respect to the other target words under consideration. The resulting matrix, which contains the distributional information (in 4,683 columns) for 92 verbs occurring in the <span class="ltx_text ltx_font_italic">hell</span>-construction, constitutes the semantic space under consideration in this case study. Pairwise distances between the target verbs were calculated using the cosine distance. The rest of the analysis was conducted on the basis of this distance matrix in the R environment <cite class="ltx_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">R Development Core Team2013</a>]</cite>.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Application of the vector-space model</h2>

<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Semantic plots</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">One of the advantages conferred by the quantification of semantic similarity is that lexical items can be precisely considered in relation to each other, and by aggregating the similarity information for all items in the distribution, we can produce a visual representation of the structure of the semantic domain of the construction in order to observe how verbs in that domain are related to each other, and to immediately identify the regions of the semantic space that are densely populated (with tight clusters of verbs), and those that are more sparsely populated (fewer and/or more scattered verbs). Multidimensional scaling (MDS) provides a way both to aggregate similarity information and to represent it visually. This technique aims to place objects in a space with two (or more) dimensions such that the between-object distances are preserved as much as possible.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">The pairwise distances between verbs were submitted to multidimensional scaling into two dimensions.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Non-metric MDS was employed <cite class="ltx_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Kruskal1964</a>]</cite>, using the function <span class="ltx_text ltx_font_typewriter">isoMDS</span> from the R package MASS.</span></span></span> To visualize the semantic development of the <span class="ltx_text ltx_font_italic">hell</span>-construction over time, the diachronic data was divided into four successive twenty-year periods: 1930-1949, 1950-1969, 1970-1989, and 1990-2009. The semantic plots corresponding to the distribution of the construction in each period are presented in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Semantic plots ‣ 4 Application of the vector-space model ‣ Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For convenience and ease of visualization, the verbs are color-coded according to four broad semantic groupings that were identified inductively by means of hierarchical clustering (using Ward’s criterion).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>Another benefit of combining clustering and MDS stems from the fact that the latter often distorts the data when fitting the objects into two dimensions, in that some objects might have to be slightly misplaced if not all distance relations can be simultaneously complied with. Since cluster analysis operates with all 4,683 dimensions of the distributional space, it is more reliable than MDS, although it lacks the visual appeal of the latter.</span></span></span></p>
</div>
<div id="S4.F2" class="ltx_figure"><img src="" id="S4.F2.g1" class="ltx_graphics ltx_centering" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Semantic plots of the <span class="ltx_text ltx_font_italic">hell</span>-construction in four time periods.</div>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">By comparing the plots in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Semantic plots ‣ 4 Application of the vector-space model ‣ Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can follow the semantic development of the <span class="ltx_text ltx_font_italic">hell</span>-construction. The construction is strikingly centered around two kinds of verbs: mental verbs (in red: <span class="ltx_text ltx_font_italic">surprise</span>, <span class="ltx_text ltx_font_italic">please</span>, <span class="ltx_text ltx_font_italic">scare</span>, etc.) and verbs of hitting (most verbs in green: <span class="ltx_text ltx_font_italic">smash</span>, <span class="ltx_text ltx_font_italic">kick</span>, <span class="ltx_text ltx_font_italic">whack</span>, etc.), a group that is orbited by other kinds of forceful actions (such as <span class="ltx_text ltx_font_italic">pinch</span>, <span class="ltx_text ltx_font_italic">push</span>, and <span class="ltx_text ltx_font_italic">tear</span>). These two types of verbs account for most of the distribution at the onset, and they continue to weigh heavily throughout the history of the construction. These two classes also correspond to the regions of the semantic domain that attract the most new members, and they constantly do so in all periods. Outside of these two clusters, the semantic space is much more sparsely populated. In the first period (1930-1949), only a few peripheral members are found. They are joined by other distantly related items in later periods, although by no more than a handful in each. In other words, the construction is markedly less productive in these outer domains, which never form proper clusters of verbs.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">In sum, the semantic plots show that densely populated regions of the semantic space appear to be the most likely to attract new members. Outside of the two identified domains of predilection, other classes never become important, assumedly because they do not receive a “critical mass” of items, and therefore attract new members more slowly.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Statistical analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">With the quantification of semantic similarity provided by the distributional semantic model, it is also possible to properly test the hypothesis that productivity is tied to the structure of the semantic space. On the reasonable assumption that the semantic contribution of the construction did not change, and therefore that all verbs ever attested in it are equally plausible from a semantic point of view, the fact that some verbs joined the distribution later than others is in want of an explanation. In view of the observations collected on the semantic plots and in line with previous research (especially Suttle and Goldberg’s notion of coverage), I suggest that the occurrence of a new item in the construction in a given period is related to the density of the semantic space around that item in the previous period. If the semantic space around the novel item is dense, i.e., if there is a high number of similar items, the coinage will be very likely. The sparser the semantic space around a given item, the less likely this item can be used.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">The measure of density used in this study considers the set of the N nearest neighbors of a given item in the semantic space, and is defined by the following formula:</p>
<table id="S4.Ex1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex1.m1" class="ltx_Math" alttext="Density_{V,N}=1-\frac{\sum_{n=1}^{N}d(V,V_{n})}{N}" display="block"><mrow><mrow><mi>D</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><msub><mi>y</mi><mrow><mi>V</mi><mo>,</mo><mi>N</mi></mrow></msub></mrow><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><mi>d</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>V</mi><mo>,</mo><msub><mi>V</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mi>N</mi></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="d(V,V_{n})" display="inline"><mrow><mi>d</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>V</mi><mo>,</mo><msub><mi>V</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></math> is the distance between a verb V and its n<sup class="ltx_sup">th</sup> nearest neighbor. In plain language, density equals one minus the mean distance to the N nearest neighbors. The latter value decreases with space density (i.e., if there are many close neighbors), and is therefore technically a measure of sparsity; since cosine distances are between 0 and 1, subtracting the mean distance from one returns a measure of density within the same boundaries.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">This measure of density was used as a factor in logistic regression to predict the first occurrence of a verb in the construction, coded as the binary variable <span class="ltx_text ltx_font_smallcaps">Occurrence</span>, set to 1 for the first period in which the verb is attested in the construction, and to 0 for all preceding periods (later periods were discarded). For each <span class="ltx_text ltx_font_smallcaps">Verb</span>-<span class="ltx_text ltx_font_smallcaps">Period</span>-<span class="ltx_text ltx_font_smallcaps">Occurrence</span> triplet, the density of the semantic space around the verb in the immediately preceding period was calculated. Six different versions of the density measure, with the number of neighbors under consideration (N) varying between 3 and 8, were used to fit six mixed effects regression models with <span class="ltx_text ltx_font_smallcaps">Occurrence</span> as the dependent variable, <span class="ltx_text ltx_font_smallcaps">Density</span> as a fixed effect, and random by-verb intercepts and slopes <cite class="ltx_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Bates et al.2011</a>]</cite>. The results of these models are summarized in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Statistical analysis ‣ 4 Application of the vector-space model ‣ Vector spaces for historical linguistics: Using distributional semantics to study syntactic productivity in diachrony" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">N</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Effect of <span class="ltx_text ltx_font_smallcaps">Density</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_italic">p</span>-value</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7211</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.195</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">4</th>
<td class="ltx_td ltx_align_center ltx_border_r">0.8836</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.135</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">5</th>
<td class="ltx_td ltx_align_center ltx_border_r">1.0487</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.091 (.)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">6</th>
<td class="ltx_td ltx_align_center ltx_border_r">1.2367</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.056 (.)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">7</th>
<td class="ltx_td ltx_align_center ltx_border_r">1.4219</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.034 (*)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">8</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">1.6625</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.017 (*)</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of logistic regression results for different values of N. Model formula: <span class="ltx_text ltx_font_smallcaps">Occurrence</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m3" class="ltx_Math" alttext="\sim" display="inline"><mo>∼</mo></math> <span class="ltx_text ltx_font_smallcaps">Density</span> + (1 + <span class="ltx_text ltx_font_smallcaps">Density<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m4" class="ltx_Math" alttext="|" display="inline"><mo mathvariant="normal">|</mo></math>Verb</span>). Marginally significant effects are marked with a period (.), significant effects with a star (*).</div>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p">For all values of N, we find a positive effect of <span class="ltx_text ltx_font_smallcaps">Density</span>, i.e., there is a positive relation between the measure of density and the probability of first occurrence of a verb in the construction. However, the effect is only significant for N <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p4.m1" class="ltx_Math" alttext="\geq" display="inline"><mo>≥</mo></math> 7; hence, the hypothesis that space density increases the odds of a coinage occurs in the construction is supported for measures of density based on these values of N.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p class="ltx_p">More generally, the <span class="ltx_text ltx_font_italic">p</span>-value decreases as N increases, which means that the positive relation between <span class="ltx_text ltx_font_smallcaps">Density</span> and <span class="ltx_text ltx_font_smallcaps">Occurrence</span> is less systematic when <span class="ltx_text ltx_font_smallcaps">Density</span> is measured with fewer neighbors. This is arguably because a higher N helps to better discriminate between dense clusters where all items are close together from looser ones that consist of a few ‘core’ items surrounded by more distant neighbors. This result illustrates the role of type frequency in syntactic productivity: a measure of density that is supported by a higher number of types makes better prediction than a measure supported by fewer types. This means that productivity not only hinges on how the existing semantic space relates to the novel item, it also occurs more reliably when this relation is attested by more items. These finding support the view that semantic density and type frequency, while they both positively influence syntactic productivity, do so in different ways: density defines the necessary conditions for a new coinage to occur, while type frequency increases the confidence that this coinage is indeed possible.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">This paper reports the first attempt at using a distributional measure of semantic similarity derived from a vector-space model for the study of syntactic productivity in diachrony. On the basis of a case study of the construction “V <span class="ltx_text ltx_font_italic">the hell out of</span> NP” from 1930 to 2009, the advantages of this approach were demonstrated. Not only does distributional semantics provide an empirically-based measure of semantic similarity that appropriately captures semantic distinctions, it also enables the use of methods for which quantification is necessary, such as data visualization and statistical analysis. Using multidimensional scaling and logistic regression, it was shown that the occurrence of new items throughout the history of the construction can be predicted by the density of the semantic space in the neighborhood of these items in prior usage. In conclusion, this work opens new perspectives for the study of syntactic productivity in line with the growing synergy between computational linguistics and other fields.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bar<math xmlns="http://www.w3.org/1998/Math/MathML" id="bib.bibx1.m1" class="ltx_Math" alttext="\eth" display="inline"><mi>ð</mi></math>dal2008</span>
<span class="ltx_bibblock">
Johana Bar<math xmlns="http://www.w3.org/1998/Math/MathML" id="bib.bibx1.m2" class="ltx_Math" alttext="\eth" display="inline"><mi>ð</mi></math>dal.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Productivity: Evidence from Case and Argument Structure in Icelandic</span>.

</span>
<span class="ltx_bibblock">John Benjamins, Amsterdam.

</span></li>
<li id="bib.bibx2" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bates et al.2011</span>
<span class="ltx_bibblock">
Douglas Bates, Martin Maechler, Ben Bolker and Steven Walker.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">lme4: Linear mixed-effects models using S4 classes. R package</span>.

</span>
<span class="ltx_bibblock">URL: http://CRAN.R-project.org/package=lme4

</span></li>
<li id="bib.bibx3" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bybee2010</span>
<span class="ltx_bibblock">
Joan Bybee.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Language, Usage and Cognition</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, Cambridge.

</span></li>
<li id="bib.bibx4" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bybee and Eddington2006</span>
<span class="ltx_bibblock">
Joan Bybee and David Eddington.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">A usage-based approach to Spanish verbs of ‘becoming’.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Language</span>, 82(2):323–355.

</span></li>
<li id="bib.bibx5" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Davies2008</span>
<span class="ltx_bibblock">
Mark Davies.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">The Corpus of Contemporary American English: 450 million words, 1990-present</span>.

</span>
<span class="ltx_bibblock">Available online at http://corpus.byu.edu/coca/

</span></li>
<li id="bib.bibx6" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Davies2010</span>
<span class="ltx_bibblock">
Mark Davies.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">The Corpus of Historical American English: 400 million words, 1810-2009</span>.

</span>
<span class="ltx_bibblock">Available online at http://corpus.byu.edu/coha/

</span></li>
<li id="bib.bibx7" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Dinu et al.2013</span>
<span class="ltx_bibblock">
Georgiana Dinu, The Nghia Pham and Marco Baroni.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">DISSECT: DIStributional SEmantics Composition Toolkit.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the System Demonstrations of ACL 2013 (51st Annual Meeting of the Association for Computational Linguistics)</span>.

</span></li>
<li id="bib.bibx8" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Erk2012</span>
<span class="ltx_bibblock">
Katrin Erk.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Vector Space Models of Word Meaning and Phrase Meaning: A Survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Language and Linguistics Compass</span>, 6(10):635–653.

</span></li>
<li id="bib.bibx9" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Goldberg1995</span>
<span class="ltx_bibblock">
Adele Goldberg.

</span>
<span class="ltx_bibblock">1995.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Constructions: A construction grammar approach to argument structure</span>.

</span>
<span class="ltx_bibblock">University of Chicago Press, Chicago.

</span></li>
<li id="bib.bibx10" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Israel1996</span>
<span class="ltx_bibblock">
Michael Israel.

</span>
<span class="ltx_bibblock">1996.

</span>
<span class="ltx_bibblock">The way constructions grow.

</span>
<span class="ltx_bibblock">In Adele E. Goldberg (ed.), <span class="ltx_text ltx_font_italic">Conceptual structure, discourse and language</span>, pages 217–230.

</span>
<span class="ltx_bibblock">CSLI Publications, Stanford, CA.

</span></li>
<li id="bib.bibx11" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Kruskal1964</span>
<span class="ltx_bibblock">
Joseph Kruskal.

</span>
<span class="ltx_bibblock">1964.

</span>
<span class="ltx_bibblock">Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Psychometrika</span>, 29(1):1–27.

</span></li>
<li id="bib.bibx12" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Lenci2008</span>
<span class="ltx_bibblock">
Alessandro Lenci.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Distributional semantics in linguistic and cognitive research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Rivista di Linguistica</span>, 20(1):1–31.

</span></li>
<li id="bib.bibx13" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Miller and Charles1991</span>
<span class="ltx_bibblock">
George Miller and Walter Charles.

</span>
<span class="ltx_bibblock">1991.

</span>
<span class="ltx_bibblock">Contextual correlates of semantic similarity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Language and Cognitive Processes</span>, 6(1):1–28.

</span></li>
<li id="bib.bibx14" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">R Development Core Team2013</span>
<span class="ltx_bibblock">
R Development Core Team.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">R: A language and environment for statistical computing</span>.

</span>
<span class="ltx_bibblock">R Foundation for Statistical Computing, Vienna; URL: http://www.R-project.org/

</span></li>
<li id="bib.bibx15" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Sahlgren2008</span>
<span class="ltx_bibblock">
Magnus Sahlgren.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">The distributional hypothesis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Rivista di Linguistica</span>, 20(1):33–53.

</span></li>
<li id="bib.bibx16" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Schmid1994</span>
<span class="ltx_bibblock">
Helmut Schmid.

</span>
<span class="ltx_bibblock">1994.

</span>
<span class="ltx_bibblock">Probabilistic Part-of-Speech Tagging Using Decision Trees.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of International Conference on New Methods in Language Processing, Manchester, UK</span>.

</span></li>
<li id="bib.bibx17" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Suttle and Goldberg2011</span>
<span class="ltx_bibblock">
Laura Suttle and Adele Goldberg.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">The partial productivity of constructions as induction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Linguistics</span>, 49(6):1237–1269.

</span></li>
<li id="bib.bibx18" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Turney and Pantel2010</span>
<span class="ltx_bibblock">
Peter Turney and Patrick Pantel.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">From Frequency to Meaning: Vector Space Models of Semantics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Journal of Artificial Intelligence Research</span>, 37:141–188.

</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:46:47 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
