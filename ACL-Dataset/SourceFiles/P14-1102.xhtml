<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bootstrapping into Filler-Gap: An Acquisition Story</title>
<!--Generated on Tue Jun 10 18:33:56 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Bootstrapping into Filler-Gap: An Acquisition Story</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Marten van Schijndel  Micha Elsner
<br class="ltx_break"/>The Ohio State University
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">vanschm,melsner</span>}<span class="ltx_text ltx_font_typewriter">@ling.ohio-state.edu</span>
</span></span></div>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives.
Therefore, this work models filler-gap acquisition as a byproduct of learning word orderings (e.g. SVO vs OSV), which must be done at a very young age anyway in order to extract meaning from language.
Specifically, this model, trained on part-of-speech tags, represents the preferred locations of semantic roles relative to a verb as Gaussian mixtures over real numbers.</p>
<p class="ltx_p">This approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance.
Additionally, this model is shown to be able to account for a characteristic error made by learners during this period (<em class="ltx_emph">A and B gorped</em> interpreted as <em class="ltx_emph">A gorped B</em>).</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The phenomenon of filler-gap, where the argument of a predicate appears outside its canonical position in the phrase structure (e.g. <em class="ltx_emph">[the apple]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m1" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> that the boy ate t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m2" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math></em> or <em class="ltx_emph">[what]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m3" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> did the boy eat t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m4" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math></em>), has long been an object of study for syntacticians <cite class="ltx_cite">[]</cite> due to its apparent processing complexity.
Such complexity is due, in part, to the arbitrary length of the dependency between a filler and its gap (e.g. <em class="ltx_emph">[the apple]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m5" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> that Mary said the boy ate t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m6" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math></em>).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Recent studies indicate that comprehension of filler-gap constructions begins around 15 months <cite class="ltx_cite">[]</cite>.
This finding raises the question of how such a complex phenomenon could be acquired so early since children at that age do not yet have a very advanced grasp of language (e.g. ditransitives do not seem to be generalized until at least 31 months; Goldberg et al. 2004<cite class="ltx_cite"/>, Bello 2012<cite class="ltx_cite"/>).
This work shows that filler-gap comprehension in English may be acquired through learning word orderings rather than relying on hierarchical syntactic knowledge.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">This work describes a cognitive model of the developmental timecourse of filler-gap comprehension with the goal of setting a lower bound on the modeling assumptions necessary for an ideal learner to display filler-gap comprehension.
In particular, the model described in this paper takes chunked child-directed speech as input and learns orderings over semantic roles.
These orderings then permit the model to successfully resolve filler-gap dependencies.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>This model does not explicitly learn gap positions, but rather assigns thematic roles to arguments based on where those arguments are expected to manifest.
This approach to filler-gap comprehension is supported by findings that show people do not actually link fillers to gap positions but instead link the filler to a verb with missing arguments <cite class="ltx_cite">[]</cite></span></span></span>
Further, the model presented here is also shown to initially reflect an idiosyncratic role assignment error observed in development (e.g. <em class="ltx_emph">A and B kradded</em> interpreted as <em class="ltx_emph">A kradded B</em>; Gertner and Fisher, 2012<cite class="ltx_cite"/>), though after training, the model is able to avoid the error.
As such, this work may be said to model a learner from 15 months to between 25 and 30 months.</p>
</div>
<div id="S1.F1" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" height="138" version="1.1" viewBox="-2 -19 300 138" width="300"><g transform="matrix(1 0 0 -1 0 100)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g stroke-width="0.8pt"><g><g><g transform="matrix(1 0 0 1 106 106)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 110 71 C 110 73 108 75 106 75 C 104 75 102 73 102 71 C 102 69 104 67 106 67 C 108 67 110 69 110 71 Z M 106 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 106 71)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 110 35 C 110 38 108 39 106 39 C 104 39 102 38 102 35 C 102 33 104 31 106 31 C 108 31 110 33 110 35 Z M 106 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 106 35)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 159 106)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 163 71 C 163 73 162 75 159 75 C 157 75 156 73 156 71 C 156 69 157 67 159 67 C 162 67 163 69 163 71 Z M 159 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 159 71)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 163 35 C 163 38 162 39 159 39 C 157 39 156 38 156 35 C 156 33 157 31 159 31 C 162 31 163 33 163 35 Z M 159 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 159 35)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 53 106)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 57 71 C 57 73 55 75 53 75 C 51 75 49 73 49 71 C 49 69 51 67 53 67 C 55 67 57 69 57 71 Z M 53 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 53 71)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 57 35 C 57 38 55 39 53 39 C 51 39 49 38 49 35 C 49 33 51 31 53 31 C 55 31 57 33 57 35 Z M 53 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 53 35)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 57 0 C 57 2 55 4 53 4 C 51 4 49 2 49 0 C 49 -2 51 -4 53 -4 C 55 -4 57 -2 57 0 Z M 53 0" style="fill:none"/><g><g transform="matrix(1 0 0 1 53 0)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 213 106)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 217 71 C 217 73 215 75 213 75 C 210 75 209 73 209 71 C 209 69 210 67 213 67 C 215 67 217 69 217 71 Z M 213 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 213 71)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 217 35 C 217 38 215 39 213 39 C 210 39 209 38 209 35 C 209 33 210 31 213 31 C 215 31 217 33 217 35 Z M 213 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 213 35)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 217 0 C 217 2 215 4 213 4 C 210 4 209 2 209 0 C 209 -2 210 -4 213 -4 C 215 -4 217 -2 217 0 Z M 213 0" style="fill:none"/><g><g transform="matrix(1 0 0 1 213 0)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 266 106)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 270 71 C 270 73 268 75 266 75 C 264 75 262 73 262 71 C 262 69 264 67 266 67 C 268 67 270 69 270 71 Z M 266 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 266 71)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 270 35 C 270 38 268 39 266 39 C 264 39 262 38 262 35 C 262 33 264 31 266 31 C 268 31 270 33 270 35 Z M 266 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 266 35)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 270 0 C 270 2 268 4 266 4 C 264 4 262 2 262 0 C 262 -2 264 -4 266 -4 C 268 -4 270 -2 270 0 Z M 266 0" style="fill:none"/><g><g transform="matrix(1 0 0 1 266 0)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 0)"><switch><foreignObject color="#000000" height="0" overflow="visible" width="0">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><path d="M 58 71 L 102 71" style="fill:none"/><path d="M 58 35 L 102 35" style="fill:none"/><path d="M 58 0 L 208 0" style="fill:none"/><g><path d="M 60 92 L 51 92 C 43 92 36 85 36 77 L 36 -3 C 36 -12 43 -19 51 -19 L 60 -19 C 69 -19 76 -12 76 -3 L 76 77 C 76 85 69 92 60 92 Z M 36 -19" style="fill:none"/><g><g transform="matrix(1 0 0 1 51 37)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 40)"><switch><foreignObject color="#000000" height="80" overflow="visible" width="9">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"/></foreignObject></switch></g></g></g></g><g><path d="M 111 71 L 151 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 151 71)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 111 35 L 151 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 151 35)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 164 71 L 205 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 205 71)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 164 35 L 205 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 205 35)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 217 71 L 258 71" style="fill:none"/><g><g transform="matrix(1 0 0 1 258 71)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 217 35 L 258 35" style="fill:none"/><g><g transform="matrix(1 0 0 1 258 35)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><path d="M 217 0 L 258 0" style="fill:none"/><g><g transform="matrix(1 0 0 1 258 0)"><g><path d="M 4 0 L -2 3 L 0 0 L -2 -3" style="stroke:none"/></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 23 103)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Age</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 20 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="66">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Wh-S</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 20 42)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="66">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Wh-O</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 23 6)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  1-1</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 72 99)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 19)"><switch><foreignObject color="#0000FF" height="25" overflow="visible" width="77">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">  <span class="ltx_text ltx_font_small">  13mo</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#FF0000"><g fill="#FF0000"><g><g stroke="#FF0000"><g fill="#FF0000"/></g></g><g><g transform="matrix(1 0 0 1 76 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#FF0000" height="12" overflow="visible" width="48">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  No</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#FF0000"><g fill="#FF0000"><g><g stroke="#FF0000"><g fill="#FF0000"/></g></g><g><g transform="matrix(1 0 0 1 76 42)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#FF0000" height="12" overflow="visible" width="48">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  No</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 125 99)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 19)"><switch><foreignObject color="#0000FF" height="25" overflow="visible" width="77">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">  <span class="ltx_text ltx_font_small">  15mo</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 130 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 130 42)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="76">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  (Yes)</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 179 103)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="66">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  20mo</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 183 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 183 42)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 183 6)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 232 103)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="66">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  25mo</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 236 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#0000FF"><g fill="#0000FF"><g><g stroke="#0000FF"><g fill="#0000FF"/></g></g><g><g transform="matrix(1 0 0 1 236 42)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#0000FF" height="12" overflow="visible" width="57">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  Yes</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#FF0000"><g fill="#FF0000"><g><g stroke="#FF0000"><g fill="#FF0000"/></g></g><g><g transform="matrix(1 0 0 1 236 6)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#FF0000" height="12" overflow="visible" width="48">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"> <span class="ltx_text ltx_font_small">  No</span></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g></svg>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The developmental timeline of subject (Wh-S) and object (Wh-O) <em class="ltx_emph">wh</em>-clause extraction comprehension suggested by experimental results <cite class="ltx_cite">[]</cite>. Parentheses indicate weak comprehension.
The final row shows the timeline of 1-1 role bias errors <cite class="ltx_cite">[]</cite>.
Missing nodes denote a lack of studies.</div>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The developmental timeline during which children acquire the ability to process filler-gap constructions is not well-understood.
Language comprehension precedes production, and the developmental literature on the acquisition of filler-gap constructions is sparsely populated due to difficulties in designing experiments to test filler-gap comprehension in preverbal infants.
Older studies typically looked at verbal children and the mistakes they make to gain insight into the acquisition process <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Recent studies, however, indicate that filler-gap comprehension likely begins earlier than production <cite class="ltx_cite">[]</cite>.
Therefore, studies of verbal children are probably actually testing the acquisition of production mechanisms (planning, motor skills, greater facility with lexical access, etc) rather than the acquisition of filler-gap.
Note that these may be related since filler-gap could introduce greater processing load which could overwhelm the child’s fragile production capacity <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> showed that children are able to process <em class="ltx_emph">wh</em>-extractions from subject position (e.g. <em class="ltx_emph">[who]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m1" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m2" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> ate pie</em>) as young as 15 months while similar extractions from object position (e.g. <em class="ltx_emph">[what]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m3" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> did the boy eat t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m4" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math></em>) remain unparseable until around 20 months of age.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Since the <em class="ltx_emph">wh</em>-phrase is in the same (or a very similar) position as the original subject when the <em class="ltx_emph">wh</em>-phrase takes subject position, it is not clear that these constructions are true extractions <cite class="ltx_cite">[]</cite>, however, this paper will continue to refer to them as such for ease of exposition.</span></span></span>
This line of investigation has been reopened and expanded by <cite class="ltx_cite"/> whose results suggest that the experimental methodology employed by <cite class="ltx_cite"/> was flawed in that it presumed infants have ideal performance mechanisms.
By providing more trials of each condition and controlling for the pragmatic felicity of test statements, <cite class="ltx_cite"/> provide evidence that 15-month old infants can process <em class="ltx_emph">wh</em>-extractions from both subject and object positions.
Object extractions are more difficult to comprehend than subject extractions, however, perhaps due to additional processing load in object extractions <cite class="ltx_cite">[]</cite>.
Similarly, <cite class="ltx_cite"/> show that relativized extractions with a <em class="ltx_emph">wh</em>-relativizer (e.g. <em class="ltx_emph">find [the boy]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m5" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> who t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m6" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> ate the apple</em>) are easier to comprehend than relativized extractions with <em class="ltx_emph">that</em> as the relativizer (e.g. <em class="ltx_emph">find [the boy]<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m7" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> that t<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p3.m8" class="ltx_Math" alttext="{}_{i}" display="inline"><msub><mi/><mi>i</mi></msub></math> ate the apple</em>).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> demonstrate that 19-month olds use their knowledge of nouns to learn both verbs and their associated argument structure.
In their study, infants were shown video of a person talking on a phone using a nonce verb with either one or two nouns (e.g. <em class="ltx_emph">Mary kradded Susan</em>).
Under the assumption that infants look longer at things that correspond to their understanding of a prompt, the infants were then shown two images that potentially depicted the described action – one picture where two actors acted independently (reflecting an intransitive proposition) and one picture where one actor acted on the other (reflecting a transitive proposition).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>There were two actors in each image to avoid biasing the infants to look at the image with more actors.</span></span></span>
Even though the infants had no extralinguistic knowledge about the verb, they consistently treated the verb as transitive if two nouns were present and intransitive if only one noun was present.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">Similarly, <cite class="ltx_cite"/> show that intransitive phrases with conjoined subjects (e.g. <em class="ltx_emph">John and Mary gorped</em>) are given a transitive interpretation (i.e. <em class="ltx_emph">John gorped Mary</em>) at 21 months (henceforth termed ‘1-1 role bias’), though this effect is no longer present at 25 months <cite class="ltx_cite">[]</cite>.
This finding suggests both that learners will ignore canonical structure in favor of using all possible arguments and that children have a bias to assign a unique semantic role to each argument.
It is important to note, however, that cross-linguistically children do not seem to generalize beyond two arguments until after at least 31 months of age <cite class="ltx_cite">[]</cite>, so a predicate occurring with three nouns would still likely be interpreted as merely transitive rather than ditransitive.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">Computational modeling provides a way to test the computational level of processing <cite class="ltx_cite">[]</cite>.
That is, given the input (child-directed speech, adult-directed speech, and environmental experiences), it is possible to probe the computational processes that result in the observed output.
However, previous computational models of grammar induction <cite class="ltx_cite">[]</cite>, including infant grammar induction <cite class="ltx_cite">[]</cite>, have not addressed filler-gap comprehension.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>As one reviewer notes, <cite class="ltx_cite"/> and subsequent work show that filler-gap phenomena can be formally captured by mildly context-sensitive grammar formalisms; these have the virtue of scaling up to adult grammar, but due to their complexity, do not seem to have been described as models of early acquisition.</span></span></span></p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">The closest work to that presented here is the work on BabySRL <cite class="ltx_cite">[]</cite>.
BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work.
BabySRL learns weights over ordering constraints (e.g. preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias.
However, no analysis has evaluated the ability of BabySRL to acquire filler-gap constructions.
Further comparison to BabySRL may be found in Section <a href="#S6" title="6 Comparison to BabySRL ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S2.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Susan</span></th>
<th class="ltx_td ltx_align_center">said</th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">John</span></th>
<th class="ltx_td ltx_align_center">gave</th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">girl</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">book</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center">-3</th>
<th class="ltx_td ltx_align_center">-2</th>
<th class="ltx_td ltx_align_center">-1</th>
<td class="ltx_td ltx_align_center">0</td>
<td class="ltx_td ltx_align_center">1</td>
<td class="ltx_td ltx_align_center">2</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>An example of a chunked sentence (<em class="ltx_emph">Susan said John gave the girl a red book</em>) with the sentence positions labelled.
Nominal heads of noun chunks are in bold.</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Assumptions</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">The present work restricts itself to acquiring filler-gap comprehension in English.
The model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles <cite class="ltx_cite">[]</cite>.
This work assumes learners can already identify nouns and verbs, which is supported by <cite class="ltx_cite"/> who show that children at an extremely young age can distinguish between content and function words and by <cite class="ltx_cite"/> who show that children can distinguish between different types of content words.
Further, since <cite class="ltx_cite"/> demonstrate that, by 14 months, children are able to distinguish nouns from modifiers, this work assumes learners can already chunk nouns and access the nominal head.
To handle recursion, this work assumes that children treat the final verb in each sentence as the main verb (implicitly assuming sentence segmentation), which ideally assigns roles to each of the nouns in the sentence.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Due to the findings of <cite class="ltx_cite"/>, this work adopts a ‘syntactic bootstrapping’ theory of acquisition <cite class="ltx_cite">[]</cite>, where structural properties (e.g. number of nouns) inform the learner about semantic properties of a predicate (e.g. how many semantic roles it confers).
Since infants infer the number of semantic roles, this work further assumes they already have expectations about where these roles tend to be realized in sentences, if they appear.
These positions may correspond to different semantic roles for different predicates (e.g. the subject of <em class="ltx_emph">run</em> and of <em class="ltx_emph">melt</em>); however, the role for predicates with a single argument is usually assigned to the noun that precedes the verb while a second argument is usually assigned after the verb.
The semantic properties of these roles may be learned lexically for each predicate, but that is beyond the scope of this work.
Therefore, this work uses syntactic and semantic roles interchangeably (e.g. <em class="ltx_emph">subject</em> and <em class="ltx_emph">agent</em>).</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">Finally, following the finding by <cite class="ltx_cite"/> that children interpret intransitives with conjoined subjects as transitives, this work assumes that semantic roles have a one-to-one correspondence with nouns in a sentence (similarly used as a soft constraint in the semantic role labelling work of Titov and Klementiev, 2012)<cite class="ltx_cite"/>.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Model</h2>

<div id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m1" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m2" class="ltx_Math" alttext="\sigma" display="inline"><mi>σ</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m3" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m4" class="ltx_Math" alttext="G_{SC}" display="inline"><msub><mi>G</mi><mrow><mi>S</mi><mo>⁢</mo><mi>C</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.999</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m5" class="ltx_Math" alttext="G_{SN}" display="inline"><msub><mi>G</mi><mrow><mi>S</mi><mo>⁢</mo><mi>N</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.001</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m6" class="ltx_Math" alttext="G_{OC}" display="inline"><msub><mi>G</mi><mrow><mi>O</mi><mo>⁢</mo><mi>C</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.999</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m7" class="ltx_Math" alttext="G_{ON}" display="inline"><msub><mi>G</mi><mrow><mi>O</mi><mo>⁢</mo><mi>N</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.001</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m8" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Φ</mi></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" colspan="3">.00001</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Initial values for the mean (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m13" class="ltx_Math" alttext="\mu" display="inline"><mi>μ</mi></math>), standard deviation (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m14" class="ltx_Math" alttext="\sigma" display="inline"><mi>σ</mi></math>), and prior (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m15" class="ltx_Math" alttext="\pi" display="inline"><mi>π</mi></math>) of each Gaussian as well as the skip penalty (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m16" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Φ</mi></math>) used in this paper.</div>
</div>
<div id="S4.p1" class="ltx_para">
<p class="ltx_p">The model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers.
This idea is adapted from <cite class="ltx_cite"/> who uses it to learn constraint rankings in optimality theory.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">In this work, the final (main) verb is placed at position 0; words (and chunks) before the verb are given progressively more negative positions, and words after the verb are given progressively more positive positions (see Table <a href="#S2.T1" title="Table 1 ‣ 2 Background ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Learner expectations of where an argument will appear relative to the verb are modelled as two-component Gaussian mixtures: one mixture of Gaussians (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m1" class="ltx_Math" alttext="G_{S\cdot}" display="inline"><msub><mi>G</mi><mrow><mi>S</mi><mo>⁣</mo><mo>⋅</mo></mrow></msub></math>) corresponds to the subject argument, another (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m2" class="ltx_Math" alttext="G_{O\cdot}" display="inline"><msub><mi>G</mi><mrow><mi>O</mi><mo>⁣</mo><mo>⋅</mo></mrow></msub></math>) corresponds to the object argument.
There is no mixture for a third argument since children do not generalize beyond two arguments until later in development <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S4.F2" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" height="262" version="1.1" viewBox="-199 -132 372 262" width="372"><g transform="matrix(1 0 0 -1 0 -2)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><path d="M 0 0" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -173 -130)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 259)"><switch><foreignObject color="#000000" height="259" overflow="visible" width="345">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><img src="P14-1102/image001.png" id="S4.F2.pic1.g1" class="ltx_graphics" width="312" height="234" alt=""/></p></foreignObject></switch></g></g></g></g><path d="M -157 -39" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -195 -128)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 176)"><switch><foreignObject color="#000000" height="176" overflow="visible" width="75">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">
<span class="ltx_inline-block ltx_transformed_outer" style="width:54.0pt;height:177.083333333333px;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:127.5pt;transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;-webkit-transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;-ms-transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;">
<p class="ltx_p">Probability</p>
</span></span></p></foreignObject></switch></g></g></g></g><path d="M 0 -118" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -130 -122)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="259">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">Position relative to verb</p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></svg><svg xmlns="http://www.w3.org/2000/svg" height="262" version="1.1" viewBox="-199 -132 372 262" width="372"><g transform="matrix(1 0 0 -1 0 -2)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><path d="M 0 0" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -173 -130)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 259)"><switch><foreignObject color="#000000" height="259" overflow="visible" width="345">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><img src="P14-1102/image002.png" id="S4.F2.pic2.g1" class="ltx_graphics" width="312" height="234" alt=""/></p></foreignObject></switch></g></g></g></g><path d="M -157 -39" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -195 -128)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 176)"><switch><foreignObject color="#000000" height="176" overflow="visible" width="75">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">
<span class="ltx_inline-block ltx_transformed_outer" style="width:54.0pt;height:177.083333333333px;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:127.5pt;transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;-webkit-transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;-ms-transform:translate(-36.75pt,36.75pt) rotate(-90deg) ;">
<p class="ltx_p">Probability</p>
</span></span></p></foreignObject></switch></g></g></g></g><path d="M 0 -118" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 -130 -122)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="259">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">Position relative to verb</p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></svg>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Visual representations of (Left) the initial model’s expectations of where arguments will appear, given the initial parameters in Table <a href="#S4.T2" title="Table 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and (Right) the converged model’s expectations of where arguments will appear. </div>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">One component of each mixture learns to represent the canonical position for the argument (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m1" class="ltx_Math" alttext="G_{\cdot C}" display="inline"><msub><mi>G</mi><mrow><mi/><mo>⋅</mo><mi>C</mi></mrow></msub></math>) while the other (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m2" class="ltx_Math" alttext="G_{\cdot N}" display="inline"><msub><mi>G</mi><mrow><mi/><mo>⋅</mo><mi>N</mi></mrow></msub></math>) represents some alternate, non-canonical position such as the filler position in filler-gap constructions.
To reflect the fact that learners have had 15 months of exposure to their language before acquiring filler-gap, the mixture is initialized so that there is a stronger probability associated with the canonical Gaussian than with the non-canonical Gaussian of each mixture.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><cite class="ltx_cite"/> finds that learners may not have strong expectations of canonical argument positions until four years of age, but the results of the current study are extremely robust to changes in initialization, as discussed in Section <a href="#S7" title="7 Discussion ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> of this paper, so this assumption is mostly adopted for ease of exposition.</span></span></span>
Finally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">Thus, the initial model conditions (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) are most likely to realize an SVO ordering, although it is possible to obtain SOV (by sampling a negative number from the blue curve) or even OSV (by also sampling the red curve very close to 0).
The model is most likely to hypothesize a preverbal object when it has already assigned the subject role to something and, in addition, there is no postverbal noun competing for the object label.
In other words, the model infers that an object extraction may have occurred if there is a ‘missing’ postverbal argument.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">Finally, the probability of a given sequence is the product of the label probabilities for the component argument positions (e.g. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m1" class="ltx_Math" alttext="G_{SC}" display="inline"><msub><mi>G</mi><mrow><mi>S</mi><mo>⁢</mo><mi>C</mi></mrow></msub></math> generating an argument at position -2, etc).
Since many sentences have more than two nouns, the model is allowed to skip nouns by multiplying a penalty term (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m2" class="ltx_Math" alttext="\Phi" display="inline"><mi mathvariant="normal">Φ</mi></math>) into the product for each skipped noun; the cost is set at 0.00001 for this study, though see Section <a href="#S7" title="7 Discussion ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for a discussion of the constraints on this parameter.
See Table <a href="#S4.T2" title="Table 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for initialization parameters and Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for a visual representation of the initial expectations of the model.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">This work uses a model with 2-component mixtures for both subjects and objects (termed the <em class="ltx_emph">symmetric model</em>).
This formulation achieves the best fit to the training data according to the Bayesian Information Criterion (BIC).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>The BIC rewards improved log-likelihood but penalizes increased model complexity.</span></span></span>
However, follow-up experiments find that the non-canonical subject Gaussian only improves the likelihood of the data by erroneously modeling postverbal nouns in imperative statements.
The lack of a canonical subject in English imperatives allows the model to improve the likelihood of the data by using the non-canonical subject Gaussian to capture fictitious postverbal arguments.
When imperatives are filtered out of the training corpus, the symmetric model obtains a worse BIC fit than a model that lacks the non-canonical subject Gaussian.
Therefore, if one makes the assumption that imperatives are prosodically-marked for learners (e.g. the learner is the implicit subject), the best model is one that lacks a non-canonical subject.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup>This finding suggests that a Dirichlet Process or other means of dynamically determining the number of components in each mixture would converge to a model that lacks non-canonical subjects if imperative filtering were employed.</span></span></span>
The remainder of this paper assumes a symmetric model to demonstrate what happens if such an assumption is not made; for the evaluations described in this paper, the results are similar in either case.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p">This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007)<cite class="ltx_cite"/> since it is not based on Hidden Markov Models.
Instead, it determines the best ordering for the sentence as a whole.
This approach bears some similarity to a Generalized Mallows model <cite class="ltx_cite">[]</cite>, but the current formulation was chosen due to being independently posited as cognitively plausible <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p class="ltx_p">Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (Right) shows the converged, final state of the model.
The model expects the first argument (usually agent) to be assigned preverbally and expects the second (say, patient) to be assigned postverbally; however, there is now a larger chance that the second argument will appear preverbally.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">The model in this work is trained using transcribed child-directed speech (CDS) from the BabySRL portions <cite class="ltx_cite">[]</cite> of CHILDES <cite class="ltx_cite">[]</cite>.
Chunking is performed using a basic noun-chunker from NLTK <cite class="ltx_cite">[]</cite>.
Based on an initial analysis of chunker performance, <em class="ltx_emph">yes</em> is hand-corrected to not be a noun.
Poor chunker perfomance is likely due to a mismatch in chunker training and testing domains (Wall Street Journal text vs transcribed speech), but chunking noise may be a good estimation of learner uncertainty, so the remaining text is left uncorrected.
All noun phrase chunks are then replaced with their final noun (presumed the head) to approximate the ability of children to distinguish nouns from modifiers <cite class="ltx_cite">[]</cite>.
Finally, for each sentence, the model assigns sentence positions to each word with the final verb at zero.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">Viterbi Expectation-Maximization is performed over each sentence in the corpus to infer the parameters of the model.
During the Expectation step, the model uses the current Gaussian parameters to label the nouns in each sentence with argument roles.
Since the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object.
The model then chooses the best label sequence for each sentence.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">These newly labelled sentences are used during the Maximization step to determine the Gaussian parameters that maximize the likelihood of that labelling.
The mean of each Gaussian is updated to the mean position of the words it labels.
Similarly, the standard deviation of each Gaussian is updated with the standard deviation of the positions it labels.
A learning rate of 0.3 is used to prevent large parameter jumps.
The prior probability of each Gaussian is updated as the ratio of that Gaussian’s labellings to the total number of labellings from that mixture in the corpus:</p>
<table id="S5.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E1.m1" class="ltx_Math" alttext="\pi_{\rho\theta}=\frac{\mid G_{\rho\theta}\mid}{\mid G_{\rho\cdot}\mid}" display="block"><mrow><msub><mi>π</mi><mrow><mi>ρ</mi><mo>⁢</mo><mi>θ</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo fence="true">∣</mo><msub><mi>G</mi><mrow><mi>ρ</mi><mo>⁢</mo><mi>θ</mi></mrow></msub><mo fence="true">∣</mo></mrow><mrow><mo fence="true">∣</mo><msub><mi>G</mi><mrow><mi>ρ</mi><mo>⁣</mo><mo>⋅</mo></mrow></msub><mo fence="true">∣</mo></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m1" class="ltx_Math" alttext="\rho\in\{S,O\}" display="inline"><mrow><mi>ρ</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>S</mi><mo>,</mo><mi>O</mi></mrow><mo>}</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m2" class="ltx_Math" alttext="\theta\in\{C,N\}" display="inline"><mrow><mi>θ</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>C</mi><mo>,</mo><mi>N</mi></mrow><mo>}</mo></mrow></mrow></math>.</p>
</div>
<div id="S5.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_rr" colspan="3">Eve (n = 4820)</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3">Adam (n = 4461)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.54</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.64</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.59</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.56</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.69</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.59<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m1" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.57<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m2" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m3" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.66</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.62</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.58</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m4" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.54</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.71</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">.61<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m5" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.53</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.67</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.59<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m6" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Overall accuracy on the Eve and Adam sections of the BabySRL corpus.
Bottom rows reflect accuracy when non-agent roles are collapsed into a single role.
Note that improvements are numerically slight since filler-gap is relatively rare <cite class="ltx_cite">[]</cite>. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m8" class="ltx_Math" alttext="{}^{*}p&lt;\!&lt;.01" display="inline"><mrow><mmultiscripts><mi>p</mi><mprescripts/><none/><mo>*</mo></mmultiscripts><mo rspace="0.8pt">&lt;</mo><mo>&lt;</mo><mn>.01</mn></mrow></math></div>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">Best results seem to be obtained when the skip-penalty is loosened by an order of magnitude during testing.
Essentially, this forces the model to tightly adhere to the perceived argument structure during training to learn more rigid parameters, but the model is allowed more leeway to skip arguments it has less confidence in during testing.
Convergence (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Model ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) tends to occur after four iterations but can take up to ten iterations depending on the initial parameters.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p">Since the model is unsupervised, it is trained on a given corpus (e.g. Eve) before being tested on the role annotations of that same corpus.
The Eve corpus was used for development purposes,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup>This is included for transparency, though the initial parameters have very little bearing on the final results as stated in Section <a href="#S7" title="7 Discussion ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, so the danger of overfitting to development data is very slight.</span></span></span>
and the Adam data was used only for testing.</p>
</div>
<div id="S5.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Subject Extraction filter:</td>
<td class="ltx_td ltx_align_center">S</td>
<td class="ltx_td ltx_align_center">x</td>
<td class="ltx_td ltx_align_center">V</td>
<td class="ltx_td ltx_align_center">…</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b">Object Extraction filter:</td>
<td class="ltx_td ltx_align_center ltx_border_b">O</td>
<td class="ltx_td ltx_align_center ltx_border_b">…</td>
<td class="ltx_td ltx_align_center ltx_border_b">V</td>
<td class="ltx_td ltx_align_center ltx_border_b">…</td></tr>
</tbody>
</table>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_rr" colspan="3">Eve (n = 1345)</td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3">Adam (n = 1287)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m1" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.57</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.52</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m2" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.67</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">.61<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m3" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.54</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.63</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.58<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m4" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>(Above) Filters to extract filler-gap constructions: A) the subject and verb are not adjacent, B) the object precedes the verb. (Below) Filler-gap accuracy on the Eve and Adam sections of the BabySRL corpus when non-agent roles are collapsed into a single role. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m6" class="ltx_Math" alttext="{}^{*}p&lt;\!&lt;.01" display="inline"><mrow><mmultiscripts><mi>p</mi><mprescripts/><none/><mo>*</mo></mmultiscripts><mo rspace="0.8pt">&lt;</mo><mo>&lt;</mo><mn>.01</mn></mrow></math></div>
</div>
<div id="S5.p6" class="ltx_para">
<p class="ltx_p">For testing, this study uses the semantic role annotations in the BabySRL corpus.
These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of <cite class="ltx_cite"/> before roughly hand-correcting them <cite class="ltx_cite">[]</cite>.
The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles.
Therefore, overall accuracy results (see Table <a href="#S5.T3" title="Table 3 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m1" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math> in all tables).</p>
</div>
<div id="S5.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r">P</td>
<td class="ltx_td ltx_align_center ltx_border_r">R</td>
<td class="ltx_td ltx_align_center ltx_border_rr">F</td>
<td class="ltx_td ltx_align_center ltx_border_r">P</td>
<td class="ltx_td ltx_align_center ltx_border_r">R</td>
<td class="ltx_td ltx_align_center ltx_border_r">F</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Eve</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="3">Subj (n = 691)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Obj (n = 654)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m1" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.66</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.83</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.35</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.33</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m2" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.84</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.72<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m3" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.48<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m4" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Adam</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" colspan="3">Subj (n = 886)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3">Obj (n = 1050)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m5" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.81</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.30</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m6" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.66</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.81</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">.73</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.44</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.48</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.46<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m7" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td></tr>
</tbody>
</table>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r">P</td>
<td class="ltx_td ltx_align_center ltx_border_r">R</td>
<td class="ltx_td ltx_align_center ltx_border_rr">F</td>
<td class="ltx_td ltx_align_center ltx_border_r">P</td>
<td class="ltx_td ltx_align_center ltx_border_r">R</td>
<td class="ltx_td ltx_align_center ltx_border_r">F</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Eve</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="3">Wh- (n = 689)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">That (n = 125)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m8" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.63</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.45</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.53</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.43</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.45</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m9" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.73</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.75</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.74<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m10" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.44</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m11" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Adam</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" colspan="3">Wh- (n = 748)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3">That (n = 189)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m12" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.37</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.50</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Trained<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m13" class="ltx_Math" alttext="{}_{c}" display="inline"><msub><mi/><mi>c</mi></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.61</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.65</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">.63<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m14" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.47</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.56</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.51<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m15" class="ltx_Math" alttext="{}^{\dagger}" display="inline"><msup><mi/><mo>†</mo></msup></math></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>(Left) Subject-extraction accuracy and object-extraction accuracy and (Right) <em class="ltx_emph">Wh</em>-relative accuracy and <em class="ltx_emph">that</em>-relative accuracy; calculated over the Eve and Adam sections of the BabySRL corpus with non-agent roles collapsed into a single role. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m18" class="ltx_Math" alttext="{}^{\dagger}p=.02" display="inline"><mrow><mmultiscripts><mi>p</mi><mprescripts/><none/><mo>†</mo></mmultiscripts><mo>=</mo><mn>.02</mn></mrow></math> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m19" class="ltx_Math" alttext="{}^{*}p&lt;\!&lt;.01" display="inline"><mrow><mmultiscripts><mi>p</mi><mprescripts/><none/><mo>*</mo></mmultiscripts><mo rspace="0.8pt">&lt;</mo><mo>&lt;</mo><mn>.01</mn></mrow></math></div>
</div>
<div id="S5.p7" class="ltx_para">
<p class="ltx_p">Since children do not generalize above two arguments during the modelled age range <cite class="ltx_cite">[]</cite>, the collapsed numbers more closely reflect the performance of a learner at this age than the raw numbers.
The increase in accuracy obtained from collapsing non-agent arguments indicates that children may initially generalize incorrectly to some verbs and would need to learn lexically-specific role assignments (e.g. double-object constructions of <em class="ltx_emph">give</em>).
Since the current work is interested in general filler-gap comprehension at this age, including over unknown verbs, the remaining analyses in this paper consider performance when non-agent arguments are collapsed.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>Though performance is slightly worse when arguments are not collapsed, all the same patterns emerge.</span></span></span></p>
</div>
<div id="S5.p8" class="ltx_para">
<p class="ltx_p">Next, a filler-gap version of the BabySRL corpus is created using a coarse filtering process:
the new corpus is comprised of all sentences where an associated object precedes the final verb and all sentences where the relevant subject is not immediately followed by the final verb (see Table <a href="#S5.T4" title="Table 4 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
For these filler-gap evaluations, the model is trained on the full version of the corpus in question (e.g. Eve) before being tested on the filler-gap subset of that corpus.
The overall results of the filler-gap evaluation (see Table <a href="#S5.T4" title="Table 4 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) indicate that the model improves significantly at parsing filler-gap constructions after training.</p>
</div>
<div id="S5.p9" class="ltx_para">
<p class="ltx_p">The performance of the model on role-assignment in filler-gap constructions may be analyzed further in terms of how the model performs on subject-extractions compared with object-extractions and in terms of how the model performs on <em class="ltx_emph">that</em>-relatives compared with <em class="ltx_emph">wh</em>-relatives (see Table <a href="#S5.T5" title="Table 5 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S5.p10" class="ltx_para">
<p class="ltx_p">The model actually performs worse at subject-extractions after training than before training.
This is unsurprising because, prior to training, subjects have little-to-no competition for preverbal role assignments; after training, there is a preverbal extracted object category, which the model can erroneously use.
This slight, though significant in Eve, deficit is counter-balanced by a very substantial and significant improvement in object-extraction labelling accuracy.</p>
</div>
<div id="S5.p11" class="ltx_para">
<p class="ltx_p">Similarly, training confers a large and significant improvement for role assignment in <em class="ltx_emph">wh</em>-relative constructions, but it yields less of an improvement for <em class="ltx_emph">that</em>-relative constructions.
This difference mimics
a finding observed in the developmental literature where
children seem slower to acquire comprehension of <em class="ltx_emph">that</em>-relatives than of <em class="ltx_emph">wh</em>-relatives <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Comparison to BabySRL</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">The acquisition of semantic role labelling (SRL) by the BabySRL model <cite class="ltx_cite">[]</cite> bears many similarities to the current work and is, to our knowledge, the only comparable line of inquiry to the current one.
The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make <cite class="ltx_cite">[]</cite>, the 1-1 role bias error (<em class="ltx_emph">John and Mary gorped</em> interpreted as <em class="ltx_emph">John gorped Mary</em>).
Similar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">This section will demonstrate that the model in this paper initially reflects 1-1 role bias comparably to BabySRL, though it progresses beyond this bias after training.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>All evaluations in this section are preceded by training on the chunked Eve corpus.</span></span></span>
Further, the model in this paper is able to reflect the concurrent acquisition of filler-gap whereas BabySRL does not seem well-suited to such a task.
Finally, BabySRL performs undesirably in intransitive settings whereas the model in this paper does not.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> demonstrate that a supervised perceptron classifier, based on positional features and trained on the silver role label annotations of the BabySRL corpus, manifests 1-1 role bias errors.
Follow-up studies show that supervision may be lessened <cite class="ltx_cite">[]</cite> or removed <cite class="ltx_cite">[]</cite> and BabySRL will still reflect a substantial 1-1 role bias.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> and <cite class="ltx_cite"/> run direct analyses of how frequently their models make 1-1 role bias errors.
A comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table <a href="#S6.T6" title="Table 6 ‣ 6 Comparison to BabySRL ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>While Table <a href="#S6.T6" title="Table 6 ‣ 6 Comparison to BabySRL ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> analyzes erroneous labellings of NNV structure, the ‘Obj’ column of Table <a href="#S5.T5" title="Table 5 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (Left) shows model accuracy on NNV structures.</span></span></span>
The results of <cite class="ltx_cite"/> and <cite class="ltx_cite"/> depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model).
Further, the model presented here from <cite class="ltx_cite"/> has a unique argument constraint, similar to the model in this paper, in order to make comparison as direct as possible.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p">The 1-1 role bias error rate (before training) of the model presented in this paper is comparable to that of <cite class="ltx_cite"/> and <cite class="ltx_cite"/>, which shows that the current model provides comparable developmental modeling benefits to the BabySRL models.
Further, similar to real children (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) the model presented in this paper develops beyond this error by the end of its training,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years <cite class="ltx_cite">[]</cite>.</span></span></span>
whereas the BabySRL models still make this error after training.</p>
</div>
<div id="S6.T6" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r">Error rate</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.36</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.11</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Initial (given 2 args)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.66</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained (given 2 args)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.13</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2008 arg-arg position</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.65</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2008 arg-verb position</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2009 arg-arg position</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.82</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2009 arg-verb position</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.63</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>1-1 role bias error in this model compared to the models of <cite class="ltx_cite"/> and <cite class="ltx_cite"/>.
That is, how frequently each model labelled an NNV sentence SOV.
Since the Connor et al. models are perceptron-based, they require both arguments be labelled.
The model presented in this paper does not share this restriction, so the raw error rate for this model is presented in the first two lines; the error rate once this additional restriction is imposed is given in the second two lines.</div>
</div>
<div id="S6.p6" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> look at how frequently their model correctly labels the agent in transitive and intransitive sentences with unknown verbs (to demonstrate that it exhibits an agent-first bias).
This evaluation can be replicated for the current study by generating 1,000 sentences with the transitive form of NVN and a further 1,000 sentences with the intransitive form of NV (see Table <a href="#S6.T7" title="Table 7 ‣ 6 Comparison to BabySRL ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<div id="S6.p7" class="ltx_para">
<p class="ltx_p">Since <cite class="ltx_cite"/> investigate the effects of different initial lexicons, this evaluation compares against the resulting BabySRL from each initializer:
they initially seed their part-of-speech tagger with either the 10 or 365 most frequent nouns in the corpus or they dispense with the tagger and use gold part-of-speech tags.</p>
</div>
<div id="S6.p8" class="ltx_para">
<p class="ltx_p">As with subject extraction, the model in this paper gets less accurate after training because of the newly minted extracted object category that can be mistakenly used in these canonical settings.
While the model of <cite class="ltx_cite"/> outperforms the model presented here when in a transitive setting, their model does much worse in an intransitive setting.
The difference in transitive settings stems from increased lexicalization, as is apparent from their results alone; the model presented here initially performs close to their weakly lexicalized model, though training impedes agent-prediction accuracy due to an increased probability of non-canonical objects.</p>
</div>
<div id="S6.T7" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r">NVN</td>
<td class="ltx_td ltx_align_center ltx_border_r">NV</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Sents in Eve</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1173</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1513</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Sents in Adam</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1029</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1353</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Initial</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.67</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Trained</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.96</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Weak (10) lexical</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.59</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Strong (365) lexical</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.41</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Gold Args</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.77</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">.58</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Agent-prediction recall accuracy in transitive (NVN) and intransitive (NV) settings of the model presented in this paper (middle) and the combined model of <cite class="ltx_cite"/> (bottom), which has features for argument-argument relative position as well as argument-predicate relative position and so is closest to the model presented in this paper.</div>
</div>
<div id="S6.p9" class="ltx_para">
<p class="ltx_p">For the intransitive case, however, whereas the model presented in this paper is generally able to successfully label the lone noun as the subject, the model of <cite class="ltx_cite"/> chooses to label lone nouns as objects about 40% of the time.
This likely stems from their model’s reliance on argument-argument relative position as a feature; when there is no additional argument to use for reference, the model’s accuracy decreases.
This is borne out by their model (not shown in Table <a href="#S6.T7" title="Table 7 ‣ 6 Comparison to BabySRL ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) that omits the argument-argument relative position feature and solely relies on verb-argument position, which achieves up to 70% accuracy in intransitive settings.
Even in that case, however, BabySRL still chooses to label lone nouns as objects 30% of the time.
The fact that intransitive sentences are more common than transitive sentences in both the Eve and Adam sections of the BabySRL corpus suggests that learners should be more likely to assign correct roles in an intransitive setting, which is not reflected in the BabySRL results.</p>
</div>
<div id="S6.p10" class="ltx_para">
<p class="ltx_p">The overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g. where a given noun is relative to the verb).
Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected.</p>
</div>
<div id="S6.p11" class="ltx_para">
<p class="ltx_p">Further, while BabySRL consistently reflects 1-1 role bias (corresponding to a pre 25-month old learner), it also learns to productively label five roles, which developmental studies have shown does not take place until at least 31 months <cite class="ltx_cite">[]</cite>.
Finally, it does not seem likely that BabySRL could be easily extended to capture filler-gap acquisition.
The argument-verb position features impede acquisition of filler-gap by classifying preverbal arguments as agents, and the argument-argument position features inhibit accurate labelling in intransitive settings and result in an agent-first bias which would tend to label extracted objects as agents.
In fact, these observations suggest that any linear classifier which relies on positioning features will have difficulties modeling filler-gap acquisition.</p>
</div>
<div id="S6.p12" class="ltx_para">
<p class="ltx_p">In sum, the unlexicalized model presented in this paper is able to achieve greater labelling accuracy than the lexicalized BabySRL models in intransitive settings, though this model does perform slightly worse in the less common transitive setting.
Further, the unsupervised model in this paper initially reflects developmental 1-1 role bias as well as the supervised BabySRL models, and it is able to progress beyond this bias.
Finally, unlike BabySRL, the model presented here provides a cognitive model of the acquisition of filler-gap comprehension, which BabySRL does not seem well-suited to model.</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">This paper has presented a simple cognitive model of filler-gap acquisition, which is able to capture several findings from developmental psychology.
Training significantly improves role labelling in the case of object-extractions, which improves the overall accuracy of the model.
This boost is accompanied by a slight decrease in labelling accuracy in subject-extraction settings.
The asymmetric ease of subject versus object comprehension is well-documented in both children and adults <cite class="ltx_cite">[]</cite>, and while training improves the model’s ability to process object-extractions, there is still a gap between object-extraction and subject-extraction comprehension even after training.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">Further, the model exhibits better comprehension of <em class="ltx_emph">wh</em>-relatives than <em class="ltx_emph">that</em>-relatives similar to children <cite class="ltx_cite">[]</cite>.
This could also be an area where a lexicalized model could do better.
As <cite class="ltx_cite"/> point out, whereas <em class="ltx_emph">wh</em>-relatives such as <em class="ltx_emph">who</em> or <em class="ltx_emph">which</em> always signify a filler-gap construction, <em class="ltx_emph">that</em> can occur for many different reasons (demonstrative, determiner, complementizer, etc) and so is a much weaker filler-gap cue.
A lexical model could potentially pick up on clues which could indicate when <em class="ltx_emph">that</em> is a relativizer or simply improve on its comprehension of <em class="ltx_emph">wh</em>-relatives even more.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">It is interesting to note that the cuurent model does not make use of <em class="ltx_emph">that</em> as a cue at all and yet is still slower at acquiring <em class="ltx_emph">that</em>-relatives than <em class="ltx_emph">wh</em>-relatives.
This fact suggests that the findings of <cite class="ltx_cite"/> may be partially explained by a frequency effect: perhaps the input to children is simply biased such that <em class="ltx_emph">wh</em>-relatives are much more common than <em class="ltx_emph">that</em>-relatives (as shown in Table <a href="#S5.T5" title="Table 5 ‣ 5 Evaluation ‣ Bootstrapping into Filler-Gap: An Acquisition Story" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">This model also initially reflects the 1-1 role bias observed in children <cite class="ltx_cite">[]</cite> as well as previous models <cite class="ltx_cite">[]</cite> without sacrificing accuracy in canonical intransitive settings.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p">Finally, this model is extremely robust to different initializations.
The canonical Gaussian expectations can begin far from the verb (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p5.m1" class="ltx_Math" alttext="\pm 3" display="inline"><mrow><mo>±</mo><mn>3</mn></mrow></math>) or close to the verb (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.p5.m2" class="ltx_Math" alttext="\pm 0.1" display="inline"><mrow><mo>±</mo><mn>0.1</mn></mrow></math>), and the standard deviations of the distributions and the skip-penalty can vary widely; the model always converges to give comparable results to those presented here.
The only constraint on the initial parameters is that the probability of the extracted object occurring preverbally must exceed the skip-penalty (i.e. extraction must be possible).
In short, this paper describes a simple, robust cognitive model of the development of a learner between 15 months until somewhere between 25- and 30-months old (since 1-1 role bias is no longer present but no more than two arguments are being generalized).</p>
</div>
<div id="S7.p6" class="ltx_para">
<p class="ltx_p">In future, it would be interesting to incorporate lexicalization into the model presented in this paper, as this feature seems likely to bridge the gap between this model and BabySRL in transitive settings.
Lexicalization should also help further distinguish modifiers from arguments and improve the overall accuracy of the model.</p>
</div>
<div id="S7.p7" class="ltx_para">
<p class="ltx_p">It would also be interesting to investigate how well this model generalizes to languages besides English.
Since the model is able to use the verb position as a semi-permeable boundary between canonical subjects and objects, it may not work as well in verb-final languages, and thus makes the prediction that filler-gap comprehension may be acquired later in development in such languages due to a greater reliance on hierarchical syntax.</p>
</div>
<div id="S7.p8" class="ltx_para">
<p class="ltx_p">Ordering is one of the definining characteristics of a language that must be acquired by learners (e.g. SVO vs SOV), and this work shows that filler-gap comprehension can be acquired as a by-product of learning orderings rather than having to resort to higher-order syntax.
Note that this model cannot capture the constraints on filler-gap usage which require a hierarchical grammar (e.g. subjacency), but such knowledge is really only needed for successful production of filler-gap constructions, which occurs much later (around 5 years; de Villiers and Roeper, 1995)<cite class="ltx_cite"/>.
Further, the kind of ordering system proposed in this paper may form an initial basis for learning such grammars <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
<div id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">8 </span>Acknowledgements</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">Thanks to Peter Culicover, William Schuler, Laura Wagner, and the attendees of the OSU 2013 Fall Linguistics Colloquium Fest for feedback on this work.
This work was partially funded by an OSU Dept. of Linguistics Targeted Investment for Excellence (TIE) grant for collaborative interdisciplinary projects conducted during the academic year 2012-13.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:33:56 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
