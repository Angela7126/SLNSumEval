<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Linguistically debatable or just plain wrong?</title>
<!--Generated on Wed Jun 11 17:58:34 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Linguistically debatable or just plain wrong?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Barbara Plank, Dirk Hovy and Anders Søgaard
<br class="ltx_break"/>Center for Language Technology
<br class="ltx_break"/>University of Copenhagen, Denmark 
<br class="ltx_break"/>Njalsgade 140, DK-2300 Copenhagen S 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">bplank@cst.dk,dirk@cst.dk,soegaard@hum.ku.dk</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">In linguistic annotation projects, we typically develop annotation guidelines to minimize disagreement. However, in this position paper we question whether we should actually limit the disagreements between annotators, rather than embracing them.
We present an empirical analysis of part-of-speech annotated data sets that suggests that disagreements are systematic across domains and to a certain extend also across languages. This points to an underlying ambiguity rather than random errors. Moreover, a quantitative analysis of tag confusions reveals that the majority of disagreements are due to linguistically debatable cases rather than annotation errors. Specifically, we show that even in the absence of annotation guidelines only 2% of annotator choices are linguistically unmotivated.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">In NLP, we often model annotation as if it reflected a single ground truth that was guided by an underlying linguistic theory. If this was true, the specific theory should be learnable from the annotated data. However, it is well known that there are linguistically <span class="ltx_text ltx_font_italic">hard cases</span> <cite class="ltx_cite">[]</cite>, where no theory provides a clear answer, so annotation schemes commit to more or less arbitrary decisions. For example, in parsing auxiliary verbs may head main verbs, or vice versa, and in part-of-speech (POS) tagging, possessive pronouns may belong to the category of determiners or the category of pronouns. This position paper argues that annotation projects should embrace these hard cases rather than pretend they can be unambiguously resolved. Instead of using overly specific annotation guidelines, designed to minimize inter-annotator disagreement <cite class="ltx_cite">[]</cite>, and adjudicating between annotators of different opinions, we should embrace systematic inter-annotator disagreements. To motivate this, we present an empirical analysis showing</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">that certain inter-annotator disagreements are systematic, and</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">that actual errors are in fact so infrequent as to be negligible, even when linguists annotate without guidelines.</p>
</div></li>
</ol>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The empirical analysis presented below relies on text corpora annotated with syntactic categories or parts-of-speech (POS). POS is part of most linguistic theories, but nevertheless, there are still many linguistic constructions – even very frequent ones – whose POS analysis is widely debated.
The following sentences exemplify some of these hard cases that annotators frequently disagree on. Note that we do not claim that both analyses in each of these cases (1–3) are equally good, but that there is some linguistic motivation for either analysis in each case.
<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_inline-block" style="width:433.6pt;height:0px;vertical-align:-0.0pt;">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">(1)</td>
<td class="ltx_td ltx_align_left">Noam</td>
<td class="ltx_td ltx_align_left">goes</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">out</span></td>
<td class="ltx_td ltx_align_left">tonight</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Noun</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Verb</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Adp/Prt</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Adv/Noun</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">(2)</td>
<td class="ltx_td ltx_align_left">Noam</td>
<td class="ltx_td ltx_align_left">likes</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">social</span></td>
<td class="ltx_td ltx_align_left">media</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Noun</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Verb</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Adj/Noun</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Noun</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">(3)</td>
<td class="ltx_td ltx_align_left">Noam</td>
<td class="ltx_td ltx_align_left">likes</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">his</span></td>
<td class="ltx_td ltx_align_left">car</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Noun</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Verb</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Det/Pron</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Noun</span></td></tr>
</tbody>
</table>
</span>

<br class="ltx_break"/></p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">To substantiate our claims, we first compare the distribution of inter-annotator disagreements across domains and languages, showing that most disagreements are systematic (Section <a href="#S2" title="2 Annotator disagreements across domains and languages ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). This suggests that most annotation differences derive from hard cases, rather than random errors.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation <em class="ltx_emph">errors</em>, and which ones reflect linguistically hard cases (Section <a href="#S3" title="3 Hard cases and annotation errors ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors. This suggests that inter-annotator agreement scores often hide the fact that the vast majority of annotations are actually linguistically motivated. In our case, less than 2% of the overall annotations are linguistically unmotivated.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Finally, in Section <a href="#S4" title="4 Learning to detect annotation errors ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present an experiment trying to learn a model to distinguish between hard cases and annotation errors.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Annotator disagreements across domains and languages</h2>

<div id="S2.fig1" class="ltx_figure">
<p class="ltx_p ltx_align_center">a) <img src="P14-2083/image006.png" id="S2.g1" class="ltx_graphics" width="167" height="166" alt=""/>
b) <img src="P14-2083/image002.png" id="S2.g2" class="ltx_graphics" width="167" height="166" alt=""/>
c) <img src="P14-2083/image003.png" id="S2.g3" class="ltx_graphics" width="167" height="166" alt=""/></p><span class="ltx_ERROR undefined ltx_centering">\captionof</span>
<p class="ltx_p ltx_align_center">figureHinton diagrams of inter-annotator disagreement on (a) excerpt from WSJ <cite class="ltx_cite">[]</cite>, (b) random Twitter sample, and (c) pre-transcribed spoken language excerpts from talkbank.org</p>
</div>
<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In this study, we had between 2-10 individual annotators with degrees in linguistics annotate different kinds of English text with POS tags, e.g., newswire text (PTB WSJ Section 00), transcripts of spoken language (from a database containing transcripts of conversations, Talkbank<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><a href="http://talkbank.org/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://talkbank.org/</span></a></span></span></span>), as well as Twitter posts. Annotators were specifically <em class="ltx_emph">not</em> presented with guidelines that would help them resolve hard cases. Moreover, we compare professional annotation to that of lay people.
We instructed annotators to use the 12 universal POS tags of Petrov et al. <cite class="ltx_cite">[]</cite>. We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Experiments with variation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams on WSJ <cite class="ltx_cite">[]</cite> and the French data lead us to estimate that the fine-to-coarse mapping of POS tags disregards about 20% of observed tag-pair confusion types, most of which relate to fine-grained verb and noun distinctions, e.g. past participle versus past in “[..] criminal lawyers speculated/VBD vs. VBN that [..]”.</span></span></span>
For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project <cite class="ltx_cite">[]</cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>We mapped POS tags into the universal POS tags using the mappings available here: <a href="https://code.google.com/p/universal-pos-tags/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://code.google.com/p/universal-pos-tags/</span></a></span></span></span> All data sets, except the French one, are publicly available at <a href="http://lowlands.ku.dk/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://lowlands.ku.dk/</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">We present disagreements as Hinton diagrams in Figure <a href="#S2.fig1" title="2 Annotator disagreements across domains and languages ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 (spoken and WSJ) to 0.869 (spoken and Twitter). Kendall’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p2.m1" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> ranges from 0.498 (Twitter and WSJ) to 0.659 (spoken and Twitter). All diagrams have a vaguely “dagger”-like shape, with the blade going down the diagonal from top left to bottom right, and a slightly curved “hilt” across the counter-diagonal, ending in the more pronounced ADP/PRT confusion cells.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Disagreements are very similar across all three domains. In particular, adpositions (ADP) are confused with particles (PRT) (as in the case of “<em class="ltx_emph">get</em> out”); adjectives (ADJ) are confused with nouns (as in “<em class="ltx_emph">stone</em> lion”); pronouns (PRON) are confused with determiners (DET) (“<em class="ltx_emph">my</em> house”); numerals are confused with adjectives, determiners, and nouns (“<em class="ltx_emph">2nd</em> time”); and adjectives are confused with adverbs (ADV) (“see you <em class="ltx_emph">later</em>”). In Twitter, the X category is often confused with punctuations, e.g., when annotating punctuation acting as discourse continuation marker.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. More surprisingly, though, we also find that, as discussed next, c) roughly the same disagreements are also observed when comparing the linguistic intuitions of lay people.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">More specifically, we had lay annotators on the crowdsourcing platform Crowdflower re-annotate the training section of <cite class="ltx_cite"/>. They collected five annotations per word. Only annotators that
had answered correctly on 4 gold items (randomly chosen from a set of 20 gold items provided by the authors) were allowed to submit annotations. In total, 177 individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 items. The full data set contains 14,619 items and is described in further detail in <cite class="ltx_cite"/>. Annotators were satisfied with the task (4.5 on a scale from 1 to 5) and felt that instructions were clear (4.4/5), and the pay reasonable (4.1/5). The crowdsourced annotations aggregated using majority voting agree with the expert annotations in 79.54% of the cases. If we pre-filter the data via Wiktionary and use an item-response model <cite class="ltx_cite">[]</cite> rather than majority voting, the agreement rises to 80.58%.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">Figure <a href="#S2.fig2" title="2 Annotator disagreements across domains and languages ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the Hinton diagram of the disagreements of lay people. Disagreements are very similar to the disagreements between expert annotators, especially on Twitter data (Figure <a href="#S2.fig1" title="2 Annotator disagreements across domains and languages ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b). One difference is that lay people do not confuse numerals very often, probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m1" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> of 0.493 and Kendall’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m2" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> of 0.366 for WSJ).</p>
</div>
<div id="S2.fig2" class="ltx_figure"><img src="P14-2083/image005.png" id="S2.g4" class="ltx_graphics ltx_centering" width="167" height="166" alt=""/><span class="ltx_ERROR undefined ltx_centering">\captionof</span>
<p class="ltx_p ltx_align_center">figureDisagreement between lay annotators</p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">Lastly, we compare the disagreements of annotators on a French social media data set <cite class="ltx_cite">[]</cite>, which we mapped to the universal POS tag set. Again, we see the familiar dagger shape. The Spearman coefficient with English Twitter is 0.288; Kendall’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m1" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> is 0.204. While the correlation is weaker across languages than across domains, it remains statistically significant (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m2" class="ltx_Math" alttext="p&lt;0.001" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow></math>).</p>
</div>
<div id="S2.fig3" class="ltx_figure"><img src="P14-2083/image007.png" id="S2.g5" class="ltx_graphics ltx_centering" width="167" height="168" alt=""/><span class="ltx_ERROR undefined ltx_centering">\captionof</span>
<p class="ltx_p ltx_align_center">figureDisagreement on French social media</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Hard cases and annotation errors</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In the previous section, we demonstrated that some disagreements are consistent across domains and languages. We noted earlier, though, that disagreements can arise both from hard cases and from annotation errors. This can explain some of the variation.
In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. The disagreements that remain are the truly hard cases.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">We use a modified version of the a priori algorithm introduced in <cite class="ltx_cite"/> to identify annotation inconsistencies. It works by collecting “variation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams”, i.e. the longest sequence of words (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram in the same corpus. The algorithm starts off by looking for unigrams and expands them until no longer <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams are found.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">For each variation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present annotators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is the tag plausible for the given context?</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">We used 3 annotators with PhD degrees in linguistics. In total, our data set contains 880 items, i.e. 440 annotated confusion tag pairs. The raw agreement was 86%. Figure <a href="#S3.fig1" title="3 Hard cases and annotation errors ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows how truly hard cases are distributed over tag pairs (dark gray bars), as well as the proportion of confusions with respect to a given tag pair that are truly hard cases (light gray bars).
The figure shows, for instance, that the variation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram regarding ADP-ADV is the second most frequent one (dark gray), and approximately 70% of ADP-ADV disagreements are linguistically hard cases (light gray). NOUN-PRON disagreements are always linguistically debatable cases, while they are less frequent.</p>
</div>
<div id="S3.fig1" class="ltx_figure"><img src="P14-2083/image004.png" id="S3.g1" class="ltx_graphics ltx_centering" width="266" height="250" alt=""/><span class="ltx_ERROR undefined ltx_centering">\captionof</span>
<p class="ltx_p ltx_align_center">figureRelative frequency of hard cases</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">A survey of hard cases.</span> To further test the idea of there being truly hard cases that probably cannot be resolved by linguistic theory, we presented nine linguistics faculty members with 10 of the above examples and asked them to pick their favorite analyses. In 8/10 cases, the faculty members disagreed on the right analysis.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Learning to detect annotation errors</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we examine whether we can learn a classifier to distinguish between hard cases and annotation errors. In order to do so, we train a classifier on the annotated data set containing 440 tag-confusion pairs by relying only on surface form features.
If we <span class="ltx_text ltx_font_italic">balance</span> the data set and perform 3-fold cross-validation, a L2-regularized logistic regression (L2-LR) model achieves an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m1" class="ltx_Math" alttext="f_{1}" display="inline"><msub><mi>f</mi><mn>1</mn></msub></math>-score for detecting errors at 70% (cf. Table <a href="#S4.T1" title="Table 1 ‣ 4 Learning to detect annotation errors ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), which is above average, but not very impressive.</p>
</div>
<div id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m1" class="ltx_Math" alttext="f_{1}" display="inline"><msub><mi>f</mi><mn>1</mn></msub></math></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">Hard cases</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">Errors</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">L2-LR</th>
<td class="ltx_td ltx_align_center ltx_border_t">73%(71-77)</td>
<td class="ltx_td ltx_align_center ltx_border_t">70%(65-75)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">NN</th>
<td class="ltx_td ltx_align_center">76%(76-77)</td>
<td class="ltx_td ltx_align_center">71%(68-72)</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Classification results</div>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">The two classes are apparently not easily separable using surface form features, as illustrated in the two-dimensional plot in Figure <a href="#S4.F1" title="Figure 1 ‣ 4 Learning to detect annotation errors ‣ Linguistically debatable or just plain wrong?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, obtained using PCA. The logistic regression decision boundary is plotted as a solid, black line. This is probably also why the nearest neighbor (NN) classifier does slightly better, but again, performance is rather low. While other features may reveal that the problem is in fact learnable, our initial experiments lead us to conclude that, given the low ratio of errors over truly hard cases, learning to detect errors is often not worthwhile.</p>
</div>
<div id="S4.F1" class="ltx_figure"><img src="P14-2083/image001.png" id="S4.F1.g1" class="ltx_graphics ltx_centering" width="302" height="226" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Hard cases and errors in 2d-PCA</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Related work</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite"/> presents work on detecting linguistically hard cases in the context of word sense annotations, e.g., cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases. This work is similar to ours in spirit, but considers a very different task. While we also quantify the proportion of hard cases and present an analysis of these cases, we also show that disagreements are systematic.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">Our work also relates to work on automatically correcting expert annotations for inconsistencies <cite class="ltx_cite">[]</cite>. This work is very different in spirit from our work, but shares an interest in reconsidering expert annotations, and we made use of their mining algorithm here. There has also been recent work on adjudicating noisy crowdsourced annotations <cite class="ltx_cite">[]</cite>. Again, their objective is orthogonal to ours, namely to collapse multiple annotations into a gold standard rather than embracing disagreements.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">Finally, <cite class="ltx_cite"/> use small samples of doubly-annotated
POS data to estimate annotator reliability and show how those metrics
can be implemented in the loss function when inducing POS taggers to reflect
confidence we can put in annotations. They show that not biasing the
theory towards a single annotator but using a cost-sensitive learning
scheme makes POS taggers more robust and more applicable for
downstream tasks.</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In this paper, we show that disagreements between professional or lay annotators are systematic and consistent across domains and some of them are systematic also across languages. In addition, we present an empirical analysis of POS annotations showing that the vast majority of inter-annotator disagreements are competing, but valid, linguistic interpretations. We propose to embrace such disagreements rather than using annotation guidelines to optimize inter-annotator agreement, which would bias our models in favor of some linguistic theory.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We would like to thank the anonymous reviewers for their feedback, as well as Djamé Seddah for the French data.
This research is funded by the ERC Starting Grant
LOWLANDS No. 313695.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p">./biblio.bib.</p>
</div>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:58:34 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
