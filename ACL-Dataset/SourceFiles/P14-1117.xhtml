<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Approximation Strategies for Multi-Structure Sentence Compression</title>
<!--Generated on Tue Jun 10 18:53:19 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Approximation Strategies for Multi-Structure Sentence Compression</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kapil Thadani
<br class="ltx_break"/>Department of Computer Science
<br class="ltx_break"/>Columbia University
<br class="ltx_break"/>New York, NY 10025, USA
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">kapil@cs.columbia.edu</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Sentence compression has been shown to benefit from
joint inference involving both n-gram and dependency-factored objectives
but this typically requires expensive integer programming.
We explore instead the use of Lagrangian relaxation
to decouple the two subproblems and solve them separately.
While dynamic programming is viable for bigram-based sentence compression,
finding optimal compressed trees within graphs is NP-hard.
We recover approximate solutions to this problem using
LP relaxation and maximum spanning tree algorithms, yielding
techniques that can be combined with the efficient
bigram-based inference approach using Lagrange multipliers.
Experiments show that these approximation strategies produce results
comparable to a state-of-the-art integer linear programming formulation
for the same joint inference task along with a significant improvement
in runtime.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Sentence compression is a text-to-text generation task
in which an input sentence must be transformed into a shorter
output sentence which accurately reflects the meaning in the input
and also remains grammatically well-formed.
The compression task has received increasing attention in recent years, in
part due to the availability of datasets such as the
Ziff-Davis corpus <cite class="ltx_cite">[<a href="#bib.bib84" title="Statistics-based summarization - step one: sentence compression" class="ltx_ref">24</a>]</cite> and the Edinburgh compression
corpora <cite class="ltx_cite">[<a href="#bib.bib30" title="Models for sentence compression: a comparison across domains, training requirements and evaluation measures" class="ltx_ref">5</a>]</cite>, from which the following example is
drawn.

<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:210.0pt;" width="210.0pt"><span class="ltx_text ltx_font_small">
<span class="ltx_text ltx_font_bold">Original:</span> In 1967 Chapman, who had cultivated a
conventional image with his ubiquitous tweed jacket and pipe, by his own
later admission stunned a party attended by his friends and future Python
colleagues by coming out as a homosexual.</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:210.0pt;" width="210.0pt"><span class="ltx_text ltx_font_bold ltx_font_small">Compressed:<span class="ltx_text ltx_font_medium"> In 1967 Chapman, who had cultivated a
conventional image, stunned a party by coming out as a homosexual.</span></span></td></tr>
</tbody>
</table>
Following an assumption often used in compression systems,
the compressed output in this corpus
is constructed by dropping tokens from the input sentence without any
paraphrasing or reordering.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>This is referred to as
<em class="ltx_emph">extractive compression</em> by <cite class="ltx_cite">Cohn and Lapata (<a href="#bib.bib35" title="Sentence compression beyond word deletion" class="ltx_ref">2008</a>)</cite> &amp;
<cite class="ltx_cite">Galanis and Androutsopoulos (<a href="#bib.bib67" title="An extractive supervised two-stage method for sentence compression" class="ltx_ref">2010</a>)</cite> following the terminology used in
document summarization. </span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">A number of diverse approaches have been proposed for deletion-based
sentence compression, including techniques that assemble the output
text under an n-gram factorization over the input
text <cite class="ltx_cite">[<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">36</a>, <a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">7</a>]</cite>
or an arc factorization over input dependency
parses <cite class="ltx_cite">[<a href="#bib.bib60" title="Dependency tree based sentence compression" class="ltx_ref">16</a>, <a href="#bib.bib67" title="An extractive supervised two-stage method for sentence compression" class="ltx_ref">17</a>, <a href="#bib.bib64" title="Overcoming the lack of parallel data in sentence compression" class="ltx_ref">15</a>]</cite>. Joint methods have also been
proposed that invoke integer linear programming (ILP) formulations to
simultaneously consider multiple structural inference
problems—both over n-grams and input dependencies <cite class="ltx_cite">[<a href="#bib.bib110" title="Summarization with a joint model for sentence extraction and compression" class="ltx_ref">34</a>]</cite>
or n-grams and all possible dependencies <cite class="ltx_cite">[<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">45</a>]</cite>.
However, it is well-established that the utility of
ILP for optimal inference in structured problems
is often outweighed by the worst-case performance of ILP solvers on
large problems without unique integral solutions. Furthermore,
approximate solutions can often be adequate for real-world
generation systems, particularly in the
presence of linguistically-motivated constraints such as those
described by <cite class="ltx_cite">Clarke and Lapata (<a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">2008</a>)</cite>, or domain-specific pruning strategies
such as the use of sentence templates to constrain the output.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this work, we develop approximate inference strategies to
the joint approach of <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite> which trade the optimality
guarantees of exact ILP for faster inference by separately solving
the n-gram and dependency subproblems and using
Lagrange multipliers to enforce consistency between their solutions.
However, while the former
problem can be solved efficiently using the dynamic programming
approach of <cite class="ltx_cite">McDonald (<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">2006</a>)</cite>, there are no efficient
algorithms to recover maximum
weighted non-projective subtrees in a general directed graph.
Maximum spanning tree algorithms, commonly used in non-projective
dependency parsing <cite class="ltx_cite">[<a href="#bib.bib118" title="Non-projective dependency parsing using spanning tree algorithms" class="ltx_ref">35</a>]</cite>, are not easily adaptable
to this task since the maximum-weight subtree is not necessarily a
part of the maximum spanning tree.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We therefore consider methods to recover approximate solutions for the
subproblem of finding the maximum weighted subtree in a graph, common
among which is the use of a linear programming relaxation.
This linear program (LP) appears empirically tight for
compression problems and our experiments indicate that simply using the
non-integral solutions of this LP in Lagrangian relaxation can empirically
lead to reasonable compressions.
In addition, we can recover approximate solutions to this problem by
using the Chu-Liu Edmonds algorithm for recovering
maximum spanning trees <cite class="ltx_cite">[<a href="#bib.bib25" title="On the shortest arborescence of a directed graph" class="ltx_ref">4</a>, <a href="#bib.bib56" title="Optimum branchings" class="ltx_ref">14</a>]</cite>
over the relatively sparse subgraph defined by a solution to the relaxed LP.
Our proposed approximation strategies are evaluated using automated
metrics
in order to address the question: under what conditions should
a real-world sentence compression system implementation consider
exact inference with an ILP or approximate inference?
The contributions of this work include:</p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">An empirically-useful technique for approximating the
maximum-weight subtree in a weighted graph using LP-relaxed inference.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">Multiple approaches to generate good approximate solutions for
joint multi-structure compression, based on Lagrangian
relaxation to enforce equality between the sequential
and syntactic inference subproblems.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">An analysis of the tradeoffs incurred by joint
approaches with regard to runtime
as well as performance under automated measures.</p>
</div></li>
</ul>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Multi-Structure Sentence Compression</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Even though compression is typically formulated as a token deletion task,
it is evident that dropping tokens independently from an input
sentence will likely not result in fluent and meaningful compressive text.
Tokens in well-formed sentences participate in
a number of syntactic and semantic relationships with other tokens,
so one might expect that accounting for heterogenous structural relationships
between tokens will improve the coherence of the output sentence.
Furthermore, much recent work has focused on
the challenge of joint sentence extraction and
compression, also known as
<em class="ltx_emph">compressive summarization</em> <cite class="ltx_cite">[<a href="#bib.bib110" title="Summarization with a joint model for sentence extraction and compression" class="ltx_ref">34</a>, <a href="#bib.bib13" title="Jointly learning to extract and compress" class="ltx_ref">2</a>, <a href="#bib.bib2" title="Fast and robust compressive summarization with dual decomposition and multi-task learning" class="ltx_ref">1</a>, <a href="#bib.bib95" title="Document summarization via guided sentence compression" class="ltx_ref">30</a>, <a href="#bib.bib137" title="Fast joint compression and summarization via graph cuts" class="ltx_ref">41</a>]</cite>,
in which questions of efficiency are paramount due to the larger
problems involved; however, these approaches largely
restrict compression to pruning parse trees, thereby
imposing a dependency on parser performance.
We focus in this work on a sentence-level compression system
to approximate the ILP-based inference of <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>
which does not restrict compressions to follow input parses but
permits the generation of novel dependency relations in output
compressions.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">The rest of this section is organized as follows: §<a href="#S2.SS1" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>
provies an overview of the joint sequential and syntactic objective
for compression from <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite> while §<a href="#S2.SS2" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>
discusses the use of Lagrange multipliers to enforce consistency
between the different structures considered. Following this,
§<a href="#S2.SS3" title="2.3 Bigram subsequences ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> discusses a dynamic program to find maximum
weight bigram subsequences from the input sentence, while
§<a href="#S2.SS4" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a> covers LP relaxation-based approaches for approximating
solutions to the problem of finding a maximum-weight subtree in
a graph of potential output dependencies.
Finally, §<a href="#S2.SS5" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a> discusses the features
and model training approach used in our experimental results which
are presented in §<a href="#S3" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Joint objective</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">We begin with some notation. For an input sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> comprised of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>
tokens including duplicates, we denote the set of tokens in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> by
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="T\triangleq\{t_{i}:1\leq i\leq n\}" display="inline"><mrow><mi>T</mi><mo>≜</mo><mrow><mo>{</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">:</mo><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi></mrow></mrow><mo>}</mo></mrow></mrow></math>. Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m5" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>
represent a compression of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m6" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> and let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m7" class="ltx_Math" alttext="x_{i}\in\{0,1\}" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math> denote an
indicator variable whose value corresponds to
whether token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m8" class="ltx_Math" alttext="t_{i}\in T" display="inline"><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi></mrow></math> is present in the compressed sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m9" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>.
In addition, we define bigram indicator variables
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m10" class="ltx_Math" alttext="y_{ij}\in\{0,1\}" display="inline"><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math> to represent whether
a particular order-preserving bigram<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Although
<cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite> is not restricted to bigrams
or order-preserving n-grams, we limit our discussion to this scenario
as it also fits the assumptions of <cite class="ltx_cite">McDonald (<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">2006</a>)</cite>
and the datasets of <cite class="ltx_cite">Clarke and Lapata (<a href="#bib.bib30" title="Models for sentence compression: a comparison across domains, training requirements and evaluation measures" class="ltx_ref">2006</a>)</cite>.
</span></span></span>
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m11" class="ltx_Math" alttext="\langle t_{i},t_{j}\rangle" display="inline"><mrow><mo>⟨</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>⟩</mo></mrow></math> from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m12" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> is present as a contiguous bigram in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m13" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>
as well as dependency indicator variables <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m14" class="ltx_Math" alttext="z_{ij}\in\{0,1\}" display="inline"><mrow><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math>
corresponding to whether the dependency arc <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m15" class="ltx_Math" alttext="t_{i}\rightarrow t_{j}" display="inline"><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>→</mo><msub><mi>t</mi><mi>j</mi></msub></mrow></math>
is present in the dependency parse of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m16" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math>.
The score for a given compression <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m17" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> can now be defined to factor over
its tokens, n-grams and dependencies as follows.</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m1" class="ltx_Math" alttext="\displaystyle score(C)" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mi>C</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m2" class="ltx_Math" alttext="\displaystyle=\quad\sum_{t_{i}\in T}\hskip{8.0pt}x_{i}\cdot\theta_{\text{tok}}%&#10;(t_{i})" display="inline"><mrow><mo>=</mo><mo mathvariant="italic" separator="true"> </mo><mpadded width="+8.0pt"><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi></mrow></munder></mstyle></mpadded><msub><mi>x</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>θ</mi><mtext>tok</mtext></msub><mrow><mo>(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m2" class="ltx_Math" alttext="\displaystyle\ +\hskip{-10.0pt}\sum_{\substack{t_{i}\in T\cup\{\textsc{start}%&#10;\},\\&#10;t_{j}\in T\cup\{\textsc{end}\}}}\hskip{-10.0pt}y_{ij}\cdot\theta_{\text{bgr}}(%&#10;\langle t_{i},t_{j}\rangle)" display="inline"><mrow><mi mathvariant="normal"> </mi><mo rspace="0pt">+</mo><mrow><mpadded width="-10.0pt"><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mstyle scriptlevel="+1"><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi><mo>∪</mo><mrow><mo>{</mo><mtext mathvariant="normal">start</mtext><mo>}</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>∈</mo><mrow><mi>T</mi><mo>∪</mo><mrow><mo>{</mo><mtext mathvariant="normal">end</mtext><mo>}</mo></mrow></mrow></mrow></mtd></mtr></mtable></mstyle></munder></mstyle></mpadded><mrow><mrow><msub><mi>y</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⋅</mo><msub><mi>θ</mi><mtext>bgr</mtext></msub></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mo>⟨</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>⟩</mo></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m2" class="ltx_Math" alttext="\displaystyle\ +\hskip{-10.0pt}\sum_{\substack{t_{i}\in T\cup\{\textsc{root}\}%&#10;,\\&#10;t_{j}\in T}}\hskip{-10.0pt}z_{ij}\cdot\theta_{\text{dep}}(t_{i}\rightarrow t_{%&#10;j})" display="inline"><mrow><mi mathvariant="normal"> </mi><mo rspace="0pt">+</mo><mpadded width="-10.0pt"><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mstyle scriptlevel="+1"><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi><mo>∪</mo><mrow><mo>{</mo><mtext mathvariant="normal">root</mtext><mo>}</mo></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd><mrow><msub><mi>t</mi><mi>j</mi></msub><mo>∈</mo><mi>T</mi></mrow></mtd></mtr></mtable></mstyle></munder></mstyle></mpadded><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>⋅</mo><msub><mi>θ</mi><mtext>dep</mtext></msub><mrow><mo>(</mo><msub><mi>t</mi><mi>i</mi></msub><mo>→</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m18" class="ltx_Math" alttext="\theta_{\text{tok}}" display="inline"><msub><mi>θ</mi><mtext>tok</mtext></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m19" class="ltx_Math" alttext="\theta_{\text{bgr}}" display="inline"><msub><mi>θ</mi><mtext>bgr</mtext></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m20" class="ltx_Math" alttext="\theta_{\text{dep}}" display="inline"><msub><mi>θ</mi><mtext>dep</mtext></msub></math>
are feature-based scoring functions for tokens,
bigrams and dependencies respectively. Specifically, each
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m21" class="ltx_Math" alttext="\theta_{v}(\cdot)\equiv\mathbf{w}_{v}^{\top}\boldsymbol{\phi}_{v}(\cdot)" display="inline"><mrow><mrow><msub><mi>θ</mi><mi>v</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow><mo>≡</mo><mrow><msubsup><mi>𝐰</mi><mi>v</mi><mo>⊤</mo></msubsup><mo>⁢</mo><msub><mi mathvariant="bold-italic">ϕ</mi><mi>v</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow></mrow></math>
where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m22" class="ltx_Math" alttext="\boldsymbol{\phi}_{v}(\cdot)" display="inline"><mrow><msub><mi mathvariant="bold-italic">ϕ</mi><mi>v</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow></math> is a feature map for a given
variable type <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m23" class="ltx_Math" alttext="v\in\{\text{tok, bgr, dep}\}" display="inline"><mrow><mi>v</mi><mo>∈</mo><mrow><mo>{</mo><mtext>tok, bgr, dep</mtext><mo>}</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m24" class="ltx_Math" alttext="\mathbf{w}_{v}" display="inline"><msub><mi>𝐰</mi><mi>v</mi></msub></math> is
the corresponding vector of learned parameters.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">The inference task involves recovering the highest
scoring compression <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="C^{*}" display="inline"><msup><mi>C</mi><mo>*</mo></msup></math> under a particular set of model parameters
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="\mathbf{w}" display="inline"><mi>𝐰</mi></math>.</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m1" class="ltx_Math" alttext="\displaystyle C^{*}" display="inline"><msup><mi>C</mi><mo>*</mo></msup></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m2" class="ltx_Math" alttext="\displaystyle=\ \operatorname*{arg\,max}_{C}\ \ score(C)" display="inline"><mrow><mi/><mo rspace="7.5pt">=</mo><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mi>C</mi></msub><mo separator="true"> </mo><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mi>C</mi><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m2" class="ltx_Math" alttext="\displaystyle=\ \operatorname*{arg\,max}_{\mathbf{x},\mathbf{y},\mathbf{z}}\ %&#10;\mathbf{x}^{\top}\boldsymbol{\theta}_{\text{tok}}+\mathbf{y}^{\top}\boldsymbol%&#10;{\theta}_{\text{bgr}}+\mathbf{z}^{\top}\boldsymbol{\theta}_{\text{dep}}" display="inline"><mrow><mi/><mo rspace="7.5pt">=</mo><mrow><mrow><mpadded width="+5.0pt"><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mi>𝐱</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi></mrow></msub></mpadded><mo>⁡</mo><mrow><msup><mi>𝐱</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>tok</mtext></msub></mrow></mrow><mo>+</mo><mrow><msup><mi>𝐲</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>bgr</mtext></msub></mrow><mo>+</mo><mrow><msup><mi>𝐳</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>dep</mtext></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">where the incidence vector
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m3" class="ltx_Math" alttext="\mathbf{x}\triangleq\langle x_{i}\rangle_{t_{i}\in T}" display="inline"><mrow><mi>𝐱</mi><mo>≜</mo><msub><mrow><mo>⟨</mo><msub><mi>x</mi><mi>i</mi></msub><mo>⟩</mo></mrow><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>∈</mo><mi>T</mi></mrow></msub></mrow></math> represents an
entire token configuration over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m4" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>, with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m5" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m6" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>
defined analogously to represent configurations of bigrams and dependencies.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m7" class="ltx_Math" alttext="\boldsymbol{\theta}_{v}\triangleq\langle\theta_{v}(\cdot)\rangle" display="inline"><mrow><msub><mi>𝜽</mi><mi>v</mi></msub><mo>≜</mo><mrow><mo>⟨</mo><mrow><msub><mi>θ</mi><mi>v</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow><mo>⟩</mo></mrow></mrow></math>
denotes a corresponding vector of scores for each variable type <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m8" class="ltx_Math" alttext="v" display="inline"><mi>v</mi></math>
under the current model parameters.
In order to recover meaningful compressions by optimizing
(<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), the inference step must ensure:</p>
<ol id="I2" class="ltx_enumerate">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p">The configurations <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i1.p1.m1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>𝐱</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i1.p1.m2" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i1.p1.m3" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math> are
<em class="ltx_emph">consistent</em> with each other, i.e., all configurations cover
the same tokens.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p">The structural configurations <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i2.p1.m1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i2.p1.m2" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math> are
<em class="ltx_emph">non-degenerate</em>, i.e, the bigram configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i2.p1.m3" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math>
represents an acyclic path while the dependency configuration
<math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.i2.p1.m4" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math> forms a tree.</p>
</div></li>
</ol>
<p class="ltx_p">These requirements naturally rule out simple approximate inference
formulations such as search-based approaches for the joint
objective.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>This work follows <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite> in recovering
non-projective trees for inference.
However, recovering projective trees is tractable
when a total ordering of output tokens is assumed.
This will be addressed in future work.</span></span></span>
An ILP-based inference solution is demonstrated in <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>
that makes use of linear constraints over the boolean variables
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m9" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m10" class="ltx_Math" alttext="y_{ij}" display="inline"><msub><mi>y</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m11" class="ltx_Math" alttext="z_{ij}" display="inline"><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> to guarantee consistency, as well as
auxiliary real-valued variables and constraints representing the flow of
commodities <cite class="ltx_cite">[<a href="#bib.bib106" title="Optimal trees" class="ltx_ref">31</a>]</cite> in order to establish structure in
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m12" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m13" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>. In the following section, we propose
an alternative formulation that exploits the modularity of this
joint objective.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Lagrangian relaxation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">Dual decomposition <cite class="ltx_cite">[<a href="#bib.bib87" title="MRF optimization via dual decomposition: message-passing revisited" class="ltx_ref">26</a>]</cite> and Lagrangian relaxation in general
are often used for solving joint inference problems
which are decomposable into individual subproblems linked by
equality
constraints <cite class="ltx_cite">[<a href="#bib.bib88" title="Dual decomposition for parsing with non-projective head automata" class="ltx_ref">27</a>, <a href="#bib.bib141" title="On dual decomposition and linear programming relaxations for natural language processing" class="ltx_ref">44</a>, <a href="#bib.bib142" title="Exact decoding of syntactic translation models through Lagrangian relaxation" class="ltx_ref">43</a>, <a href="#bib.bib52" title="Model-based aligner combination using dual decomposition" class="ltx_ref">13</a>, <a href="#bib.bib115" title="Dual decomposition with many overlapping components" class="ltx_ref">32</a>, <a href="#bib.bib46" title="An exact dual decomposition algorithm for shallow semantic parsing with constraints" class="ltx_ref">11</a>, <a href="#bib.bib2" title="Fast and robust compressive summarization with dual decomposition and multi-task learning" class="ltx_ref">1</a>]</cite>.
This approach permits sub-problems to be solved separately
using problem-specific efficient algorithms, while
consistency over the structures produced is enforced through
Lagrange multipliers via iterative optimization. Exact solutions are
guaranteed when the algorithm converges on a consistent primal solution,
although this convergence itself is not guaranteed and depends on the
tightness of the underlying LP relaxation.
The primary advantage of this technique is the ability to leverage the
underlying structure of the problems in inference rather than relying on a
generic ILP formulation while still often producing exact solutions.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">The multi-structure inference problem described in the previous
section seems in many ways
to be a natural fit to such an approach since output scores factor
over different types of structure that comprise the output
compression.
Even if ILP-based approaches perform reasonably at the scale of
single-sentence compression problems, the exponential worst-case
complexity of general-purpose ILPs will inevitably pose challenges
when scaling up to (a) handle larger inputs,
(b) use higher-order structural
fragments, or (c) incorporate additional models.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">Consider once more the optimization problem characterized by
(<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)
The two structural problems that need to be
solved in this formulation are
the extraction of a maximum-weight acyclic
subsequence of bigrams <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> from the lattice of all
order-preserving bigrams from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>
and the recovery of a maximum-weight
directed subtree <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m3" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>.
Let
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m4" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y})\in\{0,1\}^{n}" display="inline"><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow><mi>n</mi></msup></mrow></math>
denote the incidence vector of tokens
contained in the n-gram sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m5" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m6" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{z})\in\{0,1\}^{n}" display="inline"><mrow><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow><mo>∈</mo><msup><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow><mi>n</mi></msup></mrow></math>
denote the incidence vector of words contained in the dependency tree
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m7" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>.
We can now rewrite the objective in (<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)
while enforcing the constraint that the words contained in the
sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m8" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> are
the same as the words contained in the tree <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m9" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>, i.e.,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m10" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y})=\boldsymbol{\beta}(\mathbf{z})" display="inline"><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></mrow></math>,
by introducing a vector of Lagrange multipliers
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m11" class="ltx_Math" alttext="\boldsymbol{\lambda}\in\mathbb{R}^{n}" display="inline"><mrow><mi>𝝀</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow></math>.
In addition, the token configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m12" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>𝐱</mi></math> can be rewritten in
the form of a weighted combination of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m13" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y})" display="inline"><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m14" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{z})" display="inline"><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></math> to ensure its consistency with
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m15" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m16" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>.
This results in the following Lagrangian:</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle L(\boldsymbol{\lambda},\mathbf{y,z})=" display="inline"><mrow><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝝀</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m2" class="ltx_Math" alttext="\displaystyle\ \mathbf{y}^{\top}\boldsymbol{\theta}_{\text{bgr}}+\mathbf{z}^{%&#10;\top}\boldsymbol{\theta}_{\text{dep}}" display="inline"><mrow><mrow><mi mathvariant="normal"> </mi><mo>⁢</mo><msup><mi>𝐲</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>bgr</mtext></msub></mrow><mo>+</mo><mrow><msup><mi>𝐳</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>dep</mtext></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle+\boldsymbol{\theta}_{\text{tok}}^{\top}\left(\psi\cdot%&#10;\boldsymbol{\alpha}(\mathbf{y})+(1-\psi)\cdot\boldsymbol{\beta}(\mathbf{z})\right)" display="inline"><mrow><mo>+</mo><mrow><msubsup><mi>𝜽</mi><mtext>tok</mtext><mo>⊤</mo></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mrow><mi>ψ</mi><mo>⋅</mo><mi>𝜶</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>ψ</mi></mrow><mo>)</mo></mrow><mo>⋅</mo><mi>𝜷</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m2" class="ltx_Math" alttext="\displaystyle+\boldsymbol{\lambda}^{\top}\left(\boldsymbol{\alpha}(\mathbf{y})%&#10;-\boldsymbol{\beta}(\mathbf{z})\right)" display="inline"><mrow><mo>+</mo><mrow><msup><mi>𝝀</mi><mo>⊤</mo></msup><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">Finding the
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m17" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m18" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math> that maximize
this Lagrangian above yields a dual objective, and the dual problem
corresponding to
the primal objective specified in (<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) is therefore
the minimization of this objective over the Lagrange multipliers
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m19" class="ltx_Math" alttext="\boldsymbol{\lambda}" display="inline"><mi>𝝀</mi></math>.</p>
<table id="Sx1.EGx4" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex6.m2" class="ltx_Math" alttext="\displaystyle\min_{\boldsymbol{\lambda}}\max_{\mathbf{y},\mathbf{z}}L(%&#10;\boldsymbol{\lambda},\mathbf{y,z})" display="inline"><mrow><munder><mo movablelimits="false">min</mo><mi>𝝀</mi></munder><mo>⁡</mo><mrow><munder><mo movablelimits="false">max</mo><mrow><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi></mrow></munder><mo>⁡</mo><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝝀</mi><mo>,</mo><mi>𝐲</mi><mo>,</mo><mi>𝐳</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m2" class="ltx_Math" alttext="\displaystyle\min_{\boldsymbol{\lambda}}\ \max_{\mathbf{y}}\mathbf{y}^{\top}%&#10;\boldsymbol{\theta}_{\text{bgr}}+\left(\boldsymbol{\lambda}+\psi\cdot%&#10;\boldsymbol{\theta}_{\text{tok}}\right)^{\top}\boldsymbol{\alpha}(\mathbf{y})" display="inline"><mrow><mrow><mpadded width="+5.0pt"><munder><mo movablelimits="false">min</mo><mi>𝝀</mi></munder></mpadded><mo>⁡</mo><mrow><munder><mo movablelimits="false">max</mo><mi>𝐲</mi></munder><mo>⁡</mo><mrow><msup><mi>𝐲</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>bgr</mtext></msub></mrow></mrow></mrow><mo>+</mo><mrow><msup><mrow><mo>(</mo><mrow><mi>𝝀</mi><mo>+</mo><mrow><mi>ψ</mi><mo>⋅</mo><msub><mi>𝜽</mi><mtext>tok</mtext></msub></mrow></mrow><mo>)</mo></mrow><mo>⊤</mo></msup><mo>⁢</mo><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m2" class="ltx_Math" alttext="\displaystyle\ \ \ +\ \max_{\mathbf{z}}\mathbf{z}^{\top}\boldsymbol{\theta}_{%&#10;\text{dep}}-\left(\boldsymbol{\lambda}+(\psi-1)\cdot\boldsymbol{\theta}_{\text%&#10;{tok}}\right)^{\top}\boldsymbol{\beta}(\mathbf{z})" display="inline"><mrow><mi mathvariant="normal"> </mi><mo separator="true"> </mo><mrow><mo rspace="7.5pt">+</mo><mrow><munder><mo movablelimits="false">max</mo><mi>𝐳</mi></munder><mo>⁡</mo><mrow><msup><mi>𝐳</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>dep</mtext></msub></mrow></mrow><mo>-</mo><mrow><msup><mrow><mo>(</mo><mrow><mi>𝝀</mi><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mi>ψ</mi><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo>⋅</mo><msub><mi>𝜽</mi><mtext>tok</mtext></msub></mrow></mrow><mo>)</mo></mrow><mo>⊤</mo></msup><mo>⁢</mo><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m2" class="ltx_Math" alttext="\displaystyle\min_{\boldsymbol{\lambda}}\ \max_{\mathbf{y}}f(\mathbf{y},%&#10;\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mpadded width="+5.0pt"><munder><mo movablelimits="false">min</mo><mi>𝝀</mi></munder></mpadded><mo>⁡</mo><mrow><munder><mo movablelimits="false">max</mo><mi>𝐲</mi></munder><mo>⁡</mo><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐲</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E4.m2" class="ltx_Math" alttext="\displaystyle\quad+\max_{\mathbf{z}}g(\mathbf{z},\boldsymbol{\lambda},\psi,%&#10;\boldsymbol{\theta})" display="inline"><mrow><mi mathvariant="normal"> </mi><mo>+</mo><mrow><munder><mo movablelimits="false">max</mo><mi>𝐳</mi></munder><mo>⁡</mo><mrow><mi>g</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐳</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">This can now be solved with the iterative subgradient algorithm illustrated
in Algorithm <a href="#S2.SS2" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
In each iteration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m20" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>, the algorithm solves for
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m21" class="ltx_Math" alttext="\mathbf{y}^{(i)}" display="inline"><msup><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m22" class="ltx_Math" alttext="\mathbf{z}^{(i)}" display="inline"><msup><mi>𝐳</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> under
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m23" class="ltx_Math" alttext="\boldsymbol{\lambda}^{(i)}" display="inline"><msup><mi>𝝀</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>, then
generates <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m24" class="ltx_Math" alttext="\boldsymbol{\lambda}^{(i+1)}" display="inline"><msup><mi>𝝀</mi><mrow><mo>(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup></math>
to penalize inconsistencies between
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m25" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y}^{(i)})" display="inline"><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m26" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{z}^{(i)})" display="inline"><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>𝐳</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></math>.
When
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m27" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y}^{(i)})=\boldsymbol{\beta}(\mathbf{z}^{(i)})" display="inline"><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>𝐳</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow></math>,
the resulting primal solution is exact, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m28" class="ltx_Math" alttext="\mathbf{y}^{(i)}" display="inline"><msup><mi>𝐲</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m29" class="ltx_Math" alttext="\mathbf{z}^{(i)}" display="inline"><msup><mi>𝐳</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> represent the optimal structures under
(<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Otherwise, if the algorithm starts oscillating between a few primal
solutions, the underlying LP must have a non-integral solution in
which case approximation heuristics can be employed.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>
Heuristic approaches <cite class="ltx_cite">[<a href="#bib.bib87" title="MRF optimization via dual decomposition: message-passing revisited" class="ltx_ref">26</a>, <a href="#bib.bib141" title="On dual decomposition and linear programming relaxations for natural language processing" class="ltx_ref">44</a>]</cite>,
tightening <cite class="ltx_cite">[<a href="#bib.bib142" title="Exact decoding of syntactic translation models through Lagrangian relaxation" class="ltx_ref">43</a>]</cite> or branch and bound <cite class="ltx_cite">[<a href="#bib.bib46" title="An exact dual decomposition algorithm for shallow semantic parsing with constraints" class="ltx_ref">11</a>]</cite>
can still be used to retrieve optimal solutions, but we did not
explore these strategies here.</span></span></span>
The application of this Lagrangian relaxation strategy
is contingent upon the existence of algorithms to solve the
maximization subproblems for
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m30" class="ltx_Math" alttext="f(\mathbf{y},\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐲</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m31" class="ltx_Math" alttext="g(\mathbf{z},\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mi>g</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐳</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></math>.
The following sections discuss our approach to these problems.</p>
</div><span class="ltx_ERROR undefined">{algorithm}</span>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">[t]
<span class="ltx_text ltx_caption">Subgradient-based joint inference</span>

<span class="ltx_ERROR undefined">{algorithmic}</span>[1]
<span class="ltx_ERROR undefined">\Statex</span><span class="ltx_text ltx_font_bold">Input:</span> scores <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m1" class="ltx_Math" alttext="\boldsymbol{\theta}" display="inline"><mi>𝜽</mi></math>, ratio <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m2" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math>,
repetition limit <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m3" class="ltx_Math" alttext="l_{\text{max}}" display="inline"><msub><mi>l</mi><mtext>max</mtext></msub></math>, iteration limit <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m4" class="ltx_Math" alttext="i_{\text{max}}" display="inline"><msub><mi>i</mi><mtext>max</mtext></msub></math>,
learning rate schedule <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m5" class="ltx_Math" alttext="\boldsymbol{\eta}" display="inline"><mi>𝜼</mi></math>
<span class="ltx_ERROR undefined">\Statex</span><span class="ltx_text ltx_font_bold">Output</span>: token configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m6" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>𝐱</mi></math>
<span class="ltx_ERROR undefined">\Statex</span><span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m7" class="ltx_Math" alttext="\boldsymbol{\lambda}^{(0)}\leftarrow\langle 0\rangle^{n}" display="inline"><mrow><msup><mi>𝝀</mi><mrow><mo>(</mo><mn>0</mn><mo>)</mo></mrow></msup><mo>←</mo><msup><mrow><mo>⟨</mo><mn>0</mn><mo>⟩</mo></mrow><mi>n</mi></msup></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m8" class="ltx_Math" alttext="M\leftarrow\varnothing,M_{\text{repeats}}\leftarrow\varnothing" display="inline"><mrow><mrow><mi>M</mi><mo>←</mo><mi mathvariant="normal">∅</mi></mrow><mo>,</mo><mrow><msub><mi>M</mi><mtext>repeats</mtext></msub><mo>←</mo><mi mathvariant="normal">∅</mi></mrow></mrow></math>
<span class="ltx_ERROR undefined">\For</span>iteration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m9" class="ltx_Math" alttext="i&lt;i_{\text{max}}" display="inline"><mrow><mi>i</mi><mo>&lt;</mo><msub><mi>i</mi><mtext>max</mtext></msub></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m10" class="ltx_Math" alttext="\hat{\mathbf{y}}\leftarrow\operatorname*{arg\,max}_{\mathbf{y}}f(\mathbf{y},%&#10;\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>←</mo><mrow><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mi>𝐲</mi></msub><mo>⁡</mo><mi>f</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐲</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m11" class="ltx_Math" alttext="\hat{\mathbf{z}}\,\leftarrow\operatorname*{arg\,max}_{\mathbf{z}}\,g(\mathbf{z%&#10;},\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mpadded width="+1.7pt"><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover></mpadded><mo>←</mo><mrow><mrow><mpadded width="+1.7pt"><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mi>𝐳</mi></msub></mpadded><mo>⁡</mo><mi>g</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐳</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\If</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m12" class="ltx_Math" alttext="\boldsymbol{\alpha}(\hat{\mathbf{y}})=\boldsymbol{\beta}(\hat{\mathbf{z}})" display="inline"><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\Return</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m13" class="ltx_Math" alttext="\boldsymbol{\alpha}(\hat{\mathbf{y}})" display="inline"><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow></math>
<span class="ltx_ERROR undefined">\EndIf</span><span class="ltx_ERROR undefined">\If</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m14" class="ltx_Math" alttext="\boldsymbol{\alpha}(\hat{\mathbf{y}})\in M" display="inline"><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>∈</mo><mi>M</mi></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m15" class="ltx_Math" alttext="M_{\text{repeats}}\leftarrow M_{\text{repeats}}\cup\{\boldsymbol{\alpha}(\hat{%&#10;\mathbf{y}})\}" display="inline"><mrow><msub><mi>M</mi><mtext>repeats</mtext></msub><mo>←</mo><mrow><msub><mi>M</mi><mtext>repeats</mtext></msub><mo>∪</mo><mrow><mo>{</mo><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\EndIf</span><span class="ltx_ERROR undefined">\If</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m16" class="ltx_Math" alttext="\boldsymbol{\beta}(\hat{\mathbf{z}})\in M" display="inline"><mrow><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>∈</mo><mi>M</mi></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m17" class="ltx_Math" alttext="M_{\text{repeats}}\leftarrow M_{\text{repeats}}\cup\{\boldsymbol{\beta}(\hat{%&#10;\mathbf{z}})\}" display="inline"><mrow><msub><mi>M</mi><mtext>repeats</mtext></msub><mo>←</mo><mrow><msub><mi>M</mi><mtext>repeats</mtext></msub><mo>∪</mo><mrow><mo>{</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\EndIf</span><span class="ltx_ERROR undefined">\State</span><span class="ltx_text ltx_font_bold">if</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m18" class="ltx_Math" alttext="|M_{\text{repeats}}|\geq l_{\text{max}}" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>M</mi><mtext>repeats</mtext></msub><mo fence="true">|</mo></mrow><mo>≥</mo><msub><mi>l</mi><mtext>max</mtext></msub></mrow></math> <span class="ltx_text ltx_font_bold">then break</span>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m19" class="ltx_Math" alttext="M\leftarrow M\cup\{\boldsymbol{\alpha}(\hat{\mathbf{y}}),\boldsymbol{\beta}(%&#10;\hat{\mathbf{z}})\}" display="inline"><mrow><mi>M</mi><mo>←</mo><mrow><mi>M</mi><mo>∪</mo><mrow><mo>{</mo><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>,</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m20" class="ltx_Math" alttext="\boldsymbol{\lambda}^{(i+1)}\leftarrow\boldsymbol{\lambda}^{(i)}-\eta_{i}\left%&#10;(\boldsymbol{\alpha}(\hat{\mathbf{y}})-\boldsymbol{\beta}(\hat{\mathbf{z}})\right)" display="inline"><mrow><msup><mi>𝝀</mi><mrow><mo>(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow></msup><mo>←</mo><mrow><msup><mi>𝝀</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>-</mo><mrow><msub><mi>η</mi><mi>i</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐲</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\EndFor</span><span class="ltx_ERROR undefined">\Return</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m21" class="ltx_Math" alttext="\operatorname*{arg\,max}_{\mathbf{x\in M_{\text{repeats}}}}score(\mathbf{x})" display="inline"><mrow><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mi>𝐱</mi><mo>∈</mo><msub><mi>𝐌</mi><mtext>repeats</mtext></msub></mrow></msub><mo>⁡</mo><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi></mrow></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>𝐱</mi><mo>)</mo></mrow></mrow></math></p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Bigram subsequences</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">McDonald (<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">2006</a>)</cite> provides a Viterbi-like
dynamic programming algorithm to recover
the highest-scoring sequence of order-preserving bigrams from a
lattice, either in unconstrained form or with a specific
length constraint.
The latter requires a
dynamic programming table <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m1" class="ltx_Math" alttext="Q[i][r]" display="inline"><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mi>r</mi><mo>]</mo></mrow></mrow></math> which represents the best score
for a compression of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m2" class="ltx_Math" alttext="r" display="inline"><mi>r</mi></math> ending at token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>. The table can
be populated using the following recurrence:</p>
<table id="Sx1.EGx5" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m1" class="ltx_Math" alttext="\displaystyle Q[i][1]" display="inline"><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mn>1</mn><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m2" class="ltx_Math" alttext="\displaystyle=score(S,\textsc{start},i)" display="inline"><mrow><mi/><mo>=</mo><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>S</mi><mo>,</mo><mtext mathvariant="normal">start</mtext><mo>,</mo><mi>i</mi></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex11.m1" class="ltx_Math" alttext="\displaystyle Q[i][r]" display="inline"><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mi>r</mi><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex11.m2" class="ltx_Math" alttext="\displaystyle=\max_{j&lt;i}Q[j][r-1]+score(S,i,j)" display="inline"><mrow><mi/><mo>=</mo><mrow><mrow><munder><mo movablelimits="false">max</mo><mrow><mi>j</mi><mo>&lt;</mo><mi>i</mi></mrow></munder><mo>⁡</mo><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>j</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>S</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex12.m1" class="ltx_Math" alttext="\displaystyle Q[i][R+1]" display="inline"><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mrow><mi>R</mi><mo>+</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex12.m2" class="ltx_Math" alttext="\displaystyle=Q[i][R]+score(S,i,\textsc{end})" display="inline"><mrow><mi/><mo>=</mo><mrow><mrow><mi>Q</mi><mo>⁢</mo><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>⁢</mo><mrow><mo>[</mo><mi>R</mi><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>S</mi><mo>,</mo><mi>i</mi><mo>,</mo><mtext mathvariant="normal">end</mtext></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m4" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> is the required number of output tokens and the scoring function is
defined as</p>
<table id="Sx1.EGx6" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex13.m1" class="ltx_Math" alttext="\displaystyle score(S,i,j)" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>S</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex13.m2" class="ltx_Math" alttext="\displaystyle\triangleq\theta_{\text{bgr}}(\langle t_{i},t_{j}\rangle)+\lambda%&#10;_{j}+\psi\cdot\theta_{\text{tok}}(t_{j})" display="inline"><mrow><mi/><mo>≜</mo><mrow><mrow><msub><mi>θ</mi><mtext>bgr</mtext></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mo>⟨</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>⟩</mo></mrow><mo>)</mo></mrow></mrow><mo>+</mo><msub><mi>λ</mi><mi>j</mi></msub><mo>+</mo><mrow><mrow><mi>ψ</mi><mo>⋅</mo><msub><mi>θ</mi><mtext>tok</mtext></msub></mrow><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">so as to solve
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m5" class="ltx_Math" alttext="f(\mathbf{y},\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐲</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></math>
from (<a href="#S2.E4" title="(4) ‣ 2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). This approach requires <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m6" class="ltx_Math" alttext="O(n^{2}R)" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>n</mi><mn>2</mn></msup><mo>⁢</mo><mi>R</mi></mrow><mo>)</mo></mrow></mrow></math> time in
order to identify the highest scoring sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m7" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> and
corresponding token configuration
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m8" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y})" display="inline"><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow></math>.</p>
</div>
</div>
<div id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.4 </span>Dependency subtrees</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">The maximum-weight non-projective subtree problem over general graphs is not
as easily solved.
Although the maximum <em class="ltx_emph">spanning</em> tree for a given token configuration
can be recovered efficiently, Figure <a href="#S2.F1" title="Figure 1 ‣ 2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates that
the maximum-scoring subtree is not necessarily found within it.
The problem of recovering a maximum-weight subtree in a graph has been
shown to be NP-hard even with uniform edge weights <cite class="ltx_cite">[<a href="#bib.bib94" title="Finding a length-constrained maximum-sum or maximum-density subtree and its application to logistics" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.F1" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" height="94" version="1.1" viewBox="-42 -42 143 94" width="143"><g transform="matrix(1 0 0 -1 0 10)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g><path d="M -13 0 C -13 8 -20 15 -28 15 C -36 15 -42 8 -42 0 C -42 -8 -36 -15 -28 -15 C -20 -15 -13 -8 -13 0 Z M -28 0" style="fill:none"/><g><g transform="matrix(1 0 0 1 -33 -3)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">A</p></foreignObject></switch></g></g></g></g><g><path d="M 70 28 C 70 36 63 42 55 42 C 47 42 41 36 41 28 C 41 20 47 13 55 13 C 63 13 70 20 70 28 Z M 55 28" style="fill:none"/><g><g transform="matrix(1 0 0 1 50 24)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">B</p></foreignObject></switch></g></g></g></g><g><path d="M 42 -28 C 42 -20 36 -13 28 -13 C 20 -13 13 -20 13 -28 C 13 -36 20 -42 28 -42 C 36 -42 42 -36 42 -28 Z M 28 -28" style="fill:none"/><g><g transform="matrix(1 0 0 1 22 -31)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">C</p></foreignObject></switch></g></g></g></g><g><path d="M 97 -28 C 97 -20 91 -13 83 -13 C 75 -13 68 -20 68 -28 C 68 -36 75 -42 83 -42 C 91 -42 97 -36 97 -28 Z M 83 -28" style="fill:none"/><g><g transform="matrix(1 0 0 1 77 -31)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">D</p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.8pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M -18 11 C -2 29 17 35 40 31" style="fill:none"/><g><g transform="matrix(0.980581 -0.200972 0.200972 0.980581 40 31)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 -7 37)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="31">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">-20</p></foreignObject></switch></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M -19 -12 C -12 -24 -1 -29 12 -29" style="fill:none"/><g><g transform="matrix(0.998752 0.059857 -0.059857 0.998752 12 -29)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 -11 -17)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">3</p></foreignObject></switch></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 43 19 C 31 12 26 1 27 -12" style="fill:none"/><g><g transform="matrix(0.059925 -0.9982 0.9982 0.059925 27 -12)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 4 2)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="21">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">10</p></foreignObject></switch></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 67 19 C 79 12 84 1 84 -12" style="fill:none"/><g><g transform="matrix(-0.059889 -0.99875 0.99875 -0.059889 84 -12)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 85 2)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">2</p></foreignObject></switch></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M 40 -19 C 52 -12 57 -1 56 12" style="fill:none"/><g><g transform="matrix(-0.059874 0.99875 -0.99875 -0.059874 56 12)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 58 -9)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p">1</p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g></svg>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of the difficulty of recovering
the maximum-weight subtree
(B<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m6" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>C, B<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m7" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>D) from the maximum spanning
tree (A<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m8" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>C, C<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m9" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>B, B<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m10" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>D).</div>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p">In order to produce a solution to this
subproblem, we use an LP relaxation of the relevant portion of
the ILP from <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite> by omitting integer constraints over
the token and dependency variables in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>𝐱</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m2" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>
respectively. For simplicity, however, we describe the ILP version
rather than the relaxed LP in order to motivate the constraints with their
intended purpose rather than their effect in the relaxed problem.
The objective for this LP is given by</p>
<table id="Sx1.EGx7" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E5.m2" class="ltx_Math" alttext="\displaystyle\max_{\mathbf{x},\mathbf{z}}\ \mathbf{x}^{\top}\boldsymbol{\theta%&#10;^{\prime}}_{\text{tok}}+\mathbf{z}^{\top}\boldsymbol{\theta}_{\text{dep}}" display="inline"><mrow><mrow><mpadded width="+5.0pt"><munder><mo movablelimits="false">max</mo><mrow><mi>𝐱</mi><mo>,</mo><mi>𝐳</mi></mrow></munder></mpadded><mo>⁡</mo><mrow><msup><mi>𝐱</mi><mo>⊤</mo></msup><mo>⁢</mo><mmultiscripts><mi>𝜽</mi><none/><mo mathvariant="bold">′</mo><mtext>tok</mtext><none/></mmultiscripts></mrow></mrow><mo>+</mo><mrow><msup><mi>𝐳</mi><mo>⊤</mo></msup><mo>⁢</mo><msub><mi>𝜽</mi><mtext>dep</mtext></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
<p class="ltx_p">where the vector of token scores is redefined as</p>
<table id="Sx1.EGx8" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E6.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{\theta^{\prime}}_{\text{tok}}\triangleq(1-\psi)\cdot%&#10;\boldsymbol{\theta}_{\text{tok}}-\boldsymbol{\lambda}" display="inline"><mrow><mmultiscripts><mi>𝜽</mi><none/><mo mathvariant="bold">′</mo><mtext>tok</mtext><none/></mmultiscripts><mo>≜</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>ψ</mi></mrow><mo>)</mo></mrow><mo>⋅</mo><msub><mi>𝜽</mi><mtext>tok</mtext></msub></mrow><mo>-</mo><mi>𝝀</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
<p class="ltx_p">in order to solve
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m3" class="ltx_Math" alttext="g(\mathbf{z},\boldsymbol{\lambda},\psi,\boldsymbol{\theta})" display="inline"><mrow><mi>g</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝐳</mi><mo>,</mo><mi>𝝀</mi><mo>,</mo><mi>ψ</mi><mo>,</mo><mi>𝜽</mi></mrow><mo>)</mo></mrow></mrow></math>
from (<a href="#S2.E4" title="(4) ‣ 2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p class="ltx_p">Linear constraints are introduced to
produce dependency structures that are close to the optimal dependency trees.
First, tokens in the solution must only be active if they
have a single active incoming dependency edge.
In addition, to avoid producing multiple disconnected subtrees,
only one dependency is permitted to attach to the <span class="ltx_text ltx_font_smallcaps">root</span>
pseudo-token.</p>
<table id="Sx1.EGx9" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m1" class="ltx_Math" alttext="\displaystyle x_{j}-\sum_{i}z_{ij}" display="inline"><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>-</mo><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder></mstyle><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m2" class="ltx_Math" alttext="\displaystyle=0,\quad\forall t_{j}\in T" display="inline"><mrow><mo>=</mo><mn>0</mn><mo rspace="12.5pt">,</mo><mo>∀</mo><msub><mi>t</mi><mi>j</mi></msub><mo>∈</mo><mi>T</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
<tr id="S2.E8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E8.m1" class="ltx_Math" alttext="\displaystyle\sum_{j}z_{ij}" display="inline"><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder></mstyle><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E8.m2" class="ltx_Math" alttext="\displaystyle=1,\quad\text{if}\ t_{i}=\textsc{root}" display="inline"><mrow><mo>=</mo><mn>1</mn><mo rspace="12.5pt">,</mo><mpadded width="+5.0pt"><mtext>if</mtext></mpadded><msub><mi>t</mi><mi>i</mi></msub><mo>=</mo><mtext mathvariant="normal">root</mtext></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
</table>
<p class="ltx_p">In order to avoid cycles in the dependency tree,
we include additional variables to establish
<em class="ltx_emph">single-commodity flow</em> <cite class="ltx_cite">[<a href="#bib.bib106" title="Optimal trees" class="ltx_ref">31</a>]</cite>
between all pairs of tokens. These <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m1" class="ltx_Math" alttext="\gamma_{ij}" display="inline"><msub><mi>γ</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> variables
carry non-negative real values which must be consumed by active
tokens that they are incident to.</p>
<table id="Sx1.EGx10" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E9.m1" class="ltx_Math" alttext="\displaystyle\gamma_{ij}\geq 0," display="inline"><mrow><mrow><msub><mi>γ</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>≥</mo><mn>0</mn></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E9.m2" class="ltx_Math" alttext="\displaystyle\forall t_{i},t_{j}\in T" display="inline"><mrow><mrow><mrow><mo>∀</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>∈</mo><mi>T</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
<tr id="S2.E10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E10.m1" class="ltx_Math" alttext="\displaystyle\sum_{i}\gamma_{ij}-\sum_{k}\gamma_{jk}=x_{j}," display="inline"><mrow><mrow><mrow><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder></mstyle><msub><mi>γ</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow><mo>-</mo><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>k</mi></munder></mstyle><msub><mi>γ</mi><mrow><mi>j</mi><mo>⁢</mo><mi>k</mi></mrow></msub></mrow></mrow><mo>=</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E10.m2" class="ltx_Math" alttext="\displaystyle\forall t_{j}\in T" display="inline"><mrow><mrow><mo>∀</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>∈</mo><mi>T</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
</table>
<p class="ltx_p">These constraints ensure that cyclic structures are not possible
in the non-relaxed ILP. In addition, they serve to establish
connectivity for the dependency structure <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m2" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math> since
commodity can only originate in one location—at the pseudo-token
<span class="ltx_text ltx_font_smallcaps">root</span> which has no incoming commodity variables.
However, in order to enforce these properties on the output
dependency structure,
this acyclic, connected commodity structure must constrain the activation
of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m3" class="ltx_Math" alttext="z" display="inline"><mi>z</mi></math> variables.</p>
<table id="Sx1.EGx11" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m1" class="ltx_Math" alttext="\displaystyle\gamma_{ij}-C_{\text{max}}z_{ij}" display="inline"><mrow><msub><mi>γ</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>-</mo><mrow><msub><mi>C</mi><mtext>max</mtext></msub><mo>⁢</mo><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m2" class="ltx_Math" alttext="\displaystyle\leq 0,\quad\forall t_{i},t_{j}\in T" display="inline"><mrow><mo>≤</mo><mn>0</mn><mo rspace="12.5pt">,</mo><mo>∀</mo><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub><mo>∈</mo><mi>T</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m4" class="ltx_Math" alttext="C_{\text{max}}" display="inline"><msub><mi>C</mi><mtext>max</mtext></msub></math> is an arbitrary upper bound on the value of
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m5" class="ltx_Math" alttext="\gamma_{ij}" display="inline"><msub><mi>γ</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> variables.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how these commodity flow variables
constrain the output of the ILP to be a tree. However, the effect of
these constraints is diminished when solving an LP relaxation of the above
problem.</p>
</div>
<div id="S2.F2" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" height="90" version="1.1" viewBox="-51 -12 328 90" width="328"><g transform="matrix(1 0 0 -1 0 66)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g><g><g transform="matrix(1 0 0 1 67 64)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 9)"><switch><foreignObject color="#000000" height="11" overflow="visible" width="37">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_smallcaps ltx_font_small">root</span></p></foreignObject></switch></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 -47 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="93">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">Production</span></p></foreignObject></switch></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 35 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="28">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">was</span></p></foreignObject></switch></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 57 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="56">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">closed</span></p></foreignObject></switch></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 107 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="37">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">down</span></p></foreignObject></switch></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 146 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">at</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 165 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="37">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">Ford</span></p></foreignObject></switch></g></g></g></g></g></g><g><g><g transform="matrix(1 0 0 1 197 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="37">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">last</span></p></foreignObject></switch></g></g></g></g><g><g><g transform="matrix(1 0 0 1 221 -5)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="47">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">night</span></p></foreignObject></switch></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 262 -4)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="9">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_small">.</span></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke="#575757"><g fill="#575757"><g stroke-opacity="1"><g fill-opacity="1"><g stroke="#000000"><path d="M 85 57 L 85 12" style="fill:none"/><g><g fill="#000000"><g transform="matrix(1.5e-05 -1 1 1.5e-05 85 12)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 72 40)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.pic1.m1" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke="#575757"><g fill="#575757"><g stroke-opacity="1"><g fill-opacity="1"><g stroke="#000000"><path d="M 1 11 C 29 28 56 28 85 11" style="fill:none"/><g><g fill="#000000"><g transform="matrix(-0.868777 -0.501572 0.501572 -0.868777 1 11)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 -2 31)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="56">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.pic1.m2" class="ltx_Math" alttext="\gamma_{3,1}=1" display="inline"><mrow><msub><mi>γ</mi><mrow><mn>3</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow></math></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke="#575757"><g fill="#575757"><g stroke-opacity="1"><g fill-opacity="1"><g stroke="#000000"><path d="M 85 11 C 109 25 131 25 154 11" style="fill:none"/><g><g fill="#000000"><g transform="matrix(0.868777 -0.50159 0.50159 0.868777 154 11)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 125 26)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><span class="ltx_text ltx_font_footnote"> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.pic1.m3" class="ltx_Math" alttext="2" display="inline"><mn mathsize="normal" stretchy="false">2</mn></math></span></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke="#575757"><g fill="#575757"><g stroke-opacity="1"><g fill-opacity="1"><g stroke="#000000"><path d="M 155 11 C 165 17 174 17 183 11" style="fill:none"/><g><g fill="#000000"><g transform="matrix(0.868777 -0.501571 0.501571 0.868777 183 11)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 164 21)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.pic1.m4" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g stroke="#575757"><g fill="#575757"><g stroke-opacity="1"><g fill-opacity="1"><g stroke="#000000"><path d="M 85 11 C 147 46 206 46 266 11" style="fill:none"/><g><g fill="#000000"><g transform="matrix(0.868777 -0.501583 0.501583 0.868777 266 11)"><g><g stroke-width="0.64pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -2 3 C -2 2 0 0 1 0 C 0 0 -2 -2 -2 -3" style="fill:none"/></g></g></g></g></g></g></g></g><g><g stroke="#000000"><g fill="#000000"><g><g stroke="#000000"><g fill="#000000"/></g></g><g><g transform="matrix(1 0 0 1 130 46)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="56">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F2.pic1.m5" class="ltx_Math" alttext="\gamma_{3,9}=1" display="inline"><mrow><msub><mi>γ</mi><mrow><mn>3</mn><mo>,</mo><mn>9</mn></mrow></msub><mo>=</mo><mn>1</mn></mrow></math></p></foreignObject></switch></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></g></svg>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustration of commodity values for a valid solution of
the non-relaxed ILP.</div>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p class="ltx_p">In the LP relaxation, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p4.m1" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p4.m2" class="ltx_Math" alttext="z_{ij}" display="inline"><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>
are redefined as real-valued variables in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p4.m3" class="ltx_Math" alttext="[0,1]" display="inline"><mrow><mo>[</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>]</mo></mrow></math>, potentially resulting
in fractional values for dependency and token indicators. As a result,
the commodity flow network is able to establish connectivity but
cannot enforce a tree structure, for instance,
directed acyclic structures are possible and token indicators <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p4.m4" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>
may be partially be assigned to the solution structure. This poses
a challenge in implementing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p4.m5" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{z})" display="inline"><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></math> which
is needed to recover a token configuration from the solution of this
subproblem.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p class="ltx_p">We propose two alternative solutions to address this issue in the
context of the joint inference strategy. The first is to simply
use the relaxed token configuration identified by the LP in
Algorithm <a href="#S2.SS2" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, i.e., to set
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m1" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{\tilde{z}})=\tilde{x}" display="inline"><mrow><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover><mo>)</mo></mrow></mrow><mo>=</mo><mover accent="true"><mi>x</mi><mo stretchy="false">~</mo></mover></mrow></math>
where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m2" class="ltx_Math" alttext="\mathbf{\tilde{x}}" display="inline"><mover accent="true"><mi>𝐱</mi><mo stretchy="false">~</mo></mover></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m3" class="ltx_Math" alttext="\mathbf{\tilde{z}}" display="inline"><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover></math> represent
the real-valued counterparts of the incidence vectors
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m4" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>𝐱</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m5" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math>. The viability of this approximation strategy
is due to the following:</p>
<ul id="I3" class="ltx_itemize">
<li id="I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p">The relaxed LP is empirically fairly tight, yielding integral solutions
89% of the time on the compression datasets described
in §<a href="#S3" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div></li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p">The bigram subproblem is guaranteed to return a well-formed integral
solution which obeys the imposed compression rate, so we are
assured of a source of valid—if non-optimal—solutions in
line 13 of Algorithm <a href="#S2.SS2" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div></li>
</ul>
<p class="ltx_p">We also consider another strategy that attempts to approximate a valid
integral solution to the dependency subproblem. In order to do this,
we first include an additional constraint in the relaxed LP which
restrict the number of tokens in the
output to a specific number of tokens <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m6" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> that is given by an input
compression rate.</p>
<table id="Sx1.EGx12" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E12.m1" class="ltx_Math" alttext="\displaystyle\sum_{i}x_{i}=R" display="inline"><mrow><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder></mstyle><msub><mi>x</mi><mi>i</mi></msub></mrow><mo>=</mo><mi>R</mi></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
</table>
<p class="ltx_p">The addition of this constraint to the relaxed LP reduces the rate of
integral solutions drastically—from 89% to approximately 33%—but
it serves to ensure that the resulting token configuration
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m7" class="ltx_Math" alttext="\mathbf{\tilde{x}}" display="inline"><mover accent="true"><mi>𝐱</mi><mo stretchy="false">~</mo></mover></math> has at least as many non-zero elements as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m8" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math>, i.e.,
there are at least as many tokens activated in the LP
solution as are required in a valid solution.</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<p class="ltx_p">We then construct a subgraph <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m1" class="ltx_Math" alttext="G(\mathbf{\tilde{z}})" display="inline"><mrow><mi>G</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover><mo>)</mo></mrow></mrow></math> consisting of all
dependency edges that were assigned non-zero values in the solution,
assigning to each edge a score equal to the score of that edge in the
LP as well as the score of its dependent word,
i.e., each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m2" class="ltx_Math" alttext="z_{ij}" display="inline"><msub><mi>z</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m3" class="ltx_Math" alttext="G(\mathbf{\tilde{z}})" display="inline"><mrow><mi>G</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover><mo>)</mo></mrow></mrow></math> is assigned a score
of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m4" class="ltx_Math" alttext="\theta_{\text{dep}}(\langle t_{i},t_{j}\rangle)-\lambda_{j}+(1-\psi)\cdot%&#10;\theta_{\text{tok}}(t_{j})" display="inline"><mrow><mrow><msub><mi>θ</mi><mtext>dep</mtext></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mo>⟨</mo><mrow><msub><mi>t</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>⟩</mo></mrow><mo>)</mo></mrow></mrow><mo>-</mo><msub><mi>λ</mi><mi>j</mi></msub><mo>+</mo><mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>ψ</mi></mrow><mo>)</mo></mrow><mo>⋅</mo><msub><mi>θ</mi><mtext>tok</mtext></msub></mrow><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mrow></math>.
Since the
commodity flow constraints in
(<a href="#S2.E9" title="(9) ‣ 2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>)–(<a href="#S2.E11" title="(11) ‣ 2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>)
ensure a connected <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m5" class="ltx_Math" alttext="\mathbf{\tilde{z}}" display="inline"><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover></math>, it is therefore possible to
recover a maximum-weight spanning tree from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m6" class="ltx_Math" alttext="G(\mathbf{\tilde{z}})" display="inline"><mrow><mi>G</mi><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover><mo>)</mo></mrow></mrow></math>
using the Chu-Liu Edmonds algorithm <cite class="ltx_cite">[<a href="#bib.bib25" title="On the shortest arborescence of a directed graph" class="ltx_ref">4</a>, <a href="#bib.bib56" title="Optimum branchings" class="ltx_ref">14</a>]</cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>
A detailed description of the Chu-Liu Edmonds algorithm for
MSTs is available in <cite class="ltx_cite">McDonald<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib118" title="Non-projective dependency parsing using spanning tree algorithms" class="ltx_ref">2005</a>)</cite>.</span></span></span>
Although the runtime of this algorithm is cubic in the size of
the input graph, it is fairly speedy when
applied on relatively sparse graphs such as the solutions to the LP
described above.
The resulting spanning tree is a useful integral
approximation of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m7" class="ltx_Math" alttext="\mathbf{\tilde{z}}" display="inline"><mover accent="true"><mi>𝐳</mi><mo stretchy="false">~</mo></mover></math> but, as mentioned previously,
may contain more nodes than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m8" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> due to fractional values in
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m9" class="ltx_Math" alttext="\mathbf{\tilde{x}}" display="inline"><mover accent="true"><mi>𝐱</mi><mo stretchy="false">~</mo></mover></math>; we therefore repeatedly prune leaves with the lowest
incoming edge weight in the current tree until exactly <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p6.m10" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math> nodes remain.
The resulting tree is assumed to be a reasonable approximation of the
optimal integral solution to this LP.</p>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<p class="ltx_p">The Chu-Liu Edmonds algorithm is also employed
for another purpose:
when the underlying LP for the joint inference problem is not
tight—a frequent occurrence in our compression
experiments—Algorithm <a href="#S2.SS2" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>
will not converge on a single primal solution and will instead oscillate
between solutions that are close to the dual optimum. We identify this
phenomenon by counting repeated solutions and, if they exceed some
threshold <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p7.m1" class="ltx_Math" alttext="l_{\text{max}}" display="inline"><msub><mi>l</mi><mtext>max</mtext></msub></math> with at least one repeated solution from either
subproblem, we terminate the update procedure for Lagrange multipliers
and instead attempt to identify a good solution from the repeating
ones by scoring them under (<a href="#S2.E2" title="(2) ‣ 2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
It is straightforward to recover and score a bigram configuration
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p7.m2" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>𝐲</mi></math> from a token configuration <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p7.m3" class="ltx_Math" alttext="\boldsymbol{\beta}(\mathbf{z})" display="inline"><mrow><mi>𝜷</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐳</mi><mo>)</mo></mrow></mrow></math>.
However, scoring
solutions produced by the dynamic program from §<a href="#S2.SS3" title="2.3 Bigram subsequences ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> also
requires the score over a corresponding parse tree;
this can be recovered by constructing a dependency subgraph containing
across only the tokens that are active in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p7.m4" class="ltx_Math" alttext="\boldsymbol{\alpha}(\mathbf{y})" display="inline"><mrow><mi>𝜶</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝐲</mi><mo>)</mo></mrow></mrow></math>
and retrieving the maximum spanning tree for that subgraph using
the Chu-Liu Edmonds algorithm.</p>
</div>
</div>
<div id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.5 </span>Learning and Features</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p class="ltx_p">The features used in this work are largely based on the features from
<cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>.</p>
<ul id="I4" class="ltx_itemize">
<li id="I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I4.i1.p1" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I4.i1.p1.m1" class="ltx_Math" alttext="\boldsymbol{\phi}_{\text{tok}}" display="inline"><msub><mi mathvariant="bold-italic">ϕ</mi><mtext>tok</mtext></msub></math> contains features for
part-of-speech (POS) tag sequences of length up to 3 around the
token, features for the dependency label of the token
conjoined with its POS, lexical features for verb stems and non-word
symbols and morphological features that identify capitalized
sequences, negations and words in parentheses.</p>
</div></li>
<li id="I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I4.i2.p1" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I4.i2.p1.m1" class="ltx_Math" alttext="\boldsymbol{\phi}_{\text{bgr}}" display="inline"><msub><mi mathvariant="bold-italic">ϕ</mi><mtext>bgr</mtext></msub></math> contains features for
POS patterns in a bigram, the labels of dependency
edges incident to it, its likelihood under a
Gigaword language model (LM) and an indicator for whether it
is present in the input sentence.</p>
</div></li>
<li id="I4.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I4.i3.p1" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="I4.i3.p1.m1" class="ltx_Math" alttext="\boldsymbol{\phi}_{\text{dep}}" display="inline"><msub><mi mathvariant="bold-italic">ϕ</mi><mtext>dep</mtext></msub></math> contains features for the
probability of a dependency edge under a smoothed dependency grammar
constructed from the Penn Treebank and various conjunctions of
the following features: (a) whether the edge appears as a dependency
or ancestral relation in the input parse (b)
the directionality of the dependency (c) the label of the edge
(d) the POS tags of the tokens incident to the edge and (e) the
labels of their surrounding chunks and whether the edge remains
within the chunk.</p>
</div></li>
</ul>
<p class="ltx_p">For the experiments in the following section, we trained models using
a variant of the structured perceptron <cite class="ltx_cite">[<a href="#bib.bib38" title="Discriminative training methods for hidden Markov models" class="ltx_ref">10</a>]</cite>
which incorporates minibatches <cite class="ltx_cite">[<a href="#bib.bib173" title="Minibatch and parallelization for online large margin structured learning" class="ltx_ref">50</a>]</cite> for easy parallelization and
faster convergence.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>
We used a minibatch size of 4 in all experiments.</span></span></span>
Overfitting was avoided by averaging parameters and monitoring performance
against a held-out development set during training.
All models were trained using variants of the ILP-based inference
approach of <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>.
We followed <cite class="ltx_cite">Martins<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib112" title="Concise integer linear programming formulations for dependency parsing" class="ltx_ref">2009</a>)</cite> in
using LP-relaxed inference during learning,
assuming algorithmic separability <cite class="ltx_cite">[<a href="#bib.bib91" title="Structured learning with approximate inference." class="ltx_ref">28</a>]</cite>
for these problems.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We ran compression experiments over the newswire (NW) and
broadcast news transcription (BN) corpora compiled by <cite class="ltx_cite">Clarke and Lapata (<a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">2008</a>)</cite>
which contain gold compressions
produced by human annotators using only word deletion.
The datasets were filtered to eliminate instances with less than 2 and more
than 110 tokens for parser compatibility and divided into
training/development/test sections following the splits
from <cite class="ltx_cite">Clarke and Lapata (<a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">2008</a>)</cite>, yielding 953/63/603 instances
for the NW corpus and 880/78/404 for the BN corpus.
Gold dependency parses were approximated
by running the Stanford dependency
parser<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_text ltx_font_typewriter">http://nlp.stanford.edu/software/</span></span></span></span>
over reference compressions.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Following evaluations in machine translation as well as previous work in
sentence
compression <cite class="ltx_cite">[<a href="#bib.bib159" title="Trimming CFG parse trees for sentence compression using machine learning approaches" class="ltx_ref">47</a>, <a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">7</a>, <a href="#bib.bib110" title="Summarization with a joint model for sentence extraction and compression" class="ltx_ref">34</a>, <a href="#bib.bib126" title="Evaluating sentence compression: pitfalls and suggested remedies" class="ltx_ref">39</a>, <a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">45</a>]</cite>,
we evaluate system performance
using F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math> metrics over n-grams and dependency edges produced by parsing
system output with RASP <cite class="ltx_cite">[<a href="#bib.bib18" title="The second release of the RASP system" class="ltx_ref">3</a>]</cite> and the Stanford parser.
All ILPs and LPs were solved using
Gurobi,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_text ltx_font_typewriter">http://www.gurobi.com</span></span></span></span> a high-performance
commercial-grade solver. Following a recent analysis of compression
evaluations <cite class="ltx_cite">[<a href="#bib.bib126" title="Evaluating sentence compression: pitfalls and suggested remedies" class="ltx_ref">39</a>]</cite> which revealed a strong correlation between
system compression rate and human judgments of compression quality,
we constrained all systems to produce compressed
output at a specific rate—determined by the the gold
compressions available for each instance—to ensure that the
reported differences between the systems under study are meaningful.</p>
</div>
<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Inference</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">n-grams  F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m1" class="ltx_Math" alttext="{}_{1}\%" display="inline"><mmultiscripts><mi mathvariant="normal">%</mi><mprescripts/><mn>1</mn><none/></mmultiscripts></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Syntactic relations F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m2" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math>%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Inference</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l" style="width:40.0pt;" width="40.0pt">objective</th>
<th class="ltx_td ltx_align_left ltx_border_r"> technique</th>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m3" class="ltx_Math" alttext="n=1" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m4" class="ltx_Math" alttext="2" display="inline"><mn>2</mn></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m5" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m6" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m7" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math></td>
<td class="ltx_td ltx_align_center">Stanford</td>
<td class="ltx_td ltx_align_center ltx_border_r">RASP</td>
<td class="ltx_td ltx_align_center ltx_border_r">time (s)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_tt" style="width:40.0pt;" rowspan="2" width="40.0pt">n-grams</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">3-LM (CL08)</th>
<td class="ltx_td ltx_align_center ltx_border_tt">74.96</td>
<td class="ltx_td ltx_align_center ltx_border_tt">60.60</td>
<td class="ltx_td ltx_align_center ltx_border_tt">46.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">38.71</td>
<td class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt">60.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">57.49</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.72</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">DP (McD06)</th>
<td class="ltx_td ltx_align_center">78.80</td>
<td class="ltx_td ltx_align_center">66.04</td>
<td class="ltx_td ltx_align_center">52.67</td>
<td class="ltx_td ltx_align_center ltx_border_r">42.39</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">63.28</td>
<td class="ltx_td ltx_align_center ltx_border_r">57.89</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.01</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_t" style="width:40.0pt;" rowspan="2" width="40.0pt">deps</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m8" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>MST</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">79.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">64.32</td>
<td class="ltx_td ltx_align_center ltx_border_t">50.36</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.97</td>
<td class="ltx_td ltx_align_center ltx_border_t">66.57</td>
<td class="ltx_td ltx_align_center ltx_border_t">66.82</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.07</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">ILP-Dep</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">80.02</span></td>
<td class="ltx_td ltx_align_center">65.99</td>
<td class="ltx_td ltx_align_center">52.42</td>
<td class="ltx_td ltx_align_center ltx_border_r">43.07</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">72.43</span></td>
<td class="ltx_td ltx_align_center">67.63</td>
<td class="ltx_td ltx_align_center ltx_border_r">60.78</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.16</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_t" style="width:40.0pt;" width="40.0pt"/>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DP + LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m9" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>MST</th>
<td class="ltx_td ltx_align_center ltx_border_t">79.50</td>
<td class="ltx_td ltx_align_center ltx_border_t">66.75</td>
<td class="ltx_td ltx_align_center ltx_border_t">53.48</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.33</td>
<td class="ltx_td ltx_align_center ltx_border_t">64.63</td>
<td class="ltx_td ltx_align_center ltx_border_t">67.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.24</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l" style="width:40.0pt;" width="40.0pt">joint</th>
<th class="ltx_td ltx_align_left ltx_border_r">DP + LP</th>
<td class="ltx_td ltx_align_center">79.10</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">68.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">55.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">45.81</span></td>
<td class="ltx_td ltx_align_center">65.74</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">68.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">62.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">0.12</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_b ltx_border_l" style="width:40.0pt;" width="40.0pt"/>
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r">ILP-Joint (TM13)</th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">80.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">68.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">55.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">46.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">72.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">68.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">62.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.31</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental results for the BN corpus, averaged over 3
gold compressions per instance.
All systems were restricted to compress to the size of the
median gold compression yielding an average compression rate of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m11" class="ltx_Math" alttext="77.26\%" display="inline"><mrow><mn>77.26</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>.</div>
</div>
<div id="S3.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Inference</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">n-grams  F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m1" class="ltx_Math" alttext="{}_{1}\%" display="inline"><mmultiscripts><mi mathvariant="normal">%</mi><mprescripts/><mn>1</mn><none/></mmultiscripts></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Syntactic relations F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m2" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math>%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Inference</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l" style="width:40.0pt;" width="40.0pt">objective</th>
<th class="ltx_td ltx_align_left ltx_border_r"> technique</th>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m3" class="ltx_Math" alttext="n=1" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m4" class="ltx_Math" alttext="2" display="inline"><mn>2</mn></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m5" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m6" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m7" class="ltx_Math" alttext="\mathbf{z}" display="inline"><mi>𝐳</mi></math></td>
<td class="ltx_td ltx_align_center">Stanford</td>
<td class="ltx_td ltx_align_center ltx_border_r">RASP</td>
<td class="ltx_td ltx_align_center ltx_border_r">time (s)</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_tt" style="width:40.0pt;" rowspan="2" width="40.0pt">n-grams</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">3-LM (CL08)</th>
<td class="ltx_td ltx_align_center ltx_border_tt">66.66</td>
<td class="ltx_td ltx_align_center ltx_border_tt">51.59</td>
<td class="ltx_td ltx_align_center ltx_border_tt">39.33</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">30.55</td>
<td class="ltx_td ltx_align_center ltx_border_tt">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt">50.76</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.22</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">DP (McD06)</th>
<td class="ltx_td ltx_align_center">73.18</td>
<td class="ltx_td ltx_align_center">58.31</td>
<td class="ltx_td ltx_align_center">45.07</td>
<td class="ltx_td ltx_align_center ltx_border_r">34.77</td>
<td class="ltx_td ltx_align_center">-</td>
<td class="ltx_td ltx_align_center">56.23</td>
<td class="ltx_td ltx_align_center ltx_border_r">51.14</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.01</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_t" style="width:40.0pt;" rowspan="2" width="40.0pt">deps</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m8" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>MST</th>
<td class="ltx_td ltx_align_center ltx_border_t">73.32</td>
<td class="ltx_td ltx_align_center ltx_border_t">55.12</td>
<td class="ltx_td ltx_align_center ltx_border_t">41.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">31.44</td>
<td class="ltx_td ltx_align_center ltx_border_t">61.01</td>
<td class="ltx_td ltx_align_center ltx_border_t">58.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.57</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.12</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">ILP-Dep</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">73.76</span></td>
<td class="ltx_td ltx_align_center">57.09</td>
<td class="ltx_td ltx_align_center">43.47</td>
<td class="ltx_td ltx_align_center ltx_border_r">33.44</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">65.45</span></td>
<td class="ltx_td ltx_align_center">60.06</td>
<td class="ltx_td ltx_align_center ltx_border_r">54.31</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.28</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_t" style="width:40.0pt;" width="40.0pt"/>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">DP + LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m9" class="ltx_Math" alttext="\rightarrow" display="inline"><mo>→</mo></math>MST</th>
<td class="ltx_td ltx_align_center ltx_border_t">73.13</td>
<td class="ltx_td ltx_align_center ltx_border_t">57.03</td>
<td class="ltx_td ltx_align_center ltx_border_t">43.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.01</td>
<td class="ltx_td ltx_align_center ltx_border_t">57.91</td>
<td class="ltx_td ltx_align_center ltx_border_t">58.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.33</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l" style="width:40.0pt;" width="40.0pt">joint</th>
<th class="ltx_td ltx_align_left ltx_border_r">DP + LP</th>
<td class="ltx_td ltx_align_center">72.06</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">59.83</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">47.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">37.72</span></td>
<td class="ltx_td ltx_align_center">58.13</td>
<td class="ltx_td ltx_align_center">58.97</td>
<td class="ltx_td ltx_align_center ltx_border_r">53.78</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.21</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_b ltx_border_l" style="width:40.0pt;" width="40.0pt"/>
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r">ILP-Joint (TM13)</th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">74.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">59.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">47.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">37.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">65.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">61.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">56.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.60</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results for the NW corpus with
all systems compressing to the size of the
gold compression, yielding an average compression rate of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m13" class="ltx_Math" alttext="70.24\%" display="inline"><mrow><mn>70.24</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>.
In both tables, bold entries show significant gains within a column
under the paired t-test (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m14" class="ltx_Math" alttext="p&lt;0.05" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow></math>) and Wilcoxon’s signed rank test (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T2.m15" class="ltx_Math" alttext="p&lt;0.01" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.01</mn></mrow></math>).</div>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Systems</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We report results over the following systems grouped into three
categories of models:
tokens + n-grams, tokens + dependencies, and joint models.</p>
<ul id="I5" class="ltx_itemize">
<li id="I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">3-LM:</span> A reimplementation of the unsupervised ILP
of <cite class="ltx_cite">Clarke and Lapata (<a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">2008</a>)</cite> which infers order-preserving trigram variables
parameterized with log-likelihood under an LM and
a significance score for token variables inspired by
<cite class="ltx_cite">Hori and Furui (<a href="#bib.bib74" title="Speech summarization: an approach through word extraction and a method for evaluation" class="ltx_ref">2004</a>)</cite>, as well as various linguistically-motivated
constraints to encourage fluency in output compressions.</p>
</div></li>
<li id="I5.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">DP:</span> The bigram-based dynamic program of
<cite class="ltx_cite">McDonald (<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">2006</a>)</cite> described in §<a href="#S2.SS3" title="2.3 Bigram subsequences ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>For
consistent comparisons with the
other systems, our reimplementation does not include the
<math xmlns="http://www.w3.org/1998/Math/MathML" id="I5.i2.p1.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>-best inference strategy presented in <cite class="ltx_cite">McDonald (<a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">2006</a>)</cite>
for learning with MIRA.</span></span></span></p>
</div></li>
<li id="I5.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="I5.i3.p1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST:</span> An approximate inference approach
based on an LP relaxation of <span class="ltx_text ltx_font_bold">ILP-Dep</span>. As discussed
in §<a href="#S2.SS4" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>, a maximum spanning tree is recovered
from the output of the LP and greedily pruned in order to generate a
valid integral solution while observing the imposed compression rate.</p>
</div></li>
<li id="I5.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ILP-Dep:</span> A version of the joint ILP of <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>
without n-gram variables and corresponding features.</p>
</div></li>
<li id="I5.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">DP+LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="I5.i5.p1.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST:</span> An approximate joint inference approach
based on Lagrangian relaxation that uses <span class="ltx_text ltx_font_bold">DP</span> for the maximum
weight subsequence problem and <span class="ltx_text ltx_font_bold">LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="I5.i5.p1.m2" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST</span> for the
maximum weight subtree problem.</p>
</div></li>
<li id="I5.i6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i6.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">DP+LP:</span> Another Lagrangian relaxation approach that pairs
<span class="ltx_text ltx_font_bold">DP</span>
with the non-integral solutions from an LP relaxation of the
maximum weight subtree problem (cf. §<a href="#S2.SS4" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>).</p>
</div></li>
<li id="I5.i7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I5.i7.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ILP-Joint:</span> The full ILP from <cite class="ltx_cite">Thadani and McKeown (<a href="#bib.bib155" title="Sentence compression with joint structural inference" class="ltx_ref">2013</a>)</cite>,
which provides an upper bound on the performance of the proposed
approximation strategies.</p>
</div></li>
</ul>
<p class="ltx_p">The learning rate schedule for the Lagrangian relaxation approaches
was set as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="\eta_{i}\triangleq\tau/(\tau+i)" display="inline"><mrow><msub><mi>η</mi><mi>i</mi></msub><mo>≜</mo><mrow><mi>τ</mi><mo>/</mo><mrow><mo>(</mo><mrow><mi>τ</mi><mo>+</mo><mi>i</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m2" class="ltx_Math" alttext="\tau" display="inline"><mi>τ</mi></math> was set to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m3" class="ltx_Math" alttext="100" display="inline"><mn>100</mn></math> for aggressive subgradient updates.</span></span></span>
while the hyperparameter <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m4" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math> was tuned using the development
split of each corpus.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>We were surprised to observe that
performance improved significantly when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m5" class="ltx_Math" alttext="\psi" display="inline"><mi>ψ</mi></math> was set
closer to 1, thereby emphasizing token features in the
dependency subproblem. The final values chosen were
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m6" class="ltx_Math" alttext="\psi_{\text{BN}}=0.9" display="inline"><mrow><msub><mi>ψ</mi><mtext>BN</mtext></msub><mo>=</mo><mn>0.9</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m7" class="ltx_Math" alttext="\psi_{\text{NW}}=0.8" display="inline"><mrow><msub><mi>ψ</mi><mtext>NW</mtext></msub><mo>=</mo><mn>0.8</mn></mrow></math>.
</span></span></span></p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Results</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">Tables <a href="#S3.T1" title="Table 1 ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.T2" title="Table 2 ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarize
the results from our compression experiments on the BN and NW corpora
respectively.
Starting with the n-gram approaches, the performance of <span class="ltx_text ltx_font_bold">3-LM</span>
leads us to observe that the gains of supervised learning far outweigh
the utility of higher-order
n-gram factorization, which is also responsible for a significant
increase in wall-clock time.
In contrast, <span class="ltx_text ltx_font_bold">DP</span>
is an order of magnitude faster than all other approaches
studied here although it is not
competitive under parse-based measures such as RASP F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="{}_{1}\%" display="inline"><mmultiscripts><mi mathvariant="normal">%</mi><mprescripts/><mn>1</mn><none/></mmultiscripts></math> which is known
to correlate with human judgments of grammaticality <cite class="ltx_cite">[<a href="#bib.bib30" title="Models for sentence compression: a comparison across domains, training requirements and evaluation measures" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">We were surprised by the strong performance of the dependency-based
inference techniques, which yielded results that approached the
joint model in both n-gram and parse-based measures. The exact
<span class="ltx_text ltx_font_bold">ILP-Dep</span> approach halves the runtime of <span class="ltx_text ltx_font_bold">ILP-Joint</span>
to produce compressions that have similar (although statistically
distinguishable) scores.
Approximating dependency-based inference with <span class="ltx_text ltx_font_bold">LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST</span>
yields similar performance for a further halving of runtime;
however, the performance of this approach is notably worse.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">Turning to the joint approaches, the strong performance of <span class="ltx_text ltx_font_bold">ILP-Joint</span>
is expected; less so is the
relatively high but yet practically reasonable runtime that it requires.
We note, however, that these ILPs are solved using a highly-optimized
commercial-grade solver that can utilize all CPU cores<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>16 cores in our experimental
environment.</span></span></span> while our approximation approaches are implemented as
single-processed Python code without significant effort toward optimization.
Comparing the two approximation strategies shows a clear performance
advantage for <span class="ltx_text ltx_font_bold">DP+LP</span> over <span class="ltx_text ltx_font_bold">DP+LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m1" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST</span>: the
latter approach entails slower inference due to the overhead of running
the Chu-Liu Edmonds algorithm at every dual update, and furthermore, the
error introduced by approximating an integral solution results in
a significant decrease in dependency recall.
In contrast, <span class="ltx_text ltx_font_bold">DP+LP</span> directly optimizes the dual problem
by using the relaxed dependency solution to update Lagrange multipliers
and achieves the best performance on parse-based F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m2" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math>
outside of the slower ILP approaches.
Convergence rates
also vary for these two techniques: <span class="ltx_text ltx_font_bold">DP+LP</span> has a lower rate
of empirical convergence (15% on BN and 4% on NW) when compared to
<span class="ltx_text ltx_font_bold">DP+LP<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m3" class="ltx_Math" alttext="\rightarrow" display="inline"><mo mathvariant="normal">→</mo></math>MST</span> (19% on BN and 6% on NW).</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the effect of input sentence length on inference
time and performance for <span class="ltx_text ltx_font_bold">ILP-Joint</span> and <span class="ltx_text ltx_font_bold">DP+LP</span> over the
NW test corpus.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup>Similar results were observed for the BN test corpus.</span></span></span>
The timing results reveal that the approximation strategy is consistently
faster than the ILP solver.
The variation in RASP F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m1" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math>% with input size
indicates the viability of a hybrid approach which could balance accuracy
and speed by using
<span class="ltx_text ltx_font_bold">ILP-Joint</span> for smaller problems and <span class="ltx_text ltx_font_bold">DP+LP</span> for larger ones.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Sentence compression is one of the better-studied text-to-text generation
problems and has been observed to play a significant role in
human summarization <cite class="ltx_cite">[<a href="#bib.bib80" title="Sentence reduction for automatic text summarization" class="ltx_ref">23</a>, <a href="#bib.bib81" title="Cut and paste based text summarization" class="ltx_ref">22</a>]</cite>.
Most approaches to sentence compression are supervised
<cite class="ltx_cite">[<a href="#bib.bib85" title="Summarization beyond sentence extraction: a probabilistic approach to sentence compression" class="ltx_ref">25</a>, <a href="#bib.bib140" title="Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar" class="ltx_ref">42</a>, <a href="#bib.bib158" title="Supervised and unsupervised learning for sentence compression" class="ltx_ref">46</a>, <a href="#bib.bib119" title="Discriminative sentence compression with soft syntactic evidence" class="ltx_ref">36</a>, <a href="#bib.bib159" title="Trimming CFG parse trees for sentence compression using machine learning approaches" class="ltx_ref">47</a>, <a href="#bib.bib68" title="Lexicalized Markov grammars for sentence compression" class="ltx_ref">18</a>, <a href="#bib.bib131" title="Discriminative sentence compression with conditional random fields" class="ltx_ref">40</a>, <a href="#bib.bib37" title="Sentence compression as tree transduction" class="ltx_ref">9</a>, <a href="#bib.bib67" title="An extractive supervised two-stage method for sentence compression" class="ltx_ref">17</a>, <a href="#bib.bib69" title="Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation" class="ltx_ref">19</a>, <a href="#bib.bib127" title="Paraphrastic sentence compression with a character-based metric: tightening without deletion" class="ltx_ref">38</a>, <a href="#bib.bib64" title="Overcoming the lack of parallel data in sentence compression" class="ltx_ref">15</a>]</cite> following the release of
datasets such as the Ziff-Davis corpus <cite class="ltx_cite">[<a href="#bib.bib84" title="Statistics-based summarization - step one: sentence compression" class="ltx_ref">24</a>]</cite> and the
Edinburgh compression corpora <cite class="ltx_cite">[<a href="#bib.bib30" title="Models for sentence compression: a comparison across domains, training requirements and evaluation measures" class="ltx_ref">5</a>, <a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">7</a>]</cite>, although
unsupervised approaches—largely based on ILPs—have
also received consideration <cite class="ltx_cite">[<a href="#bib.bib32" title="Modelling compression with discourse constraints" class="ltx_ref">6</a>, <a href="#bib.bib33" title="Global inference for sentence compression: an integer linear programming approach" class="ltx_ref">7</a>, <a href="#bib.bib60" title="Dependency tree based sentence compression" class="ltx_ref">16</a>]</cite>.
Compression has also been used as a tool for document
summarization <cite class="ltx_cite">[<a href="#bib.bib47" title="A noisy-channel model for document compression" class="ltx_ref">12</a>, <a href="#bib.bib172" title="Multi-candidate reduction: sentence compression as a tool for document summarization tasks" class="ltx_ref">49</a>, <a href="#bib.bib32" title="Modelling compression with discourse constraints" class="ltx_ref">6</a>, <a href="#bib.bib110" title="Summarization with a joint model for sentence extraction and compression" class="ltx_ref">34</a>, <a href="#bib.bib13" title="Jointly learning to extract and compress" class="ltx_ref">2</a>, <a href="#bib.bib169" title="Multiple aspect summarization using integer linear programming" class="ltx_ref">48</a>, <a href="#bib.bib2" title="Fast and robust compressive summarization with dual decomposition and multi-task learning" class="ltx_ref">1</a>, <a href="#bib.bib125" title="Discursive sentence compression" class="ltx_ref">37</a>, <a href="#bib.bib95" title="Document summarization via guided sentence compression" class="ltx_ref">30</a>, <a href="#bib.bib137" title="Fast joint compression and summarization via graph cuts" class="ltx_ref">41</a>]</cite>, with recent work
formulating the summarization task as joint sentence extraction and
compression and often employing ILP or Lagrangian relaxation.
Monolingual compression also faces many obstacles
common to decoding in machine translation, and
a number of approaches which have been proposed to combine
phrasal and syntactic models <cite class="ltx_cite">[<a href="#bib.bib76" title="Forest rescoring: faster decoding with integrated language models" class="ltx_ref">21</a>, <a href="#bib.bib142" title="Exact decoding of syntactic translation models through Lagrangian relaxation" class="ltx_ref">43</a>]</cite> <em class="ltx_emph">inter alia</em>
offer directions for future research into
compression problems.</p>
</div>
<div id="S4.F3" class="ltx_figure"><img src="P14-1117/image001.png" id="S4.F3.g1" class="ltx_graphics" width="260" height="241" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Effect of input size on
(a) inference time, and (b) the corresponding difference
in RASP F<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F3.m2" class="ltx_Math" alttext="{}_{1}" display="inline"><msub><mi/><mn>1</mn></msub></math>% (<span class="ltx_text ltx_font_bold">ILP-Joint</span> – <span class="ltx_text ltx_font_bold">DP+LP</span>) on the NW corpus.</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We have presented approximate inference strategies to jointly compress
sentences under bigram and dependency-factored objectives
by exploiting the modularity of the task and considering the
two subproblems in isolation.
Experiments show that one of these
approximation strategies produces results comparable to
a state-of-the-art integer linear program for the same
joint inference task with a 60% reduction
in average inference time.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">The author is grateful to Alexander Rush for helpful discussions
and to the anonymous reviewers for their comments.
This work was supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior National
Business Center (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup>
The views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily
representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Government.
</span></span></span></p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Almeida and A. F. T. Martins</span><span class="ltx_text ltx_bib_year">(2013-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast and robust compressive summarization with dual decomposition and multi-task learning</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"/></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 196–206</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-937284-50-3</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Berg-Kirkpatrick, D. Gillick and D. Klein</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Jointly learning to extract and compress</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 481–490</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-87-9</span>,
<a href="http://dl.acm.org/citation.cfm?id=2002472.2002534" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Briscoe, J. Carroll and R. Watson</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The second release of the RASP system</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Chu and T. Liu</span><span class="ltx_text ltx_bib_year">(1965)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">On the shortest arborescence of a directed graph</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science Sinica</span> <span class="ltx_text ltx_bib_volume">14</span>, <span class="ltx_text ltx_bib_pages"> pp. 1396–1400</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS4.p6" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.
</span></li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Clarke and M. Lapata</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Models for sentence compression: a comparison across domains, training requirements and evaluation measures</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 377–384</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1220175.1220223" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220175.1220223" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S3.SS2.p1" title="3.2 Results ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Clarke and M. Lapata</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Modelling compression with discourse constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–11</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://jamesclarke.info/static/clarke-lapata-emnlp07.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Clarke and M. Lapata</span><span class="ltx_text ltx_bib_year">(2008-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Global inference for sentence compression: an integer linear programming approach</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal for Artificial Intelligence Research</span> <span class="ltx_text ltx_bib_volume">31</span>, <span class="ltx_text ltx_bib_pages"> pp. 399–429</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1076-9757</span>,
<a href="http://portal.acm.org/citation.cfm?id=1622655.1622667" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I5.i1.p1" title="3.1 Systems ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.p1" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Cohn and M. Lapata</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sentence compression beyond word deletion</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 137–144</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-905593-44-6</span>,
<a href="http://dl.acm.org/citation.cfm?id=1599081.1599099" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Cohn and M. Lapata</span><span class="ltx_text ltx_bib_year">(2009-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sentence compression as tree transduction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Artificial Intelligence Research</span> <span class="ltx_text ltx_bib_volume">34</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 637–674</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1076-9757</span>,
<a href="http://dl.acm.org/citation.cfm?id=1622716.1622733" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Collins</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discriminative training methods for hidden Markov models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–8</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1118693.1118694" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/http://dx.doi.org/10.3115/1118693.1118694" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS5.p1" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>.
</span></li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Das, A. F. T. Martins and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An exact dual decomposition algorithm for shallow semantic parsing with constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">SemEval ’12</span>, <span class="ltx_text ltx_bib_pages"> pp. 209–217</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=2387636.2387671" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Daumé,III and D. Marcu</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A noisy-channel model for document compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 449–456</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1073083.1073159" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1073083.1073159" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. DeNero and K. Macherey</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Model-based aligner combination using dual decomposition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 420–429</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-87-9</span>,
<a href="http://dl.acm.org/citation.cfm?id=2002472.2002526" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. R. Edmonds</span><span class="ltx_text ltx_bib_year">(1967)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Optimum branchings</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Research of the National Bureau of Standards</span> <span class="ltx_text ltx_bib_volume">71B</span>, <span class="ltx_text ltx_bib_pages"> pp. 233–240</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS4.p6" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.
</span></li>
<li id="bib.bib64" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Filippova and Y. Altun</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Overcoming the lack of parallel data in sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1481–1491</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Filippova and M. Strube</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dependency tree based sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 25–32</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://portal.acm.org/citation.cfm?id=1708322.1708329" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Galanis and I. Androutsopoulos</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An extractive supervised two-stage method for sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 885–893</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 1-932432-65-5</span>,
<a href="http://dl.acm.org/citation.cfm?id=1857999.1858130" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib68" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Galley and K. McKeown</span><span class="ltx_text ltx_bib_year">(2007-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Lexicalized Markov grammars for sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 180–187</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Ganitkevitch, C. Callison-Burch, C. Napoles and B. Van Durme</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1168–1179</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-937284-11-4</span>,
<a href="http://dl.acm.org/citation.cfm?id=2145432.2145556" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib74" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Hori and S. Furui</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Speech summarization: an approach through word extraction and a method for evaluation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEICE Transactions on Information and Systems</span> <span class="ltx_text ltx_bib_volume">E87-D</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 15–25</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I5.i1.p1" title="3.1 Systems ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib76" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Huang and D. Chiang</span><span class="ltx_text ltx_bib_year">(2007-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Forest rescoring: faster decoding with integrated language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 144–151</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P07-1019" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib81" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Jing and K. R. McKeown</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cut and paste based text summarization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 178–185</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=974305.974329" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib80" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Jing</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sentence reduction for automatic text summarization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 310–315</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/974147.974190" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/974147.974190" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib84" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Knight and D. Marcu</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistics-based summarization - step one: sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 703–710</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 0-262-51112-6</span>,
<a href="http://dl.acm.org/citation.cfm?id=647288.721086" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib85" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Knight and D. Marcu</span><span class="ltx_text ltx_bib_year">(2002-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Summarization beyond sentence extraction: a probabilistic approach to sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Artificial Intelligence</span> <span class="ltx_text ltx_bib_volume">139</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 91–107</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0004-3702</span>,
<a href="http://dx.doi.org/10.1016/S0004-3702(02)00222-9" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1016/S0004-3702(02)00222-9" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib87" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Komodakis, N. Paragios and G. Tziritas</span><span class="ltx_text ltx_bib_year">(2007-Oct.)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MRF optimization via dual decomposition: message-passing revisited</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"/>, <span class="ltx_text ltx_bib_pages"> pp. 1–8</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/ICCV.2007.4408890" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 1550-5499</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib88" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Koo, A. M. Rush, M. Collins, T. Jaakkola and D. Sontag</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dual decomposition for parsing with non-projective head automata</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1288–1298</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1870658.1870783" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib91" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Kulesza and F. Pereira</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structured learning with approximate inference.</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"/></span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dblp.uni-trier.de/db/conf/nips/nips2007.html#KuleszaP07" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS5.p1" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>.
</span></li>
<li id="bib.bib94" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. C. Lau, T. H. Ngo and B. N. Nguyen</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finding a length-constrained maximum-sum or maximum-density subtree and its application to logistics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Discrete Optimization</span> <span class="ltx_text ltx_bib_volume">3</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 385 – 391</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"/>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1572-5286</span>,
<a href="http://dx.doi.org/10.1016/j.disopt.2006.06.002" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://www.sciencedirect.com/science/article/pii/S157252860600048X" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS4.p1" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.
</span></li>
<li id="bib.bib95" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Li, F. Liu, F. Weng and Y. Liu</span><span class="ltx_text ltx_bib_year">(2013-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Document summarization via guided sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Seattle, Washington, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 490–500</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D13-1047" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib106" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. L. Magnanti and L. A. Wolsey</span><span class="ltx_text ltx_bib_year">(1994)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Optimal trees</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p2" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS4.p3" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.
</span></li>
<li id="bib.bib115" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar and M. A. T. Figueiredo</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dual decomposition with many overlapping components</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 238–249</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-937284-11-4</span>,
<a href="http://dl.acm.org/citation.cfm?id=2145432.2145460" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib112" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. F. T. Martins, N. A. Smith and E. P. Xing</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Concise integer linear programming formulations for dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 342–350</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-45-9</span>,
<a href="http://portal.acm.org/citation.cfm?id=1687878.1687928" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS5.p1" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>.
</span></li>
<li id="bib.bib110" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. F. T. Martins and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Summarization with a joint model for sentence extraction and compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–9</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-35-0</span>,
<a href="http://dl.acm.org/citation.cfm?id=1611638.1611639" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib118" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald, F. Pereira, K. Ribarov and J. Hajič</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Non-projective dependency parsing using spanning tree algorithms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 523–530</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1220575.1220641" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220575.1220641" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS4.p6" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>.
</span></li>
<li id="bib.bib119" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discriminative sentence compression with soft syntactic evidence</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 297–304</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I5.i2.p1" title="3.1 Systems ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS3.p1" title="2.3 Bigram subsequences ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib125" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Molina, J. Torres-Moreno, E. SanJuan, I. da Cunha and G. E. Sierra Martínez</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discursive sentence compression</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">7817</span>, <span class="ltx_text ltx_bib_pages"> pp. 394–407</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib127" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Napoles, C. Callison-Burch, J. Ganitkevitch and B. Van Durme</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Paraphrastic sentence compression with a character-based metric: tightening without deletion</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 84–90</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781937284053</span>,
<a href="http://dl.acm.org/citation.cfm?id=2107679.2107689" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib126" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Napoles, B. Van Durme and C. Callison-Burch</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating sentence compression: pitfalls and suggested remedies</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 91–97</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781937284053</span>,
<a href="http://dl.acm.org/citation.cfm?id=2107679.2107690" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib131" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Nomoto</span><span class="ltx_text ltx_bib_year">(2007-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Discriminative sentence compression with conditional random fields</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Processing and Management</span> <span class="ltx_text ltx_bib_volume">43</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 1571–1587</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0306-4573</span>,
<a href="http://dx.doi.org/10.1016/j.ipm.2007.01.025" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1016/j.ipm.2007.01.025" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib137" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Qian and Y. Liu</span><span class="ltx_text ltx_bib_year">(2013-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast joint compression and summarization via graph cuts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Seattle, Washington, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1492–1502</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D13-1156" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib140" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Riezler, T. H. King, R. Crouch and A. Zaenen</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 118–125</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1073445.1073471" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1073445.1073471" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib142" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. M. Rush and M. Collins</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exact decoding of syntactic translation models through Lagrangian relaxation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 72–82</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-87-9</span>,
<a href="http://dl.acm.org/citation.cfm?id=2002472.2002482" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib141" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. M. Rush, D. Sontag, M. Collins and T. Jaakkola</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">On dual decomposition and linear programming relaxations for natural language processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–11</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1870658.1870659" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Lagrangian relaxation ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib155" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Thadani and K. McKeown</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sentence compression with joint structural inference</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I5.i4.p1" title="3.1 Systems ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#I5.i7.p1" title="3.1 Systems ‣ 3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS1.p2" title="2.1 Joint objective ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS4.p2" title="2.4 Dependency subtrees ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>,
<a href="#S2.SS5.p1" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>,
<a href="#S2.p1" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p2" title="2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib158" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Turner and E. Charniak</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Supervised and unsupervised learning for sentence compression</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 290–297</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1219840.1219876" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1219840.1219876" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib159" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Unno, T. Ninomiya, Y. Miyao and J. Tsujii</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Trimming CFG parse trees for sentence compression using machine learning approaches</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 850–857</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1273073.1273182" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Experiments ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib169" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Woodsend and M. Lapata</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multiple aspect summarization using integer linear programming</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 233–243</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib172" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Zajic, B. J. Dorr, J. Lin and R. Schwartz</span><span class="ltx_text ltx_bib_year">(2007-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi-candidate reduction: sentence compression as a tool for document summarization tasks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Processing and Management</span> <span class="ltx_text ltx_bib_volume">43</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 1549–1570</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0306-4573</span>,
<a href="http://dx.doi.org/10.1016/j.ipm.2007.01.016" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1016/j.ipm.2007.01.016" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Related Work ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib173" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Zhao and L. Huang</span><span class="ltx_text ltx_bib_year">(2013-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Minibatch and parallelization for online large margin structured learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Atlanta, Georgia</span>, <span class="ltx_text ltx_bib_pages"> pp. 370–379</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N13-1038" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS5.p1" title="2.5 Learning and Features ‣ 2 Multi-Structure Sentence Compression ‣ Approximation Strategies for Multi-Structure Sentence Compression" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.5</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:53:19 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
