<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Novel Content Enriching Model for Microblog Using News Corpus</title>
<!--Generated on Wed Jun 11 17:42:46 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Novel Content Enriching Model for Microblog Using News Corpus</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yunlun Yang<math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="{}^{1}" display="inline"><msup><mi/><mn>1</mn></msup></math>, Zhihong Deng<math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="{}^{2*}" display="inline"><msup><mi/><mrow><mn>2</mn><mo>⁣</mo><mo>*</mo></mrow></msup></math>, Hongliang Yu<math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="{}^{3}" display="inline"><msup><mi/><mn>3</mn></msup></math>
<br class="ltx_break"/>Key Laboratory of Machine Perception (Ministry of Education),
<br class="ltx_break"/>School of Electronics Engineering and Computer Science,
<br class="ltx_break"/>Peking University, Beijing 100871, China
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="{}^{1}" display="inline"><msup><mi/><mn>1</mn></msup></math><span class="ltx_text ltx_font_typewriter">incomparable-lun@pku.edu.cn
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="{}^{2}" display="inline"><msup><mi/><mn mathvariant="normal">2</mn></msup></math>zhdeng@cis.pku.edu.cn
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="{}^{3}" display="inline"><msup><mi/><mn mathvariant="normal">3</mn></msup></math>yuhongliang324@gmail.com</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">In this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification. We assume that microblogs share the same topics with external knowledge.
We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge.
Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods.</p>
</div><span class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="m7" class="ltx_Math" alttext="{}^{*}" display="inline"><msup><mi/><mo>*</mo></msup></math>Corresponding author</span></span></span>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">During the past decade, the short text representation has been intensively studied. Previous researches <cite class="ltx_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Phan et al.2008</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Guo and Diab2012</a>]</cite> show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful <cite class="ltx_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Hu et al.2009</a>]</cite> in tackling the data scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Nowadays, most of the work on short text focuses on microblog. As a new form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well. Every day, a great quantity of microblogs more than we can read is pushed to us, and finding what we are interested in becomes rather difficult, so the ability of choosing what kind of microblogs to read is urgently demanded by common user. Such ability can be implemented by effective short text classification.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Treating microblogs as standard texts and directly classifying them cannot achieve the goal of effective classification because of sparseness problem. On the other hand, news on the Internet is of information abundance and many microblogs are news-related. They share up-to-date topics and sometimes quote each other. Thus, external knowledge, such as news, provides rich supplementary information for analysing and mining microblogs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Motivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog classification. The basic assumption in our model is that news articles and microblogs tend to share the same topics. We first infer the topic distribution of each microblog based on the topic-word distribution of news corpus obtained by the LDA estimation. With the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relatedness of between each word and microblog, since words not appearing in the microblog may still be highly relevant.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">To sum up, our contributions are:</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(1)</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">We formulate the topic inference problem for short texts as a convex optimization problem.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(1)</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">We enrich the content of microblogs by inferring the association between microblogs and external words in a probabilistic perspective.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(1)</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">We evaluate our method on the real datasets and experiment results outperform the baseline methods.</p>
</div></li>
</ol>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Based on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering. Among them, some directly utilize the structure information of organized knowledge base or search engine. <cite class="ltx_cite"><a href="#bib.bibx1" title="" class="ltx_ref">Banerjee et al.2007</a></cite> use the title and the description of news article as two separate query strings to select related concepts as additional feature. <cite class="ltx_cite"><a href="#bib.bibx8" title="" class="ltx_ref">Hu et al.2009</a></cite> present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">However, to better leverage external resource, some other methods introduce topic models. <cite class="ltx_cite"><a href="#bib.bibx10" title="" class="ltx_ref">Phan et al.2008</a></cite> present a framework including an approach for short text topic inference and adds abstract words as extra feature. <cite class="ltx_cite"><a href="#bib.bibx6" title="" class="ltx_ref">Guo and Diab2012</a></cite> modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Those methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack in-depth analysis for external resources. Compared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words from external knowledge are informative as well.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Our Model</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">We formulate the problem as follows. Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="EK=\{d^{e}_{1},\ldots,d^{e}_{M^{e}}\}" display="inline"><mrow><mrow><mi>E</mi><mo>⁢</mo><mi>K</mi></mrow><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>d</mi><mn>1</mn><mi>e</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>d</mi><msup><mi>M</mi><mi>e</mi></msup><mi>e</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math> denote external knowledge consisting of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="M^{e}" display="inline"><msup><mi>M</mi><mi>e</mi></msup></math> documents. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="V^{e}=\{w^{e}_{1},\ldots,w^{e}_{N^{e}}\}" display="inline"><mrow><msup><mi>V</mi><mi>e</mi></msup><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>w</mi><mn>1</mn><mi>e</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><msup><mi>N</mi><mi>e</mi></msup><mi>e</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math> represents its vocabulary. Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m4" class="ltx_Math" alttext="MB=\{d^{m}_{1},\ldots,d^{m}_{M^{m}}\}" display="inline"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>B</mi></mrow><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>d</mi><mn>1</mn><mi>m</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>d</mi><msup><mi>M</mi><mi>m</mi></msup><mi>m</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math> denote microblog set and its vocabulary is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m5" class="ltx_Math" alttext="V^{m}=\{w^{m}_{1},\ldots,w^{m}_{N^{m}}\}" display="inline"><mrow><msup><mi>V</mi><mi>m</mi></msup><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>w</mi><mn>1</mn><mi>m</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><msup><mi>N</mi><mi>m</mi></msup><mi>m</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math>. Our task is to enrich each microblog with additional information so as to improve microblog’s representation.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The model we proposed mainly consists of three steps:</p>
<ol id="I2" class="ltx_enumerate">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(a)</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p">Topic inference for external knowledge by running LDA estimation.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(b)</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p">Topic inference for microblogs by employing the word distributions of topics obtained from step (a).</p>
</div></li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">(c)</span> 
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p">Select relevant words from external knowledge to enrich the content of microblogs.</p>
</div></li>
</ol>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Topic Inference for External Knowledge</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We do topic analysis for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="EK" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>K</mi></mrow></math> using LDA estimation <cite class="ltx_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Blei et al.2003</a>]</cite> in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">In LDA, each document has a distribution over all topics <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m1" class="ltx_Math" alttext="P(z_{k}|d_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>z</mi><mi>k</mi></msub><mo>|</mo><msub><mi>d</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math>, and each topic has a distribution over all words <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m2" class="ltx_Math" alttext="P(w_{i}|z_{k})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>z</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m3" class="ltx_Math" alttext="z_{k}" display="inline"><msub><mi>z</mi><mi>k</mi></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m4" class="ltx_Math" alttext="d_{j}" display="inline"><msub><mi>d</mi><mi>j</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m5" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> represent the topic, document and word respectively. The optimization problem is formulated as maximizing the log likelihood on the corpus:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m1" class="ltx_Math" alttext="\max\ \sum_{i}\sum_{j}X_{ij}\log\sum_{k}P(z_{k}|d_{j})P(w_{i}|z_{k})" display="block"><mrow><mo movablelimits="false" rspace="7.5pt">max</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder><msub><mi>X</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mi>log</mi><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>k</mi></munder><mi>P</mi><mrow><mo>(</mo><msub><mi>z</mi><mi>k</mi></msub><mo>|</mo><msub><mi>d</mi><mi>j</mi></msub><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>z</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">In this formulation, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m1" class="ltx_Math" alttext="X_{ij}" display="inline"><msub><mi>X</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> represents the term frequency of word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m2" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> in document <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m3" class="ltx_Math" alttext="d_{j}" display="inline"><msub><mi>d</mi><mi>j</mi></msub></math>. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m4" class="ltx_Math" alttext="P(z_{k}|d_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>z</mi><mi>k</mi></msub><mo>|</mo><msub><mi>d</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m5" class="ltx_Math" alttext="P(w_{i}|z_{k})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>z</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></math> are parameters to be inferred, corresponding to the topic distribution of each document and the word distribution of each topic respectively. Estimating parameters for LDA by directly and exactly maximizing the likelihood of the corpus in (1) is intractable, so we use Gibbs Sampling for estimation.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">After performing LDA model (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m1" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> topics) estimation on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m2" class="ltx_Math" alttext="EK" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>K</mi></mrow></math>, we obtain the topic distributions of document <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m3" class="ltx_Math" alttext="d^{e}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>e</mi></msubsup></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m4" class="ltx_Math" alttext="j=1,\ldots,M^{e}" display="inline"><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>M</mi><mi>e</mi></msup></mrow></mrow></math>), denoted as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m5" class="ltx_Math" alttext="P(z^{e}_{k}|d^{e}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>e</mi></msubsup><mo>)</mo></mrow></mrow></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m6" class="ltx_Math" alttext="k=1,\ldots,K" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></math>), and the word distribution of topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m7" class="ltx_Math" alttext="z^{e}_{k}" display="inline"><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m8" class="ltx_Math" alttext="k=1,\ldots,K" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></math>), denoted as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m9" class="ltx_Math" alttext="P(w^{e}_{i}|z^{e}_{k})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow></mrow></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m10" class="ltx_Math" alttext="i=1,\ldots,N^{e}" display="inline"><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>N</mi><mi>e</mi></msup></mrow></mrow></math>). Step (b) greatly relies on the word distributions of topics we have obtained here.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Topic Inference for Microblog</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">In this section, we infer the topic distribution of each microblog. Because of the assumption that microblogs share the same topics with external corpus, the “topic distribution” here refers to a distribution over all topics on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="EK" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>K</mi></mrow></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Differing from step (a), the method used for topic inference for microblogs is not directly running LDA estimation on microblog collection but following the topics from external knowledge to ensure topic consistence. We employ the word distributions of topics obtained from step (a), i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="P(w^{e}_{i}|z^{e}_{k})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow></mrow></math>, and formulate the optimization problem in a similar form to Formula (1) as follows:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m1" class="ltx_Math" alttext="\max_{P(z^{e}_{k}|d^{m}_{j})}\ \sum_{i}\sum_{j}\underline{X}_{ij}\log\sum_{k}P%&#10;(z^{e}_{k}|d^{m}_{j})P(w^{e}_{i}|z^{e}_{k})," display="block"><mrow><mpadded width="+5.0pt"><munder><mo movablelimits="false">max</mo><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></munder></mpadded><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder><msub><munder accentunder="true"><mi>X</mi><mo>¯</mo></munder><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mi>log</mi><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>k</mi></munder><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m1" class="ltx_Math" alttext="\underline{X}_{ij}" display="inline"><msub><munder accentunder="true"><mi>X</mi><mo>¯</mo></munder><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> represents the term frequency of word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m2" class="ltx_Math" alttext="w^{e}_{i}" display="inline"><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup></math> in microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m3" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m4" class="ltx_Math" alttext="P(z^{e}_{k}|d^{m}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math> denote the distribution of microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m5" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math> over all topics on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m6" class="ltx_Math" alttext="EK" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>K</mi></mrow></math>. Obviously most <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m7" class="ltx_Math" alttext="\underline{X}_{ij}" display="inline"><msub><munder accentunder="true"><mi>X</mi><mo>¯</mo></munder><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> are zero and we ignore those words that do not appear in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m8" class="ltx_Math" alttext="V^{e}" display="inline"><msup><mi>V</mi><mi>e</mi></msup></math>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p class="ltx_p">Compared with the original LDA optimization problem (1), the topic inference problem for microblog (2) follows the idea of document generation process, but replaces topics to be estimated with known topics from other corpus. As a result, parameters to be inferred are only the topic distribution of every microblog.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p class="ltx_p">It is noteworthy that since the word distribution of every topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p6.m1" class="ltx_Math" alttext="P(w^{e}_{i}|z^{e}_{k})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow></mrow></math> is known, Formula (<a href="#S3.E2" title="(2) ‣ 3.2 Topic Inference for Microblog ‣ 3 Our Model ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) can be further solved by separating it into <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p6.m2" class="ltx_Math" alttext="M^{m}" display="inline"><msup><mi>M</mi><mi>m</mi></msup></math> subproblems:</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<table id="S3.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m1" class="ltx_Math" alttext="\begin{split}\max_{P(z^{e}_{k}|d^{m}_{j})}\ &amp;\sum_{i}\underline{X}_{ij}\log%&#10;\sum_{k}P(z^{e}_{k}|d^{m}_{j})P(w^{e}_{i}|z^{e}_{k})\\&#10;&amp;for\quad j=1,\ldots,M^{m}\end{split}" display="block"><mrow><munder><mo movablelimits="false">max</mo><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></munder><mo mathvariant="italic" separator="true"> </mo><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><msub><munder accentunder="true"><mi>X</mi><mo>¯</mo></munder><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mi>log</mi><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>k</mi></munder><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow><mi>f</mi><mi>o</mi><mi>r</mi><mo mathvariant="italic" separator="true"> </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>M</mi><mi>m</mi></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p class="ltx_p">These <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m1" class="ltx_Math" alttext="M^{m}" display="inline"><msup><mi>M</mi><mi>m</mi></msup></math> subproblems correspond to the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m2" class="ltx_Math" alttext="M^{m}" display="inline"><msup><mi>M</mi><mi>m</mi></msup></math> microblogs and can be easily proved convexity. After solving them, we obtain the topic distributions of microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m3" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m4" class="ltx_Math" alttext="j=1,\ldots,M^{m}" display="inline"><mrow><mi>j</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msup><mi>M</mi><mi>m</mi></msup></mrow></mrow></math>), denoted as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m5" class="ltx_Math" alttext="P(z^{e}_{k}|d^{m}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p8.m6" class="ltx_Math" alttext="k=1,\ldots,K" display="inline"><mrow><mi>k</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi></mrow></mrow></math>).</p>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Select Relevant Words for Microblog</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">To enrich the content of every microblog, we select relevant words from external knowledge in this section.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">Based on the results of step (a)&amp;(b), we calculate the word distributions of microblogs as follows:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="P(w^{e}_{i}|d^{m}_{j})=\sum_{k}P(z^{e}_{k}|d^{m}_{j})P(w^{e}_{i}|z^{e}_{k})," display="block"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow><mo>=</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>k</mi></munder><mi>P</mi><mrow><mo>(</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>z</mi><mi>k</mi><mi>e</mi></msubsup><mo>)</mo></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p4.m1" class="ltx_Math" alttext="P(w^{e}_{i}|d^{m}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math> represents the probability that word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p4.m2" class="ltx_Math" alttext="w^{e}_{i}" display="inline"><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup></math> will appear in microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p4.m3" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math>. In other words, though some words may not actually appears in a microblog, there is still a probability that it is highly relevant to the microblog. Intuitively, this probability indicates the strength of association between a word and a microblog. The word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the accuracy of topic inference in step (b). In fact, the more words a microblog includes, the more accurate its topic inference will be, and this can be regarded as an explanation of the low efficiency of data sparseness problem.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p class="ltx_p">For microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m1" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math>, we sort all words by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m2" class="ltx_Math" alttext="P(w^{e}_{i}|d^{m}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mi>i</mi><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math> in descending order. Having known the top <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m3" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> relevant words according to the result of sorting, we redefine the “term frequency” of every word after adding these <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m4" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> words to microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m5" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math> as additional content. Supposing these <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m6" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math> words are <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m7" class="ltx_Math" alttext="w^{e}_{j1},w^{e}_{j2},\ldots,w^{e}_{jL}" display="inline"><mrow><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mn>1</mn></mrow><mi>e</mi></msubsup><mo>,</mo><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mn>2</mn></mrow><mi>e</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mi>L</mi></mrow><mi>e</mi></msubsup></mrow></math>, the revised term frequency of word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m8" class="ltx_Math" alttext="w\in\{w^{e}_{j1},\ldots,w^{e}_{jL}\}" display="inline"><mrow><mi>w</mi><mo>∈</mo><mrow><mo>{</mo><mrow><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mn>1</mn></mrow><mi>e</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mi>L</mi></mrow><mi>e</mi></msubsup></mrow><mo>}</mo></mrow></mrow></math> is defined as follows:</p>
<table id="S3.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="RTF(w,d^{m}_{j})=\frac{P(w|d^{m}_{j})}{\sum_{p=1}^{L}P(w^{e}_{jp}|d^{m}_{j})}*L," display="block"><mrow><mrow><mrow><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>w</mi><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></msubsup><mi>P</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>j</mi><mo>⁢</mo><mi>p</mi></mrow><mi>e</mi></msubsup><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></mfrac><mo>*</mo><mi>L</mi></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m9" class="ltx_Math" alttext="RTF(\cdot)" display="inline"><mrow><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow></math> is the revised term frequency.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p class="ltx_p">As the Equation (<a href="#S3.E5" title="(5) ‣ 3.3 Select Relevant Words for Microblog ‣ 3 Our Model ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) shows, the revised term frequency of every word is proportional to probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p6.m1" class="ltx_Math" alttext="P(w_{i}|d^{m}_{j})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup><mo>)</mo></mrow></mrow></math> rather than a constant.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p class="ltx_p">So far, we can add these <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p7.m1" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math><span class="ltx_text ltx_font_bold"> words and their revised term frequency</span> as additional information to microblog <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p7.m2" class="ltx_Math" alttext="d^{m}_{j}" display="inline"><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></math>. The revised term frequency plays the same role as TF in common text representation vector, so we calculate the TFIDF of the added words as:</p>
</div>
<div id="S3.SS3.p8" class="ltx_para">
<table id="S3.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="TFIDF(w,d^{m}_{j})=RTF(w,d^{m}_{j})\cdot IDF(w)" display="block"><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msubsup><mi>d</mi><mi>j</mi><mi>m</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>⋅</mo><mi>I</mi></mrow><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
</div>
<div id="S3.SS3.p9" class="ltx_para">
<p class="ltx_p">Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p9.m1" class="ltx_Math" alttext="IDF(w)" display="inline"><mrow><mi>I</mi><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math> is changed as arrival of new words for each microblog.
The TFIDF vector of a microblog with additional words is called <span class="ltx_text ltx_font_bold">enhanced vector</span>.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">To evaluate our method, we build our own datasets. We crawl 95028 Chinese news reports from Sina News website<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Sina News: http://news.sina.com.cn/</span></span></span>, segment them, and remove stop words and rare words. After preprocessing, these news documents are used as external knowledge. As for microblog, we crawl a number of microblogs from Sina Weibo<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>Sina Weibo: http://www.weibo.com/</span></span></span>, and ask unbiased assessors to manually classify them into 9 categories following the column setting of Sina News. After the manual classification, we remove short microblogs (less than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p1.m1" class="ltx_Math" alttext="10" display="inline"><mn>10</mn></math> words), usernames, links and some special characters, then we segment them and remove rare words as well. Finally, we get 1671 classified microblogs as our microblog dataset. The size of each category is shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Experimental Setup ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Category</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">#Microblog</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Finance</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">229</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Stock</th>
<td class="ltx_td ltx_align_center ltx_border_r">80</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Entertainment</th>
<td class="ltx_td ltx_align_center ltx_border_r">162</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Military Affairs</th>
<td class="ltx_td ltx_align_center ltx_border_r">179</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Technologies</th>
<td class="ltx_td ltx_align_center ltx_border_r">204</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Digital Products</th>
<td class="ltx_td ltx_align_center ltx_border_r">194</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Sports</th>
<td class="ltx_td ltx_align_center ltx_border_r">195</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Society</th>
<td class="ltx_td ltx_align_center ltx_border_r">214</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Daily Life</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">214</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Microblog number of every category</div>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">There are some important details of our implementation. In step (a) of Section <a href="#S3.SS1" title="3.1 Topic Inference for External Knowledge ‣ 3 Our Model ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we estimate LDA model using GibbsLDA++<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>GibbsLDA++: http://gibbslda.sourceforge.net</span></span></span>, a C/C++ implementation of LDA using Gibbs Sampling. In step (b) of Section<a href="#S3.SS2" title="3.2 Topic Inference for Microblog ‣ 3 Our Model ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, OPTI toolbox<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/</span></span></span> on Matlab is used to help solve the convex problems. In the classification tasks shown below, we use LibSVM<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>SVM.NET: http://www.matthewajohnson.org/software
<br class="ltx_break"/>/svm.html</span></span></span> as classifier and perform ten-fold cross validation to evaluate the classification accuracy.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Classification Results</h3>

<div id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Representation</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Average Accuracy</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">TFIDF vector</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.7552</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Boolean vector</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.7203</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Enhanced vector</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_bold">0.8453</span></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Classification accuracy with different representations</div>
</div>
<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">In this section, we report the average precision of each method as shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Classification Results ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The <span class="ltx_text ltx_font_italic">enhanced vector</span> is the representation generated by our method. Two baselines are <span class="ltx_text ltx_font_italic">TFIDF vector</span> <cite class="ltx_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Jones1972</a>]</cite> and <span class="ltx_text ltx_font_italic">boolean vector</span> (word occurrence) of the original microblog. In the table, our method increases the classification accuracy from 75.52% to 84.53% when considering additional information, which means our method indeed improves the representation of microblogs.</p>
</div>
</div>
<div id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.3 </span>Parameter Tuning</h3>

<div id="S4.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle" style="width:433.62pt;">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:216.8pt;" width="216.8pt"><span class="ltx_text ltx_font_bold ltx_font_tiny">Microblog (Translated)</span></th>
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:194.3pt;" width="194.3pt"><span class="ltx_text ltx_font_bold ltx_font_tiny">Top Relevant Words (Translated)</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:216.8pt;" width="216.8pt"><span class="ltx_text ltx_font_tiny">Kim Jong Un held an emergency meeting this morning, and commanded the missile units to prepare for attacking U.S. military bases at any time.</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:194.3pt;" width="194.3pt"><span class="ltx_text ltx_font_tiny">South Korea, America, North Korea, work, safety, claim, military, exercise, united, report</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_tt" style="width:216.8pt;" width="216.8pt"><span class="ltx_text ltx_font_tiny">Shenzhou Nine will carry three astronauts, including the first Chinese female astronaut, and launch in a proper time during the middle of June.</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt" style="width:194.3pt;" width="194.3pt"><span class="ltx_text ltx_font_tiny">day, satellite, launch, research, technology, system, mission, aerospace, success, Chang’e Two</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_tiny"><span class="ltx_tag ltx_tag_table">Table 3: </span>Case study (Translated from Chinese)</div>
</div>
<div id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Effect of Added Words</h4>

<div id="S4.F1" class="ltx_figure"><img src="" id="S4.F1.g1" class="ltx_graphics ltx_centering" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Classification accuracy changes according to topics and added words</div>
</div>
<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">The experiment corresponding to Figure <a href="#S4.F1" title="Figure 1 ‣ 4.3.1 Effect of Added Words ‣ 4.3 Parameter Tuning ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is to discover how the classification accuracy changes when we fix the number of topics (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.SSS1.p1.m1" class="ltx_Math" alttext="K=100" display="inline"><mrow><mi>K</mi><mo>=</mo><mn>100</mn></mrow></math>) and change the number of added words (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.SSS1.p1.m2" class="ltx_Math" alttext="L" display="inline"><mi>L</mi></math>) in our method. Result shows that more added words do not mean higher accuracy. By studying some cases, we find out that if we add too many words, the proportion of “noisy words” will increase. We reach the best result when number of added words is 300.</p>
</div>
</div>
<div id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Effect of Topic Number</h4>

<div id="S4.F2" class="ltx_figure"><img src="" id="S4.F2.g1" class="ltx_graphics ltx_centering" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Classification accuracy changing according to the number of topics</div>
</div>
<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">The experiment corresponding to Figure <a href="#S4.F2" title="Figure 2 ‣ 4.3.2 Effect of Topic Number ‣ 4.3 Parameter Tuning ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is to discover how the classification accuracy changes when we fix the number of added words (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.SSS2.p1.m1" class="ltx_Math" alttext="L=300" display="inline"><mrow><mi>L</mi><mo>=</mo><mn>300</mn></mrow></math>) and change the number of topics (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.SSS2.p1.m2" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math>) in our method. As we can see, the accuracy does not grow monotonously as the number of topics increases. Blindly enlarging the topic number will not improve the accuracy. The best result is reached when topic number is 100, and similar experiments adding different number of words show the same condition of reaching the best result.</p>
</div>
</div>
<div id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Effect of Revised Term Frequency</h4>

<div id="S4.F3" class="ltx_figure"><img src="" id="S4.F3.g1" class="ltx_graphics ltx_centering" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Classification accuracy changing according to the redefinition of term frequency</div>
</div>
<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p">The experiment corrsponding to Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3.3 Effect of Revised Term Frequency ‣ 4.3 Parameter Tuning ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is to discover whether our redefining “term frequency” as revised term frequency in step (c) of Section <a href="#S3.SS3" title="3.3 Select Relevant Words for Microblog ‣ 3 Our Model ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> will affect the classification accuracy and how. The results should be analysed in two aspects. On one hand, without redefinition, the accuracy remains in a stable high level and tends to decrease as we add more words. One reason for the decreasing is that “noisy words” have a increasing negative impact on the accuracy as the proportion of “noisy words” grows with the number of added words. On the other hand, the best result is reached when we use the revise term frequency. This suggests that our redefinition for term frequency shows better improvement for microblog representation under certain conditions, but is not optimal under all situations.</p>
</div>
</div>
</div>
<div id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.4 </span>Case Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">In Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Parameter Tuning ‣ 4 Experiment ‣ A Novel Content Enriching Model for Microblog Using News Corpus" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we select several cases consisting of microblogs and their top relevant words .</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p">In the first case, we successfully find the country name according to its leader’s name and limited information in the sentence. Other related countries and events are also selected by our model as they often appear together in news. In the other case, relevant words are among the most frequently used words in news and have close semantic relations with the microblogs in certain aspects.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p class="ltx_p">As we can see, based on topic analysis, our model shows strong ability of mining relevant words. Other cases show that the model can be further improved by removing the noisy and meaningless ones among added words.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We propose an effective content enriching method for microblog, to enhance classification accuracy. News corpus is exploited as external knowledge. As for techniques, our method uses LDA as its topic analysis model and formulates topic inference for new data as convex optimization problems. Compared with traditional representation, enriched microblog shows great improvement in classification tasks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">As we do not control the quality of added words, our future work starts from building a filter to select better additional information. And to make the most of external knowledge, better ways to build topic space should be considered.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work is supported by National Natural Science Foundation of China (Grant No. 61170091).</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Banerjee et al.2007</span>
<span class="ltx_bibblock">
Banerjee, S., Ramanathan, K., and Gupta, A.

</span>
<span class="ltx_bibblock">2007, July.

</span>
<span class="ltx_bibblock">Clustering short texts using wikipedia.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</span> (pp. 787-788). ACM.

</span></li>
<li id="bib.bibx2" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Blei et al.2003</span>
<span class="ltx_bibblock">
Blei, D. M., Ng, A. Y., and Jordan, M. I.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock">Latent Dirichlet Allocation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Journal of machine Learning research</span>, 3, 993-1022.

</span></li>
<li id="bib.bibx3" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bollegala et al.2007</span>
<span class="ltx_bibblock">
Bollegala, D., Matsuo, Y., and Ishizuka, M.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock">Measuring semantic similarity between words using web search engines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">www, 7,</span> 757-766.

</span></li>
<li id="bib.bibx4" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Boyd and Vandenberghe2004</span>
<span class="ltx_bibblock">
Boyd, S. P., and Vandenberghe, L.

</span>
<span class="ltx_bibblock">2004.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Convex optimization</span>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span></li>
<li id="bib.bibx5" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Gabrilovich and Markovitch2007</span>
<span class="ltx_bibblock">
Gabrilovich, E., and Markovitch, S.

</span>
<span class="ltx_bibblock">2007, January.

</span>
<span class="ltx_bibblock">Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">IJCAI</span> (Vol. 7, pp. 1606-1611).

</span></li>
<li id="bib.bibx6" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Guo and Diab2012</span>
<span class="ltx_bibblock">
Guo, W., and Diab, M.

</span>
<span class="ltx_bibblock">2012, July.

</span>
<span class="ltx_bibblock">Modeling sentences in the latent space.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</span>: Long Papers-Volume 1 (pp. 864-872).

</span></li>
<li id="bib.bibx7" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Guo and Diab2012</span>
<span class="ltx_bibblock">
Guo, W., and Diab, M.

</span>
<span class="ltx_bibblock">2012, July.

</span>
<span class="ltx_bibblock">Learning the latent semantics of a concept from its definition.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</span>: Short Papers-Volume 2 (pp. 140-144).

</span></li>
<li id="bib.bibx8" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Hu et al.2009</span>
<span class="ltx_bibblock">
Hu, X., Sun, N., Zhang, C., and Chua, T. S.

</span>
<span class="ltx_bibblock">2009, November.

</span>
<span class="ltx_bibblock">Exploiting internal and external semantics for the clustering of short texts using world knowledge.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 18th ACM conference on Information and knowledge management</span> (pp. 919-928). ACM.

</span></li>
<li id="bib.bibx9" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Jones1972</span>
<span class="ltx_bibblock">
Jones, K. S.

</span>
<span class="ltx_bibblock">1972.

</span>
<span class="ltx_bibblock">A statistical interpretation of term specificity and its application in retrieval.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Journal of documentation</span>, 28(1), 11-21

</span></li>
<li id="bib.bibx10" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Phan et al.2008</span>
<span class="ltx_bibblock">
Phan, X. H., Nguyen, L. M., and Horiguchi, S.

</span>
<span class="ltx_bibblock">2008, April.

</span>
<span class="ltx_bibblock">Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-scale Data Collections.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 17th international conference on World Wide Web</span> (pp. 91-100). ACM.

</span></li>
<li id="bib.bibx11" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Sahami and Heilman2006</span>
<span class="ltx_bibblock">
Sahami, M., and Heilman, T. D.

</span>
<span class="ltx_bibblock">2006, May.

</span>
<span class="ltx_bibblock">A web-based kernel function for measuring the similarity of short text snippets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 15th international conference on World Wide Web</span> (pp. 377-386). ACM.

</span></li>
<li id="bib.bibx12" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Zubiaga and Ji2013</span>
<span class="ltx_bibblock">
Zubiaga, A., and Ji, H.

</span>
<span class="ltx_bibblock">2013, May.

</span>
<span class="ltx_bibblock">Harnessing web page directories for large-scale classification of tweets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 22nd international conference on World Wide Web companion</span> (pp. 225-226). International World Wide Web Conferences Steering Committee.

</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:42:46 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
