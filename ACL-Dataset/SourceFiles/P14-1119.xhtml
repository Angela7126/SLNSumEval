<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Automatic Keyphrase Extraction: A Survey of the State of the Art</title>
<!--Generated on Tue Jun 10 18:55:04 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Automatic Keyphrase Extraction: A Survey of the State of the Art</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Kazi Saidul Hasan 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vincent Ng
<br class="ltx_break"/>Human Language Technology Research Institute
<br class="ltx_break"/>University of Texas at Dallas
<br class="ltx_break"/>Richardson, TX 75083-0688
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">saidul,vince</span>}<span class="ltx_text ltx_font_typewriter">@hlt.utdallas.edu</span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">While automatic keyphrase extraction has been examined extensively, state-of-the-art performance on this task is still much lower than that on many core natural language processing tasks.
We present a survey of the state of the art in automatic keyphrase extraction, examining the major sources of errors
made by existing systems and discussing the challenges ahead.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Automatic keyphrase extraction concerns “the automatic selection of important and topical phrases from the body of a document” <cite class="ltx_cite">[<a href="#bib.bib12" title="Learning algorithms for keyphrase extraction" class="ltx_ref">50</a>]</cite>.
In other words, its goal
is to extract a set of phrases that are related to the main topics discussed in a given document <cite class="ltx_cite">[<a href="#bib.bib2" title="A language model approach to keyphrase extraction" class="ltx_ref">48</a>, <a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>, <a href="#bib.bib54" title="Keyphrase extraction from online news using binary integer programming" class="ltx_ref">8</a>, <a href="#bib.bib55" title="Topical keyphrase extraction from Twitter" class="ltx_ref">64</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Document keyphrases have enabled fast and accurate searching for a given document from a large text collection, and have exhibited their potential in improving many natural language processing (NLP) and
information retrieval (IR) tasks, such as text summarization <cite class="ltx_cite">[<a href="#bib.bib50" title="World Wide Web site summarization" class="ltx_ref">62</a>]</cite>, text categorization <cite class="ltx_cite">[<a href="#bib.bib6" title="A study on automatically extracted keywords in text categorization" class="ltx_ref">19</a>]</cite>, opinion mining <cite class="ltx_cite">[<a href="#bib.bib57" title="Opinion expression mining by exploiting keyphrase extraction" class="ltx_ref">2</a>]</cite>, and document indexing <cite class="ltx_cite">[<a href="#bib.bib51" title="Improving browsing in digital libraries with keyphrase indexes" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Owing to its importance, automatic
keyphrase extraction has received a lot of attention. However, the task is far from
being solved: state-of-the-art performance on keyphrase extraction is still much lower than that on many core NLP tasks <cite class="ltx_cite">[<a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">32</a>]</cite>.
Our goal in this paper is to survey
the state of the art in keyphrase extraction, examining the major sources of errors made by existing systems and discussing the challenges ahead.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Corpora</h2>

<div id="S2.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_small">Source</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_small">Dataset/Contributor</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_small">Statistics</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Documents</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Tokens/doc</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Keys/doc</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Paper abstracts</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_italic ltx_font_small">Inspec</span><span class="ltx_text ltx_font_small"> </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib9" title="Improved automatic keyword extraction given more linguistic knowledge" class="ltx_ref">20</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m1" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">2,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m2" class="ltx_Math" alttext="&lt;" display="inline"><mo>&lt;</mo></math><span class="ltx_text ltx_font_small">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">10</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span class="ltx_text ltx_font_small">Scientific papers</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">NUS corpus </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib10" title="Keyphrase extraction in scientific publications" class="ltx_ref">42</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m3" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">211</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m4" class="ltx_Math" alttext="\approx" display="inline"><mo>≈</mo></math><span class="ltx_text ltx_font_small">8K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">11</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">citeulike.org </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m5" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">180</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">SemEval-2010 </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib43" title="SemEval-2010 Task 5: Automatic keyphrase extraction from scientific articles" class="ltx_ref">27</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m6" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">284</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m7" class="ltx_Math" alttext="&gt;" display="inline"><mo>&gt;</mo></math><span class="ltx_text ltx_font_small">5K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">15</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Technical reports</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">NZDL </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m8" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">1,800</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_small">News articles</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">DUC-2001 </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib7" title="Single document keyphrase extraction using neighborhood knowledge" class="ltx_ref">53</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m9" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">308</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m10" class="ltx_Math" alttext="\approx" display="inline"><mo>≈</mo></math><span class="ltx_text ltx_font_small">900</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">8</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_italic ltx_font_small">Reuters</span><span class="ltx_text ltx_font_small"> corpus </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib6" title="A study on automatically extracted keywords in text categorization" class="ltx_ref">19</a><span class="ltx_text ltx_font_small">]</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">12,848</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">6</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Web pages</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite">Yih<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">2006</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">828</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><cite class="ltx_cite">Hammouda<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib36" title="CorePhrase: Keyphrase extraction for document clustering" class="ltx_ref">2005</a>)</cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m11" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">312</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m12" class="ltx_Math" alttext="\approx" display="inline"><mo>≈</mo></math><span class="ltx_text ltx_font_small">500</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Blogs </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a><span class="ltx_text ltx_font_small">]</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">252</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m13" class="ltx_Math" alttext="\approx" display="inline"><mo>≈</mo></math><span class="ltx_text ltx_font_small">1K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">8</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Meeting transcripts</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">ICSI </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib5" title="Unsupervised approaches for automatic keyword extraction using meeting transcripts" class="ltx_ref">30</a><span class="ltx_text ltx_font_small">]</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">161</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m14" class="ltx_Math" alttext="\approx" display="inline"><mo>≈</mo></math><span class="ltx_text ltx_font_small">1.6K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">4</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Emails</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Enron corpus </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib47" title="Generating summary keywords for emails using topics" class="ltx_ref">9</a><span class="ltx_text ltx_font_small">]</span></cite><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m15" class="ltx_Math" alttext="\ast" display="inline"><mo>∗</mo></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">14,659</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Live chats</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Library of Congress </span><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib61" title="Extracting keywords from multi-party live chats" class="ltx_ref">25</a><span class="ltx_text ltx_font_small">]</span></cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">10</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation datasets. Publicly available datasets are marked with an asterisk (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.T1.m17" class="ltx_Math" alttext="\ast" display="inline"><mo mathsize="normal" stretchy="false">∗</mo></math>).</div>
</div>
<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Automatic keyphrase extraction systems have been evaluated on corpora
from a variety of sources ranging from long scientific publications to short paper abstracts and email messages. Table <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a listing of the corpora grouped by their sources as well as their statistics.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>Many of the publicly available corpora can be found in
http://github.com/snkim/AutomaticKeyphraseExtraction/ and
http://code.google.com/p/maui-indexer/downloads/list.</span></span></span>
There are at least four corpus-related factors that affect the difficulty of
keyphrase extraction.</p>
</div>
<div id="S2.SS0.SSS0.P1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Length</h5>

<div id="S2.SS0.SSS0.P1.p1" class="ltx_para">
<p class="ltx_p">The difficulty of the task increases with the length of the input document as longer documents yield more candidate keyphrases (i.e., phrases that are eligible to be keyphrases
(see Section <a href="#S3.SS1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>)).
For instance, each <span class="ltx_text ltx_font_italic">Inspec</span> abstract has on average 10 annotator-assigned keyphrases and 34 candidate keyphrases. In contrast, a scientific paper typically has at least 10 keyphrases and hundreds of candidate keyphrases, yielding a much bigger search space <cite class="ltx_cite">[<a href="#bib.bib52" title="Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art" class="ltx_ref">16</a>]</cite>. Consequently, it is harder to extract keyphrases from scientific papers, technical reports, and meeting transcripts than abstracts, emails, and news articles.</p>
</div>
</div>
<div id="S2.SS0.SSS0.P2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Structural consistency</h5>

<div id="S2.SS0.SSS0.P2.p1" class="ltx_para">
<p class="ltx_p">In a structured document, there are certain locations where a keyphrase is most likely to appear. For instance, most of a scientific paper’s keyphrases should appear in the abstract and the introduction. While structural information has been exploited to extract keyphrases from scientific papers (e.g., title, section information) <cite class="ltx_cite">[<a href="#bib.bib1" title="Automatic keyphrase extraction from scientific articles" class="ltx_ref">28</a>]</cite>, web pages (e.g., metadata) <cite class="ltx_cite">[<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>]</cite>, and chats (e.g., dialogue acts) <cite class="ltx_cite">[<a href="#bib.bib61" title="Extracting keywords from multi-party live chats" class="ltx_ref">25</a>]</cite>, it is most useful when the documents from a source exhibit structural similarity. For this reason, structural information is likely to facilitate keyphrase extraction from scientific papers and technical reports
because of their standard format (i.e., standard sections such as abstract, introduction, conclusion, etc.). In contrast, the lack of structural consistency in other types of structured documents (e.g., web pages, which can be blogs, forums, or reviews) may render structural information less useful.</p>
</div>
</div>
<div id="S2.SS0.SSS0.P3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Topic change</h5>

<div id="S2.SS0.SSS0.P3.p1" class="ltx_para">
<p class="ltx_p">An observation commonly exploited in keyphrase extraction from
scientific articles and news articles is that
keyphrases typically appear not only at the beginning
<cite class="ltx_cite">[<a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>]</cite> but also at the end
<cite class="ltx_cite">[<a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite> of a document.
This observation does not necessarily hold
for conversational text (e.g., meetings, chats), however.
The reason is simple: in a conversation, the topics (i.e., its talking points)
change as the interaction moves forward in time, and so do the keyphrases
associated with a topic.
One way to address this complication is to detect a topic change in conversational text <cite class="ltx_cite">[<a href="#bib.bib61" title="Extracting keywords from multi-party live chats" class="ltx_ref">25</a>]</cite>.
However, topic change detection is not always easy: while the topics listed
in the form of an agenda at the beginning of formal meeting transcripts
can be exploited, such clues are absent in casual conversations (e.g.,
chats).</p>
</div>
</div>
<div id="S2.SS0.SSS0.P4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Topic correlation</h5>

<div id="S2.SS0.SSS0.P4.p1" class="ltx_para">
<p class="ltx_p">Another observation commonly exploited in keyphrase extraction from
scientific articles and news articles is that the keyphrases in a document
are typically <span class="ltx_text ltx_font_italic">related</span>
to each other <cite class="ltx_cite">[<a href="#bib.bib20" title="Coherent keyphrase extraction via web mining" class="ltx_ref">51</a>, <a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>]</cite>.
However, this observation does not necessarily hold
for informal text (e.g., emails, chats, informal meetings, personal blogs),
where people can talk about any number of potentially uncorrelated topics.
The presence of uncorrelated topics implies that it may no longer be
possible to exploit relatedness and therefore increases the difficulty
of keyphrase extraction.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Keyphrase Extraction Approaches</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">A keyphrase extraction system typically operates in two steps: (1) extracting a list of words/phrases that serve as <span class="ltx_text ltx_font_italic">candidate keyphrases</span> using some heuristics (Section <a href="#S3.SS1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>); and (2) determining which of these candidate keyphrases are correct keyphrases using supervised (Section <a href="#S3.SS2" title="3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) or unsupervised (Section <a href="#S3.SS3" title="3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) approaches.</p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Selecting Candidate Words and Phrases</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">As noted before, a set of phrases and words is typically extracted as candidate keyphrases using heuristic rules. These rules are designed to avoid spurious instances and keep the number of candidates to a minimum. Typical heuristics include (1) using a stop word list to remove stop words <cite class="ltx_cite">[<a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>]</cite>, (2) allowing words with certain part-of-speech tags (e.g., nouns, adjectives, verbs) to be candidate keywords <cite class="ltx_cite">[<a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>, <a href="#bib.bib7" title="Single document keyphrase extraction using neighborhood knowledge" class="ltx_ref">53</a>, <a href="#bib.bib5" title="Unsupervised approaches for automatic keyword extraction using meeting transcripts" class="ltx_ref">30</a>]</cite>, (3) allowing n-grams that appear in Wikipedia article titles to be candidates <cite class="ltx_cite">[<a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a>]</cite>, and (4) extracting n-grams <cite class="ltx_cite">[<a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>, <a href="#bib.bib9" title="Improved automatic keyword extraction given more linguistic knowledge" class="ltx_ref">20</a>, <a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite> or noun phrases <cite class="ltx_cite">[<a href="#bib.bib25" title="Using noun phrase heads to extract document keyphrases" class="ltx_ref">1</a>, <a href="#bib.bib31" title="Domain-specific keyphrase extraction" class="ltx_ref">57</a>]</cite> that satisfy pre-defined lexico-syntactic pattern(s) <cite class="ltx_cite">[<a href="#bib.bib44" title="An ontology-based approach for key phrase extraction" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Many of these heuristics have proven effective with their high recall in extracting gold keyphrases from various sources. However, for a long document, the resulting list of candidates can be long. Consequently, different <span class="ltx_text ltx_font_italic">pruning</span> heuristics have been designed to prune candidates that are unlikely to be keyphrases <cite class="ltx_cite">[<a href="#bib.bib29" title="Keyphrase extraction using semantic networks structure analysis" class="ltx_ref">17</a>, <a href="#bib.bib18" title="Automatic keyphrase extraction from scientific documents using n-gram filtration technique" class="ltx_ref">29</a>, <a href="#bib.bib26" title="KP-Miner: A keyphrase extraction system for English and Arabic documents" class="ltx_ref">10</a>, <a href="#bib.bib19" title="Automatic keyphrase extraction with a refined candidate set" class="ltx_ref">59</a>, <a href="#bib.bib59" title="Bayesian text segmentation for index term identification and keyphrase extraction" class="ltx_ref">40</a>]</cite>.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Supervised Approaches</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">Research on supervised approaches to keyphrase extraction has focused on two issues: <span class="ltx_text ltx_font_italic">task reformulation</span> and <span class="ltx_text ltx_font_italic">feature design</span>.</p>
</div>
<div id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Task Reformulation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">Early supervised approaches to keyphrase extraction recast this task as a
binary <span class="ltx_text ltx_font_italic">classification</span> problem <cite class="ltx_cite">[<a href="#bib.bib13" title="Domain-specific keyphrase extraction" class="ltx_ref">12</a>, <a href="#bib.bib23" title="Learning to extract keyphrases from text" class="ltx_ref">49</a>, <a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>, <a href="#bib.bib12" title="Learning algorithms for keyphrase extraction" class="ltx_ref">50</a>]</cite>.
The goal is to train a classifier on documents annotated with keyphrases to determine whether a candidate phrase is a keyphrase. Keyphrases and non-keyphrases are used to generate positive and negative examples, respectively.
Different learning algorithms have been used to train this classifier, including naïve Bayes <cite class="ltx_cite">[<a href="#bib.bib13" title="Domain-specific keyphrase extraction" class="ltx_ref">12</a>, <a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>]</cite>, decision trees <cite class="ltx_cite">[<a href="#bib.bib23" title="Learning to extract keyphrases from text" class="ltx_ref">49</a>, <a href="#bib.bib12" title="Learning algorithms for keyphrase extraction" class="ltx_ref">50</a>]</cite>, bagging <cite class="ltx_cite">[<a href="#bib.bib9" title="Improved automatic keyword extraction given more linguistic knowledge" class="ltx_ref">20</a>]</cite>, boosting <cite class="ltx_cite">[<a href="#bib.bib30" title="Automatic keyword extraction using domain knowledge" class="ltx_ref">18</a>]</cite>, maximum entropy <cite class="ltx_cite">[<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>, <a href="#bib.bib24" title="Re-examining automatic keyphrase extraction approaches in scientific articles" class="ltx_ref">26</a>]</cite>, multi-layer perceptron <cite class="ltx_cite">[<a href="#bib.bib46" title="HUMB: Automatic key term extraction from scientific articles in GROBID" class="ltx_ref">35</a>]</cite>, and support vector machines <cite class="ltx_cite">[<a href="#bib.bib22" title="A ranking approach to keyphrase extraction" class="ltx_ref">22</a>, <a href="#bib.bib46" title="HUMB: Automatic key term extraction from scientific articles in GROBID" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p">Recasting keyphrase extraction as a classification problem has its weaknesses, however. Recall that the goal of keyphrase extraction is to identify the most representative phrases for a document. In other words, if a candidate phrase <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.SSS1.p2.m1" class="ltx_Math" alttext="c_{1}" display="inline"><msub><mi>c</mi><mn>1</mn></msub></math> is more representative than another candidate phrase <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.SSS1.p2.m2" class="ltx_Math" alttext="c_{2}" display="inline"><msub><mi>c</mi><mn>2</mn></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.SSS1.p2.m3" class="ltx_Math" alttext="c_{1}" display="inline"><msub><mi>c</mi><mn>1</mn></msub></math> should be preferred to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.SSS1.p2.m4" class="ltx_Math" alttext="c_{2}" display="inline"><msub><mi>c</mi><mn>2</mn></msub></math>. Note that a binary classifier classifies each candidate keyphrase independently of the others, and consequently it does not allow us to determine which candidates are better than the others <cite class="ltx_cite">[<a href="#bib.bib45" title="Enhancing linguistically oriented automatic keyword extraction" class="ltx_ref">21</a>, <a href="#bib.bib60" title="CoRankBayes: Bayesian learning to rank under the co-training framework and its application in keyphrase extraction" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p class="ltx_p">Motivated by this observation, <cite class="ltx_cite">Jiang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib22" title="A ranking approach to keyphrase extraction" class="ltx_ref">2009</a>)</cite> propose a <span class="ltx_text ltx_font_italic">ranking</span> approach to keyphrase extraction, where the goal is to learn a ranker to rank two candidate keyphrases. This pairwise ranking approach therefore introduces competition between candidate keyphrases, and has been shown to significantly outperform KEA <cite class="ltx_cite">[<a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>, <a href="#bib.bib13" title="Domain-specific keyphrase extraction" class="ltx_ref">12</a>]</cite>, a popular supervised baseline that adopts the traditional supervised classification approach <cite class="ltx_cite">[<a href="#bib.bib38" title="KPSpotter: A flexible information gain-based keyphrase extraction system" class="ltx_ref">46</a>, <a href="#bib.bib39" title="Automatic hypertext keyphrase detection" class="ltx_ref">23</a>]</cite>.</p>
</div>
</div>
<div id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Features</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p">The features commonly used to represent an instance for supervised keyphrase extraction can be broadly divided into two categories.</p>
</div><span class="ltx_ERROR undefined">\smallplus</span>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">3.2.2.1 Within-Collection Features</span></p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p class="ltx_p">Within-collection features are computed based solely
on the training documents. These features can be further divided into three types.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Statistical features</span> are computed based on statistical information gathered from the training documents. Three such features have been extensively used in supervised approaches. The first one, <span class="ltx_text ltx_font_italic">tf*idf</span> <cite class="ltx_cite">[<a href="#bib.bib32" title="Term-weighting approaches in automatic text retrieval" class="ltx_ref">45</a>]</cite>, is computed based on candidate frequency in the given text and inverse document frequency (i.e., number of other documents where the candidate appears).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>A tf*idf-based baseline, where candidate keyphrases are ranked and selected according to tf*idf, has been widely used by both supervised and unsupervised approaches <cite class="ltx_cite">[<a href="#bib.bib41" title="Narrative text classification for automatic key phrase extraction in web document corpora" class="ltx_ref">63</a>, <a href="#bib.bib47" title="Generating summary keywords for emails using topics" class="ltx_ref">9</a>, <a href="#bib.bib27" title="A language-independent approach to keyphrase extraction and evaluation" class="ltx_ref">44</a>, <a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a>]</cite>.</span></span></span> The second one, the <span class="ltx_text ltx_font_italic">distance</span> of a phrase, is defined as the number of words preceding its first occurrence normalized by the number of words in the document. Its usefulness stems from the fact that keyphrases tend to appear early in a document. The third one, <span class="ltx_text ltx_font_italic">supervised keyphraseness</span>, encodes the number of times a phrase appears as a keyphrase in the training set. This feature is designed based on the assumption that a phrase frequently tagged as a keyphrase is more likely to be a keyphrase in an unseen document. These three features form the feature set of KEA <cite class="ltx_cite">[<a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>, <a href="#bib.bib13" title="Domain-specific keyphrase extraction" class="ltx_ref">12</a>]</cite>, and have been shown to perform consistently well on documents from various sources <cite class="ltx_cite">[<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>, <a href="#bib.bib1" title="Automatic keyphrase extraction from scientific articles" class="ltx_ref">28</a>]</cite>. Other statistical features include <span class="ltx_text ltx_font_italic">phrase length</span> and <span class="ltx_text ltx_font_italic">spread</span> (i.e., the number of words between the first and last occurrences of a phrase in the document).</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Structural features</span> encode how different instances of a candidate keyphrase are located in different parts of a document. A phrase is more likely to be a keyphrase if it appears in the abstract or introduction of a paper or in the metadata section of a web page. In fact, features that encode how frequently a candidate keyphrase occurs in various sections of a scientific paper (e.g., introduction, conclusion) <cite class="ltx_cite">[<a href="#bib.bib10" title="Keyphrase extraction in scientific publications" class="ltx_ref">42</a>]</cite> and those that encode the location of a candidate keyphrase in a web page (e.g., whether it appears in the title) <cite class="ltx_cite">[<a href="#bib.bib40" title="A practical system of keyphrase extraction for web pages" class="ltx_ref">7</a>, <a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>]</cite> have been shown to be useful for the task.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Syntactic features</span> encode the syntactic patterns of a candidate keyphrase. For example, a candidate keyphrase has been encoded as (1) a <span class="ltx_text ltx_font_italic">PoS tag sequence</span>, which denotes the sequence of part-of-speech tag(s) assigned to its word(s); and (2) a <span class="ltx_text ltx_font_italic">suffix sequence</span>, which is the sequence of morphological suffixes of its words
<cite class="ltx_cite">[<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>, <a href="#bib.bib10" title="Keyphrase extraction in scientific publications" class="ltx_ref">42</a>, <a href="#bib.bib24" title="Re-examining automatic keyphrase extraction approaches in scientific articles" class="ltx_ref">26</a>]</cite>.
However, ablation studies conducted on web pages <cite class="ltx_cite">[<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">58</a>]</cite> and
scientific articles <cite class="ltx_cite">[<a href="#bib.bib24" title="Re-examining automatic keyphrase extraction approaches in scientific articles" class="ltx_ref">26</a>]</cite> reveal that syntactic features are not
useful for keyphrase extraction in the presence of other feature types.</p>
</div><span class="ltx_ERROR undefined">\smallplus</span>
<div id="S3.SS2.SSS2.p7" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">3.2.2.2 External Resource-Based Features</span></p>
</div>
<div id="S3.SS2.SSS2.p8" class="ltx_para">
<p class="ltx_p">External resource-based features are computed based on information
gathered from resources other than the training documents, such as lexical
knowledge bases (e.g., Wikipedia) or the Web, with the goal of improving
keyphrase extraction performance by exploiting external knowledge.
Below we give an overview of the external resource-based features
that have proven useful for keyphrase extraction.</p>
</div>
<div id="S3.SS2.SSS2.p9" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Wikipedia-based keyphraseness</span> is computed as a candidate’s document frequency multiplied by the ratio of the number of Wikipedia articles where the candidate appears as a link to the number of articles where it appears <cite class="ltx_cite">[<a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite>.
This feature is motivated by the observation that a candidate
is likely to be a keyphrase if it occurs frequently as a link in Wikipedia. Unlike supervised keyphraseness, Wikipedia-based keyphraseness can be computed without using documents annotated with keyphrases and can work even if there is a mismatch between the training domain and the test domain.</p>
</div>
<div id="S3.SS2.SSS2.p10" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Yih<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib37" title="Finding advertising keywords on web pages" class="ltx_ref">2006</a>)</cite> employ a feature that encodes
whether a candidate keyphrase appears in the <span class="ltx_text ltx_font_italic">query log</span> of a search engine, exploiting the observation that a candidate is potentially important if it was used as a search query.
Terminological databases have been similarly exploited to encode the salience
of candidate keyphrases in scientific papers <cite class="ltx_cite">[<a href="#bib.bib46" title="HUMB: Automatic key term extraction from scientific articles in GROBID" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS2.p11" class="ltx_para">
<p class="ltx_p">While the aforementioned external resource-based features attempt to encode
how salient a candidate keyphrase is,
<cite class="ltx_cite">Turney (<a href="#bib.bib20" title="Coherent keyphrase extraction via web mining" class="ltx_ref">2003</a>)</cite> proposes features that encode the semantic relatedness
between two candidate keyphrases.
Noting that candidate keyphrases that
are not semantically related to the predicted keyphrases are unlikely to be
keyphrases in technical reports, Turney employs
<span class="ltx_text ltx_font_italic">coherence features</span> to identify such candidate keyphrases.
Semantic relatedness is encoded in the coherence features as
two candidate keyphrases’ pointwise mutual information, which Turney computes
by using the Web as a corpus.</p>
</div>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Unsupervised Approaches</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">Existing unsupervised approaches to keyphrase extraction can be categorized into four groups.</p>
</div>
<div id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Graph-Based Ranking</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p">Intuitively, keyphrase extraction is about finding the important words and phrases from a document. Traditionally, the <span class="ltx_text ltx_font_italic">importance</span> of a candidate has often been defined in terms of how related it is to other candidates in the document. Informally, a candidate is important if it is related to (1) a large number of candidates and (2) candidates that are important.
Researchers have computed <span class="ltx_text ltx_font_italic">relatedness</span> between candidates using co-occurrence counts <cite class="ltx_cite">[<a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>, <a href="#bib.bib17" title="Keyword extraction from a single document using word co-occurrence statistical information" class="ltx_ref">36</a>]</cite> and semantic relatedness <cite class="ltx_cite">[<a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a>]</cite>, and represented the relatedness information collected from a document as a graph
<cite class="ltx_cite">[<a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>, <a href="#bib.bib28" title="CollabRank: Towards a collaborative approach to single-document keyphrase extraction" class="ltx_ref">52</a>, <a href="#bib.bib7" title="Single document keyphrase extraction using neighborhood knowledge" class="ltx_ref">53</a>, <a href="#bib.bib65" title="TopicRank: graph-based topic ranking for keyphrase extraction" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p">The basic idea behind a graph-based approach is to build a graph from the input document and rank its nodes according to their importance using a graph-based ranking method (e.g., <cite class="ltx_cite">Brin and Page (<a href="#bib.bib14" title="The anatomy of a large-scale hypertextual Web search engine" class="ltx_ref">1998</a>)</cite>). Each node of the graph corresponds to a candidate keyphrase from the document and an edge connects two <span class="ltx_text ltx_font_italic">related</span> candidates. The edge weight is proportional to the syntactic and/or semantic relevance between the connected candidates. For each node, each of its edges is treated as a “vote” from the other node connected by the edge. A node’s score in the graph is defined recursively in terms of the edges it has and the scores of the neighboring nodes. The top-ranked candidates from the graph are then selected as keyphrases for the input document. TextRank <cite class="ltx_cite">[<a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>]</cite> is one of the most well-known graph-based approaches to keyphrase extraction.</p>
</div>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p class="ltx_p">This instantiation of a graph-based approach overlooks an important aspect of keyphrase extraction, however. A set of keyphrases for a document should ideally cover the main topics discussed in it, but this instantiation
does not guarantee that all the main topics will be represented by the extracted keyphrases. Despite this weakness, a graph-based representation of text was adopted by many approaches that propose different ways of computing the similarity between two candidates.</p>
</div>
</div>
<div id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Topic-Based Clustering</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p">Another unsupervised approach to keyphrase extraction involves grouping the candidate keyphrases in a document into <span class="ltx_text ltx_font_italic">topics</span>, such that each topic is composed of all and only those candidate keyphrases that are related to that topic <cite class="ltx_cite">[<a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a>, <a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>, <a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">32</a>]</cite>. There are several motivations behind this topic-based clustering approach. First, a keyphrase should ideally be relevant to one or more main topic(s) discussed in a document <cite class="ltx_cite">[<a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">32</a>, <a href="#bib.bib58" title="Topical word trigger model for keyphrase extraction" class="ltx_ref">34</a>]</cite>. Second, the extracted keyphrases should be comprehensive in the sense that they should cover all the main topics in a document <cite class="ltx_cite">[<a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>, <a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">32</a>, <a href="#bib.bib58" title="Topical word trigger model for keyphrase extraction" class="ltx_ref">34</a>]</cite>.
Below we examine three representative systems that adopt
this approach.</p>
</div>
<div id="S3.SS3.SSS2.P1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">KeyCluster</h5>

<div id="S3.SS3.SSS2.P1.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">2009b</a>)</cite> adopt a clustering-based approach (henceforth KeyCluster) that clusters
semantically similar candidates using Wikipedia and co-occurrence-based statistics. The underlying hypothesis is that each of these clusters corresponds to a topic covered in the document, and selecting the candidates close to the centroid of each cluster as keyphrases ensures that the resulting set of keyphrases covers all the topics of the document.</p>
</div>
<div id="S3.SS3.SSS2.P1.p2" class="ltx_para">
<p class="ltx_p">While empirical results show that KeyCluster performs better than both TextRank and Hulth’s <cite class="ltx_cite">[<a href="#bib.bib9" title="Improved automatic keyword extraction given more linguistic knowledge" class="ltx_ref">20</a>]</cite> supervised system, KeyCluster has a potential drawback: by extracting keyphrases from each topic cluster, it essentially gives each topic equal importance. In practice, however, there could be topics that are not important and these topics should not have keyphrase(s) representing them.</p>
</div>
</div>
<div id="S3.SS3.SSS2.P2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Topical PageRank (TPR)</h5>

<div id="S3.SS3.SSS2.P2.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">2010</a>)</cite> propose TPR, an approach that overcomes the aforementioned weakness of KeyCluster. It runs TextRank multiple times for a document, once for each of its topics induced by
a Latent Dirichlet Allocation <cite class="ltx_cite">[<a href="#bib.bib66" title="Latent Dirichlet allocation" class="ltx_ref">3</a>]</cite>.
By running TextRank once for each topic,
TPR ensures that the extracted keyphrases cover the main topics of the document.
The final score of a candidate is computed as the sum of its scores for each of the topics, weighted by the probability of that topic in that document.
Hence, unlike KeyCluster, candidates belonging to a less probable topic are given less importance.</p>
</div>
<div id="S3.SS3.SSS2.P2.p2" class="ltx_para">
<p class="ltx_p">TPR performs significantly better than both tf*idf and TextRank on the DUC-2001 and <span class="ltx_text ltx_font_italic">Inspec</span> datasets. TPR’s superior performance strengthens the hypothesis of using topic clustering for keyphrase extraction. However, though TPR is conceptually better than KeyCluster, Liu et al. did not compare TPR against KeyCluster.</p>
</div>
</div>
<div id="S3.SS3.SSS2.P3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CommunityCluster</h5>

<div id="S3.SS3.SSS2.P3.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Grineva<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">2009</a>)</cite> propose CommunityCluster, a variant of the topic clustering approach to keyphrase extraction. Like TPR, CommunityCluster gives more weight to more important topics, but unlike TPR, it extracts <span class="ltx_text ltx_font_italic">all</span> candidate keyphrases from an important topic, assuming that a candidate that receives little focus in the text should still be extracted as a keyphrase as long as it is related to an important topic.
CommunityCluster yields much
better recall (without losing precision) than extractors such as tf*idf, TextRank, and the Yahoo! term extractor.</p>
</div>
</div>
</div>
<div id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Simultaneous Learning</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p">Since keyphrases represent a dense summary of a document, researchers hypothesized that text summarization and keyphrase extraction can potentially benefit from each other if these tasks are performed simultaneously.
<cite class="ltx_cite">Zha (<a href="#bib.bib16" title="Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering" class="ltx_ref">2002</a>)</cite> proposes the first graph-based approach for simultaneous summarization and keyphrase extraction, motivated by a key observation:
a sentence is important if it contains important words, and important words appear in important sentences. <cite class="ltx_cite">Wan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib15" title="Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction" class="ltx_ref">2007</a>)</cite> extend Zha’s work by adding two assumptions: (1) an important sentence is connected to other important sentences, and (2) an important word is linked to other important words, a TextRank-like assumption. Based on these
assumptions, <cite class="ltx_cite">Wan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib15" title="Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction" class="ltx_ref">2007</a>)</cite> build three graphs to capture the association between the sentences (S) and the words (W) in an input document, namely, a S–S graph, a bipartite S–W graph, and a W–W graph. The weight of an edge connecting two sentence nodes in a S–S graph corresponds to their content similarity. An edge weight in a S–W graph denotes the word’s importance in the sentence it appears. Finally, an edge weight in a W–W graph denotes the
co-occurrence or knowledge-based similarity between the two connected words. Once the graphs are constructed for an input document, an iterative reinforcement algorithm is applied to assign scores to each sentence and word. The top-scored words are used to form keyphrases.</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p">The main advantage of this approach is that it combines the strengths of both Zha’s approach (i.e., bipartite S–W graphs) and TextRank (i.e., W–W graphs) and performs better than both of them.
However, it has a weakness: like TextRank, it does not ensure that the extracted keyphrases will cover all the main topics. To address this problem, one can employ a topic clustering algorithm on the W–W graph to produce the topic clusters, and then ensure
that keyphrases are chosen from every main topic cluster.</p>
</div>
</div>
<div id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Language Modeling</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p class="ltx_p">Many existing approaches have a separate, heuristic module for extracting candidate keyphrases prior to keyphrase ranking/extraction. In contrast, <cite class="ltx_cite">Tomokiyo and Hurst (<a href="#bib.bib2" title="A language model approach to keyphrase extraction" class="ltx_ref">2003</a>)</cite> propose an approach (henceforth LMA) that combines these two steps.</p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p class="ltx_p">LMA scores a candidate keyphrase based on two features, namely, <span class="ltx_text ltx_font_italic">phraseness</span> (i.e., the extent to which a word sequence can be treated as a phrase) and <span class="ltx_text ltx_font_italic">informativeness</span> (i.e., the extent to which a word sequence captures the central idea of the document it appears in). Intuitively, a phrase that has high scores for phraseness and informativeness is likely to be a keyphrase. These feature values are estimated using language models (LMs) trained on a <span class="ltx_text ltx_font_italic">foreground</span> corpus and a <span class="ltx_text ltx_font_italic">background</span> corpus. The foreground corpus is composed of the set of documents from which keyphrases are to be extracted. The background corpus is a large corpus that encodes general knowledge about the world (e.g., the Web). A unigram LM and an n-gram LM are constructed for each of these two corpora. Phraseness, defined using the foreground LM, is calculated as the loss of information incurred as a result of assuming a unigram LM (i.e., conditional independence among the words of the phrase) instead of an n-gram LM (i.e., the phrase is drawn from an n-gram LM). Informativeness is computed as the loss that results because of the assumption that the candidate is sampled from the background LM rather than the foreground LM. The loss values are computed using Kullback-Leibler divergence.
Candidates are ranked according to the sum of these two feature values.</p>
</div>
<div id="S3.SS3.SSS4.p3" class="ltx_para">
<p class="ltx_p">In sum, LMA uses a language model rather than heuristics to identify phrases, and relies on the language model trained on the background corpus to determine how “unique” a candidate keyphrase is to the domain represented by the foreground corpus. The more unique it is to the foreground’s domain, the more likely it is a keyphrase for that domain. While the use of language models to identify phrases cannot be considered a major strength of this approach (because heuristics can identify phrases fairly reliably), the use of a background corpus to identify candidates that are unique to the foreground’s domain is a unique aspect of this approach. We believe that this idea deserves further investigation, as it would allow us to discover a keyphrase that is unique to the foreground’s domain but may have a low tf*idf value.</p>
</div>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we describe metrics for evaluating keyphrase extraction systems as well as state-of-the-art results on commonly-used datasets.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">Designing evaluation metrics for keyphrase extraction is by no means an easy task.
To score the output of a keyphrase extraction system, the typical approach, which is also adopted by the SemEval-2010 shared task on keyphrase extraction, is (1) to create a mapping between the keyphrases in the gold standard and those in the system output using <span class="ltx_text ltx_font_italic">exact match</span>, and then (2) score the output using evaluation metrics such as precision (P), recall (R), and F-score (F).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">Conceivably, exact match is an overly strict condition, considering a predicted keyphrase incorrect even if it is a variant of a gold keyphrase. For instance, given the gold keyphrase “neural network”, exact match will consider a predicted phrase incorrect even if it is an expanded version of the gold keyphrase (“artificial neural network”) or one of its morphological (“neural networks”) or lexical (“neural net”) variants. While morphological variations can be handled using a stemmer <cite class="ltx_cite">[<a href="#bib.bib26" title="KP-Miner: A keyphrase extraction system for English and Arabic documents" class="ltx_ref">10</a>]</cite>, other variations may not be handled easily and reliably.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">Human evaluation has been suggested as a possibility <cite class="ltx_cite">[<a href="#bib.bib17" title="Keyword extraction from a single document using word co-occurrence statistical information" class="ltx_ref">36</a>]</cite>, but it is time-consuming and expensive. For this reason,
researchers have experimented with two types of automatic evaluation metrics. The first type of metrics addresses the problem with exact match. These metrics reward a partial match between a predicted keyphrase and a gold keyphrase (i.e., overlapping <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams) and are commonly used in machine translation (MT) and summarization evaluations. They include <span class="ltx_text ltx_font_smallcaps">Bleu</span>, <span class="ltx_text ltx_font_smallcaps">Meteor</span>, <span class="ltx_text ltx_font_smallcaps">Nist</span>, and <span class="ltx_text ltx_font_smallcaps">Rouge</span>.
Nevertheless, experiments show that these MT metrics only offer a partial solution to problem with exact match: they can only detect a subset of the near-misses <cite class="ltx_cite">[<a href="#bib.bib35" title="Evaluating n-gram based evaluation metrics for automatic keyphrase extraction" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p">The second type of metrics focuses on how a system ranks its predictions.
Given that two systems <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m1" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m2" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math> have the same number of correct predictions,
binary preference measure (Bpref) and mean reciprocal rank (MRR) <cite class="ltx_cite">[<a href="#bib.bib42" title="Automatic keyphrase extraction via topic decomposition" class="ltx_ref">32</a>]</cite> will award more credit to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m3" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> than to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m4" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math>
if the ranks of the correct predictions in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m5" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>’s output are higher than those in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m6" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math>’s output.
R-precision (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m7" class="ltx_Math" alttext="R_{p}" display="inline"><msub><mi>R</mi><mi>p</mi></msub></math>) is an IR metric that focuses on ranking: given a document with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m8" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> gold keyphrases, it computes the precision of a system over its <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m9" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> highest-ranked candidates <cite class="ltx_cite">[<a href="#bib.bib34" title="Approximate matching for evaluating keyphrase extraction" class="ltx_ref">60</a>]</cite>. The motivation behind the design of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m10" class="ltx_Math" alttext="R_{p}" display="inline"><msub><mi>R</mi><mi>p</mi></msub></math> is simple: a system will achieve a perfect <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p4.m11" class="ltx_Math" alttext="R_{p}" display="inline"><msub><mi>R</mi><mi>p</mi></msub></math> value if it ranks all the keyphrases above the non-keyphrases.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>The State of the Art</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists the best scores on some popular evaluation datasets and the corresponding systems. For example, the best F-scores on
the <span class="ltx_text ltx_font_italic">Inspec</span> test set, the DUC-2001 dataset, and the SemEval-2010 test set are 45.7, 31.7, and 27.5, respectively.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>A more detailed analysis of the results of the SemEval-2010 shared task and
the approaches adopted by the participating systems can be found in
<cite class="ltx_cite">Kim<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="Automatic keyphrase extraction from scientific articles" class="ltx_ref">2013</a>)</cite>.</span></span></span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">Two points deserve mention. First, F-scores decrease as document length increases.
These results are consistent with the observation we made in Section 2 that
it is more difficult to extract keyphrases correctly from longer documents. Second, recent unsupervised approaches have rivaled their supervised counterparts in performance <cite class="ltx_cite">[<a href="#bib.bib4" title="TextRank: Bringing order into texts" class="ltx_ref">38</a>, <a href="#bib.bib26" title="KP-Miner: A keyphrase extraction system for English and Arabic documents" class="ltx_ref">10</a>, <a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>]</cite>. For example, KP-Miner <cite class="ltx_cite">[<a href="#bib.bib53" title="KP-Miner: Participation in SemEval-2" class="ltx_ref">11</a>]</cite>, an unsupervised system, ranked third in the SemEval-2010 shared task with an F-score of 25.2, which is comparable to the best supervised system scoring 27.5.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">With the goal of providing directions for future work,
we identify the errors commonly made by
state-of-the-art keyphrase extractors below.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Error Analysis</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">Although a few researchers have presented a sample of their systems’ output and the corresponding gold keyphrases to show the differences between them <cite class="ltx_cite">[<a href="#bib.bib21" title="KEA: Practical automatic keyphrase extraction" class="ltx_ref">56</a>, <a href="#bib.bib10" title="Keyphrase extraction in scientific publications" class="ltx_ref">42</a>, <a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite>, a systematic analysis of the major types of errors made by state-of-the-art keyphrase extraction systems is missing.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">To fill this gap, we ran four keyphrase extraction systems on four commonly-used datasets of varying sources, including
<span class="ltx_text ltx_font_italic">Inspec</span> abstracts <cite class="ltx_cite">[<a href="#bib.bib9" title="Improved automatic keyword extraction given more linguistic knowledge" class="ltx_ref">20</a>]</cite>, DUC-2001 news articles <cite class="ltx_cite">[<a href="#bib.bib8" title="Introduction to DUC-2001: An intrinsic evaluation of generic news text summarization systems" class="ltx_ref">43</a>]</cite>, scientific papers <cite class="ltx_cite">[<a href="#bib.bib43" title="SemEval-2010 Task 5: Automatic keyphrase extraction from scientific articles" class="ltx_ref">27</a>]</cite>, and meeting transcripts <cite class="ltx_cite">[<a href="#bib.bib5" title="Unsupervised approaches for automatic keyword extraction using meeting transcripts" class="ltx_ref">30</a>]</cite>.
Specifically, we randomly selected 25 documents from each of these four datasets and manually analyzed the output of the four systems, including tf*idf, the most frequently used baseline, as well as three state-of-the-art keyphrase extractors, of which two are unsupervised <cite class="ltx_cite">[<a href="#bib.bib7" title="Single document keyphrase extraction using neighborhood knowledge" class="ltx_ref">53</a>, <a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>]</cite> and one is supervised <cite class="ltx_cite">[<a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S5.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_small">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_small">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Approach and System</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">[Supervised?]</span></td></tr>
</table></span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_small">Score</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">P</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">R</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_small">F</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Abstracts</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">(</span><span class="ltx_text ltx_font_italic ltx_font_small">Inspec</span><span class="ltx_text ltx_font_small">)</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Topic clustering</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a><span class="ltx_text ltx_font_small">]</span></cite><span class="ltx_text ltx_font_small"> [</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m1" class="ltx_Math" alttext="\times" display="inline"><mo>×</mo></math><span class="ltx_text ltx_font_small">]</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">35.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">66.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">45.7</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Blogs</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Topic community detection</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a><span class="ltx_text ltx_font_small">]</span></cite><span class="ltx_text ltx_font_small"> [</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m2" class="ltx_Math" alttext="\times" display="inline"><mo>×</mo></math><span class="ltx_text ltx_font_small">]</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">35.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">61.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">44.7</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">News</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">(DUC</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">-2001)</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Graph-based ranking</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">for extended neighborhood</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib7" title="Single document keyphrase extraction using neighborhood knowledge" class="ltx_ref">53</a><span class="ltx_text ltx_font_small">]</span></cite><span class="ltx_text ltx_font_small"> [</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m3" class="ltx_Math" alttext="\times" display="inline"><mo>×</mo></math><span class="ltx_text ltx_font_small">]</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">28.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">35.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">31.7</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Papers</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">(SemEval</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">-2010)</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table class="ltx_tabular ltx_align_middle">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Statistical, semantic, and</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">distributional features</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><cite class="ltx_cite"><span class="ltx_text ltx_font_small">[</span><a href="#bib.bib46" title="HUMB: Automatic key term extraction from scientific articles in GROBID" class="ltx_ref">35</a><span class="ltx_text ltx_font_small">]</span></cite><span class="ltx_text ltx_font_small"> [</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T2.m4" class="ltx_Math" alttext="\checkmark" display="inline"><mi mathvariant="normal">✓</mi></math><span class="ltx_text ltx_font_small">]</span></td></tr>
</table></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">27.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">27.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">27.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 2: </span>Best scores achieved on various datasets.</div>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">Our analysis reveals that the errors fall into four major types, each of which contributes significantly to the overall errors made by the four systems, despite the fact that the contribution of each of these error types varies from system to system.
Moreover, we do not observe any significant difference between the types of errors made by the four systems other than the fact that the supervised system has the expected tendency to predict keyphrases seen in the training data. Below we describe these four major types of errors.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Overgeneration errors</span> are a major type of precision error, contributing to 28–37% of the overall error.
Overgeneration errors occur when a system correctly predicts a candidate as a keyphrase because it contains a word that appears frequently in the associated document, but at the same time erroneously outputs other candidates as keyphrases because they contain the same word.
Recall that for many systems, it is not easy to reject a non-keyphrase containing a word with a high term frequency:
many unsupervised systems score a candidate by summing the score of each of its component words, and many supervised systems use unigrams as features to represent a candidate. To be more concrete, consider the news article on athlete <span class="ltx_text ltx_font_italic">Ben Johnson</span> in Figure 1, where the keyphrases are boldfaced.
As we can see, the word <span class="ltx_text ltx_font_italic">Olympic(s)</span> has a significant presence in the document.
Consequently, many systems not only correctly predict <span class="ltx_text ltx_font_italic">Olympics</span> as a keyphrase, but also erroneously predict
<span class="ltx_text ltx_font_italic">Olympic movement</span> as a keyphrase,
yielding overgeneration errors.</p>
</div>
<div id="S5.F1" class="ltx_figure">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:204.9pt;" width="204.9pt"><span class="ltx_text ltx_font_small">Canadian </span><span class="ltx_text ltx_font_bold ltx_font_small">Ben Johnson</span><span class="ltx_text ltx_font_small"> left the </span><span class="ltx_text ltx_font_bold ltx_font_small">Olympics</span><span class="ltx_text ltx_font_small">
today “in a complete state of shock,” accused of cheating with
drugs in the world’s fastest </span><span class="ltx_text ltx_font_bold ltx_font_small">100-meter dash</span><span class="ltx_text ltx_font_small"> and stripped of his
</span><span class="ltx_text ltx_font_bold ltx_font_small">gold medal</span><span class="ltx_text ltx_font_small">. The prize went to American </span><span class="ltx_text ltx_font_bold ltx_font_small">Carl Lewis</span><span class="ltx_text ltx_font_small">.
Many athletes accepted the accusation that Johnson used a
muscle-building but dangerous and illegal anabolic steroid called
</span><span class="ltx_text ltx_font_bold ltx_font_small">stanozolol</span><span class="ltx_text ltx_font_small"> as confirmation of what they said they know has been
going on in track and field.
Two tests of Johnson’s urine sample proved positive and his
denials of </span><span class="ltx_text ltx_font_bold ltx_font_small">drug use</span><span class="ltx_text ltx_font_small"> were rejected today.
“This is a blow for the Olympic Games and the Olympic
movement,” said International Olympic Committee President Juan
Antonio Samaranch.</span></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A news article on <span class="ltx_text ltx_font_italic">Ben Johnson</span> from the DUC-2001 dataset. The keyphrases are boldfaced.</div>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Infrequency errors</span> are a major type of recall error contributing to 24–27% of the overall error.
Infrequency errors occur when a system fails to identify a keyphrase owing to its infrequent presence in the associated document <cite class="ltx_cite">[<a href="#bib.bib56" title="Automatic keyphrase extraction by bridging vocabulary gap" class="ltx_ref">31</a>]</cite>.
Handling infrequency errors is a challenge because state-of-the-art keyphrase extractors rarely predict candidates that appear only once or twice in a document.
In the <span class="ltx_text ltx_font_italic">Ben Johnson</span> example, many keyphrase extractors fail to identify <span class="ltx_text ltx_font_italic">100-meter dash</span> and <span class="ltx_text ltx_font_italic">gold medal</span> as keyphrases, resulting in infrequency errors.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Redundancy errors</span> are a type of precision error contributing to 8–12% of the overall error.
Redundancy errors occur when a system correctly identifies a candidate as a keyphrase, but at the same time outputs a semantically equivalent candidate (e.g., its alias) as a keyphrase. This type of error can be attributed to a system’s failure to determine that two candidates are semantically equivalent. Nevertheless, some researchers may argue that a system should not be penalized for redundancy errors because the extracted candidates are in fact keyphrases. In our example, <span class="ltx_text ltx_font_italic">Olympics</span> and <span class="ltx_text ltx_font_italic">Olympic games</span> refer to the same concept, so a system that predicts both of them as keyphrases commits a redundancy error.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Evaluation errors</span> are a type of recall error contributing to 7–10% of the overall error.
An evaluation error occurs when a system outputs a candidate that is semantically equivalent to a gold keyphrase, but is considered erroneous by a scoring program because of its failure to recognize that the predicted phrase and the corresponding gold keyphrase are semantically equivalent.
In other words, an evaluation error is not an error made by a keyphrase extractor, but an error due to the naivety of a scoring program. In our example, while <span class="ltx_text ltx_font_italic">Olympics</span> and <span class="ltx_text ltx_font_italic">Olympic games</span> refer to the same concept, only the former is annotated as keyphrase. Hence, an evaluation error occurs if a system predicts <span class="ltx_text ltx_font_italic">Olympic games</span> but not <span class="ltx_text ltx_font_italic">Olympics</span> as a keyphrase and the scoring program fails to identify them as semantically equivalent.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Recommendations</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">We recommend that <span class="ltx_text ltx_font_italic">background knowledge</span> be extracted from external lexical databases (e.g., YAGO2 <cite class="ltx_cite">[<a href="#bib.bib62" title="YAGO: A core of semantic knowledge" class="ltx_ref">47</a>]</cite>, Freebase <cite class="ltx_cite">[<a href="#bib.bib63" title="Freebase: A collaboratively created graph database for structuring human knowledge" class="ltx_ref">4</a>]</cite>, BabelNet <cite class="ltx_cite">[<a href="#bib.bib64" title="BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network" class="ltx_ref">39</a>]</cite>)
to address the four types of errors discussed above.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">First, we discuss how <span class="ltx_text ltx_font_bold">redundancy errors</span> could be addressed by using the background knowledge extracted from external databases.
Note that if we can identify semantically equivalent candidates, then we can reduce redundancy errors.
The question, then, is: can background knowledge be used to help us identify semantically equivalent candidates?
To answer this question, note that Freebase, for instance, has over 40 million <span class="ltx_text ltx_font_italic">topics</span> (i.e., real-world entities such as people, places, and things) from over 70 domains (e.g., music, business, education).
Hence, before a system outputs a set of candidates as keyphrases, it can use Freebase to determine whether any of them is mapped to the same Freebase topic.
Referring back to our running example, both <span class="ltx_text ltx_font_italic">Olympics</span> and <span class="ltx_text ltx_font_italic">Olympic games</span> are mapped to a Freebase topic called <span class="ltx_text ltx_font_italic">Olympic games</span>. Based on this information, a keyphrase extractor can determine that the two candidates are aliases and should output only one of them, thus preventing a redundancy error.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">Next, we discuss how <span class="ltx_text ltx_font_bold">infrequency errors</span> could be addressed using background knowledge.
A natural way to handle this problem would be to make an infrequent keyphrase frequent.
To accomplish this, we suggest exploiting an influential idea in the keyphrase extraction literature: the importance of a candidate is defined in terms of how related it is to other candidates in the text (see Section 3.3.1).
In other words, if we could relate an infrequent keyphrase to other candidates in the text, we could boost its importance.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">We believe that this could be accomplished using background knowledge. The idea is to boost the importance of infrequent keyphrases using their frequent counterparts. Consider again our running example. All four systems have managed to identify <span class="ltx_text ltx_font_italic">Ben Johnson</span> as a keyphrase due to its significant presence.
Hence, we can boost the importance of <span class="ltx_text ltx_font_italic">100-meter dash</span> and <span class="ltx_text ltx_font_italic">gold medal</span> if we can relate them to <span class="ltx_text ltx_font_italic">Ben Johnson</span>.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p class="ltx_p">To do so, note that Freebase maps a candidate to one or more pre-defined topics, each of which is associated with one or more types. Types are similar to entity classes. For instance, the candidate <span class="ltx_text ltx_font_italic">Ben Johnson</span> is mapped to a Freebase topic with the same name, which is associated with Freebase types such as <span class="ltx_text ltx_font_italic">Person</span>, <span class="ltx_text ltx_font_italic">Athlete</span>, and <span class="ltx_text ltx_font_italic">Olympic athlete</span>. Types are defined for a specific domain in Freebase. For instance, <span class="ltx_text ltx_font_italic">Person</span>, <span class="ltx_text ltx_font_italic">Athlete</span>, and <span class="ltx_text ltx_font_italic">Olympic athlete</span> are defined in the <span class="ltx_text ltx_font_italic">People</span>, <span class="ltx_text ltx_font_italic">Sports</span>, and <span class="ltx_text ltx_font_italic">Olympics</span> domains, respectively. Next, consider the two infrequent candidates, <span class="ltx_text ltx_font_italic">100-meter dash</span> and <span class="ltx_text ltx_font_italic">gold medal</span>. <span class="ltx_text ltx_font_italic">100-meter dash</span> is mapped to the topic <span class="ltx_text ltx_font_italic">Sprint</span> of type <span class="ltx_text ltx_font_italic">Sports</span> in the <span class="ltx_text ltx_font_italic">Sports</span> domain, whereas <span class="ltx_text ltx_font_italic">gold medal</span> is mapped to a topic with the same name of type <span class="ltx_text ltx_font_italic">Olympic medal</span> in the <span class="ltx_text ltx_font_italic">Olympics</span> domain. Consequently, we can relate <span class="ltx_text ltx_font_italic">100-meter dash</span> to <span class="ltx_text ltx_font_italic">Ben Johnson</span> via the <span class="ltx_text ltx_font_italic">Sports</span> domain (i.e., they belong to different types under the same domain). Additionally, <span class="ltx_text ltx_font_italic">gold medal</span> can be related to <span class="ltx_text ltx_font_italic">Ben Johnson</span> via the <span class="ltx_text ltx_font_italic">Olympics</span> domain.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p class="ltx_p">As discussed before, the relationship between two candidates is traditionally established using co-occurrence information.
However, using co-occurrence windows has its shortcomings.
First, an <span class="ltx_text ltx_font_italic">ad-hoc</span> window size cannot capture related candidates that are not inside the window. So it is difficult to predict <span class="ltx_text ltx_font_italic">100-meter dash</span> and <span class="ltx_text ltx_font_italic">gold medal</span> as keyphrases: they are more than 10 tokens away from frequent words such as <span class="ltx_text ltx_font_italic">Johnson</span> and <span class="ltx_text ltx_font_italic">Olympics</span>. Second, the candidates inside a window are all assumed to be related to each other, but it is apparently an overly simplistic assumption. There have been a few attempts to design Wikipedia-based relatedness measures, with promising initial results <cite class="ltx_cite">[<a href="#bib.bib48" title="Extracting key terms from noisy and multi-theme documents" class="ltx_ref">13</a>, <a href="#bib.bib3" title="Clustering to find exemplar terms for keyphrase extraction" class="ltx_ref">33</a>, <a href="#bib.bib11" title="Human-competitive tagging using automatic keyphrase extraction" class="ltx_ref">37</a>]</cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>Note that it may be difficult to employ our recommendations to address infrequency errors in informal text with uncorrelated topics because the keyphrases it contains may not be related to each other
(see Section <a href="#S2" title="2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</span></span></span></p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Overgeneration errors</span> could similarly be addressed using background knowledge.
Recall that <span class="ltx_text ltx_font_italic">Olympic movement</span> is not a keyphrase in our example although it includes an important word (i.e., <span class="ltx_text ltx_font_italic">Olympic</span>). Freebase maps <span class="ltx_text ltx_font_italic">Olympic movement</span> to a topic with the same name, which is associated with a type called <span class="ltx_text ltx_font_italic">Musical Recording</span> in the <span class="ltx_text ltx_font_italic">Music</span> domain.
However, it does not map <span class="ltx_text ltx_font_italic">Olympic movement</span> to any topic in the <span class="ltx_text ltx_font_italic">Olympics</span> domain.
The absence of such a mapping in the <span class="ltx_text ltx_font_italic">Olympics</span> domain could be used by a keyphrase extractor as a supporting evidence against predicting <span class="ltx_text ltx_font_italic">Olympic movement</span> as a keyphrase.</p>
</div>
<div id="S5.SS2.p8" class="ltx_para">
<p class="ltx_p">Finally, as mentioned before, <span class="ltx_text ltx_font_bold">evaluation errors</span> should not be considered errors made by a system. Nevertheless, they reveal a problem with the way keyphrase extractors are currently evaluated.
To address this problem, one possibility is to conduct human evaluations. Cheaper alternatives include
having human annotators identify semantically equivalent keyphrases during manual labeling, and designing scoring programs that can automatically identify such semantic equivalences.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We have presented a survey of the state of the art in automatic keyphrase extraction.
While unsupervised approaches have started to rival their supervised counterparts in performance, the task is far from being solved, as reflected by the fairly poor state-of-the-art results on various commonly-used evaluation datasets.
Our analysis revealed that there are at least three major challenges ahead.</p>
</div>
<div id="S6.SS2.SSS4.P1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">1. Incorporating background knowledge.</h5>

<div id="S6.SS2.SSS4.P1.p1" class="ltx_para">
<p class="ltx_p">While much recent work has focused on algorithmic development,
keyphrase extractors need to have a deeper “understanding” of a document in order to reach the
next level of performance.
Such an understanding
can be facilitated by the incorporation of background knowledge.</p>
</div>
</div>
<div id="S6.SS2.SSS4.P2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">2. Handling long documents.</h5>

<div id="S6.SS2.SSS4.P2.p1" class="ltx_para">
<p class="ltx_p">While it may be possible to design better algorithms to handle the large number of candidates in long documents, we believe that employing sophisticated features, especially those that encode background knowledge, will enable keyphrases and non-keyphrases to be distinguished more easily even in the presence of a large number of candidates.</p>
</div>
</div>
<div id="S6.SS2.SSS4.P3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">3. Improving evaluation schemes.</h5>

<div id="S6.SS2.SSS4.P3.p1" class="ltx_para">
<p class="ltx_p">To more accurately measure the performance of keyphrase extractors, they should not be penalized for evaluation errors. We have suggested several possibilities as to how this problem can be addressed.</p>
</div>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We thank the anonymous reviewers for their detailed and insightful comments on
earlier drafts of this paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Barker and N. Cornacchia</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Using noun phrase heads to extract document keyphrases</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 40–52</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Berend</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Opinion expression mining by exploiting keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1162–1170</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/I11-1130" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib66" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. M. Blei, A. Y. Ng and M. I. Jordan</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Latent Dirichlet allocation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Machine Learning Research</span> <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp. 993–1022</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1532-4435</span>,
<a href="http://dl.acm.org/citation.cfm?id=944919.944937" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS2.P2.p1" title="Topical PageRank (TPR) ‣ 3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>.
</span></li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Bollacker, C. Evans, P. Paritosh, T. Sturge and J. Taylor</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Freebase: A collaboratively created graph database for structuring human knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1247–1250</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p1" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Bougouin, F. Boudin and B. Daille</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">TopicRank: graph-based topic ranking for keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 543–551</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/I13-1062" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>.
</span></li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Brin and L. Page</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The anatomy of a large-scale hypertextual Web search engine</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Networks</span> <span class="ltx_text ltx_bib_volume">30</span> (<span class="ltx_text ltx_bib_number">1–7</span>), <span class="ltx_text ltx_bib_pages"> pp. 107–117</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS1.p2" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>.
</span></li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Chen, J. Sun, H. Zeng and K. Lam</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A practical system of keyphrase extraction for web pages</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 277–278</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS2.p5" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Ding, Q. Zhang and X. Huang</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Keyphrase extraction from online news using binary integer programming</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 165–173</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/I11-1019" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Dredze, H. M. Wallach, D. Puller and F. Pereira</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generating summary keywords for emails using topics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 199–206</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. R. El-Beltagy and A. A. Rafea</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">KP-Miner: A keyphrase extraction system for English and Arabic documents</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Systems</span> <span class="ltx_text ltx_bib_volume">34</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 132–144</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS1.p2" title="4.1 Evaluation Metrics ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p2" title="4.2 The State of the Art ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. R. El-Beltagy and A. Rafea</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">KP-Miner: Participation in SemEval-2</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 190–193</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p2" title="4.2 The State of the Art ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin and C. G. Nevill-Manning</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Domain-specific keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 668–673</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS1.p3" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Grineva, M. Grinev and D. Lizorkin</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Extracting key terms from noisy and multi-theme documents</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 661–670</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>,
<a href="#S3.SS3.SSS2.P3.p1" title="CommunityCluster ‣ 3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S3.SS3.SSS2.p1" title="3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S5.SS2.p6" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>,
<a href="#S5.T2" title="Table 2 ‣ 5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib51" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Gutwin, G. Paynter, I. Witten, C. Nevill-Manning and E. Frank</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improving browsing in digital libraries with keyphrase indexes</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Decision Support Systems</span> <span class="ltx_text ltx_bib_volume">27</span>, <span class="ltx_text ltx_bib_pages"> pp. 81–104</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. M. Hammouda, D. N. Matute and M. S. Kamel</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CorePhrase: Keyphrase extraction for document clustering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 265–274</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. S. Hasan and V. Ng</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Conundrums in unsupervised keyphrase extraction: Making sense of the state-of-the-art</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 365–373</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P1.p1" title="Length ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Huang, Y. Tian, Z. Zhou, C. X. Ling and T. Huang</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Keyphrase extraction using semantic networks structure analysis</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 275–284</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Hulth, J. Karlgren, A. Jonsson, H. Boström and L. Asker</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyword extraction using domain knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 472–482</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Hulth and B. B. Megyesi</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A study on automatically extracted keywords in text categorization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 537–544</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Hulth</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved automatic keyword extraction given more linguistic knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 216–223</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS3.SSS2.P1.p2" title="KeyCluster ‣ 3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib45" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Hulth</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Enhancing linguistically oriented automatic keyword extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 17–20</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p2" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Jiang, Y. Hu and H. Li</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A ranking approach to keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 756–757</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS1.p3" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Kelleher and S. Luz</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic hypertext keyphrase detection</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1608–1609</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p3" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. N. Kim, T. Baldwin and M. Kan</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating n-gram based evaluation metrics for automatic keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 572–580</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p3" title="4.1 Evaluation Metrics ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib61" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. N. Kim and T. Baldwin</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Extracting keywords from multi-party live chats</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 199–208</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/Y12-1021" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P2.p1" title="Structural consistency ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.SS0.SSS0.P3.p1" title="Topic change ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. N. Kim and M. Kan</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Re-examining automatic keyphrase extraction approaches in scientific articles</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 9–16</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS2.p6" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. N. Kim, O. Medelyan, M. Kan and T. Baldwin</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SemEval-2010 Task 5: Automatic keyphrase extraction from scientific articles</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 21–26</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. N. Kim, O. Medelyan, M. Kan and T. Baldwin</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyphrase extraction from scientific articles</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Language Resources and Evaluation</span> <span class="ltx_text ltx_bib_volume">47</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 723–742</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P2.p1" title="Structural consistency ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S4.SS2.p1" title="4.2 The State of the Art ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Kumar and K. Srinathan</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyphrase extraction from scientific documents using n-gram filtration technique</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 199–208</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Liu, D. Pennell, F. Liu and Y. Liu</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised approaches for automatic keyword extraction using meeting transcripts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 620–628</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Liu, X. Chen, Y. Zheng and M. Sun</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyphrase extraction by bridging vocabulary gap</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 135–144</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/W11-0316" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p5" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Liu, W. Huang, Y. Zheng and M. Sun</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyphrase extraction via topic decomposition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 366–376</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS3.SSS2.P2.p1" title="Topical PageRank (TPR) ‣ 3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S3.SS3.SSS2.p1" title="3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S4.SS1.p4" title="4.1 Evaluation Metrics ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Liu, P. Li, Y. Zheng and M. Sun</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Clustering to find exemplar terms for keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 257–266</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS3.SSS2.P1.p1" title="KeyCluster ‣ 3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S3.SS3.SSS2.p1" title="3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>,
<a href="#S4.SS2.p2" title="4.2 The State of the Art ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS2.p6" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>,
<a href="#S5.T2" title="Table 2 ‣ 5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Liu, C. Liang and M. Sun</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Topical word trigger model for keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1715–1730</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/C12-1105" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS2.p1" title="3.3.2 Topic-Based Clustering ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>.
</span></li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Lopez and L. Romary</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HUMB: Automatic key term extraction from scientific articles in GROBID</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 248–251</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS2.p10" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S5.T2" title="Table 2 ‣ 5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Matsuo and M. Ishizuka</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Keyword extraction from a single document using word co-occurrence statistical information</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal on Artificial Intelligence Tools</span> <span class="ltx_text ltx_bib_volume">13</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>,
<a href="#S4.SS1.p3" title="4.1 Evaluation Metrics ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Medelyan, E. Frank and I. H. Witten</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Human-competitive tagging using automatic keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1318–1327</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P3.p1" title="Topic change ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS2.SSS2.p9" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S5.SS1.p1" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS2.p6" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Mihalcea and P. Tarau</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">TextRank: Bringing order into texts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 404–411</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P4.p1" title="Topic correlation ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>,
<a href="#S3.SS3.SSS1.p2" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>,
<a href="#S4.SS2.p2" title="4.2 The State of the Art ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span></li>
<li id="bib.bib64" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Navigli and S. P. Ponzetto</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Artificial Intelligence</span> <span class="ltx_text ltx_bib_volume">193</span>, <span class="ltx_text ltx_bib_pages"> pp. 217–250</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p1" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib59" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Newman, N. Koilada, J. H. Lau and T. Baldwin</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bayesian text segmentation for index term identification and keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2077–2092</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/C12-1127" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Q. Nguyen and T. T. Phan</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An ontology-based approach for key phrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 181–184</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. D. Nguyen and M. Kan</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Keyphrase extraction in scientific publications</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 317–326</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS2.SSS2.p5" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S3.SS2.SSS2.p6" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S5.SS1.p1" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Over</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Introduction to DUC-2001: An intrinsic evaluation of generic news text summarization systems</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Paukkeri, I. T. Nieminen, M. Pöllä and T. Honkela</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A language-independent approach to keyphrase extraction and evaluation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 83–86</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Salton and C. Buckley</span><span class="ltx_text ltx_bib_year">(1988)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Term-weighting approaches in automatic text retrieval</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Processing and Management</span> <span class="ltx_text ltx_bib_volume">24</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 513–523</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Song, I. Song and X. Hu</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">KPSpotter: A flexible information gain-based keyphrase extraction system</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 50–53</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p3" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. M. Suchanek, G. Kasneci and G. Weikum</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">YAGO: A core of semantic knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 697–706</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p1" title="5.2 Recommendations ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Tomokiyo and M. Hurst</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A language model approach to keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 33–40</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS3.SSS4.p1" title="3.3.4 Language Modeling ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.4</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Turney</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to extract keyphrases from text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">National Research Council Canada, Institute for Information Technology, Technical Report ERB-1057</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Turney</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning algorithms for keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Retrieval</span> <span class="ltx_text ltx_bib_volume">2</span>, <span class="ltx_text ltx_bib_pages"> pp. 303–336</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Turney</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Coherent keyphrase extraction via web mining</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 434–439</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P4.p1" title="Topic correlation ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS2.SSS2.p11" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wan and J. Xiao</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CollabRank: Towards a collaborative approach to single-document keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 969–976</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wan and J. Xiao</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Single document keyphrase extraction using neighborhood knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 855–860</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS3.SSS1.p1" title="3.3.1 Graph-Based Ranking ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.T2" title="Table 2 ‣ 5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wan, J. Yang and J. Xiao</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 552–559</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS3.p1" title="3.3.3 Simultaneous Learning ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>.
</span></li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Wang and S. Li</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CoRankBayes: Bayesian learning to rank under the co-training framework and its application in keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2241–2244</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS1.p2" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin and C. G. Nevill-Manning</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">KEA: Practical automatic keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 254–255</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P3.p1" title="Topic change ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS1.p3" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S5.SS1.p1" title="5.1 Error Analysis ‣ 5 Analysis ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. B. Wu, Q. Li, R. S. Bot and X. Chen</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Domain-specific keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 283–284</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Yih, J. Goodman and V. R. Carvalho</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finding advertising keywords on web pages</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 213–222</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS0.SSS0.P2.p1" title="Structural consistency ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.T1" title="Table 1 ‣ 2 Corpora ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS2.SSS1.p1" title="3.2.1 Task Reformulation ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>,
<a href="#S3.SS2.SSS2.p10" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S3.SS2.SSS2.p5" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>,
<a href="#S3.SS2.SSS2.p6" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. You, D. Fontaine and J. Barthès</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic keyphrase extraction with a refined candidate set</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 576–579</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Selecting Candidate Words and Phrases ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Zesch and I. Gurevych</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Approximate matching for evaluating keyphrase extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 484–489</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.p4" title="4.1 Evaluation Metrics ‣ 4 Evaluation ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zha</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 113–120</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.SSS3.p1" title="3.3.3 Simultaneous Learning ‣ 3.3 Unsupervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>.
</span></li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhang, N. Zincir-Heywood and E. Milios</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">World Wide Web site summarization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Web Intelligence and Agent Systems</span> <span class="ltx_text ltx_bib_volume">2</span>, <span class="ltx_text ltx_bib_pages"> pp. 39–53</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhang, N. Zincir-Heywood and E. Milios</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Narrative text classification for automatic key phrase extraction in web document corpora</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 51–58</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.SSS2.p4" title="3.2.2 Features ‣ 3.2 Supervised Approaches ‣ 3 Keyphrase Extraction Approaches ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.
</span></li>
<li id="bib.bib55" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Zhao, J. Jiang, J. He, Y. Song, P. Achanauparp, E. Lim and X. Li</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Topical keyphrase extraction from Twitter</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 379–388</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P11-1039" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Automatic Keyphrase Extraction: A Survey of the State of the Art" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:55:04 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
