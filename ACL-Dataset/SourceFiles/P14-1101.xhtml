<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Weak semantic context helps phonetic learningin a model of infant language acquisition</title>
<!--Generated on Tue Jun 10 18:30:40 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Weak semantic context helps phonetic learning
<br class="ltx_break"/>in a model of infant language acquisition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stella Frank 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">sfrank@inf.ed.ac.uk</span> 
<br class="ltx_break"/>ILCC, School of Informatics 
<br class="ltx_break"/>University of Edinburgh 
<br class="ltx_break"/>Edinburgh, EH8 9AB, UK
<br class="ltx_break"/>&amp;Naomi H. Feldman 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">nhf@umd.edu</span> 
<br class="ltx_break"/>Department of Linguistics
<br class="ltx_break"/>University of Maryland 
<br class="ltx_break"/>College Park, MD, 20742, USA 
<br class="ltx_break"/>&amp;Sharon Goldwater 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">sgwater@inf.ed.ac.uk</span> 
<br class="ltx_break"/>ILCC, School of Informatics 
<br class="ltx_break"/>University of Edinburgh 
<br class="ltx_break"/>Edinburgh, EH8 9AB, UK
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Learning phonetic categories is one of the first steps to learning a
language, yet is hard to do using only distributional phonetic
information. Semantics could potentially be useful, since words with
different meanings have distinct phonetics, but it is unclear how many word meanings are known to infants learning phonetic categories. We show that
attending to a weaker source of semantics, in the form of a
distribution over topics in the current context, can lead to
improvements in phonetic category learning. In our model, an
extension of a previous model of joint word-form and phonetic category
inference, the probability of word-forms is topic-dependent, enabling
the model to find significantly better phonetic vowel categories and
word-forms than a model with no semantic knowledge.</p>
</div><span class="ltx_ERROR undefined">\pgfplotscreateplotcyclelist</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">goodred,blue,teal,green!60!black,orange,</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Infants begin learning the phonetic categories of their native language
in their first year <cite class="ltx_cite">(<a href="#bib.bib34" title="Linguistic experience alters phonetic perception in infants by 6 months of age" class="ltx_ref">19</a>; <a href="#bib.bib40" title="Developmental changes in perception of nonnative vowel contrasts" class="ltx_ref">29</a>; <a href="#bib.bib35" title="Cross-language speech perception: evidence for perceptual reorganization during the first year of life" class="ltx_ref">51</a>)</cite>.
In theory, semantic information could offer a valuable cue for phoneme
induction<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>The models in this paper do not distinguish between
phonetic and phonemic categories, since they do not capture phonological
processes (and there are also none present in our synthetic data). We
thus use the terms interchangeably.</span></span></span>
by helping infants distinguish between minimal pairs, as linguists do
<cite class="ltx_cite">(<a href="#bib.bib11" title="Grundzüge der Phonologie" class="ltx_ref">48</a>)</cite>.
However, due to a widespread assumption that infants do not know the
meanings of many words at the age when they are learning phonetic
categories (see <cite class="ltx_cite"><a href="#bib.bib61" title="Contributions of infant word learning to language development" class="ltx_ref">42</a></cite> for a review), most recent
models of early phonetic category acquisition have explored the phonetic
learning problem in the absence of semantic information
<cite class="ltx_cite">(<a href="#bib.bib22" title="Investigating the role of infant-directed speech with a computer model" class="ltx_ref">8</a>; <a href="#bib.bib23" title="A single-stage approach to learning phonological categories: insights from Inuktitut" class="ltx_ref">9</a>; <a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>; <a href="#bib.bib38" title="Statistical learning of phonetic categories: insights from a computational approach" class="ltx_ref">26</a>; <a href="#bib.bib65" title="Unsupervised learning of vowel categories from infant-directed speech" class="ltx_ref">50</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Models without any semantic information are likely to underestimate
infants’ ability to learn phonetic categories.
Infants learn language in the wild, and quickly attune to the fact that
words have (possibly unknown) meanings.
The extent of infants’ semantic knowledge is not yet known, but existing
evidence shows that six-month-olds can associate some words with their
referents <cite class="ltx_cite">(<a href="#bib.bib20" title="At 6-9 months, human infants know the meanings of many common nouns" class="ltx_ref">4</a>; <a href="#bib.bib64" title="Some beginnings of word comprehension in 6-month-olds" class="ltx_ref">46</a>; <a href="#bib.bib63" title="Six-month-olds comprehend words that refer to parts of the body" class="ltx_ref">47</a>)</cite>, leverage
non-acoustic contexts such as objects or articulations to distinguish
similar sounds <cite class="ltx_cite">(<a href="#bib.bib62" title="Visual speech contributes to phonetic learning in 6-month-old infants" class="ltx_ref">44</a>; <a href="#bib.bib66" title="Learning words’ sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information" class="ltx_ref">52</a>)</cite>, and map meaning (in the
form of objects or images) to new word-forms in some laboratory settings
<cite class="ltx_cite">(<a href="#bib.bib26" title="Word learning in 6-month-olds: fast encoding—weak retention" class="ltx_ref">15</a>; <a href="#bib.bib27" title="Intersensory redundancy and 7-month-old infants’ memory for arbitrary syllable-object relations" class="ltx_ref">16</a>; <a href="#bib.bib60" title="Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants" class="ltx_ref">39</a>)</cite>.
These findings indicate that young infants are sensitive to
co-occurrences between linguistic stimuli and at least some aspects of
the world.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper we explore the potential contribution of semantic
information to phonetic learning by formalizing a model in which
learners attend to the word-level context in which phones appear (as in
the lexical-phonetic learning model of <cite class="ltx_cite"><a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a></cite>) and also
to the situations in which word-forms are used.
The modeled situations consist of combinations of categories of salient
activities or objects, similar to the activity contexts explored by
<cite class="ltx_cite">Roy<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Relating activity contexts to early word learning in dense longitudinal data" class="ltx_ref">37</a>)</cite>, e.g.,‘getting dressed’ or ‘eating breakfast’.
We assume that child learners are able to infer a representation of the
situational context from their non-linguistic environment.
However, in our simulations we approximate the environmental information
by running a topic model <cite class="ltx_cite">(<a href="#bib.bib14" title="Hierarchical topic models and the nested Chinese restaurant process" class="ltx_ref">5</a>)</cite> over a corpus of
child-directed speech to infer a topic distribution for each situation.
These topic distributions are then used as input to our model to
represent situational contexts.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">The situational information in our model is similar to that assumed by
theories of cross-situational word learning
<cite class="ltx_cite">(<a href="#bib.bib12" title="Using speakers’ referential intentions to model early cross-situational word learning" class="ltx_ref">14</a>; <a href="#bib.bib8" title="Infants rapidly learn word-referent mappings via cross-situational statistics" class="ltx_ref">40</a>; <a href="#bib.bib7" title="Rapid word learning under uncertainty via cross-situational statistics" class="ltx_ref">53</a>)</cite>, but our model does not require
learners to map individual words to their referents.
Even in the absence of word-meaning mappings, situational information is
potentially useful because similar-sounding words uttered in similar
situations are more likely to be tokens of the same lexeme
(containing the same phones) than similar-sounding words uttered in
different situations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">In simulations of vowel learning, inspired by <cite class="ltx_cite">Vallabha<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib65" title="Unsupervised learning of vowel categories from infant-directed speech" class="ltx_ref">50</a>)</cite> and
<cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>, we show a clear improvement over previous models
in both phonetic and lexical (word-form) categorization when situational
context is used as an additional source of information.
This improvement is especially noticeable when the word-level context is
providing less information, arguably the more realistic setting.
These results demonstrate that relying on situational co-occurrence can
improve phonetic learning, even if learners do not yet know the meanings
of individual words.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Background and overview of models</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Infants attend to distributional characteristics of their input
<cite class="ltx_cite">(<a href="#bib.bib36" title="Infant sensitivity to distributional information can affect phonetic discrimination" class="ltx_ref">24</a>; <a href="#bib.bib33" title="Statistical phonetic learning in infants: facilitation and feature generalization" class="ltx_ref">23</a>)</cite>, leading to the hypothesis that phonetic
categories could be acquired on the basis of bottom-up distributional
learning alone <cite class="ltx_cite">(<a href="#bib.bib22" title="Investigating the role of infant-directed speech with a computer model" class="ltx_ref">8</a>; <a href="#bib.bib65" title="Unsupervised learning of vowel categories from infant-directed speech" class="ltx_ref">50</a>; <a href="#bib.bib38" title="Statistical learning of phonetic categories: insights from a computational approach" class="ltx_ref">26</a>)</cite>.
However, this would require sound categories to be well separated, which
often is not the case—for example, see Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which shows
the English vowel space that is the focus of this paper.</p>
</div>
<div id="S2.F1" class="ltx_figure"><img src="P14-1101/image001.png" id="S2.F1.g1" class="ltx_graphics" width="338" height="254" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The English vowel space (generated from
<cite class="ltx_cite">Hillenbrand<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib28" title="Acoustic characteristics of American English vowels." class="ltx_ref">17</a>)</cite>, see Section <a href="#S6.SS2" title="6.2 Hillenbrand Vowels ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>), plotted
using the first two formants. </div>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Recent work has investigated whether infants could overcome such
distributional ambiguity by incorporating top-down information,
in particular, the fact that phones appear within words.
At six months, infants begin to recognize word-forms
such as their name and other frequently occurring words
<cite class="ltx_cite">(<a href="#bib.bib31" title="Infants’ recognition of the sound patterns of their own names" class="ltx_ref">21</a>; <a href="#bib.bib29" title="Infants’ memory for spoken words" class="ltx_ref">18</a>)</cite>, without necessarily linking a
meaning to these forms.
This “protolexicon” can help differentiate phonetic categories by
adding word contexts in which certain sound categories appear
<cite class="ltx_cite">(<a href="#bib.bib61" title="Contributions of infant word learning to language development" class="ltx_ref">42</a>; <a href="#bib.bib25" title="Word-level information influences phonetic learning in adults and infants" class="ltx_ref">12</a>)</cite>.
To explore this idea further, <cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite> implemented the
Lexical-Distributional (LD) model, which jointly learns a set of
phonetic vowel categories and a set of word-forms containing those
categories.
Simulations showed that the use of lexical context greatly improved
phonetic learning.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Our own Topic-Lexical-Distributional (TLD) model extends the LD model to
include an additional type of context: the situations in which words
appear.
To motivate this extension and clarify the differences between the
models, we now provide a high-level overview of both models; details are
given in Sections <a href="#S3" title="3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4" title="4 Topic-Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of LD model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">Both the LD and TLD models are computational-level models of phonetic
(specifically, vowel) categorization where phones (vowels) are presented
to the model in the context of words.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>For a related model that also tackles the word segmentation
problem, see <cite class="ltx_cite">Elsner<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib1" title="A cognitive model of early lexical acquisition with phonetic variability" class="ltx_ref">10</a>)</cite>. In a model of phonological learning,
<cite class="ltx_cite"><a href="#bib.bib4" title="A rudimentary lexicon and semantics help bootstrap phoneme acquisition" class="ltx_ref">Fourtassi and Dupoux</a></cite> (submitted) show that semantic context
information similar to that used here remains useful despite
segmentation errors.</span></span></span>
The task is to infer a set of phonetic categories and a set of
lexical items on the basis of the data observed for each word token
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math>.
In the original LD model, the observations for token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> are its frame
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="f_{i}" display="inline"><msub><mi>f</mi><mi>i</mi></msub></math>, which consists of a list of consonants and slots for vowels, and
the list of vowel tokens <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="\bm{w}_{i}" display="inline"><msub><mi>𝒘</mi><mi>i</mi></msub></math>.
(The TLD model includes additional observations, described below.)
A single vowel token, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m5" class="ltx_Math" alttext="w_{ij}" display="inline"><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>, is a two dimensional vector representing
the first two formants (peaks in the frequency spectrum, ordered from
lowest to highest).
For example, a token of the word <span class="ltx_text ltx_font_italic">kitty</span> would have the frame
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m6" class="ltx_Math" alttext="f_{i}=\texttt{k\_t\_}" display="inline"><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>=</mo><mtext mathvariant="monospace">k_t_</mtext></mrow></math>, containing two consonant phones, /<span class="ltx_text ltx_font_sansserif">k</span>/
and /<span class="ltx_text ltx_font_sansserif">t</span>/, with two vowel phone slots in between, and two vowel
formant vectors, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m7" class="ltx_Math" alttext="w_{i0}=[464,2294]" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mn>0</mn></mrow></msub><mo>=</mo><mrow><mo>[</mo><mrow><mn>464</mn><mo>,</mo><mn>2294</mn></mrow><mo>]</mo></mrow></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m8" class="ltx_Math" alttext="w_{i1}=[412,2760]" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mo>[</mo><mrow><mn>412</mn><mo>,</mo><mn>2760</mn></mrow><mo>]</mo></mrow></mrow></math>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>In simulations we also experiment with frames in which
consonants are not represented perfectly.</span></span></span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">Given the data, the model must assign each vowel token to a vowel
category, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="w_{ij}=c" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math>. Both the LD and the TLD models do this using
intermediate lexemes, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="\bm{\ell}" display="inline"><mi mathvariant="bold">ℓ</mi></math>, which contain vowel category
assignments, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m3" class="ltx_Math" alttext="v_{\ell j}=c" display="inline"><mrow><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math>, as well as a frame <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m4" class="ltx_Math" alttext="f_{\ell}" display="inline"><msub><mi>f</mi><mi mathvariant="normal">ℓ</mi></msub></math>.
If a word token is assigned to a lexeme, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m5" class="ltx_Math" alttext="x_{i}=\ell" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mi mathvariant="normal">ℓ</mi></mrow></math>, the vowels within
the word are assigned to that lexeme’s vowel categories, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m6" class="ltx_Math" alttext="w_{ij}=v_{\ell j}=c" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>The notation is overloaded: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m7" class="ltx_Math" alttext="w_{ij}" display="inline"><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> refers both to the vowel
formants and the vowel category assignments, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m8" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> refers to both
the token identity and its assignment to a lexeme.</span></span></span>
The word and lexeme frames must match, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m9" class="ltx_Math" alttext="f_{i}=f_{\ell}" display="inline"><mrow><msub><mi>f</mi><mi>i</mi></msub><mo>=</mo><msub><mi>f</mi><mi mathvariant="normal">ℓ</mi></msub></mrow></math>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">Lexical information helps with phonetic categorization because it can
disambiguate highly overlapping categories, such as the <span class="ltx_text ltx_font_italic">ae</span> and
<span class="ltx_text ltx_font_italic">eh</span> categories in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
A purely distributional learner who observes a cluster of data points in
the <span class="ltx_text ltx_font_italic">ae</span>-<span class="ltx_text ltx_font_italic">eh</span> region is likely to assume all these points belong
to a single category because the distributions of the categories are so
similar.
However, a learner who attends to lexical context will notice a
difference: contexts that only occur with <span class="ltx_text ltx_font_italic">ae</span> will be observed in
one part of the <span class="ltx_text ltx_font_italic">ae</span>-<span class="ltx_text ltx_font_italic">eh</span> region, while contexts that only
occur with <span class="ltx_text ltx_font_italic">eh</span> will be observed in a different (though partially
overlapping) space.
The learner then has evidence of two different categories occurring in
different sets of lexemes.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p class="ltx_p">Simulations with the LD model show that using lexical information to
constrain phonetic learning can greatly improve categorization accuracy
<cite class="ltx_cite">(<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>, but it can also introduce errors.
When two word tokens contain the same consonant frame but different
vowels (i.e., minimal pairs), the model is more likely to categorize
those two vowels together.
Thus, the model has trouble distinguishing minimal pairs.
Although young children also have trouble with minimal pairs
<cite class="ltx_cite">(<a href="#bib.bib6" title="Infants listen for more phonetic detail in speech perception than in word-learning tasks" class="ltx_ref">41</a>; <a href="#bib.bib5" title="The effect of distributional information on children’s use of phonemic contrasts" class="ltx_ref">45</a>)</cite>, the LD model may overestimate the
degree of the problem.
We hypothesize that if a learner is able to associate words with the
contexts of their use (as children likely are), this could provide a
weak source of information for disambiguating minimal pairs even without
knowing their exact meanings.
That is, if the learner hears <span class="ltx_text ltx_font_sansserif">k<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p4.m1" class="ltx_Math" alttext="V_{1}" display="inline"><msub><mi>V</mi><mn mathvariant="normal">1</mn></msub></math>t</span> and <span class="ltx_text ltx_font_sansserif">k<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p4.m2" class="ltx_Math" alttext="V_{2}" display="inline"><msub><mi>V</mi><mn mathvariant="normal">2</mn></msub></math>t</span> in
different situational contexts, they are likely to be different lexical
items (and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p4.m3" class="ltx_Math" alttext="V_{1}" display="inline"><msub><mi>V</mi><mn>1</mn></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p4.m4" class="ltx_Math" alttext="V_{2}" display="inline"><msub><mi>V</mi><mn>2</mn></msub></math> different phones), despite the lexical
similarity between them.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Overview of TLD model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">To demonstrate the benefit of situational information, we develop the
Topic-Lexical-Distributional (TLD) model, which extends the LD model
by assuming that words appear in <span class="ltx_text ltx_font_italic">situations</span>
analogous to documents in a topic model.
Each situation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m1" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math> is associated with a mixture of topics <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m2" class="ltx_Math" alttext="{\theta_{h}}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math>,
which is assumed to be observed.
Thus, for the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>th token in situation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m4" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math>, denoted <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m5" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>, the
observed data will be its frame <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m6" class="ltx_Math" alttext="f_{hi}" display="inline"><msub><mi>f</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>, vowels <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m7" class="ltx_Math" alttext="\bm{w}_{hi}" display="inline"><msub><mi>𝒘</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>, and topic
vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m8" class="ltx_Math" alttext="{\theta_{h}}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">From an acquisition perspective, the observed topic distribution
represents the child’s knowledge of the context of the interaction:
she can distinguish bathtime from dinnertime, and is able to recognize
that some topics appear in certain contexts (e.g. animals on walks,
vegetables at dinnertime) and not in others (few vegetables appear at
bathtime). We assume that the child would learn these topics from
observing the world around her and the co-occurrences of entities and
activities in the world. Within any given situation, there might be a
mixture of different (actual or possible) topics that are salient to the
child. We assume further that as the child learns the language, she will
begin to associate specific words with each topic as well.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">Thus, in the TLD model, the words used in a
situation are topic-dependent, implying meaning, but without pinpointing
specific referents. Although the model observes the distribution of
topics in each situation (corresponding to the child observing her
non-linguistic environment), it must learn to associate each
(phonetically and lexically ambiguous) word token with a particular
topic from that distribution.
The occurrence of similar-sounding words in different situations with
mostly non-overlapping topics will provide evidence that those words
belong to different topics and that they are therefore different
lexemes.
Conversely, potential minimal pairs that occur in situations with
similar topic distributions are more likely to belong to the same topic
and thus the same lexeme.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">Although we assume that children infer topic distributions from the
non-linguistic environment, we will use transcripts from <span class="ltx_text ltx_font_smallcaps">childes</span>
to create the word/phone learning input for our model.
These transcripts are not annotated with environmental context, but
<cite class="ltx_cite">Roy<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Relating activity contexts to early word learning in dense longitudinal data" class="ltx_ref">37</a>)</cite> found that topics learned from similar
transcript data using a topic model were strongly correlated with
immediate activities and contexts.
We therefore obtain the topic distributions used as input to the TLD
model by training an LDA topic model <cite class="ltx_cite">(<a href="#bib.bib14" title="Hierarchical topic models and the nested Chinese restaurant process" class="ltx_ref">5</a>)</cite> on a superset of
the child-directed transcript data we use for lexical-phonetic learning,
dividing the transcripts into small sections (the ‘documents’ in LDA)
that serve as our distinct situations <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m1" class="ltx_Math" alttext="\bm{h}" display="inline"><mi>𝒉</mi></math>. As noted above, the
learned document-topic distributions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m2" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math> are treated as
observed variables in the TLD model to represent the situational
context. The topic-word distributions learned by LDA are discarded,
since these are based on the (correct and unambiguous) words in the
transcript, whereas the TLD model is presented with phonetically
ambiguous versions of these word tokens and must learn to disambiguate
them and associate them with topics.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Lexical-Distributional Model</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this section we describe more formally the
generative process for the LD model <cite class="ltx_cite">(<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>,
a joint Bayesian model over phonetic categories and a lexicon,
before describing the TLD extension in the following section.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The set of phonetic categories and the lexicon are
both modeled using non-parametric Dirichlet Process
priors, which return a potentially infinite number of categories or
lexemes.
A DP is parametrized as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="DP(\alpha,H)" display="inline"><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>α</mi><mo>,</mo><mi>H</mi></mrow><mo>)</mo></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> is a
real-valued hyperparameter and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> is a base distribution.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m4" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> may be continuous, as when it generates phonetic categories in
formant space, or discrete, as when it generates lexemes as a list of
phonetic categories.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">A draw from a DP, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m1" class="ltx_Math" alttext="G\sim DP(\alpha,H)" display="inline"><mrow><mi>G</mi><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>α</mi><mo>,</mo><mi>H</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>, returns a distribution over a
set of draws from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m2" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>, i.e., a discrete distribution over a set of
categories or lexemes generated by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>.
In the mixture model setting, the category assignments are then
generated from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m4" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math>, with the datapoints themselves generated by the
corresponding components from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m5" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>.
If <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m6" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> is infinite, the support of the DP is likewise infinite.
During inference, we marginalize over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m7" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math>.</p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Phonetic Categories: IGMM</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Following previous models of vowel learning
<cite class="ltx_cite">(<a href="#bib.bib22" title="Investigating the role of infant-directed speech with a computer model" class="ltx_ref">8</a>; <a href="#bib.bib65" title="Unsupervised learning of vowel categories from infant-directed speech" class="ltx_ref">50</a>; <a href="#bib.bib38" title="Statistical learning of phonetic categories: insights from a computational approach" class="ltx_ref">26</a>; <a href="#bib.bib23" title="A single-stage approach to learning phonological categories: insights from Inuktitut" class="ltx_ref">9</a>)</cite>
we assume that vowel tokens are drawn from a Gaussian mixture model.
The Infinite Gaussian Mixture Model (IGMM) <cite class="ltx_cite">(<a href="#bib.bib16" title="The infinite Gaussian mixture model" class="ltx_ref">35</a>)</cite>
includes a DP prior, as described above, in which the base distribution
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="H_{C}" display="inline"><msub><mi>H</mi><mi>C</mi></msub></math> generates multivariate Gaussians drawn from a Normal
Inverse-Wishart prior.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>This compound distribution is equivalent to
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m2" class="ltx_Math" alttext="\Sigma_{c}\sim\textit{IW}(\Sigma_{0},\nu_{0}),\ \ \mu_{c}|\Sigma_{c}\sim N(\mu%&#10;_{0},\frac{\Sigma_{c}}{\nu_{0}})" display="inline"><mrow><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub><mo>∼</mo><mtext>𝐼𝑊</mtext><mrow><mo>(</mo><msub><mi mathvariant="normal">Σ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>ν</mi><mn>0</mn></msub><mo>)</mo></mrow><mo rspace="12.5pt">,</mo><msub><mi>μ</mi><mi>c</mi></msub><mo>|</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo>(</mo><msub><mi>μ</mi><mn>0</mn></msub><mo>,</mo><mfrac><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub><msub><mi>ν</mi><mn>0</mn></msub></mfrac><mo>)</mo></mrow></mrow></math></span></span></span>
Each observation, a formant vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m3" class="ltx_Math" alttext="w_{ij}" display="inline"><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>, is drawn from the Gaussian
corresponding to its category assignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m4" class="ltx_Math" alttext="c_{ij}" display="inline"><msub><mi>c</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m1" class="ltx_Math" alttext="\displaystyle\mu_{c},\Sigma_{c}" display="inline"><mrow><msub><mi>μ</mi><mi>c</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m2" class="ltx_Math" alttext="\displaystyle\sim H_{C}=\textit{NIW}(\mu_{0},\Sigma_{0},\nu_{0})" display="inline"><mrow><mo>∼</mo><msub><mi>H</mi><mi>C</mi></msub><mo>=</mo><mtext>𝑁𝐼𝑊</mtext><mrow><mo>(</mo><msub><mi>μ</mi><mn>0</mn></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>ν</mi><mn>0</mn></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
<tr id="S3.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m1" class="ltx_Math" alttext="\displaystyle G_{C}" display="inline"><msub><mi>G</mi><mi>C</mi></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m2" class="ltx_Math" alttext="\displaystyle\sim DP(\alpha_{c},H_{C})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>c</mi></msub><mo>,</mo><msub><mi>H</mi><mi>C</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
<tr id="S3.E3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m1" class="ltx_Math" alttext="\displaystyle c_{ij}" display="inline"><msub><mi>c</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m2" class="ltx_Math" alttext="\displaystyle\sim G_{C}" display="inline"><mrow><mi/><mo>∼</mo><msub><mi>G</mi><mi>C</mi></msub></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
<tr id="S3.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="\displaystyle w_{ij}|c_{ij}=c" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>c</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m2" class="ltx_Math" alttext="\displaystyle\sim N(\mu_{c},\Sigma_{c})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>μ</mi><mi>c</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">The above model generates a category assignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m5" class="ltx_Math" alttext="c_{ij}" display="inline"><msub><mi>c</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> for each vowel
token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m6" class="ltx_Math" alttext="w_{ij}" display="inline"><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math>.
This is the baseline IGMM model, which clusters vowel tokens using
bottom-up distributional information only; the LD model adds top-down
information by assigning categories in the lexicon, rather than on the
token level.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Lexicon</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">In the LD model, vowel phones appear within words drawn from the
lexicon. Each such lexeme is represented as a frame plus a list of
vowel categories <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="\bm{v}_{\ell}" display="inline"><msub><mi>𝒗</mi><mi mathvariant="normal">ℓ</mi></msub></math>.
Lexeme assignments for each token are drawn from a DP
with a lexicon-generating base distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math>.
The category for each vowel token in the word is determined by the
lexeme; the formant values are drawn from the corresponding Gaussian as
in the IGMM:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="\displaystyle G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m2" class="ltx_Math" alttext="\displaystyle\sim DP(\alpha_{l},H_{L})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>l</mi></msub><mo>,</mo><msub><mi>H</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
<tr id="S3.E6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="\displaystyle x_{i}=\ell" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mi mathvariant="normal">ℓ</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m2" class="ltx_Math" alttext="\displaystyle\sim G_{L}" display="inline"><mrow><mi/><mo>∼</mo><msub><mi>G</mi><mi>L</mi></msub></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
<tr id="S3.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m1" class="ltx_Math" alttext="\displaystyle w_{ij}|v_{\ell j}=c" display="inline"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m2" class="ltx_Math" alttext="\displaystyle\sim N(\mu_{c},\Sigma_{c})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>μ</mi><mi>c</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math> generates lexemes by first drawing the number of phones from a
geometric distribution and the number of consonant phones from a
binomial distribution.
The consonants are then generated from a DP with a uniform base
distribution (but note they are fixed at inference time, i.e., are
observed categorically), while the vowel phones <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m2" class="ltx_Math" alttext="\bm{v}_{\ell}" display="inline"><msub><mi>𝒗</mi><mi mathvariant="normal">ℓ</mi></msub></math> are generated
by the IGMM DP above, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m3" class="ltx_Math" alttext="v_{\ell j}\sim G_{C}" display="inline"><mrow><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>∼</mo><msub><mi>G</mi><mi>C</mi></msub></mrow></math>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">Note that two draws from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m1" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math> may result in identical lexemes; these
are nonetheless considered to be separate (homophone) lexemes.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Topic-Lexical-Distributional Model</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">The TLD model retains the IGMM vowel phone component, but extends the
lexicon of the LD model by adding topic-specific lexicons, which capture
the notion that lexeme probabilities are topic-dependent.
Specifically, the TLD model replaces the Dirichlet Process lexicon with
a Hierarchical Dirichlet Process (HDP; <cite class="ltx_cite">Teh (<a href="#bib.bib19" title="A hierarchical Bayesian language model based on Pitman-Yor processes" class="ltx_ref">43</a>)</cite>).
In the HDP lexicon, a top-level global lexicon is generated as in the LD
model.
Topic-specific lexicons are then drawn from the global lexicon,
containing a subset of the global lexicon (but since the size of the
global lexicon is unbounded, so are the topic-specific lexicons).
These topic-specific lexicons are used to generate the tokens in a
similar manner to the LD model.
There are a fixed number of lower level topic-lexicons; these
are matched to the number of topics in the LDA model used to
infer the topic distributions (see Section <a href="#S6.SS4" title="6.4 Topics ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">More formally, the global lexicon is generated as a top-level DP: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m1" class="ltx_Math" alttext="G_{L}\sim DP(\alpha_{l},H_{L})" display="inline"><mrow><msub><mi>G</mi><mi>L</mi></msub><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>l</mi></msub><mo>,</mo><msub><mi>H</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math> (see Section <a href="#S3.SS2" title="3.2 Lexicon ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>; remember <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m2" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math>
includes draws from the IGMM over vowel categories).
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m3" class="ltx_Math" alttext="G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math> is in turn used as the base distribution in the topic-level
DPs, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m4" class="ltx_Math" alttext="G_{k}\sim DP(\alpha_{k},G_{L})" display="inline"><mrow><msub><mi>G</mi><mi>k</mi></msub><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>,</mo><msub><mi>G</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math>.
In the Chinese Restaurant Franchise metaphor often used to describe
HDPs, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m5" class="ltx_Math" alttext="G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math> is a global menu of dishes (lexemes).
The topic-specific lexicons are restaurants, each with its own
distribution over dishes; this distribution is defined by seating
customers (word tokens) at <em class="ltx_emph">tables</em>, each of which serves a single
dish from the menu: all tokens <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m6" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> at the same table <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m7" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> are assigned to
the same lexeme <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m8" class="ltx_Math" alttext="\ell_{t}" display="inline"><msub><mi mathvariant="normal">ℓ</mi><mi>t</mi></msub></math>.
Inference (Section <a href="#S5" title="5 Inference: Gibbs Sampling ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) is defined in terms of tables
rather than lexemes; if multiple tables draw the same dish from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m9" class="ltx_Math" alttext="G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math>,
tokens at these tables share a lexeme.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">In the TLD model, tokens appear within situations, each of which
has a distribution over topics <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m1" class="ltx_Math" alttext="\theta_{h}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math>.
Each token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m2" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math> has a co-indexed topic assignment variable,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m3" class="ltx_Math" alttext="z_{hi}" display="inline"><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>, drawn from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m4" class="ltx_Math" alttext="\theta_{h}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math>, designating the topic-lexicon from
which the table for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m5" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math> is to be drawn.
The formant values for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m6" class="ltx_Math" alttext="w_{hij}" display="inline"><msub><mi>w</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> are drawn in the same way as in the LD
model, given the lexeme assignment at <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m7" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>.
This results in the following model, shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Topic-Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>:</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_align">

<tr id="S4.E8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E8.m1" class="ltx_Math" alttext="\displaystyle G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E8.m2" class="ltx_Math" alttext="\displaystyle\sim DP(\alpha_{l},H_{L})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>l</mi></msub><mo>,</mo><msub><mi>H</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
<tr id="S4.E9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E9.m1" class="ltx_Math" alttext="\displaystyle G_{k}" display="inline"><msub><mi>G</mi><mi>k</mi></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E9.m2" class="ltx_Math" alttext="\displaystyle\sim DP(\alpha_{k},G_{L})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>D</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>α</mi><mi>k</mi></msub><mo>,</mo><msub><mi>G</mi><mi>L</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
<tr id="S4.E10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E10.m1" class="ltx_Math" alttext="\displaystyle z_{hi}" display="inline"><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E10.m2" class="ltx_Math" alttext="\displaystyle\sim Mult(\theta_{h})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>M</mi><mo>⁢</mo><mi>u</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>θ</mi><mi>h</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
<tr id="S4.E11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E11.m1" class="ltx_Math" alttext="\displaystyle x_{hi}=t|z_{hi}=k" display="inline"><mrow><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>t</mi><mo>|</mo><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>k</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E11.m2" class="ltx_Math" alttext="\displaystyle\sim G_{k}" display="inline"><mrow><mi/><mo>∼</mo><msub><mi>G</mi><mi>k</mi></msub></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
<tr id="S4.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E12.m1" class="ltx_Math" alttext="\displaystyle w_{hij}|x_{hi}=t,v_{{\ell_{t}}j}=c" display="inline"><mrow><msub><mi>w</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>t</mi><mo>,</mo><msub><mi>v</mi><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>t</mi></msub><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E12.m2" class="ltx_Math" alttext="\displaystyle\sim N(\mu_{c},\Sigma_{c})" display="inline"><mrow><mi/><mo>∼</mo><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>μ</mi><mi>c</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
</table>
</div>
<div id="S4.F2" class="ltx_figure">
<span class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:213.8pt;height:201.666666666667px;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-106.9pt,-72.6pt) scale(0.5,0.5) ;-webkit-transform:translate(-106.9pt,-72.6pt) scale(0.5,0.5) ;-ms-transform:translate(-106.9pt,-72.6pt) scale(0.5,0.5) ;"><svg xmlns="http://www.w3.org/2000/svg" height="399" version="1.1" viewBox="-58 -139 571 399" width="571"><g transform="matrix(1 0 0 -1 0 121)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><g stroke-width="1.422638pt"><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 50 260 L -50 260 C -54 260 -58 256 -58 252 L -58 220 C -58 216 -54 213 -50 213 L 50 213 C 54 213 58 216 58 220 L 58 252 C 58 256 54 260 50 260 Z M -58 213" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 -53 234)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="106">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m1" class="ltx_Math" alttext="\mu_{0},\kappa_{0},\Sigma_{0},\nu_{0}" display="inline"><mrow><msub><mi>μ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>κ</mi><mn>0</mn></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>ν</mi><mn>0</mn></msub></mrow></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 30 157 C 30 174 16 187 0 187 C -16 187 -30 174 -30 157 C -30 141 -16 128 0 128 C 16 128 30 141 30 157 Z M 0 157" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 -9 156)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m2" class="ltx_Math" alttext="H_{C}" display="inline"><msub><mi>H</mi><mi>C</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 30 79 C 30 95 16 108 0 108 C -16 108 -30 95 -30 79 C -30 62 -16 49 0 49 C 16 49 30 62 30 79 Z M 0 79" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 -9 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m3" class="ltx_Math" alttext="G_{C}" display="inline"><msub><mi>G</mi><mi>C</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 16 24 L -16 24 C -20 24 -24 20 -24 16 L -24 -16 C -24 -20 -20 -24 -16 -24 L 16 -24 C 20 -24 24 -20 24 -16 L 24 16 C 24 20 20 24 16 24 Z M -24 -24" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 -9 -2)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m4" class="ltx_Math" alttext="\alpha_{c}" display="inline"><msub><mi>α</mi><mi>c</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><path d="M 0 212 L 0 189" style="fill:none"/><g><g transform="matrix(0 -1 1 0 0 189)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 0 127 L 0 110" style="fill:none"/><g><g transform="matrix(0 -1 1 0 0 110)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 0 24 L 0 47" style="fill:none"/><g><g transform="matrix(0 1 -1 0 0 47)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 110 -79 C 110 -62 96 -48 79 -48 C 62 -48 48 -62 48 -79 C 48 -96 62 -110 79 -110 C 96 -110 110 -96 110 -79 Z M 79 -79" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 55 -81)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="48">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m5" class="ltx_Math" alttext="\mu_{c},\Sigma_{c}" display="inline"><mrow><msub><mi>μ</mi><mi>c</mi></msub><mo>,</mo><msub><mi mathvariant="normal">Σ</mi><mi>c</mi></msub></mrow></math></p></foreignObject></switch></g></g></g></g></g><path d="M 30 -30 M 30 -30 L 30 -132 L 132 -132 L 132 -30 Z M 132 -132" style="fill:none"/><path d="M 120 -120" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 115 -124)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m6" class="ltx_Math" alttext="\infty" display="inline"><mi mathvariant="normal">∞</mi></math></p></foreignObject></switch></g></g></g></g><path d="M 14 52 L 64 -49" style="fill:none"/><g><g transform="matrix(0.448992 -0.89799 0.89799 0.448992 64 -49)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 173 260 L 142 260 C 137 260 134 256 134 252 L 134 220 C 134 216 137 213 142 213 L 173 213 C 178 213 181 216 181 220 L 181 252 C 181 256 178 260 173 260 Z M 134 213" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 152 233)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m7" class="ltx_Math" alttext="\bm{\lambda}" display="inline"><mi>𝝀</mi></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 187 157 C 187 174 174 187 157 187 C 141 187 128 174 128 157 C 128 141 141 128 157 128 C 174 128 187 141 187 157 Z M 157 157" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 148 156)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m8" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 187 79 C 187 95 174 108 157 108 C 141 108 128 95 128 79 C 128 62 141 49 157 49 C 174 49 187 62 187 79 Z M 157 79" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 148 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m9" class="ltx_Math" alttext="G_{L}" display="inline"><msub><mi>G</mi><mi>L</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 173 24 L 142 24 C 137 24 134 20 134 16 L 134 -16 C 134 -20 137 -24 142 -24 L 173 -24 C 178 -24 181 -20 181 -16 L 181 16 C 181 20 178 24 173 24 Z M 134 -24" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 148 -2)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m10" class="ltx_Math" alttext="\alpha_{l}" display="inline"><msub><mi>α</mi><mi>l</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><path d="M 157 212 L 157 189" style="fill:none"/><g><g transform="matrix(0 -1 1 0 157 189)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 157 127 L 157 110" style="fill:none"/><g><g transform="matrix(0 -1 1 0 157 110)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 157 24 L 157 47" style="fill:none"/><g><g transform="matrix(0 1 -1 0 157 47)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 27 92 L 129 143" style="fill:none"/><g><g transform="matrix(0.89799 0.448982 -0.448982 0.89799 129 143)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 266 79 C 266 95 253 108 236 108 C 220 108 207 95 207 79 C 207 62 220 49 236 49 C 253 49 266 62 266 79 Z M 236 79" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 227 77)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m11" class="ltx_Math" alttext="G_{k}" display="inline"><msub><mi>G</mi><mi>k</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 252 24 L 220 24 C 216 24 213 20 213 16 L 213 -16 C 213 -20 216 -24 220 -24 L 252 -24 C 256 -24 260 -20 260 -16 L 260 16 C 260 20 256 24 252 24 Z M 213 -24" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 227 -2)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m12" class="ltx_Math" alttext="\alpha_{k}" display="inline"><msub><mi>α</mi><mi>k</mi></msub></math></p></foreignObject></switch></g></g></g></g></g><path d="M 197 118 M 197 118 L 197 39 L 280 39 L 280 118 Z M 280 39" style="fill:none"/><path d="M 266 49" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 261 46)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m13" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math></p></foreignObject></switch></g></g></g></g><path d="M 188 79 L 205 79" style="fill:none"/><g><g transform="matrix(1 0 0 1 205 79)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 236 24 L 236 47" style="fill:none"/><g><g transform="matrix(0 1 -1 0 236 47)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 423 138 C 423 154 410 167 394 167 C 377 167 364 154 364 138 C 364 121 377 108 394 108 C 410 108 423 121 423 138 Z M 394 138" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 380 136)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="27">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m14" class="ltx_Math" alttext="z_{hi}" display="inline"><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g><g stroke-width="0.8pt"><path d="M 423 59 C 423 75 410 89 394 89 C 377 89 364 75 364 59 C 364 43 377 30 394 30 C 410 30 423 43 423 59 Z M 394 59" style="fill:none"/></g></g><g><g transform="matrix(1 0 0 1 380 57)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="27">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m15" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math></p></foreignObject></switch></g></g></g></g></g><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><path d="M 384 -20 C 384 -3 371 10 354 10 C 338 10 325 -3 325 -20 C 325 -36 338 -49 354 -49 C 371 -49 384 -36 384 -20 Z M 354 -20"/></g></g></g><g><g transform="matrix(1 0 0 1 341 -21)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="27">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m16" class="ltx_Math" alttext="f_{hi}" display="inline"><msub><mi>f</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math></p></foreignObject></switch></g></g></g></g></g></g><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><path d="M 463 -20 C 463 -3 449 10 433 10 C 417 10 404 -3 404 -20 C 404 -36 417 -49 433 -49 C 449 -49 463 -36 463 -20 Z M 433 -20"/></g></g></g><g><g transform="matrix(1 0 0 1 415 -21)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="35">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m17" class="ltx_Math" alttext="w_{hij}" display="inline"><msub><mi>w</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math></p></foreignObject></switch></g></g></g></g></g></g><path d="M 394 20 M 394 20 L 394 -79 L 480 -79 L 480 20 Z M 480 -79" style="fill:none"/><path d="M 453 -65" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 429 -67)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="48">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m18" class="ltx_Math" alttext="|\bm{w}_{hi}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>𝒘</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo fence="true">|</mo></mrow></math></p></foreignObject></switch></g></g></g></g><path d="M 394 108 L 394 91" style="fill:none"/><g><g transform="matrix(0 -1 1 0 394 91)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 380 32 L 369 9" style="fill:none"/><g><g transform="matrix(-0.447212 -0.89442 0.89442 -0.447212 369 9)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 407 32 L 419 9" style="fill:none"/><g><g transform="matrix(0.447212 -0.89442 0.89442 0.447212 419 9)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 315 177 M 315 177 L 315 -114 L 492 -114 L 492 177 Z M 492 -114" style="fill:none"/><path d="M 305 256 M 305 256 L 305 -138 L 512 -138 L 512 256 Z M 512 -138" style="fill:none"/><path d="M 470 -100" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 451 -102)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="39">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m19" class="ltx_Math" alttext="|\bm{x}_{h}|" display="inline"><mrow><mo fence="true">|</mo><msub><mi>𝒙</mi><mi>h</mi></msub><mo fence="true">|</mo></mrow></math></p></foreignObject></switch></g></g></g></g><path d="M 500 -126" style="fill:none"/><g><g><g transform="matrix(1 0 0 1 495 -129)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="12" overflow="visible" width="10">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m20" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math></p></foreignObject></switch></g></g></g></g><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><g><g stroke-width="0.8pt"><g fill="#CCCCCC"><path d="M 423 217 C 423 233 410 246 394 246 C 377 246 364 233 364 217 C 364 200 377 187 394 187 C 410 187 423 200 423 217 Z M 394 217"/></g></g></g><g><g transform="matrix(1 0 0 1 384 215)"><g class="ltx_svg_fog" transform="matrix(1 0 0 -1 0 10)"><switch><foreignObject color="#000000" height="16" overflow="visible" width="19">
<p xmlns="http://www.w3.org/1999/xhtml" class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.pic1.m21" class="ltx_Math" alttext="\theta_{h}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math></p></foreignObject></switch></g></g></g></g></g></g><path d="M 394 186 L 394 169" style="fill:none"/><g><g transform="matrix(0 -1 1 0 394 169)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 110 -84 C 197 -98 295 -98 406 -35" style="fill:none"/><g><g transform="matrix(0.868777 0.496431 -0.496431 0.868777 406 -35)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g><path d="M 266 75 L 362 63" style="fill:none"/><g><g transform="matrix(0.992877 -0.124109 0.124109 0.992877 362 63)"><g><g stroke-width="1.13811pt"><g stroke-dashoffset="0.0pt"><g stroke-linecap="round"><g stroke-linejoin="round"><path d="M -3 4 C -3 2 0 0 1 0 C 0 0 -3 -2 -3 -4" style="fill:none"/></g></g></g></g></g></g></g></g></g></g></g></g></g></g></svg>
</span></span>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>TLD model, depicting, from left to right, the IGMM component,
the LD lexicon component, the topic-specific lexicons, and finally the
token <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m11" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>, appearing in document <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m12" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math>, with observed vowel formants
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m13" class="ltx_Math" alttext="{w}_{hij}" display="inline"><msub><mi>w</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> and frame <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m14" class="ltx_Math" alttext="f_{hi}" display="inline"><msub><mi>f</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math>.
The lexeme assignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m15" class="ltx_Math" alttext="x_{hi}" display="inline"><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math> and the topic assignment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m16" class="ltx_Math" alttext="z_{hi}" display="inline"><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math> are
inferred, the latter using the observed document-topic distribution
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m17" class="ltx_Math" alttext="\theta_{h}" display="inline"><msub><mi>θ</mi><mi>h</mi></msub></math>.
Note that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m18" class="ltx_Math" alttext="f_{i}" display="inline"><msub><mi>f</mi><mi>i</mi></msub></math> is deterministic given the lexeme assignment.
Squared nodes depict hyperparameters.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m19" class="ltx_Math" alttext="\bm{\lambda}" display="inline"><mi>𝝀</mi></math> is the set of hyperparameters used by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F2.m20" class="ltx_Math" alttext="H_{L}" display="inline"><msub><mi>H</mi><mi>L</mi></msub></math> when
generating lexical items (see Section <a href="#S3.SS2" title="3.2 Lexicon ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Inference: Gibbs Sampling</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We use Gibbs sampling to infer three sets of variables in the TLD
model: assignments to vowel categories in the lexemes,
assignments of tokens to topics, and assignments of tokens to
tables (from which the assignment to lexemes can be read off).</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Sampling lexeme vowel categories</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">Each vowel in the lexicon must be assigned to a category in the
IGMM.
The posterior probability of a category assignment is composed of
the DP prior over categories and the likelihood of the observed vowels
belonging to that category.
We use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="\bm{w}_{\ell j}" display="inline"><msub><mi>𝒘</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub></math> to denote the set of vowel formants at position <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m2" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math> in
words that have been assigned to lexeme <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m3" class="ltx_Math" alttext="\ell" display="inline"><mi mathvariant="normal">ℓ</mi></math>. Then,</p>
<table id="Sx1.EGx4" class="ltx_equationgroup ltx_eqn_align">

<tr id="S5.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.Ex1.m1" class="ltx_Math" alttext="\displaystyle P(" display="inline"><mrow><mi>P</mi><mo>(</mo></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.Ex1.m2" class="ltx_Math" alttext="\displaystyle v_{\ell j}=c|\bm{w},\bm{x},\bm{\ell}^{\setminus\ell})" display="inline"><mrow><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi><mo>|</mo><mi>𝒘</mi><mo>,</mo><mi>𝒙</mi><mo>,</mo><msup><mi mathvariant="bold">ℓ</mi><mrow><mo>∖</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo>)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S5.E13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E13.m2" class="ltx_Math" alttext="\displaystyle\propto P(v_{\ell j}=c|\bm{\ell}^{\setminus\ell})p(\bm{w}_{\ell j%&#10;}|v_{\ell j}=c,\bm{w}^{\setminus\ell j})" display="inline"><mrow><mo>∝</mo><mi>P</mi><mrow><mo>(</mo><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi><mo>|</mo><msup><mi mathvariant="bold">ℓ</mi><mrow><mo>∖</mo><mi mathvariant="normal">ℓ</mi></mrow></msup><mo>)</mo></mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>𝒘</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi><mo>,</mo><msup><mi>𝒘</mi><mrow><mo>∖</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(13)</span></td></tr>
</table>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">The first (DP prior) factor is defined as:</p>
<table id="Sx1.EGx5" class="ltx_equationgroup ltx_eqn_align">

<tr id="S5.E14" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E14.m1" class="ltx_Math" alttext="\displaystyle P(v_{\ell j}=c" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>v</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo>=</mo><mi>c</mi></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E14.m2" class="ltx_Math" alttext="\displaystyle|\bm{v}^{\setminus{\ell j}})=\begin{cases}\frac{n_{c}}{\sum_{c}n_%&#10;{c}+\alpha_{c}}&amp;\text{if $c$ exists}\\&#10;\frac{\alpha_{c}}{\sum_{c}n_{c}+\alpha_{c}}&amp;\text{if $c$ new}\\&#10;\end{cases}" display="inline"><mrow><mo>|</mo><msup><mi>𝒗</mi><mrow><mo>∖</mo><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></mrow></msup><mo>)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mstyle displaystyle="true"><mfrac><msub><mi>n</mi><mi>c</mi></msub><mrow><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mi>c</mi></msub><msub><mi>n</mi><mi>c</mi></msub></mrow><mo>+</mo><msub><mi>α</mi><mi>c</mi></msub></mrow></mfrac></mstyle></mtd><mtd columnalign="left"><mrow><mtext>if </mtext><mi>c</mi><mtext> exists</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><mstyle displaystyle="true"><mfrac><msub><mi>α</mi><mi>c</mi></msub><mrow><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mi>c</mi></msub><msub><mi>n</mi><mi>c</mi></msub></mrow><mo>+</mo><msub><mi>α</mi><mi>c</mi></msub></mrow></mfrac></mstyle></mtd><mtd columnalign="left"><mrow><mtext>if </mtext><mi>c</mi><mtext> new</mtext></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(14)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m1" class="ltx_Math" alttext="n_{c}" display="inline"><msub><mi>n</mi><mi>c</mi></msub></math> is the number of other vowels in the lexicon,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m2" class="ltx_Math" alttext="\bm{v}^{\setminus{lj}}" display="inline"><msup><mi>𝒗</mi><mrow><mo>∖</mo><mrow><mi>l</mi><mo>⁢</mo><mi>j</mi></mrow></mrow></msup></math>, assigned to category <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m3" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math>.
Note that there is always positive probability of creating a new
category.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">The likelihood of the vowels is calculated by marginalizing over all
possible means and variances of the Gaussian category parameters, given
the NIW prior. For a single point (if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p3.m1" class="ltx_Math" alttext="|\bm{w}_{\ell j}|=1" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi>𝒘</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>⁢</mo><mi>j</mi></mrow></msub><mo fence="true">|</mo></mrow><mo>=</mo><mn>1</mn></mrow></math>),
this predictive posterior is in the form of a Student-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p3.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> distribution;
for the more general case see <cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>, Eq. B3.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Sampling table &amp; topic assignments</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">We jointly sample <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m1" class="ltx_Math" alttext="\bm{x}" display="inline"><mi>𝒙</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m2" class="ltx_Math" alttext="\bm{z}" display="inline"><mi>𝒛</mi></math>, the variables assigning tokens to
tables and topics.
Resampling the table assignment includes the possibility of
changing to a table with a different lexeme or drawing a new table with
a previously seen or novel lexeme.
The joint conditional probability of a table and topic assignment, given
all other current token assignments, is:</p>
<table id="Sx1.EGx6" class="ltx_equationgroup ltx_eqn_align">

<tr id="S5.Ex2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.Ex2.m1" class="ltx_Math" alttext="\displaystyle P" display="inline"><mi>P</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.Ex2.m2" class="ltx_Math" alttext="\displaystyle(x_{hi}=t,z_{hi}=k|\bm{w}_{hi},\theta_{h},\bm{t}^{\setminus{hi}},%&#10;\bm{\ell},\bm{w}^{\setminus hi})" display="inline"><mrow><mo>(</mo><msub><mi>x</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>t</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>=</mo><mi>k</mi><mo>|</mo><msub><mi>𝒘</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub><mo>,</mo><msub><mi>θ</mi><mi>h</mi></msub><mo>,</mo><msup><mi>𝒕</mi><mrow><mo>∖</mo><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></mrow></msup><mo>,</mo><mi mathvariant="bold">ℓ</mi><mo>,</mo><msup><mi>𝒘</mi><mrow><mo>∖</mo><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></mrow></msup><mo>)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S5.Ex3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.Ex3.m2" class="ltx_Math" alttext="\displaystyle=P(k|\theta_{h})P(t|k,\ell_{t},\bm{t}^{\setminus{hi}})" display="inline"><mrow><mo>=</mo><mi>P</mi><mrow><mo>(</mo><mi>k</mi><mo>|</mo><msub><mi>θ</mi><mi>h</mi></msub><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>k</mi><mo>,</mo><msub><mi mathvariant="normal">ℓ</mi><mi>t</mi></msub><mo>,</mo><msup><mi>𝒕</mi><mrow><mo>∖</mo><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S5.E15" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E15.m2" class="ltx_Math" alttext="\displaystyle\ \ \ \ \ \prod_{c\in C}p(\bm{w}_{hi\cdot}|v_{\ell_{t}\cdot}=c,%&#10;\bm{w}^{\setminus hi})" display="inline"><mrow><mi mathvariant="normal"> </mi><mo mathvariant="italic" separator="true">  </mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>c</mi><mo>∈</mo><mi>C</mi></mrow></munder></mstyle><mi>p</mi><mrow><mo>(</mo><msub><mi>𝒘</mi><mrow><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow><mo>⁣</mo><mo>⋅</mo></mrow></msub><mo>|</mo><msub><mi>v</mi><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>t</mi></msub><mo>⁣</mo><mo>⋅</mo></mrow></msub><mo>=</mo><mi>c</mi><mo>,</mo><msup><mi>𝒘</mi><mrow><mo>∖</mo><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></mrow></msup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(15)</span></td></tr>
</table>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">The first factor, the prior probability of topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> in document <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="h" display="inline"><mi>h</mi></math>,
is given by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="\theta_{hk}" display="inline"><msub><mi>θ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>k</mi></mrow></msub></math> obtained from the LDA.
The second factor is the prior probability of assigning word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m4" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> to
table <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m5" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> with lexeme <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m6" class="ltx_Math" alttext="\ell" display="inline"><mi mathvariant="normal">ℓ</mi></math> given topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m7" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>. It is given by the HDP,
and depends on whether the table <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m8" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> exists in the HDP topic-lexicon for
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m9" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> and, likewise, whether any table in the topic-lexicon has the lexeme
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m10" class="ltx_Math" alttext="\ell" display="inline"><mi mathvariant="normal">ℓ</mi></math>:</p>
<table id="Sx1.EGx7" class="ltx_equationgroup ltx_eqn_align">

<tr id="S5.E16" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E16.m1" class="ltx_Math" alttext="\displaystyle P(t|k,\ell,\bm{t}^{\setminus{hi}})\propto" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>k</mi><mo>,</mo><mi mathvariant="normal">ℓ</mi><mo>,</mo><msup><mi>𝒕</mi><mrow><mo>∖</mo><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></mrow></msup><mo>)</mo></mrow><mo>∝</mo></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E16.m2" class="ltx_Math" alttext="\displaystyle\begin{cases}\frac{n_{kt}}{n_{k}+\alpha_{k}}&amp;\text{if $t$ in $k$}%&#10;\\&#10;\frac{\alpha_{k}}{n_{k}+\alpha_{k}}\frac{m_{\ell}}{m+\alpha_{l}}&amp;\text{if $t$ %&#10;new, $\ell$ known}\\&#10;\frac{\alpha_{k}}{n_{k}+\alpha_{k}}\frac{\alpha_{\ell}}{m+\alpha_{l}}&amp;\text{if%&#10; $t$ and $\ell$ new}\\&#10;\end{cases}" display="inline"><mrow><mo>{</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mstyle displaystyle="true"><mfrac><msub><mi>n</mi><mrow><mi>k</mi><mo>⁢</mo><mi>t</mi></mrow></msub><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>+</mo><msub><mi>α</mi><mi>k</mi></msub></mrow></mfrac></mstyle></mtd><mtd columnalign="left"><mrow><mtext>if </mtext><mi>t</mi><mtext> in </mtext><mi>k</mi></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mstyle displaystyle="true"><mfrac><msub><mi>α</mi><mi>k</mi></msub><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>+</mo><msub><mi>α</mi><mi>k</mi></msub></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><msub><mi>m</mi><mi mathvariant="normal">ℓ</mi></msub><mrow><mi>m</mi><mo>+</mo><msub><mi>α</mi><mi>l</mi></msub></mrow></mfrac></mstyle></mrow></mtd><mtd columnalign="left"><mrow><mtext>if </mtext><mi>t</mi><mtext> new, </mtext><mi mathvariant="normal">ℓ</mi><mtext> known</mtext></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mstyle displaystyle="true"><mfrac><msub><mi>α</mi><mi>k</mi></msub><mrow><msub><mi>n</mi><mi>k</mi></msub><mo>+</mo><msub><mi>α</mi><mi>k</mi></msub></mrow></mfrac></mstyle><mo>⁢</mo><mstyle displaystyle="true"><mfrac><msub><mi>α</mi><mi mathvariant="normal">ℓ</mi></msub><mrow><mi>m</mi><mo>+</mo><msub><mi>α</mi><mi>l</mi></msub></mrow></mfrac></mstyle></mrow></mtd><mtd columnalign="left"><mrow><mtext>if </mtext><mi>t</mi><mtext> and </mtext><mi mathvariant="normal">ℓ</mi><mtext> new</mtext></mrow></mtd></mtr></mtable></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(16)</span></td></tr>
</table>
<p class="ltx_p">Here <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m11" class="ltx_Math" alttext="n_{kt}" display="inline"><msub><mi>n</mi><mrow><mi>k</mi><mo>⁢</mo><mi>t</mi></mrow></msub></math> is the number of other tokens at table <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m12" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m13" class="ltx_Math" alttext="n_{k}" display="inline"><msub><mi>n</mi><mi>k</mi></msub></math> are the total number of tokens in topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m14" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m15" class="ltx_Math" alttext="m_{\ell}" display="inline"><msub><mi>m</mi><mi mathvariant="normal">ℓ</mi></msub></math> is the number
of tables across all topics with the lexeme <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m16" class="ltx_Math" alttext="\ell" display="inline"><mi mathvariant="normal">ℓ</mi></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m17" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> is the total
number of tables.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">The third factor, the likelihood of the vowel formants <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m1" class="ltx_Math" alttext="\bm{w}_{hi}" display="inline"><msub><mi>𝒘</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi></mrow></msub></math> in
the categories given by the lexeme <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m2" class="ltx_Math" alttext="\bm{v}_{l}" display="inline"><msub><mi>𝒗</mi><mi>l</mi></msub></math>, is of the same
form as the likelihood of vowel categories when resampling lexeme vowel
assignments. However, here it is calculated over the set of vowels in the token
assigned to each vowel category (i.e., the vowels at indices where
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m3" class="ltx_Math" alttext="v_{\ell_{t}\cdot}=c" display="inline"><mrow><msub><mi>v</mi><mrow><msub><mi mathvariant="normal">ℓ</mi><mi>t</mi></msub><mo>⁣</mo><mo>⋅</mo></mrow></msub><mo>=</mo><mi>c</mi></mrow></math>).
For a new lexeme, we approximate the likelihood using 100 samples
drawn from the prior, each weighted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m4" class="ltx_Math" alttext="\alpha/100" display="inline"><mrow><mi>α</mi><mo>/</mo><mn>100</mn></mrow></math> <cite class="ltx_cite">(<a href="#bib.bib39" title="Markov chain sampling methods for Dirichlet process mixture models" class="ltx_ref">28</a>)</cite>.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Hyperparameters</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">The three hyperparameters governing the HDP over the lexicon, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m1" class="ltx_Math" alttext="\alpha_{l}" display="inline"><msub><mi>α</mi><mi>l</mi></msub></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m2" class="ltx_Math" alttext="\alpha_{k}" display="inline"><msub><mi>α</mi><mi>k</mi></msub></math>, and the DP over vowel categories, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m3" class="ltx_Math" alttext="\alpha_{c}" display="inline"><msub><mi>α</mi><mi>c</mi></msub></math>, are
estimated using a slice sampler.
The remaining hyperparameters for the vowel category and lexeme priors are
set to the same values used by <cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.1 </span>Corpus</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">We test our model on situated child directed speech, taken from the
C1 section of the Brent corpus in <span class="ltx_text ltx_font_smallcaps">childes</span> <cite class="ltx_cite">(<a href="#bib.bib21" title="The role of exposure to isolated words in early vocabulary development" class="ltx_ref">6</a>; <a href="#bib.bib30" title="The CHILDES project: tools for analyzing talk" class="ltx_ref">20</a>)</cite>.
This corpus consists of transcripts of speech directed at infants
between the ages of 9 and 15 months, captured in a naturalistic setting
as parent and child went about their day.
This ensures variability of situations.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p">Utterances with unintelligible words or quotes are removed.
We restrict the corpus to content words by retaining only words
tagged as <span class="ltx_text ltx_font_typewriter">adj, n, part</span> and <span class="ltx_text ltx_font_typewriter">v</span> (adjectives, nouns,
particles, and verbs).
This is in line with evidence that infants distinguish content and
function words on the basis of acoustic signals <cite class="ltx_cite">(<a href="#bib.bib41" title="The basis of preference for lexical words in 6-month-old infants" class="ltx_ref">38</a>)</cite>.
Vowel categorization improves when attending only to more prosodically
and phonologically salient tokens <cite class="ltx_cite">(<a href="#bib.bib13" title="Distributional learning of vowel categories is supported by prosody in infant-directed speech" class="ltx_ref">1</a>)</cite>, which generally
appear within content, not function words.
The final corpus consists of 13138 tokens and 1497 word types.</p>
</div>
</div>
<div id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.2 </span>Hillenbrand Vowels</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">The transcripts do not include phonetic information,
so, following <cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite>, we
synthesize the formant values using data from
<cite class="ltx_cite">Hillenbrand<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib28" title="Acoustic characteristics of American English vowels." class="ltx_ref">17</a>)</cite>.
This dataset consists of a set of 1669 manually gathered formant
values from 139 American English speakers (men, women and
children) for 12 vowels. For each vowel category, we construct a
Gaussian from the mean and covariance of the datapoints belonging to
that category, using the first and second formant values
measured at steady state.
We also construct a second dataset using only datapoints from adult
female speakers.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p">Each word in the dataset is converted to a phonemic representation
using the CMU pronunciation dictionary, which returns a sequence of Arpabet phoneme symbols.
If there are multiple possible pronunciations, the first one is used.
Each vowel phoneme in the word is then replaced by formant values drawn
from the corresponding Hillenbrand Gaussian for that vowel.</p>
</div>
</div>
<div id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.3 </span>Merging Consonant Categories</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p">The Arpabet encoding used in the phonemic representation includes 24
consonants. We construct datasets
both using the full set of consonants—the ‘C24’ dataset—and
with less fine-grained consonant categories.
Distinguishing all consonant categories assumes perfect learning of
consonants prior to vowel categorization and is thus somewhat
unrealistic <cite class="ltx_cite">(<a href="#bib.bib40" title="Developmental changes in perception of nonnative vowel contrasts" class="ltx_ref">29</a>)</cite>, but provides an upper limit on the
information that word-contexts can give.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p">In the ‘C15’ dataset,
the voicing distinction is collapsed,
leaving 15 consonant categories. The collapsed categories are
<span class="ltx_text ltx_font_sansserif">B/P, G/K, D/T, CH/JH, V/F, TH/DH, S/Z, SH/ZH, R/L</span> while
<span class="ltx_text ltx_font_sansserif">HH, M, NG, N, W, Y</span> remain separate phonemes.
This dataset mirrors the finding in <cite class="ltx_cite">Mani and Plunkett (<a href="#bib.bib32" title="Twelve-month-olds know their cups from their keps and tups" class="ltx_ref">22</a>)</cite> that 12 month old
infants are not sensitive to voicing mispronunciations.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p class="ltx_p">The ‘C6’ dataset distinguishes between only 6 coarse consonant phonemes,
corresponding to stops (<span class="ltx_text ltx_font_sansserif">B,P,G,K,D,T</span>), affricates (<span class="ltx_text ltx_font_sansserif">CH,JH</span>),
fricatives (<span class="ltx_text ltx_font_sansserif">V, F, TH, DH, S, Z, SH, ZH, HH</span>), nasals (<span class="ltx_text ltx_font_sansserif">M, NG,
N</span>), liquids (<span class="ltx_text ltx_font_sansserif">R, L</span>), and semivowels/glides (<span class="ltx_text ltx_font_sansserif">W, Y</span>).
This dataset makes minimal assumptions about the category categories
that infants could use in this learning setting.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p class="ltx_p">Decreasing the number of consonants increases the ambiguity in the
corpus: <span class="ltx_text ltx_font_italic">bat</span> not only shares a frame (<span class="ltx_text ltx_font_sansserif">b_t</span>)
with <span class="ltx_text ltx_font_italic">boat</span> and <span class="ltx_text ltx_font_italic">bite</span>, but also, in the C15 dataset, with
<span class="ltx_text ltx_font_italic">put</span>, <span class="ltx_text ltx_font_italic">pad</span> and <span class="ltx_text ltx_font_italic">bad</span> (<span class="ltx_text ltx_font_sansserif">b/p_d/t</span>),
and in the C6 dataset, with
<span class="ltx_text ltx_font_italic">dog</span> and <span class="ltx_text ltx_font_italic">kite</span>, among many others (<span class="ltx_text ltx_font_sansserif">STOP_STOP</span>).
Table <a href="#S6.T1" title="Table 1 ‣ 6.4 Topics ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the percentage of types and tokens that
are ambiguous in each dataset, that is, words in frames
that match multiple wordtypes.
Note that we always evaluate against the <em class="ltx_emph">gold</em> word identities,
even when these are not distinguished in the model’s input.
These datasets are intended to evaluate the degree of reliance on
consonant information in the LD and TLD models, and to what
extent the topics in the TLD model can replace this information.</p>
</div>
</div>
<div id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.4 </span>Topics</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p class="ltx_p">The input to the TLD model includes a distribution over topics for each
situation, which we infer in advance from the full Brent
corpus (not only the C1 subset) using LDA.
Each transcript in the Brent corpus captures about 75 minutes of
parent-child interaction, and thus multiple situations will be included
in each file.
The transcripts do not delimit situations, so we do this somewhat
arbitrarily by splitting each transcript after 50 CDS utterances,
resulting in 203 situations for the Brent C1 dataset.
As well as function words, we also remove the five most frequent
content words (<span class="ltx_text ltx_font_italic">be, go, get, want, come</span>).
On average, situations are only 59 words long, reflecting the relative
lack of content words in CDS utterances.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p class="ltx_p">We infer 50 topics for this set of situations
using the <span class="ltx_text ltx_font_typewriter">mallet</span> toolkit <cite class="ltx_cite">(<a href="#bib.bib37" title="MALLET: a machine learning for language toolkit" class="ltx_ref">25</a>)</cite>.
Hyperparameters are inferred, which leads to a dominant
topic that includes mainly light verbs (<span class="ltx_text ltx_font_italic">have, let, see, do</span>).
The other topics are less frequent but capture stronger semantic
meaning (e.g. <span class="ltx_text ltx_font_italic">yummy, peach, cookie, daddy, bib</span> in one topic,
<span class="ltx_text ltx_font_italic">shoe, let, put, hat, pants</span> in another).
The word-topic assignments are used to calculate unsmoothed
situation-topic distributions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS4.p2.m1" class="ltx_Math" alttext="\bm{\theta}" display="inline"><mi>𝜽</mi></math> used by the TLD model.</p>
</div>
<div id="S6.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">Dataset</th>
<td class="ltx_td ltx_align_right">C24</td>
<td class="ltx_td ltx_align_right">C15</td>
<td class="ltx_td ltx_align_right">C6</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t">Input Types</th>
<td class="ltx_td ltx_align_right ltx_border_t">1487</td>
<td class="ltx_td ltx_align_right ltx_border_t">1426</td>
<td class="ltx_td ltx_align_right ltx_border_t">1203</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">Frames</th>
<td class="ltx_td ltx_align_right">1259</td>
<td class="ltx_td ltx_align_right">1078</td>
<td class="ltx_td ltx_align_right">702</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">Ambig Types %</th>
<td class="ltx_td ltx_align_right">27.2</td>
<td class="ltx_td ltx_align_right">42.0</td>
<td class="ltx_td ltx_align_right">80.4</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_bb">Ambig Tokens %</th>
<td class="ltx_td ltx_align_right ltx_border_bb">41.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb">56.9</td>
<td class="ltx_td ltx_align_right ltx_border_bb">77.2</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Corpus statistics showing the increasing amount of ambiguity as
consonant categories are merged.
Input types are the number of word types with distinct input
representations
(as opposed to gold orthographic word types, of which there are 1497).
Ambiguous types and tokens are those with frames that match
multiple (orthographic) word types.
</div>
</div>
</div>
<div id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.5 </span>Evaluation</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p class="ltx_p">We evaluate against adult categories, i.e., the ‘gold-standard’, since
all learners of a language eventually converge on similar categories.
(Since our model is not a model of the learning process, we do not
compare the infant learning process to the learning algorithm.)
We evaluate both the inferred phonetic categories and words using the
clustering evaluation measure V-Measure (VM; <cite class="ltx_cite"><a href="#bib.bib17" title="V-measure: a conditional entropy-based external cluster evaluation measure" class="ltx_ref">36</a></cite>).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>Other clustering measures, such as 1-1 matching and pairwise
precision and recall (accuracy and completeness) showed the same trends,
but VM has been demonstrated to be the most stable measure when
comparing solutions with varying numbers of clusters
<cite class="ltx_cite">(<a href="#bib.bib15" title="Two decades of unsupervised POS induction: how far have we come?" class="ltx_ref">7</a>)</cite>.</span></span></span>
VM is the harmonic mean of two components, similar to F-score, where the
components (VC and VH) are measures of cross entropy between the gold
and model categorization.
For vowels, VM measures how well the inferred phonetic
categorizations match the gold categories; for lexemes, it measures
whether tokens have been assigned to the same lexemes both by
the model and the gold standard. Words are evaluated against
gold orthography, so homophones, e.g. <span class="ltx_text ltx_font_italic">hole</span> and <span class="ltx_text ltx_font_italic">whole</span>, are
distinct gold words.</p>
</div>
</div>
<div id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.6 </span>Results</h3>

<div id="S6.F3" class="ltx_figure">
<span class="ltx_inline-block ltx_transformed_outer" style="width:6.7pt;height:12.5px;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;-webkit-transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;-ms-transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" xml:id="S6.F3.pic1" fragid="S6.F3.pic1" imagesrc="P14-1101/image003.png" imagewidth="1728" imageheight="21" imagedepth="6" width="50" height="20" viewBox="-5 -10 45 10" overflow="visible"><g transform="translate(0,0)"><g transform="scale(1 -1)"><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><svg height="1" version="1.1" viewBox="0 0 1 1" width="1"><g transform="matrix(1 0 0 -1 0 1)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g/></g></g></g></g></g></svg></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">{axis}</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\legend</span></foreignObject></g></g></g></svg>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Vowel evaluation. ‘all’ refers to datasets with vowels
synthesized from all speakers, ‘w’ to datasets with vowels
synthesized from adult female speakers’ vowels.
The bars show a 95% Confidence Interval based on 5 runs.
IGMM-all results in a VM score of 53.9 (CI=0.5);
IGMM-w has a VM score of 65.0 (CI=0.2), not shown.
</div>
</div>
<div id="S6.SS6.p1" class="ltx_para">
<p class="ltx_p">We compare all three models—TLD, LD, and IGMM—on the vowel categorization
task, and TLD and LD on the lexical categorization task (since IGMM does
not infer a lexicon). The datasets correspond to two sets of conditions:
firstly, either using vowel categories synthesized from all speakers or
only adult female speakers, and secondly, varying the coarseness
of the observed consonant categories.
Each condition (model, vowel speakers, consonant set) is run five times,
using 1500 iterations of Gibbs sampling with hyperparameter sampling.
Overall, we find that TLD
outperforms the other models in both tasks, across all conditions.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<p class="ltx_p">Vowel categorization results are shown in
Figure <a href="#S6.F3" title="Figure 3 ‣ 6.6 Results ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
IGMM performs substantially worse than both TLD and LD, with scores more
than 30 points lower than the best results for these models,
clearly showing the value of the protolexicon and replicating the
results found by <cite class="ltx_cite">Feldman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="A role for the developing lexicon in phonetic category acquisition" class="ltx_ref">11</a>)</cite> on this dataset.
Furthermore, TLD consistently outperforms the LD model, finding
better phonetic categories, both for vowels generated from the combined
categories of all speakers (‘all’) and vowels generated from adult
female speakers only (‘w’), although the latter are clearly much easier
for both models to learn.
Both models perform less well when the consonant frames provide less
information, but the TLD model performance degrades less than the LD
performance.</p>
</div>
<div id="S6.SS6.p3" class="ltx_para">
<p class="ltx_p">Both the TLD and the LD models find ‘supervowel’ categories, which cover
multiple vowel categories and are used to merge minimal pairs into a
single lexical item.
Figure <a href="#S6.F4" title="Figure 4 ‣ 6.6 Results ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows example vowel categories inferred by the TLD
model, including two supervowels.
The TLD supervowels are used much less frequently than the supervowels
found by the LD model, containing, on average, only two-thirds as many
tokens.</p>
</div>
<div id="S6.F4" class="ltx_figure"><img src="P14-1101/image002.png" id="S6.F4.g1" class="ltx_graphics" width="338" height="254" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Vowels found by the TLD model; supervowels are indicated in
red. The gold-standard vowels are shown in gold in the background but
are mostly overlapped by the inferred categories. </div>
</div>
<div id="S6.F5" class="ltx_figure">
<span class="ltx_inline-block ltx_transformed_outer" style="width:6.7pt;height:12.5px;vertical-align:-2.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;-webkit-transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;-ms-transform:translate(-0.4pt,0.0pt) scale(0.9,1.00) ;"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" xml:id="S6.F5.pic1" fragid="S6.F5.pic1" imagesrc="P14-1101/image004.png" imagewidth="1728" imageheight="21" imagedepth="6" width="50" height="20" viewBox="-5 -10 45 10" overflow="visible"><g transform="translate(0,0)"><g transform="scale(1 -1)"><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><svg height="1" version="1.1" viewBox="0 0 1 1" width="1"><g transform="matrix(1 0 0 -1 0 1)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g/></g></g></g></g></g></svg></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">{axis}</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\legend</span></foreignObject></g></g></g></svg>
</span></span>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Lexeme evaluation. ‘all’ refers to datasets with vowels
synthesized from all speakers, ‘w’ to datasets with vowels
synthesized from adult female speakers’ vowels.</div>
</div>
<div id="S6.SS6.p4" class="ltx_para">
<p class="ltx_p">Figure <a href="#S6.F5" title="Figure 5 ‣ 6.6 Results ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that TLD also outperforms LD on
the lexeme/word categorization task.
Again performance decreases as the consonant categories become coarser,
but the additional semantic information in the TLD model compensates for
the lack of consonant information. In the individual components of VM,
TLD and LD have similar VC (“recall”), but TLD has higher VH
(“precision”), demonstrating that the semantic information given by
the topics can separate potentially ambiguous words, as hypothesized.</p>
</div>
<div id="S6.SS6.p5" class="ltx_para">
<p class="ltx_p">Overall, the contextual semantic information added in the TLD model
leads to both better phonetic categorization and to a better
protolexicon, especially when the input is noisier, using
degraded consonants.
Since infants are not likely to have perfect
knowledge of phonetic categories at this stage, semantic
information is a potentially rich source of information that could be
drawn upon to offset noise from other domains.
The form of the semantic information added in the TLD model is itself
quite weak, so the improvements shown here are in line with what infant
learners could achieve.</p>
</div>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">Language acquisition is a complex task, in which many heterogeneous
sources of information may be useful. In this paper, we investigated
whether contextual semantic information could be of help when learning
phonetic categories. We found that this contextual information can
improve phonetic learning performance considerably, especially in
situations where there is a high degree of phonetic ambiguity in the
word-forms that learners hear. This suggests that previous models that
have ignored semantic information may have underestimated the
information that is available to infants. Our model illustrates one way
in which language learners might harness the rich information that is
present in the world without first needing to acquire a full inventory
of word meanings.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">The contextual semantic information that the TLD model tracks is similar
to that potentially used in other linguistic learning tasks.
Theories of cross-situational word learning <cite class="ltx_cite">(<a href="#bib.bib8" title="Infants rapidly learn word-referent mappings via cross-situational statistics" class="ltx_ref">40</a>; <a href="#bib.bib7" title="Rapid word learning under uncertainty via cross-situational statistics" class="ltx_ref">53</a>)</cite>
assume that sensitivity to situational co-occurrences between words and
non-linguistic contexts is a precursor to learning the meanings of
individual words. Under this view, contextual semantics is available to
infants well before they have acquired large numbers of semantic minimal
pairs. However, recent experimental evidence indicates that learners do
not always retain detailed information about the referents that are
present in a scene when they hear a word
<cite class="ltx_cite">(<a href="#bib.bib9" title="How words can and cannot be learned by observation" class="ltx_ref">27</a>; <a href="#bib.bib10" title="Propose but verify: fast mapping meets cross-situational word learning" class="ltx_ref">49</a>)</cite>. This evidence poses a direct
challenge to theories of cross-situational word learning. Our account
does not necessarily require learners to track co-occurrences between
words and individual objects, but instead focuses on more abstract
information about salient events and topics in the environment; it will
be important to investigate to what extent infants encode this
information and use it in phonetic learning.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">Regardless of the specific way in which infants encode semantic
information, our method of adding this information by using LDA topics
from transcript data was shown to be effective. This method is
practical because it can approximate semantic information without
relying on extensive manual annotation.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">The LD model extended the phonetic categorization task by adding word
contexts; the TLD model presented here goes even further, adding larger
situational contexts. Both forms of top-down information help the
low-level task of classifying acoustic signals into phonetic categories,
furthering a holistic view of language learning with interaction across
multiple levels.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work was supported by EPSRC grant EP/H050442/1 and a James
S. McDonnell Foundation Scholar Award to the final author.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Adriaans and D. Swingley</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Distributional learning of vowel categories is supported by prosody in infant-directed speech</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib67" title="Proceedings of the 34th annual conference of the cognitive science society (cogsci)" class="ltx_ref">32</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS1.p2" title="6.1 Corpus ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.
</span></li>
<li id="bib.bib72" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Advances in neural information processing systems 13</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib16" title="The infinite Gaussian mixture model" class="ltx_ref">35</a>.
</span></li>
<li id="bib.bib71" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Advances in neural information processing systems 16</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib14" title="Hierarchical topic models and the nested Chinese restaurant process" class="ltx_ref">5</a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Bergelson and D. Swingley</span><span class="ltx_text ltx_bib_year">(2012-02)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">At 6-9 months, human infants know the meanings of many common nouns</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the National Academy of Sciences</span> <span class="ltx_text ltx_bib_volume">109</span> (<span class="ltx_text ltx_bib_number">9</span>), <span class="ltx_text ltx_bib_pages"> pp. 3253–3258</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1073/pnas.1113380109" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1073/pnas.1113380109" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. M. Blei, T. L. Griffiths, M. I. Jordan and J. B. Tenenbaum</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hierarchical topic models and the nested Chinese restaurant process</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib71" title="Advances in neural information processing systems 16" class="ltx_ref">3</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p4" title="2.2 Overview of TLD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. R. Brent and J. M. Siskind</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The role of exposure to isolated words in early vocabulary development</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">81</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. B33–B44</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS1.p1" title="6.1 Corpus ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Christodoulopoulos, S. Goldwater and M. Steedman</span><span class="ltx_text ltx_bib_year">(2010-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Two decades of unsupervised POS induction: how far have we come?</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib68" title="Proceedings of the 48th annual meeting of the association for computational linguistics (acl)" class="ltx_ref">34</a></cite></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Cambridge, MA</span>, <span class="ltx_text ltx_bib_pages"> pp. 575–584</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D10-1056" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS5.p1" title="6.5 Evaluation ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. de Boer and P. K. Kuhl</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Investigating the role of infant-directed speech with a computer model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Acoustics Research Letters Online</span> <span class="ltx_text ltx_bib_volume">4</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 129</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1121/1.1613311" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1121/1.1613311" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Phonetic Categories: IGMM ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Dillon, E. Dunbar and W. Idsardi</span><span class="ltx_text ltx_bib_year">(2013-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A single-stage approach to learning phonological categories: insights from Inuktitut</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognitive Science</span> <span class="ltx_text ltx_bib_volume">37</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 344–377</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/cogs.12008" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1111/cogs.12008" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Phonetic Categories: IGMM ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Elsner, S. Goldwater, N. Feldman and F. Wood</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A cognitive model of early lexical acquisition with phonetic variability</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib2" title="Proceedings of the 2013 conference on empirical methods on natural language processing and computational natural language learning (emnlp-conll)" class="ltx_ref">31</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Overview of LD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. H. Feldman, T. L. Griffiths, S. Goldwater and J. L. Morgan</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A role for the developing lexicon in phonetic category acquisition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychological Review</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p4" title="2.1 Overview of LD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.p2" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.p1" title="3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S5.SS1.p3" title="5.1 Sampling lexeme vowel categories ‣ 5 Inference: Gibbs Sampling ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS3.p1" title="5.3 Hyperparameters ‣ 5 Inference: Gibbs Sampling ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>,
<a href="#S6.SS2.p1" title="6.2 Hillenbrand Vowels ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>,
<a href="#S6.SS6.p2" title="6.6 Results ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.6</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. H. Feldman, E. B. Myers, K. S. White, T. L. Griffiths and J. L. Morgan</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word-level information influences phonetic learning in adults and infants</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">127</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 427–438</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_unpublished"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Fourtassi and E. Dupoux</span><span class="ltx_text ltx_bib_year">(Submitted)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A rudimentary lexicon and semantics help bootstrap phoneme acquisition</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p1" title="2.1 Overview of LD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. C. Frank, N. D. Goodman and J. B. Tenenbaum</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Using speakers’ referential intentions to model early cross-situational word learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychological Science</span> <span class="ltx_text ltx_bib_volume">20</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 578–585</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Friedrich and A. D. Friederici</span><span class="ltx_text ltx_bib_year">(2011-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word learning in 6-month-olds: fast encoding—weak retention</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Cognitive Neuroscience</span> <span class="ltx_text ltx_bib_volume">23</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp. 3228–3240</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1162/jocn_a_00002" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1162/jocn_a_00002" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. J. Gogate and L. E. Bahrick</span><span class="ltx_text ltx_bib_year">(2001-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Intersensory redundancy and 7-month-old infants’ memory for arbitrary syllable-object relations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Infancy</span> <span class="ltx_text ltx_bib_volume">2</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 219–231</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1207/S15327078IN0202_7" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1207/S15327078IN0202_7" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Hillenbrand, L. A. Getty, M. J. Clark and K. Wheeler</span><span class="ltx_text ltx_bib_year">(1995-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Acoustic characteristics of American English vowels.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of the Acoustical Society of America</span> <span class="ltx_text ltx_bib_volume">97</span> (<span class="ltx_text ltx_bib_number">5 Pt 1</span>), <span class="ltx_text ltx_bib_pages"> pp. 3099–3111</span> (<span class="ltx_text ltx_bib_language">eng</span>).
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.F1" title="Figure 1 ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S6.SS2.p1" title="6.2 Hillenbrand Vowels ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. W. Jusczyk and E. A. Hohne</span><span class="ltx_text ltx_bib_year">(1997-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infants’ memory for spoken words</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">277</span> (<span class="ltx_text ltx_bib_number">5334</span>), <span class="ltx_text ltx_bib_pages"> pp. 1984–1986</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1126/science.277.5334.1984" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1126/science.277.5334.1984" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. K. Kuhl, K. A. Williams, F. Lacerda, K. N. Stevens and B. Lindblom</span><span class="ltx_text ltx_bib_year">(1992)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Linguistic experience alters phonetic perception in infants by 6 months of age</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">255</span> (<span class="ltx_text ltx_bib_number">5044</span>), <span class="ltx_text ltx_bib_pages"> pp. 606–608</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_book"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. MacWhinney</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The CHILDES project: tools for analyzing talk</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Lawrence Erlbaum Associates</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS1.p1" title="6.1 Corpus ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. R. Mandel, P. W. Jusczyk and D. B. Pisoni</span><span class="ltx_text ltx_bib_year">(1995-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infants’ recognition of the sound patterns of their own names</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychological Science</span> <span class="ltx_text ltx_bib_volume">6</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 314–317</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/j.1467-9280.1995.tb00517.x" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1111/j.1467-9280.1995.tb00517.x" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Mani and K. Plunkett</span><span class="ltx_text ltx_bib_year">(2010-09)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Twelve-month-olds know their cups from their keps and tups</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Infancy</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 445ï¿¢ï¾ï¾470</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/j.1532-7078.2009.00027.x" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 1525-0008</span>,
<a href="http://dx.doi.org/10.1111/j.1532-7078.2009.00027.x" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS3.p2" title="6.3 Merging Consonant Categories ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>.
</span></li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Maye, D. J. Weiss and R. N. Aslin</span><span class="ltx_text ltx_bib_year">(2008-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical phonetic learning in infants: facilitation and feature generalization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Developmental Science</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 122–134</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/j.1467-7687.2007.00653.x" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1111/j.1467-7687.2007.00653.x" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Maye, J. F. Werker and L. Gerken</span><span class="ltx_text ltx_bib_year">(2002-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infant sensitivity to distributional information can affect phonetic discrimination</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">82</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. B101–B111</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1016/S0010-0277(01)00157-3" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1016/S0010-0277(01)00157-3" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_website"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. McCallum</span><span class="ltx_text ltx_bib_year">(2002)</span><span class="ltx_text ltx_bib_type">(Website)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">http://mallet.cs.umass.edu</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS4.p2" title="6.4 Topics ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>.
</span></li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. McMurray, R. N. Aslin and J. C. Toscano</span><span class="ltx_text ltx_bib_year">(2009-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistical learning of phonetic categories: insights from a computational approach</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Developmental Science</span> <span class="ltx_text ltx_bib_volume">12</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 369–378</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/j.1467-7687.2009.00822.x" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1111/j.1467-7687.2009.00822.x" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Phonetic Categories: IGMM ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. N. Medina, J. Snedeker, J. C. Trueswell and L. R. Gleitman</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How words can and cannot be learned by observation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the National Academy of Sciences</span> <span class="ltx_text ltx_bib_volume">108</span> (<span class="ltx_text ltx_bib_number">22</span>), <span class="ltx_text ltx_bib_pages"> pp. 9014–9019</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p2" title="7 Conclusion ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib39" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Neal</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Markov chain sampling methods for Dirichlet process mixture models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Computational and Graphical Statistics</span> <span class="ltx_text ltx_bib_volume">9</span>, <span class="ltx_text ltx_bib_pages"> pp. 249–265</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS2.p3" title="5.2 Sampling table &amp; topic assignments ‣ 5 Inference: Gibbs Sampling ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
</span></li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Polka and J. F. Werker</span><span class="ltx_text ltx_bib_year">(1994)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Developmental changes in perception of nonnative vowel contrasts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Experimental Psychology: Human Perception and
Performance</span> <span class="ltx_text ltx_bib_volume">20</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 421–435</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S6.SS3.p1" title="6.3 Merging Consonant Categories ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>.
</span></li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (emnlp-conll)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib17" title="V-measure: a conditional entropy-based external cluster evaluation measure" class="ltx_ref">36</a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proceedings of the 2013 conference on empirical methods on natural language processing and computational natural language learning (emnlp-conll)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib1" title="A cognitive model of early lexical acquisition with phonetic variability" class="ltx_ref">10</a>.
</span></li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proceedings of the 34th annual conference of the cognitive science society (cogsci)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib13" title="Distributional learning of vowel categories is supported by prosody in infant-directed speech" class="ltx_ref">1</a>, <a href="#bib.bib18" title="Relating activity contexts to early word learning in dense longitudinal data" class="ltx_ref">37</a>.
</span></li>
<li id="bib.bib70" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proceedings of the 44th annual meeting of the association for computational linguistics and the 21st international conference on computational linguistics</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib19" title="A hierarchical Bayesian language model based on Pitman-Yor processes" class="ltx_ref">43</a>.
</span></li>
<li id="bib.bib68" class="ltx_bibitem ltx_bib_proceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Proceedings of the 48th annual meeting of the association for computational linguistics (acl)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib15" title="Two decades of unsupervised POS induction: how far have we come?" class="ltx_ref">7</a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Rasmussen</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The infinite Gaussian mixture model</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib72" title="Advances in neural information processing systems 13" class="ltx_ref">2</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p1" title="3.1 Phonetic Categories: IGMM ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Rosenberg and J. Hirschberg</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">V-measure: a conditional entropy-based external cluster evaluation measure</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib69" title="Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (emnlp-conll)" class="ltx_ref">30</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS5.p1" title="6.5 Evaluation ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. C. Roy, M. C. Frank and D. Roy</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Relating activity contexts to early word learning in dense longitudinal data</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib67" title="Proceedings of the 34th annual conference of the cognitive science society (cogsci)" class="ltx_ref">32</a></cite></span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p4" title="2.2 Overview of TLD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib41" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Shi and J. F. Werker</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The basis of preference for lexical words in 6-month-old infants</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Developmental Science</span> <span class="ltx_text ltx_bib_volume">6</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 484–488</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text  ltx_bib_external">Review Crosslinguistic experiment (testing chinese-hearing
infants on english function words) shows that infants
prefer content words over function words, suggesting that
increased salience of content words is universal (or at
least signalled by the same features in English and
Chinese).

<li id="bib.bib49a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">49</span>
<span class="ltx_bibblock">”This is reasonable given that previous work on typolo-

<li id="bib.bib50a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">50</span>
<span class="ltx_bibblock">gically distinct languages (Shi et al., 1998) has shown

<li id="bib.bib51a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">51</span>
<span class="ltx_bibblock">that lexical words are universally more complex than

<li id="bib.bib52a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">52</span>
<span class="ltx_bibblock">function words. Acoustically, lexical words are longer,

<li id="bib.bib53a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">53</span>
<span class="ltx_bibblock">are more likely to contain non-reduced vowels, and are

<li id="bib.bib54a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">54</span>
<span class="ltx_bibblock">louder across languages. Phonologically, lexical words

<li id="bib.bib55a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">55</span>
<span class="ltx_bibblock">are also universally more complex in syllable structure

<li id="bib.bib56a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">56</span>
<span class="ltx_bibblock">(within the constraints of the given mature language).

<li id="bib.bib57a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">57</span>
<span class="ltx_bibblock">These distinctions in the input could allow prelingual

<li id="bib.bib58a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">58</span>
<span class="ltx_bibblock">infants across languages to attend preferentially to lexical

<li id="bib.bib59a" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">59</span>
<span class="ltx_bibblock">words.”
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li>
</span></li></span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS1.p2" title="6.1 Corpus ‣ 6 Experiments ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.
</span></li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Shukla, K. S. White and R. N. Aslin</span><span class="ltx_text ltx_bib_year">(2011-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the National Academy of Sciences</span> <span class="ltx_text ltx_bib_volume">108</span> (<span class="ltx_text ltx_bib_number">15</span>), <span class="ltx_text ltx_bib_pages"> pp. 6038–6043</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1073/pnas.1017617108" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1073/pnas.1017617108" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. B. Smith and C. Yu</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infants rapidly learn word-referent mappings via cross-situational statistics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">106</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 1558–1568</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S7.p2" title="7 Conclusion ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. L. Stager and J. F. Werker</span><span class="ltx_text ltx_bib_year">(1997)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infants listen for more phonetic detail in speech perception than in word-learning tasks</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature</span> <span class="ltx_text ltx_bib_volume">388</span>, <span class="ltx_text ltx_bib_pages"> pp. 381–382</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p4" title="2.1 Overview of LD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span></li>
<li id="bib.bib61" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Swingley</span><span class="ltx_text ltx_bib_year">(2009-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contributions of infant word learning to language development</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Philosophical Transactions of the Royal Society B:
Biological Sciences</span> <span class="ltx_text ltx_bib_volume">364</span> (<span class="ltx_text ltx_bib_number">1536</span>), <span class="ltx_text ltx_bib_pages"> pp. 3617–3632</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1098/rstb.2009.0107" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1098/rstb.2009.0107" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p2" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. W. Teh</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A hierarchical Bayesian language model based on Pitman-Yor processes</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib70" title="Proceedings of the 44th annual meeting of the association for computational linguistics and the 21st international conference on computational linguistics" class="ltx_ref">33</a></cite></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sydney</span>, <span class="ltx_text ltx_bib_pages"> pp. 985 – 992</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p1" title="4 Topic-Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Teinonen, R. N. Aslin, P. Alku and G. Csibra</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Visual speech contributes to phonetic learning in 6-month-old infants</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">108</span>, <span class="ltx_text ltx_bib_pages"> pp. 850–855</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. D. Thiessen</span><span class="ltx_text ltx_bib_year">(2007-01)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The effect of distributional information on children’s use of phonemic contrasts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Memory and Language</span> <span class="ltx_text ltx_bib_volume">56</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 16–34</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1016/j.jml.2006.07.002" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 0749-596X</span>,
<a href="http://dx.doi.org/10.1016/j.jml.2006.07.002" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS1.p4" title="2.1 Overview of LD model ‣ 2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span></li>
<li id="bib.bib64" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Tincoff and P. W. Jusczyk</span><span class="ltx_text ltx_bib_year">(1999-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Some beginnings of word comprehension in 6-month-olds</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychological Science</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 172–175</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/1467-9280.00127" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1111/1467-9280.00127" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Tincoff and P. W. Jusczyk</span><span class="ltx_text ltx_bib_year">(2012-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Six-month-olds comprehend words that refer to parts of the body</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Infancy</span> <span class="ltx_text ltx_bib_volume">17</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 432ï¿¢ï¾ï¾444</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1111/j.1532-7078.2011.00084.x" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 1525-0008</span>,
<a href="http://dx.doi.org/10.1111/j.1532-7078.2011.00084.x" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_book"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. S. Trubetzkoy</span><span class="ltx_text ltx_bib_year">(1939)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Grundzüge der Phonologie</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Vandenhoeck und Ruprecht</span>, <span class="ltx_text ltx_bib_place">Göttingen</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. C. Trueswell, T. N. Medina, A. Hafri and L. R. Gleitman</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Propose but verify: fast mapping meets cross-situational word learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognitive Psychology</span> <span class="ltx_text ltx_bib_volume">66</span>, <span class="ltx_text ltx_bib_pages"> pp. 126–156</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p2" title="7 Conclusion ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. K. Vallabha, J. L. McClelland, F. Pons, J. F. Werker and S. Amano</span><span class="ltx_text ltx_bib_year">(2007-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised learning of vowel categories from infant-directed speech</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Proceedings of the National Academy of Sciences</span> <span class="ltx_text ltx_bib_volume">104</span> (<span class="ltx_text ltx_bib_number">33</span>), <span class="ltx_text ltx_bib_pages"> pp. 13273–13278</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1073/pnas.0705369104" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1073/pnas.0705369104" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p5" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Background and overview of models ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS1.p1" title="3.1 Phonetic Categories: IGMM ‣ 3 Lexical-Distributional Model ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. F. Werker and R. C. Tees</span><span class="ltx_text ltx_bib_year">(1984)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cross-language speech perception: evidence for perceptual reorganization during the first year of life</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Infant Behavior and Development</span> <span class="ltx_text ltx_bib_volume">7</span>, <span class="ltx_text ltx_bib_pages"> pp. 49–63</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib66" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. H. Yeung and J. F. Werker</span><span class="ltx_text ltx_bib_year">(2009-11)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning words’ sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognition</span> <span class="ltx_text ltx_bib_volume">113</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 234–243</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1016/j.cognition.2009.08.010" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://dx.doi.org/10.1016/j.cognition.2009.08.010" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Yu and L. B. Smith</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rapid word learning under uncertainty via cross-situational statistics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychological Science</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 414–420</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S7.p2" title="7 Conclusion ‣ Weak semantic context helps phonetic learning in a model of infant language acquisition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:30:40 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
