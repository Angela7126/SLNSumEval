<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Online Learning in Tensor Space</title>
<!--Generated on Wed Jun 11 18:55:55 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Online Learning in Tensor Space</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuan Cao   Sanjeev Khudanpur
<br class="ltx_break"/>Center for Language &amp; Speech Processing and Human Language Technology Center of Excellence
<br class="ltx_break"/>The Johns Hopkins University
<br class="ltx_break"/>Baltimore, MD, USA, 21218
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">yuan.cao, khudanpur</span>}<span class="ltx_text ltx_font_typewriter">@jhu.edu</span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We propose an online learning algorithm based on tensor-space models. A tensor-space model represents data in a compact way, and via rank-1 approximation the weight tensor can be made highly structured, resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models. This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training. We apply with the proposed algorithm to a parsing task, and show that even with very little training data the learning algorithm based on a tensor model performs well, and gives significantly better results than standard learning algorithms based on traditional vector-space models.</p>
</div><span class="ltx_ERROR undefined">\DeclarePairedDelimiter</span><span class="ltx_ERROR undefined">\ceil</span>⌈⌉<span class="ltx_ERROR undefined">\DeclarePairedDelimiter</span><span class="ltx_ERROR undefined">\floor</span>⌊⌋
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Many NLP applications use models that try to incorporate a large number of linguistic features so that as much human knowledge of language can be brought to bear on the (prediction) task as possible.
This also makes training the model parameters a challenging problem, since the amount of labeled training data is usually small compared to the size of feature sets: the feature weights cannot be estimated reliably.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Most traditional models are linear models, in the sense that both the features of the data and model parameters are represented as vectors in a vector space. Many learning algorithms applied to NLP problems, such as the Perceptron <cite class="ltx_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Collins2002</a>]</cite>, MIRA <cite class="ltx_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Crammer et al.2006</a>, <a href="#bib.bibx14" title="" class="ltx_ref">McDonald et al.2005</a>, <a href="#bib.bibx4" title="" class="ltx_ref">Chiang et al.2008</a>]</cite>, PRO <cite class="ltx_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Hopkins and May2011</a>]</cite>, RAMPION <cite class="ltx_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Gimpel and Smith2012</a>]</cite> etc., are based on vector-space models. Such models require learning individual feature weights directly, so that the number of parameters to be estimated is identical to the size of the feature set. When millions of features are used but the amount of labeled data is limited, it can be difficult to precisely estimate each feature weight.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper, we shift the model from vector-space to tensor-space. Data can be represented in a compact and structured way using tensors as containers. Tensor representations have been applied to computer vision problems <cite class="ltx_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Hazan et al.2005</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Shashua and Hazan2005</a>]</cite> and information retrieval <cite class="ltx_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Cai et al.2006a</a>]</cite> a long time ago. More recently, it has also been applied to parsing <cite class="ltx_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Cohen and Collins2012</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Cohen and Satta2013</a>]</cite> and semantic analysis <cite class="ltx_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Van de Cruys et al.2013</a>]</cite>. A linear tensor model represents both features and weights in tensor-space, hence the weight tensor can be factorized and approximated by a linear sum of rank-1 tensors. This low-rank approximation imposes structural constraints on the feature weights and can be regarded as a form of regularization. With this representation, we no longer need to estimate individual feature weights directly but only a small number of “bases” instead. This property makes the the tensor model very effective when training a large number of feature weights in a low-resource environment. On the other hand,
tensor
models have many more degrees of “design freedom”
than vector space models. While this makes them very flexible, it also creates much difficulty in designing an optimal tensor structure for a given training set.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">We give detailed description of the tensor space model in Section <a href="#S2" title="2 Tensor Space Representation ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Several issues that come with the tensor model construction are addressed in Section <a href="#S3" title="3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. A tensor weight learning algorithm is then proposed in <a href="#S4" title="4 Online Learning Algorithm ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally we give our experimental results on a parsing task and analysis in Section <a href="#S5" title="5 Experiments ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Tensor Space Representation</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Most of the learning algorithms for NLP problems are based on vector space models, which represent data as vectors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="\boldsymbol{\phi}\in\mathbb{R}^{n}" display="inline"><mrow><mi mathvariant="bold-italic">ϕ</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow></math>, and try to learn feature weight vectors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m2" class="ltx_Math" alttext="\boldsymbol{w}\in\mathbb{R}^{n}" display="inline"><mrow><mi>𝒘</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow></math> such that a linear model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m3" class="ltx_Math" alttext="y=\boldsymbol{w}\cdot\boldsymbol{\phi}" display="inline"><mrow><mi>y</mi><mo>=</mo><mrow><mi>𝒘</mi><mo>⋅</mo><mi mathvariant="bold-italic">ϕ</mi></mrow></mrow></math> is able to discriminate between, say, good and bad hypotheses. While this is a natural way of representing data, it is not the only choice. Below, we reformulate the model from vector to tensor space.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Tensor Space Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">A tensor is a multidimensional array, and is a generalization of commonly used algebraic objects such as vectors and matrices. Specifically, a vector is a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="1^{\mathrm{st}}" display="inline"><msup><mn>1</mn><mi>st</mi></msup></math> order tensor, a matrix is a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math> order tensor, and data organized as a rectangular cuboid is a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="3^{\mathrm{rd}}" display="inline"><msup><mn>3</mn><mi>rd</mi></msup></math> order tensor etc. In general, a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor is represented as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m5" class="ltx_Math" alttext="\mathcal{T}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots n_{D}}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></math>, and an entry in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m6" class="ltx_Math" alttext="\mathcal{T}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒯</mi></math> is denoted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m7" class="ltx_Math" alttext="\mathcal{T}_{i_{1},i_{2},\ldots,i_{D}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></math>. Different dimensions of a tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m8" class="ltx_Math" alttext="1,2,\ldots,D" display="inline"><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></math> are named modes of the tensor.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">Using a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor as container, we can assign each feature of the task a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>-dimensional index in the tensor and represent the data as tensors. Of course, shifting from a vector to a tensor representation entails several additional degrees of freedom, e.g., the order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m3" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> of the tensor and the sizes <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m4" class="ltx_Math" alttext="\{n_{d}\}_{d=1}^{D}" display="inline"><msubsup><mrow><mo>{</mo><msub><mi>n</mi><mi>d</mi></msub><mo>}</mo></mrow><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup></math> of the modes, which must be addressed when selecting a tensor model. This will be done in Section <a href="#S3" title="3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Tensor Decomposition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">Just as a matrix can be decomposed as a linear combination of several rank-1 matrices via SVD, tensors also admit decompositions<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>The form of tensor decomposition defined here is named as CANDECOMP/PARAFAC(CP) decomposition <cite class="ltx_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Kolda and Bader2009</a>]</cite>. Another popular form of tensor decomposition is called Tucker decomposition, which decomposes a tensor into a core tensor multiplied by a matrix along each mode. We focus only on the CP decomposition in this paper.</span></span></span> into linear combinations of “rank-1” tensors. A <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m1" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m2" class="ltx_Math" alttext="\mathcal{A}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots n_{D}}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></math> is rank-1 if it can be written as the outer product of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m3" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> vectors, i.e.</p>
<table id="S2.Ex1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m1" class="ltx_Math" alttext="\mathcal{A}=\mathbf{a}^{1}\otimes\mathbf{a}^{2}\otimes,\ldots,\otimes\mathbf{a%&#10;}^{D}," display="block"><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>=</mo><msup><mi>𝐚</mi><mn>1</mn></msup><mo>⊗</mo><msup><mi>𝐚</mi><mn>2</mn></msup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msup><mi>𝐚</mi><mi>D</mi></msup><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m4" class="ltx_Math" alttext="\mathbf{a}^{i}\in\mathbb{R}^{n_{d}},1\leq d\leq D" display="inline"><mrow><mrow><msup><mi>𝐚</mi><mi>i</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>d</mi></msub></msup></mrow><mo>,</mo><mrow><mn>1</mn><mo>≤</mo><mi>d</mi><mo>≤</mo><mi>D</mi></mrow></mrow></math>. A <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m5" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m6" class="ltx_Math" alttext="\mathcal{T}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots n_{D}}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></math> can be factorized into a sum of component rank-1 tensors as</p>
<table id="S2.Ex2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m1" class="ltx_Math" alttext="\mathcal{T}=\sum\limits_{r=1}^{R}\mathcal{A}_{r}=\sum\limits_{r=1}^{R}\mathbf{%&#10;a}_{r}^{1}\otimes\mathbf{a}_{r}^{2}\otimes,\ldots,\otimes\mathbf{a}_{r}^{D}" display="block"><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>=</mo><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><msub><mi class="ltx_font_mathcaligraphic">𝒜</mi><mi>r</mi></msub><mo>=</mo><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><msubsup><mi>𝐚</mi><mi>r</mi><mn>1</mn></msubsup><mo>⊗</mo><msubsup><mi>𝐚</mi><mi>r</mi><mn>2</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝐚</mi><mi>r</mi><mi>D</mi></msubsup></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m7" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math>, called the rank of the tensor, is the minimum number of rank-1 tensors whose sum equals <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m8" class="ltx_Math" alttext="\mathcal{T}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒯</mi></math>. Via decomposition, one may approximate a tensor by the sum of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m9" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> major rank-1 tensors with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m10" class="ltx_Math" alttext="H\leq R" display="inline"><mrow><mi>H</mi><mo>≤</mo><mi>R</mi></mrow></math>.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Linear Tensor Model</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">In tensor space, a linear model may be written (ignoring a bias term) as</p>
<table id="S2.Ex3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m1" class="ltx_Math" alttext="f(\boldsymbol{W})=\boldsymbol{W}\circ\boldsymbol{\Phi}," display="block"><mrow><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝑾</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>𝑾</mi><mo>∘</mo><mi>𝚽</mi></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m1" class="ltx_Math" alttext="\boldsymbol{\Phi}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots n_{D}}" display="inline"><mrow><mi>𝚽</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></math> is the feature tensor, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m2" class="ltx_Math" alttext="\boldsymbol{W}" display="inline"><mi>𝑾</mi></math> is the corresponding weight tensor, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m3" class="ltx_Math" alttext="\circ" display="inline"><mo>∘</mo></math> denotes the Hadamard product. If <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m4" class="ltx_Math" alttext="\boldsymbol{W}" display="inline"><mi>𝑾</mi></math> is further decomposed as the sum of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m5" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> major component rank-1 tensors, i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m6" class="ltx_Math" alttext="\boldsymbol{W}\approx\sum_{h=1}^{H}\boldsymbol{w}_{h}^{1}\otimes\boldsymbol{w}%&#10;_{h}^{2}\otimes,\ldots,\otimes\boldsymbol{w}_{h}^{D}" display="inline"><mrow><mi>𝑾</mi><mo>≈</mo><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></msubsup><msubsup><mi>𝒘</mi><mi>h</mi><mn>1</mn></msubsup><mo>⊗</mo><msubsup><mi>𝒘</mi><mi>h</mi><mn>2</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mi>h</mi><mi>D</mi></msubsup></mrow></math>, then</p>
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m1" class="ltx_Math" alttext="\displaystyle f(\boldsymbol{w}_{1}^{1},\ldots,\boldsymbol{w}_{1}^{D},\ldots,%&#10;\boldsymbol{w}_{h}^{1},\ldots,\boldsymbol{w}_{h}^{D})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi>𝒘</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mn>1</mn><mi>D</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mi>h</mi><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mi>h</mi><mi>D</mi></msubsup></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
<tr id="S2.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{h=1}^{H}\boldsymbol{\Phi}\times_{1}\boldsymbol{w}_{h%&#10;}^{1}\times_{2}\boldsymbol{w}_{h}^{2}\ldots\times_{D}\boldsymbol{w}_{h}^{D}," display="inline"><mrow><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></munderover></mstyle><mrow><mrow><mrow><mrow><mi>𝚽</mi><msub><mo>×</mo><mn>1</mn></msub><msubsup><mi>𝒘</mi><mi>h</mi><mn>1</mn></msubsup></mrow><msub><mo>×</mo><mn>2</mn></msub><msubsup><mi>𝒘</mi><mi>h</mi><mn>2</mn></msubsup></mrow><mo>⁢</mo><mi mathvariant="normal">…</mi></mrow><msub><mo>×</mo><mi>D</mi></msub><msubsup><mi>𝒘</mi><mi>h</mi><mi>D</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m7" class="ltx_Math" alttext="\times_{l}" display="inline"><msub><mo>×</mo><mi>l</mi></msub></math> is the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m8" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math>-mode product operator between a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m9" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m10" class="ltx_Math" alttext="\mathcal{T}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒯</mi></math> and a vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m11" class="ltx_Math" alttext="\mathbf{a}" display="inline"><mi>𝐚</mi></math> of dimension <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m12" class="ltx_Math" alttext="n_{d}" display="inline"><msub><mi>n</mi><mi>d</mi></msub></math>, yielding a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m13" class="ltx_Math" alttext="(D-1)^{\mathrm{th}}" display="inline"><msup><mrow><mo>(</mo><mrow><mi>D</mi><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow><mi>th</mi></msup></math> order tensor such that</p>
<table id="A1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.Ex5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle(\mathcal{T}\times_{l}\mathbf{a})_{i_{1},\ldots,i_{l-1},i_{l+1},%&#10;\ldots,i_{D}}" display="inline"><msub><mrow><mo>(</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒯</mi><msub><mo>×</mo><mi>l</mi></msub><mi>𝐚</mi></mrow><mo>)</mo></mrow><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>i</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{i_{l}=1}^{n_{d}}\mathcal{T}_{i_{1},\ldots,i_{l-1},i_%&#10;{l},i_{l+1},\ldots,i_{D}}\cdot a_{i_{l}}." display="inline"><mrow><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>i</mi><mi>l</mi></msub><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>d</mi></msub></munderover></mstyle><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>i</mi><mi>l</mi></msub><mo>,</mo><msub><mi>i</mi><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub><mo>⋅</mo><msub><mi>a</mi><msub><mi>i</mi><mi>l</mi></msub></msub></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The linear tensor model is illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Linear Tensor Model ‣ 2 Tensor Space Representation ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.F1" class="ltx_figure"><img src="P14-1063/image001.png" id="S2.F1.g1" class="ltx_graphics ltx_centering" width="318" height="144" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m3" class="ltx_Math" alttext="3^{\mathrm{rd}}" display="inline"><msup><mn>3</mn><mi>rd</mi></msup></math> order linear tensor model. The feature weight tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.F1.m4" class="ltx_Math" alttext="\boldsymbol{W}" display="inline"><mi>𝑾</mi></math> can be decomposed as the sum of a sequence of rank-1 component tensors.</div>
</div>
</div>
<div id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.4 </span>Why Learning in Tensor Space?</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">So what is the advantage of learning with a tensor model instead of a vector model? Consider the case where we have defined 1,000,000 features for our task. A vector space linear model requires estimating 1,000,000 free parameters. However if we use a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p1.m1" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math> order tensor model, organize the features into a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p1.m2" class="ltx_Math" alttext="1000\times 1000" display="inline"><mrow><mn>1000</mn><mo>×</mo><mn>1000</mn></mrow></math> matrix <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p1.m3" class="ltx_Math" alttext="\boldsymbol{\Phi}" display="inline"><mi>𝚽</mi></math>, and use just one rank-1 matrix to approximate the weight tensor, then the linear model becomes</p>
<table id="S2.Ex6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex6.m1" class="ltx_Math" alttext="f(\boldsymbol{w}_{1},\boldsymbol{w}_{2})=\boldsymbol{w}_{1}^{T}\boldsymbol{%&#10;\Phi}\boldsymbol{w}_{2}," display="block"><mrow><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>𝒘</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒘</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mi>𝒘</mi><mn>1</mn><mi>T</mi></msubsup><mo>⁢</mo><mi>𝚽</mi><mo>⁢</mo><msub><mi>𝒘</mi><mn>2</mn></msub></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p1.m4" class="ltx_Math" alttext="\boldsymbol{w}_{1},\boldsymbol{w}_{2}\in\mathbb{R}^{1000}" display="inline"><mrow><mrow><msub><mi>𝒘</mi><mn>1</mn></msub><mo>,</mo><msub><mi>𝒘</mi><mn>2</mn></msub></mrow><mo>∈</mo><msup><mi>ℝ</mi><mn>1000</mn></msup></mrow></math>. That is to say, now we only need to estimate 2000 parameters!</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p">In general, if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m1" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> features are defined for a learning problem, and we (i) organize the feature set as a tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m2" class="ltx_Math" alttext="\boldsymbol{\Phi}\in\mathbb{R}^{n_{1}\times n_{2}\times\ldots n_{D}}" display="inline"><mrow><mi>𝚽</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></math> and (ii) use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> component rank-1 tensors to approximate the corresponding target weight tensor. Then the total number of parameters to be learned for this tensor model is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m4" class="ltx_Math" alttext="H\sum_{d=1}^{D}n_{d}" display="inline"><mrow><mi>H</mi><mo>⁢</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow></math>, which is usually much smaller than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m5" class="ltx_Math" alttext="V=\prod_{d=1}^{D}n_{d}" display="inline"><mrow><mi>V</mi><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow></math> for a traditional vector space model. Therefore we expect the tensor model to be more effective in a low-resource training environment.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p class="ltx_p">Specifically, a vector space model assumes each feature weight to be a “free” parameter, and estimating them reliably could therefore be hard when training data are not sufficient or the feature set is huge. By contrast, a linear tensor model only needs to learn <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m1" class="ltx_Math" alttext="H\sum_{d=1}^{D}n_{d}" display="inline"><mrow><mi>H</mi><mo>⁢</mo><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow></math> “bases” of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m2" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> feature weights instead of individual weights directly. The weight corresponding to the feature <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m3" class="ltx_Math" alttext="\Phi_{i_{1},i_{2},\ldots,i_{D}}" display="inline"><msub><mi mathvariant="normal">Φ</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></math> in the tensor model is expressed as</p>
<table id="A1.EGx3" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m1" class="ltx_Math" alttext="\displaystyle w_{i_{1},i_{2},\ldots,i_{D}}" display="inline"><msub><mi>w</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{h=1}^{H}w_{h,i_{1}}^{1}w_{h,i_{2}}^{2}\ldots w_{h,i_%&#10;{D}}^{D}," display="inline"><mrow><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></munderover></mstyle><mrow><msubsup><mi>w</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>i</mi><mn>1</mn></msub></mrow><mn>1</mn></msubsup><mo>⁢</mo><msubsup><mi>w</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>i</mi><mn>2</mn></msub></mrow><mn>2</mn></msubsup><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msubsup><mi>w</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow><mi>D</mi></msubsup></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m4" class="ltx_Math" alttext="w_{h,i_{j}}^{j}" display="inline"><msubsup><mi>w</mi><mrow><mi>h</mi><mo>,</mo><msub><mi>i</mi><mi>j</mi></msub></mrow><mi>j</mi></msubsup></math> is the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m5" class="ltx_Math" alttext="i_{j}^{\mathrm{th}}" display="inline"><msubsup><mi>i</mi><mi>j</mi><mi>th</mi></msubsup></math> element in the vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p3.m6" class="ltx_Math" alttext="\boldsymbol{w}_{h}^{j}" display="inline"><msubsup><mi>𝒘</mi><mi>h</mi><mi>j</mi></msubsup></math>.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p class="ltx_p">In other words, a true feature weight is now approximated by a set of bases. This reminds us of the well-known low-rank matrix approximation of images via SVD, and we are applying similar techniques to approximate target feature weights, which is made possible only after we shift from vector to tensor space models.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p class="ltx_p">This approximation can be treated as a form of model regularization, since the weight tensor is represented in a constrained form and made highly structured via the rank-1 tensor approximation. Of course, as we reduce the model complexity, e.g. by choosing a smaller and smaller <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p5.m1" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>, the model’s expressive ability is weakened at the same time. We will elaborate on this point in Section <a href="#S3.SS1" title="3.1 Tensor Order ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Tensor Model Construction</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">To apply a tensor model, we first need to convert the feature vector into a tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="\boldsymbol{\Phi}" display="inline"><mi>𝚽</mi></math>. Once the structure of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="\boldsymbol{\Phi}" display="inline"><mi>𝚽</mi></math> is determined, the structure of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="\boldsymbol{W}" display="inline"><mi>𝑾</mi></math> is fixed as well. As mentioned in Section <a href="#S2.SS1" title="2.1 Tensor Space Model ‣ 2 Tensor Space Representation ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, a tensor model has many more degrees of “design freedom” than a vector model, which makes the problem of finding a good tensor structure a nontrivial one.</p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Tensor Order</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">The order of a tensor affects the model in two ways: the expressiveness of the model and the number of parameters to be estimated. We assume <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="H=1" display="inline"><mrow><mi>H</mi><mo>=</mo><mn>1</mn></mrow></math> in the analysis below, noting that one can always add as many rank-1 component tensors as needed to approximate a tensor with arbitrary precision.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Obviously, the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m1" class="ltx_Math" alttext="1^{\mathrm{st}}" display="inline"><msup><mn>1</mn><mi>st</mi></msup></math> order tensor (vector) model is the most expressive, since it is structureless and any arbitrary set of numbers can always be represented exactly as a vector. The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m2" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math> order rank-1 tensor (rank-1 matrix) is less expressive because not every set of numbers can be organized into a rank-1 matrix. In general, a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m3" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order rank-1 tensor is more expressive than a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m4" class="ltx_Math" alttext="(D+1)^{\mathrm{th}}" display="inline"><msup><mrow><mo>(</mo><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow><mo>)</mo></mrow><mi>th</mi></msup></math> order rank-1 tensor, as a lower-order tensor imposes less structural constraints on the set of numbers it can express. We formally state this fact as follows:</p>
</div>
<div id="Thmthm1" class="ltx_theorem ltx_theorem_thm">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem">Theorem 1</span>.</h6>
<div id="Thmthm1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A set of real numbers that can be represented by a <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m1" class="ltx_Math" alttext="(D+1)^{\mathrm{th}}" display="inline"><msup><mrow><mo mathvariant="italic">(</mo><mrow><mi>D</mi><mo mathvariant="normal">+</mo><mn mathvariant="normal">1</mn></mrow><mo mathvariant="italic">)</mo></mrow><mi mathvariant="normal">th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m2" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒬</mi></math> can also be represented by a <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m3" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi mathvariant="normal">th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m4" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>, provided <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m5" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="Thmthm1.p1.m6" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒬</mi></math> have the same volume. But the reverse is not true.</span></p>
</div>
</div>
<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">See appendix.
∎</p>
</div>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">On the other hand, tensor order also affects the number of parameters to be trained. Assuming that a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m1" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order has equal size on each mode (we will elaborate on this point in Section <a href="#S3.SS2" title="3.2 Mode Size ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and the volume (number of entries) of the tensor is fixed as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m2" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math>, then the total number of parameters of the model is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m3" class="ltx_Math" alttext="DV^{\frac{1}{D}}" display="inline"><mrow><mi>D</mi><mo>⁢</mo><msup><mi>V</mi><mfrac><mn>1</mn><mi>D</mi></mfrac></msup></mrow></math>. This is a convex function of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m4" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>, and the minimum<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>The optimal integer solution can be determined simply by comparing the two function values.</span></span></span> is reached at either <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m5" class="ltx_Math" alttext="D^{\ast}=\floor{\ln V}" display="inline"><mrow><msup><mi>D</mi><mo>∗</mo></msup><mo>=</mo><mrow><merror class="ltx_ERROR undefined undefined"><mtext>\floor</mtext></merror><mo>⁢</mo><mrow><mi>ln</mi><mo>⁡</mo><mi>V</mi></mrow></mrow></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p4.m6" class="ltx_Math" alttext="D^{\ast}=\ceil{\ln V}" display="inline"><mrow><msup><mi>D</mi><mo>∗</mo></msup><mo>=</mo><mrow><merror class="ltx_ERROR undefined undefined"><mtext>\ceil</mtext></merror><mo>⁢</mo><mrow><mi>ln</mi><mo>⁡</mo><mi>V</mi></mrow></mrow></mrow></math>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">Therefore, as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> increases from 1 to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m2" class="ltx_Math" alttext="D^{*}" display="inline"><msup><mi>D</mi><mo>*</mo></msup></math>, we lose more and more of the expressive power of the model but reduce the number of parameters to be trained. However it would be a bad idea to choose a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m3" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> beyond <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p5.m4" class="ltx_Math" alttext="D^{*}" display="inline"><msup><mi>D</mi><mo>*</mo></msup></math>. The optimal tensor order depends on the nature of the actual problem, and we tune this hyper-parameter on a held-out set.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Mode Size</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">The size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="n_{d}" display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> of each tensor mode, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="d=1,\ldots,D" display="inline"><mrow><mi>d</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></mrow></math>, determines the structure of feature weights a tensor model can precisely represent, as well as the number of parameters to estimate (we also assume <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m3" class="ltx_Math" alttext="H=1" display="inline"><mrow><mi>H</mi><mo>=</mo><mn>1</mn></mrow></math> in the analysis below). For example, if the tensor order is 2 and the volume <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m4" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> is 12, then we can either choose <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m5" class="ltx_Math" alttext="n_{1}=3,n_{2}=4" display="inline"><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mn>4</mn></mrow></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m6" class="ltx_Math" alttext="n_{1}=2,n_{2}=6" display="inline"><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn></mrow><mo>,</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mn>6</mn></mrow></mrow></math>. For <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m7" class="ltx_Math" alttext="n_{1}=3,n_{2}=4" display="inline"><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mn>3</mn></mrow><mo>,</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mn>4</mn></mrow></mrow></math>, the numbers that can be precisely represented are divided into 3 groups, each having 4 numbers, that are scaled versions of one another. Similarly for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m8" class="ltx_Math" alttext="n_{1}=2,n_{2}=6" display="inline"><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn></mrow><mo>,</mo><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mn>6</mn></mrow></mrow></math>, the numbers can be divided into 2 groups with different scales. Obviously, the two possible choices of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m9" class="ltx_Math" alttext="(n_{1},n_{2})" display="inline"><mrow><mo>(</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></math> also lead to different numbers of free parameters (7 vs. 8).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Given <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m2" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math>, there are many possible combinations of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m3" class="ltx_Math" alttext="n_{d},d=1,\ldots,D" display="inline"><mrow><mrow><mrow><msub><mi>n</mi><mi>d</mi></msub><mo>,</mo><mi>d</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></mrow></math>, and the optimal combination should indeed be determined by the structure of target features weights. However it is hard to know the structure of target feature weights before learning, and it would be impractical to try every possible combination of mode sizes, therefore we choose the criterion of determining the mode sizes as minimization of the total number of parameters, namely we solve the problem:</p>
<table id="A1.EGx4" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S3.Ex7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex7.m1" class="ltx_Math" alttext="\displaystyle\min\limits_{n_{1},\ldots,n_{D}}\sum\limits_{d=1}^{D}n_{d}\quad s%&#10;.t\quad\prod\limits_{d=1}^{D}n_{d}=V" display="inline"><mrow><mrow><mrow><munder><mo movablelimits="false">min</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></munder><mo>⁡</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover></mstyle><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow><mo separator="true"> </mo><mi>s</mi></mrow><mo separator="true">.</mo><mrow><mrow><mi>t</mi><mo separator="true"> </mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover></mstyle><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow><mo>=</mo><mi>V</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The optimal solution is reached when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m4" class="ltx_Math" alttext="n_{1}=n_{2}=\ldots=n_{D}=V^{\frac{1}{D}}" display="inline"><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mi mathvariant="normal">…</mi><mo>=</mo><msub><mi>n</mi><mi>D</mi></msub><mo>=</mo><msup><mi>V</mi><mfrac><mn>1</mn><mi>D</mi></mfrac></msup></mrow></math>. Of course it is not guaranteed that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m5" class="ltx_Math" alttext="V^{\frac{1}{D}}" display="inline"><msup><mi>V</mi><mfrac><mn>1</mn><mi>D</mi></mfrac></msup></math> is an integer, therefore we choose <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m6" class="ltx_Math" alttext="n_{d}=\floor{V^{\frac{1}{D}}}" display="inline"><mrow><msub><mi>n</mi><mi>d</mi></msub><mo>=</mo><mrow><merror class="ltx_ERROR undefined undefined"><mtext>\floor</mtext></merror><mo>⁢</mo><msup><mi>V</mi><mfrac><mn>1</mn><mi>D</mi></mfrac></msup></mrow></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m7" class="ltx_Math" alttext="\ceil{V^{\frac{1}{D}}},d=1,\ldots,D" display="inline"><mrow><mrow><mrow><mrow><merror class="ltx_ERROR undefined undefined"><mtext>\ceil</mtext></merror><mo>⁢</mo><msup><mi>V</mi><mfrac><mn>1</mn><mi>D</mi></mfrac></msup></mrow><mo>,</mo><mi>d</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></mrow></math> such that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m8" class="ltx_Math" alttext="\prod_{d=1}^{D}n_{d}\geq V" display="inline"><mrow><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow><mo>≥</mo><mi>V</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m9" class="ltx_Math" alttext="\left[\prod_{d=1}^{D}n_{d}\right]-V" display="inline"><mrow><mrow><mo>[</mo><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow><mo>]</mo></mrow><mo>-</mo><mi>V</mi></mrow></math> is minimized. The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m10" class="ltx_Math" alttext="\left[\prod_{d=1}^{D}n_{d}\right]-V" display="inline"><mrow><mrow><mo>[</mo><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msub><mi>n</mi><mi>d</mi></msub></mrow><mo>]</mo></mrow><mo>-</mo><mi>V</mi></mrow></math> extra entries of the tensor correspond to no features and are used just for padding. Since for each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m11" class="ltx_Math" alttext="n_{d}" display="inline"><msub><mi>n</mi><mi>d</mi></msub></math> there are only two possible values to choose, we can simply enumerate all the possible <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m12" class="ltx_Math" alttext="2^{D}" display="inline"><msup><mn>2</mn><mi>D</mi></msup></math> (which is usually a small number) combinations of values and pick the one that matches the conditions given above. This way <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p2.m13" class="ltx_Math" alttext="n_{1},\ldots,n_{D}" display="inline"><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></math> are fully determined.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">Here we are only following the principle of minimizing the parameter number. While this strategy might work well with small amount of training data, it is not guaranteed to be the best strategy in all cases, especially when more data is available we might want to increase the number of parameters, making the model more complex so that the data can be more precisely modeled. Ideally the mode size needs to be adaptive to the amount of training data as well as the property of target weights. A theoretically guaranteed optimal approach to determining the mode sizes remains an open problem, and will be explored in our future work.</p>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Number of Rank-1 Tensors</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">The impact of using <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m1" class="ltx_Math" alttext="H&gt;1" display="inline"><mrow><mi>H</mi><mo>&gt;</mo><mn>1</mn></mrow></math> rank-1 tensors is obvious: a larger <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m2" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> increases the model complexity and makes the model more expressive, since we are able to approximate target weight tensor with smaller error. As a trade-off, the number of parameters and training complexity will be increased. To find out the optimal value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p1.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> for a given problem, we tune this hyper-parameter too on a held-out set.</p>
</div>
</div>
<div id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.4 </span>Vector to Tensor Mapping</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p">Finally, we need to find a way to map the original feature vector to a tensor, i.e. to associate each feature with an index in the tensor. Assuming the tensor volume <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p1.m1" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> is the same as the number of features, then there are in all <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p1.m2" class="ltx_Math" alttext="V!" display="inline"><mrow><mi>V</mi><mi mathvariant="normal">!</mi></mrow></math> ways of mapping, which is an intractable number of possibilities even for modest sized feature sets, making it impractical to carry out a brute force search. However while we are doing the mapping, we hope to arrange the features in a way such that the corresponding target weight tensor has approximately a low-rank structure, this way it can be well approximated by very few component rank-1 tensors.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p">Unfortunately we have no knowledge about the target weights in advance, since that is what we need to learn after all. As a way out, we first run a simple vector-model based learning algorithm (say the Perceptron) on the training data and estimate a weight vector, which serves as a “surrogate” weight vector. We then use this surrogate vector to guide the design of the mapping. Ideally we hope to find a permutation of the surrogate weights to map to a tensor in such a way that the tensor has a rank as low as possible. However matrix rank minimization is in general a hard problem <cite class="ltx_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Fazel2002</a>]</cite>. Therefore, we follow an approximate algorithm given in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, whose main idea is illustrated via an example in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.F4" class="ltx_figure"><span class="ltx_ERROR undefined">{subfigure}</span>
<p class="ltx_p">0.48
<span class="ltx_ERROR undefined ltx_centering">{algorithmic}</span>

<span class="ltx_ERROR undefined ltx_centering">\REQUIRE</span><span class="ltx_ERROR undefined ltx_centering">\STATE</span>Tensor order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m1" class="ltx_centering" alttext="D" display="inline"><mi>D</mi></math>, tensor volume <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m2" class="ltx_centering" alttext="V" display="inline"><mi>V</mi></math>, mode size <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m3" class="ltx_centering" alttext="n_{d},d=1,\ldots,D" display="inline"><mrow><mrow><mrow><msub><mi>n</mi><mi>d</mi></msub><mo>,</mo><mi>d</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></mrow></math>, surrogate weight vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m4" class="ltx_centering" alttext="\boldsymbol{v}" display="inline"><mi>𝒗</mi></math>
<span class="ltx_ERROR undefined ltx_centering">\STATE</span>Let
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m5" class="ltx_centering" alttext="\boldsymbol{v}^{+}=[v_{1}^{+},\ldots,v_{p}^{+}]" display="inline"><mrow><msup><mi>𝒗</mi><mo>+</mo></msup><mo>=</mo><mrow><mo>[</mo><mrow><msubsup><mi>v</mi><mn>1</mn><mo>+</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>v</mi><mi>p</mi><mo>+</mo></msubsup></mrow><mo>]</mo></mrow></mrow></math> be the non-negative part of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m6" class="ltx_centering" alttext="\boldsymbol{v}" display="inline"><mi>𝒗</mi></math>
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m7" class="ltx_centering" alttext="\boldsymbol{v}^{-}=[v_{1}^{-},\ldots,v_{q}^{-}]" display="inline"><mrow><msup><mi>𝒗</mi><mo>-</mo></msup><mo>=</mo><mrow><mo>[</mo><mrow><msubsup><mi>v</mi><mn>1</mn><mo>-</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>v</mi><mi>q</mi><mo>-</mo></msubsup></mrow><mo>]</mo></mrow></mrow></math> be the negative part of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m8" class="ltx_centering" alttext="\boldsymbol{v}" display="inline"><mi>𝒗</mi></math>

<span class="ltx_ERROR undefined ltx_centering">\REQUIRE</span><span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m9" class="ltx_centering" alttext="\tilde{\boldsymbol{v}}^{+}" display="inline"><msup><mover accent="true"><mi>𝒗</mi><mo stretchy="false">~</mo></mover><mo>+</mo></msup></math> = <em class="ltx_emph ltx_centering">sort</em>(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m10" class="ltx_centering" alttext="\boldsymbol{v}^{+}" display="inline"><msup><mi>𝒗</mi><mo>+</mo></msup></math>) in descending order
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m11" class="ltx_centering" alttext="\tilde{\boldsymbol{v}}^{-}" display="inline"><msup><mover accent="true"><mi>𝒗</mi><mo stretchy="false">~</mo></mover><mo>-</mo></msup></math> = <em class="ltx_emph ltx_centering">sort</em>(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m12" class="ltx_centering" alttext="\boldsymbol{v}^{-}" display="inline"><msup><mi>𝒗</mi><mo>-</mo></msup></math>) in ascending order
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m13" class="ltx_centering" alttext="u=V/n_{D}" display="inline"><mrow><mi>u</mi><mo>=</mo><mrow><mi>V</mi><mo>/</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></mrow></math>
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m14" class="ltx_centering" alttext="e=p-mod(p,u)" display="inline"><mrow><mi>e</mi><mo>=</mo><mrow><mi>p</mi><mo>-</mo><mrow><mi>m</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>p</mi><mo>,</mo><mi>u</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m15" class="ltx_centering" alttext="f=q-mod(q,u)" display="inline"><mrow><mi>f</mi><mo>=</mo><mrow><mi>q</mi><mo>-</mo><mrow><mi>m</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>q</mi><mo>,</mo><mi>u</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined ltx_centering">\STATE</span>Construct vector
<span class="ltx_ERROR undefined ltx_centering">\STATE</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m16" class="ltx_centering" alttext="\mathbf{X}=[\tilde{v}_{1}^{+},\ldots,\tilde{v}_{e}^{+},\tilde{v}_{1}^{-},%&#10;\ldots,\tilde{v}_{f}^{-}," display="inline"><mrow><mi>𝐗</mi><mo>=</mo><mrow><mo>[</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mn>1</mn><mo>+</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mi>e</mi><mo>+</mo></msubsup><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mn>1</mn><mo>-</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mi>f</mi><mo>-</mo></msubsup><mo>,</mo></mrow></mrow></math>
<br class="ltx_break ltx_centering"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m17" class="ltx_centering" alttext="\qquad\tilde{v}_{e+1}^{+},\ldots,\tilde{v}_{p}^{+},\tilde{v}_{f+1}^{-},\ldots,%&#10;\tilde{v}_{q}^{-}]" display="inline"><mrow><msubsup><mpadded lspace="20.0pt" width="+20.0pt"><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover></mpadded><mrow><mi>e</mi><mo>+</mo><mn>1</mn></mrow><mo>+</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mi>p</mi><mo>+</mo></msubsup><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mrow><mi>f</mi><mo>+</mo><mn>1</mn></mrow><mo>-</mo></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mover accent="true"><mi>v</mi><mo stretchy="false">~</mo></mover><mi>q</mi><mo>-</mo></msubsup><mo>]</mo></mrow></math>
<span class="ltx_ERROR undefined ltx_centering">\STATE</span>Map <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m18" class="ltx_centering" alttext="X_{a},a=1,\ldots,p+q" display="inline"><mrow><mrow><mrow><msub><mi>X</mi><mi>a</mi></msub><mo>,</mo><mi>a</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>p</mi><mo>+</mo><mi>q</mi></mrow></mrow></mrow></math> to the tensor entry <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m19" class="ltx_centering" alttext="\mathcal{T}_{i_{1},\ldots,i_{D}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></math>, such that
<span class="ltx_ERROR undefined ltx_centering">\STATE</span></p>
<table id="S3.Ex8" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex8.m1" class="ltx_Math" alttext="a=\sum\limits_{d=1}^{D}(i_{d}-1)l_{d-1}+1" display="block"><mrow><mi>a</mi><mo>=</mo><mrow><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mrow><mo>(</mo><mrow><msub><mi>i</mi><mi>d</mi></msub><mo>-</mo><mn>1</mn></mrow><mo>)</mo></mrow><mo>⁢</mo><msub><mi>l</mi><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>+</mo><mn>1</mn></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table><span class="ltx_ERROR undefined">\STATE</span>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m20" class="ltx_Math" alttext="l_{d}=l_{d-1}n_{d}" display="inline"><mrow><msub><mi>l</mi><mi>d</mi></msub><mo>=</mo><mrow><msub><mi>l</mi><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>⁢</mo><msub><mi>n</mi><mi>d</mi></msub></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m21" class="ltx_Math" alttext="l_{0}=1" display="inline"><mrow><msub><mi>l</mi><mn>0</mn></msub><mo>=</mo><mn>1</mn></mrow></math></p>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mapping a surrogate weight vector to a tensor</div><span class="ltx_ERROR undefined">{subfigure}</span>
<p class="ltx_p">0.5
<img src="P14-1063/image002.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_centering" width="497" height="518" alt=""/></p>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the algorithm</div>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Algorithm for mapping a surrogate weight vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m26" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> to a tensor. (<a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) provides the algorithm; (<a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) illustrates it by mapping a vector of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m27" class="ltx_Math" alttext="V=12" display="inline"><mrow><mi>V</mi><mo>=</mo><mn>12</mn></mrow></math> to a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m28" class="ltx_Math" alttext="(n_{1},n_{2},n_{3})=(2,2,3)" display="inline"><mrow><mrow><mo>(</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>3</mn></msub></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mo>(</mo><mrow><mn>2</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow><mo>)</mo></mrow></mrow></math> tensor. The bars <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.m29" class="ltx_Math" alttext="X_{i}" display="inline"><msub><mi>X</mi><mi>i</mi></msub></math> represent the surrogate weights — after separately sorting the positive and negative parts — and the labels along a path of the tree correspond to the tensor-index of the weight represented by the leaf resulting from the mapping.</div>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p class="ltx_p">Basically, what the algorithm does is to divide the surrogate weights into hierarchical groups such that groups on the same level are approximately proportional to each other. Using these groups as units we are able to “fill” the tensor in a hierarchical way. The resulting tensor will have an approximate low-rank structure, provided that the sorted feature weights have roughly group-wise proportional relations.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p class="ltx_p">For comparison, we also experimented a trivial solution which maps each entry of the feature tensor to the tensor just in sequential order, namely <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m1" class="ltx_Math" alttext="\phi_{0}" display="inline"><msub><mi>ϕ</mi><mn>0</mn></msub></math> is mapped to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m2" class="ltx_Math" alttext="\Phi_{0,0,\ldots,0}" display="inline"><msub><mi mathvariant="normal">Φ</mi><mrow><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>0</mn></mrow></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m3" class="ltx_Math" alttext="\phi_{1}" display="inline"><msub><mi>ϕ</mi><mn>1</mn></msub></math> is mapped to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.p4.m4" class="ltx_Math" alttext="\Phi_{0,0,\ldots,1}" display="inline"><msub><mi mathvariant="normal">Φ</mi><mrow><mn>0</mn><mo>,</mo><mn>0</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></msub></math> etc. This of course ignores correlation between features since the original feature order in the vector could be totally meaningless, and this strategy is not expected to be a good solution for vector to tensor mapping.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Online Learning Algorithm</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We now turn to the problem of learning the feature weight tensor. Here we propose an online learning algorithm similar to MIRA but modified to accommodate tensor models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">Let the model be <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m1" class="ltx_Math" alttext="f(\boldsymbol{T})=\boldsymbol{T}\circ\boldsymbol{\Phi}(x,y)" display="inline"><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝑻</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>𝑻</mi><mo>∘</mo><mi>𝚽</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m2" class="ltx_Math" alttext="\boldsymbol{T}=\sum_{h=1}^{H}\boldsymbol{w}_{h}^{1}\otimes\boldsymbol{w}_{h}^{%&#10;2}\otimes,\ldots,\otimes\boldsymbol{w}_{h}^{D}" display="inline"><mrow><mi>𝑻</mi><mo>=</mo><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></msubsup><msubsup><mi>𝒘</mi><mi>h</mi><mn>1</mn></msubsup><mo>⊗</mo><msubsup><mi>𝒘</mi><mi>h</mi><mn>2</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mi>h</mi><mi>D</mi></msubsup></mrow></math> is the weight tensor, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m3" class="ltx_Math" alttext="\boldsymbol{\Phi}(x,y)" display="inline"><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></math> is the feature tensor for an input-output pair <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m4" class="ltx_Math" alttext="(x,y)" display="inline"><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></math>. Training samples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m5" class="ltx_Math" alttext="(x_{i},y_{i}),i=1,\ldots,m" display="inline"><mrow><mrow><mrow><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>i</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>m</mi></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m6" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> is the input and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m7" class="ltx_Math" alttext="y_{i}" display="inline"><msub><mi>y</mi><mi>i</mi></msub></math> is the reference or oracle hypothesis, are fed to the weight learning algorithm in sequential order. A prediction <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m8" class="ltx_Math" alttext="z_{t}" display="inline"><msub><mi>z</mi><mi>t</mi></msub></math> is made by the model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m9" class="ltx_Math" alttext="T_{t}" display="inline"><msub><mi>T</mi><mi>t</mi></msub></math> at time <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m10" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> from a set of candidates <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m11" class="ltx_Math" alttext="\mathcal{Z}(x_{t})" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>)</mo></mrow></mrow></math>, and the model updates the weight tensor by solving the following problem:</p>
<table id="A1.EGx5" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E4.m3" class="ltx_Math" alttext="\displaystyle\min\limits_{\boldsymbol{T}\in\mathbb{R}^{n_{1}\times n_{2}\times%&#10;\ldots n_{D}}}\frac{1}{2}\|\boldsymbol{T}-\boldsymbol{T}_{t}\|^{2}+C\xi" display="inline"><mrow><mrow><munder><mo movablelimits="false">min</mo><mrow><mi>𝑻</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>×</mo><msub><mi>n</mi><mn>2</mn></msub><mo>×</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mi>n</mi><mi>D</mi></msub></mrow></msup></mrow></munder><mo>⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><msup><mrow><mo fence="true">∥</mo><mrow><mi>𝑻</mi><mo>-</mo><msub><mi>𝑻</mi><mi>t</mi></msub></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
<tr id="S4.Ex9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex9.m3" class="ltx_Math" alttext="\displaystyle s.t." display="inline"><mrow><mrow><mi>s</mi><mo separator="true">.</mo><mi>t</mi></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.Ex10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex10.m3" class="ltx_Math" alttext="\displaystyle\mathscr{L}_{t}\leq\xi,\xi\geq 0" display="inline"><mrow><mrow><msub><mi class="ltx_font_mathscript">ℒ</mi><mi>t</mi></msub><mo>≤</mo><mi>ξ</mi></mrow><mo>,</mo><mrow><mi>ξ</mi><mo>≥</mo><mn>0</mn></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m12" class="ltx_Math" alttext="\boldsymbol{T}" display="inline"><mi>𝑻</mi></math> is a decomposed weight tensor and</p>
<table id="A1.EGx6" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex11.m1" class="ltx_Math" alttext="\displaystyle\mathscr{L}_{t}=\boldsymbol{T}\circ\boldsymbol{\Phi}(x_{t},z_{t})%&#10;-\boldsymbol{T}\circ\boldsymbol{\Phi}(x_{t},y_{t})+\rho(y_{t},z_{t})" display="inline"><mrow><msub><mi class="ltx_font_mathscript">ℒ</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mrow><mi>𝑻</mi><mo>∘</mo><mi>𝚽</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mrow><mi>𝑻</mi><mo>∘</mo><mi>𝚽</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">is the structured hinge loss.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">This problem setting follows the same “passive-aggressive” strategy as in the original MIRA. To optimize the vectors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m1" class="ltx_Math" alttext="\boldsymbol{w}_{h}^{d},h=1,\ldots,H,d=1,\ldots,D" display="inline"><mrow><mrow><mrow><msubsup><mi>𝒘</mi><mi>h</mi><mi>d</mi></msubsup><mo>,</mo><mi>h</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>H</mi><mo>,</mo><mi>d</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow></mrow></math>, we use a similar iterative strategy as proposed in <cite class="ltx_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Cai et al.2006b</a>]</cite>. Basically, the idea is that instead of optimizing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m2" class="ltx_Math" alttext="\boldsymbol{w}_{h}^{d}" display="inline"><msubsup><mi>𝒘</mi><mi>h</mi><mi>d</mi></msubsup></math> all together, we optimize <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m3" class="ltx_Math" alttext="\boldsymbol{w}_{1}^{1},\boldsymbol{w}_{1}^{2},\ldots,\boldsymbol{w}_{H}^{D}" display="inline"><mrow><msubsup><mi>𝒘</mi><mn>1</mn><mn>1</mn></msubsup><mo>,</mo><msubsup><mi>𝒘</mi><mn>1</mn><mn>2</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mi>H</mi><mi>D</mi></msubsup></mrow></math> in turn. While we are updating one vector, the rest are fixed. For the problem setting given above, each of the sub-problems that need to be solved is convex, and according to <cite class="ltx_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Cai et al.2006b</a>]</cite> the objective function value will decrease after each individual weight update and eventually this procedure will converge.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">We now give this procedure in more detail. Denote the weight vector of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m1" class="ltx_Math" alttext="d^{\mathrm{th}}" display="inline"><msup><mi>d</mi><mi>th</mi></msup></math> mode of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m2" class="ltx_Math" alttext="h^{\mathrm{th}}" display="inline"><msup><mi>h</mi><mi>th</mi></msup></math> tensor at time <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m3" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m4" class="ltx_Math" alttext="\boldsymbol{w}_{h,t}^{d}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math>. We will update the vectors in turn in the following order: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m5" class="ltx_Math" alttext="\boldsymbol{w}_{1,t}^{1},\ldots,\boldsymbol{w}_{1,t}^{D},\boldsymbol{w}_{2,t}^%&#10;{1},\ldots,\boldsymbol{w}_{2,t}^{D},\ldots,\boldsymbol{w}_{H,t}^{1},\ldots,%&#10;\boldsymbol{w}_{H,t}^{D}" display="inline"><mrow><msubsup><mi>𝒘</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup><mo>,</mo><msubsup><mi>𝒘</mi><mrow><mn>2</mn><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mrow><mn>2</mn><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mrow><mi>H</mi><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>𝒘</mi><mrow><mi>H</mi><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup></mrow></math>. Once a vector has been updated, it is fixed for future updates.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">By way of notation, define</p>
<table id="A1.EGx7" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E5.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{W}_{h,t}^{d}" display="inline"><msubsup><mi>𝑾</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="3" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex12.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex12.m3" class="ltx_Math" alttext="\displaystyle\boldsymbol{w}_{h,t+1}^{1}\otimes,\ldots,\otimes\boldsymbol{w}_{h%&#10;,t+1}^{d-1}\otimes\boldsymbol{w}_{h,t}^{d}\otimes,\ldots,\otimes\boldsymbol{w}%&#10;_{h,t}^{D}" display="inline"><mrow><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mn>1</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex13.m3" class="ltx_Math" alttext="\displaystyle\text{(and let }\boldsymbol{W}_{h,t}^{D+1}\triangleq\boldsymbol{w%&#10;}_{h,t+1}^{1}\otimes,\ldots,\otimes\boldsymbol{w}_{h,t+1}^{D}\text{)}," display="inline"><mrow><mtext>(and let </mtext><msubsup><mi>𝑾</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>≜</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mn>1</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>D</mi></msubsup><mtext>)</mtext><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.Ex15" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E6.m1" class="ltx_Math" alttext="\displaystyle\widehat{\boldsymbol{W}}_{h,t}^{d}" display="inline"><msubsup><mover accent="true"><mi>𝑾</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="3" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex14.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex14.m3" class="ltx_Math" alttext="\displaystyle\boldsymbol{w}_{h,t+1}^{1}\otimes,\ldots,\otimes\boldsymbol{w}_{h%&#10;,t+1}^{d-1}\otimes\boldsymbol{w}^{d}\otimes,\ldots,\otimes\boldsymbol{w}_{h,t}%&#10;^{D}" display="inline"><mrow><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mn>1</mn></msubsup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⊗</mo><msup><mi>𝒘</mi><mi>d</mi></msup><mo>⊗</mo><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mo>⊗</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex15.m3" class="ltx_Math" alttext="\displaystyle\text{(where }\boldsymbol{w}^{d}\in\mathbb{R}^{n_{d}}\text{)}," display="inline"><mrow><mrow><mrow><mtext>(where </mtext><mo>⁢</mo><msup><mi>𝒘</mi><mi>d</mi></msup></mrow><mo>∈</mo><mrow><msup><mi>ℝ</mi><msub><mi>n</mi><mi>d</mi></msub></msup><mo>⁢</mo><mtext>)</mtext></mrow></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<table id="A1.EGx8" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E7.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{T}_{h,t}^{d}" display="inline"><msubsup><mi>𝑻</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E7.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E7.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{h^{\prime}=1}^{h-1}\!\boldsymbol{W}_{h^{\prime},t}^{%&#10;D+1}\!+\boldsymbol{W}_{h,t}^{d}+\!\!\!\!\sum\limits_{h^{\prime}=h+1}^{H}%&#10;\boldsymbol{W}_{h^{\prime},t}^{1}" display="inline"><mrow><mrow><mpadded width="-1.7pt"><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle></mpadded><mpadded width="-1.7pt"><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mpadded></mrow><mo>+</mo><msubsup><mi>𝑾</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>H</mi></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
<tr id="S4.Ex16" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex16.m1" class="ltx_Math" alttext="\displaystyle\widehat{\boldsymbol{T}}_{h,t}^{d}" display="inline"><msubsup><mover accent="true"><mi>𝑻</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex16.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex16.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{h^{\prime}=1}^{h-1}\!\boldsymbol{W}_{h^{\prime},t}^{%&#10;D+1}\!+\widehat{\boldsymbol{W}}_{h,t}^{d}+\!\!\!\!\sum\limits_{h^{\prime}=h+1}%&#10;^{H}\boldsymbol{W}_{h^{\prime},t}^{1}" display="inline"><mrow><mrow><mpadded width="-1.7pt"><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle></mpadded><mpadded width="-1.7pt"><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mpadded></mrow><mo>+</mo><msubsup><mover accent="true"><mi>𝑾</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>+</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>H</mi></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<table id="A1.EGx9" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.E9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E8.m1" class="ltx_Math" alttext="\displaystyle\boldsymbol{\phi}_{h,t}^{d}(x,y)" display="inline"><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="3" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex18.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex18.m3" class="ltx_Math" alttext="\displaystyle\boldsymbol{\Phi}(x,y)\times_{2}\boldsymbol{w}_{h,t+1}^{2}\ldots%&#10;\times_{d-1}\boldsymbol{w}_{h,t+1}^{d-1}\times_{d+1}" display="inline"><mrow><mi>𝚽</mi><mrow><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo></mrow><msub><mo>×</mo><mn>2</mn></msub><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mn>2</mn></msubsup><mi mathvariant="normal">…</mi><msub><mo>×</mo><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msubsup><msub><mo>×</mo><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E9.m3" class="ltx_Math" alttext="\displaystyle\boldsymbol{w}_{h,t}^{d+1}\ldots\times_{D}\boldsymbol{w}_{h,t}^{D}" display="inline"><mrow><mrow><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi mathvariant="normal">…</mi></mrow><msub><mo>×</mo><mi>D</mi></msub><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">In order to update from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m1" class="ltx_Math" alttext="\boldsymbol{w}_{h,t}^{d}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math> to get <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m2" class="ltx_Math" alttext="\boldsymbol{w}_{h,t+1}^{d}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup></math>, the sub-problem to solve is:</p>
<table id="A1.EGx10" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex22" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td colspan="3" class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex19.m1" class="ltx_Math" alttext="\displaystyle\min\limits_{\boldsymbol{w}^{d}\in\mathbb{R}^{n_{d}}}\frac{1}{2}%&#10;\|\widehat{\boldsymbol{T}}_{h,t}^{d}-\boldsymbol{T}_{h,t}^{d}\|^{2}+C\xi" display="inline"><mrow><mrow><munder><mo movablelimits="false">min</mo><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>d</mi></msub></msup></mrow></munder><mo>⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><msup><mrow><mo fence="true">∥</mo><mrow><msubsup><mover accent="true"><mi>𝑻</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>𝑻</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex20.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex20.m3" class="ltx_Math" alttext="\displaystyle\min\limits_{\boldsymbol{w}^{d}\in\mathbb{R}^{n_{d}}}\frac{1}{2}%&#10;\|\widehat{\boldsymbol{W}}_{h,t}^{d}-\boldsymbol{W}_{h,t}^{d}\|^{2}+C\xi" display="inline"><mrow><mrow><munder><mo movablelimits="false">min</mo><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>d</mi></msub></msup></mrow></munder><mo>⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><msup><mrow><mo fence="true">∥</mo><mrow><msubsup><mover accent="true"><mi>𝑾</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>-</mo><msubsup><mi>𝑾</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex21.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex21.m3" class="ltx_Math" alttext="\displaystyle\min\limits_{\boldsymbol{w}^{d}\in\mathbb{R}^{n_{d}}}\frac{1}{2}%&#10;\beta_{h,t+1}^{1}\ldots\beta_{h,t+1}^{d-1}\beta_{h,t}^{d+1}\ldots\beta_{h,t}^{D}" display="inline"><mrow><munder><mo movablelimits="false">min</mo><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>∈</mo><msup><mi>ℝ</mi><msub><mi>n</mi><mi>d</mi></msub></msup></mrow></munder><mo>⁡</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo>⁢</mo><msubsup><mi>β</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mn>1</mn></msubsup><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msubsup><mi>β</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><msubsup><mi>β</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msubsup><mi>β</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>D</mi></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex22.m3" class="ltx_Math" alttext="\displaystyle\qquad\qquad\|\boldsymbol{w}^{d}-\boldsymbol{w}_{h,t}^{d}\|^{2}+C\xi" display="inline"><mrow><mi>  </mi><mo separator="true">  </mo><mrow><msup><mrow><mo fence="true">∥</mo><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>-</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mi>ξ</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.Ex23" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex23.m1" class="ltx_Math" alttext="\displaystyle s.t." display="inline"><mrow><mrow><mi>s</mi><mo separator="true">.</mo><mi>t</mi></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex23.m3" class="ltx_Math" alttext="\displaystyle\mathscr{L}_{h,t}^{d}\leq\xi,\xi\geq 0." display="inline"><mrow><mrow><msubsup><mi class="ltx_font_mathscript">ℒ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>≤</mo><mi>ξ</mi></mrow><mo>,</mo><mrow><mi>ξ</mi><mo>≥</mo><mn>0.</mn></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where</p>
<table id="A1.EGx11" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex24" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex24.m1" class="ltx_Math" alttext="\displaystyle\beta_{h,t}^{d}" display="inline"><msubsup><mi>β</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex24.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex24.m3" class="ltx_Math" alttext="\displaystyle\|\boldsymbol{w}_{h,t}^{d}\|^{2}" display="inline"><msup><mrow><mo fence="true">∥</mo><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo fence="true">∥</mo></mrow><mn>2</mn></msup></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.Ex32" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex26.m1" class="ltx_Math" alttext="\displaystyle\mathscr{L}_{h,t}^{d}" display="inline"><msubsup><mi class="ltx_font_mathscript">ℒ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex26.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex26.m3" class="ltx_Math" alttext="\displaystyle\widehat{\boldsymbol{T}}_{h,t}^{d}\circ\boldsymbol{\Phi}(x_{t},z_%&#10;{t})-\widehat{\boldsymbol{T}}_{h,t}^{d}\circ\boldsymbol{\Phi}(x_{t},y_{t})" display="inline"><mrow><mrow><mrow><msubsup><mover accent="true"><mi>𝑻</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>∘</mo><mi>𝚽</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mrow><msubsup><mover accent="true"><mi>𝑻</mi><mo>^</mo></mover><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>∘</mo><mi>𝚽</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex27.m3" class="ltx_Math" alttext="\displaystyle+\rho(y_{t},z_{t})" display="inline"><mrow><mo>+</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex29.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex29.m3" class="ltx_Math" alttext="\displaystyle{\boldsymbol{w}^{d}}\cdot\left(\boldsymbol{\phi}_{h,t}^{d}(x_{t},%&#10;z_{t})-\boldsymbol{\phi}_{h,t}^{d}(x_{t},y_{t})\right)" display="inline"><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>⋅</mo><mrow><mo>(</mo><mrow><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex30.m3" class="ltx_Math" alttext="\displaystyle-\left(\sum\limits_{h^{\prime}=1}^{h-1}\boldsymbol{W}_{h^{\prime}%&#10;,t}^{D+1}+\sum\limits_{h^{\prime}=h+1}^{H}\boldsymbol{W}_{h^{\prime},t}^{1}%&#10;\right)\circ" display="inline"><mrow><mo>-</mo><mrow><mo>(</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>H</mi></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup><mo>)</mo></mrow><mo>∘</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex31.m3" class="ltx_Math" alttext="\displaystyle\quad\left(\boldsymbol{\Phi}(x_{t},y_{t})-\boldsymbol{\Phi}(x_{t}%&#10;,z_{t})\right)" display="inline"><mrow><mi mathvariant="normal"> </mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex32.m3" class="ltx_Math" alttext="\displaystyle+\rho(y_{t},z_{t})" display="inline"><mrow><mo>+</mo><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Letting</p>
<table id="S4.Ex33" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex33.m1" class="ltx_Math" alttext="\Delta\boldsymbol{\phi}_{h,t}^{d}\triangleq\boldsymbol{\phi}_{h,t}^{d}(x_{t},y%&#10;_{t})-\boldsymbol{\phi}_{h,t}^{d}(x_{t},z_{t})" display="block"><mrow><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow><mo>≜</mo><mrow><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">and</p>
<table id="A1.EGx12" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S4.Ex34" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex34.m3" class="ltx_Math" alttext="\displaystyle s_{h,t}^{d}\triangleq\left(\sum\limits_{h^{\prime}=1}^{h-1}%&#10;\boldsymbol{W}_{h^{\prime},t}^{D+1}+\sum\limits_{h^{\prime}=h+1}^{H}%&#10;\boldsymbol{W}_{h^{\prime},t}^{1}\right)\circ" display="inline"><mrow><msubsup><mi>s</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>≜</mo><mrow><mo>(</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>h</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>H</mi></munderover></mstyle><msubsup><mi>𝑾</mi><mrow><msup><mi>h</mi><mo>′</mo></msup><mo>,</mo><mi>t</mi></mrow><mn>1</mn></msubsup><mo>)</mo></mrow><mo>∘</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.Ex35" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex35.m3" class="ltx_Math" alttext="\displaystyle\qquad\quad\left(\boldsymbol{\Phi}(x_{t},y_{t})-\boldsymbol{\Phi}%&#10;(x_{t},z_{t})\right)" display="inline"><mrow><mi>  </mi><mo separator="true"> </mo><mrow><mo>(</mo><mrow><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">we may compactly write</p>
<table id="S4.Ex36" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex36.m1" class="ltx_Math" alttext="\mathscr{L}_{h,t}^{d}=\rho(y_{t},z_{t})-s_{h,t}^{d}-\boldsymbol{w}^{d}\cdot%&#10;\Delta\boldsymbol{\phi}_{h,t}^{d}." display="block"><mrow><mrow><msubsup><mi class="ltx_font_mathscript">ℒ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>=</mo><mrow><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><msubsup><mi>s</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>-</mo><mrow><mrow><msup><mi>𝒘</mi><mi>d</mi></msup><mo>⋅</mo><mi mathvariant="normal">Δ</mi></mrow><mo>⁢</mo><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">This convex optimization problem is just like the original MIRA and may be solved in a similar way. The updating strategy for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m3" class="ltx_Math" alttext="\boldsymbol{w}_{h,t}^{d}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></math> is derived as</p>
<table id="A1.EGx13" class="ltx_equationgroup ltx_eqn_align">

<tr id="S4.Ex37" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex37.m2" class="ltx_Math" alttext="\displaystyle\boldsymbol{w}_{h,t+1}^{d}=\boldsymbol{w}_{h,t}^{d}+\tau\Delta%&#10;\boldsymbol{\phi}_{h,t}^{d}" display="inline"><mrow><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow><mi>d</mi></msubsup><mo>=</mo><mrow><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>+</mo><mrow><mi>τ</mi><mo>⁢</mo><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S4.E10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E10.m2" class="ltx_Math" alttext="\displaystyle\tau=" display="inline"><mrow><mi>τ</mi><mo>=</mo><mi/></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
<tr id="S4.Ex38" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex38.m2" class="ltx_Math" alttext="\displaystyle\min\left\{C,\frac{\rho(y_{t},z_{t})-\boldsymbol{T}_{h,t}^{d}%&#10;\circ\left(\boldsymbol{\Phi}(x_{t},y_{t})-\boldsymbol{\Phi}(x_{t},z_{t})\right%&#10;)}{\|\Delta\boldsymbol{\phi}_{h,t}^{d}\|^{2}}\right\}" display="inline"><mrow><mo movablelimits="false">min</mo><mo>⁡</mo><mrow><mo>{</mo><mrow><mi>C</mi><mo>,</mo><mstyle displaystyle="true"><mfrac><mrow><mrow><mi>ρ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><msubsup><mi>𝑻</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>∘</mo><mrow><mo>(</mo><mrow><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>𝚽</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>z</mi><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><msup><mrow><mo fence="true">∥</mo><mrow><mi mathvariant="normal">Δ</mi><mo>⁢</mo><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mfrac></mstyle></mrow><mo>}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The initial vectors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m4" class="ltx_Math" alttext="\boldsymbol{w}_{h,1}^{i}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mn>1</mn></mrow><mi>i</mi></msubsup></math> cannot be made all zero, since otherwise the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m5" class="ltx_Math" alttext="l" display="inline"><mi>l</mi></math>-mode product in Equation (<a href="#S4.E9" title="(9) ‣ 4 Online Learning Algorithm ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) would yield all zero <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m6" class="ltx_Math" alttext="\boldsymbol{\phi}_{h,t}^{d}(x,y)" display="inline"><mrow><msubsup><mi mathvariant="bold-italic">ϕ</mi><mrow><mi>h</mi><mo>,</mo><mi>t</mi></mrow><mi>d</mi></msubsup><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></math> and the model would never get a chance to be updated. Therefore, we initialize the entries of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m7" class="ltx_Math" alttext="\boldsymbol{w}_{h,1}^{i}" display="inline"><msubsup><mi>𝒘</mi><mrow><mi>h</mi><mo>,</mo><mn>1</mn></mrow><mi>i</mi></msubsup></math> uniformly such that the Frobenius-norm of the weight tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m8" class="ltx_Math" alttext="\boldsymbol{W}" display="inline"><mi>𝑾</mi></math> is unity.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">We call the algorithm above “Tensor-MIRA” and abbreviate it as T-MIRA.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this section we shows empirical results of the training algorithm on a parsing task. We used the Charniak parser <cite class="ltx_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Charniak et al.2005</a>]</cite> for our experiment, and we used the proposed algorithm to train the reranking feature weights. For comparison, we also investigated training the reranker with Perceptron and MIRA.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Settings</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">To simulate a low-resource training environment, our training sets were selected from sections 2-9 of the Penn WSJ treebank, section 24 was used as the held-out set and section 23 as the evaluation set. We applied the default settings of the parser. There are around <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="V=1.33" display="inline"><mrow><mi>V</mi><mo>=</mo><mn>1.33</mn></mrow></math> million features in all defined for reranking, and the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-best size for reranking is set to 50. We selected the parse with the highest <em class="ltx_emph">f</em>-score from the 50-best list as the oracle.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">We would like to observe from the experiments how the amount of training data as well as different settings of the tensor degrees of freedom affects the algorithm performance. Therefore we tried all combinations of the following experimental parameters:</p>
</div>
<div id="S5.SS1.tab1" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Parameters</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Settings</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Training data (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m1" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Sec. 2, 2-3, 2-5, 2-9</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Tensor order (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m2" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_r">2, 3, 4</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r"># rank-1 tensors (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m3" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_r">1, 2, 3</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Vec. to tensor mapping</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">approximate, sequential</td></tr>
</tbody>
</table>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">Here “approximate” and “sequential” means using, respectively, the algorithm given in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and the sequential mapping mentioned in Section <a href="#S3.SS4" title="3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>. According to the strategy given in <a href="#S3.SS2" title="3.2 Mode Size ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, once the tensor order and number of features are fixed, the sizes of modes and total number of parameters to estimate are fixed as well, as shown in the tables below:</p>
</div>
<div id="S5.SS1.tab2" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m4" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Size of modes</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number of parameters</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">2</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m5" class="ltx_Math" alttext="1155\times 1155" display="inline"><mrow><mn>1155</mn><mo>×</mo><mn>1155</mn></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2310</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r">3</th>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m6" class="ltx_Math" alttext="110\times 110\times 111" display="inline"><mrow><mn>110</mn><mo>×</mo><mn>110</mn><mo>×</mo><mn>111</mn></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r">331</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">4</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.m7" class="ltx_Math" alttext="34\times 34\times 34\times 34" display="inline"><mrow><mn>34</mn><mo>×</mo><mn>34</mn><mo>×</mo><mn>34</mn><mo>×</mo><mn>34</mn></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">136</td></tr>
</tbody>
</table>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Results and Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p1.m1" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>-scores of the held-out and evaluation set given by T-MIRA as well as the Perceptron and MIRA baseline are given in Table <a href="#I2.ix1" title="Table 5 ‣ 5.2 Results and Analysis ‣ 5 Experiments ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. From the results, we have the following observations:</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">When very few labeled data are available for training (compared with the number of features), T-MIRA performs much better than the vector-based models MIRA and Perceptron. However as the amount of training data increases, the advantage of T-MIRA fades away, and vector-based models catch up. This is because the weight tensors learned by T-MIRA are highly structured, which significantly reduces model/training complexity and makes the learning process very effective in a low-resource environment, but as the amount of data increases, the more complex and expressive vector-based models adapt to the data better, whereas further improvements from the tensor model is impeded by its structural constraints, making it insensitive to the increase of training data.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">To further contrast the behavior of T-MIRA, MIRA and Perceptron, we plot the <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i2.p1.m1" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>-scores on both the training and held-out sets given by these algorithms after each training epoch in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Results and Analysis ‣ 5 Experiments ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The plots are for the experimental setting with mapping=surrogate, # rank-1 tensors=2, tensor order=2, training data=sections 2-3. It is clearly seen that both MIRA and Perceptron do much better than T-MIRA on the training set. Nevertheless, with a huge number of parameters to fit a limited amount of data, they tend to over-fit and give much worse results on the held-out set than T-MIRA does.</p>
</div>
<div id="I1.i2.p2" class="ltx_para">
<p class="ltx_p">As an aside, observe that MIRA consistently outperformed Perceptron, as expected.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">Properties of linear tensor model: The heuristic vector-to-tensor mapping strategy given by Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4 Vector to Tensor Mapping ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> gives consistently better results than the sequential mapping strategy, as expected.</p>
</div>
<div id="I1.i3.p2" class="ltx_para">
<p class="ltx_p">To make further comparison of the two strategies, in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 Results and Analysis ‣ 5 Experiments ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> we plot the 20 largest singular values of the matrices which the surrogate weights (given by the Perceptron after running for 1 epoch) are mapped to by both strategies (from the experiment with training data sections 2-5). From the contrast between the largest and the <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p2.m1" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math>-largest singular values, it can be seen that the matrix generated by the first strategy approximates a low-rank structure much better than the second strategy. Therefore, the performance of T-MIRA is influenced significantly by the way features are mapped to the tensor. If the corresponding target weight tensor has internal structure that makes it approximately low-rank, the learning procedure becomes more effective.</p>
</div>
<div id="I1.i3.p3" class="ltx_para">
<p class="ltx_p">The best results are consistently given by <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m1" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math> order tensor models, and the differences between the <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m2" class="ltx_Math" alttext="3^{\mathrm{rd}}" display="inline"><msup><mn>3</mn><mi>rd</mi></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m3" class="ltx_Math" alttext="4^{\mathrm{th}}" display="inline"><msup><mn>4</mn><mi>th</mi></msup></math> order tensors are not significant. As discussed in Section <a href="#S3.SS1" title="3.1 Tensor Order ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, although <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m4" class="ltx_Math" alttext="3^{\mathrm{rd}}" display="inline"><msup><mn>3</mn><mi>rd</mi></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m5" class="ltx_Math" alttext="4^{\mathrm{th}}" display="inline"><msup><mn>4</mn><mi>th</mi></msup></math> order tensors have less parameters, the benefit of reduced training complexity does not compensate for the loss of expressiveness. A <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p3.m6" class="ltx_Math" alttext="2^{\mathrm{nd}}" display="inline"><msup><mn>2</mn><mi>nd</mi></msup></math> order tensor has already reduced the number of parameters from the original 1.33 million to only 2310, and it does not help to further reduce the number of parameters using higher order tensors.</p>
</div></li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">4.</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p">As the amount of training data increases, there is a trend that the best results come from models with more rank-1 component tensors. Adding more rank-1 tensors increases the model’s complexity and ability of expression, making the model more adaptive to larger data sets.</p>
</div></li>
</ol>
</div>
<div id="S5.T5" class="ltx_table">
<ul class="ltx_itemize">
<li id="I2.ix1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize"/> <span class="ltx_ERROR undefined">{subtable}</span>
<div id="I2.ix1.p1" class="ltx_para">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Mapping</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Approximate</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Sequential</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Rank-1 tensors</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Tensor order</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Held-out score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.99</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Evaluation score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">89.69</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_tiny">MIRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="18"><span class="ltx_text ltx_font_tiny">88.57</span></td>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Percep</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="18"><span class="ltx_text ltx_font_tiny">88.23</span></td>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_para">Table 1: </span>Training data: Section 2 only</div>
</div><span class="ltx_ERROR undefined">{subtable}</span>
<div id="I2.ix1.p2" class="ltx_para">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Mapping</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Approximate</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Sequential</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Rank-1 tensors</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Tensor order</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Held-out score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.98</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Evaluation score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_bold ltx_font_tiny">90.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">89.82</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_tiny">MIRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="18"><span class="ltx_text ltx_font_tiny">89.00</span></td>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Percep</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="18"><span class="ltx_text ltx_font_tiny">88.59</span></td>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_para">Table 2: </span>Training data: Section 2-3 </div>
</div><span class="ltx_ERROR undefined">{subtable}</span>
<div id="I2.ix1.p3" class="ltx_para">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Mapping</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Approximate</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Sequential</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Rank-1 tensors</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Tensor order</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Held-out score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.85</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Evaluation score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">89.78</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_tiny">MIRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="18"><span class="ltx_text ltx_font_tiny">89.49</span></td>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Percep</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="18"><span class="ltx_text ltx_font_tiny">89.10</span></td>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_para">Table 3: </span>Training data: Section 2-5 </div>
</div><span class="ltx_ERROR undefined">{subtable}</span>
<div id="I2.ix1.p4" class="ltx_para">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Mapping</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Approximate</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">Sequential</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Rank-1 tensors</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Tensor order</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">4</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Held-out score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">89.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">88.93</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Evaluation score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_bold ltx_font_tiny">89.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="9"><span class="ltx_text ltx_font_tiny">89.84</span></td>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_tiny">MIRA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="18"><span class="ltx_text ltx_font_tiny">89.95</span></td>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_border_tt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_tiny">Percep</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" colspan="18"><span class="ltx_text ltx_font_tiny">89.77</span></td>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/>
<td class="ltx_td ltx_border_b ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_para">Table 4: </span>Training data: Section 2-9 </div>
</div>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_item">Table 5: </span>Parsing <math xmlns="http://www.w3.org/1998/Math/MathML" id="I2.ix1.m2" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>-scores. Tables (a) to (d) correspond to training data with increasing size. The upper-part of each table shows the T-MIRA results with different settings, the lower-part shows the MIRA and Perceptron baselines. The evaluation scores come from the settings indicated by the best held-out scores. The best results on the held-out and evaluation data are marked in bold. </div></li>
</ul>
</div>
<div id="S5.F7" class="ltx_figure"><span class="ltx_ERROR undefined ltx_centering">{subfigure}</span>
<p class="ltx_p ltx_align_center">[b]0.4
<img src="P14-1063/image003.png" id="S5.F7.g1" class="ltx_graphics" width="708" height="487" alt=""/></p>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Training set</div><span class="ltx_ERROR undefined ltx_centering">{subfigure}</span>
<p class="ltx_p ltx_align_center">[b]0.4
<img src="P14-1063/image004.png" id="S5.F7.g2" class="ltx_graphics" width="708" height="487" alt=""/></p>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Held-out set</div>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.F7.m2" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math>-scores given by three algorithms on training and held-out set (see text for the setting).</div>
</div>
<div id="S5.F8" class="ltx_figure"><img src="P14-1063/image005.png" id="S5.F8.g1" class="ltx_graphics ltx_centering" width="282" height="191" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The top 20 singular values of the surrogate weight matrices given by two mapping algorithms.</div>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In this paper, we reformulated the traditional linear vector-space models as tensor-space models, and proposed an online learning algorithm named Tensor-MIRA. A tensor-space model is a compact representation of data, and via rank-1 tensor approximation, the weight tensor can be made highly structured hence the number of parameters to be trained is significantly reduced. This can be regarded as a form of model regularization.Therefore, compared with the traditional vector-space models, learning in the tensor space is very effective when a large feature set is defined, but only small amount of training data is available. Our experimental results corroborated this argument.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">As mentioned in Section <a href="#S3.SS2" title="3.2 Mode Size ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, one interesting problem that merits further investigation is how to determine optimal mode sizes. The challenge of applying a tensor model comes from finding a proper tensor structure for a given problem, and the key to solving this problem is to find a balance between the model complexity (indicated by the order and sizes of modes) and the number of parameters. Developing a theoretically guaranteed approach of finding the optimal structure for a given task will make the tensor model not only perform well in low-resource environments, but adaptive to larger data sets.</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">This work was partially supported by IBM via DARPA/BOLT contract number HR0011-12-C-0015 and by the National Science Foundation via award number IIS-0963898.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cai et al.2006a</span>
<span class="ltx_bibblock">
Deng Cai , Xiaofei He , and Jiawei Han.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">Tensor Space Model for Document Analysis

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval(SIGIR)</span>,
625–626.

</span></li>
<li id="bib.bibx2" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cai et al.2006b</span>
<span class="ltx_bibblock">
Deng Cai, Xiaofei He, and Jiawei Han.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">Learning with Tensor Representation

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Technical Report, Department of Computer Science, University of Illinois at Urbana-Champaign</span>.

</span></li>
<li id="bib.bibx3" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Charniak et al.2005</span>
<span class="ltx_bibblock">
Eugene Charniak, and Mark Johnson

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Coarse-to-fine n-Best Parsing and MaxEnt Discriminative Reranking

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL)</span>
173–180.

</span></li>
<li id="bib.bibx4" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Chiang et al.2008</span>
<span class="ltx_bibblock">
David Chiang, Yuval Marton, and Philip Resnik.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Online Large-Margin Training of Syntactic and Structural Translation Features

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language Processing(EMNLP)</span>,
224–233.

</span></li>
<li id="bib.bibx5" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cohen and Collins2012</span>
<span class="ltx_bibblock">
Shay Cohen and Michael Collins.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Tensor Decomposition for Fast Parsing with Latent-Variable PCFGs

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of Advances in Neural Information Processing Systems(NIPS)</span>.

</span></li>
<li id="bib.bibx6" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cohen and Satta2013</span>
<span class="ltx_bibblock">
Shay Cohen and Giorgio Satta.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Approximate PCFG Parsing Using Tensor Decomposition

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of NAACL-HLT</span>,
487–496.

</span></li>
<li id="bib.bibx7" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Collins2002</span>
<span class="ltx_bibblock">
Michael Collins.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Discriminative training methods for hidden Markov Models: Theory and Experiments with Perceptron. Algorithms

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language Processing(EMNLP)</span>,
10:1–8.

</span></li>
<li id="bib.bibx8" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Crammer et al.2006</span>
<span class="ltx_bibblock">
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwartz, and Yoram Singer.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">Online Passive-Aggressive Algorithms

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Journal of Machine Learning Research(JMLR)</span>,
7:551–585.

</span></li>
<li id="bib.bibx9" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Fazel2002</span>
<span class="ltx_bibblock">
Maryam Fazel.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Matrix Rank Minimization with Applications

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">PhD thesis, Stanford University</span>.

</span></li>
<li id="bib.bibx10" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Gimpel and Smith2012</span>
<span class="ltx_bibblock">
Kevin Gimpel, and Noah A. Smith

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Structured Ramp Loss Minimization for Machine Translation

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of North American Chapter of the Association for Computational Linguistics(NAACL)</span>,
221-231.

</span></li>
<li id="bib.bibx11" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Hazan et al.2005</span>
<span class="ltx_bibblock">
Tamir Hazan, Simon Polak, and Amnon Shashua

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Sparse Image Coding using a 3D Non-negative Tensor Factorization

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of the International Conference on Computer Vision (ICCV)</span>.

</span></li>
<li id="bib.bibx12" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Hopkins and May2011</span>
<span class="ltx_bibblock">
Mark Hopkins and Jonathan May.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Tuning as Reranking

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of Empirical Methods in Natural Language Processing(EMNLP)</span>,
1352-1362.

</span></li>
<li id="bib.bibx13" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Kolda and Bader2009</span>
<span class="ltx_bibblock">
Tamara Kolda and Brett Bader.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">Tensor Decompositions and Applications

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">SIAM Review</span>,
51:455-550.

</span></li>
<li id="bib.bibx14" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">McDonald et al.2005</span>
<span class="ltx_bibblock">
Ryan McDonald, Koby Crammer, and Fernando Pereira.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Online Large-Margin Training of Dependency Parsers

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of the 43rd Annual Meeting of the ACL</span>,
91–98.

</span></li>
<li id="bib.bibx15" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Shashua and Hazan2005</span>
<span class="ltx_bibblock">
Amnon Shashua, and Tamir Hazan.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Non-Negative Tensor Factorization with Applications to Statistics and Computer Vision

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of the International Conference on Machine Learning (ICML)</span>.

</span></li>
<li id="bib.bibx16" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Van de Cruys et al.2013</span>
<span class="ltx_bibblock">
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">A Tensor-based Factorization Model of Semantic Compositionality

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Proceedings of NAACL-HLT</span>,
1142–1151.

</span></li>
</ul>
</div>
<div id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix A </span>Proof of Theorem <a href="#Thmthm1" title="Theorem 1. ‣ 3.1 Tensor Order ‣ 3 Tensor Model Construction ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></h2>

<div class="ltx_proof">
<h6 class="ltx_title ltx_runin ltx_font_italic ltx_title_proof">Proof.</h6>
<div id="A1.p1" class="ltx_para">
<p class="ltx_p">For <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m1" class="ltx_Math" alttext="D=1" display="inline"><mrow><mi>D</mi><mo>=</mo><mn>1</mn></mrow></math>, it is obvious that if a set of real numbers <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m2" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> can be represented by a rank-1 matrix, it can always be represented by a vector, but the reverse is not true.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p class="ltx_p">For <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m1" class="ltx_Math" alttext="D&gt;1" display="inline"><mrow><mi>D</mi><mo>&gt;</mo><mn>1</mn></mrow></math>, if <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m2" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> can be represented by <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m3" class="ltx_Math" alttext="\mathcal{P}=\mathbf{p}_{1}\otimes\mathbf{p}_{2}\otimes\ldots\otimes\mathbf{p}_%&#10;{D}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>=</mo><mrow><msub><mi>𝐩</mi><mn>1</mn></msub><mo>⊗</mo><msub><mi>𝐩</mi><mn>2</mn></msub><mo>⊗</mo><mi mathvariant="normal">…</mi><mo>⊗</mo><msub><mi>𝐩</mi><mi>D</mi></msub></mrow></mrow></math>, namely <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m4" class="ltx_Math" alttext="x_{i}=\mathcal{P}_{i_{1},\ldots,i_{D}}=\prod_{d=1}^{D}p_{i_{d}}^{d}" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></msubsup><msubsup><mi>p</mi><msub><mi>i</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow></mrow></math>, then for any component vector in mode <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m5" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math>,</p>
<table id="A1.Ex39" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex39.m1" class="ltx_Math" alttext="[p_{1}^{d},p_{2}^{d},\ldots,p_{n_{d}}^{d}]=[s_{1}^{d}p_{1}^{d},s_{2}^{d}p_{1}^%&#10;{d},\ldots,s_{n_{d}^{p}}^{d}p_{1}^{d}]" display="block"><mrow><mrow><mo>[</mo><mrow><msubsup><mi>p</mi><mn>1</mn><mi>d</mi></msubsup><mo>,</mo><msubsup><mi>p</mi><mn>2</mn><mi>d</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>p</mi><msub><mi>n</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mrow><msubsup><mi>s</mi><mn>1</mn><mi>d</mi></msubsup><mo>⁢</mo><msubsup><mi>p</mi><mn>1</mn><mi>d</mi></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>s</mi><mn>2</mn><mi>d</mi></msubsup><mo>⁢</mo><msubsup><mi>p</mi><mn>1</mn><mi>d</mi></msubsup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>s</mi><msubsup><mi>n</mi><mi>d</mi><mi>p</mi></msubsup><mi>d</mi></msubsup><mo>⁢</mo><msubsup><mi>p</mi><mn>1</mn><mi>d</mi></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m6" class="ltx_Math" alttext="n_{d}^{p}" display="inline"><msubsup><mi>n</mi><mi>d</mi><mi>p</mi></msubsup></math> is the size of mode <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m7" class="ltx_Math" alttext="d" display="inline"><mi>d</mi></math> of <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m8" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m9" class="ltx_Math" alttext="s_{j}^{d}" display="inline"><msubsup><mi>s</mi><mi>j</mi><mi>d</mi></msubsup></math> is a constant and <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m10" class="ltx_Math" alttext="s_{j}^{d}=\frac{p_{i1,\ldots,i_{d-1},j,i_{d+1},\ldots,i_{D}}}{p_{i1,\ldots,i_{%&#10;d-1},1,i_{d+1},\ldots,i_{D}}}" display="inline"><mrow><msubsup><mi>s</mi><mi>j</mi><mi>d</mi></msubsup><mo>=</mo><mfrac><msub><mi>p</mi><mrow><mrow><mi>i</mi><mo>⁢</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>j</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub><msub><mi>p</mi><mrow><mrow><mi>i</mi><mo>⁢</mo><mn>1</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>d</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>,</mo><mn>1</mn><mo>,</mo><msub><mi>i</mi><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub></mfrac></mrow></math>
Therefore</p>
<table id="A1.EGx14" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="A1.E11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E11.m1" class="ltx_Math" alttext="\displaystyle x_{i}=\mathcal{P}_{i_{1},\ldots,i_{D}}=x_{1,\ldots,1}\prod%&#10;\limits_{d=1}^{D}s_{i_{d}}^{d}" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mi>D</mi></msub></mrow></msub><mo>=</mo><mrow><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>⁢</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover></mstyle><msubsup><mi>s</mi><msub><mi>i</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
</table>
<p class="ltx_p">and this representation is unique for a given <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m11" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math>(up to the ordering of <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m12" class="ltx_Math" alttext="\mathbf{p}_{j}" display="inline"><msub><mi>𝐩</mi><mi>j</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m13" class="ltx_Math" alttext="s_{j}^{d}" display="inline"><msubsup><mi>s</mi><mi>j</mi><mi>d</mi></msubsup></math> in <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m14" class="ltx_Math" alttext="\mathbf{p}_{j}" display="inline"><msub><mi>𝐩</mi><mi>j</mi></msub></math>, which simply assigns <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m15" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> with different indices in the tensor), due to the pairwise proportional constraint imposed by <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p2.m16" class="ltx_Math" alttext="x_{i}/x_{j},i,j=1,\ldots,n" display="inline"><mrow><mrow><mrow><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>/</mo><msub><mi>x</mi><mi>j</mi></msub></mrow><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow></math>.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p class="ltx_p">If <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m1" class="ltx_Math" alttext="x_{i}" display="inline"><msub><mi>x</mi><mi>i</mi></msub></math> can also be represented by <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m2" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒬</mi></math>, then <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m3" class="ltx_Math" alttext="x_{i}=\mathcal{Q}_{i_{1},\ldots,i_{D+1}}=x_{1,\ldots,1}\prod_{d=1}^{D+1}t_{i_{%&#10;d}}^{d}" display="inline"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">𝒬</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>=</mo><mrow><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mn>1</mn></mrow></msub><mo>⁢</mo><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msubsup><msubsup><mi>t</mi><msub><mi>i</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m4" class="ltx_Math" alttext="t_{j}^{d}" display="inline"><msubsup><mi>t</mi><mi>j</mi><mi>d</mi></msubsup></math> has a similar definition as <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m5" class="ltx_Math" alttext="s_{j}^{d}" display="inline"><msubsup><mi>s</mi><mi>j</mi><mi>d</mi></msubsup></math>. Then it must be the case that</p>
<table id="A1.EGx15" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="A1.Ex40" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex40.m3" class="ltx_Math" alttext="\displaystyle\exists d_{1},d_{2}\in\{1,\ldots,D+1\},d\in\{1,\ldots,D\},d_{1}%&#10;\neq d_{2}" display="inline"><mrow><mrow><mrow><mrow><mo>∃</mo><msub><mi>d</mi><mn>1</mn></msub></mrow><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub></mrow><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>}</mo></mrow></mrow><mo>,</mo><mrow><mrow><mi>d</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow><mo>}</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>≠</mo><msub><mi>d</mi><mn>2</mn></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="A1.Ex41" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex41.m3" class="ltx_Math" alttext="\displaystyle s.t." display="inline"><mrow><mrow><mi>s</mi><mo separator="true">.</mo><mi>t</mi></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="A1.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E12.m3" class="ltx_Math" alttext="\displaystyle t_{i_{d_{1}}}^{d_{1}}t_{i_{d_{2}}}^{d_{2}}=s_{i_{d}}^{d}," display="inline"><mrow><mrow><mrow><msubsup><mi>t</mi><msub><mi>i</mi><msub><mi>d</mi><mn>1</mn></msub></msub><msub><mi>d</mi><mn>1</mn></msub></msubsup><mo>⁢</mo><msubsup><mi>t</mi><msub><mi>i</mi><msub><mi>d</mi><mn>2</mn></msub></msub><msub><mi>d</mi><mn>2</mn></msub></msubsup></mrow><mo>=</mo><msubsup><mi>s</mi><msub><mi>i</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
<tr id="A1.Ex42" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex42.m3" class="ltx_Math" alttext="\displaystyle t_{i_{d_{a}}}^{d_{a}}=s_{i_{d_{b}}}^{d_{b}},\quad d_{a}\neq d_{1%&#10;},d_{2},\quad d_{b}\neq d" display="inline"><mrow><mrow><msubsup><mi>t</mi><msub><mi>i</mi><msub><mi>d</mi><mi>a</mi></msub></msub><msub><mi>d</mi><mi>a</mi></msub></msubsup><mo>=</mo><msubsup><mi>s</mi><msub><mi>i</mi><msub><mi>d</mi><mi>b</mi></msub></msub><msub><mi>d</mi><mi>b</mi></msub></msubsup></mrow><mo separator="true">, </mo><mrow><mrow><msub><mi>d</mi><mi>a</mi></msub><mo>≠</mo><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>,</mo><msub><mi>d</mi><mn>2</mn></msub></mrow></mrow><mo separator="true">, </mo><mrow><msub><mi>d</mi><mi>b</mi></msub><mo>≠</mo><mi>d</mi></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">since otherwise <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p3.m6" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> would be represented by a different set of factors than those given in Equation (<a href="#A1.E11" title="(11) ‣ Appendix A Proof of Theorem 1 ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>).</p>
</div>
<div id="A1.p4" class="ltx_para">
<p class="ltx_p">Therefore, in order for tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p4.m1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒬</mi></math> to represent the same set of real numbers that <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p4.m2" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> represents, there needs to exist a vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p4.m3" class="ltx_Math" alttext="[s_{1}^{d},\ldots,s_{n_{d}}^{d}]" display="inline"><mrow><mo>[</mo><mrow><msubsup><mi>s</mi><mn>1</mn><mi>d</mi></msubsup><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msubsup><mi>s</mi><msub><mi>n</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow><mo>]</mo></mrow></math> that can be represented by a rank-1 matrix as indicated by Equation (<a href="#A1.E12" title="(12) ‣ Appendix A Proof of Theorem 1 ‣ Online Learning in Tensor Space" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>), which is in general not guaranteed.</p>
</div>
<div id="A1.p5" class="ltx_para">
<p class="ltx_p">On the other hand, if <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m1" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> can be represented by <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m2" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒬</mi></math>, namely</p>
<table id="A1.Ex43" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex43.m1" class="ltx_Math" alttext="x_{i}=\mathcal{Q}_{i_{1},\ldots,i_{D+1}}=\prod\limits_{d=1}^{D+1}q_{i_{d}}^{d}" display="block"><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><msub><mi class="ltx_font_mathcaligraphic">𝒬</mi><mrow><msub><mi>i</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>i</mi><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></munderover><msubsup><mi>q</mi><msub><mi>i</mi><mi>d</mi></msub><mi>d</mi></msubsup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">then we can just pick <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m3" class="ltx_Math" alttext="d_{1}\in\{1,\ldots,D\},d_{2}=d_{1}+1" display="inline"><mrow><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>D</mi></mrow><mo>}</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>d</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>+</mo><mn>1</mn></mrow></mrow></mrow></math> and let</p>
<table id="A1.Ex44" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex44.m1" class="ltx_Math" alttext="\mathbf{q}^{\prime}=[q_{1}^{d_{1}}q_{1}^{d_{2}},q_{1}^{d_{1}}q_{2}^{d_{2}},%&#10;\ldots,q_{n_{d_{2}}^{q}}^{d_{1}}q_{n_{d_{1}}^{q}}^{d_{2}}]" display="block"><mrow><msup><mi>𝐪</mi><mo>′</mo></msup><mo>=</mo><mrow><mo>[</mo><mrow><mrow><msubsup><mi>q</mi><mn>1</mn><msub><mi>d</mi><mn>1</mn></msub></msubsup><mo>⁢</mo><msubsup><mi>q</mi><mn>1</mn><msub><mi>d</mi><mn>2</mn></msub></msubsup></mrow><mo>,</mo><mrow><msubsup><mi>q</mi><mn>1</mn><msub><mi>d</mi><mn>1</mn></msub></msubsup><mo>⁢</mo><msubsup><mi>q</mi><mn>2</mn><msub><mi>d</mi><mn>2</mn></msub></msubsup></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><msubsup><mi>q</mi><msubsup><mi>n</mi><msub><mi>d</mi><mn>2</mn></msub><mi>q</mi></msubsup><msub><mi>d</mi><mn>1</mn></msub></msubsup><mo>⁢</mo><msubsup><mi>q</mi><msubsup><mi>n</mi><msub><mi>d</mi><mn>1</mn></msub><mi>q</mi></msubsup><msub><mi>d</mi><mn>2</mn></msub></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">and</p>
<table id="A1.Ex45" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex45.m1" class="ltx_Math" alttext="\mathcal{Q}^{\prime}=\mathbf{q}_{1}\otimes\ldots\otimes\mathbf{q}_{d_{1}-1}%&#10;\otimes\mathbf{q}^{\prime}\otimes\mathbf{q}_{d_{2}+1}\otimes\ldots\otimes%&#10;\mathbf{q}_{D+1}" display="block"><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo>′</mo></msup><mo>=</mo><mrow><msub><mi>𝐪</mi><mn>1</mn></msub><mo>⊗</mo><mi mathvariant="normal">…</mi><mo>⊗</mo><msub><mi>𝐪</mi><mrow><msub><mi>d</mi><mn>1</mn></msub><mo>-</mo><mn>1</mn></mrow></msub><mo>⊗</mo><msup><mi>𝐪</mi><mo>′</mo></msup><mo>⊗</mo><msub><mi>𝐪</mi><mrow><msub><mi>d</mi><mn>2</mn></msub><mo>+</mo><mn>1</mn></mrow></msub><mo>⊗</mo><mi mathvariant="normal">…</mi><mo>⊗</mo><msub><mi>𝐪</mi><mrow><mi>D</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Hence <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m4" class="ltx_Math" alttext="\left\{x_{1},\ldots,x_{n}\right\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><mo>}</mo></mrow></math> can also be represented by a <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m5" class="ltx_Math" alttext="D^{\mathrm{th}}" display="inline"><msup><mi>D</mi><mi>th</mi></msup></math> order tensor <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p5.m6" class="ltx_Math" alttext="\mathcal{Q}^{\prime}" display="inline"><msup><mi class="ltx_font_mathcaligraphic">𝒬</mi><mo>′</mo></msup></math>.
∎</p>
</div>
</div>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 18:55:55 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
