<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Comparing Automatic Evaluation Measures for Image Description</title>
<!--Generated on Wed Jun 11 17:57:03 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Comparing Automatic Evaluation Measures for Image Description</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Desmond Elliott 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frank Keller 
<br class="ltx_break"/>Institute for Language, Cognition, and Computation
<br class="ltx_break"/>School of Informatics, University of Edinburgh
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">d.elliott@ed.ac.uk</span>,  <span class="ltx_text ltx_font_typewriter">keller@inf.ed.ac.uk
<br class="ltx_break"/></span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Image description is a new natural language generation task, where
the aim is to generate a human-like description of an image. The evaluation of
computer-generated text is a notoriously difficult problem, however, the
quality of image descriptions has typically been measured using
unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> and human judgements. The focus of this paper is to
determine the correlation of automatic measures with human judgements for this
task. We estimate the correlation of unigram and Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span>,
<span class="ltx_text ltx_font_smallcaps">ter</span>, <span class="ltx_text ltx_font_smallcaps">rouge-su4</span>, and Meteor against human judgements on two data
sets. The main finding is that unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> has a weak correlation, and
Meteor has the strongest correlation with human judgements.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.F1" class="ltx_figure"><img src="P14-2074/image003.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="190" height="190" alt=""/>
<br class="ltx_break ltx_centering"/>
<span class="ltx_inline-block ltx_parbox ltx_align_center ltx_align_middle" style="width:199.5pt;">
<ol id="I1" class="ltx_enumerate">[itemsep=2pt,leftmargin=1em]

<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">An older woman with a small dog in the snow.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">A woman and a cat are outside in the snow.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">A woman in a brown vest is walking on the snow with an animal.</p>
</div></li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">4.</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p">A woman with a red scarf covering her head walks with her cat on snow-covered ground.</p>
</div></li>
<li id="I1.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">5.</span> 
<div id="I1.i5.p1" class="ltx_para">
<p class="ltx_p">Heavy set woman in snow with a cat.</p>
</div></li>
</ol>
</span>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An image from the Flickr8K data set and five human-written
descriptions. These descriptions vary in the adjectives or prepositional
phrases that describe the woman (1, 3, 4, 5), incorrect or uncertain
identification of the cat (1, 3), and include a sentence without a verb
(5).</div>
</div>
<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Recent advances in computer vision and natural language processing have led to
an upsurge of research on tasks involving both vision and language. State of
the art visual detectors have made it possible to hypothesise <span class="ltx_text ltx_font_italic">what</span> is in
an image <cite class="ltx_cite">[<a href="#bib.bib9" title="Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation" class="ltx_ref">6</a>, <a href="#bib.bib8" title="Object Detection with Discriminatively Trained Part-Based Models" class="ltx_ref">5</a>]</cite>, paving the way for automatic
image description systems. The aim of such systems is to extract and reason
about visual aspects of images to generate a human-like description. An example
of the type of image and gold-standard descriptions available can be seen in
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Recent approaches to this task have been based on
slot-filling <cite class="ltx_cite">[<a href="#bib.bib23" title="Corpus-Guided Sentence Generation of Natural Images" class="ltx_ref">17</a>, <a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">3</a>]</cite>, combining web-scale n-grams
<cite class="ltx_cite">[<a href="#bib.bib15" title="Composing simple image descriptions using web-scale n-grams" class="ltx_ref">11</a>]</cite>, syntactic tree substitution <cite class="ltx_cite">[<a href="#bib.bib16" title="Midge : Generating Image Descriptions From Computer Vision Detections" class="ltx_ref">12</a>]</cite>, and
description-by-retrieval <cite class="ltx_cite">[<a href="#bib.bib7" title="Every picture tells a story: generating sentences from images" class="ltx_ref">4</a>, <a href="#bib.bib18" title="Im2Text: Describing Images Using 1 Million Captioned Photographs" class="ltx_ref">14</a>, <a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">7</a>]</cite>. Image
description has been compared to translating an image into text
<cite class="ltx_cite">[<a href="#bib.bib15" title="Composing simple image descriptions using web-scale n-grams" class="ltx_ref">11</a>, <a href="#bib.bib12" title="Baby talk: Understanding and generating simple image descriptions" class="ltx_ref">9</a>]</cite> or summarising an image <cite class="ltx_cite">[<a href="#bib.bib23" title="Corpus-Guided Sentence Generation of Natural Images" class="ltx_ref">17</a>]</cite>, resulting
in the adoption of the evaluation measures from those communities.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In this paper we estimate the correlation of human judgements with five
automatic evaluation measures on two image description data sets. Our work
extends previous studies of evaluation measures for image description
<cite class="ltx_cite">[<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">7</a>]</cite>, which focused on unigram-based measures and reported
agreement scores such as Cohen’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p2.m1" class="ltx_Math" alttext="\kappa" display="inline"><mi>κ</mi></math> rather than correlations. The main
finding of our analysis is that <span class="ltx_text ltx_font_smallcaps">ter</span> and unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> are weakly
correlated against human judgements, <span class="ltx_text ltx_font_smallcaps">rouge-su4</span> and Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span>
are moderately correlated, and the strongest correlation is found with Meteor.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">We estimate Spearman’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> for five different
automatic evaluation measures against human judgements for the automatic image
description task.
Spearman’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m2" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> is a non-parametric correlation co-efficient that restricts
the ability of outlier data points to skew the co-efficient value.
The automatic measures are calculated on the sentence level and correlated
against human judgements of semantic correctness.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">We perform the correlation analysis on the Flickr8K data set of
<cite class="ltx_cite">Hodosh<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">2013</a>)</cite>, and the data set of <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">The test data of the Flickr8K data set contains 1,000 images paired with five
reference descriptions. The images were retrieved from Flickr, the reference
descriptions were collected from Mechanical Turk, and the human judgements were
collected from expert annotators as follows: each image in the test data was
paired with the highest scoring sentence(s) retrieved from all possible test
sentences by the <span class="ltx_text ltx_font_smallcaps">tri5sem</span> model in <cite class="ltx_cite">Hodosh<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">2013</a>)</cite>. Each
image–description pairing in the test data was judged for semantic correctness
by three expert human judges on a scale of 1–4. We calculate automatic
measures for each image–retrieved sentence pair against the five reference
descriptions for the original image.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">The test data of <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite> contains 101 images paired with three
reference descriptions. The images were taken from the PASCAL VOC Action
Recognition Task, the reference descriptions were collected from Mechanical
Turk, and the judgements were also collected from Mechanical Turk.
<cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite> generated two-sentence descriptions for each of the test
images using four variants of a slot-filling model, and collected five human
judgements of the semantic correctness and grammatical correctness of the
description on a scale of 1–5 for each image–description pair, resulting in a
total of 2,042 human judgement–description pairings. In this analysis, we use
only the first sentence of the description, which describes the event depicted
in the image.</p>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Automatic Evaluation Measures</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">bleu</span> measures the effective overlap between a reference sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m1" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and a
candidate sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m2" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math>. It is defined as the geometric mean of the effective
n-gram precision scores, multiplied by the brevity penalty factor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m3" class="ltx_Math" alttext="BP" display="inline"><mrow><mi>B</mi><mo>⁢</mo><mi>P</mi></mrow></math> to
penalise short translations. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m4" class="ltx_Math" alttext="p_{n}" display="inline"><msub><mi>p</mi><mi>n</mi></msub></math> measures the effective overlap by
calculating the proportion of the maximum number of n-grams co-occurring
between a candidate and a reference and the total number of n-grams in the
candidate text. More formally,</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m1" class="ltx_Math" alttext="\displaystyle\mathit{BLEU}" display="inline"><mi>𝐵𝐿𝐸𝑈</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m2" class="ltx_Math" alttext="\displaystyle=BP\cdot\exp\left(\sum^{N}_{n=1}w_{n}\log p_{n}\right)" display="inline"><mrow><mi/><mo>=</mo><mrow><mrow><mi>B</mi><mo>⁢</mo><mi>P</mi></mrow><mo>⋅</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover></mstyle><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>n</mi></msub></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m1" class="ltx_Math" alttext="\displaystyle p_{n}" display="inline"><msub><mi>p</mi><mi>n</mi></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m2" class="ltx_Math" alttext="\displaystyle=\frac{\sum\limits_{c\in cand}\quad\sum\limits_{ngram\in c}count_%&#10;{clip}(ngram)}{\sum\limits_{c\in cand}\quad\sum\limits_{ngram\in c}count(ngram)}" display="inline"><mrow><mi/><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>c</mi><mo>∈</mo><mrow><mi>c</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow></mrow></munder><mo separator="true"> </mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mrow><mi>n</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>m</mi></mrow><mo>∈</mo><mi>c</mi></mrow></munder><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>u</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><msub><mi>t</mi><mrow><mi>c</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>p</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>n</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>m</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>c</mi><mo>∈</mo><mrow><mi>c</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow></mrow></munder><mo separator="true"> </mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mrow><mi>n</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>m</mi></mrow><mo>∈</mo><mi>c</mi></mrow></munder><mrow><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>u</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>n</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>m</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m1" class="ltx_Math" alttext="\displaystyle BP" display="inline"><mrow><mi>B</mi><mo>⁢</mo><mi>P</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle=\left\{\begin{array}[]{l l}1&amp;\quad\text{if }c&gt;r\\&#10;e^{(1-r/c)}&amp;\quad\text{if }c\leq r\end{array}\right." display="inline"><mrow><mi/><mo>=</mo><mrow><mo>{</mo><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mn>1</mn></mtd><mtd columnalign="left"><mrow><mrow><mpadded lspace="10.0pt" width="+10.0pt"><mtext>if </mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>&gt;</mo><mi>r</mi></mrow></mtd></mtr><mtr><mtd columnalign="left"><msup><mi>e</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>r</mi><mo>/</mo><mi>c</mi></mrow></mrow><mo>)</mo></mrow></msup></mtd><mtd columnalign="left"><mrow><mrow><mpadded lspace="10.0pt" width="+10.0pt"><mtext>if </mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>≤</mo><mi>r</mi></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> without a brevity penalty has been reported by
<cite class="ltx_cite">Kulkarni<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib12" title="Baby talk: Understanding and generating simple image descriptions" class="ltx_ref">2011</a>)</cite>, <cite class="ltx_cite">Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib15" title="Composing simple image descriptions using web-scale n-grams" class="ltx_ref">2011</a>)</cite>, <cite class="ltx_cite">Ordonez<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Im2Text: Describing Images Using 1 Million Captioned Photographs" class="ltx_ref">2011</a>)</cite>, and
<cite class="ltx_cite">Kuznetsova<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Collective Generation of Natural Image Descriptions" class="ltx_ref">2012</a>)</cite>; to the best of our knowledge, the only image
description work to
use higher-order n-grams with <span class="ltx_text ltx_font_smallcaps">bleu</span> is <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite>. In this
paper we use the smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span> implementation of <cite class="ltx_cite">Clark<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib3" title="Better hypothesis testing for statistical machine translation: Controlling for optimizer instability" class="ltx_ref">2011</a>)</cite> to
perform a sentence-level analysis, setting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m5" class="ltx_Math" alttext="n=1" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow></math> and no brevity penalty to get
the unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> measure, or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m6" class="ltx_Math" alttext="n=4" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow></math> with the brevity penalty to get the
Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span> measure. We note that a higher <span class="ltx_text ltx_font_smallcaps">bleu</span> score is better.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">rouge</span> measures the longest common subsequence of tokens between a
candidate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m1" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> and reference <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>. There is also a variant that measures the
co-occurrence of pairs of tokens in both the candidate and reference (a
skip-bigram): <span class="ltx_text ltx_font_smallcaps">rouge-su*</span>. The skip-bigram calculation is parameterised
with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m3" class="ltx_Math" alttext="d_{\mathit{skip}}" display="inline"><msub><mi>d</mi><mi>𝑠𝑘𝑖𝑝</mi></msub></math>, the maximum number of tokens between the words in the
skip-bigram. Setting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m4" class="ltx_Math" alttext="d_{\mathit{skip}}" display="inline"><msub><mi>d</mi><mi>𝑠𝑘𝑖𝑝</mi></msub></math> to 0 is equivalent to bigram overlap and
setting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m5" class="ltx_Math" alttext="d_{\mathit{skip}}" display="inline"><msub><mi>d</mi><mi>𝑠𝑘𝑖𝑝</mi></msub></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m6" class="ltx_Math" alttext="\infty" display="inline"><mi mathvariant="normal">∞</mi></math> means tokens can be any distance apart. If
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m7" class="ltx_Math" alttext="\alpha=|\mathit{SKIP2}(X,Y)|" display="inline"><mrow><mi>α</mi><mo>=</mo><mrow><mo fence="true">|</mo><mrow><mi mathvariant="italic">SKIP2</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow><mo>)</mo></mrow></mrow><mo fence="true">|</mo></mrow></mrow></math> is the number of matching skip-bigrams between the
reference and the candidate, then skip-bigram <span class="ltx_text ltx_font_smallcaps">rouge</span> is formally defined
as:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex6.m1" class="ltx_Math" alttext="\displaystyle R_{\mathit{SKIP2}}=\alpha\;/\dbinom{\alpha}{2}" display="inline"><mrow><msub><mi>R</mi><mi mathvariant="italic">SKIP2</mi></msub><mo>=</mo><mrow><mpadded width="+2.8pt"><mi>α</mi></mpadded><mo>/</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mi>α</mi></mtd></mtr><mtr><mtd><mn>2</mn></mtd></mtr></mtable></mstyle><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">rouge</span> has been used by only <cite class="ltx_cite">Yang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="Corpus-Guided Sentence Generation of Natural Images" class="ltx_ref">2011</a>)</cite> to measure the quality of
generated descriptions, using a variant they describe as <span class="ltx_text ltx_font_smallcaps">rouge-1</span>. We set
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m8" class="ltx_Math" alttext="d_{\mathit{skip}}=4" display="inline"><mrow><msub><mi>d</mi><mi>𝑠𝑘𝑖𝑝</mi></msub><mo>=</mo><mn>4</mn></mrow></math> and award partial credit for unigram only matches, otherwise known
as <span class="ltx_text ltx_font_smallcaps">rouge-su4</span>. We use <span class="ltx_text ltx_font_smallcaps">rouge</span> v.1.5.5 for the analysis, and configure
the evaluation script to return the result for the average score for matching
between the candidate and the references. A higher <span class="ltx_text ltx_font_smallcaps">rouge</span> score is better.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">ter</span> measures the number of modifications a human would need to make to
transform a candidate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math> into a reference <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m2" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math>. The modifications available
are insertion, deletion, substitute a single word, and shift a word an
arbitrary distance. <span class="ltx_text ltx_font_smallcaps">ter</span> is expressed as the percentage of the sentence
that needs to be changed, and can be greater than 100 if the candidate is
longer than the reference. More formally,</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m1" class="ltx_Math" alttext="\displaystyle\mathit{TER}=\frac{|\text{edits}|}{|\text{reference tokens}|}" display="inline"><mrow><mi>𝑇𝐸𝑅</mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mo fence="true">|</mo><mtext>edits</mtext><mo fence="true">|</mo></mrow><mrow><mo fence="true">|</mo><mtext>reference tokens</mtext><mo fence="true">|</mo></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p"><span class="ltx_text ltx_font_smallcaps">ter</span> has not yet been used to evaluate image description models. We use
v.0.8.0 of the <span class="ltx_text ltx_font_smallcaps">ter</span> evaluation tool, and a lower <span class="ltx_text ltx_font_smallcaps">ter</span> is better.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">Meteor is the harmonic mean of unigram precision and recall that allows for
exact, synonym, and paraphrase matchings between candidates and references. It
is calculated by generating an alignment between the tokens in the candidate
and reference sentences, with the aim of a 1:1 alignment between
tokens and minimising the number of chunks <span class="ltx_text ltx_font_italic">ch</span> of contiguous and
identically ordered tokens in the sentence pair. The alignment is based on
exact token matching, followed by Wordnet synonyms, and then stemmed tokens.
We can calculate precision, recall, and F-measure, where <span class="ltx_text ltx_font_italic">m</span> is the number
of aligned unigrams between candidate and reference. Meteor is defined as:</p>
<table id="Sx1.EGx4" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m1" class="ltx_Math" alttext="\displaystyle M" display="inline"><mi>M</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m2" class="ltx_Math" alttext="\displaystyle=(1-Pen)\cdot F_{mean}" display="inline"><mrow><mi/><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mi>P</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>n</mi></mrow></mrow><mo>)</mo></mrow><mo>⋅</mo><msub><mi>F</mi><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>n</mi></mrow></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m1" class="ltx_Math" alttext="\displaystyle Pen" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>n</mi></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m2" class="ltx_Math" alttext="\displaystyle=\gamma\left(\frac{ch}{m}\right)^{\theta}" display="inline"><mrow><mi/><mo>=</mo><mrow><mi>γ</mi><mo>⁢</mo><msup><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mi>c</mi><mo>⁢</mo><mi>h</mi></mrow><mi>m</mi></mfrac></mstyle><mo>)</mo></mrow><mi>θ</mi></msup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m1" class="ltx_Math" alttext="\displaystyle F_{mean}" display="inline"><msub><mi>F</mi><mrow><mi>m</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>n</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m2" class="ltx_Math" alttext="\displaystyle=\frac{PR}{\alpha P+(1-\alpha)R}" display="inline"><mrow><mi/><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mo>⁢</mo><mi>R</mi></mrow><mrow><mrow><mi>α</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>α</mi></mrow><mo>)</mo></mrow><mo>⁢</mo><mi>R</mi></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex11.m1" class="ltx_Math" alttext="\displaystyle P" display="inline"><mi>P</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex11.m2" class="ltx_Math" alttext="\displaystyle=\frac{|m|}{|\text{unigrams in candidate}|}" display="inline"><mrow><mi/><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mo fence="true">|</mo><mi>m</mi><mo fence="true">|</mo></mrow><mrow><mo fence="true">|</mo><mtext>unigrams in candidate</mtext><mo fence="true">|</mo></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex12.m1" class="ltx_Math" alttext="\displaystyle R" display="inline"><mi>R</mi></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex12.m2" class="ltx_Math" alttext="\displaystyle=\frac{|m|}{|\text{unigrams in reference}|}" display="inline"><mrow><mi/><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mo fence="true">|</mo><mi>m</mi><mo fence="true">|</mo></mrow><mrow><mo fence="true">|</mo><mtext>unigrams in reference</mtext><mo fence="true">|</mo></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">We calculated the Meteor scores using release 1.4.0 with the package-provided free
parameter settings of 0.85, 0.2, 0.6, and 0.75 for the matching components.
Meteor has not yet been reported to evaluate the performance of different
models on the image description task; a higher Meteor score is better.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Protocol</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">We performed the correlation analysis as follows. The sentence-level
evaluation measures were calculated for each
image–description–reference tuple. We collected the <span class="ltx_text ltx_font_smallcaps">bleu</span>, <span class="ltx_text ltx_font_smallcaps">ter</span>, and Meteor scores using MultEval <cite class="ltx_cite">[<a href="#bib.bib3" title="Better hypothesis testing for statistical machine translation: Controlling for optimizer instability" class="ltx_ref">1</a>]</cite>, and the
<span class="ltx_text ltx_font_smallcaps">rouge-su4</span> scores using the RELEASE-1.5.5.pl script. The
evaluation measure scores were then compared with the human judgements
using Spearman’s correlation estimated at the sentence-level.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_align_center ltx_border_tt">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<p class="ltx_p ltx_align_center">Flickr 8K</p>
<p class="ltx_p ltx_align_center">co-efficient</p>
<p class="ltx_p ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m1" class="ltx_Math" alttext="n=17,466" display="inline"><mrow><mi>n</mi><mo>=</mo><mrow><mn>17</mn><mo>,</mo><mn>466</mn></mrow></mrow></math></p>
</span></th>
<th class="ltx_td ltx_align_center ltx_border_tt">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<p class="ltx_p ltx_align_center">E&amp;K (2013)</p>
<p class="ltx_p ltx_align_center">co-efficient</p>
<p class="ltx_p ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m2" class="ltx_Math" alttext="n=2,040" display="inline"><mrow><mi>n</mi><mo>=</mo><mrow><mn>2</mn><mo>,</mo><mn>040</mn></mrow></mrow></math></p>
</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps">meteor</span></th>
<td class="ltx_td ltx_align_center ltx_border_t">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.233</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">rouge su-4</span></th>
<td class="ltx_td ltx_align_center">0.435</td>
<td class="ltx_td ltx_align_center">0.188</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span></th>
<td class="ltx_td ltx_align_center">0.429</td>
<td class="ltx_td ltx_align_center">0.177</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left">Unigram <span class="ltx_text ltx_font_smallcaps">bleu</span></th>
<td class="ltx_td ltx_align_center">0.345</td>
<td class="ltx_td ltx_align_center">0.097</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_smallcaps">ter</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb">-0.279</td>
<td class="ltx_td ltx_align_center ltx_border_bb">-0.044</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Spearman’s correlation co-efficient of automatic evaluation
measures against human judgements. All correlations are significant at p <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m4" class="ltx_Math" alttext="&lt;" display="inline"><mo>&lt;</mo></math>
0.001.</div>
</div>
<div id="S3.F2" class="ltx_figure">
<table style="width:100%;">
<tr>
<td class="ltx_subfigure">
<div id="S3.F1.sf1" class="ltx_figure"><img src="P14-2074/image001.png" id="S3.F1.sf1.g1" class="ltx_graphics" width="338" height="338" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Flick8K data set, n=17,466.</div>
</div></td>
<td class="ltx_subfigure">
<div id="S3.F1.sf2" class="ltx_figure"><img src="P14-2074/image002.png" id="S3.F1.sf2.g1" class="ltx_graphics" width="338" height="338" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>E&amp;K (2013) data set, n=2,042.</div>
</div></td></tr>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of automatic evaluation measures against
human judgements. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F2.m2" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> is the correlation between human judgements and the
automatic measure. The intensity of each point indicates the number of
occurrences that fall into that range.</div>
</div>
<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the correlation co-efficients between
automatic measures and human judgements and Figures <a href="#S3.F2" title="Figure 2 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)
and (b) show the distribution of scores for each measure against human
judgements. To classify the strength of the correlations, we followed the
guidance of <cite class="ltx_cite">Dancey and Reidy (<a href="#bib.bib4" title="Statistics Without Maths for Psychology" class="ltx_ref">2011</a>)</cite>, who posit that a co-efficient of 0.0–0.1 is
uncorrelated, 0.11–0.4 is <span class="ltx_text ltx_font_italic">weak</span>, 0.41–0.7 is <span class="ltx_text ltx_font_italic">moderate</span>, 0.71–0.90
is <span class="ltx_text ltx_font_italic">strong</span>, and 0.91–1.0 is <span class="ltx_text ltx_font_italic">perfect</span>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">On the Flickr8k data set, all evaluation measures can be classified as either
<span class="ltx_text ltx_font_italic">weakly</span> correlated or <span class="ltx_text ltx_font_italic">moderately</span> correlated with human judgements
and all results are significant. <span class="ltx_text ltx_font_smallcaps">ter</span> is only weakly correlated with
human judgements but could prove useful in comparing the types of differences
between models. An analysis of the distribution of <span class="ltx_text ltx_font_smallcaps">ter</span> scores in
Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a) shows that differences in candidate and
reference length are prevalent in the image description task. Unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> is also only weakly correlated against human judgements, even though it
has been reported extensively for this task. Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)
shows an almost uniform distribution of unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> scores, regardless
of the human judgement. Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span> and <span class="ltx_text ltx_font_smallcaps">rouge-su4</span> are moderately
correlated with human judgements, and the correlation is stronger than with
unigram <span class="ltx_text ltx_font_smallcaps">bleu</span>. Finally, Meteor is most strongly correlated measure
against human judgements.
A similar pattern is observed in the <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite> data set,
though the correlations are lower across all measures. This could be
caused by the smaller sample size or because the descriptions were
generated by a computer, and not retrieved from a collection of
human-written descriptions containing the gold-standard text, as in the
Flickr8K data set.</p>
</div>
<div id="S3.F3" class="ltx_figure">
<table style="width:100%;">
<tr>
<td class="ltx_subfigure">
<div id="S3.F3.fig1" class="ltx_figure"><img src="P14-2074/image005.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering" width="190" height="127" alt=""/>
</div></td>
<td class="ltx_subfigure">
<div id="S3.F3.fig2" class="ltx_figure"><img src="P14-2074/image004.jpg" id="S3.F3.g2" class="ltx_graphics" width="190" height="135" alt=""/>
<br class="ltx_break"/>
</div></td>
<td class="ltx_subfigure">
<div id="S3.F3.fig3" class="ltx_figure">
<span class="ltx_inline-block ltx_parbox ltx_align_bottom" style="width:195.1pt;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Candidate</span>: Football players gathering to contest something to collaborating officials.</p>
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Reference</span>: A football player in red and white is holding both hands up.</p>
</span>
</div></td>
<td class="ltx_subfigure">
<div id="S3.F3.fig4" class="ltx_figure">
<span class="ltx_inline-block ltx_parbox ltx_align_bottom" style="width:195.1pt;">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Candidate</span>: A man is attempting a stunt with a bicycle.</p>
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Reference</span>: Bmx biker Jumps off of ramp.</p>
</span>
</div></td></tr>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples in the test data with low Meteor scores
and the maximum expert human judgement. (a) the candidate and reference are
from the same image, and show differences in <span class="ltx_text ltx_font_italic">what</span> to describe, in (b) the
descriptions are retrieved from different images and show differences in <span class="ltx_text ltx_font_italic">how</span> to describe an image.</div>
</div>
<div id="S3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Qualitative Analysis</h3>

<div id="S3.SSx1.p1" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows two images from the test collection of the
Flickr8K data set with a low Meteor score and a maximum human judgement of
semantic correctness. The main difference between the candidates and references
are in deciding <span class="ltx_text ltx_font_italic">what</span> to describe (content selection), and <span class="ltx_text ltx_font_italic">how</span> to
describe it (realisation). We can hypothesise that in both translation and
summarisation, the source text acts as a lexical and semantic framework within
which the translation or summarisation process takes place. In
Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a), the authors of the descriptions made different
decisions on <span class="ltx_text ltx_font_italic">what</span> to describe. A decision has been made to describe the
role of the officials in the candidate text, and not in the reference text.
The underlying cause of this is an active area of research in the human vision
literature and can be attributed to bottom-up effects, such as saliency
<cite class="ltx_cite">[<a href="#bib.bib11" title="A model of saliency-based visual attention for rapid scene analysis" class="ltx_ref">8</a>]</cite>, top-down contextual effects <cite class="ltx_cite">[<a href="#bib.bib22" title="Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search." class="ltx_ref">16</a>]</cite>, or
rapidly-obtained scene properties <cite class="ltx_cite">[<a href="#bib.bib17" title="Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope" class="ltx_ref">13</a>]</cite>. In (b), we can see the
problem of deciding how to describe the selected content. The reference uses a
more specific noun to describe the person on the bicycle than the candidate.</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">There are several differences between our analysis and that of
<cite class="ltx_cite">Hodosh<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">2013</a>)</cite>. First, we report Spearman’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m1" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> correlation
coefficient of automatic measures against human judgements, whereas they report
agreement between judgements and automatic measures in terms of Cohen’s
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m2" class="ltx_Math" alttext="\kappa" display="inline"><mi>κ</mi></math>. The use of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m3" class="ltx_Math" alttext="\kappa" display="inline"><mi>κ</mi></math> requires the transformation of real-valued
scores into categorical values, and thus loses information; we use the
judgement and evaluation measure scores in their original forms. Second, our
use of Spearman’s <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m4" class="ltx_Math" alttext="\rho" display="inline"><mi>ρ</mi></math> means we can readily use all of the available data for
the correlation analysis, whereas <cite class="ltx_cite">Hodosh<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">2013</a>)</cite> report agreement on
thresholded subsets of the data. Third, we report the correlation coefficients
against five evaluation measures, some of which go beyond unigram matchings
between references and candidates, whereas they only report unigram <span class="ltx_text ltx_font_smallcaps">bleu</span>
and unigram <span class="ltx_text ltx_font_smallcaps">rouge</span>. It is therefore difficult to directly compare the
results of our correlation analysis against Hodosh et al.’s agreement analysis,
but they also reach the conclusion that unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> is not an
appropriate measure of image description performance. However, we do find
stronger correlations with Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span>, skip-bigram <span class="ltx_text ltx_font_smallcaps">rouge</span>, and
Meteor.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">In contrast to the results presented here, <cite class="ltx_cite">Reiter and Belz (<a href="#bib.bib20" title="An investigation into the validity of some metrics for automatically evaluating natural language generation systems" class="ltx_ref">2009</a>)</cite> found no
significant correlations of automatic evaluation measures against human
judgements of the <span class="ltx_text ltx_font_italic">accuracy</span> of machine-generated weather forecasts. They
did, however, find significant correlations of automatic measures against <span class="ltx_text ltx_font_italic">fluency</span> judgements. There are no fluency judgements available for Flickr8K,
but <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite> report grammaticality judgements for their data,
which are comparable to fluency ratings. We failed to find significant
correlations between grammatlicality judgements and any of the automatic
measures on the <cite class="ltx_cite">Elliott and Keller (<a href="#bib.bib6" title="Image Description using Visual Dependency Representations" class="ltx_ref">2013</a>)</cite> data. This discrepancy could be explained
in terms of the differences between the weather forecast generation and image
description tasks, or because the image description data sets contain thousands
of texts and a few human judgements per text, whereas the data sets of
<cite class="ltx_cite">Reiter and Belz (<a href="#bib.bib20" title="An investigation into the validity of some metrics for automatically evaluating natural language generation systems" class="ltx_ref">2009</a>)</cite> included hundreds of texts with 30 human judges.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this paper we performed a sentence-level correlation analysis of automatic
evaluation measures against expert human judgements for the automatic image
description task. We found that sentence-level unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> is only weakly
correlated with human judgements, even though it has extensively reported in
the literature for this task. Meteor was found to have the highest correlation
with human judgements, but it requires Wordnet and paraphrase resources that
are not available for all languages. Our findings held when judgements were
made on human-written or computer-generated descriptions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">The variability in what and how people describe images will cause problems for
all of the measures compared in this paper. Nevertheless, we propose
that unigram <span class="ltx_text ltx_font_smallcaps">bleu</span> should no longer be used as an objective function for
automatic image description because it has a weak correlation with human
accuracy judgements. We recommend adopting either Meteor, Smoothed <span class="ltx_text ltx_font_smallcaps">bleu</span>,
or <span class="ltx_text ltx_font_smallcaps">rouge-su4</span> because they show stronger correlations with human
judgements. We believe these suggestions are also applicable to the ranking
tasks proposed in <cite class="ltx_cite">Hodosh<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib10" title="Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics" class="ltx_ref">2013</a>)</cite>, where automatic evaluation scores could
act as features to a ranking function.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">Alexandra Birch and R. Calen Walshe, and the anonymous reviewers provided
valuable feedback on this paper. The research is funded by ERC Starting Grant
SYNPROC No. 203427.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. H. Clark, C. Dyer, A. Lavie and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Portland, Oregon, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 176–181</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=2002774" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS3.p1" title="2.3 Protocol ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_inbook"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Dancey and J. Reidy</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Statistics Without Maths for Psychology</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 175</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Elliott and F. Keller</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Image Description using Visual Dependency Representations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Seattle, Washington, U.S.A.</span>, <span class="ltx_text ltx_bib_pages"> pp. 1292–1302</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Data ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS1.p3" title="2.1 Data ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S3.p2" title="3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S4.p2" title="4 Discussion ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier and D. Forsyth</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Every picture tells a story: generating sentences from images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Heraklion, Crete, Greece</span>, <span class="ltx_text ltx_bib_pages"> pp. 15–29</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 3-642-15560-X, 978-3-642-15560-4</span>,
<a href="http://portal.acm.org/citation.cfm?id=1888089.1888092" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. F. Felzenszwalb, R. B. Girshick, D. McAllester and D. Ramanan</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Object Detection with Discriminatively Trained Part-Based Models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Pattern Analysis and Machine
Intelligence</span> <span class="ltx_text ltx_bib_volume">32</span> (<span class="ltx_text ltx_bib_number">9</span>), <span class="ltx_text ltx_bib_pages"> pp. 1627–1645</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/TPAMI.2009.167" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 0162-8828</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Guillaumin, T. Mensink, J. J. Verbeek and C. Schmid</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Kyoto, Japan</span>, <span class="ltx_text ltx_bib_pages"> pp. 309–316</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 9781424444199</span>,
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5459266" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Hodosh, P. Young and J. Hockenmaier</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Framing Image Description as a Ranking Task : Data , Models and Evaluation Metrics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Artificial Intelligence Research</span> <span class="ltx_text ltx_bib_volume">47</span>, <span class="ltx_text ltx_bib_pages"> pp. 853–899</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS1.p1" title="2.1 Data ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S2.SS1.p2" title="2.1 Data ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>,
<a href="#S4.p1" title="4 Discussion ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>,
<a href="#S5.p2" title="5 Conclusions ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Itti, C. Koch and E. Niebur</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A model of saliency-based visual attention for rapid scene analysis</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Pattern Analysis and Machine
Intelligence</span> <span class="ltx_text ltx_bib_volume">20</span> (<span class="ltx_text ltx_bib_number">11</span>), <span class="ltx_text ltx_bib_pages"> pp. 1254–1259</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/34.730558" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 0162-8828</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SSx1.p1" title="Qualitative Analysis ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg and T. L. Berg</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Baby talk: Understanding and generating simple image descriptions</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Colorado Springs, Colorado, U.S.A.</span>, <span class="ltx_text ltx_bib_pages"> pp. 1601–1608</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1109/CVPR.2011.5995466" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<span class="ltx_text isbn ltx_bib_external">ISBN 978-1-4577-0394-2</span>,
<a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995466" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg and Y. Choi</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Collective Generation of Natural Image Descriptions</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Jeju Island, South Korea</span>, <span class="ltx_text ltx_bib_pages"> pp. 359–368</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P/P12/P12-1038.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Li, G. Kulkarni, T. L. Berg, A. C. Berg and Y. Choi</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Composing simple image descriptions using web-scale n-grams</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Portland, Oregon, U.S.A.</span>, <span class="ltx_text ltx_bib_pages"> pp. 220–228</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-932432-92-3</span>,
<a href="http://dl.acm.org/citation.cfm?id=2018936.2018962" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, K. Stratos, A. Mensch, A. Berg, T. Berg and H. Daumé III</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Midge : Generating Image Descriptions From Computer Vision Detections</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Avignon, France</span>, <span class="ltx_text ltx_bib_pages"> pp. 747–756</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Oliva and A. Torralba</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Computer Vision</span> <span class="ltx_text ltx_bib_volume">42</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 145–175</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SSx1.p1" title="Qualitative Analysis ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Ordonez, G. Kulkarni and T. L. Berg</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Im2Text: Describing Images Using 1 Million Captioned Photographs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Granada, Spain</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.tamaraberg.com/papers/generation_nips2011.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p1" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Reiter and A. Belz</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An investigation into the validity of some metrics for automatically evaluating natural language generation systems</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">35</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 529–558</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1667994" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p2" title="4 Discussion ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Torralba, A. Oliva, M. S. Castelhano and J. M. Henderson</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Psychologial Review</span> <span class="ltx_text ltx_bib_volume">113</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 766–786</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.1037/0033-295X.113.4.766" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SSx1.p1" title="Qualitative Analysis ‣ 3 Results ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Yang, C. L. Teo, H. Daumé III and Y. Aloimonos</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Corpus-Guided Sentence Generation of Natural Images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, Scotland, UK.</span>, <span class="ltx_text ltx_bib_pages"> pp. 444–454</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D11-1041" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p2" title="2.2 Automatic Evaluation Measures ‣ 2 Methodology ‣ Comparing Automatic Evaluation Measures for Image Description" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:57:03 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
