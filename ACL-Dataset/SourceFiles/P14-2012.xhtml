<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Employing Word Representations and Regularization forDomain Adaptation of Relation Extraction</title>
<!--Generated on Wed Jun 11 17:32:53 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Employing Word Representations and Regularization for
<br class="ltx_break"/>Domain Adaptation of Relation Extraction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thien Huu Nguyen 
<br class="ltx_break"/>Computer Science Department 
<br class="ltx_break"/>New York University 
<br class="ltx_break"/>New York, NY 10003 USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">thien@cs.nyu.edu</span> 
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ralph Grishman 
<br class="ltx_break"/>Computer Science Department 
<br class="ltx_break"/>New York University 
<br class="ltx_break"/>New York, NY 10003 USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">grishman@cs.nyu.edu</span> 
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Relation extraction suffers from a performance loss when a model is applied to out-of-domain data. This has fostered the development of domain adaptation techniques for relation extraction. This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems. We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information. Finally, we demonstrate the effectiveness of regularization for the adaptability of relation extractors.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The goal of Relation Extraction (RE) is to detect and classify relation mentions between entity pairs into predefined relation types such as <span class="ltx_text ltx_font_italic">Employment</span> or <span class="ltx_text ltx_font_italic">Citizenship</span> relationships. Recent research in this area, whether feature-based <cite class="ltx_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Kambhatla2004</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Boschee et al.2005</a>, <a href="#bib.bibx34" title="" class="ltx_ref">Zhou et al.2005</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Grishman et al.2005</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Jiang and Zhai2007a</a>, <a href="#bib.bibx10" title="" class="ltx_ref">Chan and Roth2010</a>, <a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> or kernel-based <cite class="ltx_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Zelenko et al.2003</a>, <a href="#bib.bibx8" title="" class="ltx_ref">Bunescu and Mooney2005a</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Bunescu and Mooney2005b</a>, <a href="#bib.bibx33" title="" class="ltx_ref">Zhang et al.2006</a>, <a href="#bib.bibx26" title="" class="ltx_ref">Qian et al.2008</a>, <a href="#bib.bibx24" title="" class="ltx_ref">Nguyen et al.2009</a>]</cite>, attempts to improve the RE performance by enriching the feature sets from multiple sentence analyses and knowledge resources. The fundamental assumption of these supervised systems is that the training data and the data to which the systems are applied are sampled independently and identically from the same distribution. When there is a mismatch between data distributions, the RE performance of these systems tends to degrade dramatically <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>. This is where we need to resort to domain adaptation techniques (DA) to adapt a model trained on one domain (the source domain) into a new model which can perform well on new domains (the target domains).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The consequences of linguistic variation between training and testing data on NLP tools have been studied extensively in the last couple of years for various NLP tasks such as Part-of-Speech tagging <cite class="ltx_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Blitzer et al.2006</a>, <a href="#bib.bibx15" title="" class="ltx_ref">Huang and Yates2010</a>, <a href="#bib.bibx27" title="" class="ltx_ref">Schnabel and Schütze2014</a>]</cite>, named entity recognition <cite class="ltx_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Daumé III2007</a>]</cite> and sentiment analysis <cite class="ltx_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Blitzer et al.2007</a>, <a href="#bib.bibx12" title="" class="ltx_ref">Daumé III2007</a>, <a href="#bib.bibx13" title="" class="ltx_ref">Daumé III et al.2010</a>, <a href="#bib.bibx5" title="" class="ltx_ref">Blitzer et al.2011</a>]</cite>, etc. Unfortunately, there is very little work on domain adaptation for RE. The only study explicitly targeting this problem so far is by Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> who find that the out-of-domain performance of kernel-based relation extractors can be improved by embedding semantic similarity information generated from word clustering and latent semantic analysis (LSA) into syntactic tree kernels. Although this idea is interesting, it suffers from two major limitations:</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">+ It does not incorporate word cluster information at different levels of granularity. In fact, Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> only use the 10-bit cluster prefix in their study. We will demonstrate later that the adaptability of relation extractors can benefit significantly from the addition of word cluster features at various granularities.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">+ It is unclear if this approach can encode real-valued features of words (such as word embeddings <cite class="ltx_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Mnih and Hinton2007</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Collobert and Weston2008</a>]</cite>) effectively. As the real-valued features are able to capture latent yet useful properties of words, the augmentation of lexical terms with these features is desirable to provide a more general representation, potentially helping relation extractors perform more robustly across domains.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">In this work, we propose to avoid these limitations by applying a feature-based approach for RE which allows us to integrate various word features of generalization into a single system more naturally and effectively.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">The application of word representations such as word clusters in domain adaptation of RE <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> is motivated by its successes in semi-supervised methods <cite class="ltx_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Chan and Roth2010</a>, <a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> where word representations help to reduce data-sparseness of lexical information in the training data. In DA terms, since the vocabularies of the source and target domains are usually different, word representations would mitigate the lexical sparsity by providing general features of words that are shared across domains, hence bridge the gap between domains. The underlying hypothesis here is that the absence of lexical target-domain features in the source domain can be compensated by these general features to improve RE performance on the target domains.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p">We extend this motivation by further evaluating word embeddings <cite class="ltx_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Bengio et al.2001</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Bengio et al.2003</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Mnih and Hinton2007</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Collobert and Weston2008</a>, <a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite> on feature-based methods to adapt RE systems to new domains. We explore the embedding-based features in a principled way and demonstrate that word embedding itself is also an effective representation for domain adaptation of RE. More importantly, we show empirically that word embeddings and word clusters capture different information and their combination would further improve the adaptability of relation extractors.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Regularization</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Given the more general representations provided by word representations above, how can we learn a relation extractor from the labeled source domain data that generalizes well to new domains? In traditional machine learning where the challenge is to utilize the training data to make predictions on unseen data points (generated from the same distribution as the training data), the classifier with a good generalization performance is the one that not only fits the training data, but also avoids ovefitting over it. This is often obtained via regularization methods to penalize complexity of classifiers. Exploiting the shared interest in generalization performance with traditional machine learning, in domain adaptation for RE, we would prefer the relation extractor that fits the source domain data, but also circumvents the overfitting problem over this source domain<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_text ltx_font_italic">domain overfitting</span> <cite class="ltx_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Jiang and Zhai2007b</a>]</cite></span></span></span> so that it could generalize well on new domains. Eventually, regularization methods can be considered naturally as a simple yet general technique to cope with DA problems.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Following Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>, we assume that we only have labeled data in a single source domain but no labeled as well as unlabeled target data. Moreover, we consider the single-system DA setting where we construct a single system able to work robustly with different but related domains (multiple target domains). This setting differs from most previous studies <cite class="ltx_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Blitzer et al.2006</a>]</cite> on DA which have attempted to design a specialized system for every specific target domain. In our view, although this setting is more challenging, it is more practical for RE. In fact, this setting can benefit considerably from our general approach of applying word representations and regularization. Finally, due to this setting, the best way to set up the regularization parameter is to impose the same regularization parameter on every feature rather than a skewed regularization <cite class="ltx_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Jiang and Zhai2007b</a>]</cite>.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Related Work</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Although word embeddings have been successfully employed in many NLP tasks <cite class="ltx_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Collobert and Weston2008</a>, <a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Maas and Ng2010</a>]</cite>, the application of word embeddings in RE is very recent. Kuksa et al. <cite class="ltx_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Kuksa et al.2010</a>]</cite> propose an abstraction-augmented string kernel for bio-relation extraction via word embeddings. In the surge of deep learning, Socher et al. <cite class="ltx_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Socher et al.2012</a>]</cite> and Khashabi <cite class="ltx_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Khashabi2013</a>]</cite> use pre-trained word embeddings as input for Matrix-Vector Recursive Neural Networks (MV-RNN) to learn compositional structures for RE. However, none of these works evaluate word embeddings for domain adaptation of RE which is our main focus in this paper.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Regarding domain adaptation, in representation learning, Blitzer et al. <cite class="ltx_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Blitzer et al.2006</a>]</cite> propose structural correspondence learning (SCL) while Huang and Yates <cite class="ltx_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Huang and Yates2010</a>]</cite> attempt to learn a multi-dimensional feature representation. Unfortunately, these methods require unlabeled target domain data which are unavailable in our single-system setting of DA. Daumé III <cite class="ltx_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Daumé III2007</a>]</cite> proposes an easy adaptation framework (EA) which is later extended to a semi-supervised version (EA++) to incorporate unlabeled data <cite class="ltx_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">Daumé III et al.2010</a>]</cite>. In terms of word embeddings for DA, recently, Xiao and Guo <cite class="ltx_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Xiao and Guo2013</a>]</cite> present a log-bilinear language adaptation framework for sequential labeling tasks. However, these methods assume some labeled data in target domains and are thus not applicable in our setting of unsupervised DA. Above all, we move one step further by evaluating the effectiveness of word embeddings on domain adaptation for RE which is very different from the principal topic of sequence labeling in the previous research.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Word Representations</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We consider two types of word representations and use them as additional features in our DA system, namely Brown word clustering <cite class="ltx_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Brown et al.1992</a>]</cite> and word embeddings <cite class="ltx_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Bengio et al.2001</a>]</cite>. While word clusters can be recognized as an one-hot vector representation over a small vocabulary, word embeddings are dense, low-dimensional, and real-valued vectors (distributed representations). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities <cite class="ltx_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite>. We investigate word embeddings induced by two typical language models: Collobert and Weston (2008) embeddings (C&amp;W) <cite class="ltx_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Collobert and Weston2008</a>, <a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite> and Hierarchical log-bilinear embeddings (HLBL) <cite class="ltx_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Mnih and Hinton2007</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Mnih and Hinton2009</a>, <a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite>.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Feature Set</h2>

<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Baseline Feature Set</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">Sun et al. <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> utilize the full feature set from <cite class="ltx_cite">[<a href="#bib.bibx34" title="" class="ltx_ref">Zhou et al.2005</a>]</cite> plus some additional features and achieve the state-of-the-art feature-based RE system. Unfortunately, this feature set includes the <span class="ltx_text ltx_font_italic">human-annotated</span> (gold-standard) information on entity and mention types which is often missing or noisy in reality <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>. This issue becomes more serious in our setting of single-system DA where we have a single source domain with multiple dissimilar target domains and an automatic system able to recognize entity and mention types very well in different domains may not be available. Therefore, following the settings of Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>, we will only assume entity boundaries and not rely on the gold standard information in the experiments. We apply the same feature set as Sun et al. <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> but remove the entity and mention type information<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>We have the same observation as Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> that when the gold-standard labels are used, the impact of word representations is limited since the gold-standard information seems to dominate. However, whenever the gold labels are not available or inaccurate, the word representations would be useful for improving adaptability performance. Moreover, in all the cases, regularization methods are still effective for domain adaptation of RE.</span></span></span>.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Lexical Feature Augmentation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">While Sun et al. <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> show that adding word clusters to the heads of the two mentions is the most effective way to improve the generalization accuracy, the right lexical features into which word embeddings should be introduced to obtain the best adaptability improvement are unexplored. Also, which dimensionality of which word embedding should we use with which lexical features? In order to answer these questions, following Sun et al. <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite>, we first group lexical features into 4 groups and rank their importance based on linguistic intuition and illustrations of the contributions of different lexical features from various feature-based RE systems. After that, we evaluate the effectiveness of these lexical feature groups for word embedding augmentation individually and incrementally according to the rank of importance. For each of these group combinations, we assess the system performance with different numbers of dimensions for both C&amp;W and HLBL word embeddings. Let M1 and M2 be the first and second mentions in the relation. Table 1 describes the lexical feature groups.</p>
</div>
<div id="S5.T1" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">Rank</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">Group</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">Lexical Features</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">1</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">HM</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">HM1 (head of M1)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">HM2 (head of M2)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">2</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">BagWM</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">WM1 (words in M1)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">WM2 (words in M2)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">3</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">HC</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">heads of chunks in context</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:17.1pt;" width="17.1pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:28.5pt;" width="28.5pt"><span class="ltx_ERROR undefined ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_bold ltx_font_small ltx_align_center">BagWC</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:99.6pt;" width="99.6pt"><span class="ltx_ERROR undefined ltx_centering ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">words of context</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 1: </span> Lexical feature groups ordered by importance.</div>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.1 </span>Tools and Data</h3>

<div id="S6.T2" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_l ltx_border_r ltx_border_t" colspan="2"/>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t" colspan="5"><span class="ltx_text ltx_font_small">In-domain (bn+nw)</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t" style="background-color:#B3B3B3;" colspan="5"><span class="ltx_text ltx_font_small" style="background-color:#B3B3B3;">Out-of-domain (bc development set)</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="2"><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,25</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,50</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,100</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">HLBL,50</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">HLBL,100</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,25</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,50</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">C&amp;W,100</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">HLBL,50</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">HLBL,100</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">1</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">Baseline</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">2</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">1+HM_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.0(+2.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.1(+2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">55.7(+4.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.7(+2.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">55.2(+3.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.5(+2.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">52.7(+3.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.5(+3.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.2(+1.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.6(+1.6)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">3</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">1+BagWM_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.3(+0.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.9(-0.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.5(+0.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.8(+0.4)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.5(+1.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.5(-0.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.9(-0.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.6(-0.4)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.7(-0.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.0(+0.0)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">4</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">1+HC_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.3(-0.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.9(-0.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.3(-3.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.8(-0.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.8(-1.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">44.9(-4.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">45.8(-3.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">45.8(-3.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.7(-0.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">47.3(-1.7)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">5</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">1+BagWC_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.5(+0.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.8(-0.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.5(-1.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4(+0.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.3(-1.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.3(-0.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">46.3(-2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">44.0(-5.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">46.6(-2.4)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">44.8(-4.2)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">6</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">2+BagWM_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.3(+2.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.2(+1.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.2(+1.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.0(+2.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.8(+2.4)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.5(+3.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4(+2.4)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.6(+1.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.0(+1.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.6(-0.4)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">7</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">6+HC_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.4(+2.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.3(+0.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.7(+1.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.2(+2.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.1(+1.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.5(+1.5)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.9(+1.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.4(-0.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.0(+1.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.9(-0.1)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">8</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:55.5pt;" width="55.5pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">7+BagWC_ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.4(+2.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.2(+0.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.8(-0.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.5(+2.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.6(+2.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.2(+0.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.7(+1.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.2(+0.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">47.9(-1.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span><span class="ltx_text ltx_font_small">49.5(+0.5)</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 2: </span> In-domain and Out-of-domain performance for different embedding features. The cells in bold are the best results.</div>
</div>
<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">Our relation extraction system is hierarchical <cite class="ltx_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Bunescu and Mooney2005b</a>, <a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite> and apply maximum entropy (MaxEnt) in the MALLET<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><a href="http://mallet.cs.umass.edu/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://mallet.cs.umass.edu/</span></a></span></span></span> toolkit as the machine learning tool. For Brown word clusters, we directly apply the clustering trained by Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> to facilitate system comparison later. We evaluate C&amp;W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al. <cite class="ltx_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite> and can be downloaded here<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="http://metaoptimize.com/projects/wordreprs/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://metaoptimize.com/projects/wordreprs/</span></a></span></span></span>. The fact that we utilize the large, general and unbiased resources generated from the previous works for evaluation not only helps to verify the effectiveness of the resources across different tasks and settings but also supports our setting of single-system DA.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p">We use the ACE 2005 corpus for DA experiments (as in Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>). It involves 6 relation types and 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). We follow the standard practices on ACE <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> and use <span class="ltx_text ltx_font_bold">news (the union of bn and nw)</span> as the source domain and <span class="ltx_text ltx_font_bold">bc</span>, <span class="ltx_text ltx_font_bold">cts</span> and <span class="ltx_text ltx_font_bold">wl</span> as our target domains. We take half of bc as the only target development set, and use the remaining data and domains for testing purposes (as they are small already). As noted in Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>, the distributions of relations as well as the vocabularies of the domains are quite different.</p>
</div>
</div>
<div id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.2 </span>Evaluation of Word Embedding Features</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">We investigate the effectiveness of word embeddings on lexical features by following the procedure described in Section <a href="#S5.SS2" title="5.2 Lexical Feature Augmentation ‣ 5 Feature Set ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. We test our system on two scenarios: In-domain: the system is trained and evaluated on the source domain (bn+nw, 5-fold cross validation); Out-of-domain: the system is trained on the source domain and evaluated on the target development set of bc (bc dev). Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Tools and Data ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the F measures of this experiment<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>All the in-domain improvement in rows 2, 6, 7 of Table 2 are significant at confidence levels <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS2.p1.m1" class="ltx_Math" alttext="\geq" display="inline"><mo>≥</mo></math> 95%.</span></span></span> (the suffix <span class="ltx_text ltx_font_italic">ED</span> in lexical group names is to indicate the embedding features).</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p">From the tables, we find that for C&amp;W and HLBL embeddings of 50 and 100 dimensions, the most effective way to introduce word embeddings is to add embeddings to the heads of the two mentions (row 2; both in-domain and out-of-domain) although it is less pronounced for HLBL embedding with 50 dimensions. Interestingly, for C&amp;W embedding with 25 dimensions, adding the embedding to both heads and words of the two mentions (row 6) performs the best for both in-domain and out-of-domain scenarios. This is new compared to the word cluster features where the heads of the two mentions are always the best places for augmentation <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Sun et al.2011</a>]</cite>. It suggests that a suitable amount of embeddings for words in the mentions might be useful for the augmentation of the heads and inspires further exploration. Introducing embeddings to words of mentions alone has mild impact while it is generally a bad idea to augment chunk heads and words in the contexts.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p class="ltx_p">Comparing C&amp;W and HLBL embeddings is somehow more complicated. For both in-domain and out-of-domain settings with different numbers of dimensions, C&amp;W embedding outperforms HLBL embedding when only the heads of the mentions are augmented while the degree of negative impact of HLBL embedding on chunk heads as well as context words seems less serious than C&amp;W’s. Regarding the incremental addition of features (rows 6, 7, 8), C&amp;W is better for the out-of-domain performance when 50 dimensions are used, whereas HLBL (with both 50 and 100 dimensions) is more effective for the in-domain setting. For the next experiments, we will apply the C&amp;W embedding of 50 dimensions to the heads of the mentions for its best out-of-domain performance.</p>
</div>
</div>
<div id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.3 </span>Domain Adaptation with Word Embeddings</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p">This section examines the effectiveness of word representations for RE across domains. We evaluate word cluster and embedding (denoted by ED) features by adding them individually as well as simultaneously into the baseline feature set. For word clusters, we experiment with two possibilities: (i) only using a single prefix length of 10 (as Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> did) (denoted by WC10) and (ii) applying multiple prefix lengths of 4, 6, 8, 10 together with the full string (denoted by WC). Table <a href="#S6.T3" title="Table 3 ‣ 6.3 Domain Adaptation with Word Embeddings ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the system performance (F measures) for both in-domain and out-of-domain settings.</p>
</div>
<div id="S6.T3" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">In-domain</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">bc</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">cts</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">wl</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">Baseline(B)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">51.4</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">49.7</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">41.5</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">36.6</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC10</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.3(+0.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">50.8(+1.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">45.7(+4.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">39.6(+3)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">53.7(+2.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.8(+3.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">46.8(+5.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">41.7(+5.1)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">54.1(+2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.4(+2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">46.2(+4.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">42.5(+5.9)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC+ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">55.5(+4.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">53.8(+4.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">47.4(+5.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small ltx_align_center">44.7(+8.1)</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 3: </span> Domain Adaptation Results with Word Representations. All the improvements over the baseline in Table <a href="#S6.T3" title="Table 3 ‣ 6.3 Domain Adaptation with Word Embeddings ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> are significant at confidence level <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T3.m2" class="ltx_Math" alttext="\geq" display="inline"><mo mathsize="normal" stretchy="false">≥</mo></math> 95%.</div>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p">The key observations from the table are:</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p class="ltx_p">(i): The baseline system achieves a performance of 51.4% within its own domain while the performance on target domains bc, cts, wl drops to 49.7%, 41.5% and 36.6% respectively. Our baseline performance is worse than that of Plank and Moschitti <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite> only on the target domain cts and better in the other cases. This might be explained by the difference between our baseline feature set and the feature set underlying their kernel-based system. However, the performance order across domains of the two baselines are the same. Besides, the baseline performance is improved over all target domains when the system is enriched with word cluster features of the 10 prefix length only (row 2).</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p class="ltx_p">(ii): Over all the target domains, the performance of the system augmented with word cluster features of various granularities (row 3) is superior to that when only cluster features for the prefix length 10 are added (row 2). This is significant (at confidence level <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p4.m1" class="ltx_Math" alttext="\geq" display="inline"><mo>≥</mo></math> 95%) for domains bc and wl and verifies our assumption that various granularities for word cluster features are more effective than a single granularity for domain adaptation of RE.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p class="ltx_p">(iii): Row 4 shows that word embedding itself is also very useful for domain adaptation in RE since it improves the baseline system for all the target domains.</p>
</div>
<div id="S6.SS3.p6" class="ltx_para">
<p class="ltx_p">(iv): In row 5, we see that the addition of both word cluster and word embedding features improves the system further and results in the best performance over all target domains (this is significant with confidence level <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS3.p6.m1" class="ltx_Math" alttext="\geq" display="inline"><mo>≥</mo></math> 95% in domains bc and wl). The result suggests that word embeddings seem to capture different information from word clusters and their combination would be effective to generalize relation extractors across domains. However, in domain cts, the improvement that word embeddings provide for word clusters is modest. This is because the RCV1 corpus used to induce the word embeddings <cite class="ltx_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Turian et al.2010</a>]</cite> does not cover spoken language words in cts very well.</p>
</div>
<div id="S6.SS3.p7" class="ltx_para">
<p class="ltx_p">(v): Finally, the in-domain performance is also improved consistently demonstrating the robustness of word representations <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Plank and Moschitti2013</a>]</cite>.</p>
</div>
</div>
<div id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">6.4 </span>Domain Adaptation with Regularization</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p class="ltx_p">All the experiments we have conducted so far do not apply regularization for training. In this section, in order to evaluate the effect of regularization on the generalization capacity of relation extractors across domains, we replicate all the experiments in Section <a href="#S6.SS3" title="6.3 Domain Adaptation with Word Embeddings ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a> but apply regularization when relation extractors are trained<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>We use a L2 regularizer with the regularization parameter of 0.5 for its best experimental results.</span></span></span>. Table <a href="#S6.T4" title="Table 4 ‣ 6.4 Domain Adaptation with Regularization ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results.</p>
</div>
<div id="S6.T4" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">In-domain</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">bc</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">cts</span></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">wl</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">Baseline(B)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">56.2</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">55.5</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.7</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">42.2</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC10</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">57.5(+1.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">57.3(+1.8)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.3(+3.6)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">45.0(+2.8)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">58.9(+2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">58.4(+2.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.8(+4.1)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">47.3(+5.1)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">58.9(+2.7)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">59.5(+4.0)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">52.6(+3.9)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">48.6(+6.4)</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:39.8pt;" width="39.8pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_font_small">B+WC+ED</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">59.4(+3.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">59.8(+4.3)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small">52.9(+4.2)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" style="width:36.7pt;" width="36.7pt"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span><span class="ltx_text ltx_align_left ltx_font_bold ltx_font_small ltx_align_center">49.7(+7.5)</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 4: </span> Domain Adaptation Results with Regularization. All the improvements over the baseline in Table <a href="#S6.T4" title="Table 4 ‣ 6.4 Domain Adaptation with Regularization ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are significant at confidence level <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T4.m2" class="ltx_Math" alttext="\geq" display="inline"><mo mathsize="normal" stretchy="false">≥</mo></math> 95%.</div>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p class="ltx_p">For this experiment, every statement in (ii), (iii), (iv) and (v) of Section <a href="#S6.SS3" title="6.3 Domain Adaptation with Word Embeddings ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a> also holds. More importantly, the performance in every cell of Table <a href="#S6.T4" title="Table 4 ‣ 6.4 Domain Adaptation with Regularization ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> is significantly better than the corresponding cell in Table <a href="#S6.T3" title="Table 3 ‣ 6.3 Domain Adaptation with Word Embeddings ‣ 6 Experiments ‣ Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (5% or better gain in F measure, a significant improvement at confidence level <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS4.p2.m1" class="ltx_Math" alttext="\geq" display="inline"><mo>≥</mo></math> 95%). This demonstrates the effectiveness of regularization for RE in general and for domain adaptation of RE specifically.</p>
</div>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bengio et al.2001</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.

</span>
<span class="ltx_bibblock">2001.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Neural Probabilistic Language Model</span>.

</span>
<span class="ltx_bibblock">In Advances in Neural Information Processing Systems (NIPS’13), pages 932-938, MIT Press, 2001.

</span></li>
<li id="bib.bibx2" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bengio et al.2003</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Neural Probabilistic Language Model</span>.

</span>
<span class="ltx_bibblock">In Journal of Machine Learning Research (JMLR), 3, pages 1137-1155, 2003.

</span></li>
<li id="bib.bibx3" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Blitzer et al.2006</span>
<span class="ltx_bibblock">
John Blitzer, Ryan McDonald, and Fernando Pereira.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Domain Adaptation with Structural Correspondence Learning</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.

</span></li>
<li id="bib.bibx4" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Blitzer et al.2007</span>
<span class="ltx_bibblock">
John Blitzer, Mark Dredze, and Fernando Pereira.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the ACL, pages 440-447, Prague, Czech Republic, June 2007.

</span></li>
<li id="bib.bibx5" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Blitzer et al.2011</span>
<span class="ltx_bibblock">
John Blitzer, Dean Foster, and Sham Kakade.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Domain Adaptation with Coupled Subspaces</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 173-181, Fort Lauderdale, FL, USA.

</span></li>
<li id="bib.bibx6" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Boschee et al.2005</span>
<span class="ltx_bibblock">
Elizabeth Boschee, Ralph Weischedel, and Alex Zamanian.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Automatic Information Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the International Conference on Intelligence Analysis.

</span></li>
<li id="bib.bibx7" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Brown et al.1992</span>
<span class="ltx_bibblock">
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.

</span>
<span class="ltx_bibblock">1992.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Class-Based n-gram Models of Natural Language</span>.

</span>
<span class="ltx_bibblock">In Journal of Computational Linguistics, Volume 18, Issue 4, pages 467-479, December 1992.

</span></li>
<li id="bib.bibx8" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bunescu and Mooney2005a</span>
<span class="ltx_bibblock">
Razvan C. Bunescu and Raymond J. Mooney.

</span>
<span class="ltx_bibblock">2005a.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Shortest Path Dependency Kenrel for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of HLT/EMNLP.

</span></li>
<li id="bib.bibx9" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Bunescu and Mooney2005b</span>
<span class="ltx_bibblock">
Razvan C. Bunescu and Raymond J. Mooney.

</span>
<span class="ltx_bibblock">2005b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Subsequence Kernels for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of NIPS.

</span></li>
<li id="bib.bibx10" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Chan and Roth2010</span>
<span class="ltx_bibblock">
Yee S. Chan and Dan Roth.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Exploiting Background Knowledge for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 152-160, Beijing, China, August.

</span></li>
<li id="bib.bibx11" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Collobert and Weston2008</span>
<span class="ltx_bibblock">
Ronan Collobert and Jason Weston.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A UniÞed Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</span>.

</span>
<span class="ltx_bibblock">In International Conference on Machine Learning, ICML, 2008.

</span></li>
<li id="bib.bibx12" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Daumé III2007</span>
<span class="ltx_bibblock">
Hal Daumé III.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Frustratingly Easy Domain Adaptation</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the ACL, pages 256-263, Prague, Czech Republic, June 2007.

</span></li>
<li id="bib.bibx13" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Daumé III et al.2010</span>
<span class="ltx_bibblock">
Hal Daumé III, Abhishek Kumar and Avishek Saha.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Co-regularization Based Semi-supervised Domain Adaptation</span>.

</span>
<span class="ltx_bibblock">In Advances in Neural Information Processing Systems 23 (2010).

</span></li>
<li id="bib.bibx14" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Grishman et al.2005</span>
<span class="ltx_bibblock">
Ralph Grishman, David Westbrook and Adam Meyers.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">NYU’s English ACE 2005 System Description</span>.

</span>
<span class="ltx_bibblock">ACE 2005 Evaluation Workshop.

</span></li>
<li id="bib.bibx15" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Huang and Yates2010</span>
<span class="ltx_bibblock">
Fei Huang and Alexander Yates.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Exploring Representation-Learning Approaches to Domain Adaptation</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, pages 23-30, Uppsala, Sweden, July 2010.

</span></li>
<li id="bib.bibx16" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Jiang and Zhai2007a</span>
<span class="ltx_bibblock">
Jing Jiang and ChengXiang Zhai.

</span>
<span class="ltx_bibblock">2007a.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Systematic Exploration of the Feature Space for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT’07), pages 113-120, 2007.

</span></li>
<li id="bib.bibx17" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Jiang and Zhai2007b</span>
<span class="ltx_bibblock">
Jing Jiang and ChengXiang Zhai.

</span>
<span class="ltx_bibblock">2007b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Two-stage Approach to Domain Adaptation for Statistical Classifiers</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM’07), pages 401-410, 2007.

</span></li>
<li id="bib.bibx18" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Kambhatla2004</span>
<span class="ltx_bibblock">
Nanda Kambhatla.

</span>
<span class="ltx_bibblock">2004.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of ACL-04.

</span></li>
<li id="bib.bibx19" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Khashabi2013</span>
<span class="ltx_bibblock">
Daniel Khashabi.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">On the Recursive Neural Networks for Relation Extraction and Entity Recognition</span>.

</span>
<span class="ltx_bibblock">Technical Report (May, 2013), UIUC.

</span></li>
<li id="bib.bibx20" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Kuksa et al.2010</span>
<span class="ltx_bibblock">
Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Collobert, Jason Weston, Vladimir Pavlovic, and Xia Ning.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Semi-Supervised Abstraction-Augmented String Kernel for Multi-Level Bio-Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases, Part II (ECML PKDD’10), pages 128-144, 2010.

</span></li>
<li id="bib.bibx21" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Maas and Ng2010</span>
<span class="ltx_bibblock">
Andrew L. Maas and Andrew Y. Ng.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Probabilistic Model for Semantic Word Vectors</span>.

</span>
<span class="ltx_bibblock">In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.

</span></li>
<li id="bib.bibx22" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Mnih and Hinton2007</span>
<span class="ltx_bibblock">
Andriy Mnih and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Three new Graphical Models for Statistical Language Modelling</span>.

</span>
<span class="ltx_bibblock">In Proceedings of ICML’07, pages 641-648, Corvallis, OR, 2007.

</span></li>
<li id="bib.bibx23" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Mnih and Hinton2009</span>
<span class="ltx_bibblock">
Andriy Mnih and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Scalable Hierarchical Distributed Language Model</span>.

</span>
<span class="ltx_bibblock">In NIPS, page 1081-1088.

</span></li>
<li id="bib.bibx24" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Nguyen et al.2009</span>
<span class="ltx_bibblock">
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of EMNLP Õ09, pages 1378-1387, Stroudsburg, PA, USA.

</span></li>
<li id="bib.bibx25" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Plank and Moschitti2013</span>
<span class="ltx_bibblock">
Barbara Plank and Alessandro Moschitti.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the ACL 2013, pages 1498-1507, Sofia, Bulgaria.

</span></li>
<li id="bib.bibx26" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Qian et al.2008</span>
<span class="ltx_bibblock">
Longhua Qian, Guodong Zhou, Qiaoming Zhu and Peide Qian.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Exploiting Constituent Ddependencies for Tree Kernel-based Semantic Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of COLING, pages 697-704, Manchester.

</span></li>
<li id="bib.bibx27" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Schnabel and Schütze2014</span>
<span class="ltx_bibblock">
Tobias Schnabel and Hinrich Schütze.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging</span>.

</span>
<span class="ltx_bibblock">In Transactions of the Association for Computational Linguistics, 2 (2014), pages 15Ð26.

</span></li>
<li id="bib.bibx28" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Socher et al.2012</span>
<span class="ltx_bibblock">
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Semantic Compositionality through Recursive Matrix-Vector Spaces</span>.

</span>
<span class="ltx_bibblock">In Proceedings EMNLP-CoNLL’12, pages 1201-1211, Jeju Island, Korea, July 2012.

</span></li>
<li id="bib.bibx29" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Sun et al.2011</span>
<span class="ltx_bibblock">
Ang Sun, Ralph Grishman, and Satoshi Sekine.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Semi-supervised Relation Extraction with Large-scale Word Clustering</span>.

</span>
<span class="ltx_bibblock">In Proceedings of ACL-HLT, pages 521-529, Portland, Oregon, USA.

</span></li>
<li id="bib.bibx30" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Turian et al.2010</span>
<span class="ltx_bibblock">
Joseph Turian, Lev Ratinov, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Word representations: A simple and general method for semi-supervised learning</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10), pages 384-394, Uppsala, Sweden, July, 2010.

</span></li>
<li id="bib.bibx31" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Xiao and Guo2013</span>
<span class="ltx_bibblock">
Min Xiao and Yuhong Guo.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model</span>.

</span>
<span class="ltx_bibblock">In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 293-301, 2013.

</span></li>
<li id="bib.bibx32" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Zelenko et al.2003</span>
<span class="ltx_bibblock">
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella.

</span>
<span class="ltx_bibblock">2003.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Kernel Methods for Relation Extraction</span>.

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research, 3:1083Ð1106.

</span></li>
<li id="bib.bibx33" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Zhang et al.2006</span>
<span class="ltx_bibblock">
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features</span>.

</span>
<span class="ltx_bibblock">In Proceedings of COLING-ACL-06, pages 825-832, Sydney.

</span></li>
<li id="bib.bibx34" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Zhou et al.2005</span>
<span class="ltx_bibblock">
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Exploring various Knowledge in Relation Extraction</span>.

</span>
<span class="ltx_bibblock">In Proceedings of ACL’05, pages 427-434, Ann Arbor, USA, 2005.

</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:32:53 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
