<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Response-based Learning for Grounded Machine Translation</title>
<!--Generated on Tue Jun 10 18:11:04 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Response-based Learning for Grounded Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefan Riezler 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patrick Simianer 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Carolin Haas 
<br class="ltx_break"/>Department of Computational Linguistics 
<br class="ltx_break"/>Heidelberg University, 69120 Heidelberg, Germany 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">riezler,simianer,haas1</span>}<span class="ltx_text ltx_font_typewriter">@cl.uni-heidelberg.de</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We propose a novel learning approach for statistical machine translation (SMT) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input. We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database. Experiments on the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database show an improvement of about 6 points in F1-score for response-based learning over learning from references only on returning the correct answer from a semantic parse of a translated query. In general, our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">In this paper, we propose a novel approach for learning and evaluation in statistical machine translation (SMT) that borrows ideas from response-based learning for grounded semantic parsing. In this framework, the meaning of a sentence is defined in the context of an extrinsic task. Successful communication of meaning is measured by a successful interaction in this task, and feedback from this interaction is used for learning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">We suggest that in a similar way the preservation of meaning in machine translation should be defined in the context of an interaction in an extrinsic task. For example, in the context of a game, a description of a game rule is translated successfully if correct game moves can be performed based only on the translation. In the context of a question-answering scenario, a question is translated successfully if the correct answer is returned based only on the translation of the query.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">We propose a framework of response-based learning that allows to extract supervision signals for structured learning from the response of an extrinsic task to a translation input. Here, learning proceeds by “trying out” translation hypotheses, receiving a response from interacting in the task, and converting this response into a supervision signal for updating model parameters. In case of positive feedback, the predicted translation can be treated as reference translation for a structured learning update. In case of negative feedback, a structural update can be performed against translations that have been approved previously by positive task feedback. This framework has several advantages:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">The supervision signal in response-based learning has a different quality than supervision by human-generated reference translations. While a human reference translation is generated independently of the SMT task, conversion of predicted translations into references is always done with respect to a specific task. In this sense we speak of grounding meaning transfer in an extrinsic task.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">Response-based learning can repeatedly try out system predictions by interacting in the extrinsic task. Instead of and in addition to learning from human reference translations, response-based learning allows to convert multiple system translations into references. This alleviates the supervision problem in cases where parallel data are scarce.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">Task-specific response acts upon system translations. This avoids the problem of unreachability of independently generated reference translations by the SMT system.</p>
</div></li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">The proposed approach of response-based learning opens the doors for various extrinsic tasks in which SMT systems can be trained and evaluated. In this paper, we present a proof-of-concept experiment that uses feedback from a simulated world environment. Building on prior work in grounded semantic parsing, we generate translations of queries, and receive feedback by executing semantic parses of translated queries against the database. Successful response is defined as receiving the same answer from the semantic parses for the translation and the original query. Our experimental results show an improvement of about 6 points in F1-score for response-based learning over standard structured learning from reference translations. We show in an error analysis that this improvement can be attributed to using structural and lexical variants of reference translations as positive examples in response-based learning. Furthermore, translations produced by response-based learning are found to be grammatical. This is due to the possibility to boost similarity to human reference translations by the additional use of a cost function in our approach.</p>
</div>
<div id="S1.F1" class="ltx_figure"><img src="P14-1083/image001.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="351" height="292" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Response-based learning cycle for grounding SMT in virtual trivia gameplay.</div>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The key idea of <em class="ltx_emph">grounded language learning</em> is to study natural language in the context of a non-linguistic environment, in which meaning is grounded in perception and/or action. This presents an analogy to human learning, where a learner tests her understanding in an actionable setting.
Such a setting can be a simulated world environment in which the linguistic representation can be directly executed by a computer system. For example, in semantic parsing, the learning goal is to produce and successfully execute a meaning representation. Executable system actions include access to databases such as the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database on U.S. geography (<cite class="ltx_cite"><a href="#bib.bibx37" title="" class="ltx_ref">Wong and Mooney2006</a></cite>, <em class="ltx_emph">inter alia</em>), the <span class="ltx_text ltx_font_smallcaps">Atis</span> travel planning database (<cite class="ltx_cite"><a href="#bib.bibx38" title="" class="ltx_ref">Zettlemoyer and Collins2009</a></cite>, <em class="ltx_emph">inter alia</em>), robotic control in simulated navigation tasks (<cite class="ltx_cite"><a href="#bib.bibx7" title="" class="ltx_ref">Chen and Mooney2011</a></cite>, <em class="ltx_emph">inter alia</em>), databases of simulated card games (<cite class="ltx_cite"><a href="#bib.bibx15" title="" class="ltx_ref">Goldwasser and Roth2013</a></cite>, <em class="ltx_emph">inter alia</em>), or the user-generated contents of <span class="ltx_text ltx_font_smallcaps">Freebase</span> (<cite class="ltx_cite"><a href="#bib.bibx4" title="" class="ltx_ref">Cai and Yates2013</a></cite>, <em class="ltx_emph">inter alia</em>). Since there are many possible correct parses, matching against a single gold standard falls short of grounding in a non-linguistic environment. Rather, the semantic context for interpretation, as well as the success criterion in evaluation is defined by successful execution of an action in the extrinsic environment, e.g., by receiving the correct answer from the database or by successful navigation to the destination.
Recent attempts to learn semantic parsing from question-answer pairs without recurring to annotated logical forms have been presented by <cite class="ltx_cite"><a href="#bib.bibx20" title="" class="ltx_ref">Kwiatowski et al.2013</a></cite>, <cite class="ltx_cite"><a href="#bib.bibx3" title="" class="ltx_ref">Berant et al.2013</a></cite>, or <cite class="ltx_cite"><a href="#bib.bibx15" title="" class="ltx_ref">Goldwasser and Roth2013</a></cite>. The algorithms presented in these works are variants of structured prediction that take executability of semantic parses into account. Our work builds upon these ideas, however, to our knowledge the presented work is the first to embed translations into grounded scenarios in order to use feedback from interactions in these scenarios for structured learning in SMT.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">A recent important research direction in SMT has focused on employing automated translation as an aid to human translators. <em class="ltx_emph">Computer assisted translation</em> (CAT) subsumes several modes of interaction, ranging from binary feedback on the quality of the system prediction <cite class="ltx_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Saluja et al.2012</a>]</cite>, to human post-editing operations on a system prediction resulting in a reference translation <cite class="ltx_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Cesa-Bianchi et al.2008</a>]</cite>, to human acceptance or overriding of sentence completion predictions <cite class="ltx_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Langlais et al.2000</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Barrachina et al.2008</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Koehn and Haddow2009</a>]</cite>. In all interaction scenarios, it is important that the system learns dynamically from its errors in order to offer the user the experience of a system that adapts to the provided feedback. Since retraining the SMT model after each interaction is too costly, <em class="ltx_emph">online adaptation</em> after each interaction has become the learning protocol of choice for CAT. Online learning has been applied in generative SMT, e.g., using incremental versions of the EM algorithm <cite class="ltx_cite">[<a href="#bib.bibx28" title="" class="ltx_ref">Ortiz-Martínez et al.2010</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Hardt and Elming2010</a>]</cite>, or in discriminative SMT, e.g., using perceptron-type algorithms <cite class="ltx_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Cesa-Bianchi et al.2008</a>, <a href="#bib.bibx23" title="" class="ltx_ref">Martínez-Gómez et al.2012</a>, <a href="#bib.bibx36" title="" class="ltx_ref">Wäschle et al.2013</a>, <a href="#bib.bibx11" title="" class="ltx_ref">Denkowski et al.2014</a>]</cite>.
In a similar way to deploying human feedback, extrinsic loss functions have been used to provide learning signals for SMT. For example, <cite class="ltx_cite"><a href="#bib.bibx26" title="" class="ltx_ref">Nikoulina et al.2012</a></cite> propose a setup where an SMT system feeds into cross-language information retrieval, and receives feedback from the performance of translated queries with respect to cross-language retrieval performance. This feedback is used to train a reranker on an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-best list of translations order with respect to retrieval performance.
In contrast to our work, all mentioned approaches to interactive or adaptive learning in SMT rely on human post-edits or human reference translations. Our work differs from these approaches in that exactly this dependency is alleviated by learning from responses in an extrinsic task.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Interactive scenarios have been used for evaluation purposes of translation systems for nearly 50 years, especially using <em class="ltx_emph">human reading comprehension</em> testing <cite class="ltx_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Pfafflin1965</a>, <a href="#bib.bibx13" title="" class="ltx_ref">Fuji1999</a>, <a href="#bib.bibx17" title="" class="ltx_ref">Jones et al.2005</a>]</cite>, and more recently, using face-to-face conversation mediated via machine translation <cite class="ltx_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Sakamoto et al.2013</a>]</cite>. However, despite offering direct and reliable prediction of translation quality, the cost and lack of reusability has confined task-based evaluations involving humans to <em class="ltx_emph">testing</em> scenarios, but prevented a use for interactive <em class="ltx_emph">training</em> of SMT systems as in our work.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Lastly, our work is related to <em class="ltx_emph">cross-lingual natural language processing</em> such as cross-lingual question answering or cross-lingual information retrieval as conducted at recent evaluation campaigns of the CLEF initiative.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><a href="http://www.clef-initiative.eu" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.clef-initiative.eu</span></a></span></span></span> While these approaches focus on improvements of the respective natural language processing task, our goal is to improve SMT by gathering feedback from the task.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Grounding SMT in Semantic Parsing</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this paper, we present a proof-of-concept of our ideas of embedding SMT into simulated world environments as used in semantic parsing. We use the well-known <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database on U.S. geography for this purpose. Embedding SMT in a semantic parsing scenario means to define translation quality by the ability of a semantic parser to construct a meaning representation from the translated query, which returns the correct answer when executed against the database. If viewed as simulated gameplay, a valid game move in this scenario returns the correct answer to a translated query.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The diagram in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> gives a sketch of response-based learning from semantic parsing in the geographical domain. Given a manual German translation of the English query as source sentence, the SMT system produces an English target translation. This sentence is fed into a semantic parser that produces an executable parse representation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="p_{h}" display="inline"><msub><mi>p</mi><mi>h</mi></msub></math>. Feedback is generated by executing the parse against the database of geographical facts. Positive feedback means that the correct answer is received, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="\textrm{exec}(p_{g})\stackrel{?}{=}\textrm{exec}(p_{h})" display="inline"><mrow><mrow><mtext>exec</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>p</mi><mi>g</mi></msub><mo>)</mo></mrow></mrow><mover><mo movablelimits="false">=</mo><mi mathvariant="normal">?</mi></mover><mrow><mtext>exec</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>p</mi><mi>h</mi></msub><mo>)</mo></mrow></mrow></mrow></math> indicates that the same answer is received from the gold standard parse <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m3" class="ltx_Math" alttext="p_{g}" display="inline"><msub><mi>p</mi><mi>g</mi></msub></math> and the parse for the hypothesis translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m4" class="ltx_Math" alttext="p_{h}" display="inline"><msub><mi>p</mi><mi>h</mi></msub></math>; negative feedback results in case a different or no answer is received.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The key advantage of response-based learning is the possibility to receive positive feedback even from predictions that differ from gold standard reference translations, but yet receive the correct answer when parsed and matched against the database. Such structural and lexical variation broadens the learning capabilities in contrast to learning from fixed labeled data.
For example, assume the following English query in the geographical domain, and assume positive feedback from executing the corresponding semantic parse against the geographical database:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_typewriter ltx_font_small">Name prominent elevations in the USA</span></p>
</blockquote>
<p class="ltx_p">The manual translation of the English original reads</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_typewriter ltx_font_small">Nenne prominente Erhebungen in den USA</span></p>
</blockquote>
<p class="ltx_p">An automatic translation<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><a href="http://translate.google.com" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://translate.google.com</span></a></span></span></span> of the German string produces the result</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_typewriter ltx_font_small">Give prominent surveys in the US</span></p>
</blockquote>
<p class="ltx_p">This translation will trigger negative task-based feedback: A comparison with the original allows the error to be traced back to the ambiguity of the German word <span class="ltx_text ltx_font_typewriter ltx_font_small">Erhebung</span>. Choosing a general domain translation instead of a translation appropriate for the geographical domain hinders the construction of a semantic parse that returns the correct answer from the database. An alternative translation might look as follows:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_typewriter ltx_font_small">Give prominent heights in the US</span></p>
</blockquote>
<p class="ltx_p">Despite a large difference to the original English string, key terms such as <span class="ltx_text ltx_font_typewriter ltx_font_small">elevations</span> and <span class="ltx_text ltx_font_typewriter ltx_font_small">heights</span>, or <span class="ltx_text ltx_font_typewriter ltx_font_small">USA</span> and <span class="ltx_text ltx_font_typewriter ltx_font_small">US</span>, can be mapped into the same predicate in the semantic parse, thus allowing to receive positive feedback from parse execution against the geographical database.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Response-based Online Learning</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Recent approaches to machine learning for SMT formalize the task of discriminating good from bad translations as a structured prediction problem.
Assume a joint feature representation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m1" class="ltx_Math" alttext="\phi(x,y)" display="inline"><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></math> of input sentences <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m2" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> and output translations <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m3" class="ltx_Math" alttext="y\in Y(x)" display="inline"><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></math>, and a linear scoring function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m4" class="ltx_Math" alttext="s(x,y;w)" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></math> for predicting a translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m5" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math> (where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m6" class="ltx_Math" alttext="\left&lt;\cdot,\cdot\right&gt;" display="inline"><mrow><mo>⟨</mo><mo>⋅</mo><mo>,</mo><mo>⋅</mo><mo>⟩</mo></mrow></math> denotes the standard vector dot product) s.t.</p>
<table id="S4.Ex1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex1.m1" class="ltx_Math" alttext="\hat{y}=\operatorname*{arg\,max}_{y\in Y(x)}\;\textrm{s}(x,y;w)=\operatorname*%&#10;{arg\,max}_{y\in Y(x)}\left&lt;w,\phi(x,y)\right&gt;." display="block"><mrow><mrow><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mo>=</mo><mrow><mrow><mpadded width="+2.8pt"><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></msub></mpadded><mo>⁡</mo><mtext>s</mtext></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mrow></msub><mo>⁡</mo><mrow><mo>⟨</mo><mrow><mi>w</mi><mo>,</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>⟩</mo></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The structured perceptron algorithm <cite class="ltx_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Collins2002</a>]</cite> learns an optimal weight vector <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m7" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> by updating <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m8" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> on input <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m9" class="ltx_Math" alttext="x^{(i)}" display="inline"><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> by the following rule, in case the predicted translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m10" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math> is different from and scored higher than the reference translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m11" class="ltx_Math" alttext="y^{(i)}" display="inline"><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>:</p>
<table id="S4.Ex2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex2.m1" class="ltx_Math" alttext="w=w+\phi(x^{(i)},y^{(i)})-\phi(x^{(i)},\hat{y})." display="block"><mrow><mrow><mi>w</mi><mo>=</mo><mrow><mi>w</mi><mo>+</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">This stochastic structural update aims to demote weights of features corresponding to incorrect decisions, and to promote weights of features for correct decisions.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">An application of structured prediction to SMT involves more than a straightforward replacement of labeled output structures by reference translations. Firstly, update rules that require to compute a feature representation for the reference translation are suboptimal in SMT, because often human-generated reference translations cannot be generated by the SMT system. Such “unreachable” gold-standard translations need to be replaced by “surrogate” gold-standard translations that are close to the human-generated translations and still lie within the reach of the SMT system. Computation of distance to the reference translation usually involves cost functions based on sentence-level BLEU (<cite class="ltx_cite"><a href="#bib.bibx25" title="" class="ltx_ref">Nakov et al.2012</a></cite>, <em class="ltx_emph">inter alia</em>) and incorporates the current model score, leading to various ramp loss objectives described in <cite class="ltx_cite"><a href="#bib.bibx14" title="" class="ltx_ref">Gimpel and Smith2012</a></cite>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">An alternative approach to alleviate the dependency on labeled training data is response-based learning. <cite class="ltx_cite"><a href="#bib.bibx8" title="" class="ltx_ref">Clarke et al.2010</a></cite> or <cite class="ltx_cite"><a href="#bib.bibx15" title="" class="ltx_ref">Goldwasser and Roth2013</a></cite> describe a response-driven learning framework for the area of semantic parsing: Here a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Applied to SMT, this means that we predict translations and use positive response from acting in the world to create “surrogate” gold-standard translations. This decreases the dependency on a few (mostly only one) reference translations and guides the learner to promote translations that perform well with respect to the extrinsic task.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">In the following, we will present a framework that combines standard structured learning from given reference translations with response-based learning from task-approved references. We need to ensure that gold-standard translations lead to positive task-based feedback, that means they can be parsed and executed successfully against the database. In addition, we can use translation-specific cost functions based on sentence-level BLEU in order to boost similarity of translations to human reference translations.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">We denote feedback by a binary execution function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m1" class="ltx_Math" alttext="\textrm{e}(y)\in\{1,0\}" display="inline"><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow></math> that tests whether executing the semantic parse for the prediction against the database receives the same answer as the parse for the gold standard reference. Our cost function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m2" class="ltx_Math" alttext="\textrm{c}(y^{(i)},y)=(1-\textrm{BLEU}(y^{(i)},y))" display="inline"><mrow><mrow><mtext>c</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mrow><mtext>BLEU</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></math> is based on a version of sentence-level BLEU <cite class="ltx_cite"><a href="#bib.bibx25" title="" class="ltx_ref">Nakov et al.2012</a></cite>. Define <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m3" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> as a surrogate gold-standard translation that receives positive feedback, has a high model score, and a low cost of predicting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m4" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> instead of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m5" class="ltx_Math" alttext="y^{(i)}" display="inline"><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>:</p>
<table id="S4.Ex3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex3.m1" class="ltx_Math" alttext="y^{+}=\operatorname*{arg\,max}_{y\in Y(x^{(i)}):\textrm{e}(y)=1}\left(\textrm{%&#10;s}(x^{(i)},y;w)-\textrm{c}(y^{(i)},y)\right)." display="block"><mrow><mrow><msup><mi>y</mi><mo>+</mo></msup><mo>=</mo><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></msub><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mtext>s</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mtext>c</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The opposite of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m6" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> is the translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m7" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math> that leads to negative feedback, has a high model score, and a high cost. It is defined as follows:</p>
<table id="S4.Ex4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex4.m1" class="ltx_Math" alttext="y^{-}=\operatorname*{arg\,max}_{y\in Y(x^{(i)}):\textrm{e}(y)=0}\left(\textrm{%&#10;s}(x^{(i)},y;w)+\textrm{c}(y^{(i)},y)\right)." display="block"><mrow><mrow><msup><mi>y</mi><mo>-</mo></msup><mo>=</mo><mrow><msub><mrow><mpadded width="+1.7pt"><mi>arg</mi></mpadded><mo>⁢</mo><mi>max</mi></mrow><mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></msub><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mtext>s</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mtext>c</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Update rules can be derived by minimization of the following ramp loss objective:</p>
<table id="S4.Ex5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex5.m1" class="ltx_Math" alttext="\begin{split}\min_{w}\left(-\max_{y\in Y(x^{(i)}):\textrm{e}(y)=1}\left(%&#10;\textrm{s}(x^{(i)},y;w)-\textrm{c}(y^{(i)},y)\right)\right.\\&#10;\left.+\max_{y\in Y(x^{(i)}):\textrm{e}(y)=0}\left(\textrm{s}(x^{(i)},y;w)+%&#10;\textrm{c}(y^{(i)},y)\right)\right).\end{split}" display="block"><mrow><mrow><munder><mo movablelimits="false">min</mo><mi>w</mi></munder><mo>⁡</mo><mrow><mo>(</mo><mrow><mo>-</mo><mrow><munder><mo movablelimits="false">max</mo><mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></mrow></munder><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mtext>s</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mtext>c</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><munder><mo movablelimits="false">max</mo><mrow><mrow><mi>y</mi><mo>∈</mo><mrow><mi>Y</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><mo>:</mo><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow><mo>=</mo><mn>0</mn></mrow></mrow></munder><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mtext>s</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi><mo>;</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mtext>c</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Minimization of this objective using stochastic (sub)gradient descent <cite class="ltx_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">McAllester and Keshet2011</a>]</cite> yields the following update rule:</p>
<table id="S4.Ex6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex6.m1" class="ltx_Math" alttext="w=w+\phi(x^{(i)},y^{+})-\phi(x^{(i)},y^{-})." display="block"><mrow><mrow><mi>w</mi><mo>=</mo><mrow><mi>w</mi><mo>+</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>y</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>y</mi><mo>-</mo></msup></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">The intuition behind this update rule is to discriminate the translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m8" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> that leads to positive feedback and best approximates (or is identical to) the reference within the means of the model from a translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m9" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math> which is favored by the model but does not execute and has high cost. This is done by putting all the weight on the former.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">Algorithm 1 presents pseudo-code for our response-driven learning scenario. Upon predicting translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m1" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math>, in case of positive feedback from the task, we treat the prediction as surrogate reference by setting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m2" class="ltx_Math" alttext="y^{+}\leftarrow\hat{y}" display="inline"><mrow><msup><mi>y</mi><mo>+</mo></msup><mo>←</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow></math>, and by adding it to the set of reference translations for future use. Then we need to compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m3" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math>, and update by the difference in feature representations of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m4" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m5" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math>, at a learning rate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m6" class="ltx_Math" alttext="\eta" display="inline"><mi>η</mi></math>. If the feedback is negative, we want to move the weights away from the prediction, thus we treat it as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m7" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math>. To perform an update, we need to compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m8" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math>. If either <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m9" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m10" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math> cannot be computed, the example is skipped.</p>
</div>
<div id="S4.T1" class="ltx_table"><span class="ltx_ERROR undefined">{algorithm}</span>
<p class="ltx_p">[H]</p>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Response-based Online Learning</div><span class="ltx_ERROR undefined">{algorithmic}</span>
<p class="ltx_p">[0]
<span class="ltx_ERROR undefined">\Repeat</span><span class="ltx_ERROR undefined">\For</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m1" class="ltx_Math" alttext="i=1,\ldots,n" display="inline"><mrow><mi>i</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>n</mi></mrow></mrow></math>
<span class="ltx_ERROR undefined">\State</span>Receive input string <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m2" class="ltx_Math" alttext="x^{(i)}" display="inline"><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
<span class="ltx_ERROR undefined">\State</span>Predict translation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m3" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math>
<span class="ltx_ERROR undefined">\State</span>Receive task feedback <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m4" class="ltx_Math" alttext="\textrm{e}(\hat{y})\in\{1,0\}" display="inline"><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>∈</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow></math>
<span class="ltx_ERROR undefined">\If</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m5" class="ltx_Math" alttext="\textrm{e}(\hat{y})=1" display="inline"><mrow><mrow><mtext>e</mtext><mo>⁢</mo><mrow><mo>(</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mo>)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></math>
<span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m6" class="ltx_Math" alttext="y^{+}\leftarrow\hat{y}" display="inline"><mrow><msup><mi>y</mi><mo>+</mo></msup><mo>←</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow></math>
<span class="ltx_ERROR undefined">\State</span>Store <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m7" class="ltx_Math" alttext="\hat{y}" display="inline"><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></math> as reference <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m8" class="ltx_Math" alttext="y^{(i)}" display="inline"><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m9" class="ltx_Math" alttext="x^{(i)}" display="inline"><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
<span class="ltx_ERROR undefined">\State</span>Compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m10" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math>
<span class="ltx_ERROR undefined">\Else</span><span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m11" class="ltx_Math" alttext="y^{-}\leftarrow\hat{y}" display="inline"><mrow><msup><mi>y</mi><mo>-</mo></msup><mo>←</mo><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover></mrow></math>
<span class="ltx_ERROR undefined">\State</span>Receive reference <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m12" class="ltx_Math" alttext="y^{(i)}" display="inline"><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
<span class="ltx_ERROR undefined">\State</span>Compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m13" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math>
<span class="ltx_ERROR undefined">\EndIf</span><span class="ltx_ERROR undefined">\State</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T1.m14" class="ltx_Math" alttext="w\leftarrow w+\eta(\phi(x^{(i)},y^{+})-\phi(x^{(i)},y^{-}))" display="inline"><mrow><mi>w</mi><mo>←</mo><mrow><mi>w</mi><mo>+</mo><mrow><mi>η</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>y</mi><mo>+</mo></msup></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>,</mo><msup><mi>y</mi><mo>-</mo></msup></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>
<span class="ltx_ERROR undefined">\EndFor</span><span class="ltx_ERROR undefined">\Until</span>Convergence</p>
</div>
<div id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"></th>
<th class="ltx_td ltx_align_left ltx_border_tt">method</th>
<th class="ltx_td ltx_align_left ltx_border_tt">precision</th>
<th class="ltx_td ltx_align_left ltx_border_tt">recall</th>
<th class="ltx_td ltx_align_left ltx_border_tt">F1</th>
<th class="ltx_td ltx_align_left ltx_border_tt">BLEU</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">1</span></th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps">cdec</span></td>
<td class="ltx_td ltx_align_left ltx_border_t">63.67</td>
<td class="ltx_td ltx_align_left ltx_border_t">58.21</td>
<td class="ltx_td ltx_align_left ltx_border_t">60.82</td>
<td class="ltx_td ltx_align_left ltx_border_t">46.53</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">2</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Exec</span></td>
<td class="ltx_td ltx_align_left">70.36</td>
<td class="ltx_td ltx_align_left">63.57</td>
<td class="ltx_td ltx_align_left">66.79<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m1" class="ltx_Math" alttext="{}^{1}" display="inline"><msup><mi/><mn>1</mn></msup></math></td>
<td class="ltx_td ltx_align_left">48.00<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m2" class="ltx_Math" alttext="{}^{1}" display="inline"><msup><mi/><mn>1</mn></msup></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">3</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Rampion</span></td>
<td class="ltx_td ltx_align_left">75.58</td>
<td class="ltx_td ltx_align_left">69.64</td>
<td class="ltx_td ltx_align_left">72.49<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m3" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn>12</mn></msup></math></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">56.64<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m4" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn mathvariant="normal">12</mn></msup></math></span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_bb"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">4</span></th>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_smallcaps">Rebol</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">81.15</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">75.36</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">78.15<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m5" class="ltx_Math" alttext="{}^{123}" display="inline"><msup><mi/><mn mathvariant="normal">123</mn></msup></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb">55.66<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m6" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn>12</mn></msup></math></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental results using extended parser for returning answers from <span class="ltx_text ltx_font_smallcaps">Geoquery</span> (precision, recall, F1) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m9" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram match to original English query (BLEU) on 280 re-translated test examples. Best results for each column are highlighted in <span class="ltx_text ltx_font_bold">bold face</span>. Superscripts <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m10" class="ltx_Math" alttext="{}^{1234}" display="inline"><msup><mi/><mn>1234</mn></msup></math> denote a significant improvement over the respective method.</div>
</div>
<div id="S4.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"></th>
<th class="ltx_td ltx_align_left ltx_border_tt">method</th>
<th class="ltx_td ltx_align_left ltx_border_tt">precision</th>
<th class="ltx_td ltx_align_left ltx_border_tt">recall</th>
<th class="ltx_td ltx_align_left ltx_border_tt">F1</th>
<th class="ltx_td ltx_align_left ltx_border_tt">BLEU</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">1</span></th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_smallcaps">cdec</span></td>
<td class="ltx_td ltx_align_left ltx_border_t">65.59</td>
<td class="ltx_td ltx_align_left ltx_border_t">57.86</td>
<td class="ltx_td ltx_align_left ltx_border_t">61.48</td>
<td class="ltx_td ltx_align_left ltx_border_t">46.53</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">2</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Exec</span></td>
<td class="ltx_td ltx_align_left">66.54</td>
<td class="ltx_td ltx_align_left">61.79</td>
<td class="ltx_td ltx_align_left">64.07</td>
<td class="ltx_td ltx_align_left">46.00</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">3</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_smallcaps">Rampion</span></td>
<td class="ltx_td ltx_align_left">67.68</td>
<td class="ltx_td ltx_align_left">63.57</td>
<td class="ltx_td ltx_align_left">65.56</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">55.67<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T3.m1" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn mathvariant="normal">12</mn></msup></math></span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_bb"> <span class="ltx_text ltx_font_script" style="position:relative; bottom:0.1pt;">4</span></th>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_smallcaps">Rebol</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">70.68</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">67.14</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">68.86<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T3.m2" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn mathvariant="normal">12</mn></msup></math></span></td>
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">55.67<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T3.m3" class="ltx_Math" alttext="{}^{12}" display="inline"><msup><mi/><mn mathvariant="normal">12</mn></msup></math></span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental results using the original parser for returning answers from <span class="ltx_text ltx_font_smallcaps">Geoquery</span> (precision, recall, F1) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T3.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram match to original English query (BLEU) on 280 re-translated test examples.</div>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p">The sketched algorithm allows several variations. In the form depicted above, it allows to use human reference translations in addition to task-approved surrogate references. The cost function can be implemented by different versions of sentence-wise BLEU, or it can be omitted completely so that learning relies on task-based feedback alone, similar to algorithms recently suggested for semantic parsing <cite class="ltx_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Goldwasser and Roth2013</a>, <a href="#bib.bibx20" title="" class="ltx_ref">Kwiatowski et al.2013</a>, <a href="#bib.bibx3" title="" class="ltx_ref">Berant et al.2013</a>]</cite>. Lastly, regularization can be introduced by using update rules corresponding to primal form optimization variants of support vector machines <cite class="ltx_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Collobert and Bengio2004</a>, <a href="#bib.bibx6" title="" class="ltx_ref">Chapelle2007</a>, <a href="#bib.bibx33" title="" class="ltx_ref">Shalev-Shwartz et al.2007</a>]</cite>.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt">prediction:</td>
<td class="ltx_td ltx_align_left ltx_border_tt">how many inhabitants has new york</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">how many people live in new york</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">how big is the population of texas</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">how many people live in texas</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">which are the cities of the state with the highest elevation</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">what are the cities of the state with the highest point</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">how big is the population of states , through which the mississippi runs</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">what are the populations of the states through which the mississippi river runs</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">what state borders california</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">what is the adjacent state of california</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">what are the capitals of the states which have cities with the name durham</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">reference:</td>
<td class="ltx_td ltx_align_left">what is the capital of states that have cities named durham</td></tr>
<tr class="ltx_tr">
<td class="ltx_td"/>
<td class="ltx_td"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">prediction:</td>
<td class="ltx_td ltx_align_left">what rivers go through states with the least cities</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">reference:</td>
<td class="ltx_td ltx_align_left ltx_border_bb">which rivers run through states with fewest cities</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Predicted translations by response-based learning (<span class="ltx_text ltx_font_smallcaps">Rebol</span>) leading to positive feedback versus gold standard references.</div>
</div>
<div id="S5.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt">reference</th>
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt"><span class="ltx_text ltx_font_smallcaps">Rampion</span></th>
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt"><span class="ltx_text ltx_font_smallcaps">Rebol</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many colorado rivers are there</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many rivers with the name colorado gives it</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many rivers named colorado are there</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what are the populations of states which border texas</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">how big are the populations of the states , which in texas borders</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">how big are the populations of the states which on texas border</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is the biggest capital city in the us</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is the largest city in the usa</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is the largest capital in the usa</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what state borders new york</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what states limits of new york</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what states border new york</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">which states border the state with the smallest area</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">what states boundaries of the state with the smallest surface area</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">what states border the state with the smallest surface area</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Predicted translations by response-based learning (<span class="ltx_text ltx_font_smallcaps">Rebol</span>) leading to positive feedback versus translations by supervised structured learning (<span class="ltx_text ltx_font_smallcaps">Rampion</span>) leading to negative feedback.</div>
</div>
<div id="S5.T6" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt">reference</th>
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt"><span class="ltx_text ltx_font_smallcaps">Rampion</span></th>
<th class="ltx_td ltx_align_justify ltx_border_tt" style="width:142.3pt;" width="142.3pt"><span class="ltx_text ltx_font_smallcaps">Rebol</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many states have a higher point than the highest point of the state with the largest capital city in the us</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many states have a higher nearby point as the highest point of the state with the largest capital in the usa</td>
<td class="ltx_td ltx_align_justify ltx_border_t" style="width:142.3pt;" width="142.3pt">how many states have a high point than the highest point of the state with the largest capital in the usa</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">how tall is mount mckinley</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">how high is mount mckinley</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is mount mckinley</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is the longest river that flows through a state that borders indiana</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">how is the longest river , which runs through a state , borders the of indiana</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what is the longest river which runs through a state of indiana borders</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">what states does the mississippi river run through</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">through which states runs the mississippi</td>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt">through which states is the mississippi</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/>
<td class="ltx_td ltx_align_justify" style="width:142.3pt;" width="142.3pt"/></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">which is the highest peak not in alaska</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">how is the highest peaks of not in alaska is</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" style="width:142.3pt;" width="142.3pt">what is the highest peak in alaska is</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Predicted translations where supervised structured learning (<span class="ltx_text ltx_font_smallcaps">Rampion</span>) leads to positive feedback versus translations by response-based learning (<span class="ltx_text ltx_font_smallcaps">Rebol</span>) leading to negative feedback.</div>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Setup</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">In our experiments, we use the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database on U.S. geography as provided by <cite class="ltx_cite"><a href="#bib.bibx18" title="" class="ltx_ref">Jones et al.2012</a></cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><a href="http://homepages.inf.ed.ac.uk/s1051107/geoquery-2012-08-27.zip" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://homepages.inf.ed.ac.uk/s1051107/geoquery-2012-08-27.zip</span></a></span></span></span> The dataset includes 880 English questions and their logical forms. The English strings were manually translated into German by the authors of <cite class="ltx_cite"><a href="#bib.bibx18" title="" class="ltx_ref">Jones et al.2012</a></cite>), and corrected for typos by the authors of this paper. We follow the provided split into 600 training examples and 280 test examples.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">For response-based learning, we retrained the semantic parser of <cite class="ltx_cite"><a href="#bib.bibx1" title="" class="ltx_ref">Andreas et al.2013</a></cite><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="https://github.com/jacobandreas/smt-semparse" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/jacobandreas/smt-semparse</span></a></span></span></span> on the full 880 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> examples in order to reach full parse coverage. This parser is itself based on SMT, trained on parallel data consisting of English queries and linearized logical forms, and on a language model trained on linearized logical forms. We used the hierarchical phrase-based variant of the parser.
Note that we do not use <span class="ltx_text ltx_font_smallcaps">Geoquery</span> test data in SMT training. Parser training includes <span class="ltx_text ltx_font_smallcaps">Geoquery</span> test data in order to be less dependent on parse and execution failures in the evaluation: If a translation system, response-based or reference-based, translates the German input into the gold standard English query it should be rewarded by positive task feedback. To double-check whether including the 280 test examples in parser training gives an unfair advantage to response-based learning, we also present experimental results using the original parser of <cite class="ltx_cite"><a href="#bib.bibx1" title="" class="ltx_ref">Andreas et al.2013</a></cite> that is trained only on the 600 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> training examples.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">The bilingual SMT system used in our experiments is the state-of-the-art SCFG decoder <span class="ltx_text ltx_font_smallcaps">cdec</span> <cite class="ltx_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Dyer et al.2010</a>]</cite><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><a href="https://github.com/redpony/cdec" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/redpony/cdec</span></a></span></span></span>. We built grammars using its implementation of the suffix array extraction method described in <cite class="ltx_cite"><a href="#bib.bibx22" title="" class="ltx_ref">Lopez2007</a></cite>. For language modeling, we built a modified Kneser-Ney smoothed 5-gram language model using the English side of the training data.
We trained the SMT system on the English-German parallel web data provided in the <span class="ltx_text ltx_font_smallcaps">Common Crawl<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><a href="http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter ltx_font_upright">http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz</span></a></span></span></span></span> <cite class="ltx_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Smith et al.2013</a>]</cite> dataset.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Compared Systems</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">Method 1 is the baseline system, consisting of the <span class="ltx_text ltx_font_smallcaps">cdec</span> SMT system trained on the <span class="ltx_text ltx_font_smallcaps">Common Crawl</span> data as described above. This system does not use any <span class="ltx_text ltx_font_smallcaps">Geoquery</span> data for training. Methods 2-4 use the 600 training examples from <span class="ltx_text ltx_font_smallcaps">Geoquery</span> for discriminative training only.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">Variants of the response-based learning algorithm described above are implemented as a stand-alone tool that operates on <span class="ltx_text ltx_font_smallcaps">cdec</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-best lists of 10,000 translations of the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> training data. All variants use sparse features of <span class="ltx_text ltx_font_smallcaps">cdec</span> as described in <cite class="ltx_cite"><a href="#bib.bibx34" title="" class="ltx_ref">Simianer et al.2012</a></cite> that extract rule shapes, rule identifiers, and bigrams in rule source and target directly from grammar rules. Method 4, named <span class="ltx_text ltx_font_smallcaps">Rebol</span>, implements REsponse-Based Online Learning by instantiating <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math> to the form described in Section <a href="#S4" title="4 Response-based Online Learning ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>: In addition to the model score <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m4" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>, it uses a cost function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m5" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> based on sentence-level BLEU <cite class="ltx_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Nakov et al.2012</a>]</cite> and tests translation hypotheses for task-based feedback using a binary execution function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m6" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math>. This algorithm can convert predicted translations into references by task-feedback, and additionally use the given original English queries as references. Method 2, named <span class="ltx_text ltx_font_smallcaps">Exec</span>, relies on task-execution by function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m7" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> and searches for executable or non-executable translations with highest score <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m8" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> to distinguish positive from negative training examples. It does not use a cost function and thus cannot make use of the original English queries.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">We compare response-based learning with a standard structured prediction setup that omits the use of the execution function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m1" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> in the definition of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m2" class="ltx_Math" alttext="y^{+}" display="inline"><msup><mi>y</mi><mo>+</mo></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m3" class="ltx_Math" alttext="y^{-}" display="inline"><msup><mi>y</mi><mo>-</mo></msup></math>. This algorithm can be seen as a stochastic (sub)gradient descent variant of <span class="ltx_text ltx_font_smallcaps">Rampion</span> <cite class="ltx_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Gimpel and Smith2012</a>]</cite>. It does not make use of the semantic parser, but defines positive and negative examples based on score <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m4" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> and cost <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m5" class="ltx_Math" alttext="c" display="inline"><mi>c</mi></math> with respect to human reference translations.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">We report BLEU <cite class="ltx_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Papineni et al.2001</a>]</cite> of translation system output measured against the original English queries. Furthermore, we report precision, recall, and F1-score for executing semantic parses built from translation system outputs against the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database. Precision is defined as the percentage of correctly answered examples out of those for which a parse could be produced; recall is defined as the percentage of total examples answered correctly; F1-score is the harmonic mean of both. Statistical significance is measured using Approximate Randomization <cite class="ltx_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Noreen1989</a>]</cite> where result differences with a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>-value smaller than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m2" class="ltx_Math" alttext="0.05" display="inline"><mn>0.05</mn></math> are considered statistically significant.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p class="ltx_p">Methods 2-4 perform structured learning for SMT on the 600 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> training examples and re-translate the 280 unseen <span class="ltx_text ltx_font_smallcaps">Geoquery</span> test data, following the data split of <cite class="ltx_cite"><a href="#bib.bibx18" title="" class="ltx_ref">Jones et al.2012</a></cite>. Training for <span class="ltx_text ltx_font_smallcaps">Rampion</span>, <span class="ltx_text ltx_font_smallcaps">Rebol</span> and <span class="ltx_text ltx_font_smallcaps">Exec</span> was repeated for 10 epochs. The learning rate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p5.m1" class="ltx_Math" alttext="\eta" display="inline"><mi>η</mi></math> is set to a constant that is adjusted by cross-validation on the 600 training examples.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Empirical Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">We present an experimental comparison of the four different systems according to BLEU and F1, using an extended semantic parser (trained on 880 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> examples) and the original parser (trained on 600 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> training examples). The extended parser reaches and F1-score of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m1" class="ltx_Math" alttext="99.64\%" display="inline"><mrow><mn>99.64</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> on the 280 <span class="ltx_text ltx_font_smallcaps">Geoquery</span> test examples; the original parser yields an F1-score of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p1.m2" class="ltx_Math" alttext="82.76\%" display="inline"><mrow><mn>82.76</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4 Response-based Online Learning ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports results for the extended semantic parser. A system ranking according to F1-score shows about 6 points difference between the respective methods, ranking <span class="ltx_text ltx_font_smallcaps">Rebol</span> over <span class="ltx_text ltx_font_smallcaps">Rampion</span>, <span class="ltx_text ltx_font_smallcaps">Exec</span> and <span class="ltx_text ltx_font_smallcaps">cdec</span>. The exploitation of task-feedback allows both <span class="ltx_text ltx_font_smallcaps">Exec</span> and <span class="ltx_text ltx_font_smallcaps">Rebol</span> to improve task-performance over the baseline. <span class="ltx_text ltx_font_smallcaps">Rebol</span>’s combination of task feedback with a cost function achieves the best results since positively executable hypotheses and reference translations can both be exploited to guide the learning process. Since all English reference queries lead to positively executable parses in the setup that uses the extended semantic parser, <span class="ltx_text ltx_font_smallcaps">Rampion</span> implicitly also has access to task feedback. This allows <span class="ltx_text ltx_font_smallcaps">Rampion</span> to improve F1 over the baseline. All result differences are statistically significant.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p">In terms of BLEU score measured against the original English <span class="ltx_text ltx_font_smallcaps">Geoquery</span> queries, the best nominal result is obtained by <span class="ltx_text ltx_font_smallcaps">Rampion</span> which uses them as reference translations. <span class="ltx_text ltx_font_smallcaps">Rebol</span> performs worse since BLEU performance is optimized only implicitly in cases where original English queries function as positive examples. However, the result differences between these two systems do not score as statistically significant. Despite not optimizing for BLEU performance against references, the fact that positively executable translations include the references allows even <span class="ltx_text ltx_font_smallcaps">Exec</span> to improve BLEU over <span class="ltx_text ltx_font_smallcaps">cdec</span> which does not use <span class="ltx_text ltx_font_smallcaps">Geoquery</span> data at all in training. This result difference is statistically significant.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4 Response-based Online Learning ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares the same systems using the original parser trained on 600 training examples. The system ranking according to F1-score shows the same ordering that is obtained when using an extended semantic parser. However, the respective methods are separated only by 3 or less points in F1 score such that only the result difference of <span class="ltx_text ltx_font_smallcaps">Rebol</span> over the baseline <span class="ltx_text ltx_font_smallcaps">cdec</span> and over <span class="ltx_text ltx_font_smallcaps">Exec</span> is statistically significant. We conjecture that this is due to a higher number of empty parses on the test set which makes this comparison unstable.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p class="ltx_p">In terms of BLEU measured against the original queries, the result differences between <span class="ltx_text ltx_font_smallcaps">Rebol</span> and <span class="ltx_text ltx_font_smallcaps">Rampion</span> are not statistically significant, and neither are the result differences between <span class="ltx_text ltx_font_smallcaps">Exec</span> and <span class="ltx_text ltx_font_smallcaps">cdec</span>. The result differences between systems of the former group and the systems of latter group are statistically significant.</p>
</div>
</div>
<div id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.4 </span>Error Analysis</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p class="ltx_p">For a better understanding of the differences between the results produced by supervised and response-based learning, we conducted an error analysis on the test examples. Table <a href="#S5.T4" title="Table 4 ‣ 5 Experiments ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows examples where the translation predicted by response-based learning (<span class="ltx_text ltx_font_smallcaps">Rebol</span>) differs from the gold standard reference translation, but yet leads to positive feedback via a parse that returns the correct answer from the database. The examples show structural and lexical variation that leads to differences on the string level at equivalent positive feedback from the extrinsic task. This can explain the success of response-based learning: Lexical and structural variants of reference translations can be used to boost model parameters towards translations with positive feedback, while the same translations might be considered as negative examples in standard structured learning.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5 Experiments ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows examples where translations from <span class="ltx_text ltx_font_smallcaps">Rebol</span> and <span class="ltx_text ltx_font_smallcaps">Rampion</span> differ from the gold standard reference, and predictions by <span class="ltx_text ltx_font_smallcaps">Rebol</span> lead to positive feedback, while predictions by <span class="ltx_text ltx_font_smallcaps">Rampion</span> lead to negative feedback. Table <a href="#S5.T6" title="Table 6 ‣ 5 Experiments ‣ Response-based Learning for Grounded Machine Translation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows examples where translations from <span class="ltx_text ltx_font_smallcaps">Rampion</span> outperform translations from <span class="ltx_text ltx_font_smallcaps">Rebol</span> in terms of task feedback.
We see that predictions from both systems are in general grammatical. This can be attributed to the use of sentence-level BLEU as cost function in <span class="ltx_text ltx_font_smallcaps">Rampion</span> and <span class="ltx_text ltx_font_smallcaps">Rebol</span>. Translation errors of <span class="ltx_text ltx_font_smallcaps">Rampion</span> can be traced back to mistranslations of key terms (<span class="ltx_text ltx_font_typewriter ltx_font_small">city</span> versus <span class="ltx_text ltx_font_typewriter ltx_font_small">capital</span>, <span class="ltx_text ltx_font_typewriter ltx_font_small">limits</span> or <span class="ltx_text ltx_font_typewriter ltx_font_small">boundaries</span> versus <span class="ltx_text ltx_font_typewriter ltx_font_small">border</span>). Translation errors of <span class="ltx_text ltx_font_smallcaps">Rebol</span> more frequently show missing translations of terms.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We presented a proposal for a new learning and evaluation framework for SMT. The central idea is to ground meaning transfer in successful interaction in an extrinsic task, and use task-based feedback for structured learning. We presented a proof-of-concept experiment that defines the extrinsic task as executing semantic parses of translated queries against the <span class="ltx_text ltx_font_smallcaps">Geoquery</span> database. Our experiments show an improvement of about 6 points in F1-score for response-based learning over structured learning from reference translations. Our error analysis shows that response-based learning generates grammatical translations which is due to the additional use of a cost function that boosts similarity of translations to human reference translations.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">In future work, we would like to extend our work on embedding SMT in virtual gameplay to larger and more diverse datasets, and involve human feedback in the response-based learning loop.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Andreas et al.2013</span>
<span class="ltx_bibblock">
Jacob Andreas, Andreas Vlachos, and Stephen Clark.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Semantic parsing as machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL’13)</span>, Sofia, Bulgaria.

</span></li>
<li id="bib.bibx2" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Barrachina et al.2008</span>
<span class="ltx_bibblock">
Sergio Barrachina, Oliver Bender, Francisco Casacuberta, Jorge Civera, Elsa
Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jesús Tomás,
Enrique Vidal, and Juan-Miguel Vilar.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Statistical approaches to computer-assisted translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Computational Linguistics</span>, 35(1):3–28.

</span></li>
<li id="bib.bibx3" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Berant et al.2013</span>
<span class="ltx_bibblock">
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Semantic parsing on freebase from question-answer pairs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP’13)</span>, Seattle, WA.

</span></li>
<li id="bib.bibx4" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cai and Yates2013</span>
<span class="ltx_bibblock">
Qingqing Cai and Alexander Yates.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Large-scale semantic parsing via schema matching and lexicon
extenstion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL’13)</span>, Sofia, Bulgaria.

</span></li>
<li id="bib.bibx5" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Cesa-Bianchi et al.2008</span>
<span class="ltx_bibblock">
Nicolò Cesa-Bianchi, Gabriele Reverberi, and Sandor Szedmak.

</span>
<span class="ltx_bibblock">2008.

</span>
<span class="ltx_bibblock">Online learning algorithms for computer-assisted translation.

</span>
<span class="ltx_bibblock">Technical report, SMART (<span class="ltx_text ltx_font_typewriter">www.smart-project.eu</span>).

</span></li>
<li id="bib.bibx6" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Chapelle2007</span>
<span class="ltx_bibblock">
Olivier Chapelle.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock">Training a support vector machine in the primal.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Neural Computation</span>, 19(5):1155–1178.

</span></li>
<li id="bib.bibx7" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Chen and Mooney2011</span>
<span class="ltx_bibblock">
David L. Chen and Raymond J. Mooney.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Learning to interpret natural language navigation instructions from
observations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 25th AAAI Conference on Artificial
Intelligence (AAAI’11)</span>, pages 859–866, San Francisco, CA.

</span></li>
<li id="bib.bibx8" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Clarke et al.2010</span>
<span class="ltx_bibblock">
James Clarke, Dan Goldwasser, Wing-Wei Chang, and Dan Roth.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">Driving semantic parsing from the world’s response.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 14th Conference on Natural Language
Learning (CoNLL’10)</span>, pages 18–27, Uppsala, Sweden.

</span></li>
<li id="bib.bibx9" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Collins2002</span>
<span class="ltx_bibblock">
Michael Collins.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Discriminative training methods for hidden markov models: theory and
experiments with perceptron algorithms.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the conference on Empirical Methods in Natural
Language Processing (EMNLP’02)</span>, Philadelphia, PA.

</span></li>
<li id="bib.bibx10" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Collobert and Bengio2004</span>
<span class="ltx_bibblock">
Ronan Collobert and Samy Bengio.

</span>
<span class="ltx_bibblock">2004.

</span>
<span class="ltx_bibblock">Links between perceptrons, MLPs, and SVMs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 21st International Conference on Machine
Learning (ICML’04)</span>, Banff, Canada.

</span></li>
<li id="bib.bibx11" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Denkowski et al.2014</span>
<span class="ltx_bibblock">
Michael Denkowski, Chris Dyer, and Alon Lavie.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Learning from post-editing: Online model adaptation for statistical
machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 14th Conference of the European Chapter of
the Association for Computational Linguistics (EACL’14)</span>, Gothenburg,
Sweden.

</span></li>
<li id="bib.bibx12" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Dyer et al.2010</span>
<span class="ltx_bibblock">
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil
Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">cdec: A decoder, alignment, and learning framework for finite-state
and context-free translation models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the ACL 2010 System Demonstrations</span>,
Uppsala, Sweden.

</span></li>
<li id="bib.bibx13" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Fuji1999</span>
<span class="ltx_bibblock">
Masaru Fuji.

</span>
<span class="ltx_bibblock">1999.

</span>
<span class="ltx_bibblock">Evaluation experiment for reading comprehension of machine
translation outputs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Machine Translation Summit VII</span>,
Singapore.

</span></li>
<li id="bib.bibx14" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Gimpel and Smith2012</span>
<span class="ltx_bibblock">
Kevin Gimpel and Noah A. Smith.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Structured ramp loss minimization for machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies
(NAACL-HLT 2012)</span>, Montreal, Canada.

</span></li>
<li id="bib.bibx15" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Goldwasser and Roth2013</span>
<span class="ltx_bibblock">
Dan Goldwasser and Dan Roth.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Learning from natural instructions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Machine Learning</span>, 94(2):205–232.

</span></li>
<li id="bib.bibx16" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Hardt and Elming2010</span>
<span class="ltx_bibblock">
Daniel Hardt and Jakob Elming.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">Incremental re-training for post-editing SMT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 9th Conference of the Association for
Machine Tranlation in the Americas (AMTA’10)</span>, Denver, CO.

</span></li>
<li id="bib.bibx17" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Jones et al.2005</span>
<span class="ltx_bibblock">
Douglas Jones, Wade Shen, Neil Granoien, Martha Herzog, and Clifford Weinstein.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Measuring translation quality by testing english speakers with a new
defense language proficiency test for arabic.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of 2005 International Conference on Intelligence
Analysis</span>, McLean, VA.

</span></li>
<li id="bib.bibx18" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Jones et al.2012</span>
<span class="ltx_bibblock">
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Semantic parsing with bayesion tree transducers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL’12)</span>, Jeju Island, Korea.

</span></li>
<li id="bib.bibx19" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Koehn and Haddow2009</span>
<span class="ltx_bibblock">
Philipp Koehn and Barry Haddow.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">Interactive assistance to human translators using statistical machine
translation methods.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of MT Summit XII</span>, Ottawa, Ontario, Canada.

</span></li>
<li id="bib.bibx20" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Kwiatowski et al.2013</span>
<span class="ltx_bibblock">
Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Scaling semantic parsers with on-the-fly ontology matching.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP’13)</span>, Seattle, WA.

</span></li>
<li id="bib.bibx21" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Langlais et al.2000</span>
<span class="ltx_bibblock">
Philippe Langlais, George Foster, and Guy Lapalme.

</span>
<span class="ltx_bibblock">2000.

</span>
<span class="ltx_bibblock">Transtype: a computer-aided translation typing system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the ANLP-NAACL 2000 Workshop on Embedded
Machine Translation Systems</span>, Seattle, WA.

</span></li>
<li id="bib.bibx22" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Lopez2007</span>
<span class="ltx_bibblock">
Adam Lopez.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock">Hierarchical phrase-based translation with suffix arrays.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL 2007)</span>, Prague, Czech Republic.

</span></li>
<li id="bib.bibx23" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Martínez-Gómez et al.2012</span>
<span class="ltx_bibblock">
Pascual Martínez-Gómez, Germán Sanchis-Trilles, and Francisco
Casacuberta.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Online adaptation strategies for statistical machine translation in
post-editing scenarios.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Pattern Recognition</span>, 45(9):3193–3202.

</span></li>
<li id="bib.bibx24" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">McAllester and Keshet2011</span>
<span class="ltx_bibblock">
David McAllester and Joseph Keshet.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Generalization bounds and consistency for latent structural probit
and ramp loss.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 25th Annual Conference on Neural
Information Processing Sytems (NIPS 2011)</span>, Granada, Spain.

</span></li>
<li id="bib.bibx25" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Nakov et al.2012</span>
<span class="ltx_bibblock">
Preslav Nakov, Francisco Guzmán, and Stephan Vogel.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Optimizing for sentence-level bleu+1 yields short translations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012)</span>, Bombay, India.

</span></li>
<li id="bib.bibx26" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Nikoulina et al.2012</span>
<span class="ltx_bibblock">
Vassilina Nikoulina, Bogomil Kovachev, Nikolaos Lagos, and Christof Monz.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Adaptation of statistical machine translation model for cross-lingual
information retrieval in a service context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 13th Conference of the European Chapter of
the Association for Computational Linguistics (EACL’12)</span>, Avignon, France.

</span></li>
<li id="bib.bibx27" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Noreen1989</span>
<span class="ltx_bibblock">
Eric W. Noreen.

</span>
<span class="ltx_bibblock">1989.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Computer Intensive Methods for Testing Hypotheses. An
Introduction</span>.

</span>
<span class="ltx_bibblock">Wiley, New York.

</span></li>
<li id="bib.bibx28" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Ortiz-Martínez et al.2010</span>
<span class="ltx_bibblock">
Daniel Ortiz-Martínez, Ismal García-Varea, and Francisco Casacuberta.

</span>
<span class="ltx_bibblock">2010.

</span>
<span class="ltx_bibblock">Online learning for interactive statistical machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Human Language Technologies conference and
the 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL’10)</span>, Los Angeles, CA.

</span></li>
<li id="bib.bibx29" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Papineni et al.2001</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">2001.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">Technical Report IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.

</span></li>
<li id="bib.bibx30" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Pfafflin1965</span>
<span class="ltx_bibblock">
Sheila M. Pfafflin.

</span>
<span class="ltx_bibblock">1965.

</span>
<span class="ltx_bibblock">Evaluation of machine translations by reading comprehension tests and
subjective judgements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Mechanical Translation and Computational Linguistics</span>,
8(2):2–8.

</span></li>
<li id="bib.bibx31" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Sakamoto et al.2013</span>
<span class="ltx_bibblock">
Akiko Sakamoto, Nayuko Watanabe, Satoshi Kamatani, and Kazuo Sumita.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Development of a simultaneous interpretation system for face-to-face
services and its evaluation experiment in real situation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Machine Translation Summit XIV</span>, Nice,
France.

</span></li>
<li id="bib.bibx32" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Saluja et al.2012</span>
<span class="ltx_bibblock">
Avneesh Saluja, Ian Lane, and Ying Zhang.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Machine translation with binary feedback: A large-margin approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 10th Biennial Conference of the
Association for Machine Translation in the Americas (AMTA’12)</span>, San Diego,
CA.

</span></li>
<li id="bib.bibx33" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Shalev-Shwartz et al.2007</span>
<span class="ltx_bibblock">
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.

</span>
<span class="ltx_bibblock">2007.

</span>
<span class="ltx_bibblock">Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 24th International Conference on Machine
Learning (ICML’07)</span>, Corvallis, OR.

</span></li>
<li id="bib.bibx34" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Simianer et al.2012</span>
<span class="ltx_bibblock">
Patrick Simianer, Stefan Riezler, and Chris Dyer.

</span>
<span class="ltx_bibblock">2012.

</span>
<span class="ltx_bibblock">Joint feature selection in distributed stochastic learning for
large-scale discriminative training in SMT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012)</span>, Jeju, Korea.

</span></li>
<li id="bib.bibx35" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Smith et al.2013</span>
<span class="ltx_bibblock">
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris
Callison-Burch, and Adam Lopez.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Dirt cheap web-scale parallel text from the common crawl.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics (ACL’13)</span>, Sofia, Bulgaria.

</span></li>
<li id="bib.bibx36" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Wäschle et al.2013</span>
<span class="ltx_bibblock">
Katharina Wäschle, Patrick Simianer, Nicola Bertoldi, Stefan Riezler, and
Marcello Federico.

</span>
<span class="ltx_bibblock">2013.

</span>
<span class="ltx_bibblock">Generative and discriminative methods for online adaptation in SMT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Machine Translation Summit XIV</span>, Nice,
France.

</span></li>
<li id="bib.bibx37" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Wong and Mooney2006</span>
<span class="ltx_bibblock">
Yuk Wah Wong and Raymond J. Mooney.

</span>
<span class="ltx_bibblock">2006.

</span>
<span class="ltx_bibblock">Learning for semantic parsing with statistical machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Human Language Technology Conference of
the North American Chapter of the Association for Computational Linguistics
(HLT/NAACL’06)</span>, New York City, NY.

</span></li>
<li id="bib.bibx38" class="ltx_bibitem"><span class="ltx_bibtag ltx_role_refnum">Zettlemoyer and Collins2009</span>
<span class="ltx_bibblock">
Luke S. Zettlemoyer and Michael Collins.

</span>
<span class="ltx_bibblock">2009.

</span>
<span class="ltx_bibblock">Learning context-dependent mappings from sentences to logical form.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL-IJCNLP’09)</span>, Singapore.

</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 18:11:04 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
