<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Humans Require Context to Infer Ironic Intent(so Computers Probably do, too)</title>
<!--Generated on Wed Jun 11 17:58:46 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Humans Require Context to Infer Ironic Intent
<br class="ltx_break"/>(so Computers Probably do, too)</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Byron C. Wallace, Do Kook Choe, Laura Kertz 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eugene Charniak 
<br class="ltx_break"/>Brown University 
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">byron_wallace, do_kook_choe, laura_kertz, eugene_charniak</span>}<span class="ltx_text ltx_font_typewriter">@brown.edu</span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Automatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than – and often opposite to – what they actually mean. Discerning ironic intent exclusively from the words and syntax comprising texts (e.g., tweets, forum posts) is therefore not always possible: additional contextual information about the speaker and/or the topic at hand is often necessary. We introduce a new corpus that provides empirical evidence for this claim. We show that annotators frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction &amp; Motivation</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">This work concerns the task of detecting verbal irony online. Our principal argument is that simple bag-of-words based text classification models – which, when coupled with sufficient data, have proven to be extremely successful for many natural language processing tasks <cite class="ltx_cite">[]</cite> – are inadequate for irony detection. In this paper we provide empirical evidence that <em class="ltx_emph">context</em> is often necessary to recognize ironic intent.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">This is consistent with the large body of pragmatics/linguistics literature on irony and its usage, which has emphasized the role that context plays in recognizing and decoding ironic utterances <cite class="ltx_cite">[]</cite>. But existing work on automatic irony detection – reviewed in Section <a href="#S2" title="2 Previous Work ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> – has not explicitly attempted to operationalize such theories, and has instead relied on features (mostly word counts) intrinsic to the texts that are to be classified as ironic. These approaches have achieved some success, but necessarily face an upper-bound: the <em class="ltx_emph">exact same sentence</em> can be both intended ironically and unironically, depending on the context (including the speaker and the topic at hand). Only obvious verbal ironies will be recognizable from intrinsic features alone.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Here we provide empirical evidence for the above claims. We also introduce a new annotated corpus that will allow researchers to build models that augment existing approaches to irony detection with contextual information regarding the text (utterance) to be classified and its author. Briefly, our contributions are summarized as follows.</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">We introduce the first version of the <em class="ltx_emph">reddit irony corpus</em>, composed of annotated comments from the social news website reddit. Each sentence in every comment in this corpus has been labeled by three independent annotators as having been intended by the author ironically or not. This dataset is publicly available.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><a href="https://github.com/bwallace/ACL-2014-irony" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/bwallace/ACL-2014-irony</span></a></span></span></span></p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">We provide empirical evidence that human annotators consistently rely on contextual information to make ironic/unironic sentence judgements.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">We show that the standard ‘bag-of-words’ approach to text classification fails to accurately judge ironic intent on those cases for which humans required additional context. This suggests that, as humans require context to make their judgements for this task, so too do computers.</p>
</div></li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Our hope is that these observations and this dataset will spur innovative new research on methods for verbal irony detection.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Previous Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">There has recently been a flurry of interesting work on automatic irony detection <cite class="ltx_cite">[]</cite>. In these works, verbal irony detection has mostly been treated as a standard text classification task, though with some innovative approaches specific to detecting irony.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">The most common data source used to experiment with irony detection systems has been Twitter <cite class="ltx_cite">[]</cite>, though Amazon product reviews have been used experimentally as well <cite class="ltx_cite">[]</cite>. Walker et al. <cite class="ltx_cite">[]</cite> also recently introduced the Internet Argument Corpus (IAC), which includes a <em class="ltx_emph">sarcasm</em> label (among others).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Some of the findings from these previous efforts have squared with intuition: e.g., overzealous punctuation (as in “great idea!!!!”) is indicative of ironic intent <cite class="ltx_cite">[]</cite>. Other works have proposed novel approaches specifically for irony detection: Davidov et al. <cite class="ltx_cite">[]</cite>, for example, proposed a semi-supervised approach in which they look for sentence <em class="ltx_emph">templates</em> indicative of irony. Elsewhere, Riloff et al. <cite class="ltx_cite">[]</cite> proposed a method that exploits contrasting sentiment in the same utterance to detect irony.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">To our knowledge, however, no previous work on irony detection has attempted to leverage <em class="ltx_emph">contextual</em> information regarding the author or speaker (external to the utterance). But this is necessary in some cases, however. For example, in the case of Amazon product reviews, knowing the kinds of books that an individual typically likes might inform our judgement: someone who tends to read and review Dostoevsky is probably being ironic if she writes a glowing review of <em class="ltx_emph">Twilight</em>. Of course, many people genuinely do enjoy <em class="ltx_emph">Twilight</em> and so if the review is written subtly it will likely be difficult to discern the author’s intent without this background. In the case of Twitter, it is likely to be difficult to classify utterances without considering the contextualizing exchange of tweets (i.e., the conversation) to which they belong.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Introducing the reddit Irony Dataset</h2>

<div id="S3.F1" class="ltx_figure">
<p class="ltx_p ltx_align_center"><span class="ltx_text" style="border:1px solid black;"><img src="" id="S3.F1.g1" class="ltx_graphics" alt=""/></span></p>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The web-based tool used by our annotators to label reddit comments. Enumerated interface elements are described as follows: <span class="ltx_text ltx_font_bold">1</span> the text of the comment to be annotated – sentences marked as <em class="ltx_emph">ironic</em> are highlighted; <span class="ltx_text ltx_font_bold">2</span> buttons to label sentences as <em class="ltx_emph">ironic</em> or <em class="ltx_emph">unironic</em>; <span class="ltx_text ltx_font_bold">3</span> buttons to request additional <em class="ltx_emph">context</em> (the embedding discussion thread or associated webpage – see Section <a href="#S3.SS2" title="3.2 Context ‣ 3 Introducing the reddit Irony Dataset ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>); <span class="ltx_text ltx_font_bold">4</span> radio button to provide <em class="ltx_emph">confidence</em> in comment labels (<em class="ltx_emph">low</em>, <em class="ltx_emph">medium</em> or <em class="ltx_emph">high</em>).</div>
</div>
<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">sub-reddit (URL)</span></th>
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">description</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">number of labeled comments</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">politics (</span><a href="http://reddit.com/r/politics" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/politics</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">Political news and editorials; focus on the US.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">873</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">conservative (</span><a href="http://www.reddit.com/r/Conservative/" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/conservative</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">A community for political conservatives.</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">573</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">progressive (</span><a href="http://www.reddit.com/r/Progressive/" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/progressive</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">A community for political progressives (liberals).</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">543</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">atheism (</span><a href="http://reddit.com/r/atheism" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/atheism</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">A community for non-believers.</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">442</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">Christianity (</span><a href="http://reddit.com/r/Christianity" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/Christianity</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">News and viewpoints on the Christian faith.</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">312</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">technology (</span><a href="http://reddit.com/r/technology" title="" class="ltx_ref"><span class="ltx_text ltx_font_small">r/technology</span></a><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">Technology news and commentary.</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">277</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 1: </span>The six sub-reddits that we have downloaded comments from and the corresponding number of comments for which we have acquired annotations in this <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T1.m2" class="ltx_Math" alttext="\beta" display="inline"><mi mathsize="normal" stretchy="false">β</mi></math> version of the corpus. Note that we acquired labels at the <em class="ltx_emph">sentence</em> level, whereas the counts above reflect <em class="ltx_emph">comments</em>, all of which contain at least one sentence.</div>
</div>
<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Here we introduce the first version (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="\beta" display="inline"><mi>β</mi></math> 1.0) of our irony corpus. Reddit (<a href="http://reddit.com" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://reddit.com</span></a>) is a social-news website to which news stories (and other links) are posted, voted on and commented upon. The forum component of reddit is extremely active: popular posts often have well into 1000’s of user comments. Reddit comprises ‘sub-reddits’, which focus on specific topics. For example, <a href="http://reddit.com/r/politics" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://reddit.com/r/politics</span></a> features articles (and hence comments) centered around political news. The current version of the corpus is available at: <a href="https://github.com/bwallace/ACL-2014-irony" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/bwallace/ACL-2014-irony</span></a>. Data collection and annotation is ongoing, so we will continue to release new (larger) versions of the corpus in the future. The present version comprises 3,020 annotated comments scraped from the six subreddits enumerated in Table <a href="#S3.T1" title="Table 1 ‣ 3 Introducing the reddit Irony Dataset ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. These comments in turn comprise a total of 10,401 labeled sentences.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>We performed naïve ‘segmentation’ of comments based on punctuation.</span></span></span></p>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Annotation Process</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Three university undergraduates independently annotated each sentence in the corpus. More specifically, annotators have provided binary ‘labels’ for each sentence indicating whether or not they (the annotator) believe it was intended by the author ironically (or not). This annotation was provided via a custom-built browser-based annotation tool, shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Introducing the reddit Irony Dataset ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">We intentionally did not provide much guidance to annotators regarding the criteria for what constitutes an ‘ironic’ statement, for two reasons. First, verbal irony is a notoriously slippery concept <cite class="ltx_cite">[]</cite> and coming up with an operational definition to be consistently applied is non-trivial. Second, we were interested in assessing the extent of natural agreement between annotators for this task. The raw average agreement between all annotators on all sentences is 0.844. Average pairwise Cohen’s Kappa <cite class="ltx_cite">[]</cite> is 0.341, suggesting fair to moderate agreement <cite class="ltx_cite">[]</cite>, as we might expect for a subjective task like this one.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Context</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">Reddit is a good corpus for the irony detection task in part because it provides a natural practical realization of the otherwise ill-defined <em class="ltx_emph">context</em> for comments. In particular, each comment is associated with a specific user (the author), and we can view their previous comments. Moreover, comments are embedded within discussion <em class="ltx_emph">threads</em> that pertain to the (usually external) content linked to in the corresponding submission (see Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Context ‣ 3 Introducing the reddit Irony Dataset ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). These pieces of information (previous comments by the same user, the external link of the embedding reddit thread, and the other comments in this thread) constitute our context. All of this is readily accessible. Labelers can opt to request these pieces of context via the annotation tool, and we record when they do so.</p>
</div>
<div id="S3.F2" class="ltx_figure">
<p class="ltx_p"><span class="ltx_text" style="border:1px solid black;"><img src="" id="S3.F2.g1" class="ltx_graphics" alt=""/></span></p>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustrative reddit comment (highlighted). The title (“Virginia Republican …”) links to an article, providing one example of contextualizing content. The conversational thread in which this comment is embedded provides additional context. The comment in question was presumably intended ironically, though without the aforementioned context this would be difficult to conclude with any certainty.</div>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Consider the following example comment taken from our dataset: “Great idea on the talkathon Cruz. Really made the republicans look like the sane ones.” Did the author intend this statement ironically, or was this a subtle dig on Senator Ted Cruz? Without additional context it is difficult to know. And indeed, all three annotators requested additional context for this comment. This context at first suggests that the comment may have been intended literally: it was posted in the r/conservative subreddit (Ted Cruz is a conservative senator). But if we peruse the author’s comment history, we see that he or she repeatedly derides Senator Cruz (e.g., writing “Ted Cruz is no Ronald Reagan. They aren’t even close.”). From this contextual information, then, we can reasonably assume that the comment was intended ironically (and all three annotators did so after assessing the available contextual information).</p>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Humans Need Context to Infer Irony</h2>

<div id="S4.F3" class="ltx_figure"><img src="" id="S4.F3.g1" class="ltx_graphics" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>This plot illustrates the effect of viewing contextual information for three annotators (one table for each annotator). For all comments for which these annotators requested context, we show <em class="ltx_emph">forced</em> (before viewing the requested contextual content) and <em class="ltx_emph">final</em> (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four possible decision sequences (e.g., a judgement of <em class="ltx_emph">ironic</em> prior to seeing context and <em class="ltx_emph">unironic</em> after). Numbers correspond to counts of these sequences for each annotator (e.g., the first annotator changed their mind from <em class="ltx_emph">ironic</em> to <em class="ltx_emph">unironic</em> 86 times). Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided: these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results in annotators changing their judgement.</div>
</div>
<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We explore the extent to which human annotators rely on contextual information to decide whether or not sentences were intended ironically. Recall that our annotation tool allows labelers to request additional context if they cannot make a decision based on the comment text alone (Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Introducing the reddit Irony Dataset ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). On average, annotators requested additional context for 30% of comments (range across annotators of 12% to 56%). As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Humans Need Context to Infer Irony ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, annotators are consistently more confident once they have consulted this information.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">We tested for a correlation between these requests for context and the final decisions regarding whether comments contain at least one ironic sentence. We denote the probability of at least one annotator requesting additional context for comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m1" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m2" class="ltx_Math" alttext="P(\mathcal{C}_{i})" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math>. We then model the probability of this event as a linear function of whether or not any annotator labeled any sentence in comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> as ironic. We code this via the indicator variable <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m4" class="ltx_Math" alttext="\mathcal{I}_{i}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>i</mi></msub></math> which is 1 when comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m5" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> has been deemed to contain an ironic sentence (by any of the three annotators) and 0 otherwise.</p>
</div>
<div id="S4.p3" class="ltx_para">
<table id="S4.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E1.m1" class="ltx_Math" alttext="logit\{P(\mathcal{C}_{i})\}=\beta_{0}+\beta_{1}\mathcal{I}_{i}" display="block"><mrow><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>{</mo><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>β</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>β</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>i</mi></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">We used the regression model shown in Equation <a href="#S4.E1" title="(1) ‣ 4 Humans Need Context to Infer Irony ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m1" class="ltx_Math" alttext="\beta_{0}" display="inline"><msub><mi>β</mi><mn>0</mn></msub></math> is an intercept and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m2" class="ltx_Math" alttext="\beta_{1}" display="inline"><msub><mi>β</mi><mn>1</mn></msub></math> captures the correlation between requests for context for a given comment and its ultimately being deemed to contain at least one ironic sentence. We fit this model to the annotated corpus, and found a significant correlation: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m3" class="ltx_Math" alttext="\hat{\beta}_{1}=1.508" display="inline"><mrow><msub><mover accent="true"><mi>β</mi><mo stretchy="false">^</mo></mover><mn>1</mn></msub><mo>=</mo><mn>1.508</mn></mrow></math> with a 95% confidence interval of (1.326, 1.690); <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m4" class="ltx_Math" alttext="p&lt;0.001" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow></math>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">In other words, annotators request context significantly more frequently for those comments that (are ultimately deemed to) contain an ironic sentence. This would suggest that the words and punctuation comprising online comments alone are not sufficient to distinguish ironic from unironic comments. Despite this, most machine learning based approaches to irony detection have relied nearly exclusively on such intrinsic features.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Machines Probably do, too</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We show that the misclassifications (with respect to whether comments contain irony or not) made by a standard text classification model significantly correlate with those comments for which human annotators requested additional context. This provides evidence that bag-of-words approaches are insufficient for the general task of irony detection: more context is necessary.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">We implemented a baseline classification approach using vanilla token count features (binary bag-of-words). We removed stop-words and limited the vocabulary to the 50,000 most frequently occurring unigrams and bigrams. We added additional binary features coding for the presence of punctuational features, such as exclamation points, emoticons (for example, ‘;)’) and question marks: previous work <cite class="ltx_cite">[]</cite> has found that these are good indicators of ironic intent.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">For our predictive model, we used a linear-kernel SVM (tuning the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m1" class="ltx_Math" alttext="C" display="inline"><mi>C</mi></math> parameter via grid-search over the training dataset to maximize F1 score). We performed five-fold cross-validation, recording the predictions <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m2" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub></math> for each (held-out) comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>. Average F1 score over the five-folds was 0.383 with range (0.330, 0.412); mean recall was 0.496 (0.446, 0.548) and average precision was 0.315 (0.261, 0.380). The five most predictive tokens were: <em class="ltx_emph">!</em>, <em class="ltx_emph">yeah</em>, <em class="ltx_emph">guys</em>, <em class="ltx_emph">oh</em> and <em class="ltx_emph">shocked</em>. This represents reasonable performance (with intuitive predictive tokens); but obviously there is quite a bit of room for improvement.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>Some of the recently proposed strategies mentioned in Section <a href="#S2" title="2 Previous Work ‣ Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> may improve performance here, but none of these address the fundamental issue of <em class="ltx_emph">context</em>.</span></span></span></p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">We now explore empirically whether these misclassifications are made on the same comments for which annotators requested context. To this end, we introduce a variable <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m1" class="ltx_Math" alttext="\mathcal{M}_{i}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>i</mi></msub></math> for each comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m2" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> such that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m3" class="ltx_Math" alttext="\mathcal{M}_{i}=1" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math> if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m4" class="ltx_Math" alttext="\hat{y}_{i}\neq y_{i}" display="inline"><mrow><msub><mover accent="true"><mi>y</mi><mo stretchy="false">^</mo></mover><mi>i</mi></msub><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></math>, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m5" class="ltx_Math" alttext="\mathcal{M}_{i}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>i</mi></msub></math> is an indicator variable that encodes whether or not the classifier misclassified comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>. We then ran a second regression in which the output variable was the logit-transformed probability of the model misclassifying comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m7" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>, i.e., <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m8" class="ltx_Math" alttext="P(\mathcal{M}_{i})" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math>. Here we are interested in the correlation of the event that one or more annotators requested additional context for comment <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m9" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> (denoted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m10" class="ltx_Math" alttext="\mathcal{C}_{i}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>i</mi></msub></math>) and model misclassifications (adjusting for the comment’s true label). Formally:</p>
</div>
<div id="S5.p5" class="ltx_para">
<table id="S5.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E2.m1" class="ltx_Math" alttext="logit\{P(\mathcal{M}_{i})\}=\theta_{0}+\theta_{1}\mathcal{I}_{i}+\theta_{2}%&#10;\mathcal{C}_{i}" display="block"><mrow><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mrow><mo>{</mo><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>}</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>+</mo><mrow><msub><mi>θ</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℐ</mi><mi>i</mi></msub></mrow><mo>+</mo><mrow><msub><mi>θ</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">𝒞</mi><mi>i</mi></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
</div>
<div id="S5.p6" class="ltx_para">
<p class="ltx_p">Fitting this to the data, we estimated <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m1" class="ltx_Math" alttext="\hat{\theta}_{2}=0.971" display="inline"><mrow><msub><mover accent="true"><mi>θ</mi><mo stretchy="false">^</mo></mover><mn>2</mn></msub><mo>=</mo><mn>0.971</mn></mrow></math> with a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m2" class="ltx_Math" alttext="95\%" display="inline"><mrow><mn>95</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> CI of (0.810, 1.133); <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m3" class="ltx_Math" alttext="p&lt;0.001" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow></math>. Put another way, the model makes mistakes on those comments for which annotators requested additional context (even after accounting for the annotator designation of comments).</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">We have described a new (publicly available) corpus for the task of verbal irony detection. The data comprises comments scraped from the social news website reddit. We recorded confidence judgements and requests for contextualizing information for each comment during annotation. We analyzed this corpus to provide empirical evidence that annotators quite often require context beyond the comment under consideration to discern irony; especially for those comments ultimately deemed as being intended ironically. We demonstrated that a standard token-based machine learning approach misclassified many of the same comments for which annotators tend to request context.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">We have shown that annotators rely on contextual cues (in addition to word and grammatical features) to discern irony and argued that this implies computers should, too. The obvious next step is to develop new machine learning models that exploit the contextual information available in the corpus we have curated (e.g., previous comments by the same user, the thread topic).</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Acknowledgement</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">This work was made possible by the Army Research Office (ARO), grant #64481-MA.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:58:46 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
