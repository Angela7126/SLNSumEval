<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Sprinkling Topics for Weakly Supervised Text Classification</title>
<!--Generated on Wed Jun 11 17:32:43 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_normal ltx_title_document">Sprinkling Topics for Weakly Supervised Text Classification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_normal">Swapnil Hingmire</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="{}^{\textrm{1}}" display="inline"><msup><mi/><mtext mathsize="normal" stretchy="false">1</mtext></msup></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="{}^{\textrm{,}}" display="inline"><msup><mi/><mtext mathsize="normal" stretchy="false">,</mtext></msup></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="{}^{\textrm{2}}" display="inline"><msup><mi/><mtext mathsize="normal" stretchy="false">2</mtext></msup></math>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter ltx_font_normal">swapnil.hingmire@tcs.com</span><span class="ltx_text ltx_font_normal">
</span>
<br class="ltx_break"/>&amp;<span class="ltx_text ltx_font_normal">Sutanu Chakraborti</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="{}^{\textrm{2}}" display="inline"><msup><mi/><mtext mathsize="normal" stretchy="false">2</mtext></msup></math>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter ltx_font_normal">sutanuc@cse.iitm.ac.in<span class="ltx_ERROR undefined">\@close@row</span></span><span class="ltx_text ltx_font_normal">  </span>
<br class="ltx_break"/><span class="ltx_text" style="width:433.6pt;">  <span class="ltx_text" style="width:0.0pt;">
<table class="ltx_tabular ltx_align_top">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="{}^{\textrm{1}}" display="inline"><msup><mi/><mtext>1</mtext></msup></math>Systems Research Lab, Tata Research Development and Design Center, Pune, India</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="{}^{\textrm{2}}" display="inline"><msup><mi/><mtext>2</mtext></msup></math>Department of Computer Science and Engineering,</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_right">Indian Institute of Technology Madras, Chennai, India</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">1. for each topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="m7" class="ltx_Math" alttext="t" display="inline"><mi mathsize="normal" stretchy="false">t</mi></math>, draw a distribution over words: <span class="ltx_text ltx_font_footnote">
<math xmlns="http://www.w3.org/1998/Math/MathML" id="m8" class="ltx_Math" alttext="\phi_{t}\sim\mathrm{Dirichlet}(\beta_{w})" display="inline"><mrow><msub><mi mathsize="normal" stretchy="false">ϕ</mi><mi mathsize="normal" stretchy="false">t</mi></msub><mo mathsize="normal" stretchy="false">∼</mo><mrow><mi mathsize="normal" stretchy="false">Dirichlet</mi><mo mathsize="small" stretchy="false">⁢</mo><mrow><mo mathsize="small" stretchy="false">(</mo><msub><mi mathsize="normal" stretchy="false">β</mi><mi mathsize="normal" stretchy="false">w</mi></msub><mo mathsize="small" stretchy="false">)</mo></mrow></mrow></mrow></math></span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">2. for each document <math xmlns="http://www.w3.org/1998/Math/MathML" id="m9" class="ltx_Math" alttext="d\in D" display="inline"><mrow><mi mathsize="normal" stretchy="false">d</mi><mo mathsize="normal" stretchy="false">∈</mo><mi mathsize="normal" stretchy="false">D</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">a. Draw a vector of topic proportions:</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="m10" class="ltx_Math" alttext="\theta_{d}\sim\mathrm{Dirichlet}(\alpha_{t})" display="inline"><mrow><msub><mi mathsize="normal" stretchy="false">θ</mi><mi mathsize="normal" stretchy="false">d</mi></msub><mo mathsize="normal" stretchy="false">∼</mo><mrow><mi mathsize="normal" stretchy="false">Dirichlet</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi mathsize="normal" stretchy="false">α</mi><mi mathsize="normal" stretchy="false">t</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">b. for each word <math xmlns="http://www.w3.org/1998/Math/MathML" id="m11" class="ltx_Math" alttext="w" display="inline"><mi mathsize="normal" stretchy="false">w</mi></math> at position <math xmlns="http://www.w3.org/1998/Math/MathML" id="m12" class="ltx_Math" alttext="n" display="inline"><mi mathsize="normal" stretchy="false">n</mi></math> in <math xmlns="http://www.w3.org/1998/Math/MathML" id="m13" class="ltx_Math" alttext="d" display="inline"><mi mathsize="normal" stretchy="false">d</mi></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">i. Draw a topic assignment:</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="m14" class="ltx_Math" alttext="z_{d,n}\sim\mathrm{Multinomial}(\theta_{d})" display="inline"><mrow><msub><mi mathsize="normal" stretchy="false">z</mi><mrow><mi mathsize="normal" stretchy="false">d</mi><mo>,</mo><mi mathsize="normal" stretchy="false">n</mi></mrow></msub><mo mathsize="normal" stretchy="false">∼</mo><mrow><mi mathsize="normal" stretchy="false">Multinomial</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi mathsize="normal" stretchy="false">θ</mi><mi mathsize="normal" stretchy="false">d</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">ii. Draw a word:</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="m15" class="ltx_Math" alttext="w_{d,n}\sim\mathrm{Multinomial}(z_{d,n})" display="inline"><mrow><msub><mi mathsize="normal" stretchy="false">w</mi><mrow><mi mathsize="normal" stretchy="false">d</mi><mo>,</mo><mi mathsize="normal" stretchy="false">n</mi></mrow></msub><mo mathsize="normal" stretchy="false">∼</mo><mrow><mi mathsize="normal" stretchy="false">Multinomial</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi mathsize="normal" stretchy="false">z</mi><mrow><mi mathsize="normal" stretchy="false">d</mi><mo>,</mo><mi mathsize="normal" stretchy="false">n</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></math></td></tr>
</tbody>
</table></span></span>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Supervised text classification algorithms require a large number of documents labeled by humans, that involve a labor-intensive and time consuming process. In this paper, we propose a weakly supervised algorithm in which supervision comes in the form of labeling of Latent Dirichlet Allocation (LDA) topics. We then use this weak supervision to “sprinkle” artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations. We evaluate this approach to improve performance of text classification on three real world datasets.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">In supervised text classification learning algorithms, the learner (a program) takes
human labeled documents as input and learns a decision function that can classify a
previously unseen document to one of the predefined classes.
Usually a large number of documents labeled by humans are used by the learner
to classify unseen documents with adequate accuracy. Unfortunately, labeling a large
number of documents is a labor-intensive and time consuming process.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) <cite class="ltx_cite">[]</cite> which does not need labeled documents. LDA is an unsupervised probabilistic topic model and it is widely used to discover latent semantic structure of a document collection by modeling words in the documents. Blei et al. <cite class="ltx_cite">[]</cite> used LDA topics as features in text classification, but they use labeled documents while learning a classifier. sLDA <cite class="ltx_cite">[]</cite>, DiscLDA <cite class="ltx_cite">[]</cite> and MedLDA <cite class="ltx_cite">[]</cite> are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">An approach that is less demanding in terms of knowledge engineering is ClassifyLDA (Hingmire et al., 2013). In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the basis of its most prominent topics. We extend ClassifyLDA algorithm by “sprinkling” topics to unlabeled documents.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Sprinkling <cite class="ltx_cite">[]</cite> integrates class labels of documents into Latent Semantic Indexing (LSI)<cite class="ltx_cite">[]</cite>. The basic idea involves encoding of class labels as artificial words which are “sprinkled” (appended) to training documents. As LSI uses higher order word associations <cite class="ltx_cite">[]</cite>, sprinkling of artificial words gives better and class-enriched latent semantic structure. However, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents. The paper revolves around the idea of labeling topics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these labeled topic for sprinkling.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">As in ClassifyLDA, we ask an annotator to assign class labels to a set of topics inferred on the unlabeled training documents. We use the labeled topics to find probability distribution of each training document over the class labels. We create a set of artificial words corresponding to a class label and add (or sprinkle) them to the document. The number of such artificial terms is proportional to the probability of generating the document by the class label. We then infer a set of topics on the sprinkled training documents. As LDA uses higher order word associations <cite class="ltx_cite">[]</cite> while discovering topics, we hypothesize that sprinkling will improve text classification performance of ClassifyLDA. We experimentally verify this hypothesis on three real world datasets.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Several researchers have proposed semi-supervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in <cite class="ltx_cite">[]</cite>, <cite class="ltx_cite">[]</cite>, <cite class="ltx_cite">[]</cite> and <cite class="ltx_cite">[]</cite> are a few examples of this type. However, these algorithms are sensitive to initial labeled documents and hyper-parameters of the algorithm.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">In the second category, supervision comes in the form of labeled words (features). <cite class="ltx_cite">[]</cite> and <cite class="ltx_cite">[]</cite> are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a polysemous word, which may affect the performance of a text classifier.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">The third type of semi-supervised text classification algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).<cite class="ltx_cite">[]</cite>, <cite class="ltx_cite">[]</cite>, <cite class="ltx_cite">[]</cite> are a few examples of active learning based text classification algorithms. However, these algorithms are sensitive to the sampling strategy used to query documents or features.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">In our approach, an annotator does not label documents or words, rather she labels a small set of interpretable topics which are inferred in an unsupervised manner. These topics are very few, when compared to the number of documents. As the most probable words of topics are representative of the dataset, there is no need for the annotator to search for the right set of features for each class. As LDA topics are semantically more meaningful than individual words and can be acquired easily, our approach overcomes limitations of the semi-supervised methods discussed above.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Background</h2>

<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>LDA</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">LDA is an unsupervised probabilistic generative model for collections of discrete data such as text documents. The generative process of LDA can be described as follows:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">1. for each topic t, draw a distribution over words: φt ∼ Dirichlet(βw)</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">2. for each document d ∈ D</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">a. Draw a vector of topic proportions:θd ∼ Dirichlet(αt)</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p class="ltx_p">b. for each word w at position n in d: i. Draw a topic assignment: zd,n ∼ Multinomial(θd); ii. Draw a word: wd,n ∼ Multinomial(zd,n)</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p class="ltx_p">Where, T is the number of topics, φt is the word probabilities for topic t, θd is the topic probability distribution, zd,n is topic assignment and wd,n is word assignment for nth word position in document d respectively. αt and βw are topic and word Dirichlet priors.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p class="ltx_p">The key problem in LDA is posterior inference. The posterior inference involves the inference of the hidden topic structure given the observed documents. However, computing the exact posterior inference is intractable. In this paper we estimate approximate posterior inference using collapsed Gibbs sampling (Griffiths and Steyvers, 2004).</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<p class="ltx_p">The Gibbs sampling equation used to update the assignment of a topic t to the word w ∈ W at the position n in document d, conditioned on αt, βw is:</p>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p class="ltx_p">The Gibbs sampling equation used to update the assignment of a topic t to the word w ∈ W at the position n in document d, conditioned on αt, βw is:</p>
</div>
<div id="S3.SS1.p10" class="ltx_para">
<p class="ltx_p">P(zd,n = t|zd,¬n, wd,n = w, αt, βw) ∝ ψw,t + βw − 1 P v∈W ψv,t + βv − 1 × (Ωt,d + αt − 1) (1)</p>
</div>
<div id="S3.SS1.p11" class="ltx_para">
<p class="ltx_p">where ψw,c is the count of the word w assigned to the topic c, Ωc,d is the count of the topic c assigned to words in the document d and W is
the vocabulary of the corpus. We use a subscript d, ¬n to denote the current token, zd,n is ignored in the Gibbs sampling update. After performing collapsed Gibbs sampling using equation 1, we
use word topic assignments to compute a point estimate of the distribution over words φw,c and a point estimate of the posterior distribution over topics for each document d (θd) is:
φw,t = ψw,t + βw P v∈W ψv,t + βv (2)
θt,d = Ωt,d + αt PT i=1 Ωi,d + αi (3)</p>
</div>
<div id="S3.SS1.p12" class="ltx_para">
<p class="ltx_p">Let MD = &lt; Z, Φ, Θ &gt; be the hidden topic structure, where Z is per word per document topic assignment, Φ = {φt} and Θ = {θd}.</p>
</div>
</div>

<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span> Sprinkling</h3>
<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">(Chakraborti et al., 2007) propose a simple approach called “sprinkling” to incorporate class labels of documents into LSI. In sprinkling, a set of artificial words are appended to a training document which are specific to the class label of the document. Consider a case of binary classification with classes c1 and c2. If a document d belongs to the class c1 then a set of artificial words which represent the class c1 are appended into the document d, otherwise a set of artificial words which represent the class c2 are appended.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">Singular Value Decomposition (SVD) is then performed on the sprinkled training documents and a lower rank approximation is constructed by ignoring dimensions corresponding to lower singular values. Then, the sprinkled terms are removed from the lower rank approximation. (Chakraborti et al., 2007) empirically show that sprinkled words boost higher order word associations and projects documents with same class labels close to each other in latent semantic space.</p>
</div>
</div>
</div>

<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Topic Sprinkling in LDA</h2>
<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In our text classification algorithm, we first infer a set of topics on the given unlabeled document corpus. We then ask a human annotator to assign one or more class labels to the topics based on their most probable words. We use these labeled topics to create a new LDA model as follows. If the topic assigned to the word w at the position n in document d is t, then we replace it by the class label assigned to the topic t. If more than one class labels are assigned to the topic t, then we randomly select one of the class labels assigned to the topic t. If the annotator is unable to label a topic then we randomly select a class label from the set of all class labels. We then update the new LDA model using collapsed Gibbs sampling.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">We use this new model to infer the probability distribution of each unlabeled training document over the class labels. Let, θc,d be the probability of generating document d by class c. We then sprinkle s artificial words of class label c to document d, such that s = K ∗ θc,d for some constant K.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">We then infer a set of |C| number of topics on the sprinkled dataset using collapsed Gibbs sampling, where C is the set of class labels of the training documents. We modify collapsed Gibbs sampling update in Equation 1 to carry class label information while inferring topics. If a word in a document is a sprinkled word then while sampling a class label for it, we sample the class label associated with the sprinkled word, otherwise we sample a class label for the word using Gibbs update in Equation 1.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">We name this model as Topic Sprinkled LDA (TS-LDA). While classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label. Algorithm for TS-LDA is summarized in Table 1.</p>
</div>
</div>>

<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experimental Evaluation</h2>
<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We determine the effectiveness of our algorithm in relation to ClassifyLDA algorithm proposed in (Hingmire et al., 2013). We evaluate and compare our text classification algorithm by computing Macro averaged F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average MacroF1. Similar to (Blei et al., 2003) we also learn supervised SVM classifier (LDA-SVM) for each dataset using topics as features and report average Macro-F1.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>
<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">We use the following datasets in our experiments.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">1. 20 Newsgroups: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset1. This version of the dataset is divided into training (60%) and test (40%) datasets. We construct classifiers on training datasets and evaluate them on test datasets.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">2. SRAA: Simulated/Real/Aviation/AutoUseNet data2: This dataset contains 73,218 UseNet articles from four discussion groups, for simulated auto racing (sim auto), simulated aviation (sim aviation), real autos (real auto), real aviation (real aviation). Following are the three classification tasks associated with this dataset.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p">1. sim auto vs sim aviation vs real auto vs real aviation</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p class="ltx_p">2. auto (sim auto + real auto) vs aviation (sim aviation + real aviation)</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p class="ltx_p">3. simulated (sim auto + sim aviation) vs real (real auto + real aviation)</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p class="ltx_p">We randomly split SRAA dataset such that 80% is used as training data and remaining is used as test data.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p class="ltx_p">3. WebKB: The WebKB dataset3 contains 8145 web pages gathered from university computer science departments. The task is to classify the webpages as student, course, faculty or project. We randomly split this dataset such that 80% is used as training and 20% is used as test data.</p>
</div>
<div id="S5.SS1.p9" class="ltx_para">
<p class="ltx_p">We preprocess these datasets by removing HTML tags and stop-words.</p>
</div>
<div id="S5.SS1.p10" class="ltx_para">
<p class="ltx_p">For various subsets of the 20Newsgroups and WebKB datasets discussed above, we choose number of topics as twice the number of classes. For SRAA dataset we infer 8 topics on the training dataset and label these 8 topics for all the three classification tasks. While labeling a topic, we show its 30 most probable words to the human annotator.</p>
</div>
<div id="S5.SS1.p11" class="ltx_para">
<p class="ltx_p">Similar to (Griffiths and Steyvers, 2004), we set symmetric Dirichlet word prior (βw) for each topic to 0.01 and symmetric Dirichlet topic prior (αt) for each document to 50/T, where T is number of topics. We set K i.e. maximum number of words sprinkled per class to 10.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Results</h3>
<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">Table 2 shows experimental results. We can observe that, TS-LDA performs better than ClassifyLDA in 5 of the total 9 subsets. For the compreligion-sci dataset TS-LDA and ClassifyLDA have the same performance. However, ClassifyLDA performs better than TS-LDA for the three classification tasks of SRAA dataset. We can also observe that, performance of TS-LDA is close to supervised LDA-SVM. We should note here that in TS-LDA, the annotator only labels a few topics and not a single document. Hence, our approach exerts a low cognitive load on the annotator, at the same time achieves text classification performance close to LDA-SVM which needs labeled documents.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Example</h3>
<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">Table 3 shows most prominent words of four topics inferred on the med-space subset of the 20Newsgroup dataset. We can observe here that most prominent words of the first topic do not represent a single class, while other topics represent either med (medical) or space class. We can say here that, these topics are not “coherent”.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">TWe use these labeled topics and create a TSLDA model using the algorithm described in Table 1. Table 4 shows words corresponding to the top two topics of the TS-LDA model. We can observe here that these two topics are more coherent than the topics in Table 3.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p">Hence, we can say here that, in addition to text classification, sprinkling improves coherence of topics.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p class="ltx_p">We should note here that, in ClassifyLDA, the annotator is able to assign a single class label to a topic. If the annotator assigns a wrong class label to a topic representing multiple classes (e.g. first topic in Table 3), then it may affect the performance of the resulting classifier. However, in our approach the annotator can assign multiple class labels to a topic, hence our approach is more flexible for the annotator to encode her domain knowledge efficiently.</p>
</div>
</div>
</div>

<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In this paper we propose a novel algorithm that classifies documents based on class labels over few topics. This reduces the need to label a large
collection of documents. We have used the idea of sprinkling originally proposed in the context of supervised Latent Semantic Analysis, but the
setting here is quite different. Unlike the work in (Chakraborti et al., 2007), we do not assume that we have class labels over the set of training
documents. Instead, to realize our goal of reducing knowledge acquisition overhead, we propose a way of propagating knowledge of few topic labels
to the words and inducing a new topic distribution that has its topics more closely aligned to the class labels. The results show that the approach
can yield performance comparable to entirely supervised settings. In future work, we also envision the possibility of sprinkling knowledge from
background knowledge sources like Wikipedia (Gabrilovich and Markovitch, 2007) to realize an alignment of topics to Wikipedia concepts. We
would like to study effect of change in number of topics on the text classification performance. We will also explore techniques which will help annotators
to encode their domain knowledge efficiently when the topics are not well aligned to the class labels.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_bibblock">David M. Blei and Jon D. McAuliffe. </span>
<span class="ltx_bibblock">2007.</span>
<span class="ltx_bibblock"> Supervised Topic Models.</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In NIPS.</span></span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_bibblock">David M. Blei, Andrew Y. Ng, and Michael I. Jordan. </span>
<span class="ltx_bibblock">2003.</span>
<span class="ltx_bibblock"> Latent Dirichlet Allocation.</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">The Journal of Machine Learning Research, 3:993–1022, March.</span></span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_bibblock">Avrim Blum and Tom Mitchell. </span>
<span class="ltx_bibblock">1998. </span>
<span class="ltx_bibblock">Combining Labeled and Unlabeled Data with Co-Training</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic"> In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100.</span></span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_bibblock">Sutanu Chakraborti, Rahman Mukras, Robert Lothian, Nirmalie Wiratunga, Stuart N. K. Watt, David J. Harper.</span>
<span class="ltx_bibblock">2007. </span>
<span class="ltx_bibblock">Supervised Latent Semantic Indexing Using Adaptive Sprinkling. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In IJCAI, pages 1582-1587.</span></span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_bibblock">Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman.</span>
<span class="ltx_bibblock">1990. </span>
<span class="ltx_bibblock">Indexing by Latent Semantic Analysis. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">JASIS, 41(6):391–407.</span></span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_bibblock">Gregory Druck, Gideon Mann, and Andrew McCallum.</span>
<span class="ltx_bibblock">2008. </span>
<span class="ltx_bibblock">Learning from Labeled Features using Generalized Expectation criteria. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In SIGIR, pages 595–602.</span></span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_bibblock">Gregory Druck, Burr Settles, and Andrew McCallum. </span>
<span class="ltx_bibblock">2009. </span>
<span class="ltx_bibblock">Active Learning by Labeling Features. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In EMNLP, pages 81–90.</span></span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_bibblock">Shantanu Godbole, Abhay Harpale, Sunita Sarawagi, and Soumen Chakrabarti. </span>
<span class="ltx_bibblock">2004. </span>
<span class="ltx_bibblock">Document Classification through Interactive Supervision of Document and Term Labels. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In PKDD, pages 185–196.</span></span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_bibblock">Thomas L. Griffiths and Mark Steyvers. </span>
<span class="ltx_bibblock">2004. </span>
<span class="ltx_bibblock">Finding Scientific Topics. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">PNAS, 101(suppl. 1):5228–5235, April.</span></span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_bibblock">Swapnil Hingmire, Sandeep Chougule, Girish K. Palshikar, and Sutanu Chakraborti. </span>
<span class="ltx_bibblock">2013. </span>
<span class="ltx_bibblock">Document Classification by Topic Labeling. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic"> In SIGIR, pages 877–880.</span></span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_bibblock">Thorsten Joachims. </span>
<span class="ltx_bibblock">1999. </span>
<span class="ltx_bibblock">Transductive Inference for Text Classification using Support Vector Machines. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In ICML, pages 200–209.</span></span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_bibblock">Thorsten Joachims. </span>
<span class="ltx_bibblock">1999. </span>
<span class="ltx_bibblock">Transductive Inference for Text Classification using Support Vector Machines. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In ICML, pages 200–209.</span></span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_bibblock">April Kontostathis and William M. Pottenger. </span>
<span class="ltx_bibblock">2006. </span>
<span class="ltx_bibblock">A Framework for Understanding Latent Semantic Indexing (LSI) Performance. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Inf. Process. Manage., 42(1):56–73, January.</span></span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_bibblock">Simon Lacoste-Julien, Fei Sha, and Michael I. Jordan. </span>
<span class="ltx_bibblock">2008. </span>
<span class="ltx_bibblock"> DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In NIPS. </span></span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_bibblock">Sangno Lee, Jeff Baker, Jaeki Song, and James C. Wetherbe. </span>
<span class="ltx_bibblock">2010. </span>
<span class="ltx_bibblock"> An Empirical Comparison of Four Text Mining Methods. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic"> In Proceedings of the 2010 43rd Hawaii International Conference on System Sciences, pages 1–10.</span></span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_bibblock">Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. </span>
<span class="ltx_bibblock">2004. </span>
<span class="ltx_bibblock">Text Classification by Labeling Words. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In Proceedings of the 19th national conference on Artifical intelligence, pages 425–430.</span></span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_bibblock">Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. </span>
<span class="ltx_bibblock">2000. </span>
<span class="ltx_bibblock">Text Classification from Labeled and Unlabeled Documents using EM. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Machine Learning - Special issue on information retrieval, 39(2-3), May-June.</span></span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_bibblock">Hema Raghavan, Omid Madani, and Rosie Jones. </span>
<span class="ltx_bibblock">2006. </span>
<span class="ltx_bibblock">Active Learning with Feedback on Features and Instances. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">JMLR, 7:1655–1686, December.</span></span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_bibblock">Xiaojin Zhu and Zoubin Ghahramani. </span>
<span class="ltx_bibblock">2002. </span>
<span class="ltx_bibblock"> Learning from Labeled and Unlabeled Data with Label Propagation. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Technical report, Carnegie Mellon University.</span></span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_bibblock">Jun Zhu, Amr Ahmed, and Eric P. Xing. </span>
<span class="ltx_bibblock">2009. </span>
<span class="ltx_bibblock">MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In ICML, pages 1257–1264.</span></span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_bibblock">Evgeniy Gabrilovich and Shaul Markovitch. </span>
<span class="ltx_bibblock">2007. </span>
<span class="ltx_bibblock">Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis. </span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">In IJCAI, pages 1606–1611.</span></span>
</li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:32:43 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
