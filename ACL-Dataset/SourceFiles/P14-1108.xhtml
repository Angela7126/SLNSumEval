<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing</title>
<!--Generated on Wed Jun 11 19:02:05 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on June 11, 2014.-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Generalized Language Model as the Combination of Skipped <math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and Modified Kneser-Ney Smoothing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Rene Pickhardt, Thomas Gottron, Martin Körner, Steffen Staab 
<br class="ltx_break"/>Institute for Web Science and Technologies,
<br class="ltx_break"/>University of Koblenz-Landau, Germany
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">rpickhardt,gottron,mkoerner,staab</span>}<span class="ltx_text ltx_font_typewriter">@uni-koblenz.de</span> 
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Georg Wagner and Till Speicher 
<br class="ltx_break"/>Typology GbR 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">mail@typology.de</span>

</span></span></div>
<div class="ltx_date ltx_role_creation">June 11, 2014</div>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We introduce a novel approach for building language models based on a systematic, recursive exploration of skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models which are interpolated using modified Kneser-Ney smoothing.
Our approach generalizes language models as it contains the classical interpolation with lower order models as a special case.
In this paper we motivate, formalize and present our approach.
In an extensive empirical experiment over English text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between <math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="3.1\%" display="inline"><mrow><mn>3.1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="12.7\%" display="inline"><mrow><mn>12.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> in comparison to traditional language models using modified Kneser-Ney smoothing.
Furthermore, we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements.
Finally, we also show that the strength of our approach lies in its ability to cope in particular with sparse training data.
Using a very small training data set of only <math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="736" display="inline"><mn>736</mn></math> KB text we yield improvements of even <math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="25.7\%" display="inline"><mrow><mn>25.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> reduction of perplexity.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction motivation</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Language Models are a probabilistic approach for predicting the occurrence of a sequence of words.
They are used in many applications, e.g. word prediction <cite class="ltx_cite">[]</cite>, speech recognition <cite class="ltx_cite">[]</cite>, machine translation <cite class="ltx_cite">[]</cite>, or spelling correction <cite class="ltx_cite">[]</cite>.
The task language models attempt to solve is the estimation of a probability of a given sequence of words <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m1" class="ltx_Math" alttext="w_{1}^{l}=w_{1},\dots,w_{l}" display="inline"><mrow><msubsup><mi>w</mi><mn>1</mn><mi>l</mi></msubsup><mo>=</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>w</mi><mi>l</mi></msub></mrow></mrow></math>.
The probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p1.m2" class="ltx_Math" alttext="P(w_{1}^{l})" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mn>1</mn><mi>l</mi></msubsup><mo>)</mo></mrow></mrow></math> of this sequence can be broken down into a product of conditional probabilities:</p>
</div>
<div id="S1.p2" class="ltx_para">
<table id="A1.EGx1" class="ltx_equationgroup ltx_eqn_align">

<tr id="S1.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex1.m1" class="ltx_Math" alttext="\displaystyle P(w_{1}^{l})=" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mn>1</mn><mi>l</mi></msubsup><mo>)</mo></mrow></mrow><mo>=</mo><mi/></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.Ex1.m2" class="ltx_Math" alttext="\displaystyle P(w_{1})\cdot P(w_{2}|w_{1})\cdot\ldots\cdot P(w_{l}|w_{1}\cdots&#10;w%&#10;_{l-1})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mn>1</mn></msub><mo>)</mo></mrow><mo>⋅</mo><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mn>2</mn></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mo>)</mo></mrow><mo>⋅</mo><mi mathvariant="normal">…</mi><mo>⋅</mo><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>l</mi></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mi mathvariant="normal">⋯</mi><msub><mi>w</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S1.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.E1.m1" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.E1.m2" class="ltx_Math" alttext="\displaystyle\prod_{i=1}^{l}P(w_{i}|w_{1}\cdots w_{i-1})" display="inline"><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover></mstyle><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mi>w</mi><mn>1</mn></msub><mi mathvariant="normal">⋯</mi><msub><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Because of combinatorial explosion and data sparsity, it is very difficult to reliably estimate the probabilities that are conditioned on a longer subsequence.
Therefore, by making a Markov assumption the true probability of a word sequence is only approximated by restricting conditional probabilities to depend only on a local context <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p3.m1" class="ltx_Math" alttext="w_{i-n+1}^{i-1}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math> of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p3.m2" class="ltx_Math" alttext="n-1" display="inline"><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></math> preceding words rather than the full sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p3.m3" class="ltx_Math" alttext="w^{i-1}_{1}" display="inline"><msubsup><mi>w</mi><mn>1</mn><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math>.
The challenge in the construction of language models is to provide reliable estimators for the conditional probabilities.
While the estimators can be learnt—using, e.g., a maximum likelihood estimator over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p3.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams obtained from training data—the obtained values are not very reliable for events which may have been observed only a few times or not at all in the training data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Smoothing is a standard technique to overcome this data sparsity problem.
Various smoothing approaches have been developed and applied in the context of language models.
Chen and Goodman <cite class="ltx_cite">[]</cite> introduced modified Kneser-Ney Smoothing, which up to now has been considered the state-of-the-art method for language modelling over the last 15 years.
Modified Kneser-Ney Smoothing is an interpolating method which combines the estimated conditional probabilities <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m1" class="ltx_Math" alttext="P(w_{i}|w_{i-n+1}^{i-1})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math> recursively with lower order models involving a shorter local context <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m2" class="ltx_Math" alttext="w_{i-n+2}^{i-1}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math> and their estimate for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p4.m3" class="ltx_Math" alttext="P(w_{i}|w_{i-n+2}^{i-1})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math>.
The motivation for using lower order models is that shorter contexts may be observed more often and, thus, suffer less from data sparsity.
However, a single rare word towards the end of the local context will always cause the context to be observed rarely in the training data and hence will lead to an unreliable estimation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Because of Zipfian word distributions, most words occur very rarely and hence their true probability of occurrence may be estimated only very poorly.
One word that appears at the end of a local context <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p5.m1" class="ltx_Math" alttext="w^{i-1}_{i-n+1}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math> and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths — leading to severe errors even for smoothed language models.
Thus, the idea motivating our approach is to involve several lower order models which systematically leave out one position in the context (one may think of replacing the affected word in the context with a wildcard) instead of shortening the sequence only by one word at the beginning.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">This concept of introducing gaps in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p6.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams is referred to as skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p6.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams <cite class="ltx_cite">[]</cite>.
Among other techniques, skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p6.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams have also been considered as an approach to overcome problems of data sparsity <cite class="ltx_cite">[]</cite>.
However, to best of our knowledge, language models making use of skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S1.p6.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams models have never been investigated to their full extent and over different levels of lower order models.
Our approach differs as we consider all possible combinations of gaps in a local context and interpolate the higher order model with all possible lower order models derived from adding gaps in all different ways.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p">In this paper we make the following contributions:</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">We provide a framework for using modified Kneser-Ney smoothing in combination with a systematic exploration of lower order models based on skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">We show how our novel approach can indeed easily be interpreted as a generalized version of the current state-of-the-art language models.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">We present a large scale empirical analysis of our generalized language models on eight data sets spanning four different languages, namely, a wikipedia-based text corpus and the JRC-Acquis corpus of legislative texts.</p>
</div></li>
<li id="I1.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">4.</span> 
<div id="I1.i4.p1" class="ltx_para">
<p class="ltx_p">We empirically observe that introducing skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i4.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models may reduce perplexity by <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i4.p1.m2" class="ltx_Math" alttext="12.7\%" display="inline"><mrow><mn>12.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> compared to the current state-of-the-art using modified Kneser-Ney models on large data sets.
Using small training data sets we observe even higher reductions of perplexity of up to <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i4.p1.m3" class="ltx_Math" alttext="25.6\%" display="inline"><mrow><mn>25.6</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>.</p>
</div></li>
</ol>
</div>
<div id="S1.p8" class="ltx_para">
<p class="ltx_p">The rest of the paper is organized as follows.
We start with reviewing related work in Section <a href="#S2" title="2 Related Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We will then introduce our generalized language models in Section <a href="#S3" title="3 Generalized Language Models ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
After explaining the evaluation methodology and introducing the data sets in Section <a href="#S4" title="4 Experimental Setup and Data Sets ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we will present the results of our evaluation in Section <a href="#S5" title="5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
In Section  <a href="#S6" title="6 Discussion ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we discuss why a generalized language model performs better than a standard language model.
Finally, in Section <a href="#S7" title="7 Conclusion and Future Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we summarize our findings and conclude with an overview of further interesting research challenges in the field of generalized language models.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Work related to our generalized language model approach can be divided in two categories: various smoothing techniques for language models and approaches making use of skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Smoothing techniques for language models have a long history.
Their aim is to overcome data sparsity and provide more reliable estimators—in particular for rare events.
The Good Turing estimator <cite class="ltx_cite">[]</cite>, deleted interpolation <cite class="ltx_cite">[]</cite>, Katz backoff <cite class="ltx_cite">[]</cite> and Kneser-Ney smoothing <cite class="ltx_cite">[]</cite> are just some of the approaches to be mentioned.
Common strategies of these approaches are to either backoff to lower order models when a higher order model lacks sufficient training data for good estimation, to interpolate between higher and lower order models or to interpolate with a prior distribution.
Furthermore, the estimation of the amount of unseen events from rare events aims to find the right weights for interpolation as well as for discounting probability mass from unreliable estimators and to retain it for unseen events.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">The state of the art is a modified version of Kneser-Ney smoothing introduced in <cite class="ltx_cite">[]</cite>.
The modified version implements a recursive interpolation with lower order models, making use of different discount values for more or less frequently observed events.
This variation has been compared to other smoothing techniques on various corpora and has shown to outperform competing approaches.
We will review modified Kneser-Ney smoothing in Section <a href="#S2.SS1" title="2.1 Review of Modified Kneser-Ney Smoothing ‣ 2 Related Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> in more detail as we reuse some ideas to define our generalized language model.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Smoothing techniques which do not rely on using lower order models involve clustering <cite class="ltx_cite">[]</cite>, i.e. grouping together similar words to form classes of words, as well as skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams <cite class="ltx_cite">[]</cite>.
Yet other approaches make use of permutations of the word order in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">Skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams are typically used to incorporate long distance relations between words.
Introducing the possibility of gaps between the words in an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram allows for capturing word relations beyond the level of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> consecutive words without an exponential increase in the parameter space.
However, with their restriction on a subsequence of words, skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams are also used as a technique to overcome data sparsity <cite class="ltx_cite">[]</cite>.
In related work different terminology and different definitions have been used to describe skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.
Variations modify the number of words which can be skipped between elements in an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram as well as the manner in which the skipped words are determined (e.g. fixed patterns <cite class="ltx_cite">[]</cite> or functional words <cite class="ltx_cite">[]</cite>).</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">The impact of various extensions and smoothing techniques for language models is investigated in <cite class="ltx_cite">[]</cite>.
In particular, the authors compared Kneser-Ney smoothing, Katz backoff smoothing, caching, clustering, inclusion of higher order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams, sentence mixture and skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.
They also evaluated combinations of techniques, for instance, using skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models in combination with Kneser-Ney smoothing.
The experiments in this case followed two paths: (1) interpolating a <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m4" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math>-gram model with lower order distribution introducing a single gap and (2) interpolating higher order models with skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams which retained only combinations of two words.
Goodman reported on small data sets and in the best case a moderate improvement of cross entropy in the range of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m6" class="ltx_Math" alttext="0.02" display="inline"><mn>0.02</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m7" class="ltx_Math" alttext="0.04" display="inline"><mn>0.04</mn></math>.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">In <cite class="ltx_cite">[]</cite>, the authors investigated the increase of observed word combinations when including skips in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.
The conclusion was that using skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams is often more effective for increasing the number of observations than increasing the corpus size.
This observation aligns well with our experiments.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Review of Modified Kneser-Ney Smoothing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">We briefly recall modified Kneser-Ney Smoothing as presented in <cite class="ltx_cite">[]</cite>.
Modified Kneser-Ney implements smoothing by interpolating between higher and lower order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram language models.
The highest order distribution is interpolated with lower order distribution as follows:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<table id="A1.EGx2" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m1" class="ltx_Math" alttext="\displaystyle P_{\text{MKN}}" display="inline"><msub><mi>P</mi><mtext mathsize="small" stretchy="false">MKN</mtext></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m2" class="ltx_Math" alttext="\displaystyle(w_{i}|w_{i-n+1}^{i-1})=" display="inline"><mrow><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>=</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m2" class="ltx_Math" alttext="\displaystyle\frac{\text{max}\{c(w_{i-n+1}^{i})-D(c(w_{i-n+1}^{i})),0\}}{c(w_{%&#10;i-n+1}^{i-1})}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mtext mathsize="small" stretchy="false">max</mtext><mo>⁢</mo><mrow><mo>{</mo><mrow><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m2" class="ltx_Math" alttext="\displaystyle+\gamma_{high}(w_{i-n+1}^{i-1}){\hat{P}}_{\text{MKN}}(w_{i}|w_{i-%&#10;n+2}^{i-1})" display="inline"><mrow><mo>+</mo><msub><mi>γ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>h</mi></mrow></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">MKN</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m1" class="ltx_Math" alttext="c(w_{i-n+1}^{i})" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow></math> provides the frequency count that sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m2" class="ltx_Math" alttext="w_{i-n+1}^{i}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></math> occurs in training data, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m3" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> is a discount value (which depends on the frequency of the sequence) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m4" class="ltx_Math" alttext="\gamma_{high}" display="inline"><msub><mi>γ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>h</mi></mrow></msub></math> depends on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m5" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> and is the interpolation factor to mix in the lower order distribution<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>The factors <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m6" class="ltx_Math" alttext="\gamma" display="inline"><mi>γ</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p3.m7" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> are quite technical and lengthy. As they do not play a significant role for understanding our novel approach we refer to Appendix <a href="#A1" title="Appendix A Discount Values and Weights in Modified Kneser Ney ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> for details.</span></span></span>.
Essentially, interpolation with a lower order model corresponds to leaving out the first word in the considered sequence.
The lower order models are computed differently using the notion of continuation counts rather than absolute counts:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<table id="A1.EGx3" class="ltx_equationgroup ltx_eqn_align">

<tr id="S2.Ex4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle{\hat{P}}_{\text{MKN}}" display="inline"><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">MKN</mtext></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m2" class="ltx_Math" alttext="\displaystyle(w_{i}|(w_{i-n+1}^{i-1}))=" display="inline"><mrow><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>)</mo></mrow><mo>=</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.Ex5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle\frac{\text{max}\{N_{1+}(\bullet w_{i-n+1}^{i})-D(c(w_{i-n+1}^{i}%&#10;)),0\}}{N_{1+}(\bullet w_{i-n+1}^{i-1}\bullet)}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mtext mathsize="small" stretchy="false">max</mtext><mrow><mo>{</mo><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mo>∙</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><mi>c</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow><mo>)</mo></mrow><mo>,</mo><mn>0</mn><mo>}</mo></mrow></mrow><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mo>∙</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S2.E3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m2" class="ltx_Math" alttext="\displaystyle+\gamma_{mid}(w_{i-n+1}^{i-1}){\hat{P}}_{\text{MKN}}(w_{i}|w_{i-n%&#10;+2}^{i-1}))" display="inline"><mrow><mo>+</mo><msub><mi>γ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">MKN</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p class="ltx_p">where the continuation counts are defined as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p5.m1" class="ltx_Math" alttext="N_{1+}(\bullet w_{i-n+1}^{i})=|\{w_{i-n}:c(w_{i-n}^{i})&gt;0\}|" display="inline"><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mo>∙</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow><mo>=</mo><mo>|</mo><mrow><mo>{</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow></msub><mo>:</mo><mi>c</mi><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow><mo>&gt;</mo><mn>0</mn><mo>}</mo></mrow><mo>|</mo></mrow></math>, i.e. the number of different words which precede the sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p5.m2" class="ltx_Math" alttext="w_{i-n+1}^{i}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></math>.
The term <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p5.m3" class="ltx_Math" alttext="\gamma_{mid}" display="inline"><msub><mi>γ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub></math> is again an interpolation factor which depends on the discounted probability mass <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p5.m4" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> in the first term of the formula.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Generalized Language Models</h2>

<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Notation for Skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> Skips</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">We express skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams using an operator notation.
The operator <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m2" class="ltx_Math" alttext="\partial_{i}" display="inline"><msub><mo>∂</mo><mi>i</mi></msub></math> applied to an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram removes the word at the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m4" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math>-th position.
For instance: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m5" class="ltx_Math" alttext="\partial_{3}w_{1}w_{2}w_{3}w_{4}=w_{1}w_{2}\_w_{4}" display="inline"><mrow><mrow><msub><mo>∂</mo><mn>3</mn></msub><mo>⁡</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></mrow><mo>=</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m6" class="ltx_Math" alttext="\_" display="inline"><mi mathvariant="normal">_</mi></math> is used as wildcard placeholder to indicate a removed word.
The wildcard operator allows for larger number of matches. For instance, when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m7" class="ltx_Math" alttext="c(w_{1}w_{2}w_{3a}w_{4})=x" display="inline"><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mrow><mn>3</mn><mo>⁢</mo><mi>a</mi></mrow></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mi>x</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m8" class="ltx_Math" alttext="c(w_{1}w_{2}w_{3b}w_{4})=y" display="inline"><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mrow><mn>3</mn><mo>⁢</mo><mi>b</mi></mrow></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mi>y</mi></mrow></math> then <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m9" class="ltx_Math" alttext="c(w_{1}w_{2}\_w4)\geq x+y" display="inline"><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi>w</mi><mo>⁢</mo><mn>4</mn></mrow><mo>)</mo></mrow></mrow><mo>≥</mo><mrow><mi>x</mi><mo>+</mo><mi>y</mi></mrow></mrow></math> since at least the two sequences <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m10" class="ltx_Math" alttext="w_{1}w_{2}w_{3a}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mrow><mn>3</mn><mo>⁢</mo><mi>a</mi></mrow></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m11" class="ltx_Math" alttext="w_{1}w_{2}w_{3b}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mrow><mn>3</mn><mo>⁢</mo><mi>b</mi></mrow></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math> match the sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m12" class="ltx_Math" alttext="w_{1}w_{2}\_w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math>.
In order to align with standard language models the skip operator applied to the first word of a sequence will remove the word instead of introducing a wildcard.
In particular the equation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m13" class="ltx_Math" alttext="\partial_{1}w_{i-n+1}^{i}=w_{i-n+2}^{i}" display="inline"><mrow><mrow><msub><mo>∂</mo><mn>1</mn></msub><mo>⁡</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow><mo>=</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mi>i</mi></msubsup></mrow></math> holds where the right hand side is the subsequence of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m14" class="ltx_Math" alttext="w_{i-n+1}^{i}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></math> omitting the first word.
We can thus formulate the interpolation step of modified Kneser-Ney smoothing using our notation as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p1.m15" class="ltx_Math" alttext="{\hat{P}}_{\text{MKN}}(w_{i}|w_{i-n+2}^{i-1})={\hat{P}}_{\text{MKN}}(w_{i}|%&#10;\partial_{1}w_{i-n+1}^{i-1})" display="inline"><mrow><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext>MKN</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>2</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>=</mo><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext>MKN</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mo>∂</mo><mn>1</mn></msub><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">Thus, our skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams correspond to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams of which we only use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m3" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> words, after having applied
the skip operators <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS1.p2.m4" class="ltx_Math" alttext="\partial_{i_{1}}\dots\partial_{i_{n-k}}" display="inline"><mrow><mrow><msub><mo>∂</mo><msub><mi>i</mi><mn>1</mn></msub></msub><mo>⁡</mo><mi mathvariant="normal">…</mi></mrow><mo>⁢</mo><msub><mo>∂</mo><msub><mi>i</mi><mrow><mi>n</mi><mo>-</mo><mi>k</mi></mrow></msub></msub></mrow></math></p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Generalized Language Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">Interpolation with lower order models is motivated by the problem of data sparsity in higher order models.
However, lower order models omit only the first word in the local context, which might not necessarily be the cause for the overall <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram to be rare.
This is the motivation for our generalized language models to not only interpolate with one lower order model, where the first word in a sequence is omitted, but also with all other skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models, where one word is left out.
Combining this idea with modified Kneser-Ney smoothing leads to a formula similar to (<a href="#S2.Ex2" title="2.1 Review of Modified Kneser-Ney Smoothing ‣ 2 Related Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<table id="A1.EGx4" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.Ex6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex6.m1" class="ltx_Math" alttext="\displaystyle P_{\text{GLM}}" display="inline"><msub><mi>P</mi><mtext mathsize="small" stretchy="false">GLM</mtext></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex6.m2" class="ltx_Math" alttext="\displaystyle(w_{i}|w_{i-n+1}^{i-1})=" display="inline"><mrow><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>=</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S3.Ex7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex7.m2" class="ltx_Math" alttext="\displaystyle\frac{\text{max}\{c(w_{i-n+1}^{i})-D(c(w_{i-n+1}^{i})),0\}}{c(w_{%&#10;i-n+1}^{i-1})}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mtext mathsize="small" stretchy="false">max</mtext><mo>⁢</mo><mrow><mo>{</mo><mrow><mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S3.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m2" class="ltx_Math" alttext="\displaystyle+\gamma_{high}(w_{i-n+1}^{i-1})\sum_{j=1}^{n-1}\frac{1}{n\!-\!1}{%&#10;\hat{P}}_{\text{GLM}}(w_{i}|\partial_{j}w_{i-n+1}^{i-1})" display="inline"><mrow><mo>+</mo><msub><mi>γ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>h</mi></mrow></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></mfrac></mstyle><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">GLM</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mo>∂</mo><mi>j</mi></msub><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">The difference between formula (<a href="#S2.Ex2" title="2.1 Review of Modified Kneser-Ney Smoothing ‣ 2 Related Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>) and formula (<a href="#S3.Ex6" title="3.2 Generalized Language Model ‣ 3 Generalized Language Models ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) is the way in which lower order models are interpolated.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p class="ltx_p">Note, the sum over all possible positions in the context <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m1" class="ltx_Math" alttext="w_{i-n+1}^{i-1}" display="inline"><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup></math> for which we can skip a word and the according lower order models <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m2" class="ltx_Math" alttext="P_{\text{GLM}}(w_{i}|\partial_{j}(w_{i-n+1}^{i-1}))" display="inline"><mrow><msub><mi>P</mi><mtext>GLM</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mo>∂</mo><mi>j</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>)</mo></mrow></mrow></math>.
We give all lower order models the same weight <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m3" class="ltx_Math" alttext="\frac{1}{n-1}" display="inline"><mfrac><mn>1</mn><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac></math>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p class="ltx_p">The same principle is recursively applied in the lower order models in which some words of the full <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p5.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram are already skipped.
As in modified Kneser-Ney smoothing we use continuation counts for the lower order models, incorporating the skip operator also for these counts.
Incorporating this directly into modified Kneser-Ney smoothing leads in the second highest model to:</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<table id="A1.EGx5" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="\displaystyle{\hat{P}}_{\text{GLM}}" display="inline"><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">GLM</mtext></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m2" class="ltx_Math" alttext="\displaystyle(w_{i}|\partial_{j}(w_{i\!-\!n\!+\!1}^{i-1}))=" display="inline"><mrow><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mo>∂</mo><mi>j</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>)</mo></mrow><mo>=</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
<tr id="S3.Ex8" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex8.m2" class="ltx_Math" alttext="\displaystyle\frac{\text{max}\{N_{1+}(\partial_{j}(w_{i\!-\!n}^{i}))-D(c(%&#10;\partial_{j}(w_{i\!-\!n\!+\!1}^{i}))),0\}}{N_{1+}(\partial_{j}(w_{i\!-\!n\!+\!%&#10;1}^{i\!-\!1})\bullet)}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mtext mathsize="small" stretchy="false">max</mtext><mo>⁢</mo><mrow><mo>{</mo><mrow><mrow><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mo>∂</mo><mi>j</mi></msub><mo>⁡</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mo>∂</mo><mi>j</mi></msub><mo>⁡</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mn>0</mn></mrow><mo>}</mo></mrow></mrow><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><msub><mo>∂</mo><mi>j</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>∙</mo><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr id="S3.Ex9" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex9.m2" class="ltx_Math" alttext="\displaystyle+\!\gamma_{mid}(\partial_{j}(w_{i\!-\!n\!+\!1}^{i\!-\!1}))\!\sum_%&#10;{k=1\atop k\neq j}^{n-1}\frac{1}{n\!-\!2}{\hat{P}}_{\text{GLM}}(w_{i}|\partial%&#10;_{j}\partial_{k}(w_{i\!-\!n\!+\!1}^{i\!-\!1}))" display="inline"><mrow><mo rspace="0.8pt">+</mo><msub><mi>γ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub><mrow><mo>(</mo><msub><mo>∂</mo><mi>j</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo rspace="0.8pt">)</mo></mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mstyle scriptlevel="+1"><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd><mrow><mi>k</mi><mo>≠</mo><mi>j</mi></mrow></mtd></mtr></mtable></mstyle><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover></mstyle><mstyle displaystyle="true"><mfrac><mn>1</mn><mrow><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">-</mo><mn>2</mn></mrow></mfrac></mstyle><msub><mover accent="true"><mi>P</mi><mo stretchy="false">^</mo></mover><mtext mathsize="small" stretchy="false">GLM</mtext></msub><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>|</mo><msub><mo>∂</mo><mi>j</mi></msub><msub><mo>∂</mo><mi>k</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p class="ltx_p">Given that we skip words at different positions, we have to extend the notion of the count function and the continuation counts.
The count function applied to a skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p7.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram is given by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p7.m2" class="ltx_Math" alttext="c(\partial_{j}(w_{\!i-\!n}^{i}))\!=\!\sum_{w_{j}}c(w_{\!i-\!n}^{i})" display="inline"><mrow><mrow><mi>c</mi><mo>⁢</mo><mpadded width="-1.7pt"><mrow><mo>(</mo><mrow><msub><mo>∂</mo><mi>j</mi></msub><mo>⁡</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded lspace="-1.7pt" width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mpadded></mrow><mo rspace="0.8pt">=</mo><mrow><msub><mo largeop="true" symmetric="true">∑</mo><msub><mi>w</mi><mi>j</mi></msub></msub><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded lspace="-1.7pt" width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow></mrow></mrow></math>, i.e. we aggregate the count information over all words which fill the gap in the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p7.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram.
Regarding the continuation counts we define:</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<table id="A1.EGx6" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E6" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="\displaystyle N_{1+}(\partial_{j}(w_{\!i-\!n}^{i}))" display="inline"><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mo>∂</mo><mi>j</mi></msub><mo>⁡</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded lspace="-1.7pt" width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m3" class="ltx_Math" alttext="\displaystyle|\{w_{i\!-\!n\!+\!j\!-\!1}\!:\!c(w_{i\!-\!n}^{i})\!&gt;\!0\}|" display="inline"><mrow><mo fence="true">|</mo><mrow><mo>{</mo><mrow><mpadded width="-1.7pt"><msub><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mpadded width="-1.7pt"><mi>j</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msub></mpadded><mo separator="true">:</mo><mrow><mrow><mi>c</mi><mo>⁢</mo><mpadded width="-1.7pt"><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mpadded></mrow><mo rspace="0.8pt">&gt;</mo><mn>0</mn></mrow></mrow><mo>}</mo></mrow><mo fence="true">|</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
<tr id="S3.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m1" class="ltx_Math" alttext="\displaystyle N_{1+}(\partial_{j}(w_{i\!-\!n}^{i\!-\!1})\bullet)" display="inline"><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><msub><mo>∂</mo><mi>j</mi></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>∙</mo><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m3" class="ltx_Math" alttext="\displaystyle|\{(w_{i\!-\!n\!+\!j\!-\!1},w_{i})\!:\!c(w_{i\!-\!n}^{i})\!&gt;\!0\}|" display="inline"><mrow><mo fence="true">|</mo><mrow><mo>{</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mpadded width="-1.7pt"><mi>j</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mo separator="true">:</mo><mrow><mrow><mi>c</mi><mo>⁢</mo><mpadded width="-1.7pt"><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mi>n</mi></mrow><mi>i</mi></msubsup><mo>)</mo></mrow></mpadded></mrow><mo rspace="0.8pt">&gt;</mo><mn>0</mn></mrow></mrow><mo>}</mo></mrow><mo fence="true">|</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
</div>
<div id="S3.SS2.p9" class="ltx_para">
<p class="ltx_p">As lowest order model we use—just as done for traditional modified Kneser-Ney <cite class="ltx_cite">[]</cite>—a unigram model interpolated with a uniform distribution for unseen words.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p class="ltx_p">The overall process is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Generalized Language Model ‣ 3 Generalized Language Models ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, illustrating how the higher level models are recursively smoothed with several lower order ones.</p>
</div>
<div id="S3.F1" class="ltx_figure"><img src="P14-1108/image001.png" id="S3.F1.g1" class="ltx_graphics ltx_centering" width="607" height="313" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Interpolation of models of different order and using skip patterns.
The value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F1.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> indicates the length of the raw <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F1.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams necessary for computing the model, the value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F1.m7" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> indicates the number of words actually used in the model.
The wild card symbol _ marks skipped words in an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F1.m8" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram.
The arrows indicate how a higher order model is interpolated with lower order models which skips one word.
The bold arrows correspond to interpolation of models in traditional modified Kneser-Ney smoothing.
The lighter arrows illustrate the additional interpolations introduced by our generalized language models.
</div>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup and Data Sets</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">To evaluate the quality of our generalized language models we empirically compare their ability to explain sequences of words.
To this end we use text corpora, split them into test and training data, build language models as well as generalized language models over the training data and apply them on the test data.
We employ established metrics, such as cross entropy and perplexity.
In the following we explain the details of our experimental setup.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Sets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">For evaluation purposes we employed eight different data sets.
The data sets cover different domains and languages.
As languages we considered English (<em class="ltx_emph">en</em>), German (<em class="ltx_emph">de</em>), French (<em class="ltx_emph">fr</em>), and Italian (<em class="ltx_emph">it</em>).
As general domain data set we used the full collection of articles from Wikipedia (<em class="ltx_emph">wiki</em>) in the corresponding languages.
The download dates of the dumps are displayed in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Data Sets ‣ 4 Experimental Setup and Data Sets ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle" style="width:203.8014pt;">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">de</td>
<td class="ltx_td ltx_align_center ltx_border_r">en</td>
<td class="ltx_td ltx_align_center ltx_border_r">fr</td>
<td class="ltx_td ltx_align_center">it</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Nov 22<sup class="ltx_sup">nd</sup></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Nov 04<sup class="ltx_sup">th</sup></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Nov 20<sup class="ltx_sup">th</sup></td>
<td class="ltx_td ltx_align_center ltx_border_t">Nov 25<sup class="ltx_sup">th</sup></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Download dates of Wikipedia snapshots in November 2013.</div>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">Special purpose domain data are provided by the multi-lingual JRC-Acquis corpus of legislative texts (<em class="ltx_emph">JRC</em>) <cite class="ltx_cite">[]</cite>.
Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Data Sets ‣ 4 Experimental Setup and Data Sets ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives an overview of the data sets and provides some simple statistics of the covered languages and the size of the collections.</p>
</div>
<div id="S4.T2" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:195.129pt;">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Statistics</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>Corpus</th>
<td class="ltx_td ltx_align_center">total words</td>
<td class="ltx_td ltx_align_center">unique words</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span></th>
<td class="ltx_td ltx_align_center">in Mio.</td>
<td class="ltx_td ltx_align_center">in Mio.</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-de</th>
<td class="ltx_td ltx_align_center ltx_border_t">579</td>
<td class="ltx_td ltx_align_center ltx_border_t">9.82</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-de</th>
<td class="ltx_td ltx_align_center">30.9</td>
<td class="ltx_td ltx_align_center">0.66</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-en</th>
<td class="ltx_td ltx_align_center ltx_border_t">1689</td>
<td class="ltx_td ltx_align_center ltx_border_t">11.7</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-en</th>
<td class="ltx_td ltx_align_center">39.2</td>
<td class="ltx_td ltx_align_center">0.46</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-fr</th>
<td class="ltx_td ltx_align_center ltx_border_t">339</td>
<td class="ltx_td ltx_align_center ltx_border_t">4.06</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-fr</th>
<td class="ltx_td ltx_align_center">35.8</td>
<td class="ltx_td ltx_align_center">0.46</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-it</th>
<td class="ltx_td ltx_align_center ltx_border_t">193</td>
<td class="ltx_td ltx_align_center ltx_border_t">3.09</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span>JRC-it</th>
<td class="ltx_td ltx_align_center ltx_border_bb">34.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.47</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Word statistics and size of of evaluation corpora</div>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p">The data sets come in the form of structured text corpora which we cleaned from markup and tokenized to generate word sequences.
We filtered the word tokens by removing all character sequences which did not contain any letter, digit or common punctuation marks.
Eventually, the word token sequences were split into word sequences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> which provided the basis for the training and test sets for all algorithms.
Note that we did not perform case-folding nor did we apply stemming algorithms to normalize the word forms.
Also, we did our evaluation using case sensitive training and test data.
Additionally, we kept all tokens for named entities such as names of persons or places.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Methodology</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">All data sets have been randomly split into a training and a test set on a sentence level.
The training sets consist of 80% of the sentences, which have been used to derive <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams, skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and corresponding continuation counts for values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> between 1 and 5.
Note that we have trained a prediction model for each data set individually.
From the remaining 20% of the sequences we have randomly sampled a separate set of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m4" class="ltx_Math" alttext="100,000" display="inline"><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow></math> sequences of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m5" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math> words each.
These test sequences have also been shortened to sequences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m6" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p1.m7" class="ltx_Math" alttext="4" display="inline"><mn>4</mn></math> and provide a basis to conduct our final experiments to evaluate the performance of the different algorithms.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">We learnt the generalized language models on the same split of the training corpus as the standard language model using modified Kneser-Ney smoothing and we also used the same set of test sequences for a direct comparison.
To ensure rigour and openness of research the data set for training as well as the test sequences and the entire source code is open source. <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>http://west.uni-koblenz.de/Research</span></span></span> <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>https://github.com/renepickhardt/generalized-language-modeling-toolkit</span></span></span> <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>http://glm.rene-pickhardt.de</span></span></span>
We compared the probabilities of our language model implementation (which is a subset of the generalized language model) using KN as well as MKN smoothing with the Kyoto Language Model Toolkit <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>http://www.phontron.com/kylm/</span></span></span>. Since we got the same results for small <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> and small data sets we believe that our implementation is correct.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">In a second experiment we have investigated the impact of the size of the training data set.
The wikipedia corpus consists of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m1" class="ltx_Math" alttext="1.7" display="inline"><mn>1.7</mn></math> bn. words.
Thus, the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m2" class="ltx_Math" alttext="80\%" display="inline"><mrow><mn>80</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> split for training consists of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m3" class="ltx_Math" alttext="1.3" display="inline"><mn>1.3</mn></math> bn. words.
We have iteratively created smaller training sets by decreasing the split factor by an order of magnitude.
So we created <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m4" class="ltx_Math" alttext="8\%" display="inline"><mrow><mn>8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> / <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m5" class="ltx_Math" alttext="92\%" display="inline"><mrow><mn>92</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m6" class="ltx_Math" alttext="0.8\%" display="inline"><mrow><mn>0.8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> / <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m7" class="ltx_Math" alttext="99.2\%" display="inline"><mrow><mn>99.2</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> split, and so on.
We have stopped at the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m8" class="ltx_Math" alttext="0.008\%/99.992\%" display="inline"><mrow><mrow><mrow><mn>0.008</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow><mo>/</mo><mn>99.992</mn></mrow><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> split as the training data set in this case consisted of less words than our 100k test sequences which we still randomly sampled from the test data of each split.
Then we trained a generalized language model as well as a standard language model with modified Kneser-Ney smoothing on each of these samples of the training data.
Again we have evaluated these language models on the same random sample of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m9" class="ltx_Math" alttext="100,000" display="inline"><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow></math> sequences as mentioned above.</p>
</div>
</div>
<div id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation Metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">As evaluation metric we use <em class="ltx_emph">perplexity</em>: a standard measure in the field of language models  <cite class="ltx_cite">[]</cite>.
First we calculate the <em class="ltx_emph">cross entropy</em> of a trained language model given a test set using</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<table id="S4.E8" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E8.m1" class="ltx_Math" alttext="H(P_{\tt{alg}})=-\sum_{s\in T}P_{\tt{MLE}}(s)\cdot\log_{2}{P_{\tt{alg}}(s)}" display="block"><mrow><mrow><mi>H</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>P</mi><mi>𝚊𝚕𝚐</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>s</mi><mo>∈</mo><mi>T</mi></mrow></munder><mrow><mrow><mrow><msub><mi>P</mi><mi>𝙼𝙻𝙴</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow></mrow><mo>⋅</mo><mrow><msub><mi>log</mi><mn>2</mn></msub><mo>⁡</mo><msub><mi>P</mi><mi>𝚊𝚕𝚐</mi></msub></mrow></mrow><mo>⁢</mo><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
</table>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p">Where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p3.m1" class="ltx_Math" alttext="P_{\tt{alg}}" display="inline"><msub><mi>P</mi><mi>𝚊𝚕𝚐</mi></msub></math> will be replaced by the probability estimates provided by our generalized language models and the estimates of a language model using modified Kneser-Ney smoothing.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p3.m2" class="ltx_Math" alttext="P_{\tt{MLE}}" display="inline"><msub><mi>P</mi><mi>𝙼𝙻𝙴</mi></msub></math>, instead, is a maximum likelihood estimator of the test sequence to occur in the test corpus.
Finally, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p3.m3" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is the set of test sequences.
The perplexity is defined as:</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<table id="S4.E9" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E9.m1" class="ltx_Math" alttext="\textit{Perplexity}(P_{\tt{alg}})=2^{H(P_{\tt{alg}})}" display="block"><mrow><mrow><mtext>𝑃𝑒𝑟𝑝𝑙𝑒𝑥𝑖𝑡𝑦</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>P</mi><mi>𝚊𝚕𝚐</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><msup><mn>2</mn><mrow><mi>H</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>P</mi><mi>𝚊𝚕𝚐</mi></msub><mo>)</mo></mrow></mrow></msup></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
</table>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p class="ltx_p">Lower perplexity values indicate better results.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Baseline</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">As a baseline for our generalized language model (GLM) we have trained standard language models using modified Kneser-Ney Smoothing (MKN).
These models have been trained for model lengths <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m2" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math>.
For unigram and bigram models MKN and GLM are identical.</p>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Experiments</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">The perplexity values for all data sets and various model orders can be seen in Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Evaluation Experiments ‣ 5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
In this table we also present the relative reduction of perplexity in comparison to the baseline.</p>
</div>
<div id="S5.T3" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:195.129pt;">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"/>
<th class="ltx_td ltx_align_center ltx_border_tt" colspan="3">model length</th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>Experiments</th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m1" class="ltx_Math" alttext="n=3" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow></math></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m2" class="ltx_Math" alttext="n=4" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow></math></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m3" class="ltx_Math" alttext="n=5" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>5</mn></mrow></math></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-de MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">1074.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">778.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">597.1</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-de GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1031.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">709.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">521.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">4.0%</td>
<td class="ltx_td ltx_align_center">8.9%</td>
<td class="ltx_td ltx_align_center">12.7%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-de MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">235.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">138.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">94.7</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-de GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">229.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">131.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">86.0</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">2.5%</td>
<td class="ltx_td ltx_align_center">4.8%</td>
<td class="ltx_td ltx_align_center">9.2%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-en MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">586.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">404</td>
<td class="ltx_td ltx_align_center ltx_border_t">307.3</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-en GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">571.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">378.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">275</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">2.6%</td>
<td class="ltx_td ltx_align_center">6.1%</td>
<td class="ltx_td ltx_align_center">10.5%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-en MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">147.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">82.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">54.6</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-en GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">145.3</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">80.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">52.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">1.3%</td>
<td class="ltx_td ltx_align_center">2.8%</td>
<td class="ltx_td ltx_align_center">3.9%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-fr MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">538.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">385.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">298.9</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-fr GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">526.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">363.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">272.9</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">2.2%</td>
<td class="ltx_td ltx_align_center">5.7%</td>
<td class="ltx_td ltx_align_center">8.7%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-fr MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">155.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">92.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">63.9</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-fr GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">153.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">90.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">61.7</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">1.1%</td>
<td class="ltx_td ltx_align_center">2.5%</td>
<td class="ltx_td ltx_align_center">3.5%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-it MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">738.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">532.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">416.7</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>wiki-it GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">718.2</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">500.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">382.2</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">2.7%</td>
<td class="ltx_td ltx_align_center">6.0%</td>
<td class="ltx_td ltx_align_center">8.3%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-it MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">177.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">104.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">71.8</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>JRC-it GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">175.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">101.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">69.6</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">2.6%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">3.1%</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Absolute perplexity values and relative reduction of perplexity from MKN to GLM on all data sets for models of order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m6" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T3.m7" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math></div>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">As we can see, the GLM clearly outperforms the baseline for all model lengths and data sets.
In general we see a larger improvement in performance for models of higher orders (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="n=5" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>5</mn></mrow></math>).
The gain for 3-gram models, instead, is negligible.
For German texts the increase in performance is the highest (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="12.7\%" display="inline"><mrow><mn>12.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>) for a model of order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math>.
We also note that GLMs seem to work better on broad domain text rather than special purpose text as the reduction on the wiki corpora is constantly higher than the reduction of perplexity on the JRC corpora.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">We made consistent observations in our second experiment where we iteratively shrank the size of the training data set.
We calculated the relative reduction in perplexity from MKN to GLM for various model lengths and the different sizes of the training data.
The results for the English Wikipedia data set are illustrated in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2 Evaluation Experiments ‣ 5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.F2" class="ltx_figure"><img src="P14-1108/image002.png" id="S5.F2.g1" class="ltx_graphics ltx_centering" width="607" height="205" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Variation of the size of the training data on 100k test sequences on the English Wikipedia data set with different model lengths for GLM.</div>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p class="ltx_p">We see that the GLM performs particularly well on small training data.
As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p4.m1" class="ltx_Math" alttext="25.7\%" display="inline"><mrow><mn>25.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> compared to language models with modified Kneser-Ney smoothing on the same data set.
The absolute perplexity values for this experiment are presented in Table <a href="#S5.T4" title="Table 4 ‣ 5.2 Evaluation Experiments ‣ 5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.T4" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:195.129pt;">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">model length</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>Experiments</td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m1" class="ltx_Math" alttext="n=3" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m2" class="ltx_Math" alttext="n=4" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m3" class="ltx_Math" alttext="n=5" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>5</mn></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m4" class="ltx_align_left" alttext="80\%" display="inline"><mrow><mn>80</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">586.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">404</td>
<td class="ltx_td ltx_align_center ltx_border_t">307.3</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m5" class="ltx_align_left" alttext="80\%" display="inline"><mrow><mn>80</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">571.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">378.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">275</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">2.6%</td>
<td class="ltx_td ltx_align_center">6.5%</td>
<td class="ltx_td ltx_align_center">10.5%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m6" class="ltx_align_left" alttext="8\%" display="inline"><mrow><mn>8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">712.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">539.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">436.5</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m7" class="ltx_align_left" alttext="8\%" display="inline"><mrow><mn>8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">683.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">492.8</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">382.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">4.1%</td>
<td class="ltx_td ltx_align_center">8.7%</td>
<td class="ltx_td ltx_align_center">12.4%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m8" class="ltx_align_left" alttext="0.8\%" display="inline"><mrow><mn>0.8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">894.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">730.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">614.1</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m9" class="ltx_align_left" alttext="0.8\%" display="inline"><mrow><mn>0.8</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">838.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">650.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">528.7</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">6.2%</td>
<td class="ltx_td ltx_align_center">10.9%</td>
<td class="ltx_td ltx_align_center">13.9%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m10" class="ltx_align_left" alttext="0.08\%" display="inline"><mrow><mn>0.08</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">1099.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">963.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">845.2</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m11" class="ltx_align_left" alttext="0.08\%" display="inline"><mrow><mn>0.08</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">996.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">820.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">693.4</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center">9.4%</td>
<td class="ltx_td ltx_align_center">14.9%</td>
<td class="ltx_td ltx_align_center">18.0%</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m12" class="ltx_align_left" alttext="0.008\%" display="inline"><mrow><mn>0.008</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> MKN</td>
<td class="ltx_td ltx_align_center ltx_border_t">1212.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">1120.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">1009.6</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m13" class="ltx_align_left" alttext="0.008\%" display="inline"><mrow><mn>0.008</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> GLM</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1025.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">875.5</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">750.3</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span>rel. change</td>
<td class="ltx_td ltx_align_center ltx_border_bb">15.4%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">21.9%</td>
<td class="ltx_td ltx_align_center ltx_border_bb">25.7%</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Absolute perplexity values and relative reduction of perplexity from MKN to GLM on shrunk training data sets for the English Wikipedia for models of order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m16" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T4.m17" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math></div>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p class="ltx_p">Our theory as well as the results so far suggest that the GLM performs particularly well on sparse training data.
This conjecture has been investigated in a last experiment.
For each model length we have split the test data of the largest English Wikipedia corpus into two disjoint evaluation data sets.
The data set <span class="ltx_text ltx_font_italic">unseen</span> consists of all test sequences which have never been observed in the training data.
The set <span class="ltx_text ltx_font_italic">observed</span> consists only of test sequences which have been observed at least once in the training data.
Again we have calculated the perplexity of each set.
For reference, also the values of the complete test data set are shown in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Evaluation Experiments ‣ 5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.T5" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:203.8014pt;">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_tt"/>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3">model length</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>Experiments</th>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m1" class="ltx_Math" alttext="n=3" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m2" class="ltx_Math" alttext="n=4" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>4</mn></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m3" class="ltx_Math" alttext="n=5" display="inline"><mrow><mi>n</mi><mo>=</mo><mn>5</mn></mrow></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>MKN<sup class="ltx_sup ltx_align_left">complete</sup></th>
<td class="ltx_td ltx_align_center ltx_border_t">586.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">404</td>
<td class="ltx_td ltx_align_center ltx_border_t">307.3</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>GLM<sup class="ltx_sup ltx_align_left">complete</sup></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">571.6</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">378.1</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">275</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</th>
<td class="ltx_td ltx_align_center">2.6%</td>
<td class="ltx_td ltx_align_center">6.5%</td>
<td class="ltx_td ltx_align_center">10.5%</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>MKN<sup class="ltx_sup ltx_align_left">unseen</sup></th>
<td class="ltx_td ltx_align_center ltx_border_t">14696.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">2199.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">846.1</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>GLM<sup class="ltx_sup ltx_align_left">unseen</sup></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">13058.7</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1902.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">714.4</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>rel. change</th>
<td class="ltx_td ltx_align_center">11.2%</td>
<td class="ltx_td ltx_align_center">13.5%</td>
<td class="ltx_td ltx_align_center">15.6%</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>MKN<sup class="ltx_sup ltx_align_left">observed</sup></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">220.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">88.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">43.4</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span>GLM<sup class="ltx_sup ltx_align_left">observed</sup></th>
<td class="ltx_td ltx_align_center">220.6</td>
<td class="ltx_td ltx_align_center">88.3</td>
<td class="ltx_td ltx_align_center">43.5</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span>rel. change</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m4" class="ltx_Math" alttext="-0.16\%" display="inline"><mrow><mo>-</mo><mrow><mn>0.16</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m5" class="ltx_Math" alttext="-0.28\%" display="inline"><mrow><mo>-</mo><mrow><mn>0.28</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m6" class="ltx_Math" alttext="-0.15\%" display="inline"><mrow><mo>-</mo><mrow><mn>0.15</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></mrow></math></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Absolute perplexity values and relative reduction of perplexity from MKN to GLM for the complete and split test file into observed and unseen sequences for models of order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m9" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T5.m10" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math>. The data set is the largest English Wikipedia corpus.</div>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p class="ltx_p">As expected we see the overall perplexity values rise for the <span class="ltx_text ltx_font_italic">unseen</span> test case and decline for the <span class="ltx_text ltx_font_italic">observed</span> test case.
More interestingly we see that the relative reduction of perplexity of the GLM over MKN increases from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p6.m1" class="ltx_Math" alttext="10.5\%" display="inline"><mrow><mn>10.5</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p6.m2" class="ltx_Math" alttext="15.6\%" display="inline"><mrow><mn>15.6</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> on the <span class="ltx_text ltx_font_italic">unseen</span> test case.
This indicates that the superior performance of the GLM on small training corpora and for higher order models indeed comes from its good performance properties with regard to sparse training data.
It also confirms that our motivation to produce lower order <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p6.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams by omitting not only the first word of the local context but systematically all words has been fruitful.
However, we also see that for the <span class="ltx_text ltx_font_italic">observed</span> sequences the GLM performs slightly worse than MKN.
For the observed cases we find the relative change to be negligible.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">In our experiments we have observed an improvement of our generalized language models over classical language models using Kneser-Ney smoothing.
The improvements have been observed for different languages, different domains as well as different sizes of the training data.
In the experiments we have also seen that the GLM performs well in particular for small training data sets and sparse data, encouraging our initial motivation.
This feature of the GLM is of particular value, as data sparsity becomes a more and more immanent problem for higher values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>.
This known fact is underlined also by the statistics shown in Table <a href="#S6.T6" title="Table 6 ‣ 6 Discussion ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
The fraction of total <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams which appear only once in our Wikipedia corpus increases for higher values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>.
However, for the same value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> the skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m5" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams are less rare.
Our generalized language models leverage this additional information to obtain more reliable estimates for the probability of word sequences.</p>
</div>
<div id="S6.T6" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:173.448pt;">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m1" class="ltx_align_left" alttext="w_{1}^{n}" display="inline"><msubsup><mi>w</mi><mn>1</mn><mi>n</mi></msubsup></math></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">total</span></th>
<th class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">unique</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m2" class="ltx_align_left" alttext="w_{1}" display="inline"><msub><mi>w</mi><mn>1</mn></msub></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m3" class="ltx_Math" alttext="0.5\%" display="inline"><mrow><mn>0.5</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m4" class="ltx_Math" alttext="64.0\%" display="inline"><mrow><mn>64.0</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m5" class="ltx_align_left" alttext="w_{1}w_{2}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m6" class="ltx_Math" alttext="5.1\%" display="inline"><mrow><mn>5.1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m7" class="ltx_Math" alttext="68.2\%" display="inline"><mrow><mn>68.2</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m8" class="ltx_align_left" alttext="w_{1}\_w_{3}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m9" class="ltx_Math" alttext="8.0\%" display="inline"><mrow><mn>8.0</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m10" class="ltx_Math" alttext="79.9\%" display="inline"><mrow><mn>79.9</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m11" class="ltx_align_left" alttext="w_{1}\_\_w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m12" class="ltx_Math" alttext="9.6\%" display="inline"><mrow><mn>9.6</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m13" class="ltx_Math" alttext="72.1\%" display="inline"><mrow><mn>72.1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m14" class="ltx_align_left" alttext="w_{1}\_\_\_w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m15" class="ltx_Math" alttext="10.1\%" display="inline"><mrow><mn>10.1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m16" class="ltx_Math" alttext="72.7\%" display="inline"><mrow><mn>72.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m17" class="ltx_align_left" alttext="w_{1}w_{2}w_{3}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m18" class="ltx_Math" alttext="21.1\%" display="inline"><mrow><mn>21.1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m19" class="ltx_Math" alttext="77.5\%" display="inline"><mrow><mn>77.5</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m20" class="ltx_align_left" alttext="w_{1}\_w_{3}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m21" class="ltx_Math" alttext="28.2\%" display="inline"><mrow><mn>28.2</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m22" class="ltx_Math" alttext="80.4\%" display="inline"><mrow><mn>80.4</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m23" class="ltx_align_left" alttext="w_{1}w_{2}\_w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m24" class="ltx_Math" alttext="28.2\%" display="inline"><mrow><mn>28.2</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m25" class="ltx_Math" alttext="80.7\%" display="inline"><mrow><mn>80.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m26" class="ltx_align_left" alttext="w_{1}\_\_w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m27" class="ltx_Math" alttext="31.7\%" display="inline"><mrow><mn>31.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m28" class="ltx_Math" alttext="81.9\%" display="inline"><mrow><mn>81.9</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m29" class="ltx_align_left" alttext="w_{1}\_w_{3}\_w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m30" class="ltx_Math" alttext="35.3\%" display="inline"><mrow><mn>35.3</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m31" class="ltx_Math" alttext="83.0\%" display="inline"><mrow><mn>83.0</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m32" class="ltx_align_left" alttext="w_{1}w_{2}\_\_w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m33" class="ltx_Math" alttext="31.5\%" display="inline"><mrow><mn>31.5</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m34" class="ltx_Math" alttext="82.2\%" display="inline"><mrow><mn>82.2</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m35" class="ltx_align_left" alttext="w_{1}w_{2}w_{3}w_{4}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m36" class="ltx_Math" alttext="44.7\%" display="inline"><mrow><mn>44.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m37" class="ltx_Math" alttext="85.4\%" display="inline"><mrow><mn>85.4</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m38" class="ltx_align_left" alttext="w_{1}\_w_{3}w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m39" class="ltx_Math" alttext="52.7\%" display="inline"><mrow><mn>52.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m40" class="ltx_Math" alttext="87.6\%" display="inline"><mrow><mn>87.6</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m41" class="ltx_align_left" alttext="w_{1}w_{2}\_w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m42" class="ltx_Math" alttext="52.6\%" display="inline"><mrow><mn>52.6</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m43" class="ltx_Math" alttext="88.0\%" display="inline"><mrow><mn>88.0</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_r"><span class="ltx_ERROR undefined ltx_align_left">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m44" class="ltx_align_left" alttext="w_{1}w_{2}w_{3}\_w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><mi mathvariant="normal">_</mi><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m45" class="ltx_Math" alttext="52.3\%" display="inline"><mrow><mn>52.3</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m46" class="ltx_Math" alttext="87.7\%" display="inline"><mrow><mn>87.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined ltx_align_left ltx_centering">\arraybackslash</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m47" class="ltx_align_left ltx_centering" alttext="w_{1}w_{2}w_{3}w_{4}w_{5}" display="inline"><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>3</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>4</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>5</mn></msub></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m48" class="ltx_Math" alttext="64.4\%" display="inline"><mrow><mn>64.4</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m49" class="ltx_Math" alttext="90.7\%" display="inline"><mrow><mn>90.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Percentage of generalized <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T6.m51" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams which occur only once in the English Wikipedia corpus. Total means a percentage relative to the total amount of sequences. Unique means a percentage relative to the amount of unique sequences of this pattern in the data set.</div>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">Beyond the general improvements there is an additional path for benefitting from generalized language models.
As it is possible to better leverage the information in smaller and sparse data sets, we can build smaller models of competitive performance.
For instance, when looking at Table <a href="#S5.T4" title="Table 4 ‣ 5.2 Evaluation Experiments ‣ 5 Results ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we observe the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m1" class="ltx_Math" alttext="3" display="inline"><mn>3</mn></math>-gram MKN approach on the full training data set to achieve a perplexity of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m2" class="ltx_Math" alttext="586.9" display="inline"><mn>586.9</mn></math>.
This model has been trained on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m3" class="ltx_Math" alttext="7" display="inline"><mn>7</mn></math> GB of text and the resulting model has a size of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m4" class="ltx_Math" alttext="15" display="inline"><mn>15</mn></math> GB and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m5" class="ltx_Math" alttext="742" display="inline"><mn>742</mn></math> Mio. entries for the count and continuation count values.
Looking for a GLM with comparable but better performance we see that the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m6" class="ltx_Math" alttext="5" display="inline"><mn>5</mn></math>-gram model trained on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m7" class="ltx_Math" alttext="1\%" display="inline"><mrow><mn>1</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> of the training data has a perplexity of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m8" class="ltx_Math" alttext="528.7" display="inline"><mn>528.7</mn></math>.
This GLM model has a size of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m9" class="ltx_Math" alttext="9.5" display="inline"><mn>9.5</mn></math> GB and contains only <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m10" class="ltx_Math" alttext="427" display="inline"><mn>427</mn></math> Mio. entries.
So, using a far smaller set of training data we can build a smaller model which still demonstrates a competitive performance.</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>

<div id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">7.1 </span>Conclusion</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p class="ltx_p">We have introduced a novel generalized language model as the systematic combination of skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and modified Kneser-Ney smoothing.
The main strength of our approach is the combination of a simple and elegant idea with an an empirically convincing result.
Mathematically one can see that the GLM includes the standard language model with modified Kneser-Ney smoothing as a sub model and is consequently a real generalization.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p class="ltx_p">In an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases.
The relative improvement in perplexity is up to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS1.p2.m1" class="ltx_Math" alttext="12.7\%" display="inline"><mrow><mn>12.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math> for large data sets.
GLMs also performs particularly well on small and sparse sets of training data.
On a very small training data set we observed a reduction of perplexity by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS1.p2.m2" class="ltx_Math" alttext="25.7\%" display="inline"><mrow><mn>25.7</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></math>.
Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.</p>
</div>
</div>
<div id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">7.2 </span>Future work</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p class="ltx_p">A desirable extension of our current definition of GLMs will be the combination of different lower lower order models in our generalized language model using different weights for each model.
Such weights can be used to model the statistical reliability of the different lower order models.
The value of the weights would have to be chosen according to the probability or counts of the respective skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p class="ltx_p">Another important step that has not been considered yet is compressing and indexing of generalized language models to improve the performance of the computation and be able to store them in main memory.
Regarding the scalability of the approach to very large data sets we intend to apply the Map Reduce techniques from <cite class="ltx_cite">[]</cite> to our generalized language models in order to have a more scalable calculation.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p class="ltx_p">This will open the path also to another interesting experiment.
Goodman <cite class="ltx_cite">[]</cite> observed that increasing the length of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS2.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams in combination with modified Kneser-Ney smoothing did not lead to improvements for values of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS2.p3.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math> beyond 7.
We believe that our generalized language models could still benefit from such an increase. They suffer less from the sparsity of long <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS2.p3.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and can overcome this sparsity when interpolating with the lower order skip <math xmlns="http://www.w3.org/1998/Math/MathML" id="S7.SS2.p3.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams while benefiting from the larger context.</p>
</div>
<div id="S7.SS2.p4" class="ltx_para">
<p class="ltx_p">Finally, it would be interesting to see how applications of language models—like next word prediction, machine translation, speech recognition, text classification, spelling correction, e.g.—benefit from the better performance of generalized language models.</p>
</div>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We would like to thank Heinrich Hartmann for a fruitful discussion regarding notation of the skip operator for <math xmlns="http://www.w3.org/1998/Math/MathML" id="Sx1.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.
The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013), REVEAL (Grant agree number 610928).</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
<div id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix"><span class="ltx_tag ltx_tag_appendix">Appendix A </span>Discount Values and Weights in Modified Kneser Ney</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">The discount value <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m1" class="ltx_Math" alttext="D(c)" display="inline"><mrow><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mi>c</mi><mo>)</mo></mrow></mrow></math> used in formula (<a href="#S2.Ex2" title="2.1 Review of Modified Kneser-Ney Smoothing ‣ 2 Related Work ‣ A Generalized Language Model as the Combination of Skipped n-grams and Modified Kneser-Ney Smoothing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>) is defined as <cite class="ltx_cite">[]</cite>:</p>
<table id="A1.E10" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E10.m1" class="ltx_Math" alttext="D(c)=\begin{cases}\ 0&amp;\text{if}\ c=0\\&#10;D_{1}&amp;\text{if}\ c=1\\&#10;D_{2}&amp;\text{if}\ c=2\\&#10;D_{3+}&amp;\text{if}\ c&gt;2\\&#10;\end{cases}" display="block"><mrow><mrow><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mi>c</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mn> 0</mn></mtd><mtd columnalign="left"><mrow><mrow><mpadded width="+5.0pt"><mtext mathsize="small" stretchy="false">if</mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><msub><mi>D</mi><mn>1</mn></msub></mtd><mtd columnalign="left"><mrow><mrow><mpadded width="+5.0pt"><mtext mathsize="small" stretchy="false">if</mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>=</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><msub><mi>D</mi><mn>2</mn></msub></mtd><mtd columnalign="left"><mrow><mrow><mpadded width="+5.0pt"><mtext mathsize="small" stretchy="false">if</mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>=</mo><mn>2</mn></mrow></mtd></mtr><mtr><mtd columnalign="left"><msub><mi>D</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub></mtd><mtd columnalign="left"><mrow><mrow><mpadded width="+5.0pt"><mtext mathsize="small" stretchy="false">if</mtext></mpadded><mo>⁢</mo><mi>c</mi></mrow><mo>&gt;</mo><mn>2</mn></mrow></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
</table>
<p class="ltx_p">The discounting values <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m2" class="ltx_Math" alttext="D_{1}" display="inline"><msub><mi>D</mi><mn>1</mn></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m3" class="ltx_Math" alttext="D_{2}" display="inline"><msub><mi>D</mi><mn>2</mn></msub></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m4" class="ltx_Math" alttext="D_{3+}" display="inline"><msub><mi>D</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub></math> are defined as <cite class="ltx_cite">[]</cite></p>
<table id="A1.E11" class="ltx_equationgroup">
<tbody id="A1.EGx7">
<tr id="A1.E11.1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E11.1.m1" class="ltx_Math" alttext="\displaystyle D_{1}=1-2Y\frac{n_{2}}{n_{1}}" display="inline"><mrow><msub><mi>D</mi><mn>1</mn></msub><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mrow><mn>2</mn><mo>⁢</mo><mi>Y</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><msub><mi>n</mi><mn>2</mn></msub><msub><mi>n</mi><mn>1</mn></msub></mfrac></mstyle></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11a)</span></td></tr>
<tr id="A1.E11.2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E11.2.m1" class="ltx_Math" alttext="\displaystyle D_{2}=2-3Y\frac{n_{3}}{n_{2}}" display="inline"><mrow><msub><mi>D</mi><mn>2</mn></msub><mo>=</mo><mrow><mn>2</mn><mo>-</mo><mrow><mn>3</mn><mo>⁢</mo><mi>Y</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><msub><mi>n</mi><mn>3</mn></msub><msub><mi>n</mi><mn>2</mn></msub></mfrac></mstyle></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11b)</span></td></tr>
<tr id="A1.E11.3" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E11.3.m1" class="ltx_Math" alttext="\displaystyle D_{3+}=3-4Y\frac{n_{4}}{n_{3}}" display="inline"><mrow><msub><mi>D</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><mo>=</mo><mrow><mn>3</mn><mo>-</mo><mrow><mn>4</mn><mo>⁢</mo><mi>Y</mi><mo>⁢</mo><mstyle displaystyle="true"><mfrac><msub><mi>n</mi><mn>4</mn></msub><msub><mi>n</mi><mn>3</mn></msub></mfrac></mstyle></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11c)</span></td></tr></tbody>
</table>
<p class="ltx_p">with <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m5" class="ltx_Math" alttext="Y=\frac{n_{1}}{n_{1}+n_{2}}" display="inline"><mrow><mi>Y</mi><mo>=</mo><mfrac><msub><mi>n</mi><mn>1</mn></msub><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></mfrac></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m6" class="ltx_Math" alttext="n_{i}" display="inline"><msub><mi>n</mi><mi>i</mi></msub></math> is the total number of <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m7" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams which appear exactly <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m8" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> times in the training data.
The weight <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m9" class="ltx_Math" alttext="\gamma_{high}(w_{i-n+1}^{i-1})" display="inline"><mrow><msub><mi>γ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>h</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math> is defined as:</p>
<table id="A1.EGx8" class="ltx_equationgroup ltx_eqn_align">

<tr id="A1.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E12.m1" class="ltx_Math" alttext="\displaystyle\gamma_{high}" display="inline"><msub><mi>γ</mi><mrow><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>h</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E12.m2" class="ltx_Math" alttext="\displaystyle(w_{i\!-\!n\!+\!1}^{i\!-\!1})=" display="inline"><mrow><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>=</mo><mi/></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
<tr id="A1.Ex10" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex10.m2" class="ltx_Math" alttext="\displaystyle\frac{D_{1}N_{1}(w_{i\!-\!n\!+\!1}^{i\!-\!1}\!\bullet)\!+\!D_{2}N%&#10;_{2}(w_{i\!-\!n\!+\!1}^{i\!-\!1}\!\bullet)\!+\!D_{3+}N_{3+}(w_{i\!-\!n\!+\!1}^%&#10;{i\!-\!1}\!\bullet)}{c(w_{i\!-\!n\!+\!1}^{i\!-\!1})}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><msub><mi>D</mi><mn>1</mn></msub><msub><mi>N</mi><mn>1</mn></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo rspace="0.8pt">)</mo></mrow><mo rspace="0.8pt">+</mo><msub><mi>D</mi><mn>2</mn></msub><msub><mi>N</mi><mn>2</mn></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo rspace="0.8pt">)</mo></mrow><mo rspace="0.8pt">+</mo><msub><mi>D</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><msub><mi>N</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo>)</mo></mrow></mrow><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">And the weight <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m10" class="ltx_Math" alttext="\gamma_{mid}(w_{i-n+1}^{i-1})" display="inline"><mrow><msub><mi>γ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow></mrow></math> is defined as:</p>
<table id="A1.EGx9" class="ltx_equationgroup ltx_eqn_align">

<tr id="A1.E13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E13.m1" class="ltx_Math" alttext="\displaystyle\gamma_{mid}" display="inline"><msub><mi>γ</mi><mrow><mi>m</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>d</mi></mrow></msub></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.E13.m2" class="ltx_Math" alttext="\displaystyle(w_{i\!-\!n\!+\!1}^{i\!-\!1})=" display="inline"><mrow><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>)</mo></mrow><mo>=</mo><mi/></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(13)</span></td></tr>
<tr id="A1.Ex11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.Ex11.m2" class="ltx_Math" alttext="\displaystyle\frac{D_{1}N_{1}(w_{i\!-\!n\!+\!1}^{i\!-\!1}\!\bullet)\!+\!D_{2}N%&#10;_{2}(w_{i\!-\!n\!+\!1}^{i\!-\!1}\!\bullet)\!+\!D_{3+}N_{3+}(w_{i\!-\!n\!+\!1}^%&#10;{i\!-\!1}\!\bullet)}{N_{1+}(\bullet\!w_{i\!-\!n\!+\!1}^{i\!-\!1}\bullet)}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><msub><mi>D</mi><mn>1</mn></msub><msub><mi>N</mi><mn>1</mn></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo rspace="0.8pt">)</mo></mrow><mo rspace="0.8pt">+</mo><msub><mi>D</mi><mn>2</mn></msub><msub><mi>N</mi><mn>2</mn></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo rspace="0.8pt">)</mo></mrow><mo rspace="0.8pt">+</mo><msub><mi>D</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><msub><mi>N</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mpadded width="-1.7pt"><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup></mpadded><mo>∙</mo><mo>)</mo></mrow></mrow><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><mo rspace="0.8pt">∙</mo><msubsup><mi>w</mi><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mpadded width="-1.7pt"><mi>n</mi></mpadded><mo rspace="0.8pt">+</mo><mn>1</mn></mrow><mrow><mpadded width="-1.7pt"><mi>i</mi></mpadded><mo rspace="0.8pt">-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></mfrac></mstyle></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m11" class="ltx_Math" alttext="N_{1}(w_{i-n+1}^{i-1}\bullet)" display="inline"><mrow><msub><mi>N</mi><mn>1</mn></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m12" class="ltx_Math" alttext="N_{2}(w_{i-n+1}^{i-1}\bullet)" display="inline"><mrow><msub><mi>N</mi><mn>2</mn></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m13" class="ltx_Math" alttext="N_{3+}(w_{i-n+1}^{i-1}\bullet)" display="inline"><mrow><msub><mi>N</mi><mrow><mn>3</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></math> are analogously defined to <math xmlns="http://www.w3.org/1998/Math/MathML" id="A1.p1.m14" class="ltx_Math" alttext="N_{1+}(w_{i-n+1}^{i-1}\bullet)" display="inline"><mrow><msub><mi>N</mi><mrow><mn>1</mn><mo>+</mo></mrow></msub><mrow><mo>(</mo><msubsup><mi>w</mi><mrow><mi>i</mi><mo>-</mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>∙</mo><mo>)</mo></mrow></mrow></math>.</p>
</div>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 19:02:05 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
