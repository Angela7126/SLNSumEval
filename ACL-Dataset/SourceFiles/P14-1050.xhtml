<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>New Word Detection for Sentiment Analysis </title>
<!--Generated on Tue Jun 10 17:48:08 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
<link rel="stylesheet" href="ltx-ulem.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">New Word Detection for Sentiment Analysis </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minlie Huang, Borui Ye*, Yichen Wang, Haiqiang Chen**, Junjun Cheng**, Xiaoyan Zhu 
<br class="ltx_break"/>State Key Lab. of Intelligent Technology and Systems, National Lab. for Information Science 
<br class="ltx_break"/>and Technology, Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China 
<br class="ltx_break"/>*Dept. of Communication Engineering, Beijing University of Posts and Telecommunications 
<br class="ltx_break"/>**China Information Technology Security Evaluation Center 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">aihuang@tsinghua.edu.cn</span> 
<br class="ltx_break"/>
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation,
named entity extraction, and sentiment analysis.
This paper aims at extracting <span class="ltx_text ltx_font_italic">new sentiment words</span> from large-scale user-generated content.
We propose a fully unsupervised, purely data-driven framework for this purpose.
We design statistical measures respectively to quantify the utility of a lexical pattern and
to measure the possibility of a word being a new word.
The method is almost free of linguistic resources (except POS tags), and requires no elaborated linguistic rules.
We also demonstrate how new sentiment word will benefit sentiment analysis.
Experiment results demonstrate the effectiveness of the proposed method.</p>
</div><span class="ltx_ERROR undefined">\XeTeXlinebreaklocale</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">”zh”<span class="ltx_ERROR undefined">\XeTeXlinebreakskip</span>=0ptplus1pt</p>
</div><span class="ltx_ERROR undefined">\zhspacing</span>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">New words on the Internet have been emerging all the time, particularly in user-generated content.
Users like to update and share their information on social websites with their own language styles,
among which new political/social/cultural words are constantly used.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">However, such new words have made many natural language processing tasks more challenging.
Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation, machine translation, named entity extraction,
question answering, and sentiment analysis.
New word detection is one of the most critical issues in Chinese word segmentation.
Recent studies <cite class="ltx_cite">[<a href="#bib.bib13" title="The first international chinese word segmentation bakeoff" class="ltx_ref">17</a>]</cite> <cite class="ltx_cite">[<a href="#bib.bib14" title="Chinese word segmentation using minimal linguistic knowledge" class="ltx_ref">3</a>]</cite> have shown that more than 60% of word segmentation errors result from new words.
Statistics show that more than 1000 new Chinese words appear every year <cite class="ltx_cite">[<a href="#bib.bib12" title="Xinhua xin ciyu cidian" class="ltx_ref">19</a>]</cite>. These words are mostly domain-specific technical terms
and time-sensitive political/social /cultural terms. Most of them are not yet correctly recognized by the segmentation algorithm, and remain as out of vocabulary (OOV) words.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification.
A sentiment phrase with complete meaning should have a correct boundary, however, characters in a new word may be
broken up. For example, in a sentence ” 表演/n 非常/adv 给/v 力/n （artists’ performance is very impressive）” the two Chinese characters
“给/v 力/n(cool; powerful)” should always be extracted together. In polarity classification, new words can be informative features for classification models.
In the previous example, ”给力(cool; powerful)” is a strong feature for classification models while each single character is not.
Adding new words as feature in classification models will improve the performance of polarity classification, as demonstrated later in this paper.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">This paper aims to detect new word for sentiment analysis. We are particulary interested in extracting <span class="ltx_text ltx_font_italic">new sentiment word</span> that can express opinions or sentiment, which is of high value towards sentiment analysis.
<span class="ltx_text ltx_font_italic">New sentiment word</span>, as exemplified in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is a sub-class of multi-word expressions which is a sequence of neighboring words
<span class="ltx_text ltx_font_italic">”whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components”</span> <cite class="ltx_cite">[<a href="#bib.bib7" title="Looking for needles in a haystack or locating interesting collocation expressions in large textual databases" class="ltx_ref">5</a>]</cite>.
Such new words cannot be directly identified using grammatical rules, which poses a major challenge to automatic analysis.
Moreover, existing lexical resources never have adequate and timely coverage since new words appear constantly. People thus resort to statistical methods
such as Pointwise Mutual Information <cite class="ltx_cite">[<a href="#bib.bib8" title="Word association norms, mutual information, and lexicography" class="ltx_ref">6</a>]</cite>, Symmetrical Conditional Probability <cite class="ltx_cite">[<a href="#bib.bib17" title="A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora" class="ltx_ref">7</a>]</cite>, Mutual Expectation <cite class="ltx_cite">[<a href="#bib.bib9" title="Mining textual associations in text corpora" class="ltx_ref">8</a>]</cite>,
Enhanced Mutual Information <cite class="ltx_cite">[<a href="#bib.bib10" title="Improving effectiveness of mutual information for substantival multiword expression extraction" class="ltx_ref">22</a>]</cite>, and Multi-word Expression Distance <cite class="ltx_cite">[<a href="#bib.bib11" title="Measuring the non-compositionality of multiword expressions" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">New word</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English Translation</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Polarity</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">口爱</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">lovely</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">positive</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">杯具</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">tragic/tragedy</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">negative</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">给力</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">very cool; powerful</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">positive</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">坑爹</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">reverse one’s expectation</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">negative</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of new sentiment word.</div>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Our central idea for new sentiment word detection is as follows:
Starting from very few seed words (for example, just one seed word),
we can extract lexical patterns that have strong statistical association with the seed words;
the extracted lexical patterns can be further used in finding more new words, and the most probable new words can be added into
the seed word set for the next iteration; and the process can be
run iteratively until a stop condition is met.
The key issues are to measure the utility of a pattern and to quantify the possibility
of a word being a new word.
The main contributions of this paper are summarized as follows:</p>
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">We propose a novel framework for new word detection from large-scale user-generated data. This framework is fully unsupervised and purely data-driven, and requires very lightweight linguistic resources (i.e., only POS tags).</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">We design statistical measures to quantify the utility of a pattern and to quantify the possibility of a word being a new word, respectively. No elaborated linguistic rules are needed to filter undesirable results. This feature may enable our approach to be portable to other languages.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">We investigate the problem of polarity prediction of new sentiment word and demonstrate that inclusion of new sentiment word benefits sentiment classification tasks.</p>
</div></li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p">The rest of the paper is structured as follows:
we will introduce related work in the next section.
We will describe the proposed method in Section 3, including definitions,
the overview of the algorithm, and the statistical measures for addressing the two key issues.
We then present the experiments in Section 4. Finally, the work is summarized
in Section 5.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">New word detection has been usually interweaved with word segmentation, particularly in Chinese NLP.
In these works, new word detection is considered as an integral part of segmentation, where new words
are identified as the most probable segments inferred by the probabilistic models;
and the detected new word can be further used to improve word segmentation.
Typical models include conditional random fields proposed by <cite class="ltx_cite">[<a href="#bib.bib20" title="Chinese segmentation and new word detection using conditional random fields" class="ltx_ref">14</a>]</cite>,
and a joint model trained with adaptive online gradient descent based on feature frequency information <cite class="ltx_cite">[<a href="#bib.bib21" title="Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Another line is to treat new word detection as a separate task, usually preceded by part-of-speech tagging.
The first genre of such studies is to leverage complex linguistic rules or knowledge.
For example, Justeson and Katz <cite class="ltx_cite">[<a href="#bib.bib23" title="Technical terminology: some linguistic properties and an algorithm for identification in text." class="ltx_ref">11</a>]</cite> extracted technical terminologies from documents using a regular expression.
Argamon et al. <cite class="ltx_cite">[<a href="#bib.bib26" title="A memory-based approach to learning shallow natural language patterns" class="ltx_ref">1</a>]</cite> segmented the POS sequence of a multi-word into small POS tiles, counted tile
frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts.
Chen and Ma <cite class="ltx_cite">[<a href="#bib.bib24" title="Unknown word extraction for chinese documents" class="ltx_ref">4</a>]</cite> employed morphological and statistical rules to extract Chinese new word.
The second genre of the studies is to treat new word detection as a classification problem.
Zhou <cite class="ltx_cite">[<a href="#bib.bib25" title="A chunking strategy towards unknown word detection in chinese word segmentation" class="ltx_ref">25</a>]</cite> proposed a discriminative Markov Model to detect new words by chunking one or more separated words.
In <cite class="ltx_cite">[<a href="#bib.bib19" title="The use of svm for chinese new word identification" class="ltx_ref">12</a>]</cite>, new word detection was viewed as a binary classification problem. However,
these supervised models requires not only heavy engineering of linguistic features, but also expensive annotation of training data.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">User behavior data has recently been explored for finding new words.
Zheng et al. <cite class="ltx_cite">[<a href="#bib.bib4" title="Incorporating user behaviors in new word detection" class="ltx_ref">24</a>]</cite> explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words.
Zhang et al. <cite class="ltx_cite">[<a href="#bib.bib18" title="Chinese new word detection from query logs" class="ltx_ref">23</a>]</cite> proposed to use dynamic time warping to detect new words from query logs.
However, both of the work are limited due to the public unavailability of expensive commercial resources.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Statistical methods for new word detection have been extensively studied, and in some sense exhibit advantages over linguistics-based methods.
In this setting, new word detection is mostly known as multi-word expression extraction.
To measure multi-word association, the first model is Pointwise Mutual Information (PMI) <cite class="ltx_cite">[<a href="#bib.bib8" title="Word association norms, mutual information, and lexicography" class="ltx_ref">6</a>]</cite>.
Since then, a variety of statistical methods have been proposed to measure <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m1" class="ltx_Math" alttext="bi" display="inline"><mrow><mi>b</mi><mo>⁢</mo><mi>i</mi></mrow></math>-gram association,
such as Log-likelihood <cite class="ltx_cite">[<a href="#bib.bib5" title="Accurate methods for the statistics of surprise and coincidence" class="ltx_ref">9</a>]</cite> and Symmetrical Conditional Probability (SCP) <cite class="ltx_cite">[<a href="#bib.bib17" title="A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora" class="ltx_ref">7</a>]</cite>.
Among all the 84 <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m2" class="ltx_Math" alttext="bi" display="inline"><mrow><mi>b</mi><mo>⁢</mo><mi>i</mi></mrow></math>-gram association measures, PMI has been reported to be the best one in Czech data <cite class="ltx_cite">[<a href="#bib.bib15" title="An extensive empirical study of collocation extraction methods" class="ltx_ref">13</a>]</cite>.
In order to measure arbitrary <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams, most common strategies are to separate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram into two parts X and Y so that existing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m5" class="ltx_Math" alttext="bi" display="inline"><mrow><mi>b</mi><mo>⁢</mo><mi>i</mi></mrow></math>-gram methods can be used <cite class="ltx_cite">[<a href="#bib.bib17" title="A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora" class="ltx_ref">7</a>, <a href="#bib.bib9" title="Mining textual associations in text corpora" class="ltx_ref">8</a>, <a href="#bib.bib16" title="Is knowledge-free induction of multiword unit dictionary headwords a solved problem" class="ltx_ref">16</a>]</cite>.
Zhang et al. <cite class="ltx_cite">[<a href="#bib.bib10" title="Improving effectiveness of mutual information for substantival multiword expression extraction" class="ltx_ref">22</a>]</cite> proposed Enhanced Mutual Information (EMI) which measures the cohesion of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m6" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram
by the frequency of itself and the frequency of each single word.
Based on the information distance theory, Bu et al. <cite class="ltx_cite">[<a href="#bib.bib11" title="Measuring the non-compositionality of multiword expressions" class="ltx_ref">2</a>]</cite> proposed multi-word expression distance (MED) and the normalized version,
and reported superior performance to EMI, SCP, and other measures.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Definitions</h3>

<div id="S3.Thmtheorem1" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem">Definition 3.1</span> (Adverbial word).</h6>
<div id="S3.Thmtheorem1.p1" class="ltx_para">
<p class="ltx_p">Words that are used mainly to modify a verb or an adjective, such as
”太(too)”, ”非常(very)”, ”十分(very)”, and ”特别(specially)”.</p>
</div>
</div>
<div id="S3.Thmtheorem2" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem">Definition 3.2</span> (Auxiliary word).</h6>
<div id="S3.Thmtheorem2.p1" class="ltx_para">
<p class="ltx_p">Words that are auxiliaries, model particles, or punctuation marks.
In Chinese, such words are like ”着,了,啦,的,啊”, and punctuation marks include ”，。！？；：” and so on.</p>
</div>
</div>
<div id="S3.Thmtheorem3" class="ltx_theorem ltx_theorem_definition">
<h6 class="ltx_title ltx_runin ltx_font_bold ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem">Definition 3.3</span> (Lexical Pattern).</h6>
<div id="S3.Thmtheorem3.p1" class="ltx_para">
<p class="ltx_p">A lexical pattern is a triplet <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Thmtheorem3.p1.m1" class="ltx_Math" alttext="&lt;AD,*,AU&gt;" display="inline"><mrow><mo>&lt;</mo><mi>A</mi><mi>D</mi><mo>,</mo><mo>*</mo><mo>,</mo><mi>A</mi><mi>U</mi><mo>&gt;</mo></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Thmtheorem3.p1.m2" class="ltx_Math" alttext="AD" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mi>D</mi></mrow></math> is an adverbial word,
the wildcard <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Thmtheorem3.p1.m3" class="ltx_Math" alttext="*" display="inline"><mo>*</mo></math> means an arbitrary number of words
<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>We set the number to 3 words in this work considering computation costs.
</span></span></span>,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Thmtheorem3.p1.m4" class="ltx_Math" alttext="AU" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mi>U</mi></mrow></math> denotes an auxiliary word.</p>
</div>
</div>
<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Definitions ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives some examples of lexical patterns.
In order to obtain lexical patterns, we can define regular expressions with POS tags
<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>Such expressions are very simple and easy to write because
we only need to consider POS tags of adverbial and auxiliary word.</span></span></span>
and apply the regular expressions on POS tagged texts.
Since the tags of adverbial and auxiliary words are relatively static and can be easily identified,
such a method can safely obtain lexical patterns.</p>
</div>
<div id="S3.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Pattern</th>
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Frequency</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">¡”都”,*,”了”¿</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">562,057</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">¡”都”,*,”的”¿</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">387,649</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">¡”太”,*,”了”¿</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">380,470</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">¡”不”,*,”，”¿</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">369,702</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of lexical pattern. The frequency is counted on 237,108,977 Weibo posts.</div>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>The Algorithm Overview</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">The algorithm works as follows:
starting from very few seed words (for example, a word in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>),
the algorithm can find lexical patterns that have strong statistical association with the seed words in which
the likelihood ratio test (LRT) is used to quantify the degree of association.
Subsequently, the extracted lexical patterns can be further used in finding more new words.
We design several measures to quantify the possibility of a candidate word being a new word,
and the top-ranked words will be added into the seed word set for the next iteration.
The process can be run iteratively until a stop condition is met.
Note that we do not augment the pattern set (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p1.m1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>) at each iteration, instead,
we keep a fixed small number of patterns during iteration because this strategy produces optimal results.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p class="ltx_p">From linguistic perspectives, new sentiment words are commonly modified by adverbial words
and thus can be extracted by lexical patterns. This is the reason why the algorithm will work.
Our algorithm is in spirit to
double propagation <cite class="ltx_cite">[<a href="#bib.bib27" title="Opinion word expansion and target extraction through double propagation" class="ltx_ref">15</a>]</cite>, however, the differences are apparent in that: firstly,
we use very lightweight linguistic information (except POS tags); secondly, our major contributions are
to propose statistical measures to address the following key issues: first, to measure the utility of lexical patterns;
second, to measure the possibility of a candidate word being a new word.</p>
</div><span class="ltx_ERROR undefined">{algorithm}</span>
<div id="S3.SS2.p3" class="ltx_para">
<p class="ltx_p">[t]
<span class="ltx_text ltx_caption">New word detection algorithm</span>

<span class="ltx_ERROR undefined">\KwIn</span>
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒟</mi></math>: a large set of POS tagged posts 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m2" class="ltx_Math" alttext="\mathcal{W}_{s}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>s</mi></msub></math>: a set of seed words 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m3" class="ltx_Math" alttext="k_{p}" display="inline"><msub><mi>k</mi><mi>p</mi></msub></math>: the number of patterns chosen at each iteration 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m4" class="ltx_Math" alttext="k_{c}" display="inline"><msub><mi>k</mi><mi>c</mi></msub></math>: the number of patterns in the candidate pattern set 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m5" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math>: the number of words added at each iteration 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m6" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math>: the number of words returned

<span class="ltx_ERROR undefined">\KwOut</span>A list of ranked new words <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m7" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math>
Obtain all lexical patterns using regular expressions on <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m8" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒟</mi></math> 
Count the frequency of each lexical pattern and extract words matched by each pattern  
Obtain top <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m9" class="ltx_Math" alttext="k_{c}" display="inline"><msub><mi>k</mi><mi>c</mi></msub></math> frequent patterns as candidate pattern set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m10" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> and top 5,000 frequent words as
candidate word set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m11" class="ltx_Math" alttext="\mathcal{W}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>c</mi></msub></math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m12" class="ltx_Math" alttext="\mathcal{P}=\Phi" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>=</mo><mi mathvariant="normal">Φ</mi></mrow></math>; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m13" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math>=<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m14" class="ltx_Math" alttext="\mathcal{W}_{s}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>s</mi></msub></math>; t = 0  
<span class="ltx_ERROR undefined">\For</span> <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m15" class="ltx_Math" alttext="|\mathcal{W}|&lt;K" display="inline"><mrow><mrow><mo fence="true">|</mo><mi class="ltx_font_mathcaligraphic">𝒲</mi><mo fence="true">|</mo></mrow><mo>&lt;</mo><mi>K</mi></mrow></math>

Use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m16" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math> to score each pattern in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m17" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m18" class="ltx_Math" alttext="U(p)" display="inline"><mrow><mi>U</mi><mo>⁢</mo><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow></mrow></math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m19" class="ltx_Math" alttext="\mathcal{P}=\{top\ k_{p}\ patterns\}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒫</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mpadded width="+5.0pt"><mi>p</mi></mpadded><mo>⁢</mo><mpadded width="+5.0pt"><msub><mi>k</mi><mi>p</mi></msub></mpadded><mo>⁢</mo><mi>p</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>s</mi></mrow><mo>}</mo></mrow></mrow></math>  
Use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m20" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> to extract new words and if the words are in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m21" class="ltx_Math" alttext="\mathcal{W}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>c</mi></msub></math>, score them with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m22" class="ltx_Math" alttext="F(w)" display="inline"><mrow><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m23" class="ltx_Math" alttext="\mathcal{W}=\mathcal{W}\bigcup\{top\ k_{w}\ words\}" display="inline"><mrow><mi class="ltx_font_mathcaligraphic">𝒲</mi><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒲</mi><mo>⁢</mo><mrow><mo largeop="true" mathsize="1.5em" stretchy="false" symmetric="true">⋃</mo><mrow><mo>{</mo><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mpadded width="+5.0pt"><mi>p</mi></mpadded><mo>⁢</mo><mpadded width="+5.0pt"><msub><mi>k</mi><mi>w</mi></msub></mpadded><mo>⁢</mo><mi>w</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>s</mi></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m24" class="ltx_Math" alttext="\mathcal{W}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>c</mi></msub></math> = <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m25" class="ltx_Math" alttext="\mathcal{W}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒲</mi><mi>c</mi></msub></math> - <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p3.m26" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math></p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p class="ltx_p">Sort words in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math> with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m2" class="ltx_Math" alttext="F(w)" display="inline"><mrow><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>  
Output the ranked list of words in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS2.p4.m3" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math></p>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Measuring the Utility of a Pattern</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">The first key issue is to quantify the utility of a pattern at each iteration.
This can be measured by the association of a pattern to the current word set used in the algorithm.
The likelihood ratio tests <cite class="ltx_cite">[<a href="#bib.bib5" title="Accurate methods for the statistics of surprise and coincidence" class="ltx_ref">9</a>]</cite> is used for this purpose. This association model has also been used
to model association between opinion target words by <cite class="ltx_cite">[<a href="#bib.bib6" title="One seed to find them all: mining opinion features via association" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">The LRT is well known for not relying critically on the assumption of normality, instead, it uses the asymptotic assumption of the generalized likelihood ratio. In practice, the use of likelihood ratios tends to result in significant improvements in text-analysis performance.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">In our problem, LRT computes a contingency table of a pattern <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> and a word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m2" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math>,
derived from the corpus statistics, as given in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Measuring the Utility of a Pattern ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m3" class="ltx_Math" alttext="k_{1}(w,p)" display="inline"><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow></math> is the number of documents that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m4" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> matches pattern <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m5" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m6" class="ltx_Math" alttext="k_{2}(w,\bar{p})" display="inline"><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><mover accent="true"><mi>p</mi><mo stretchy="false">¯</mo></mover></mrow><mo>)</mo></mrow></mrow></math> is the number of documents that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m7" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> occurs while <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m8" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> does not,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m9" class="ltx_Math" alttext="k_{3}(\bar{w},p)" display="inline"><mrow><msub><mi>k</mi><mn>3</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">¯</mo></mover><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow></math> is the number of documents that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m10" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> occurs while <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m11" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> does not, and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m12" class="ltx_Math" alttext="k_{4}(\bar{w},\bar{p})" display="inline"><mrow><msub><mi>k</mi><mn>4</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">¯</mo></mover><mo>,</mo><mover accent="true"><mi>p</mi><mo stretchy="false">¯</mo></mover></mrow><mo>)</mo></mrow></mrow></math> is the number of documents containing neither <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m13" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> nor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p3.m14" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math>.</p>
</div>
<div id="S3.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Statistics</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m2" class="ltx_Math" alttext="\bar{p}" display="inline"><mover accent="true"><mi>p</mi><mo stretchy="false">¯</mo></mover></math></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m3" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m4" class="ltx_Math" alttext="k_{1}(w,p)" display="inline"><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m5" class="ltx_Math" alttext="k_{2}(w,\bar{p})" display="inline"><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><mover accent="true"><mi>p</mi><mo stretchy="false">¯</mo></mover></mrow><mo>)</mo></mrow></mrow></math></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m6" class="ltx_Math" alttext="\bar{w}" display="inline"><mover accent="true"><mi>w</mi><mo stretchy="false">¯</mo></mover></math></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m7" class="ltx_Math" alttext="k_{3}(\bar{w},p)" display="inline"><mrow><msub><mi>k</mi><mn>3</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">¯</mo></mover><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.T3.m8" class="ltx_Math" alttext="k_{4}(\bar{w},\bar{p})" display="inline"><mrow><msub><mi>k</mi><mn>4</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">¯</mo></mover><mo>,</mo><mover accent="true"><mi>p</mi><mo stretchy="false">¯</mo></mover></mrow><mo>)</mo></mrow></mrow></math></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Contingency table for likelihood ratio test (LRT).</div>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p class="ltx_p">Based on the statistics shown in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Measuring the Utility of a Pattern ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the likelihood ratio tests (LRT) model captures the statistical association between a pattern <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p4.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> and a word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p4.m2" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> by employing the following formula:</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<table id="S3.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m1" class="ltx_Math" alttext="LRT(p,w)=log\frac{L(\rho_{1},k_{1},n_{1})*L(\rho_{2},k_{2},n_{2})}{L(\rho,k_{1%&#10;},n_{1})*L(\rho,k_{2},n_{2})}" display="block"><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>p</mi><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mfrac><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>ρ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>ρ</mi><mn>2</mn></msub><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></mrow><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>ρ</mi><mo>,</mo><msub><mi>k</mi><mn>1</mn></msub><mo>,</mo><msub><mi>n</mi><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>ρ</mi><mo>,</mo><msub><mi>k</mi><mn>2</mn></msub><mo>,</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where: 
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m1" class="ltx_Math" alttext="L(\rho,k,n)=\rho^{k}*(1-\rho)^{n-k}" display="inline"><mrow><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>ρ</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>n</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>ρ</mi><mi>k</mi></msup><mo>*</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>ρ</mi></mrow><mo>)</mo></mrow><mrow><mi>n</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></mrow></math>; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m2" class="ltx_Math" alttext="n_{1}=k_{1}+k_{3}" display="inline"><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>+</mo><msub><mi>k</mi><mn>3</mn></msub></mrow></mrow></math>; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m3" class="ltx_Math" alttext="n_{2}=k_{2}+k_{4}" display="inline"><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>+</mo><msub><mi>k</mi><mn>4</mn></msub></mrow></mrow></math>;
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m4" class="ltx_Math" alttext="\rho_{1}=k_{1}/n_{1}" display="inline"><mrow><msub><mi>ρ</mi><mn>1</mn></msub><mo>=</mo><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>/</mo><msub><mi>n</mi><mn>1</mn></msub></mrow></mrow></math>; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m5" class="ltx_Math" alttext="\rho_{2}=k_{2}/n_{2}" display="inline"><mrow><msub><mi>ρ</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>k</mi><mn>2</mn></msub><mo>/</mo><msub><mi>n</mi><mn>2</mn></msub></mrow></mrow></math>; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p5.m6" class="ltx_Math" alttext="\rho=(k_{1}+k_{2})/(n_{1}+n_{2})" display="inline"><mrow><mi>ρ</mi><mo>=</mo><mrow><mrow><mo>(</mo><mrow><msub><mi>k</mi><mn>1</mn></msub><mo>+</mo><msub><mi>k</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow><mo>/</mo><mrow><mo>(</mo><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></mrow></mrow></math>.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p class="ltx_p">Thus, the utility of a pattern can be measured as follows:</p>
<table id="S3.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m1" class="ltx_Math" alttext="U(p)=\sum_{w_{i}\in\mathcal{W}}{LRT(p,w_{i})}" display="block"><mrow><mrow><mi>U</mi><mo>⁢</mo><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒲</mi></mrow></munder><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>p</mi><mo>,</mo><msub><mi>w</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS3.p6.m1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒲</mi></math> is the current word set used in the algorithm (see Algorithm 1).</p>
</div>
</div>
<div id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.4 </span>Measuring the Possibility of Being New Words</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p">Another key issue in the proposed algorithm is to quantify the possibility of a candidate word being a new word.
We consider several factors for this purpose.</p>
</div>
<div id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Likelihood Ratio Test</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p class="ltx_p">Very similar to the pattern utility measure, LRT can also be used to measure the association of a candidate word to a given pattern set, as follows:</p>
<table id="S3.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m1" class="ltx_Math" alttext="LRT(w)=\sum_{p_{i}\in\mathcal{P}}{LRT(w,p_{i})}" display="block"><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒫</mi></mrow></munder><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>p</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS1.p1.m1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> is the current pattern set used in the algorithm (see Algorithm 1),
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS1.p1.m2" class="ltx_Math" alttext="p_{i}" display="inline"><msub><mi>p</mi><mi>i</mi></msub></math> is a lexical pattern.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p class="ltx_p">This measure only quantifies the association of a candidate word to the given pattern set. It
tells nothing about the possibility of a word being a new word, however, a new <span class="ltx_text ltx_font_italic">sentiment</span> word,
should have close association with the lexical patterns.
This has linguistic interpretations because new sentiment
words are commonly modified by adverbial words
and thus should have close association with lexical patterns.
This measure is proved to be an influential factor by our experiments in Section <a href="#S4.SS3" title="4.3 Evaluation of Different Measures and Comparison to Baselines ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</div>
<div id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Left Pattern Entropy</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p class="ltx_p">If a candidate word is a new word, it will be more commonly used with diversified lexical patterns since the non-compositionality of new word means that the word can be used in many different linguistic scenarios. This can be measured by information entropy, as follows:</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<table id="S3.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="LPE(w)=-\sum_{l_{i}\in L(\mathcal{P}_{c},w)}{\frac{c(l_{i},w)}{N(w)}*log\frac{%&#10;c(l_{i},w)}{N(w)}}" display="block"><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>l</mi><mi>i</mi></msub><mo>∈</mo><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></mrow></munder><mrow><mrow><mfrac><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>l</mi><mi>i</mi></msub><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mfrac><mo>*</mo><mi>l</mi></mrow><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mfrac><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>l</mi><mi>i</mi></msub><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m1" class="ltx_Math" alttext="L(\mathcal{P}_{c},w)" display="inline"><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></math> is the set of left word of all patterns by which word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m2" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> can be matched in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m3" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> , <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m4" class="ltx_Math" alttext="c(l_{i},w)" display="inline"><mrow><mi>c</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>l</mi><mi>i</mi></msub><mo>,</mo><mi>w</mi></mrow><mo>)</mo></mrow></mrow></math> is the count that word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m5" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> can be matched by patterns whose left word is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m6" class="ltx_Math" alttext="l_{i}" display="inline"><msub><mi>l</mi><mi>i</mi></msub></math>,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m7" class="ltx_Math" alttext="N(w)" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math> is the count that word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m8" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> can be matched by the patterns in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m9" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math>.
Note that we use <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m10" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math>, instead of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m11" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>, because the latter set is very small while computing entropy needs a large number of patterns.
Tuning the size of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS2.p2.m12" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> will be further discussed in Section <a href="#S4.SS4" title="4.4 Parameter Tuning ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
</div>
<div id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>New Word Probability</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p class="ltx_p">Some words occur very frequently and can be widely matched by lexical patterns,
but they are not new words. For example, ” 爱吃(love to eat)” and ” 爱说(love to talk)”
can be matched by many lexical patterns, however, they are not new words due to the lack of
non-compositionality. In such words, each single character
has high probability to be a word. Thus, we design the following measure to favor this observation.</p>
<table id="S3.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m1" class="ltx_Math" alttext="NWP(w)=\prod_{i=1}^{n}\frac{p(w_{i})}{1-p(w_{i})}" display="block"><mrow><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munderover><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mfrac><mrow><mi>p</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mrow><mn>1</mn><mo>-</mo><mrow><mi>p</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m1" class="ltx_Math" alttext="w=w_{1}w_{2}\ldots w_{n}" display="inline"><mrow><mi>w</mi><mo>=</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>w</mi><mi>n</mi></msub></mrow></mrow></math>, each <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m2" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is a single character, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m3" class="ltx_Math" alttext="p(w_{i})" display="inline"><mrow><mi>p</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> is the probability of the character <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m4" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> being a word,
as computed as follows:</p>
<table id="S3.Ex1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex1.m1" class="ltx_Math" alttext="p(w_{i})=\frac{all(w_{i})-s(w_{i})}{all(w_{i})}" display="block"><mrow><mrow><mi>p</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mi>a</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>-</mo><mrow><mi>s</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow><mrow><mi>a</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m5" class="ltx_Math" alttext="all(w_{i})" display="inline"><mrow><mi>a</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> is the total frequency of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m6" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m7" class="ltx_Math" alttext="s(w_{i})" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> is the frequency of
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m8" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> being a single character word. Obviously, in order to obtain the value of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS3.p1.m9" class="ltx_Math" alttext="s(w_{i})" display="inline"><mrow><mi>s</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>w</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math>, some particular Chinese
word segmentation tool is required. In this work, we resort to ICTCLAS <cite class="ltx_cite">[<a href="#bib.bib2" title="HHMM-based chinese lexical analyzer ictclas" class="ltx_ref">21</a>]</cite>,
a widely used tool in the literature.</p>
</div>
</div>
<div id="S3.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Non-compositionality Measures</h4>

<div id="S3.SS4.SSS4.p1" class="ltx_para">
<p class="ltx_p">New words are usually multi-word expressions,
where a variety of statistical measures have been proposed to detect multi-word expressions.
Thus, such measures can be naturally incorporated into our algorithm.</p>
</div>
<div id="S3.SS4.SSS4.p2" class="ltx_para">
<p class="ltx_p">The first measure is enhanced mutual information (EMI) <cite class="ltx_cite">[<a href="#bib.bib10" title="Improving effectiveness of mutual information for substantival multiword expression extraction" class="ltx_ref">22</a>]</cite>:</p>
<table id="S3.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E6.m1" class="ltx_Math" alttext="EMI(w)=log_{2}\frac{F/N}{\prod_{i=1}^{n}{\frac{F_{i}-F}{N}}}" display="block"><mrow><mrow><mi>E</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>g</mi><mn>2</mn></msub><mo>⁢</mo><mfrac><mrow><mi>F</mi><mo>/</mo><mi>N</mi></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mfrac><mrow><msub><mi>F</mi><mi>i</mi></msub><mo>-</mo><mi>F</mi></mrow><mi>N</mi></mfrac></mrow></mfrac></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p2.m1" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math> is the number of posts in which a multi-word expression <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p2.m2" class="ltx_Math" alttext="w=w_{1}w_{2}\ldots w_{n}" display="inline"><mrow><mi>w</mi><mo>=</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>w</mi><mi>n</mi></msub></mrow></mrow></math> occurs,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p2.m3" class="ltx_Math" alttext="F_{i}" display="inline"><msub><mi>F</mi><mi>i</mi></msub></math> is the number of posts where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p2.m4" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> occurs,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p2.m5" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> is the total number of posts.
The key idea of EMI is to measure word pair’s dependency as the ratio of its probability of being a multi-word to its probability of not being a multi-word.
The larger the value, the more possible the expression will be a multi-word expression.</p>
</div>
<div id="S3.SS4.SSS4.p3" class="ltx_para">
<p class="ltx_p">The second measure we take into account is normalized multi-word expression distance <cite class="ltx_cite">[<a href="#bib.bib11" title="Measuring the non-compositionality of multiword expressions" class="ltx_ref">2</a>]</cite>, which has been proposed to measure the non-compositionality of multi-word expressions.</p>
<table id="S3.E7" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E7.m1" class="ltx_Math" alttext="NMED(w)=\frac{log|\mu(w)|-log|\phi(w)|}{logN-log|\phi(w)|}" display="block"><mrow><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mrow><mo fence="true">|</mo><mrow><mi>μ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo fence="true">|</mo></mrow></mrow><mo>-</mo><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mrow><mo fence="true">|</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo fence="true">|</mo></mrow></mrow></mrow><mrow><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>N</mi></mrow><mo>-</mo><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mrow><mo fence="true">|</mo><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo fence="true">|</mo></mrow></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p3.m1" class="ltx_Math" alttext="\mu(w)" display="inline"><mrow><mi>μ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math> is the set of documents in which all single words in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p3.m2" class="ltx_Math" alttext="w=w_{1}w_{2}\ldots w_{n}" display="inline"><mrow><mi>w</mi><mo>=</mo><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>⁢</mo><msub><mi>w</mi><mn>2</mn></msub><mo>⁢</mo><mi mathvariant="normal">…</mi><mo>⁢</mo><msub><mi>w</mi><mi>n</mi></msub></mrow></mrow></math> co-occur,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p3.m3" class="ltx_Math" alttext="\phi(w)" display="inline"><mrow><mi>ϕ</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math> is the set of documents in which word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p3.m4" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> occurs as a whole, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.SS4.SSS4.p3.m5" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> is the total number of
documents.
Different from EMI, this measure is a strict distance metric, meaning that a smaller value
indicates a larger possibility of being a multi-word expression.
As can be seen from the formula, the key idea of this metric is to compute the ratio of the co-occurrence of
all words in a multi-word expressions to the occurrence of the whole expression.</p>
</div>
</div>
<div id="S3.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">3.4.5 </span> Configurations to Combine Various Factors</h4>

<div id="S3.SS4.SSS5.p1" class="ltx_para">
<p class="ltx_p">Taking into account the aforementioned factors, we have different settings to score a new word, as follows:</p>
</div>
<div id="S3.SS4.SSS5.p2" class="ltx_para">
<table id="S3.E8" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E8.m1" class="ltx_Math" alttext="F_{LRT}(w)=LRT(w)" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
</table>
<table id="S3.E9" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E9.m1" class="ltx_Math" alttext="F_{LPE}(w)=LRT(w)*LPE(w)" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>L</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
</table>
<table id="S3.E10" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E10.m1" class="ltx_Math" alttext="F_{NWP}(w)=LRT(w)*LPE(w)*NWP(w)" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi><mo>⁢</mo><mi>P</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>N</mi></mrow><mo>⁢</mo><mi>W</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
</table>
<table id="S3.E11" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E11.m1" class="ltx_Math" alttext="F_{EMI}(w)=LRT(w)*LPE(w)*EMI(w)" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>E</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>E</mi></mrow><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
</table>
<table id="S3.E12" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E12.m1" class="ltx_Math" alttext="F_{NMED}(w)=\frac{LRT(w)*LPE(w)}{NMED(w)}" display="block"><mrow><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>L</mi></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
</table>
</div>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we will conduct the following experiments: first, we will compare our method to several
baselines, and perform parameter tuning with extensive experiments; second, we will classify polarity
of new sentiment words using two methods; third, we will demonstrate how new sentiment words will benefit
sentiment classification.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Preparation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">We crawled 237,108,977 Weibo posts from http://www.weibo.com, the largest social website in China.
These posts range from January of 2011 to December of 2012.
The posts were then part-of-speech tagged using a Chinese word segmentation tool named ICTCLAS <cite class="ltx_cite">[<a href="#bib.bib2" title="HHMM-based chinese lexical analyzer ictclas" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p">Then, we asked two annotators to label the top 5,000 frequent words
that were extracted by lexical patterns as described in Algorithm 1.
The annotators were requested to judge whether a candidate
word is a new word, and also to judge the polarity of a new word (positive, negative, and neutral).
If there is a disagreement on either of the two tasks, discussions are required to make the final decision.
The annotation led to 323 new words, among which there are 116 positive words, 112 negative words,
and 95 neutral words<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>All the resources are available upon request.</span></span></span>.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metric</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">As our algorithm outputs a ranked list of words, we adapt average precision to evaluate the performance of
new sentiment word detection. The metric is computed as follows:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<table id="S4.Ex2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex2.m1" class="ltx_Math" alttext="AP(K)=\frac{\sum_{k=1}^{K}{P(k)*rel(k)}}{\sum_{k=1}^{K}{rel(k)}}" display="block"><mrow><mrow><mi>A</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>K</mi><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>r</mi></mrow><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m1" class="ltx_Math" alttext="P(k)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mrow></math> is the precision at cut-off <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m2" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m3" class="ltx_Math" alttext="rel(k)" display="inline"><mrow><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mrow><mo>(</mo><mi>k</mi><mo>)</mo></mrow></mrow></math> is 1 if the word at position <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is a new word and 0 otherwise, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m5" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> is the number of words in the ranked list.
A perfect list (all top <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p2.m6" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> items are correct) has an AP value of 1.0.</p>
</div>
</div>
<div id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation of Different Measures and Comparison to Baselines</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">First, we assess the influence of likelihood ratio test, which measures the association of a word to the pattern set.
As can be seen from Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Evaluation of Different Measures and Comparison to Baselines ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>,
the association model (LRT) remarkably boosts the performance of new word detection, indicating LRT is a key factor for new sentiment word extraction. From linguistic perspectives, new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns.</p>
</div>
<div id="S4.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m1" class="ltx_Math" alttext="top\ K\ words\Rightarrow" display="inline"><mrow><mrow><mi>t</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mpadded width="+5.0pt"><mi>p</mi></mpadded><mo>⁢</mo><mpadded width="+5.0pt"><mi>K</mi></mpadded><mo>⁢</mo><mi>w</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>s</mi></mrow><mo>⇒</mo><mi/></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">LPE</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.366</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.324</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.286</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.270</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.259</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">LRT+LPE</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.743</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.652</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.613</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.582</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.548</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_tt">LPE+NWP</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.467</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.400</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.330</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.320</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">LRT+LPE+NWP</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.755</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.680</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.612</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.571</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.543</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_tt">LPE+EMI</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.608</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.551</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.519</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.486</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.467</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">LRT+LPE+EMI</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.859</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.759</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.717</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.662</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.632</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_tt">LPE+NMED</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.749</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.690</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.641</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.612</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.576</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">LRT+LPE+NMED</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.907</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.808</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.741</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.723</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.699</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Results with vs. without likelihood ratio test (LRT).</div>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p">Second, we compare different settings of our method to two baselines. The first one is enhanced mutual information (EMI) where we set
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m1" class="ltx_Math" alttext="F(w)=EMI(w)" display="inline"><mrow><mrow><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>E</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math> <cite class="ltx_cite">[<a href="#bib.bib10" title="Improving effectiveness of mutual information for substantival multiword expression extraction" class="ltx_ref">22</a>]</cite> and
the second baseline is normalized multi-word expression distance (NMED) <cite class="ltx_cite">[<a href="#bib.bib11" title="Measuring the non-compositionality of multiword expressions" class="ltx_ref">2</a>]</cite> where we set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m2" class="ltx_Math" alttext="F(w)=NMED(w)" display="inline"><mrow><mrow><mi>F</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></mrow></math>.
The results are shown
in Figure <a href="#S4.F1" title="Figure 1 ‣ 4.3 Evaluation of Different Measures and Comparison to Baselines ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As can be seen, all the proposed measures outperform
the two baselines (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m3" class="ltx_Math" alttext="EMI" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m4" class="ltx_Math" alttext="NMED" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></math>) remarkably and consistently.
The setting of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m5" class="ltx_Math" alttext="F_{NMED}" display="inline"><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub></math> produces the best performance.
Adding <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m6" class="ltx_Math" alttext="NMED" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></math> or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m7" class="ltx_Math" alttext="EMI" display="inline"><mrow><mi>E</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi></mrow></math> leads to remarkable improvements because of their capability
of measuring non-compositionality of new words. Only using <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m8" class="ltx_Math" alttext="LRT" display="inline"><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi></mrow></math> can obtain a fairly good
results when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m9" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> is small, however, the performance drops sharply because it’s unable to measure non-compositionality. Comparison between <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m10" class="ltx_Math" alttext="LRT+LPE" display="inline"><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi></mrow><mo>+</mo><mrow><mi>L</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi></mrow></mrow></math> (or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m11" class="ltx_Math" alttext="LRT+LPE+NWP" display="inline"><mrow><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi></mrow><mo>+</mo><mrow><mi>L</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mi>E</mi></mrow><mo>+</mo><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi><mo>⁢</mo><mi>P</mi></mrow></mrow></math>) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m12" class="ltx_Math" alttext="LRT" display="inline"><mrow><mi>L</mi><mo>⁢</mo><mi>R</mi><mo>⁢</mo><mi>T</mi></mrow></math>
shows that inclusion of left pattern entropy also boosts the performance apparently.
However, the new word probability (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p2.m13" class="ltx_Math" alttext="NWP" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi><mo>⁢</mo><mi>P</mi></mrow></math>) has only marginal contribution to improvement.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p">In the above experiments, we set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p3.m1" class="ltx_Math" alttext="k_{p}=5" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><mn>5</mn></mrow></math> (the number of patterns chosen at each iteration) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS3.p3.m2" class="ltx_Math" alttext="k_{w}=10" display="inline"><mrow><msub><mi>k</mi><mi>w</mi></msub><mo>=</mo><mn>10</mn></mrow></math> (the number of words added at each iteration),
which is the optimal setting and will be discussed in the next subsection.
And only one seed word ”坑爹(reverse one’s expectation)” is used.</p>
</div>
<div id="S4.F1" class="ltx_figure"><img src="P14-1050/image001.jpg" id="S4.F1.g1" class="ltx_graphics ltx_centering" width="486" height="301" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparative results of different measure settings. X-axis is the number of words returned (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F1.m3" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math>), and Y-axis
is average precision (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.F1.m4" class="ltx_Math" alttext="AP(K)" display="inline"><mrow><mi>A</mi><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo>(</mo><mi>K</mi><mo>)</mo></mrow></mrow></math>).</div>
</div>
</div>
<div id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.4 </span>Parameter Tuning</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">Firstly, we will show how to obtain the optimal settings of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m1" class="ltx_Math" alttext="k_{p}" display="inline"><msub><mi>k</mi><mi>p</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m2" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math>.
The measure setting we take here is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m3" class="ltx_Math" alttext="F_{NMED}(w)" display="inline"><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>, as shown in Formula (<a href="#S3.E12" title="(12) ‣ 3.4.5  Configurations to Combine Various Factors ‣ 3.4 Measuring the Possibility of Being New Words ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>).
Again, we choose only one seed word ”坑爹(reverse one’s expectation)”, and the number of words returned is set to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m4" class="ltx_Math" alttext="K=300" display="inline"><mrow><mi>K</mi><mo>=</mo><mn>300</mn></mrow></math>. Results in Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Parameter Tuning ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show that the performance drops consistently across different <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m5" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math> settings when the number of patterns increases. Note that at the early stage of Algorithm 1, larger <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m6" class="ltx_Math" alttext="k_{p}" display="inline"><msub><mi>k</mi><mi>p</mi></msub></math> (perhaps with noisy patterns) may lead to lower quality of new words; while larger <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m7" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math>
(perhaps with noisy seed words) may lead to lower quality of lexical patterns.
Therefore, we choose the optimal setting to small numbers, as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p1.m8" class="ltx_Math" alttext="k_{p}=5,k_{w}=10" display="inline"><mrow><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><mn>5</mn></mrow><mo>,</mo><mrow><msub><mi>k</mi><mi>w</mi></msub><mo>=</mo><mn>10</mn></mrow></mrow></math>.</p>
</div>
<div id="S4.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_ERROR undefined">\backslashbox</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m1" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m2" class="ltx_Math" alttext="k_{p}" display="inline"><msub><mi>k</mi><mi>p</mi></msub></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">5</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.753</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.746</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.734</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.715</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">10</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.753</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.746</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.728</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.712</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">15</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.753</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.746</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.754</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.734</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.718</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">20</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.763</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.744</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.749</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.749</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.735</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.717</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Parameter tuning results for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m7" class="ltx_Math" alttext="k_{p}" display="inline"><msub><mi>k</mi><mi>p</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m8" class="ltx_Math" alttext="k_{w}" display="inline"><msub><mi>k</mi><mi>w</mi></msub></math>. The measure setting is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m9" class="ltx_Math" alttext="F_{NMED}(w)" display="inline"><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>,
the seed word set is {”坑爹(reverse one’s expectation)”}, and the number of words returned is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m10" class="ltx_Math" alttext="K=300" display="inline"><mrow><mi>K</mi><mo>=</mo><mn>300</mn></mrow></math>. </div>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p">Secondly, we justify whether the proposed algorithm is sensitive to the number of seed words.
We set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p2.m1" class="ltx_Math" alttext="k_{p}=5" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><mn>5</mn></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p2.m2" class="ltx_Math" alttext="k_{w}=10" display="inline"><mrow><msub><mi>k</mi><mi>w</mi></msub><mo>=</mo><mn>10</mn></mrow></math>, and take <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p2.m3" class="ltx_Math" alttext="F_{NMED}" display="inline"><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub></math> as the weighting measure of new word.
We experimented with only one seed word, two, three, and four seed words, respectively. The results in Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Parameter Tuning ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> show
very stable performance when different numbers of seed words are chosen.
It’s interesting that the performance is totally the same with different numbers of seed words.
By looking into the pattern set and the selected words at each iteration,
we found that the pattern set (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p2.m4" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math>) converges soon to the same set after a few iterations;
and at the beginning several iterations, the selected words are almost the same although the order of adding the words is different.
Since the algorithm will finally sort the words at step (11) and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p2.m5" class="ltx_Math" alttext="\mathcal{P}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒫</mi></math> is the same, the ranking of the words becomes
all the same.</p>
</div>
<div id="S4.T6" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T6.m1" class="ltx_Math" alttext="\#\ seeds\Rightarrow" display="inline"><mrow><mrow><mpadded width="+5.0pt"><mi mathvariant="normal">#</mi></mpadded><mo>⁢</mo><mi>s</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>d</mi><mo>⁢</mo><mi>s</mi></mrow><mo>⇒</mo><mi/></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=100</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=200</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.808</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.808</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.808</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.808</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=300</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=400</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=500</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance with different numbers of seed words. The measure setting is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T6.m5" class="ltx_Math" alttext="F_{NMED}(w)" display="inline"><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T6.m6" class="ltx_Math" alttext="k_{p}=5" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><mn>5</mn></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T6.m7" class="ltx_Math" alttext="k_{w}=10" display="inline"><mrow><msub><mi>k</mi><mi>w</mi></msub><mo>=</mo><mn>10</mn></mrow></math>.
The seed words are chosen from Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</div>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p class="ltx_p">Lastly, we need to decide the optimal number of patterns in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m1" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> (that is, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m2" class="ltx_Math" alttext="k_{c}" display="inline"><msub><mi>k</mi><mi>c</mi></msub></math> in Algorithm 1) because
the set has been used in computing left pattern entropy, see Formula (<a href="#S3.E4" title="(4) ‣ 3.4.2 Left Pattern Entropy ‣ 3.4 Measuring the Possibility of Being New Words ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
Too small size of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m3" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> may lead to insufficient estimation of left pattern entropy. Results in Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Parameter Tuning ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that larger <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m4" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math> decrease the performance, particularly when the number of words returned (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m5" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math>) becomes larger. Therefore, we set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS4.p3.m6" class="ltx_Math" alttext="|\mathcal{P}_{c}|=100" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub><mo fence="true">|</mo></mrow><mo>=</mo><mn>100</mn></mrow></math>.</p>
</div>
<div id="S4.T7" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T7.m1" class="ltx_Math" alttext="|\mathcal{P}_{c}|\Rightarrow" display="inline"><mrow><mrow><mo fence="true">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub><mo fence="true">|</mo></mrow><mo>⇒</mo><mi/></mrow></math></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">300</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">400</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">500</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=100</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.905</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.916</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.916</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.888</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.887</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=200</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.808</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.810</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.776</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.766</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.764</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=300</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.731</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.722</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.726</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.712</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.713</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=400</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.709</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.708</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.677</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.675</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.656</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.655</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">K=500</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.685</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.683</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.653</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.646</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.626</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.627</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Tuning the number of patterns in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T7.m6" class="ltx_Math" alttext="\mathcal{P}_{c}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>c</mi></msub></math>. The measure setting is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T7.m7" class="ltx_Math" alttext="F_{NMED}(w)" display="inline"><mrow><msub><mi>F</mi><mrow><mi>N</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>E</mi><mo>⁢</mo><mi>D</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T7.m8" class="ltx_Math" alttext="k_{p}=5" display="inline"><mrow><msub><mi>k</mi><mi>p</mi></msub><mo>=</mo><mn>5</mn></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T7.m9" class="ltx_Math" alttext="k_{w}=10" display="inline"><mrow><msub><mi>k</mi><mi>w</mi></msub><mo>=</mo><mn>10</mn></mrow></math>, and
the seed word set is {”坑爹(reverse one’s expectation)”}. </div>
</div>
</div>
<div id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.5 </span>Polarity Prediction of New Sentiment Words</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p">In this section, we attempt to classifying the polarity of the annotated 323 new words.
Two methods are adapted with different settings for this purpose.
The first one is majority vote (MV), and the second one is pointwise mutual information,
similar to <cite class="ltx_cite">[<a href="#bib.bib1" title="Measuring praise and criticism: inference of semantic orientation from association" class="ltx_ref">20</a>]</cite>. The majority vote method is formulated as below:</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<table id="S4.Ex3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex3.m1" class="ltx_Math" alttext="MV(w)=\sum_{w_{p}\in PW}{\frac{\#(w,w_{p})}{|PW|}}-\sum_{w_{n}\in NW}{\frac{\#%&#10;(w,w_{n})}{|NW|}}" display="block"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>V</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>w</mi><mi>p</mi></msub><mo>∈</mo><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow></mrow></munder><mfrac><mrow><mi mathvariant="normal">#</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>w</mi><mi>p</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo fence="true">|</mo><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow><mo fence="true">|</mo></mrow></mfrac></mrow><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>∈</mo><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow></mrow></munder><mfrac><mrow><mi mathvariant="normal">#</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo fence="true">|</mo><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow><mo fence="true">|</mo></mrow></mfrac></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m1" class="ltx_Math" alttext="PW" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m2" class="ltx_Math" alttext="NW" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow></math> are a positive and negative set of emoticons (or seed words) respectively,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m3" class="ltx_Math" alttext="\#(w,w_{p})" display="inline"><mrow><mi mathvariant="normal">#</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>w</mi><mi>p</mi></msub></mrow><mo>)</mo></mrow></mrow></math> is the co-occurrence count of the input word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m4" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> and the item <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m5" class="ltx_Math" alttext="w_{p}" display="inline"><msub><mi>w</mi><mi>p</mi></msub></math>.
The polarity is judged according to this rule: if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m6" class="ltx_Math" alttext="MV(w)&gt;th_{1}" display="inline"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>V</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>1</mn></msub></mrow></mrow></math>, the word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m7" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> is positive;
if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m8" class="ltx_Math" alttext="MV(w)&lt;-th_{1}" display="inline"><mrow><mrow><mi>M</mi><mo>⁢</mo><mi>V</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>&lt;</mo><mrow><mo>-</mo><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>1</mn></msub></mrow></mrow></mrow></math> the word negative; otherwise neutral. The threshold <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p2.m9" class="ltx_Math" alttext="th_{1}" display="inline"><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>1</mn></msub></mrow></math> is manually tuned.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p class="ltx_p">And PMI is computed as follows:</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<table id="S4.Ex4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex4.m1" class="ltx_Math" alttext="PMI(w)=\sum_{w_{p}\in PW}{\frac{PMI(w,w_{p})}{|PW|}}-\sum_{w_{n}\in NW}{\frac{%&#10;PMI(w,w_{n})}{|NW|}}" display="block"><mrow><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>w</mi><mi>p</mi></msub><mo>∈</mo><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow></mrow></munder><mfrac><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>w</mi><mi>p</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo fence="true">|</mo><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow><mo fence="true">|</mo></mrow></mfrac></mrow><mo>-</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>∈</mo><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow></mrow></munder><mfrac><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>w</mi><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mo fence="true">|</mo><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow><mo fence="true">|</mo></mrow></mfrac></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m1" class="ltx_Math" alttext="PMI(x,y)=log_{2}(\frac{Pr(x,y)}{Pr(x)*Pr(y)})" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>l</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><msub><mi>g</mi><mn>2</mn></msub><mo>⁢</mo><mrow><mo>(</mo><mfrac><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow></mrow><mrow><mrow><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow><mo>*</mo><mi>P</mi></mrow><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></math>, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m2" class="ltx_Math" alttext="Pr(\cdot)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow></mrow></math> denotes probability.
The polarity is judged according to the rule:
if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m3" class="ltx_Math" alttext="PMI(w)&gt;th_{2}" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m4" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> is positive; if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m5" class="ltx_Math" alttext="PMI(w)&lt;-th_{2}" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mi>M</mi><mo>⁢</mo><mi>I</mi><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>&lt;</mo><mrow><mo>-</mo><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></mrow></mrow></math> negative; otherwise neutral. The threshold
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p4.m6" class="ltx_Math" alttext="th_{2}" display="inline"><mrow><mi>t</mi><mo>⁢</mo><msub><mi>h</mi><mn>2</mn></msub></mrow></math> is manually tuned.</p>
</div>
<div id="S4.T8" class="ltx_table">
<table class="ltx_tabular ltx_align_middle" style="width:216.81pt;">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">Emoticon</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Polarity</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Emoticon</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Polarity</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g1" class="ltx_graphics" alt=""/></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">positive</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g2" class="ltx_graphics" alt=""/></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">negative</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g3" class="ltx_graphics" alt=""/></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">positive</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g4" class="ltx_graphics" alt=""/></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">negative</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g5" class="ltx_graphics" alt=""/></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">positive</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g6" class="ltx_graphics" alt=""/></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">negative</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g7" class="ltx_graphics" alt=""/></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">positive</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g8" class="ltx_graphics" alt=""/></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">negative</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g9" class="ltx_graphics" alt=""/></span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">positive</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text" style="position:relative; bottom:-0.4pt;"><img src="" id="S4.T8.g10" class="ltx_graphics" alt=""/></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">negative</td>
<td class="ltx_td ltx_border_t"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>The ten emoticons used for polarity prediction.</div>
</div>
<div id="S4.SS5.p5" class="ltx_para">
<p class="ltx_p">As for the resources <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p5.m1" class="ltx_Math" alttext="PW" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p5.m2" class="ltx_Math" alttext="NW" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow></math>, we have three settings. The first setting (denoted by Large_Emo) is a set of most frequent 36 emoticons
in which there are 21 positive and 15 negative emoticons respectively.
The second one (denoted by Small_Emo) is a set of 10 emoticons, which are chosen from the 36 emoticons,
as shown in Table <a href="#S4.T8" title="Table 8 ‣ 4.5 Polarity Prediction of New Sentiment Words ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
The third one (denoted by Opin_Words) is two sets of seed opinion words, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p5.m3" class="ltx_Math" alttext="PW" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mi>W</mi></mrow></math>={
高兴(happy),大方(generous),漂亮(beautiful), 善良(kind), 聪明(smart)}
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.p5.m4" class="ltx_Math" alttext="NW=" display="inline"><mrow><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow><mo>=</mo><mi/></mrow></math>{伤心(sad),小气(mean),难看(ugly), 邪恶(wicked), 笨(stupid)}.</p>
</div>
<div id="S4.SS5.p6" class="ltx_para">
<p class="ltx_p">The performance of polarity prediction is shown in Table <a href="#S4.T9" title="Table 9 ‣ 4.5 Polarity Prediction of New Sentiment Words ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
In two-class polarity classification, we remove neutral words and only make prediction with positive/negative classes.
The first observation is that the performance of using emoticons is much better than that of using seed opinion words.
We conjecture that this may be because new sentiment words are more frequently co-occurring with emoticons than with these opinion words.
The second observation is that three-class polarity classification is much more difficult than
two-class polarity classification because many extracted new words are nouns such as ”基友(gay)”,”菇凉(girl)”, and
”盆友(friend)”. Such nouns are more difficult to classify sentiment orientation.</p>
</div>
<div id="S4.T9" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Methods <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T9.m1" class="ltx_Math" alttext="\Rightarrow" display="inline"><mo>⇒</mo></math></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Majority vote</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">PMI</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3">Two-class polarity classification</th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Large_Emo</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.861</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.865</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Small_Emo</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.846</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.851</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Opin_Words</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.697</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.654</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="3">Three-class polarity classification</th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Large_Emo</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.598</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.632</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Small_Emo</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.551</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.635</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t">Opin_Words</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.449</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0.486</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_right ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>The accuracy of two/three-class polarity classification.</div>
</div>
</div>
<div id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.6 </span>Application of New Sentiment Words to Sentiment Classification</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p class="ltx_p">In this section, we justify whether inclusion of new sentiment word would benefit sentiment classification.
For this purpose, we randomly sampled and annotated 4,500 Weibo posts that contain at least one opinion word in the union of the Hownet <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>http://www.keenage.com/html/c_index.html.</span></span></span>
opinion lexicons and our annotated new words.
We apply two models for polarity classification. The first model is a lexicon-based model (denoted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS6.p1.m1" class="ltx_Math" alttext="Lexicon" display="inline"><mrow><mi>L</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>x</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi></mrow></math>) that counts the number of
positive and negative opinion words in a post respectively, and classifies a post to be positive if there are more positive words than negative ones,
and to be negative otherwise.
The second model is a SVM model in which opinion words are used as feature, and 5-fold cross validation is conducted.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p class="ltx_p">We experiment with different settings of Hownet lexicon resources:</p>
<ul id="I2" class="ltx_itemize">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p">Hownet opinion words (denoted by Hownet): After removing some obviously inappropriate words,
the left lexicons have 627 positive opinion words and 1,038 negative opinion words, respectively.</p>
</div></li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p">Compact Hownet opinion words (denoted by cptHownet): we count the frequency of the above opinion words on the training data and remove words whose document frequency is less than 2. This results in 138 positive words and 125 negative words.</p>
</div></li>
</ul>
<p class="ltx_p">Then, we add into the above resources the labeled new polar words(denoted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS6.p2.m1" class="ltx_Math" alttext="NW" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>W</mi></mrow></math>, including 116 positive and 112 negative words) and the top 100 words produced by the algorithm (denoted by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS6.p2.m2" class="ltx_Math" alttext="T100" display="inline"><mrow><mi>T</mi><mo>⁢</mo><mn>100</mn></mrow></math>), respectively.
Note that the lexicon-based model requires the sentiment orientation of each dictionary entry
<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>This is not necessary for the SVM model. All words in the top 100 words can be used as feature.</span></span></span>, we thus manually label the polarity of all top 100 words (we did NOT remove incorrect new word). This results in 52 positive and 34 negative words.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p class="ltx_p">Results in Table <a href="#S4.T10" title="Table 10 ‣ 4.6 Application of New Sentiment Words to Sentiment Classification ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> show that
inclusion of new words in both models improves the performance remarkably.
In the setting of the original lexicon (Hownet), both models obtain 2-3% gains
from the inclusion of new words. Similar improvement is observed in the setting
of the compact lexicon. Note, that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS6.p3.m1" class="ltx_Math" alttext="T100" display="inline"><mrow><mi>T</mi><mo>⁢</mo><mn>100</mn></mrow></math> is automatically obtained
from Algorithm 1 so that it may contain words that are not new sentiment words, but
the resource also improves performance remarkably.</p>
</div>
<div id="S4.T10" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T10.m1" class="ltx_Math" alttext="\#" display="inline"><mi mathvariant="normal">#</mi></math> Pos/Neg</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Lexicon</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SVM</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hownet</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">627/1,038</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.737</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.756</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hownet+NW</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">743/1,150</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.770</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.779</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hownet+T100</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">679/1,172</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.761</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.774</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">cptHownet</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">138/125</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.738</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.758</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">cptHownet+NW</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">254/237</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.774</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.782</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">cptHownet+T100</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">190/159</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.764</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.775</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/>
<td class="ltx_td ltx_border_t"/></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>The accuracy of polarity classfication of Weibo post with/without new sentiment words.
NW includes 116/112 positive/negative words, and
T100 contains 52/34 positive/negative words.</div>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In order to extract <span class="ltx_text ltx_font_italic">new sentiment words</span> from large-scale user-generated content,
this paper proposes a fully unsupervised, purely data-driven, and almost knowledge-free (except POS tags) framework.
We design statistical measures to quantify the utility of a lexical pattern and
to measure the possibility of a word being a new word, respectively.
The method is almost free of linguistic resources (except POS tags),
and does not rely on elaborated linguistic rules.
We conduct extensive experiments to reveal the influence of different statistical measures in new word finding.
Comparative experiments show that our proposed method outperforms baselines remarkably.
Experiments also demonstrate that inclusion of new sentiment words benefits sentiment classification definitely.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">From linguistic perspectives, our framework is capable to extract <span class="ltx_text ltx_font_italic">adjective</span> new words
because the lexical patterns usually modify adjective words. As future work,
we are considering how to extract other types of new sentiment words,
such as <span class="ltx_text ltx_font_italic">nounal</span> new words that can express sentiment.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work was partly supported by the following grants from: the National Basic Research Program (973 Program) under grant No. 2012CB316301 and 2013CB329403, the National Science Foundation of China project under grant No. 61332007 and No. 60803075, and the Beijing Higher Education Young Elite Teacher Project.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Argamon, I. Dagan and Y. Krymolowski</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A memory-based approach to learning shallow natural language patterns</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">COLING ’98</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 67–73</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/980451.980857" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/980451.980857" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Bu, X. Zhu and M. Li</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Measuring the non-compositionality of multiword expressions</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">COLING ’10</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 116–124</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1873781.1873795" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS4.SSS4.p3" title="3.4.4 Non-compositionality Measures ‣ 3.4 Measuring the Possibility of Being New Words ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.4</span></a>,
<a href="#S4.SS3.p2" title="4.3 Evaluation of Different Measures and Comparison to Baselines ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span></li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Chen</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Chinese word segmentation using minimal linguistic knowledge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">SIGHAN ’03</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 148–151</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1119250.1119271" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1119250.1119271" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Chen and W. Ma</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unknown word extraction for chinese documents</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">COLING ’02</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–7</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1072228.1072277" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1072228.1072277" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Choueka</span><span class="ltx_text ltx_bib_year">(1988)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Looking for needles in a haystack or locating interesting collocation expressions in large textual databases</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 21–24</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. W. Church and P. Hanks</span><span class="ltx_text ltx_bib_year">(1990-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word association norms, mutual information, and lexicography</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Comput. Linguist.</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 22–29</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0891-2017</span>,
<a href="http://dl.acm.org/citation.cfm?id=89086.89095" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. F. da Silva and G. P. Lopes</span><span class="ltx_text ltx_bib_year">(1999)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 369–381</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Dias, S. Guilloré and J. G. P. Lopes</span><span class="ltx_text ltx_bib_year">(2000)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mining textual associations in text corpora</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">6th ACM SIGKDD Work. Text Mining</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Dunning</span><span class="ltx_text ltx_bib_year">(1993-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Accurate methods for the statistics of surprise and coincidence</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Comput. Linguist.</span> <span class="ltx_text ltx_bib_volume">19</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 61–74</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0891-2017</span>,
<a href="http://dl.acm.org/citation.cfm?id=972450.972454" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS3.p1" title="3.3 Measuring the Utility of a Pattern ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Hai, K. Chang and G. Cong</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">One seed to find them all: mining opinion features via association</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">CIKM ’12</span>, <span class="ltx_text ltx_bib_place">New York, NY, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 255–264</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-4503-1156-4</span>,
<a href="http://doi.acm.org/10.1145/2396761.2396797" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/2396761.2396797" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Measuring the Utility of a Pattern ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. S. Justeson and S. M. Katz</span><span class="ltx_text ltx_bib_year">(1995)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Technical terminology: some linguistic properties and an algorithm for identification in text.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Natural language engineering</span> <span class="ltx_text ltx_bib_volume">1</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 9–27</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Li, C. Huang, J. Gao and X. Fan</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The use of svm for chinese new word identification</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_inbook">Natural Language Processing–IJCNLP 2004</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 723–732</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Pecina</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An extensive empirical study of collocation extraction methods</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">ACLstudent ’05</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 13–18</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1628960.1628964" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Peng, F. Feng and A. McCallum</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Chinese segmentation and new word detection using conditional random fields</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">COLING ’04</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1220355.1220436" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220355.1220436" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Qiu, B. Liu, J. Bu and C. Chen</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Opinion word expansion and target extraction through double propagation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational linguistics</span> <span class="ltx_text ltx_bib_volume">37</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 9–27</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS2.p2" title="3.2 The Algorithm Overview ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Schone and D. Jurafsky</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Is knowledge-free induction of multiword unit dictionary headwords a solved problem</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 100–108</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Sproat and T. Emerson</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The first international chinese word segmentation bakeoff</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">SIGHAN ’03</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 133–143</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1119250.1119269" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1119250.1119269" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Sun, H. Wang and W. Li</span><span class="ltx_text ltx_bib_year">(2012)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">ACL ’12</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 253–262</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=2390524.2390560" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_book"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Thesaurus Research Center</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Xinhua xin ciyu cidian</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Commercial Press</span>, <span class="ltx_text ltx_bib_place">Beijing</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. D. Turney and M. L. Littman</span><span class="ltx_text ltx_bib_year">(2003-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Measuring praise and criticism: inference of semantic orientation from association</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ACM Trans. Inf. Syst.</span> <span class="ltx_text ltx_bib_volume">21</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 315–346</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1046-8188</span>,
<a href="http://doi.acm.org/10.1145/944012.944013" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/944012.944013" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS5.p1" title="4.5 Polarity Prediction of New Sentiment Words ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhang, H. Yu, D. Xiong and Q. Liu</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HHMM-based chinese lexical analyzer ictclas</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">SIGHAN ’03</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 184–187</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1119250.1119280" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1119250.1119280" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS4.SSS3.p1" title="3.4.3 New Word Probability ‣ 3.4 Measuring the Possibility of Being New Words ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>,
<a href="#S4.SS1.p1" title="4.1 Data Preparation ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Zhang, T. Yoshida, X. Tang and T. Ho</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improving effectiveness of mutual information for substantival multiword expression extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Expert Systems with Applications</span> <span class="ltx_text ltx_bib_volume">36</span> (<span class="ltx_text ltx_bib_number">8</span>), <span class="ltx_text ltx_bib_pages"> pp. 10919–10930</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p4" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S3.SS4.SSS4.p2" title="3.4.4 Non-compositionality Measures ‣ 3.4 Measuring the Possibility of Being New Words ‣ 3 Methodology ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.4</span></a>,
<a href="#S4.SS3.p2" title="4.3 Evaluation of Different Measures and Comparison to Baselines ‣ 4 Experiment ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhang, M. Sun and Y. Zhang</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Chinese new word detection from query logs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_inbook">Advanced Data Mining and Applications</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 233–243</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zheng, Z. Liu, M. Sun, L. Ru and Y. Zhang</span><span class="ltx_text ltx_bib_year">(2009)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Incorporating user behaviors in new word detection</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">IJCAI’09</span>, <span class="ltx_text ltx_bib_place">San Francisco, CA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 2101–2106</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1661445.1661781" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Zhou</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A chunking strategy towards unknown word detection in chinese word segmentation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_inbook">Natural Language Processing–IJCNLP 2005</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 530–541</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ New Word Detection for Sentiment Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 17:48:08 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
