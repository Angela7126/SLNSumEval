<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Learning Word Sense Distributions, Detecting Unattested Senses
and Identifying Novel Senses Using Topic Models</title>
<!--Generated on Tue Jun 10 17:23:06 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Word Sense Distributions, Detecting Unattested Senses
and Identifying Novel Senses Using Topic Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jey Han Lau,<math xmlns="http://www.w3.org/1998/Math/MathML" id="m1" class="ltx_Math" alttext="{}^{\spadesuit}" display="inline"><msup><mi/><mi mathvariant="normal">♠</mi></msup></math> Paul Cook,<math xmlns="http://www.w3.org/1998/Math/MathML" id="m2" class="ltx_Math" alttext="{}^{\heartsuit}" display="inline"><msup><mi/><mi mathvariant="normal">♡</mi></msup></math> Diana
McCarthy,<math xmlns="http://www.w3.org/1998/Math/MathML" id="m3" class="ltx_Math" alttext="{}^{\diamondsuit}" display="inline"><msup><mi/><mi mathvariant="normal">♢</mi></msup></math> Spandana Gella,<math xmlns="http://www.w3.org/1998/Math/MathML" id="m4" class="ltx_Math" alttext="{}^{\heartsuit}" display="inline"><msup><mi/><mi mathvariant="normal">♡</mi></msup></math> 
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Timothy
Baldwin<math xmlns="http://www.w3.org/1998/Math/MathML" id="m5" class="ltx_Math" alttext="{}^{\heartsuit}" display="inline"><msup><mi/><mi mathvariant="normal">♡</mi></msup></math>
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m6" class="ltx_Math" alttext="\spadesuit" display="inline"><mi mathvariant="normal">♠</mi></math> Dept of Philosophy, King’s College London
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m7" class="ltx_Math" alttext="\heartsuit" display="inline"><mi mathvariant="normal">♡</mi></math> Dept of Computing and Information Systems, The University of
Melbourne
<br class="ltx_break"/><math xmlns="http://www.w3.org/1998/Math/MathML" id="m8" class="ltx_Math" alttext="\diamondsuit" display="inline"><mi mathvariant="normal">♢</mi></math> University of Cambridge
<br class="ltx_break"/><span class="ltx_ERROR undefined">\smaller</span>[1]<a href="jeyhan.lau@gmail.com" title="" class="ltx_ref ltx_Url"><span class="ltx_text ltx_font_typewriter">jeyhan.lau@gmail.com</span></a>,  
<span class="ltx_ERROR undefined">\smaller</span>[1]<a href="paulcook@unimelb.edu.au" title="" class="ltx_ref ltx_Url"><span class="ltx_text ltx_font_typewriter">paulcook@unimelb.edu.au</span></a>,
<br class="ltx_break"/>
<span class="ltx_ERROR undefined">\smaller</span>[1]<a href="diana@dianamccarthy.co.uk" title="" class="ltx_ref ltx_Url"><span class="ltx_text ltx_font_typewriter">diana@dianamccarthy.co.uk</span></a>,  
<span class="ltx_ERROR undefined">\smaller</span>[1]<a href="spandanagella@gmail.com" title="" class="ltx_ref ltx_Url"><span class="ltx_text ltx_font_typewriter">spandanagella@gmail.com</span></a>,  
<span class="ltx_ERROR undefined">\smaller</span>[1]<a href="tb@ldwin.net" title="" class="ltx_ref ltx_Url"><span class="ltx_text ltx_font_typewriter">tb@ldwin.net</span></a>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Unsupervised word sense disambiguation (<span class="ltx_text ltx_font_smallcaps">wsd</span>) methods are an
attractive approach to all-words <span class="ltx_text ltx_font_smallcaps">wsd</span>due to their non-reliance on
expensive annotated data. Unsupervised estimates of sense frequency
have been shown to be very useful for <span class="ltx_text ltx_font_smallcaps">wsd</span>due to the skewed nature of
word sense distributions. This paper presents a fully unsupervised
topic modelling-based approach to sense frequency estimation, which is
highly portable to different corpora and sense inventories, in being
applicable to any part of speech, and not requiring a hierarchical
sense inventory, parsing or parallel text. We demonstrate the
effectiveness of the method over the tasks of predominant sense
learning and sense distribution acquisition, and also the novel tasks
of detecting senses which aren’t attested in the corpus, and
identifying novel senses in the corpus which aren’t captured in the
sense inventory.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The automatic determination of word sense information has been a
long-term pursuit of the NLP community
<cite class="ltx_cite">[]</cite>. Word sense distributions tend
to be Zipfian, and as such, a simple but surprisingly high-accuracy
back-off heuristic for word sense disambiguation (<span class="ltx_text ltx_font_smallcaps">wsd</span>) is to tag each
instance of a given word with its predominant sense
<cite class="ltx_cite">[]</cite>. Such an approach requires knowledge of
predominant senses; however, word sense distributions — and
predominant senses too — vary from corpus to corpus. Therefore,
methods for automatically learning predominant senses and sense
distributions for specific corpora are required
<cite class="ltx_cite">[<a href="#bib.bib10" title="Verb class disambiguation using informative priors" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In this paper, we propose a method which uses topic models to estimate
word sense distributions. This method is in principle applicable to all
parts of speech, and moreover does not require a parser, a hierarchical
sense representation or parallel text. Topic models have been used for
<span class="ltx_text ltx_font_smallcaps">wsd</span>in a number of
studies <cite class="ltx_cite">[<a href="#bib.bib17" title="A topic model for word sense disambiguation" class="ltx_ref">1</a>, <a href="#bib.bib20" title="Topic models for word sense disambiguation and token-based idiom detection" class="ltx_ref">13</a>, <a href="#bib.bib18" title="Unsupervised domain tuning to improve word sense disambiguation" class="ltx_ref">19</a>, <a href="#bib.bib15" title="NUS-ML: improving word sense disambiguation using topic features" class="ltx_ref">3</a>, <a href="#bib.bib9" title="Topic modeling for word sense induction" class="ltx_ref">11</a>]</cite>,
but our work extends significantly on this earlier work in focusing on
the acquisition of prior word sense distributions (and predominant
senses).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Because of domain differences and the skewed nature of word sense
distributions, it is often the case that some senses in a sense
inventory will not be attested in a given corpus. A system capable of
automatically finding such senses could reduce ambiguity, particularly
in domain adaptation settings, while retaining rare but nevertheless
viable senses. We further propose a method for applying our sense
distribution acquisition system to the task of finding unattested senses
— i.e., senses that are in the sense inventory but not attested in a
given corpus. In contrast to the previous work of <cite class="ltx_cite">McCarthy<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib13" title="Automatic identification of infrequent word senses" class="ltx_ref">2004</a>)</cite>
on this topic which uses the sense ranking score from
<cite class="ltx_cite"/> to remove low-frequency senses from <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>,
we focus on finding senses that are unattested in the corpus on the
premise that, given accurate disambiguation, rare senses in a corpus
contribute to correct interpretation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Corpus instances of a word can also correspond to senses that are not
present in a given sense inventory. This can be due to, for example,
words taking on new meanings over time (e.g. the relatively recent
senses of <span class="ltx_text ltx_font_italic">tablet</span> and <span class="ltx_text ltx_font_italic">swipe</span> related to touchscreen
computers) or domain-specific terms not being included in a more
general-purpose sense inventory. A system for automatically identifying
such novel senses — i.e. senses that are attested in the corpus but
not in the sense inventory — would be a very valuable lexicographical
tool for keeping sense inventories up-to-date <cite class="ltx_cite">[]</cite>. We
further propose an application of our proposed method to the
identification of such novel senses. In contrast to
<cite class="ltx_cite"/>, the use of topic models makes this possible,
using topics as a proxy for
sense <cite class="ltx_cite">[]</cite>. Earlier
work on identifying novel senses focused on individual tokens
<cite class="ltx_cite">[<a href="#bib.bib27" title="Unknown word sense detection as outlier detection" class="ltx_ref">7</a>]</cite>, whereas our approach goes further in identifying groups
of tokens exhibiting the same novel sense.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">There has been a considerable amount of research on representing word
senses and disambiguating usages of words in context (<span class="ltx_text ltx_font_smallcaps">wsd</span>) as, in order
to produce computational systems that understand and produce natural
language, it is essential to have a means of representing and
disambiguating word sense. <span class="ltx_text ltx_font_smallcaps">wsd</span>algorithms require word sense
information to disambiguate token instances of a given ambiguous word,
e.g. in the form of sense definitions <cite class="ltx_cite">[]</cite>, semantic
relationships <cite class="ltx_cite">[<a href="#bib.bib22" title="Structural semantic interconnections: a knowledge-based approach to word sense disambiguation" class="ltx_ref">17</a>]</cite> or annotated
data <cite class="ltx_cite">[<a href="#bib.bib23" title="It makes sense: a wide-coverage word sense disambiguation system for free text" class="ltx_ref">20</a>]</cite>. One extremely useful piece of
information is the word sense prior or expected word sense frequency
distribution. This is important because word sense distributions are
typically skewed <cite class="ltx_cite">[]</cite>, and systems do far better when
they take bias into account <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Typically, word frequency distributions are estimated with respect to a
sense-tagged corpus such as SemCor <cite class="ltx_cite">[<a href="#bib.bib21" title="A semantic concordance" class="ltx_ref">15</a>]</cite>, a 220,000 word
corpus tagged with <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span> <cite class="ltx_cite">[]</cite> senses. Due to the
expense of hand tagging, and sense distributions being sensitive to
domain and genre, there has been some work on trying to estimate sense
frequency information
automatically <cite class="ltx_cite">[<a href="#bib.bib8" title="Word sense disambiguation with distribution estimation" class="ltx_ref">5</a>, <a href="#bib.bib16" title="Determining word sense dominance using a thesaurus" class="ltx_ref">16</a>, <a href="#bib.bib12" title="Estimating class priors in domain adaptation for word sense disambiguation" class="ltx_ref">6</a>]</cite>.
Much of this work has been focused on ranking word senses to find the
predominant sense in a given corpus <cite class="ltx_cite">[<a href="#bib.bib16" title="Determining word sense dominance using a thesaurus" class="ltx_ref">16</a>]</cite>,
which is a very powerful heuristic approach to <span class="ltx_text ltx_font_smallcaps">wsd</span>. Most <span class="ltx_text ltx_font_smallcaps">wsd</span>systems rely upon this heuristic for back-off in the absence of strong
contextual evidence <cite class="ltx_cite">[]</cite>. <cite class="ltx_cite"/>
proposed a method which relies on distributionally similar words
(nearest neighbours) associated with the target word in an automatically
acquired thesaurus <cite class="ltx_cite">[]</cite>. The distributional similarity scores
of the nearest neighbours are associated with the respective target word
senses using a <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>similarity measure, such as those proposed
by <cite class="ltx_cite"/> and <cite class="ltx_cite"/>. The word
senses are ranked based on these similarity scores, and the most
frequent sense is selected for the corpus that the distributional
similarity thesaurus was trained over.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">As well as sense ranking for predominant sense acquisition, automatic
estimates of sense frequency distribution can be very useful for <span class="ltx_text ltx_font_smallcaps">wsd</span>for training data sampling purposes <cite class="ltx_cite">[]</cite>, entropy
estimation <cite class="ltx_cite">[]</cite>, and prior probability
estimates, all of which can be integrated within a <span class="ltx_text ltx_font_smallcaps">wsd</span>system
<cite class="ltx_cite">[<a href="#bib.bib8" title="Word sense disambiguation with distribution estimation" class="ltx_ref">5</a>, <a href="#bib.bib12" title="Estimating class priors in domain adaptation for word sense disambiguation" class="ltx_ref">6</a>, <a href="#bib.bib10" title="Verb class disambiguation using informative priors" class="ltx_ref">12</a>]</cite>. Various approaches have
been adopted, such as normalizing sense ranking scores to obtain a
probability distribution <cite class="ltx_cite">[]</cite>, using subcategorisation information as
an indication of verb sense <cite class="ltx_cite">[<a href="#bib.bib10" title="Verb class disambiguation using informative priors" class="ltx_ref">12</a>]</cite> or alternatively using parallel
text <cite class="ltx_cite">[<a href="#bib.bib8" title="Word sense disambiguation with distribution estimation" class="ltx_ref">5</a>, <a href="#bib.bib12" title="Estimating class priors in domain adaptation for word sense disambiguation" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">The work of <cite class="ltx_cite">Boyd-Graber and Blei (<a href="#bib.bib19" title="PUTOP: turning predominant senses into a topic model for word sense disambiguation" class="ltx_ref">2007</a>)</cite> is highly
related in that it extends the method of <cite class="ltx_cite"/> to
provide a generative model which assumes the words in a given document
are generated according to the topic distribution appropriate for that
document. They then predict the most likely sense for each word in the
document based on the topic distribution and the words in context
(“corroborators”), each of which, in turn, depends on the document’s
topic distribution. Using this approach, they get comparable results to
McCarthy et al. when context is ignored (i.e. using a model with one
topic), and at most a 1% improvement on SemCor when they use more
topics in order to take context into account.  Since the results do not improve on
McCarthy et al. as regards sense distribution acquisition irrespective
of context, we will compare our model with that proposed by McCarthy et
al.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">Recent work on finding novel senses has tended to focus on comparing
diachronic corpora
<cite class="ltx_cite">[]</cite> and has
also considered topic models <cite class="ltx_cite">[]</cite>. In a similar vein,
<cite class="ltx_cite">Peirsman<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib26" title="The automatic identification of lexical variation between language varieties" class="ltx_ref">2010</a>)</cite> considered the identification of words having
a sense particular to one language variety with respect to another
(specifically Belgian and Netherlandic Dutch). In contrast to these
studies, we propose a model for comparing a corpus with a sense
inventory. <cite class="ltx_cite">Carpuat<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib28" title="SenseSpotting: never let your parallel data tie you to an old domain" class="ltx_ref">2013</a>)</cite> exploit parallel corpora to
identify words in domain-specific monolingual corpora with
previously-unseen translations; the method we propose does not require
parallel data.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Our methodology is based on the WSI system described in
<cite class="ltx_cite"/>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>Based on the implementation available at:
<a href="https://github.com/jhlau/hdp-wsi" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/jhlau/hdp-wsi</span></a></span></span></span> which has been shown
<cite class="ltx_cite">[]</cite> to achieve state-of-the-art
results over the WSI tasks from SemEval-2007 <cite class="ltx_cite">[]</cite>,
SemEval-2010 <cite class="ltx_cite">[]</cite> and SemEval-2013
<cite class="ltx_cite">[]</cite>. The system is built
around a Hierarchical Dirichlet Process (HDP: <cite class="ltx_cite"/>),
a non-parametric variant of a Latent Dirichlet Allocation topic model
<cite class="ltx_cite">[]</cite> where the model automatically optimises the number of
topics in a fully-unsupervised fashion over the training data.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">To learn the senses of a target lemma, we train a single topic model per
target lemma. The system reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the form of a multinomial
distribution over words, and per-usage topic assignments (=
probabilistic sense assignments) in the form of a multinomial
distribution over topics. Following <cite class="ltx_cite"/>, we assign one
topic to each usage by selecting the topic that has the highest
cumulative probability density, based on the topic allocations of all
words in the context window for that usage.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>This includes all
words in the usage sentence except stopwords, which were filtered in
the preprocessing step.</span></span></span> Note that in their original work,
<cite class="ltx_cite"/> experimented with the use of features extracted
from a dependency parser. Due to the computational overhead associated
with these features, and the fact that the empirical impact of the
features was found to be marginal, we make no use of parser-based
features in this paper.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>For hyper-parameters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="\gamma" display="inline"><mi>γ</mi></math>, we used 0.1 for both. We did not tune the parameters, and
opted to use the default parameters introduced in <cite class="ltx_cite"/>.</span></span></span></p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The induced topics take the form of word multinomials, and are often
represented by the top-<math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p3.m1" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> words in descending order of conditional
probability. We interpret each topic as a sense of the target
lemma.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>To avoid confusion, we will refer to the HDP-induced
topics as <span class="ltx_text ltx_font_italic">topics</span>, and reserve the term <span class="ltx_text ltx_font_italic">sense</span> to denote
senses in a sense inventory.</span></span></span> To illustrate this, we give the example
of topics induced by the HDP model for <span class="ltx_text ltx_font_italic">network</span> in
Table <a href="#S3.T1" title="Table 1 ‣ 3 Methodology ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">We refer to this method as HDP-WSIhenceforth.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>The code used to learn
predominant sense and run all experiments described in this paper is
available at: <a href="https://github.com/jhlau/predom_sense" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://github.com/jhlau/predom_sense</span></a>.</span></span></span></p>
</div>
<div id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Topic Num</span></th>
<th class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Top-10 Terms</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">1</span></th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_footnote">network support @card@ information research service group
development community member</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">2</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">service @card@ road company transport rail area government network
public</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">3</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">network social model system family structure analysis form
relationship neural</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">4</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">network @card@ computer system service user access internet datum
server</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">5</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">system network management software support corp company service
application product</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">6</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">@card@ radio news television show bbc programme call think film</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">7</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">police drug criminal terrorist intelligence network vodafone iraq
attack cell</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">8</span></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_footnote">network atm manager performance craigavon group conference working
modelling assistant</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">9</span></th>
<td class="ltx_td ltx_align_left ltx_border_b"><span class="ltx_text ltx_font_footnote">root panos comenius etd unipalm lse brazil telephone xxx discuss</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_footnote"><span class="ltx_tag ltx_tag_table">Table 1: </span>An example to illustrate the topics induced for <span class="ltx_text ltx_font_italic">network</span>
by the HDP model. The top-10 highest probability terms are displayed
to represent each topic (<span class="ltx_text ltx_font_italic">@card@</span> denotes a tokenised cardinal number).</div>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">In predominant sense acquisition, the task is to learn, for each target
lemma, the most frequently occurring word sense in a particular domain
or corpus, relative to a predefined sense inventory. The WSI system
provides us with a topic allocation per usage of a given word, from
which we can derive a distribution of topics over usages and a
<span class="ltx_text ltx_font_bold">predominant topic</span>. In order to map this onto the
<span class="ltx_text ltx_font_bold">predominant sense</span>, we need to have some way of aligning a
topic with a sense. We design our topic–sense alignment methodology
with portability in mind — it should be applicable to any sense
inventory. As such, our alignment methodology assumes only that we have
access to a conventional sense gloss or definition for each sense, and
does not rely on ontological/structural knowledge (e.g. the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>hierarchy).</p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">To compute the similarity between a sense and a topic, we first convert
the words in the gloss/definition into a multinomial distribution over
words, based on simple maximum likelihood estimation.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>Words are
tokenised using OpenNLP and lemmatised with Morpha
<cite class="ltx_cite">[]</cite>. We additionally remove the target lemma, stopwords
and words that are less than 3 characters in length.</span></span></span>
We then calculate the Jensen–Shannon divergence between the multinomial
distribution (over words) of the gloss and that of the topic, and
convert the divergence value into a similarity score by subtracting it
from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m1" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math>. Formally, the similarity sense <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m2" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> and topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m3" class="ltx_Math" alttext="t_{j}" display="inline"><msub><mi>t</mi><mi>j</mi></msub></math> is:</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m1" class="ltx_Math" alttext="\displaystyle\text{sim}(s_{i},t_{j})" display="inline"><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E1.m2" class="ltx_Math" alttext="\displaystyle=1-\text{JS}(S\|T)" display="inline"><mrow><mo>=</mo><mn>1</mn><mo>-</mo><mtext>JS</mtext><mrow><mo>(</mo><mi>S</mi><mo>∥</mo><mi>T</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m4" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m5" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> are the multinomial distributions over words for
sense <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m6" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> and topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m7" class="ltx_Math" alttext="t_{j}" display="inline"><msub><mi>t</mi><mi>j</mi></msub></math>, respectively, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m8" class="ltx_Math" alttext="\text{JS}(X\|Y)" display="inline"><mrow><mtext>JS</mtext><mrow><mo>(</mo><mi>X</mi><mo>∥</mo><mi>Y</mi><mo>)</mo></mrow></mrow></math> is the
Jensen–Shannon divergence for distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m9" class="ltx_Math" alttext="X" display="inline"><mi>X</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m10" class="ltx_Math" alttext="Y" display="inline"><mi>Y</mi></math>.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p class="ltx_p">To learn the predominant sense, we compute the <span class="ltx_text ltx_font_bold">prevalence
score</span> of each sense and take the sense with the highest prevalence
score as the predominant sense. The prevalence score for a sense is
computed by summing the product of its similarity scores with each topic
(i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m1" class="ltx_Math" alttext="\text{sim}(s_{i},t_{j})" display="inline"><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math>) and the prior probability of the topic in
question (based on maximum likelihood estimation). Formally, the
prevalence score of sense <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m2" class="ltx_Math" alttext="s_{i}" display="inline"><msub><mi>s</mi><mi>i</mi></msub></math> is given as follows:</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align">

<tr id="S3.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m1" class="ltx_Math" alttext="\displaystyle\text{prevalence}(s_{i})" display="inline"><mrow><mtext>prevalence</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E2.m2" class="ltx_Math" alttext="\displaystyle=\sum_{j}^{T}\left(\text{sim}(s_{i},t_{j})\times P(t_{j})\right)" display="inline"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi><mi>T</mi></munderover></mstyle><mrow><mo>(</mo><mrow><mrow><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>×</mo><mi>P</mi></mrow><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
<tr id="S3.Ex1" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.Ex1.m2" class="ltx_Math" alttext="\displaystyle=\sum_{j}^{T}\left(\text{sim}(s_{i},t_{j})\times\frac{f(t_{j})}{%&#10;\sum_{k}^{T}f(t_{k})}\right)" display="inline"><mrow><mi/><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi><mi>T</mi></munderover></mstyle><mrow><mo>(</mo><mrow><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>×</mo><mstyle displaystyle="true"><mfrac><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>k</mi><mi>T</mi></msubsup><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>k</mi></msub><mo>)</mo></mrow></mrow></mrow></mfrac></mstyle></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m3" class="ltx_Math" alttext="f(t_{j})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math> is the frequency of topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m4" class="ltx_Math" alttext="t_{j}" display="inline"><msub><mi>t</mi><mi>j</mi></msub></math> (i.e. the number of
usages assigned to topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m5" class="ltx_Math" alttext="t_{j}" display="inline"><msub><mi>t</mi><mi>j</mi></msub></math>), and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p7.m6" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> is the number of topics.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p class="ltx_p">The intuition behind the approach is that the predominant sense should
be the sense that has relatively high similarity (in terms of lexical
overlap) with high-probability topic(s).</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We first test the proposed method over the tasks of predominant sense
learning and
sense distribution induction, using the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>-tagged dataset
of <cite class="ltx_cite"/>, which is made up of 3 collections of documents:
a domain-neutral corpus (<span class="ltx_text ltx_font_smallcaps">BNC</span>), and two domain-specific corpora (<span class="ltx_text ltx_font_smallcaps">SPORTS</span>and <span class="ltx_text ltx_font_smallcaps">FINANCE</span>). For each domain, annotators were asked to sense-annotate
a random selection of sentences for each of 40 target nouns, based on
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>v1.7. The predominant sense and distribution across senses for
each target lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in terms of <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy over a given corpus, based on assigning all instances of a
target word with the predominant sense learned from that corpus. For the
remainder of the paper, we denote their system as
MKWC.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">To compare our system (HDP-WSI) with MKWC, we apply it to the three
datasets of <cite class="ltx_cite"/>. For each dataset, we use HDP to
induce topics for each target lemma, compute the similarity between the
topics and the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>senses (Equation (<a href="#S3.E1" title="(1) ‣ 3 Methodology ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)), and rank the senses
based on the prevalence scores (Equation (<a href="#S3.E2" title="(2) ‣ 3 Methodology ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>)). In addition to
the <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy based on the predominant sense inferred from a
particular corpus, we additionally compute: (1) <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m1" class="ltx_Math" alttext="\text{Acc}_{\text{$\text{\sc ub}$}}" display="inline"><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub></math>, the upper
bound for the first sense-based <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy (using the gold standard
predominant sense for disambiguation);<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup>The upper bound for a
<span class="ltx_text ltx_font_smallcaps">wsd</span>approach which tags all token occurrences of a given word with
the same sense, as a first step towards context-sensitive unsupervised
<span class="ltx_text ltx_font_smallcaps">wsd</span>.</span></span></span> and (2) <span class="ltx_text ltx_markedasmath">ERR</span>, the error rate
reduction between the accuracy for a given system (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m3" class="ltx_Math" alttext="\text{Acc}_{\text{}}" display="inline"><msub><mtext>Acc</mtext><mrow/></msub></math>) and the upper
bound (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p2.m4" class="ltx_Math" alttext="\text{Acc}_{\text{$\text{\sc ub}$}}" display="inline"><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub></math>), calculated as follows:</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align">

<tr id="S4.Ex2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.Ex2.m1" class="ltx_Math" alttext="\displaystyle\text{ERR}=1-\frac{\text{Acc}_{\text{$\text{\sc ub}$}}-\text{Acc}%&#10;_{\text{}}}{\text{Acc}_{\text{$\text{\sc ub}$}}}" display="inline"><mrow><mtext>ERR</mtext><mo>=</mo><mrow><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><mfrac><mrow><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub><mo>-</mo><msub><mtext>Acc</mtext><mrow/></msub></mrow><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub></mfrac></mstyle></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">Looking at the results in Table <a href="#S4.T2" title="Table 2 ‣ 4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see little
difference in the results for the two methods, with MKWCperforming
better over two of the datasets (<span class="ltx_text ltx_font_smallcaps">BNC</span>and <span class="ltx_text ltx_font_smallcaps">SPORTS</span>) and HDP-WSIperforming better over the third (<span class="ltx_text ltx_font_smallcaps">FINANCE</span>), but all differences are
small. Based on the McNemar’s Test with Yates correction for continuity,
MKWCis significantly better over <span class="ltx_text ltx_font_smallcaps">BNC</span>and HDP-WSIis significantly
better over <span class="ltx_text ltx_font_smallcaps">FINANCE</span>(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m1" class="ltx_Math" alttext="p&lt;0.0001" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow></math> in both cases), but the difference
over <span class="ltx_text ltx_font_smallcaps">SPORTS</span>is not statistically significance (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m2" class="ltx_Math" alttext="p&gt;0.1" display="inline"><mrow><mi>p</mi><mo>&gt;</mo><mn>0.1</mn></mrow></math>). Note that
there is still much room for improvement with both systems, as we see in
the gap between the upper bound (based on perfect determination of the
first sense) and the respective system accuracies.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">Given that both systems compute a continuous-valued prevalence score for
each sense of a target lemma, a distribution of senses can be obtained
by normalising the prevalence scores across all senses. The predominant
sense learning task of <cite class="ltx_cite"/> evaluates the ability of
a method to identify only the head of this distribution, but it is also
important to evaluate the full sense distribution <cite class="ltx_cite">[]</cite>. To
this end, we introduce a second evaluation metric: the Jensen–Shannon
(JS) divergence between the inferred sense distribution and the
gold-standard sense distribution, noting that smaller values are better
in this case, and that it is now theoretically possible to obtain a JS
divergence of 0 in the case of a perfect estimate of the sense
distribution. Results are presented in Table <a href="#S4.T3" title="Table 3 ‣ 4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">HDP-WSIconsistently achieves lower JS divergence, indicating that the
distribution of senses that it finds is closer to the gold standard
distribution. Testing for statistical significance over the paired JS divergence values for
each lemma using the Wilcoxon signed-rank test, the result for <span class="ltx_text ltx_font_smallcaps">FINANCE</span>is significant (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow></math>) but the
results for the other two datasets are not (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p5.m2" class="ltx_Math" alttext="p&gt;0.1" display="inline"><mrow><mi>p</mi><mo>&gt;</mo><mn>0.1</mn></mrow></math> in each
case).</p>
</div>
<div id="S4.T2" class="ltx_table"><span class="ltx_ERROR undefined">\smaller</span>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2">Dataset</th>
<th class="ltx_td ltx_align_center ltx_border_t">FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m1" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math></th>
<th class="ltx_td ltx_align_center ltx_border_t" colspan="2">MKWC</th>
<th class="ltx_td ltx_align_center ltx_border_t" colspan="2">HDP-WSI</th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m2" class="ltx_Math" alttext="\text{Acc}_{\text{$\text{\sc ub}$}}" display="inline"><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub></math></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m3" class="ltx_Math" alttext="\text{Acc}_{\text{}}" display="inline"><msub><mtext>Acc</mtext><mrow/></msub></math></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_markedasmath">ERR</span></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m5" class="ltx_Math" alttext="\text{Acc}_{\text{}}" display="inline"><msub><mtext>Acc</mtext><mrow/></msub></math></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_markedasmath">ERR</span></th></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_smallcaps">BNC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.407</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_ERROR undefined">\smaller</span>(0.777)</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.376</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_ERROR undefined">\smaller</span>(0.718)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">FINANCE</span></td>
<td class="ltx_td ltx_align_center">0.801</td>
<td class="ltx_td ltx_align_center">0.499</td>
<td class="ltx_td ltx_align_center"><span class="ltx_ERROR undefined">\smaller</span>(0.623)</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.555</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_ERROR undefined">\smaller</span>(0.693)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_smallcaps">SPORTS</span></td>
<td class="ltx_td ltx_align_center ltx_border_b">0.774</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">0.437</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_ERROR undefined">\smaller</span>(0.565)</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.422</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_ERROR undefined">\smaller</span>(0.545)</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy for MKWCand HDP-WSIon the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>-annotated datasets, as compared to the upper-bound based on actual first sense in
the corpus (higher values indicate
better performance; the <span class="ltx_text ltx_font_bold">best</span> system in each row [other than the
FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T2.m8" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math> upper bound] is indicated in boldface).</div>
</div>
<div id="S4.T3" class="ltx_table"><span class="ltx_ERROR undefined">\smaller</span>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t">Dataset</th>
<th class="ltx_td ltx_align_center ltx_border_t">MKWC</th>
<th class="ltx_td ltx_align_center ltx_border_t">HDP-WSI</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_smallcaps">BNC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.226</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.214</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">FINANCE</span></td>
<td class="ltx_td ltx_align_center">0.426</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.375</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_smallcaps">SPORTS</span></td>
<td class="ltx_td ltx_align_center ltx_border_b">0.420</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">0.363</span></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Sense distribution evaluation of MKWCand HDP-WSIon the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>-annotated datasets, evaluated using
JS divergence (lower values indicate better performance; the <span class="ltx_text ltx_font_bold">best</span> system in each row is indicated in boldface).</div>
</div>
<div id="S4.T4" class="ltx_table"><span class="ltx_ERROR undefined">\smaller</span>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2">Dataset</th>
<th class="ltx_td ltx_align_center ltx_border_t">FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m1" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math></th>
<th class="ltx_td ltx_align_center ltx_border_t" colspan="2">FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m2" class="ltx_Math" alttext="{}_{\text{\sc dict}}" display="inline"><msub><mi/><mtext mathvariant="normal">dict</mtext></msub></math></th>
<th class="ltx_td ltx_align_center ltx_border_t" colspan="2">HDP-WSI</th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m3" class="ltx_Math" alttext="\text{Acc}_{\text{$\text{\sc ub}$}}" display="inline"><msub><mtext>Acc</mtext><mtext mathvariant="normal">ub</mtext></msub></math></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m4" class="ltx_Math" alttext="\text{Acc}_{\text{}}" display="inline"><msub><mtext>Acc</mtext><mrow/></msub></math></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_markedasmath">ERR</span></th>
<th class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m6" class="ltx_Math" alttext="\text{Acc}_{\text{}}" display="inline"><msub><mtext>Acc</mtext><mrow/></msub></math></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_markedasmath">ERR</span></th></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_smallcaps">ukWaC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.574</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.387</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_ERROR undefined">\smaller</span>(0.674)</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.514</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_ERROR undefined">\smaller</span>(0.895)</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_smallcaps">Twitter</span></td>
<td class="ltx_td ltx_align_center ltx_border_b">0.468</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.297</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_ERROR undefined">\smaller</span>(0.635)</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">0.335</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_ERROR undefined">\smaller</span>(0.716)</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy for HDP-WSIon the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>-annotated datasets, as compared to the upper-bound based on actual first sense in
the corpus (higher values indicate
better performance; the <span class="ltx_text ltx_font_bold">best</span> system in each row [other than the
FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T4.m9" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math> upper bound] is indicated in boldface).</div>
</div>
<div id="S4.T5" class="ltx_table"><span class="ltx_ERROR undefined">\smaller</span>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t">Dataset</th>
<th class="ltx_td ltx_align_center ltx_border_t">FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m1" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math></th>
<th class="ltx_td ltx_align_center ltx_border_t">FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.T5.m2" class="ltx_Math" alttext="{}_{\text{\sc dict}}" display="inline"><msub><mi/><mtext mathvariant="normal">dict</mtext></msub></math></th>
<th class="ltx_td ltx_align_center ltx_border_t">HDP-WSI</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_smallcaps">ukWaC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.210</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.393</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.156</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_smallcaps">Twitter</span></td>
<td class="ltx_td ltx_align_center ltx_border_b">0.259</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.472</td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_bold">0.171</span></td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Sense distribution evaluation of HDP-WSIon the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>-annotated datasets as compared to corpus- and
dictionary-based first sense methods, evaluated using
JS divergence (lower values indicate better performance; the <span class="ltx_text ltx_font_bold">best</span> system in each row is indicated in boldface).</div>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">To summarise, the results for MKWCand HDP-WSIare fairly even for
predominant sense learning (each outperforms the other at a level of
statistical significance over one dataset), but HDP-WSIis better at
inducing the overall sense distribution.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p">It is important to bear in mind that MKWCin these experiments
makes use of full-text parsing in calculating the distributional
similarity thesaurus, and the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>graph structure in calculating
the similarity between associated words and different senses. Our
method, on the other hand, uses no parsing, and only the synset
definitions (and not the graph structure) of
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><cite class="ltx_cite"/> obtained good results with
definition overlap, but their implementation uses the relation
structure alongside the
definitions <cite class="ltx_cite">[]</cite>. <cite class="ltx_cite">Iida<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib6" title="Gloss-based semantic similarity metrics for predominant sense acquisition" class="ltx_ref">2008</a>)</cite>
demonstrate that further extensions using distributional data are
required when applying the method to resources without hierarchical
relations.</span></span></span> The non-reliance on parsing is significant in terms of
portability to text sources which are less amenable to parsing (such as
Twitter: <cite class="ltx_cite">[]</cite>), and the non-reliance on the graph
structure of <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>is significant in terms of portability to
conventional “flat” sense inventories. While comparable results on a
different dataset have been achieved with a proximity thesaurus
<cite class="ltx_cite">[]</cite> compared to a dependency one,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup>The
thesauri used in the reimplementation of MKWCin this paper were
obtained from
<a href="http://webdocs.cs.ualberta.ca/~lindek/downloads.htm" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://webdocs.cs.ualberta.ca/~lindek/downloads.htm</span></a>.</span></span></span> it is not
stated how wide a window is needed for the proximity thesaurus. This
could be a significant issue with Twitter data, where context tends to
be limited. In the next section, we demonstrate the robustness of the
method in experimenting with two new datasets, based on Twitter and a
web corpus, and the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan English Dictionary</span>.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In our second set of experiments, we move to a new dataset
<cite class="ltx_cite">[<a href="#bib.bib29" title="One sense per tweeter … and other lexical semantic tales of Twitter" class="ltx_ref">9</a>]</cite>
based on text from ukWaC <cite class="ltx_cite">[<a href="#bib.bib3" title="Introducing and evaluating ukWaC, a very large web-derived corpus of English" class="ltx_ref">8</a>]</cite> and Twitter, and annotated using the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan English
Dictionary<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><a href="http://www.macmillandictionary.com/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.macmillandictionary.com/</span></a></span></span></span></span>
(henceforth “<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>”). For the purposes of this research, the
choice of <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>is significant in that it is a conventional
dictionary with sense definitions and examples, but no linking between
senses.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>Strictly speaking, there is limited linking in the form
of sets of synonyms in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>, but we choose to not use this
information in our research.</span></span></span> In terms of the original research which
gave rise to the sense-tagged dataset, <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>was chosen over
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>for reasons including: (1) the well-documented difficulties of
sense tagging with fine-grained <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>senses
<cite class="ltx_cite">[]</cite>; (2) the regular update cycle of
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>(meaning it contains many recently-emerged senses); and (3)
the finding in a preliminary sense-tagging task that it better captured
Twitter usages than <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>(and also <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">OntoNotes</span>:
<cite class="ltx_cite"/>).</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">The dataset is made up of 20 target nouns which were selected to span
the high- to mid-frequency range in both Twitter and the ukWaC corpus,
and have at least 3 <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>senses. The average sense ambiguity of
the 20 target nouns in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>is 5.6 (but 12.3 in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>). 100
usages of each target noun were sampled from each of Twitter (from a
crawl over the time period Jan 3–Feb 28, 2013 using the Twitter
Streaming API) and ukWaC, after language identification using <span class="ltx_text ltx_font_typewriter">langid.py<cite class="ltx_cite"><span class="ltx_text ltx_font_serif">[</span><span class="ltx_text ltx_font_serif">]</span></cite></span> and POS tagging (based on the CMU ARK Twitter
POS tagger v2.0 <cite class="ltx_cite">[]</cite> for Twitter, and the POS tags
provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was
then used to 5-way sense-tag each usage relative to <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>,
including allowing the annotators the option to label a usage as
“Other” in instances where the usage was not captured by any of the
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>senses. After quality control over the annotators/annotations
(see <cite class="ltx_cite">Gella<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib29" title="One sense per tweeter … and other lexical semantic tales of Twitter" class="ltx_ref">to appear</a>)</cite> for details), and aggregation of the
annotations into a single sense per usage (possibly “Other”), there
were 2000 sense-tagged ukWaC sentences and Twitter messages over the 20
target nouns. We refer to these two datasets as <span class="ltx_text ltx_font_smallcaps">ukWaC</span>and <span class="ltx_text ltx_font_smallcaps">Twitter</span>henceforth.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">To apply our method to the two datasets, we use HDP-WSIto train a model
for each target noun, based on the combined set of usages of that lemma
in each of the two background corpora, namely the original Twitter crawl
that gave rise to the <span class="ltx_text ltx_font_smallcaps">Twitter</span>dataset, and all of ukWaC.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Learning Sense Distributions</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">As in Section <a href="#S4" title="4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we evaluate in terms of <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy
(Table <a href="#S4.T4" title="Table 4 ‣ 4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and JS divergence over the
gold-standard sense distribution (Table <a href="#S4.T5" title="Table 5 ‣ 4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). We also
present the results for: (a) a supervised baseline
(“FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m1" class="ltx_Math" alttext="{}_{\text{\sc corpus}}" display="inline"><msub><mi/><mtext mathvariant="normal">corpus</mtext></msub></math>”), based on the most frequent sense in the
corpus; and (b) an unsupervised baseline (“FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p1.m2" class="ltx_Math" alttext="{}_{\text{\sc dict}}" display="inline"><msub><mi/><mtext mathvariant="normal">dict</mtext></msub></math>”), based on the
first-listed sense in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>. In each case, the sense distribution
is based on allocating all probability mass for a given word to the
single sense identified by the respective method.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">We first notice that, despite the coarser-grained senses of <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>as compared to <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>, the upper bound <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy using
<span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>is comparable to that of the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>-based datasets over
the balanced BNC, and quite a bit lower than that of the two domain
corpora of <cite class="ltx_cite"/>. This suggests that both datasets
are diverse in domain and content.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">In terms of <span class="ltx_text ltx_font_smallcaps">wsd</span>accuracy, the results over <span class="ltx_text ltx_font_smallcaps">ukWaC</span>(<span class="ltx_text ltx_markedasmath">ERR</span>= 0.895) are
substantially higher than those for <span class="ltx_text ltx_font_smallcaps">BNC</span>, while those over <span class="ltx_text ltx_font_smallcaps">Twitter</span>(<span class="ltx_text ltx_markedasmath">ERR</span>= 0.716) are comparable. The accuracy is significantly higher than
the dictionary-based first sense baseline (FS<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p3.m3" class="ltx_Math" alttext="{}_{\text{\sc dict}}" display="inline"><msub><mi/><mtext mathvariant="normal">dict</mtext></msub></math>) over both
datasets (McNemar’s test; <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p3.m4" class="ltx_Math" alttext="p&lt;0.0001" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow></math>), and the <span class="ltx_text ltx_markedasmath">ERR</span>is also
considerably higher than for the two domain datasets in Section <a href="#S4" title="4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
(<span class="ltx_text ltx_font_smallcaps">FINANCE</span>and <span class="ltx_text ltx_font_smallcaps">SPORTS</span>). One cause of difficulty in sense-modelling
<span class="ltx_text ltx_font_smallcaps">Twitter</span>is large numbers of missing senses, with 12.3% of usages in
<span class="ltx_text ltx_font_smallcaps">Twitter</span>and 6.6% in <span class="ltx_text ltx_font_smallcaps">ukWaC</span>having no corresponding <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>sense.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup>The relative occurrence of unlisted/unclear senses in
the datasets of <cite class="ltx_cite"/> is comparable to <span class="ltx_text ltx_font_smallcaps">ukWaC</span>.</span></span></span> This
challenges the assumption built into the sense prevalence calculation
that all topics will align to a pre-existing sense, a point we return
to in Section <a href="#S5.SS2" title="5.2 Identification of Unattested Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p">The JS divergence results for both datasets are well below (= better
than) the results for all three <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>-based datasets, and also
superior to both the supervised and unsupervised first-sense
baselines. Part of the reason for this improvement is simply that the
average polysemy in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>(5.6 senses per target lemma) is slightly
less than in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">WordNet</span>(6.7 senses per target lemma),<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup>Note that
the set of lemmas differs between the respective datasets, so this
isn’t an accurate reflection of the relative granularity of the two
dictionaries.</span></span></span> making the task slightly easier in the <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>case.</p>
</div>
<div id="S5.T6" class="ltx_table"><span class="ltx_ERROR undefined">\smaller</span>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T6.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T6.m2" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_center ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T6.m3" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_smallcaps">ukWaC</span></th>
<td class="ltx_td ltx_align_center ltx_border_t">0.73</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.74</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_smallcaps">Twitter</span></th>
<td class="ltx_td ltx_align_center ltx_border_b">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_b">0.65</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Evaluation of our method for identifying unattested senses,
averaged over 10 runs of 10-fold cross validation</div>
</div>
</div>
<div id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.2 </span>Identification of Unattested Senses</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p">We observed in Section <a href="#S5.SS1" title="5.1 Learning Sense Distributions ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> that there are relatively
frequent occurrences of usages (e.g. 12.3% for <span class="ltx_text ltx_font_smallcaps">Twitter</span>) which aren’t
captured by <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>. Conversely, there are also senses in <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>which aren’t attested in the annotated sample of usages. Specifically,
of the 112 senses defined for the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either corpora. Given that our
methodology computes a prevalence score for each sense, it can equally
be applied to the detection of these unattested senses, and it is this
task that we address in this section: the identification of senses that
are defined in the sense inventory but not attested in a given corpus.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p">Intuitively, an unused sense should have low similarity with the HDP
induced topics. As such, we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in the
corpus:</p>
<table id="S5.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E3.m1" class="ltx_Math" alttext="\text{st-affinity}(s_{i})=\frac{\sum^{T}_{j}\text{sim}(s_{i},t_{j})}{\sum^{S}_%&#10;{k}\sum^{T}_{l}\text{sim}(s_{k},t_{l})}" display="block"><mrow><mrow><mtext>st-affinity</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>j</mi><mi>T</mi></msubsup><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>k</mi><mi>S</mi></msubsup><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>l</mi><mi>T</mi></msubsup><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>k</mi></msub><mo>,</mo><msub><mi>t</mi><mi>l</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m1" class="ltx_Math" alttext="\text{sim}(s_{i},t_{j})" display="inline"><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math> is carried over from Equation (<a href="#S3.E1" title="(1) ‣ 3 Methodology ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m2" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p2.m3" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> represent the number of topics and senses, respectively.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p class="ltx_p">We treat the task of identification of unused senses as a binary
classification problem, where the goal is to find a sense-to-topic
affinity threshold below which a sense will be considered to be
unused. We pool together all the senses and run 10-fold cross validation
to learn the threshold for identifying unused senses,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup>We used a
fixed step and increment at steps of 0.001, up to the max value of
<span class="ltx_text ltx_markedasmath">st-affinity</span> when optimising the threshold.</span></span></span> evaluated using
sense-level precision (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m2" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math>), recall (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m3" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math>) and F-score
(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m4" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math>) at detecting unattested senses. We repeat the experiment 10
times (partitioning the items randomly into folds) and collect the mean
precision, recall and F-scores across the 10 runs. We found encouraging
results for the task, as detailed in Table <a href="#S5.T6" title="Table 6 ‣ 5.1 Learning Sense Distributions ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. For the
threshold, the average value with standard deviation is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m5" class="ltx_Math" alttext="0.092\pm 0.044" display="inline"><mrow><mn>0.092</mn><mo>±</mo><mn>0.044</mn></mrow></math>
over <span class="ltx_text ltx_font_smallcaps">ukWaC</span>and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS2.p3.m6" class="ltx_Math" alttext="0.125{\pm}0.052" display="inline"><mrow><mn>0.125</mn><mo>±</mo><mn>0.052</mn></mrow></math> over <span class="ltx_text ltx_font_smallcaps">Twitter</span>, indicating relative
stability in the value of the threshold both internally within a
dataset, and also across datasets.</p>
</div>
</div>
<div id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.3 </span>Identification of Novel Senses</h3>

<div id="S5.T7" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">No. Lemmas with</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Relative Freq</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Threshold</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">P</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">R</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">F</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">a Removed Sense</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">of Removed Sense</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">Mean<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T7.m1" class="ltx_Math" alttext="\pm" display="inline"><mo mathsize="normal" mathvariant="normal" stretchy="false">±</mo></math>stdev</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">20</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.0–0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.052</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T7.m2" class="ltx_Math" alttext="\pm" display="inline"><mo>±</mo></math><span class="ltx_text ltx_font_footnote">0.009</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.36</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">9</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.2–0.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.089</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T7.m3" class="ltx_Math" alttext="\pm" display="inline"><mo>±</mo></math><span class="ltx_text ltx_font_footnote">0.024</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.24</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.59</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.29</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">6</span></th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.4–0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.061</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T7.m4" class="ltx_Math" alttext="\pm" display="inline"><mo>±</mo></math><span class="ltx_text ltx_font_footnote">0.004</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.63</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_footnote"><span class="ltx_tag ltx_tag_table">Table 7: </span>Classification of usages with novel sense for all target lemmas.</div>
</div>
<div id="S5.T8" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">No. Lemmas with</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Relative Freq</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Threshold</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">P</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">R</span></th>
<th class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span class="ltx_text ltx_font_bold ltx_font_footnote">F</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">a Removed Sense</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">of Removed Sense</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">Mean<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T8.m1" class="ltx_Math" alttext="\pm" display="inline"><mo mathsize="normal" mathvariant="normal" stretchy="false">±</mo></math>stdev</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">9</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.2–0.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.093</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T8.m2" class="ltx_Math" alttext="\pm" display="inline"><mo>±</mo></math><span class="ltx_text ltx_font_footnote">0.023</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.52</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">6</span></th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.4–0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.099</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T8.m3" class="ltx_Math" alttext="\pm" display="inline"><mo>±</mo></math><span class="ltx_text ltx_font_footnote">0.018</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.80</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_footnote"><span class="ltx_tag ltx_tag_table">Table 8: </span>Classification of usages with novel sense for target lemmas
with a removed sense.</div>
</div>
<div id="S5.T9" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">No. of Lemmas with</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">No. of Lemmas without</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Relative Freq</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold ltx_font_footnote">Wilcoxon Rank Sum</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">a Removed Sense</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">a Removed Sense</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_footnote">of
Removed Sense</span></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T9.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math><span class="ltx_text ltx_font_bold ltx_font_footnote">-value</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">10</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.0–0.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_footnote">0.4543</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">9</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">11</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.2–0.4</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_footnote">0.0391</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">6</span></th>
<th class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">14</span></th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.4–0.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_footnote">0.0247</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_footnote"><span class="ltx_tag ltx_tag_table">Table 9: </span>Wilcoxon Rank Sum <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T9.m3" class="ltx_Math" alttext="p" display="inline"><mi mathsize="normal" stretchy="false">p</mi></math>-value results for testing target lemmas
with removed sense vs. target lemmas without removed sense using
novelty.</div>
</div>
<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p">In both <span class="ltx_text ltx_font_smallcaps">Twitter</span>and <span class="ltx_text ltx_font_smallcaps">ukWaC</span>, we observed frequent occurrences of usages
of our target nouns which didn’t map onto a pre-existing <span class="ltx_ERROR undefined">\smaller</span><span class="ltx_text ltx_font_sansserif">Macmillan</span>sense. A natural question to ask is whether our method can be used to
predict word senses that are missing from our sense inventory, and
identify usages associated with each such missing sense. We will term
these “novel senses”, and define “novel sense identification” to be
the task of identifying new senses that are not recorded in the
inventory but are seen in the corpus.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p">An immediate complication in evaluating novel sense identification is
that we are attempting to identify senses which explicitly aren’t in our
sense inventory. This contrasts with the identification of unattested
senses, e.g., where we were attempting to identify which of the
<span class="ltx_text ltx_font_italic">known</span> senses wasn’t observed in the corpus. Also, while we have
annotations of “Other” usages in <span class="ltx_text ltx_font_smallcaps">Twitter</span>and <span class="ltx_text ltx_font_smallcaps">ukWaC</span>, there is no real
expectation that all such usages will correspond to the same sense: in
practice, they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword expression, and errors in
POS tagging (i.e. the usage not being nominal). As such, we can’t use
the “Other” annotations to evaluate novel sense identification. The
evaluation of systems for this task is a known challenge, which we
address similarly to <cite class="ltx_cite">Erk (<a href="#bib.bib27" title="Unknown word sense detection as outlier detection" class="ltx_ref">2006</a>)</cite> by artificially synthesising
novel senses through removal of senses from the sense inventory. In this way,
even if we remove multiple senses for a given word, we still have access
to information about which usages correspond to which novel sense. An
additional advantage of this procedure is that it allows us to control
an important property of novel senses: their frequency of occurrence.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p class="ltx_p">In the experiments that follow, we randomly select senses for removal
from three frequency bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the annotated usages: low
= 0.0–0.2; medium = 0.2–0.4; and high = 0.4–0.6. Note that we do not
consider high-frequency senses with frequency higher than 0.6, as it is
rare for a medium- to high-frequency word to take on a novel sense which
is then the predominant sense in a given corpus. Note also that not all
target lemmas will have a novel sense through synthesis, as they may
have no senses that fall within the indicated bounds of relative
occurrence (e.g. if <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p3.m1" class="ltx_Math" alttext="&gt;60\%" display="inline"><mrow><mi/><mo>&gt;</mo><mrow><mn>60</mn><mo>⁢</mo><mi mathvariant="normal">%</mi></mrow></mrow></math> of usages are a single sense). For
example, only 6 of our 20 target nouns have senses which are candidates
for high-frequency novel senses.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p class="ltx_p">As before, we treat the novel sense identification task as a
classification problem, although with a significantly different
formulation: we are no longer attempting to identify pre-existing
senses, as novel senses are by definition not included in the sense
inventory. Instead, we are seeking to identify clusters of usages which
are instances of a novel sense, e.g. for presentation to a
lexicographer as part of a dictionary update process
<cite class="ltx_cite">[]</cite>. That is, for each usage, we
want to classify whether it is an instance of a given novel sense.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p class="ltx_p">A usage that corresponds to a novel sense should have a topic that does
not align well with any of the pre-existing senses in the sense
inventory. Based on this intuition, we introduce topic-to-sense affinity
to estimate the similarity of a topic to the set of senses, as follows:</p>
<table id="S5.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E4.m1" class="ltx_Math" alttext="\text{ts-affinity}(t_{j})=\frac{\sum^{S}_{i}\text{sim}(s_{i},t_{j})}{\sum^{T}_%&#10;{l}\sum^{S}_{k}\text{sim}(s_{k},t_{l})}" display="block"><mrow><mrow><mtext>ts-affinity</mtext><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>i</mi><mi>S</mi></msubsup><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>l</mi><mi>T</mi></msubsup><mrow><msubsup><mo largeop="true" symmetric="true">∑</mo><mi>k</mi><mi>S</mi></msubsup><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>k</mi></msub><mo>,</mo><msub><mi>t</mi><mi>l</mi></msub></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">where, once again, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p5.m1" class="ltx_Math" alttext="\text{sim}(s_{i},t_{j})" display="inline"><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math> is defined as in Equation (<a href="#S3.E1" title="(1) ‣ 3 Methodology ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>),
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p5.m2" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p5.m3" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math> represent the number of topics and senses, respectively.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p class="ltx_p">Using topic-to-sense affinity as the sole feature, we pool together all
instances and optimise the affinity feature to classify instances that
have novel senses. Evaluation is done by computing the mean precision,
recall and F-score across 10 separate runs; results are summarised in
Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Identification of Novel Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Note that we evaluate only over
<span class="ltx_text ltx_font_smallcaps">ukWaC</span>in this section, for ease of presentation.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p class="ltx_p">The results show that instances with high-frequency novel senses are
more easily identifiable than instances with medium/low-frequency novel
senses. This is unsurprising given that high-frequency senses have a
higher probability of generating related topics (sense-related words are
observed more frequently in the corpus), and as such are more easily
identifiable.</p>
</div>
<div id="S5.SS3.p8" class="ltx_para">
<p class="ltx_p">We are interested in understanding whether pooling all instances —
instances from target lemmas that have a sense artificially removed and
those that do not — impacted the results (recall that not all target
lemmas have a removed sense). To that end, we chose to include only
instances from lemmas with a removed sense, and repeated the experiment
for the medium- and high-frequency novel sense condition (for the
low-frequency condition, all target lemmas have a novel sense). In
other words, we are assuming knowledge of which words have novel sense,
and the task is to identify specifically what the novel sense is, as
represented by novel usages. Results are presented in
Table <a href="#S5.T8" title="Table 8 ‣ 5.3 Identification of Novel Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S5.SS3.p9" class="ltx_para">
<p class="ltx_p">From the results, we see that the F-scores improved notably. This
reveals that an additional step is necessary to determine whether a
target lemma has a potential novel sense before feeding its instances to
learn which of them contains the usage of the novel sense.</p>
</div>
<div id="S5.SS3.p10" class="ltx_para">
<p class="ltx_p">In the last experiment, we propose a new measure to tackle this: the
identification of target lemmas that have a novel sense. We introduce
<span class="ltx_text ltx_markedasmath">novelty</span>, a measure of the likelihood of a target lemma <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p10.m2" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> having a
novel sense:</p>
<table id="S5.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.E5.m1" class="ltx_Math" alttext="\text{novelty}(w)=\min_{t_{j}}\left(\max_{s_{i}}\frac{\text{sim}(s_{i},t_{j})}%&#10;{f(t_{j})}\right)" display="block"><mrow><mrow><mtext>novelty</mtext><mo>⁢</mo><mrow><mo>(</mo><mi>w</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo movablelimits="false">min</mo><msub><mi>t</mi><mi>j</mi></msub></munder><mo>⁡</mo><mrow><mo>(</mo><mrow><munder><mo movablelimits="false">max</mo><msub><mi>s</mi><mi>i</mi></msub></munder><mo>⁡</mo><mfrac><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p10.m3" class="ltx_Math" alttext="f(t_{j})" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>t</mi><mi>j</mi></msub><mo>)</mo></mrow></mrow></math> is the frequency of topic <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p10.m4" class="ltx_Math" alttext="t_{j}" display="inline"><msub><mi>t</mi><mi>j</mi></msub></math> in the corpus. The
intuition behind <span class="ltx_text ltx_markedasmath">novelty</span> is that a target lemma with a novel sense
should have a (somewhat-)frequent topic that has low association with
any sense. That we use the frequency rather than the probability of the
topic here is deliberate, as topics with a higher raw number of
occurrences (whether as a low-probability topic for a high-frequency word, or a
high-probability topic for a low-frequency word) are indicative of a
novel word sense.</p>
</div>
<div id="S5.SS3.p11" class="ltx_para">
<p class="ltx_p">For each of our three datasets (with low-, medium- and high-frequency
novel senses, respectively), we compute the novelty of the target lemmas
and the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p11.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>-value of a one-tailed Wilcoxon rank sum test to test if the
two groups of lemmas (i.e. lemmas with a novel sense vs. lemmas
without a novel sense) are statistically different.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup>Note that
the number of words with low-frequency novel senses here is restricted
to 10 (cf. 20 in Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Identification of Novel Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) to ensure we have
both positive and negative lemmas in the dataset.</span></span></span> Results are
presented in Table <a href="#S5.T9" title="Table 9 ‣ 5.3 Identification of Novel Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We see that the novelty measure
can readily identify target lemmas with high- and medium-frequency novel
senses (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS3.p11.m2" class="ltx_Math" alttext="p&lt;0.05" display="inline"><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow></math>), but the results are less promising for the
low-frequency novel senses.</p>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">Our methodologies for the two proposed tasks of identifying unused and
novel senses are simple extensions to demonstrate the flexibility and
robustness of our methodology. Future
work could pursue a more sophisticated methodology, using non-linear
combinations of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m1" class="ltx_Math" alttext="\text{sim}(s_{i},t_{j})" display="inline"><mrow><mtext>sim</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>,</mo><msub><mi>t</mi><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></math> for computing the affinity measures
or multiple features in a supervised context. We contend, however, that
these extensions are ultimately a preliminary demonstration to the
flexibility and robustness of our methodology.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">A natural next step for this research would be to couple sense
distribution estimation and the detection of unattested senses with
evidence from the context, using topics or other information about the
local context (e.g. <cite class="ltx_cite"/>) to carry out
unsupervised <span class="ltx_text ltx_font_smallcaps">wsd</span>of individual token occurrences of a given word.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">In summary, we have proposed a topic modelling-based method for
estimating word sense distributions, based on Hierarchical Dirichlet
Processes and the earlier work of <cite class="ltx_cite"/> on word sense
induction, in probabilistically mapping the automatically-learned topics
to senses in a sense inventory. We evaluated the ability of the method
to learn predominant senses and induce word sense distributions, based
on a broad range of datasets and two separate sense inventories. In
doing so, we established that our method is comparable to the approach
of <cite class="ltx_cite"/> at predominant sense learning, and superior
at inducing word sense distributions. We further demonstrated the
applicability of the method to the novel tasks of detecting word senses
which are unattested in a corpus, and identifying novel senses which are
found in a corpus but not captured in a word sense inventory.</p>
</div>
<div id="S6.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Acknowledgements</h3>

<div id="S6.SSx1.p1" class="ltx_para">
<p class="ltx_p">We wish to thank the anonymous reviewers for their valuable comments. This research was
supported in part by funding from the Australian Research Council.</p>
</div>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Boyd-Graber, D. Blei and X. Zhu</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A topic model for word sense disambiguation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech Republic</span>, <span class="ltx_text ltx_bib_pages"> pp. 1024–1033</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D/D07/D07-1109" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Boyd-Graber and D. Blei</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PUTOP: turning predominant senses into a topic model for word sense disambiguation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech Republic</span>, <span class="ltx_text ltx_bib_pages"> pp. 277–281</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/S/S07/S07-1060" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. F. Cai, W. S. Lee and Y. W. Teh</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">NUS-ML: improving word sense disambiguation using topic features</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech Republic</span>, <span class="ltx_text ltx_bib_pages"> pp. 249–252</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/S/S07/S07-1053" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Carpuat, H. Daumé III, K. Henry, A. Irvine, J. Jagarlamudi and R. Rudinger</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SenseSpotting: never let your parallel data tie you to an old domain</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sofia, Bulgaria</span>, <span class="ltx_text ltx_bib_pages"> pp. 1435–1445</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. S. Chan and H. T. Ng</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word sense disambiguation with distribution estimation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, UK</span>, <span class="ltx_text ltx_bib_pages"> pp. 1010–1015</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p3" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. S. Chan and H. T. Ng</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Estimating class priors in domain adaptation for word sense disambiguation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sydney, Australia</span>, <span class="ltx_text ltx_bib_pages"> pp. 89–96</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P06-1012" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220175.1220187" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S2.p3" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Erk</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unknown word sense detection as outlier detection</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">New York City, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 128–135</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S5.SS3.p2" title="5.3 Identification of Novel Senses ‣ 5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.
</span></li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Ferraresi, E. Zanchetta, M. Baroni and S. Bernardini</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Introducing and evaluating ukWaC, a very large web-derived corpus of English</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Marrakech, Morocco</span>, <span class="ltx_text ltx_bib_pages"> pp. 47–54</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Gella, P. Cook and T. Baldwin</span><span class="ltx_text ltx_bib_year">(to appear)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">One sense per tweeter … and other lexical semantic tales of Twitter</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Gothenburg, Sweden</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p2" title="5 \smallerMacmillanExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span></li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Iida, D. McCarthy and R. Koeling</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gloss-based semantic similarity metrics for predominant sense acquisition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 561–568</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p7" title="4 \smallerWordNetExperiments ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Knopp, J. Völker and S. P. Ponzetto</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Topic modeling for word sense induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Darmstadt, Germany</span>, <span class="ltx_text ltx_bib_pages"> pp. 97–103</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Lapata and C. Brew</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Verb class disambiguation using informative priors</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">30</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 45–75</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p3" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Li, B. Roth and C. Sporleder</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Topic models for word sense disambiguation and token-based idiom detection</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 1138–1147</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P10-1116" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. McCarthy, R. Koeling, J. Weeds and J. Carroll</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic identification of infrequent word senses</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Geneva, Switzerland</span>, <span class="ltx_text ltx_bib_pages"> pp. 1220–1226</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. A. Miller, C. Leacock, R. Tengi and R. T. Bunker</span><span class="ltx_text ltx_bib_year">(1993)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A semantic concordance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 303–308</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Mohammad and G. Hirst</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Determining word sense dominance using a thesaurus</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Trento, Italy</span>, <span class="ltx_text ltx_bib_pages"> pp. 121–128</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Navigli and P. Velardi</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structural semantic interconnections: a knowledge-based approach to word sense disambiguation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> <span class="ltx_text ltx_bib_volume">27</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp. 1075–1088</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Peirsman, D. Geeraerts and D. Speelman</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The automatic identification of lexical variation between language varieties</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Natural Language Engineering</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 469–491</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Preiss and M. Stevenson</span><span class="ltx_text ltx_bib_year">(2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised domain tuning to improve word sense disambiguation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Atlanta, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 680–684</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N13-1079" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Zhong and H. T. Ng</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">It makes sense: a wide-coverage word sense disambiguation system for free text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 78–83</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P10-4014" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background and Related Work ‣ Learning Word Sense Distributions, Detecting Unattested Senses&#10;and Identifying Novel Senses Using Topic Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 17:23:06 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
