<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Experiments with crowdsourced re-annotation of a POS tagging data set</title>
<!--Generated on Wed Jun 11 17:50:37 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Experiments with crowdsourced re-annotation of a POS tagging data set</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dirk Hovy, Barbara Plank, and Anders S√∏gaard 
<br class="ltx_break"/>Center for Language Technology 
<br class="ltx_break"/>University of Copenhagen
<br class="ltx_break"/>Njalsgade 140, 2300 Copenhagen
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">dirk|bplank</span>}<span class="ltx_text ltx_font_typewriter">@cst.dk, soegaard@hum.ku.dk</span>

</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Crowdsourcing lets us collect multiple annotations for an item from several annotators. Typically, these are annotations for non-sequential classification tasks. While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced. This paper shows that workers <em class="ltx_emph">can</em> actually annotate sequential data almost as well as experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.</p>
</div>
<div id="p1" class="ltx_para">
<p class="ltx_p">plus1ptminus2ptplus1ptminus2pt</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often time-consuming and expensive. <span class="ltx_ERROR undefined">\newcite</span>snow2008 showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon‚Äôs Mechanical Turk has since been successfully used for various annotation tasks in NLP <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica <cite class="ltx_cite">[]</cite>. A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition¬†<cite class="ltx_cite">[]</cite>. Results for this are good. However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable.
The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>One of the reviewers alerted us to an unpublished masters thesis, which uses an app to reduce annotation to fewer multiple choice questions, though. See Related Work section for details.</span></span></span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the short-lived nature of Twitter messages, models quickly lose predictive power <cite class="ltx_cite">[]</cite>, and re-training models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE <cite class="ltx_cite">[]</cite>. We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations. We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks. We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality. We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexica <cite class="ltx_cite">[]</cite>. Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Our Approach</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">We crowdsource the training section of the data from <span class="ltx_ERROR undefined">\newcite</span>Gimpel:ea:11<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>http://www.ark.cs.cmu.edu/TweetNLP/</span></span></span> with POS tags. We use Crowdflower,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>http://crowdflower.com</span></span></span> to collect five annotations for each word, and then find the most likely label for each word among the possible annotations. See Figure <a href="#S2.tab1" title="2 Our Approach ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for an example. If the correct label is not among the annotations, we are unable to recover the correct answer. This was the case for 1497¬†instances in our data (cf. the token ‚Äú:‚Äù in the example). We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the missing tokens. Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs.¬†crowdsourced annotations and downstream applications thereof (chunking and NER). We take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations. All the data sets used in our experiments are publicly available at <a href="http://lowlands.ku.dk/results/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://lowlands.ku.dk/results/</span></a>.</p>
</div>
<div id="S2.tab1" class="ltx_table">
<span class="ltx_inline-block ltx_align_center" style="width:433.6pt;height:0px;vertical-align:-0.0pt;">
<table class="ltx_tabular ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m1" class="ltx_Math" alttext="\mathbf{x}" display="inline"><mi>ùê±</mi></math></th>
<th class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m2" class="ltx_Math" alttext="\mathbf{Z}" display="inline"><mi>ùêô</mi></math></th>
<th class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m3" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>ùê≤</mi></math></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">@USER</td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_typewriter">NOUN,NOUN,X,NOUN,-,NOUN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t">NOUN</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">:</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_typewriter">.,.,-,.,.,.</span></td>
<td class="ltx_td ltx_align_left">X</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">I</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_typewriter">PRON,NOUN,PRON,NOUN,PRON,-</span></td>
<td class="ltx_td ltx_align_left">PRON</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">owe</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_typewriter">VERB,VERB,-,VERB,VERB,VERB</span></td>
<td class="ltx_td ltx_align_left">VERB</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">U</td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_typewriter">PRON,X,-,NOUN,NOUN,PRON</span></td>
<td class="ltx_td ltx_align_left">PRON</td></tr>
</tbody>
</table>
</span>
<p class="ltx_p ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m4" class="ltx_Math" alttext="\theta=0.9,0.4,0.2,0.8,0.8,0.9" display="inline"><mrow><mi>Œ∏</mi><mo>=</mo><mrow><mn>0.9</mn><mo>,</mo><mn>0.4</mn><mo>,</mo><mn>0.2</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.8</mn><mo>,</mo><mn>0.9</mn></mrow></mrow></math></p><span class="ltx_ERROR undefined">\captionof</span>
<p class="ltx_p">figureFive annotations per token, supplied by 6 different annotators (<span class="ltx_text ltx_font_typewriter">-</span> = missing annotation), gold label <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m5" class="ltx_Math" alttext="\mathbf{y}" display="inline"><mi>ùê≤</mi></math>. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.m6" class="ltx_Math" alttext="\theta" display="inline"><mi>Œ∏</mi></math> = competence values for each annotator.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Crowdsourcing Sequential Annotation</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section <a href="#S5" title="5 Experiments ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), we follow Hovy et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Hovy:ea:14 in using the universal tag set ¬†<cite class="ltx_cite">[]</cite> with 12 labels.</p>
</div>
<div id="S3.fig1" class="ltx_figure"><img src="" id="S3.g1" class="ltx_graphics ltx_centering" alt=""/><span class="ltx_ERROR undefined ltx_centering">\captionof</span>
<p class="ltx_p ltx_align_center">figureScreen shot of the annotation interface on Crowdflower</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu. For each tag, we spell out the name of the syntactic category, and provide a few example words.
See Figure <a href="#S3.fig1" title="3 Crowdsourcing Sequential Annotation ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for a screenshot of the interface.
Annotators were also told that words can belong to several classes, depending on the context. No additional guidelines were given.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations. In total, 177¬†individual annotators supplied answers. We paid annotators a reward of $0.05 for 10 tokens. The full data set contains 14,619 tokens. Completion of the task took slightly less than 10 days. Contributors were very satisfied with the task (4.5 on a scale from 1 to 5). In particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5).</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Label Aggregation</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">After collecting the annotations, we need to aggregate the annotations to derive a single answer for each token. In the simplest scheme, we choose the majority label, i.e., the label picked by most annotators. In case of ties, we select the final label at random. Since this is a stochastic process, we average results over 100 runs. We refer to this as <span class="ltx_text ltx_font_smallcaps">Majority voting</span> (MV). Note that in MV we trust all annotators to the same degree. However, crowdsourcing attracts people with different motives, and not all of them are equally reliable‚Äîeven the ones with Bronze level. Ideally, we would like to factor this into our decision process.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">We use MACE<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="http://www.isi.edu/publications/licensed-sw/mace/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.isi.edu/publications/licensed-sw/mace/</span></a></span></span></span> <cite class="ltx_cite">[]</cite> as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators. MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM <cite class="ltx_cite">[]</cite>. We use MACE with default parameter settings to give us the weighted average for each annotated example.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">Finally, we also tried applying the joint learning scheme in Rodrigues et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Rodrigues:ea:13, but their scheme requires that entire sequences are annotated by the same annotators, which we don‚Äôt have, and it expects BIO sequences, rather than POS tags.</p>
</div>
<div id="S4.SS0.SSS0.P1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Dictionaries</h3>

<div id="S4.SS0.SSS0.P1.p1" class="ltx_para">
<p class="ltx_p">Decoding tasks profit from the use of dictionaries <cite class="ltx_cite">[]</cite> by restricting the number of tags that need to be considered for each word, also known as <em class="ltx_emph">type constraints</em> <cite class="ltx_cite">[]</cite>. We follow Li et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Li:ea:12 in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry. If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations. Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">Each of the two aggregation schemes above produces a final label sequence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p1.m1" class="ltx_Math" alttext="\mathbf{\hat{y}}" display="inline"><mover accent="true"><mi>ùê≤</mi><mo stretchy="false">^</mo></mover></math> for our training corpus. We evaluate the resulting annotated data in three ways.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">1. We compare <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m1" class="ltx_Math" alttext="\mathbf{\hat{y}}" display="inline"><mover accent="true"><mi>ùê≤</mi><mo stretchy="false">^</mo></mover></math> to the available expert annotation on the <em class="ltx_emph">training</em> data. This tells us how similar lay annotation is to professional annotation.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">2. Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out <em class="ltx_emph">test</em> data.
To test this, we train a CRF model <cite class="ltx_cite">[]</cite> with simple orthographic features and word clusters <cite class="ltx_cite">[]</cite> on the annotated Twitter data described in Gimpel et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Gimpel:ea:11. Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: <span class="ltx_text ltx_font_smallcaps">Ritter</span> (the 10%¬†test split of the data in Ritter et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Ritter:ea:11 used in Derczynski et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Derczynski:ea:13), the test set from <span class="ltx_ERROR undefined">\newcite</span>Foster:ea:11, and the data set described in Hovy et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Hovy:ea:14.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">We will make the preprocessed data sets available to the public to facilitate comparison. In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system <cite class="ltx_cite">[]</cite> re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference. We use parameter settings from Li et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Li:ea:12, as well as their Wikipedia dump, available from their project website.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><a href="https://code.google.com/p/wikily-supervised-pos-tagger/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://code.google.com/p/wikily-supervised-pos-tagger/</span></a></span></span></span></p>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p">3. POS tagging is often the first step for further analysis, such as chunking, parsing, etc. We test the downstream performance of the POS models from the previous step on chunking and NER. We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.
For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets.
For chunking, we use the test sets from <span class="ltx_ERROR undefined">\newcite</span>Foster:ea:11 and <span class="ltx_ERROR undefined">\newcite</span>Ritter:ea:11 (with the splits from <span class="ltx_ERROR undefined">\newcite</span>Derczynski:ea:13). For NER, we use data from <span class="ltx_ERROR undefined">\newcite</span>Finin:ea:10 and again <span class="ltx_ERROR undefined">\newcite</span>Ritter:ea:11.
For chunking, we follow¬†<span class="ltx_ERROR undefined">\newcite</span>Sha:Pereira:03 for the set of features, including token and POS information. For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><a href="http://www.ark.cs.cmu.edu/TweetNLP/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.ark.cs.cmu.edu/TweetNLP/</span></a></span></span></span> with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus¬†<cite class="ltx_cite">[]</cite>. We report macro-averages over all these data sets.</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.SS0.SSS0.P1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Agreement with expert annotators</h3>

<div id="S6.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">majority</th>
<td class="ltx_td ltx_align_center ltx_border_t">79.54</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">MACE-EM</th>
<td class="ltx_td ltx_align_center">79.89</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">majority+Wiktionary</th>
<td class="ltx_td ltx_align_center ltx_border_t">80.58</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">MACE-EM+Wiktionary</th>
<td class="ltx_td ltx_align_center">80.75</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">oracle</th>
<td class="ltx_td ltx_align_center ltx_border_t">89.63</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table¬†1: </span>Accuracy (%) of different annotations wrt gold data </div>
</div>
<div id="S6.SS0.SSS0.P1.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S6.T1" title="Table¬†1 ‚Ä£ Agreement with expert annotators ‚Ä£ 6 Results ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the accuracy of each aggregation compared to the gold labels.
The crowdsourced annotations aggregated using MV agree with the expert annotations in 79.54% of the cases. If we pre-filter the data using Wiktionary, the agreement becomes 80.58%. MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75). The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required.
Both schemes cannot recover the correct answer for the 1497¬†cases where none of the crowdsourced labels matched the gold label, i.e.¬†<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS0.SSS0.P1.p1.m1" class="ltx_Math" alttext="y\notin\mathbf{Z}_{i}" display="inline"><mrow><mi>y</mi><mo>‚àâ</mo><msub><mi>ùêô</mi><mi>i</mi></msub></mrow></math>. The best possible result either of them could achieve (the <em class="ltx_emph">oracle</em>) would be matching all but the missing labels, an agreement of 89.63%.</p>
</div>
<div id="S6.SS0.SSS0.P1.p2" class="ltx_para">
<p class="ltx_p">Most of the cases where the correct label was not among the annotations belong to a small set of confusions. The most frequent was mislabeling ‚Äú<span class="ltx_text ltx_font_italic">:</span>‚Äù and ‚Äú<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.SS0.SSS0.P1.p2.m1" class="ltx_Math" alttext="\ldots" display="inline"><mi mathvariant="normal">‚Ä¶</mi></math>‚Äù, both mapped to <span class="ltx_text ltx_font_italic">X</span>. Annotators mostly decided to label these tokens as punctuation (<span class="ltx_text ltx_font_italic">.</span>). They also predominantly labeled <span class="ltx_text ltx_font_italic">your</span>, <span class="ltx_text ltx_font_italic">my</span> and <span class="ltx_text ltx_font_italic">this</span> as <span class="ltx_text ltx_font_italic">PRON</span> (for the former two), and a variety of labels for the latter, when the gold label is <span class="ltx_text ltx_font_italic">DET</span>.</p>
</div>
</div>
<div id="S6.SS0.SSS0.P2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Effect on POS Tagging Accuracy</h3>

<div id="S6.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_rr"/>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">Ritter</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">Foster</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">Hovy</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt">Li et al.¬†(2012)</th>
<td class="ltx_td ltx_align_center ltx_border_tt">73.8</td>
<td class="ltx_td ltx_align_center ltx_border_tt">77.4</td>
<td class="ltx_td ltx_align_center ltx_border_tt">79.7</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MV</th>
<td class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td class="ltx_td ltx_align_center ltx_border_t">81.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">83.7</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">MACE</th>
<td class="ltx_td ltx_align_center">80.4</td>
<td class="ltx_td ltx_align_center">81.7</td>
<td class="ltx_td ltx_align_center">82.6</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MV+Wik</th>
<td class="ltx_td ltx_align_center ltx_border_t">80.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">83.7</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">MACE+Wik</th>
<td class="ltx_td ltx_align_center">80.5</td>
<td class="ltx_td ltx_align_center">81.9</td>
<td class="ltx_td ltx_align_center">83.7</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t" style="background-color:#E6E6E6;" colspan="4"><span class="ltx_text" style="background-color:#E6E6E6;">Upper bounds</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">oracle</th>
<td class="ltx_td ltx_align_center ltx_border_t">82.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">83.7</td>
<td class="ltx_td ltx_align_center ltx_border_t">85.1</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">gold</th>
<td class="ltx_td ltx_align_center">82.6</td>
<td class="ltx_td ltx_align_center">84.7</td>
<td class="ltx_td ltx_align_center">86.8</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table¬†2: </span>POS tagging accuracies (%). </div>
</div>
<div id="S6.SS0.SSS0.P2.p1" class="ltx_para">
<p class="ltx_p">Usually, we don‚Äôt want to match a gold standard, but we rather want to create new annotated training data. Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it. After all, inter-annotator agreement among professional annotators on this task is only around 90% <cite class="ltx_cite">[]</cite>. In order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme‚Äôs annotations and test on the same four data sets. Table <a href="#S6.T2" title="Table¬†2 ‚Ä£ Effect on POS Tagging Accuracy ‚Ä£ 6 Results ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results. Note that the differences between the four schemes are insignificant. More importantly, however, POS tagging accuracy using crowdsourced annotations are on average <span class="ltx_text ltx_font_italic">only 2.6%¬†worse</span>¬†than gold using professional annotations. On the other hand, performance is <span class="ltx_text ltx_font_italic">much better</span>¬†than the weakly supervised approach by Li et al.¬†<span class="ltx_ERROR undefined">\shortcite</span>Li:ea:12, which only relies on a crowdsourced POS lexicon.</p>
</div>
</div>
<div id="S6.SS0.SSS0.P3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Downstream Performance</h3>

<div id="S6.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">POS model from</th>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_smallcaps">chunking</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_smallcaps">NER</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt">MV</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">74.80</td>
<td class="ltx_td ltx_align_center ltx_border_tt">75.74</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">MACE</th>
<td class="ltx_td ltx_align_center ltx_border_r">75.04</td>
<td class="ltx_td ltx_align_center">75.83</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MV+Wik</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.86</td>
<td class="ltx_td ltx_align_center ltx_border_t">76.08</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">MACE+Wik</th>
<td class="ltx_td ltx_align_center ltx_border_r">75.86</td>
<td class="ltx_td ltx_align_center">76.15</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_t" style="background-color:#E6E6E6;" colspan="3"><span class="ltx_text" style="background-color:#E6E6E6;">Upper bounds</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">oracle</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.22</td>
<td class="ltx_td ltx_align_center ltx_border_t">75.85</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_rr">gold</th>
<td class="ltx_td ltx_align_center ltx_border_r">79.97</td>
<td class="ltx_td ltx_align_center">75.81</td></tr>
</tbody>
</table>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table¬†3: </span>Downstream accuracy for chunking (l) and NER (r) of models using POS. </div>
</div>
<div id="S6.SS0.SSS0.P3.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S6.T3" title="Table¬†3 ‚Ä£ Downstream Performance ‚Ä£ 6 Results ‚Ä£ Experiments with crowdsourced re-annotation of a POS tagging data set" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the accuracy when using the POS models trained in the previous evaluation step. Note that we present the average over the two data sets used for each task. Note also how the Wiktionary constraints lead to improvements in downstream performance. In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations. For NER, however, we find that some of the POS taggers trained on aggregated data produce better NER performance than POS taggers trained on expert-annotated gold data. Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be <span class="ltx_text ltx_font_italic">as good</span>¬†as those learned from expert annotations.</p>
</div>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Related Work</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables <cite class="ltx_cite">[]</cite>.
<span class="ltx_ERROR undefined">\newcite</span>Rodrigues:ea:13 recently presented a sequential model for this. They estimate annotator competence as latent variables in a CRF model using EM.
They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by <span class="ltx_ERROR undefined">\newcite</span>Dredze:ea:09. The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning.
Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible. Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators.</p>
</div><span class="ltx_ERROR undefined">\newcite</span>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">Mainzer:2011 actually presents an earlier paper on crowdsourcing POS tagging. However, it differs from our approach in several ways. It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet. The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder. This is highly effective, as it eliminates some sources of possible disagreement. In contrast, we do not pre-label any tokens, but always present the annotators with all labels.</p>
</div>
</div>
<div id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">We use crowdsourcing to collect POS annotations with minimal context (five-word windows). While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging. In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method. For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">We would like to thank the anonymous reviewers for valuable comments and feedback.
This research is funded by the ERC Starting Grant
LOWLANDS No.¬†313695.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p">./biblio.bib.</p>
</div>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:50:37 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
