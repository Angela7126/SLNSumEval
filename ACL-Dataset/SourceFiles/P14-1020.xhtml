<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Sparser, Better, Faster GPU Parsing</title>
<!--Generated on Tue Jun 10 17:20:22 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sparser, Better, Faster GPU Parsing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Hall  <span class="ltx_text ltx_phantom"><span style="visibility:hidden">and</span></span>  Taylor
Berg-Kirkpatrick  <span class="ltx_text ltx_phantom"><span style="visibility:hidden">and</span></span>  Dan Klein 
<br class="ltx_break"/>Computer Science Division 
<br class="ltx_break"/>University of California, Berkeley
<br class="ltx_break"/>{<span class="ltx_text ltx_font_typewriter">dlwh,tberg,klein</span>}<span class="ltx_text ltx_font_typewriter">@cs.berkeley.edu</span> 
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Due to their origin in computer graphics, graphics processing
units (GPUs) are highly optimized for dense problems, where the exact same
operation is applied repeatedly to all data points. Natural language processing
algorithms, on the other hand, are traditionally constructed in ways that exploit
structural sparsity. Recently, <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> presented an approach to GPU parsing that sacrifices
traditional sparsity in exchange for raw computational power, obtaining a system
that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range
GPU.
In this work, we reintroduce sparsity to GPU parsing by adapting a
coarse-to-fine pruning approach to the constraints of a GPU. The resulting system is capable of computing
over 404 Viterbi parses per second—more than a 2x speedup—on
the same hardware.
Moreover, our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk
inference, improving throughput for this more accurate algorithm from only 32 sentences per
second unpruned to over 190 sentences per second using pruning—nearly a 6x
speedup.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Because NLP models typically treat sentences independently, NLP problems have long
been seen as “embarrassingly parallel” – large corpora can be
processed arbitrarily fast by simply sending different sentences to different machines. However, recent trends in computer architecture, particularly the
development of powerful “general purpose” GPUs, have changed the landscape even for problems that parallelize at the sentence level. First, classic single-core processors
and main memory architectures are no longer getting substantially faster over time, so speed gains must now come from parallelism within a single machine.
Second, compared to CPUs, GPUs devote a much larger fraction of their computational power to actual arithmetic. Since tasks like parsing boil down to
repeated read-multiply-write loops, GPUs should be many times more efficient in time, power, or cost. The challenge is that GPUs are not a good fit for
the kinds of sparse computations that most current CPU-based NLP algorithms rely on.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Recently, <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> proposed a GPU implementation of a
constituency parser that sacrifices all sparsity in exchange for
the sheer horsepower that GPUs can provide. Their system uses a
grammar based on the Berkeley parser <cite class="ltx_cite">[<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">9</a>]</cite> (which is particularly amenable to GPU processing),
“compiling” the grammar into a sequence of GPU kernels that are applied
densely to every item in the parse chart. Together these kernels
implement the Viterbi inside algorithm. On a mid-range GPU, their
system can compute Viterbi derivations at 164 sentences per
second on sentences of length 40 or less (see timing details below).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper, we develop algorithms that can exploit sparsity on a GPU by adapting coarse-to-fine pruning to a GPU setting. On a CPU, pruning methods can give speedups of up to 100x.
Such extreme speedups over a dense GPU baseline currently seem
unlikely because fine-grained sparsity appears to be directly at
odds with dense parallelism. However, in this paper, we present a
system that finds a middle ground, where some level of sparsity can be maintained
without losing the parallelism of the GPU. We use a coarse-to-fine
approach as in <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite>, but with only one coarse
pass. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of the approach: we first parse densely with a coarse grammar and then parse
sparsely with the fine grammar, skipping symbols that the coarse
pass deemed sufficiently unlikely. Using this approach, we see gains
of nearly 2.5x over the dense GPU implementation, resulting in overall
speeds of up to 404 sentences per second. For comparison, the
publicly available CPU implementation of <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite> parses
approximately 7 sentences per second per core on a modern CPU.</p>
</div>
<div id="S1.F1" class="ltx_figure"><img src="P14-1020/image001.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="353" height="225" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Overview of the architecture of our system, which is an extension of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s system. The GPU and CPU communicate via a work queue, which
ferries parse items from the CPU to the GPU. Our system uses a coarse-to-fine approach, where the coarse pass computes a pruning mask that is used by the CPU when deciding which items to queue during the
fine pass. The original system of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>
only used the fine pass, with no pruning.</div>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">A further drawback of the dense approach in <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> is that it only computes
Viterbi parses. As with other grammars with a parse/derivation distinction, the grammars of <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite>
only achieve their full accuracy using minimum-Bayes-risk parsing, with improvements of over 1.5 F1 over best-derivation Viterbi parsing on the Penn
Treebank <cite class="ltx_cite">[<a href="#bib.bib26" title="Building a large annotated corpus of English: the Penn Treebank" class="ltx_ref">6</a>]</cite>. To that end, we extend our coarse-to-fine GPU approach
to computing marginals, along the way proposing
a new way to exploit the coarse pass to avoid expensive log-domain
computations in the fine pass. We then implement minimum-Bayes-risk parsing via the max recall algorithm of
<cite class="ltx_cite">Goodman (<a href="#bib.bib182" title="Parsing algorithms and metrics" class="ltx_ref">1996</a>)</cite>. Without the coarse pass,
the dense marginal computation is not efficient on a GPU, processing only 32 sentences per
second.
However, our approach allows us to process over 190 sentences per second, almost a
6x speedup.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>A Note on Experiments</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">We build up our approach incrementally, with experiments interspersed throughout
the paper, and summarized in Tables <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S9.T2" title="Table 2 ‣ 9.1 Computing marginal probabilities ‣ 9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In
this paper, we focus our attention on current-generation NVIDIA GPUs. Many of
the ideas described here apply to other GPUs (such as those from AMD),
but some specifics will differ. All experiments are run with an NVIDIA GeForce GTX
680, a mid-range GPU that costs around $500 at time of writing. Unless
otherwise noted, all experiments are conducted on sentences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m1" class="ltx_Math" alttext="\leq 40" display="inline"><mrow><mi/><mo>≤</mo><mn>40</mn></mrow></math>
words, and we estimate times based on batches of 20K sentences. We should note
that our experimental condition differs from
that of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>:
they evaluate on sentences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p1.m2" class="ltx_Math" alttext="\leq 30" display="inline"><mrow><mi/><mo>≤</mo><mn>30</mn></mrow></math>. Furthermore, they use two NVIDIA GeForce GTX
<span class="ltx_text ltx_font_italic">690</span>s—each of which is essentially a repackaging of two 680s—meaning that
our system and experiments would run approximately four times faster on their hardware (this expected 4x factor is empirically consistent with the result of running their system on our hardware).</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Sparsity and CPUs</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">One successful approach for speeding up constituency parsers has been to use
coarse-to-fine inference <cite class="ltx_cite">[<a href="#bib.bib3" title="Multilevel coarse-to-fine pcfg parsing" class="ltx_ref">2</a>]</cite>. In coarse-to-fine
inference, we have a sequence of increasingly complex grammars <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m1" class="ltx_Math" alttext="G_{\ell}" display="inline"><msub><mi>G</mi><mi mathvariant="normal">ℓ</mi></msub></math>. Typically, each successive grammar <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m2" class="ltx_Math" alttext="G_{\ell}" display="inline"><msub><mi>G</mi><mi mathvariant="normal">ℓ</mi></msub></math> is a <span class="ltx_text ltx_font_italic">refinement</span> of the preceding grammar <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m3" class="ltx_Math" alttext="G_{\ell-1}" display="inline"><msub><mi>G</mi><mrow><mi mathvariant="normal">ℓ</mi><mo>-</mo><mn>1</mn></mrow></msub></math>.
That is, for each symbol <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m4" class="ltx_Math" alttext="A_{x}" display="inline"><msub><mi>A</mi><mi>x</mi></msub></math> in the fine grammar, there is some symbol <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m5" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math> in the coarse grammar. For instance, in a latent variable parser, the coarse grammar would
have symbols like <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m6" class="ltx_Math" alttext="NP" display="inline"><mrow><mi>N</mi><mo>⁢</mo><mi>P</mi></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m7" class="ltx_Math" alttext="VP" display="inline"><mrow><mi>V</mi><mo>⁢</mo><mi>P</mi></mrow></math>, etc., and the fine pass would have refined symbols <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m8" class="ltx_Math" alttext="NP_{0}" display="inline"><mrow><mi>N</mi><mo>⁢</mo><msub><mi>P</mi><mn>0</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m9" class="ltx_Math" alttext="NP_{1}" display="inline"><mrow><mi>N</mi><mo>⁢</mo><msub><mi>P</mi><mn>1</mn></msub></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p1.m10" class="ltx_Math" alttext="VP_{4}" display="inline"><mrow><mi>V</mi><mo>⁢</mo><msub><mi>P</mi><mn>4</mn></msub></mrow></math>, and so on.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">In coarse-to-fine inference, one applies the grammars in sequence,
computing inside and outside scores. Next, one computes (max) marginals
for every labeled span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="(A,i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>A</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> in a sentence. These max marginals
are used to compute a <span class="ltx_text ltx_font_italic">pruning mask</span> for every span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>.
This mask is the set of symbols allowed for
that span. Then, in the next pass, one only processes rules
that are licensed by the
pruning mask computed at the previous level.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">This approach works because a low quality coarse grammar
can still reliably be used to prune many symbols from the fine chart
without loss of accuracy. <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite> found that
over 98% of symbols can be pruned from typical charts using
a simple X-bar grammar without any loss of accuracy. Thus, the vast
majority of rules can be skipped, and therefore most computation
can be avoided. It is worth pointing out that although 98% of labeled spans can be skipped due to X-bar pruning,
we found that only about 79% of binary rule applications can be skipped, because the unpruned symbols tend to be
the ones with a larger grammar footprint.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>GPU Architectures</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Unfortunately, the standard coarse-to-fine approach does not naïvely translate to GPU
architectures. GPUs work by executing thousands of threads at once,
but impose the constraint that large blocks of threads must be executing the same instructions in lockstep, differing only in their input data. Thus
sparsely skipping rules and symbols will not save any work. Indeed,
it may actually slow the system down. In this section, we provide an overview
of GPU architectures, focusing on the details that are relevant to
building an efficient parser.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">The large number of threads that a GPU executes are packaged into blocks of 32 threads called <span class="ltx_text ltx_font_italic">warps</span>.
All threads in a warp must execute the same instruction at every
clock cycle: if one thread takes a branch the others do not, then
<em class="ltx_emph">all</em> threads in the warp must follow both code paths. This situation is called
<span class="ltx_text ltx_font_italic">warp divergence</span>. Because all threads execute all code paths that any thread takes,
time can only be saved if an entire warp agrees to skip any particular branch.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">NVIDIA GPUs have 8-15 processors called <span class="ltx_text ltx_font_italic">streaming multi-processors</span>
or SMs.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>Older hardware (600 series or older) has 8 SMs.
Newer hardware has more.</span></span></span> Each SM can process up to 48 different
warps at a time: it interleaves the execution of each warp, so that
when one warp is stalled another warp can execute. Unlike threads within a single warp, the 48 warps do not have
to execute the same instructions. However, the memory architecture is such that
they will be faster if they access related memory locations.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">A further consideration is that the number of registers available to a thread in a warp is rather limited compared to a CPU. On the
600 series, maximum occupancy can only be achieved if each thread uses at most
63 registers <cite class="ltx_cite">[<a href="#bib.bib2" title="Programming guide" class="ltx_ref">8</a>]</cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>A thread can use more
registers than this, but the full complement of 48 warps cannot execute if too many are used.</span></span></span> Registers are many times faster than variables located in thread-local memory, which is actually the same speed as global memory.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Anatomy of a Dense GPU Parser</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">This architecture environment puts very different constraints on parsing algorithms
from a CPU environment. <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> proposed an implementation
of a PCFG parser that sacrifices standard sparse methods like coarse-to-fine pruning, focusing instead on maximizing
the instruction and memory throughput of the parser. They assume
that they are parsing batches of thousands of sentences at a time. In this section, we describe
their dense algorithm, which we take as the baseline for our work; we present it in a way that sets up the changes to follow.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">At the top level, the CPU and GPU communicate via a <em class="ltx_emph">work queue</em> of parse items of the
form <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m1" class="ltx_Math" alttext="(s,i,k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>s</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m2" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> is an identifier of a sentence,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m3" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> is the start of a span, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> is the split point, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m5" class="ltx_Math" alttext="j" display="inline"><mi>j</mi></math> is the end
point. The GPU takes large numbers of parse items and applies the <em class="ltx_emph">entire</em>
grammar to them in parallel. These parse items are enqueued
in order of increasing span size, blocking until all items of a
given length are complete. This approach is diagrammed in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">Because all rules are applied to all parse items, all threads are executing the same sequence of instructions. Thus, there is no concern of warp divergence.</p>
</div>
<div id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">5.1 </span>Grammar Compilation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">One important feature of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s system is <span class="ltx_text ltx_font_italic">grammar compilation</span>. Because registers are so much faster than thread-local
memory, it is critical to keep as many variables in registers as possible. One way to accomplish this is to unroll loops at compilation
time. Therefore, they inlined the iteration over the grammar directly into the GPU kernels (i.e. the code itself), which allows the compiler to more
effectively use all of its registers.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">However, register space is limited on GPUs. Because the Berkeley
grammar is so large, the compiler is not able to efficiently schedule
all of the operations in the grammar, resulting in register spills.
<cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> found they had to partition the grammar into multiple
different kernels. We discuss this partitioning in more detail in
Section <a href="#S7" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. However, in short, the entire grammar <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m1" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> is broken into multiple clusters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.SS1.p2.m2" class="ltx_Math" alttext="G_{i}" display="inline"><msub><mi>G</mi><mi>i</mi></msub></math> where each rule belongs to exactly one cluster.</p>
</div>
<div id="S5.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Clustering</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Pruning</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Sent/Sec</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Speedup</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Canny et al.</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">164.0</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Reimpl</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">192.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">1.2x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Reimpl</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Empty, Coarse</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">185.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.1x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Reimpl</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Labeled, Coarse</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">187.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.1x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Parent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">158.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.0x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Parent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Labeled, Coarse</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">278.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.7x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Parent</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Labeled, 1-split</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">404.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">2.5x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Parent</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">Labeled, 2-split</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">343.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">2.1x</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 1: </span> Performance numbers for computing Viterbi inside
charts on 20,000 sentences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.T1.m2" class="ltx_Math" alttext="\leq" display="inline"><mo mathsize="normal" stretchy="false">≤</mo></math>40 from the Penn Treebank.
All times are measured on an NVIDIA GeForce GTX 680. ‘Reimpl’ is our reimplementation of their approach. Speedups are measured in reference to their
system. See Section <a href="#S7" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for discussion of the clustering algorithms and Section
<a href="#S6" title="6 Pruning on a GPU ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for a description of the pruning methods.</div>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">All in all, <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s system is able to compute Viterbi charts at 164
sentences per second, for sentences up to length 40. Our reimplementation of their
approach is able to achieve 193 sentences per second on the same hardware. (See
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.)</p>
</div>
<div id="S5.F2" class="ltx_figure"><img src="P14-1020/image002.png" id="S5.F2.g1" class="ltx_graphics ltx_centering" width="210" height="130" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Schematic representation of the work queue used in <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>.
The Viterbi inside loop for the grammar is inlined into a kernel. The kernel is
applied to all items in the queue in a blockwise manner.</div>
</div>
<div id="S5.F3" class="ltx_figure"><img src="P14-1020/image003.png" id="S5.F3.g1" class="ltx_graphics ltx_centering" width="240" height="225" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Schematic representation of the work queue and grammar clusters used
in the fine pass of our work. Here, the
rules of the grammar are clustered by their coarse parent symbol. We then have multiple work
queues, with parse items only being enqueued if the span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.F3.m2" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> allows that symbol in its pruning
mask.</div>
</div>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Pruning on a GPU</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">Now we turn to the algorithmic and architectural changes in our approach.
First, consider trying to directly apply the coarse-to-fine method sketched in Section <a href="#S3" title="3 Sparsity and CPUs ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> to the dense baseline described above. The natural implementation would be for each
thread to check if each rule is licensed before applying it. However, we would only avoid the work of applying the rule if all threads in the warp agreed to skip it. Since each thread in the warp is processing a different span (perhaps even from a different sentence), consensus from all 32 threads on any skip would be unlikely.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">Another approach would be to skip enqueuing any parse item <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m1" class="ltx_Math" alttext="(s,i,k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>s</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> where the pruning mask for any of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m2" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m3" class="ltx_Math" alttext="(i,k)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mo>)</mo></mrow></math>, or <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m4" class="ltx_Math" alttext="(k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>
is entirely empty (i.e. all symbols are pruned in this cell by the coarse grammar). However, our
experiments showed that only 40% of parse items are pruned in this manner. Because of the overhead associated with creating pruning masks and the further overhead
of GPU communication, we found that this method did not actually produce any time savings at all. The result is a parsing speed of 185.5 sentences per second, as shown in
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> on the row labeled ‘Reimpl’ with ‘Empty, Coarse’ pruning.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">Instead, we take advantage of the partitioned structure of the grammar and organize our computation around the
coarse symbol set. Recall that the baseline already partitions the grammar <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m1" class="ltx_Math" alttext="G" display="inline"><mi>G</mi></math> into rule clusters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m2" class="ltx_Math" alttext="G_{i}" display="inline"><msub><mi>G</mi><mi>i</mi></msub></math> to improve
register sharing. (See Section <a href="#S7" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for more on the baseline clustering.) We create a separate work queue for each partition.
We call each such queue a <span class="ltx_text ltx_font_italic">labeled work queue</span>, and each one only queues items to which some rule in the corresponding partition applies. We call the set of coarse symbols for a partition (and therefore the corresponding labeled work queue) a <span class="ltx_text ltx_font_italic">signature</span>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p">During parsing, we only enqueue items <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m1" class="ltx_Math" alttext="(s,i,k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>s</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> to a labeled
queue if two conditions are met. First, the span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m2" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>’s pruning
mask must have a non-empty intersection with the signature of the
queue. Second, the pruning mask for the children <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m3" class="ltx_Math" alttext="(i,k)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mo>)</mo></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m4" class="ltx_Math" alttext="(k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> must be non-empty.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p">Once on the GPU, parse items are processed using the same style of
compiled kernel as in <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>. Because the entire partition (though not necessarily the entire grammar)
is applied to each item in the queue, we still do not need to worry
about warp divergence.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p class="ltx_p">At the top level, our system first computes pruning masks with a coarse grammar.
Then it processes the same sentences with the fine grammar. However, to the
extent that the signatures are small, items can be selectively queued only to certain queues. This approach is diagrammed in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S6.p7" class="ltx_para">
<p class="ltx_p">We tested our new pruning approach using an X-bar grammar as the coarse pass.
The resulting speed is 187.5 sentences per second, labeled in
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as row labeled ‘Reimpl’ with ‘Labeled, Coarse’ pruning.
Unfortunately, this approach again does not produce a large speedup relative to
our
reimplemented baseline. To improve upon this result, we need to consider how
the grammar clustering interacts with the coarse pruning phase.</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Grammar Clustering</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">Recall that the rules in the grammar are partitioned into a set of clusters, and
that these clusters are further divided into subclusters. How can we best
cluster and subcluster the grammar so as to maximize performance? A good
clustering will group rules together that use the same symbols, since this means
fewer memory accesses to read and write scores for symbols.
Moreover, we would like the time spent processing each of the subclusters within
a cluster to be about the same. We cannot move on to the next cluster until all
threads from a cluster are finished, which means that the time a cluster takes
is the amount of time taken by the longest-running subcluster.
Finally, when pruning, it is best if symbols that have the same coarse
projection are clustered together.
That way, we are more likely to be able to skip a subcluster, since fewer
distinct symbols need to be “off” for a parse item to be skipped in a given
subcluster.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite> clustered <span class="ltx_text ltx_font_italic">symbols</span> of the grammar using a sophisticated spectral clustering algorithm to obtain a permutation of the symbols. Then the rules of the grammar
were laid out in a (sparse) three-dimensional tensor, with one dimension representing the parent of the rule, one representing the left child, and one
representing the right child. They then split the cube into 6x2x2 contiguous “major cubes,” giving a partition of the rules into 24 clusters. They then
further subdivided these cubes into 2x2x2 minor cubes, giving 8 subclusters that executed in parallel. Note that the
clusters induced by these major and minor cubes need not be
of similar sizes; indeed, they often are not. Clustering using this method is
labeled ‘Reimplementation’ in Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">The addition of pruning introduces further considerations. First, we have a
coarse grammar, with many fewer rules and symbols. Second, we are able to skip a
parse item for an entire cluster if that item’s pruning mask does not
intersect the cluster’s signature. Spreading symbols across clusters may be
inefficient: if a parse item licenses a given symbol, we will have to enqueue
that item to any queue that has the symbol in its signature, no matter how many
other symbols are in that cluster.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">Thus, it makes sense to choose a clustering algorithm that exploits the
structure introduced by the pruning masks. We use a very simple method: we
cluster the rules in the grammar by coarse parent symbol. When coarse symbols
are extremely unlikely (and therefore have few corresponding rules), we merge
their clusters to avoid the overhead of beginning work on clusters where little
work has to be done.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>Specifically, after clustering based on the coarse
parent symbol, we merge all clusters with less than 300 rules in them into one
large cluster.</span></span></span> In order to subcluster, we divde up rules among subclusters
so that each subcluster has the same number of active parent symbols. We found this
approach to subclustering worked well in practice.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p">Clustering using this method is labeled ‘Parent’ in
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Now, when we use a coarse pruning pass, we
are able to parse nearly 280 sentences per second, a 70% increase
in parsing performance relative to <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s system, and nearly
50% over our reimplemented baseline.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p class="ltx_p">It turns out that this simple clustering algorithm produces
relatively efficient kernels even in the unpruned case. The unpruned Viterbi
computations in a fine grammar using the clustering method of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>
yields a speed of 193 sentences per second, whereas the same computation using
coarse parent clustering has a speed of 159 sentences per second (see
Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). This is not as efficient as <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s highly tuned method, but it is
still fairly fast, and much simpler to implement.</p>
</div>
</div>
<div id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">8 </span>Pruning with Finer Grammars</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">The coarse to fine pruning approach of <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite>
employs an X-bar grammar as its first pruning phase, but there is
no reason why we cannot begin with a more complex grammar for our
initial pass. As <cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite> have shown, intermediate-sized
Berkeley grammars prune many more symbols than the X-bar system.
However, they are slower to parse with in a CPU context, and so
they begin with an X-bar grammar.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p class="ltx_p">Because of the overhead associated with transferring work
items to GPU, using a very small grammar may not be an efficient
use of the GPU’s computational resources. To that end, we tried
computing pruning masks with one-split and two-split Berkeley grammars.
The X-bar grammar can compute pruning masks at just over 1000 sentences per second,
the 1-split grammar parses 858 sentences per second, and the 2-split grammar parses
526 sentences per second.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p class="ltx_p">Because parsing with these grammars is still quite fast, we tried
using them as the coarse pass instead. As shown in Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
using a 1-split grammar as a coarse pass allows us to produce over
400 sentences per second, a full 2x improvement over our original
system, and 2.5x over <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>’s system. Conducting a coarse
pass with a 2-split grammar is somewhat slower, at a “mere” 343
sentences per second.</p>
</div>
</div>
<div id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">9 </span>Minimum Bayes risk parsing</h2>

<div id="S9.p1" class="ltx_para">
<p class="ltx_p">The Viterbi algorithm is a reasonably effective method for parsing.
However, many authors have noted that parsers benefit substantially
from minimum Bayes risk decoding <cite class="ltx_cite">[<a href="#bib.bib182" title="Parsing algorithms and metrics" class="ltx_ref">3</a>, <a href="#bib.bib200" title="On maximizing metrics for syntactic disambiguation" class="ltx_ref">10</a>, <a href="#bib.bib69" title="Probabilistic CFG with latent annotations" class="ltx_ref">7</a>, <a href="#bib.bib199" title="Loss minimization in parse reranking" class="ltx_ref">11</a>, <a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">9</a>]</cite>.
MBR algorithms for parsing do not compute the best derivation, as
in Viterbi parsing, but instead the parse tree that maximizes the
expected count of some figure of merit. For instance, one might
want to maximize the expected number of correct
constituents <cite class="ltx_cite">[<a href="#bib.bib182" title="Parsing algorithms and metrics" class="ltx_ref">3</a>]</cite>, or the expected
rule counts <cite class="ltx_cite">[<a href="#bib.bib200" title="On maximizing metrics for syntactic disambiguation" class="ltx_ref">10</a>, <a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">9</a>]</cite>. MBR
parsing has proven especially useful in latent variable grammars.
<cite class="ltx_cite">Petrov and Klein (<a href="#bib.bib71" title="Improved inference for unlexicalized parsing" class="ltx_ref">2007</a>)</cite> showed that MBR trees substantially
improved performance over Viterbi parses for latent variable grammars, earning up to 1.5F1.</p>
</div>
<div id="S9.p2" class="ltx_para">
<p class="ltx_p">Here, we implement the Max Recall algorithm of
<cite class="ltx_cite">Goodman (<a href="#bib.bib182" title="Parsing algorithms and metrics" class="ltx_ref">1996</a>)</cite>. This algorithm maximizes
the expected number of correct coarse symbols <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p2.m1" class="ltx_Math" alttext="(A,i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>A</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> with
respect to the posterior distribution over parses for a sentence.</p>
</div>
<div id="S9.p3" class="ltx_para">
<p class="ltx_p">This particular MBR algorithm has the advantage that it is relatively straightforward
to implement. In essence, we must compute the marginal probability of each fine-labeled span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p3.m1" class="ltx_Math" alttext="\mu(A_{x},i,j)" display="inline"><mrow><mi>μ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>A</mi><mi>x</mi></msub><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></math>, and
then marginalize to obtain <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p3.m2" class="ltx_Math" alttext="\mu(A,i,j)" display="inline"><mrow><mi>μ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>A</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></math>. Then, for each span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p3.m3" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>, we find the best possible split point <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p3.m4" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math>
that maximizes <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.p3.m5" class="ltx_Math" alttext="C(i,j)=\mu(A,i,j)+\max_{k}\left(C(i,k)+C(k,j)\right)" display="inline"><mrow><mrow><mi>C</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>μ</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>A</mi><mo>,</mo><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><msub><mo>max</mo><mi>k</mi></msub><mo>⁡</mo><mrow><mo>(</mo><mrow><mrow><mi>C</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mo>)</mo></mrow></mrow><mo>+</mo><mrow><mi>C</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math>. Parse extraction
is then just a matter of following back pointers from the root, as in the Viterbi algorithm.</p>
</div>
<div id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">9.1 </span>Computing marginal probabilities</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p class="ltx_p">The easiest way to compute marginal probabilities is to use the log
space semiring rather than the Viterbi semiring, and then to run
the inside and outside algorithms as before. We should expect this
algorithm to be at least a factor of two slower: the outside pass
performs at least as much work as the inside pass. Moreover, it
typically has worse memory access patterns, leading to slower
performance.</p>
</div>
<div id="S9.SS1.p2" class="ltx_para">
<p class="ltx_p">Without pruning, our approach does not handle these log domain
computations well at all: we are only able to compute marginals for 32.1 sentences/second, more than a factor
of 5 slower than our coarse pass.
To begin, log space addition requires significantly
more operations than max, which is a primitive operation on GPUs.
Beyond the obvious consequence that executing more operations means more time
taken, the sheer number of operations becomes too much for the
compiler to handle. Because the grammars are compiled into code,
the additional operations are all inlined into the kernels, producing
much larger kernels. Indeed, in practice the compiler will often
hang if we use the same size grammar clusters as we did for
Viterbi. In practice, we found there is an effective maximum of 2000
rules per kernel using log sums, while we can use more than 10,000 rules
rules in a single kernel with Viterbi.</p>
</div>
<div id="S9.SS1.p3" class="ltx_para">
<p class="ltx_p">With coarse pruning, however, we can avoid
much of the increased cost associated with log domain computations.
Because so many labeled spans are pruned, we are able to skip many
of the grammar clusters and thus avoid many of the expensive
operations. Using coarse pruning and log domain calculations, our system produces MBR trees at a rate
of 130.4 sentences per second, a four-fold increase.</p>
</div>
<div id="S9.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Sent/Sec</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Speedup</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Unpruned Log Sum MBR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">32.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Log Sum MBR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">130.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">4.1x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Scaling MBR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">190.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold ltx_font_small">5.9x</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Pruned Viterbi</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">404.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">12.6x</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 2: </span> Performance numbers for computing max
constituent <cite class="ltx_cite">[<a href="#bib.bib182" title="Parsing algorithms and metrics" class="ltx_ref">3</a>]</cite> trees on 20,000 sentences of length 40
or less from the Penn Treebank. For convenience, we have copied our pruned Viterbi
system’s result.</div>
</div>
</div>
<div id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">9.2 </span>Scaling with the Coarse Pass</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p class="ltx_p">One way to avoid the expense of log domain computations is to use scaled
probabilities rather than log probabilities.
Scaling is one of the folk techniques that are commonly used in the NLP community,
but not generally written about.
Recall that floating point numbers are composed of a mantissa <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m1" class="ltx_Math" alttext="m" display="inline"><mi>m</mi></math> and an
exponent <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m2" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math>, giving a number <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m3" class="ltx_Math" alttext="f=m\cdot 2^{e}" display="inline"><mrow><mi>f</mi><mo>=</mo><mrow><mi>m</mi><mo>⋅</mo><msup><mn>2</mn><mi>e</mi></msup></mrow></mrow></math>. When a float underflows, the
exponent becomes too low to represent the available number of bits. In scaling,
floating point numbers are paired with an additional number that extends the
exponent.
That is, the number is represented as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m4" class="ltx_Math" alttext="f^{\prime}=f\cdot\exp(s)" display="inline"><mrow><msup><mi>f</mi><mo>′</mo></msup><mo>=</mo><mrow><mi>f</mi><mo>⋅</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mi>s</mi><mo>)</mo></mrow></mrow></mrow></mrow></math>.
Whenever <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m5" class="ltx_Math" alttext="f" display="inline"><mi>f</mi></math> becomes either too big or too small, the number is rescaled back to
a less “dangerous” range by shifting mass from the exponent <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m6" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> to the scaling factor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p1.m7" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>.</p>
</div>
<div id="S9.SS2.p2" class="ltx_para">
<p class="ltx_p">In practice, one scale <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p2.m1" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> is used for an entire span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p2.m2" class="ltx_Math" alttext="(i,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math>, and
all scores for that span are rescaled in concert. In our GPU system,
multiple scores in any given span are being updated at the same
time, which makes this dynamic rescaling tricky and expensive,
especially since inter-warp communication is fairly limited.</p>
</div>
<div id="S9.SS2.p3" class="ltx_para">
<p class="ltx_p">We propose a much simpler static solution that exploits the coarse
pass. In the coarse pass, we compute Viterbi inside and outside
scores for every span. Because the grammar used in the coarse pass
is a projection of the grammar used in the fine pass, these coarse scores
correlate reasonably closely with the probabilities computed in the
fine pass: If a span has a very high or very low score in the coarse
pass, it typically has a similar score in the fine pass. Thus, we
can use the coarse pass’s inside and outside scores as the scaling
values for the fine pass’s scores. That is, in addition to computing
a pruning mask, in the coarse pass we store the maximum inside and
outside score in each span, giving two arrays of scores <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m1" class="ltx_Math" alttext="s^{I}_{i,j}" display="inline"><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m2" class="ltx_Math" alttext="s^{O}_{i,j}" display="inline"><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>O</mi></msubsup></math>. Then, when applying rules in the fine pass, each fine
inside score over a split span <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m3" class="ltx_Math" alttext="(i,k,j)" display="inline"><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></math> is scaled to the appropriate <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m4" class="ltx_Math" alttext="s^{I}_{i,j}" display="inline"><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup></math> by multiplying the score by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m5" class="ltx_Math" alttext="\exp\left(s^{I}_{i,k}+s^{I}_{k,j}-s^{I}_{i,j}\right)" display="inline"><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mi>I</mi></msubsup><mo>+</mo><msubsup><mi>s</mi><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup><mo>-</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup></mrow><mo>)</mo></mrow></mrow></math>, where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p3.m6" class="ltx_Math" alttext="s^{I}_{i,k},s^{I}_{k,j},s^{I}_{i,j}" display="inline"><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow><mi>I</mi></msubsup><mo>,</mo><msubsup><mi>s</mi><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup><mo>,</mo><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup></mrow></math> are the scaling factors for the left child, right child, and parent,
respectively. The outside scores are scaled analogously.</p>
</div>
<div id="S9.SS2.p4" class="ltx_para">
<p class="ltx_p">By itself, this approach works on nearly every sentence. However,
scores for approximately 0.5% of sentences overflow (<span class="ltx_text ltx_font_italic">sic</span>). Because
we are summing instead of maxing scores in the fine pass, the scaling
factors computed using max scores are not quite large enough, and
so the rescaled inside probabilities grow too large when multiplied
together. Most of this difference arises at the leaves, where the
lexicon typically has more uncertainty than higher up in the tree.
Therefore, in the fine pass, we normalize the inside scores at the
leaves to sum to 1.0.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>One can instead interpret this
approach as changing the scaling factors to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS2.p4.m1" class="ltx_Math" alttext="s^{I^{\prime}}_{i,j}=s^{I}_{i,j}\cdot\prod_{i\leq k&lt;j}\sum_{A}\textrm{inside}(%&#10;A,k,k+1)" display="inline"><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><msup><mi>I</mi><mo>′</mo></msup></msubsup><mo>=</mo><mrow><msubsup><mi>s</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mi>I</mi></msubsup><mo>⋅</mo><mrow><msub><mo largeop="true" symmetric="true">∏</mo><mrow><mi>i</mi><mo>≤</mo><mi>k</mi><mo>&lt;</mo><mi>j</mi></mrow></msub><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mi>A</mi></msub><mrow><mtext>inside</mtext><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>A</mi><mo>,</mo><mi>k</mi><mo>,</mo><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></mrow></math>, where inside is the array of scores for the fine pass.</span></span></span> Using
this slight modification, no sentences from the Treebank under- or
overflow.</p>
</div>
<div id="S9.SS2.p5" class="ltx_para">
<p class="ltx_p">We know of no reason why this same trick cannot be employed in more
traditional parsers, but it is especially useful here: with this
static scaling, we can avoid the costly log sums without introducing
any additional inter-thread communication, making the kernels much
smaller and much faster. Using scaling, we are able to push our
parser to 190.6 sentences/second for MBR extraction, just under
half the speed of the Viterbi system.</p>
</div>
</div>
<div id="S9.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">9.3 </span>Parsing Accuracies</h3>

<div id="S9.SS3.p1" class="ltx_para">
<p class="ltx_p">It is of course important verify the correctness of our system; one
easy way to do so is to examine parsing accuracy, as compared
to the original Berkeley parser. We measured parsing accuracy on
sentences of length <math xmlns="http://www.w3.org/1998/Math/MathML" id="S9.SS3.p1.m1" class="ltx_Math" alttext="\leq 40" display="inline"><mrow><mi/><mo>≤</mo><mn>40</mn></mrow></math> from section 22 of the Penn Treebank.
Our Viterbi parser achieves 89.7 F1, while our MBR parser scores 91.0.
These results are nearly identical to the Berkeley parserâs most
comparable numbers: 89.8 for Viterbi, and 90.9 for their “Max-Rule-Sum”
MBR algorithm. These slight differences arise from the usual minor
variation in implementation details. In particular, we use one
coarse pass instead of several, and a different MBR algorithm. In addition,
there are some differences in unary processing.</p>
</div>
</div>
</div>
<div id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">10 </span>Analyzing System Performance</h2>

<div id="S10.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Coarse Pass</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Fine Pass</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Unpruned Viterbi</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">6.4</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Viterbi</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">1.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Unpruned Logsum MBR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">—</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">28.6</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Scaling MBR</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">4.3</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 3: </span> Time spent in the passes of our different systems, in seconds
per 1000 sentences. Pruning refers to using a 1-split grammar for the coarse pass.</div>
</div>
<div id="S10.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">System</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span class="ltx_text ltx_font_small">Coarse Pass</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4"><span class="ltx_text ltx_font_small">Fine Pass</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_l ltx_border_r"/>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Binary</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Unary</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Queueing</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Masks</span></th>
<th class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Overhead</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Binary</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Unary</span></th>
<th class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">Queueing</span></th>
<th class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">Overhead</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Unpruned Viterbi</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">5.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">0.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">0.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">0.40</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Viterbi</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.59</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.19</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">0.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">0.22</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_small">Pruned Scaling</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">0.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">1.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_small">0.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">0.84</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering ltx_font_small"><span class="ltx_tag ltx_tag_table">Table 4: </span> Breakdown of time spent in our
different systems, in seconds per 1000 sentences. Binary and Unary refer to spent processing binary rules. Queueing refers to the amount of time used
to move memory around within the GPU for processing. Overhead includes all other time, which includes communication between the GPU and the CPU.</div>
</div>
<div id="S10.p1" class="ltx_para">
<p class="ltx_p">In this section we attempt to break down how exactly our system is
spending its time. We do this in an effort to give a sense of how
time is spent during computation on GPUs. These timing numbers are computed
using the built-in profiling capabilities of the programming
environment. As usual, profiles exhibit an observer effect, where
the act of measuring the system changes the execution. Nevertheless,
the general trends should more or less be preserved as compared to
the unprofiled code.</p>
</div>
<div id="S10.p2" class="ltx_para">
<p class="ltx_p">To begin, we can compute the number of seconds needed to parse 1000
sentences. (We use seconds per sentence rather than sentences per
second because the former measure is additive.) The results are in
Table <a href="#S10.T3" title="Table 3 ‣ 10 Analyzing System Performance ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In the case of pruned Viterbi, pruning
reduces the amount of time spent in the fine pass by more than 4x,
though half of those gains are lost to computing the pruning masks.</p>
</div>
<div id="S10.p3" class="ltx_para">
<p class="ltx_p">In Table <a href="#S10.T4" title="Table 4 ‣ 10 Analyzing System Performance ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we break down the time taken by our
system into individual components. As expected, binary rules account
for the vast majority of the time in the unpruned Viterbi case, but
much less time in the pruned case, with the total time taken for
binary rules in the coarse and fine passes taking about 1/5 of the
time taken by binaries in the unpruned version. Queueing, which
involves copying memory around within the GPU to process the
individual parse items, takes a fairly consistent amount of time
in all systems. Overhead, which includes transport time between the
CPU and GPU and other processing on the CPU, is relatively small
for most system configurations. There is greater overhead in the
scaling system, because scaling factors are copied to the CPU between
the coarse and fine passes.</p>
</div>
<div id="S10.F4" class="ltx_figure"><img src="P14-1020/image004.png" id="S10.F4.g1" class="ltx_graphics ltx_centering" width="252" height="163" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Plot of speeds (sentences / second) for various sizes of input corpora.
The full power of the GPU parser is only reached when run on large numbers of sentences.</div>
</div>
<div id="S10.p4" class="ltx_para">
<p class="ltx_p">A final question is: how many sentences per second do we need to
process to saturate the GPU’s processing power? We computed Viterbi
parses of successive powers of 10, from 1 to 100,000 sentences.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>We
replicated the Treebank for the 100,000 sentences pass.</span></span></span> In Figure <a href="#S10.F4" title="Figure 4 ‣ 10 Analyzing System Performance ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we then
plotted the throughput, in terms of number of sentences per second. Throughput
increases through parsing 10,000 sentences, and then levels off by the time
it reaches 100,000 sentences.</p>
</div>
</div>
<div id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">11 </span>Related Work</h2>

<div id="S11.p1" class="ltx_para">
<p class="ltx_p">Apart from the model of <cite class="ltx_cite">Canny<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib198" title="A multi-teraflop constituency parser using GPUs" class="ltx_ref">2013</a>)</cite>, there have been a few
attempts at using GPUs in NLP contexts before. <cite class="ltx_cite">Johnson (<a href="#bib.bib202" title="Parsing in parallel on multiple cores and gpus" class="ltx_ref">2011</a>)</cite>
and <cite class="ltx_cite">Yi<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib201" title="Efficient parallel cky parsing on gpus" class="ltx_ref">2011</a>)</cite> both had early attempts at porting parsing
algorithms to the GPU. However, they did not demonstrate significantly
increased speed over a CPU implementation. In machine translation,
<cite class="ltx_cite">He<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib203" title="Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus" class="ltx_ref">2013</a>)</cite> adapted algorithms designed for GPUs in
the computational biology literature to speed up on-demand phrase
table extraction.</p>
</div>
</div>
<div id="S12" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">12 </span>Conclusion</h2>

<div id="S12.p1" class="ltx_para">
<p class="ltx_p">GPUs represent a challenging opportunity for natural language
processing. By carefully designing within the constraints imposed
by the architecture, we have created a parser that can exploit the
same kinds of sparsity that have been developed for more traditional
architectures.</p>
</div>
<div id="S12.p2" class="ltx_para">
<p class="ltx_p">One of the key remaining challenges going forward is confronting
the kind of lexicalized sparsity common in other NLP models. The
Berkeley parser’s grammars—by virtue of being unlexicalized—can
be applied uniformly to all parse items. The bilexical features
needed by dependency models and lexicalized constituency models are
not directly amenable to acceleration using the techniques we
described here. Determining how to efficiently implement these
kinds of models is a promising area for new research.</p>
</div>
<div id="S12.p3" class="ltx_para">
<p class="ltx_p">Our system is available as open-source at <a href="https://www.github.com/dlwh/puck" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://www.github.com/dlwh/puck</span></a>.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This work was partially supported by BBN under DARPA contract
HR0011-12-C-0014, by a Google PhD fellowship to the first author,
and an NSF fellowship to the second. We further gratefully
acknowledge a hardware donation by NVIDIA Corporation.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib198" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Canny, D. Hall and D. Klein</span><span class="ltx_text ltx_bib_year">(2013-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A multi-teraflop constituency parser using GPUs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1898–1907</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D13-1195" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Sparser, Better, Faster GPU Parsing</span></span>,
<a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p2" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S11.p1" title="11 Related Work ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>,
<a href="#S2.p1" title="2 A Note on Experiments ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S5.F2" title="Figure 2 ‣ 5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S5.SS1.p1" title="5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS1.p2" title="5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.SS1.p3" title="5.1 Grammar Compilation ‣ 5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>,
<a href="#S5.p1" title="5 Anatomy of a Dense GPU Parser ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S6.p5" title="6 Pruning on a GPU ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>,
<a href="#S7.p2" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S7.p5" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S7.p6" title="7 Grammar Clustering ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S8.p3" title="8 Pruning with Finer Grammars ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
</span></li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore and M. Pozar</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multilevel coarse-to-fine pcfg parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 168–175</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Sparsity and CPUs ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span></li>
<li id="bib.bib182" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Goodman</span><span class="ltx_text ltx_bib_year">(1996)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Parsing algorithms and metrics</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 177–183</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S9.T2" title="Table 2 ‣ 9.1 Computing marginal probabilities ‣ 9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#S9.p1" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>,
<a href="#S9.p2" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></li>
<li id="bib.bib203" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. He, J. Lin and A. Lopez</span><span class="ltx_text ltx_bib_year">(2013-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Massively parallel suffix array queries and on-demand phrase extraction for statistical machine translation using gpus</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Atlanta, Georgia</span>, <span class="ltx_text ltx_bib_pages"> pp. 325–334</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N13-1033" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S11.p1" title="11 Related Work ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
</span></li>
<li id="bib.bib202" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Johnson</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Parsing in parallel on multiple cores and gpus</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S11.p1" title="11 Related Work ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. P. Marcus, B. Santorini and M. A. Marcinkiewicz</span><span class="ltx_text ltx_bib_year">(1993)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Building a large annotated corpus of English: the Penn Treebank</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">19</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 313–330</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Matsuzaki, Y. Miyao and J. Tsujii</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Probabilistic CFG with latent annotations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Morristown, NJ, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 75–82</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.3115/1219840.1219850" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/http://dx.doi.org/10.3115/1219840.1219850" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.p1" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_misc"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Nvidia</span><span class="ltx_text ltx_bib_year">(2008)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Programming guide</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.p4" title="4 GPU Architectures ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
</span></li>
<li id="bib.bib71" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Petrov and D. Klein</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved inference for unlexicalized parsing</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N/N07/N07-1051" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p4" title="1 Introduction ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.p3" title="3 Sparsity and CPUs ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S8.p1" title="8 Pruning with Finer Grammars ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>,
<a href="#S9.p1" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></li>
<li id="bib.bib200" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Simaâan</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">On maximizing metrics for syntactic disambiguation</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.p1" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></li>
<li id="bib.bib199" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Titov and J. Henderson</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Loss minimization in parse reranking</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 560–567</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.p1" title="9 Minimum Bayes risk parsing ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></li>
<li id="bib.bib201" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Yi, C. Lai, S. Petrov and K. Keutzer</span><span class="ltx_text ltx_bib_year">(2011-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient parallel cky parsing on gpus</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Dublin, Ireland</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S11.p1" title="11 Related Work ‣ Sparser, Better, Faster GPU Parsing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 17:20:22 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
