<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Cross-lingual Model Transfer Using Feature Representation Projection</title>
<!--Generated on Wed Jun 11 18:08:42 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cross-lingual Model Transfer Using Feature Representation Projection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mikhail Kozhevnikov
<br class="ltx_break"/>MMCI, University of Saarland
<br class="ltx_break"/>Saarbrücken, Germany 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">mkozhevn@mmci.uni-saarland.de</span> 
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivan Titov
<br class="ltx_break"/>ILLC, University of Amsterdam
<br class="ltx_break"/>Amsterdam, Netherlands 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">titov@uva.nl</span> 
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We propose a novel approach to cross-lingual model transfer based on <span class="ltx_text ltx_font_italic">feature representation projection</span>.
First, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data.
The target instance can then be mapped into the source-side feature representation using the derived mapping and handled directly by the source-side model.
This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research.</p>
</div><span class="ltx_ERROR undefined">\pgfplotsset</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">compat=1.8</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Cross-lingual model transfer approaches are concerned with creating statistical models for various tasks for languages poor in annotated resources, utilising resources or models available for these tasks in other languages.
That includes approaches such as <span class="ltx_text ltx_font_italic">direct model transfer</span> <cite class="ltx_cite">[]</cite> and <span class="ltx_text ltx_font_italic">annotation projection</span> <cite class="ltx_cite">[]</cite>.
Such methods have been successfully applied to a variety of tasks, including POS tagging <cite class="ltx_cite">[]</cite>, syntactic parsing <cite class="ltx_cite">[]</cite>, semantic role labeling <cite class="ltx_cite">[]</cite> and others.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Direct model transfer attempts to find a shared feature representation for samples from the two languages, usually generalizing and abstracting away from language-specific representations.
Once this is achieved, instances from both languages can be mapped into this space and a model trained on the source-language data directly applied to the target language.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of <span class="ltx_text ltx_font_italic">projected transfer</span> <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">The shared feature representation depends on the task in question, but most often each aspect of the original feature representation is handled separately.
Word types, for example, may be replaced by cross-lingual word clusters <cite class="ltx_cite">[]</cite> or cross-lingual distributed word representations <cite class="ltx_cite">[]</cite>.
Part-of-speech tags, which are often language-specific, can be converted into universal part-of-speech tags <cite class="ltx_cite">[]</cite> and morpho-syntactic information can also be represented in a unified way <cite class="ltx_cite">[]</cite>.
Unfortunately, the design of such representations and corresponding conversion procedures is by no means trivial.</p>
</div>
<div id="S1.F1" class="ltx_figure"><img src="P14-2095/image001.png" id="S1.F1.g1" class="ltx_graphics ltx_centering" width="505" height="139" alt=""/>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Dependency-based semantic role labeling example. The top arcs depict dependency relations, the bottom ones – semantic role structure. Rendered with <span class="ltx_text ltx_font_small"> <a href="https://code.google.com/p/whatswrong/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://code.google.com/p/whatswrong/</span></a></span>.</div>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Annotation projection, on the other hand, does not require any changes to the feature representation.
Instead, it operates on translation pairs, usually on sentence level, applying the available source-side model to the source sentence and transferring the resulting annotations through the word alignment links to the target one.
The quality of predictions on source sentences depends heavily on the quality of parallel data and the domain it belongs to (or, rather, the similarity between this domain and that of the corpus the source-language model was trained on).
The transfer itself also introduces errors due to translation shifts <cite class="ltx_cite">[]</cite> and word alignment errors, which may lead to inaccurate predictions.
These issues are generally handled using heuristics <cite class="ltx_cite">[]</cite> and filtering, for example based on alignment coverage <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p class="ltx_p">The approach proposed here, which we will refer to as <span class="ltx_text ltx_font_italic">feature representation projection</span> (<span class="ltx_text ltx_font_smallcaps">FRP</span>), constitutes an alternative to direct model transfer and annotation projection and can be seen as a compromise between the two.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p class="ltx_p">It is similar to direct transfer in that we also use a shared feature representation.
Instead of designing this representation manually, however, we create compact monolingual feature representations for source and target languages separately and automatically estimate the mapping between the two from parallel data.
This allows us to make use of language-specific annotations and account for the interplay between different types of information.
For example, a certain preposition attached to a token in the source language might map into a morphological tag in the target language, which would be hard to handle for traditional direct model transfer other than using some kind of refinement procedure involving parallel data.
Note also that any such refinement procedure applicable to direct transfer would likely work for <span class="ltx_text ltx_font_smallcaps">FRP</span> as well.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p class="ltx_p">Compared to annotation projection, our approach may be expected to be less sensitive to parallel data quality, since we do not have to commit to a particular prediction on a given instance from parallel data.
We also believe that <span class="ltx_text ltx_font_smallcaps">FRP</span> may profit from using other sources of information about the correspondence between source and target feature representations, such as dictionary entries, and thus have an edge over annotation projection in those cases where the amount of parallel data available is limited.</p>
</div>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Evaluation</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">We evaluate feature representation projection on the task of dependency-based semantic role labeling (SRL) <cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">This task consists in identifying predicates and their arguments in sentences and assigning each argument a semantic role with respect to its predicate (see figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Cross-lingual Model Transfer Using Feature Representation Projection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Note that only a single word – the syntactic head of the argument phrase – is marked as an argument in this case, as opposed to constituent- or span-based SRL <cite class="ltx_cite">[]</cite>.
We focus on the assignment of semantic roles to identified arguments.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">For the sake of simplicity we cast it as a multiclass classification problem, ignoring the interaction between different arguments in a predicate.
It is well known that such interaction plays an important part in SRL <cite class="ltx_cite">[]</cite>, but it is not well understood which kinds of interactions are preserved across languages and which are not.
Also, should one like to apply constraints on the set of semantic roles in a given predicate, or, for example, use a reranker <cite class="ltx_cite">[]</cite>, this can be done using a factorized model obtained by cross-lingual transfer.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">In our setting, each <span class="ltx_text ltx_font_italic">instance</span> includes the word type and part-of-speech and morphological tags (if any) of argument token, its parent and corresponding predicate token, as well as their dependency relations to their respective parents.
This representation is further denoted <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m1" class="ltx_Math" alttext="\omega_{0}" display="inline"><msub><mi>ω</mi><mn>0</mn></msub></math>.</p>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Approach</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">We consider a pair of languages <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="(L^{s},L^{t})" display="inline"><mrow><mo>(</mo><mrow><msup><mi>L</mi><mi>s</mi></msup><mo>,</mo><msup><mi>L</mi><mi>t</mi></msup></mrow><mo>)</mo></mrow></math> and assume that an annotated training set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="D^{s}_{T}=\left\{\left(x^{s},y^{s}\right)\right\}" display="inline"><mrow><msubsup><mi>D</mi><mi>T</mi><mi>s</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mi>s</mi></msup><mo>,</mo><msup><mi>y</mi><mi>s</mi></msup></mrow><mo>)</mo></mrow><mo>}</mo></mrow></mrow></math> is available in the source language as well as a parallel corpus of instance pairs <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="D^{st}=\left\{\left(x^{s},x^{t}\right)\right\}" display="inline"><mrow><msup><mi>D</mi><mrow><mi>s</mi><mo>⁢</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><msup><mi>x</mi><mi>s</mi></msup><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup></mrow><mo>)</mo></mrow><mo>}</mo></mrow></mrow></math> and a target dataset <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="D^{t}_{E}=\left\{x^{t}\right\}" display="inline"><mrow><msubsup><mi>D</mi><mi>E</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mo>{</mo><msup><mi>x</mi><mi>t</mi></msup><mo>}</mo></mrow></mrow></math> that needs to be labeled.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">We design a pair of intermediate compact monolingual feature representations <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="\omega^{s}_{1}" display="inline"><msubsup><mi>ω</mi><mn>1</mn><mi>s</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="\omega^{t}_{1}" display="inline"><msubsup><mi>ω</mi><mn>1</mn><mi>t</mi></msubsup></math> and models <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m3" class="ltx_Math" alttext="M_{s}" display="inline"><msub><mi>M</mi><mi>s</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m4" class="ltx_Math" alttext="M_{t}" display="inline"><msub><mi>M</mi><mi>t</mi></msub></math> to map source and target samples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m5" class="ltx_Math" alttext="x^{s}" display="inline"><msup><mi>x</mi><mi>s</mi></msup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m6" class="ltx_Math" alttext="x^{t}" display="inline"><msup><mi>x</mi><mi>t</mi></msup></math> from their original representations, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m7" class="ltx_Math" alttext="\omega^{s}_{0}" display="inline"><msubsup><mi>ω</mi><mn>0</mn><mi>s</mi></msubsup></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m8" class="ltx_Math" alttext="\omega^{t}_{0}" display="inline"><msubsup><mi>ω</mi><mn>0</mn><mi>t</mi></msubsup></math>, to the new ones.
We use the parallel instances in the new feature representation</p>
<table id="S2.Ex1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m1" class="ltx_Math" alttext="\bar{D}^{st}=\left\{\left(x^{s}_{1},x^{t}_{1}\right)\right\}=\left\{\left(M_{s%&#10;}(x^{s}),M_{t}(x^{t})\right)\right\}" display="block"><mrow><msup><mover accent="true"><mi>D</mi><mo stretchy="false">¯</mo></mover><mrow><mi>s</mi><mo>⁢</mo><mi>t</mi></mrow></msup><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mn>1</mn><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>1</mn><mi>t</mi></msubsup></mrow><mo>)</mo></mrow><mo>}</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>M</mi><mi>s</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mi>s</mi></msup><mo>)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>M</mi><mi>t</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow><mo>)</mo></mrow><mo>}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">to determine the mapping <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m9" class="ltx_Math" alttext="M_{ts}" display="inline"><msub><mi>M</mi><mrow><mi>t</mi><mo>⁢</mo><mi>s</mi></mrow></msub></math> (usually, linear) between the two spaces:</p>
<table id="S2.Ex2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m1" class="ltx_Math" alttext="M_{ts}=argmax_{M}{\sum_{(x^{s}_{1},x^{t}_{1}\in{}\bar{D}^{st})}{\left\|x^{s}_{%&#10;1}-M(x^{t}_{1})\right\|_{2}}}" display="block"><mrow><msub><mi>M</mi><mrow><mi>t</mi><mo>⁢</mo><mi>s</mi></mrow></msub><mo>=</mo><mrow><mi>a</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><msub><mi>x</mi><mi>M</mi></msub><mo>⁢</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mo>(</mo><msubsup><mi>x</mi><mn>1</mn><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>1</mn><mi>t</mi></msubsup><mo>∈</mo><msup><mover accent="true"><mi>D</mi><mo stretchy="false">¯</mo></mover><mrow><mi>s</mi><mo>⁢</mo><mi>t</mi></mrow></msup><mo>)</mo></mrow></munder><msub><mrow><mo fence="true">∥</mo><mrow><msubsup><mi>x</mi><mn>1</mn><mi>s</mi></msubsup><mo>-</mo><mrow><mi>M</mi><mo>⁢</mo><mrow><mo>(</mo><msubsup><mi>x</mi><mn>1</mn><mi>t</mi></msubsup><mo>)</mo></mrow></mrow></mrow><mo fence="true">∥</mo></mrow><mn>2</mn></msub></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Then a classification model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m10" class="ltx_Math" alttext="M_{y}" display="inline"><msub><mi>M</mi><mi>y</mi></msub></math> is trained on the source training data</p>
<table id="S2.Ex3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m1" class="ltx_Math" alttext="\bar{D}^{s}_{T}=\left\{\left(x^{s}_{1},y^{s}\right)\right\}=\left\{\left(M_{s}%&#10;(x^{s}),y^{s}\right)\right\}" display="block"><mrow><msubsup><mover accent="true"><mi>D</mi><mo stretchy="false">¯</mo></mover><mi>T</mi><mi>s</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><msubsup><mi>x</mi><mn>1</mn><mi>s</mi></msubsup><mo>,</mo><msup><mi>y</mi><mi>s</mi></msup></mrow><mo>)</mo></mrow><mo>}</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mo>(</mo><mrow><mrow><msub><mi>M</mi><mi>s</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mi>s</mi></msup><mo>)</mo></mrow></mrow><mo>,</mo><msup><mi>y</mi><mi>s</mi></msup></mrow><mo>)</mo></mrow><mo>}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">and the labels are assigned to the target samples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m11" class="ltx_Math" alttext="x^{t}\in{}D^{t}_{E}" display="inline"><mrow><msup><mi>x</mi><mi>t</mi></msup><mo>∈</mo><msubsup><mi>D</mi><mi>E</mi><mi>t</mi></msubsup></mrow></math> using a composition of the models:</p>
<table id="S2.Ex4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m1" class="ltx_Math" alttext="y^{t}=M_{y}(M_{ts}(M_{t}(x^{t})))" display="block"><mrow><msup><mi>y</mi><mi>t</mi></msup><mo>=</mo><mrow><msub><mi>M</mi><mi>y</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>M</mi><mrow><mi>t</mi><mo>⁢</mo><mi>s</mi></mrow></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msub><mi>M</mi><mi>t</mi></msub><mo>⁢</mo><mrow><mo>(</mo><msup><mi>x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Feature Representation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">Our objective is to make the feature representation sufficiently compact that the mapping between source and target feature spaces could be reliably estimated from a limited amount of parallel data, while preserving, insofar as possible, the information relevant for classification.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">Estimating the mapping directly from raw categorical features (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m1" class="ltx_Math" alttext="\omega_{0}" display="inline"><msub><mi>ω</mi><mn>0</mn></msub></math>) is both computationally expensive and likely inaccurate – using one-hot encoding the feature vectors in our experiments would have tens of thousands of components.
There is a number of ways to make this representation more compact.
To start with, we replace word types with corresponding neural language model representations estimated using the skip-gram model <cite class="ltx_cite">[]</cite>.
This corresponds to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m2" class="ltx_Math" alttext="M_{s}" display="inline"><msub><mi>M</mi><mi>s</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m3" class="ltx_Math" alttext="M_{t}" display="inline"><msub><mi>M</mi><mi>t</mi></msub></math> above and reduces the dimension of the feature space, making direct estimation of the mapping practical.
We will refer to this representation as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m4" class="ltx_Math" alttext="\omega_{1}" display="inline"><msub><mi>ω</mi><mn>1</mn></msub></math>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">To go further, one can, for example, apply dimensionality reduction techniques to obtain a more compact representation of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="\omega_{1}" display="inline"><msub><mi>ω</mi><mn>1</mn></msub></math> by eliminating redundancy or define auxiliary tasks and produce a vector representation useful for those tasks.
In source language, one can even directly tune an intermediate representation for the target problem.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Baselines</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">As mentioned above we compare the performance of this approach to that of direct transfer and annotation projection.
Both baselines are using the same set of features as the proposed model, as described earlier.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">The shared feature representation for direct transfer is derived from <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m1" class="ltx_Math" alttext="\omega_{0}" display="inline"><msub><mi>ω</mi><mn>0</mn></msub></math> by replacing language-specific part-of-speech tags with universal ones <cite class="ltx_cite">[]</cite> and adding cross-lingual word clusters <cite class="ltx_cite">[]</cite> to word types.
The word types themselves are left as they are in the source language and replaced with their gloss translations in the target one <cite class="ltx_cite">[]</cite>.
In English-Czech and Czech-English we also use the dependency relation information, since the annotations are partly compatible.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p">The annotation projection baseline implementation is straightforward.
The source-side instances from a parallel corpus are labeled using a classifier trained on source-language training data and transferred to the target side. The resulting annotations are then used to train a target-side classifier for evaluation.
Note that predicate and argument identification in both languages is performed using monolingual classifiers and only aligned pairs are used in projection.
A more common approach would be to project the whole structure from the source language, but in our case this may give unfair advantage to feature representation projection, which relies on target-side argument identification.</p>
</div>
</div>
<div id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.4 </span>Tools</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">We use the same type of log-linear classifiers in the model itself and the two baselines to avoid any discrepancy due to learning procedure.
These classifiers are implemented using <span class="ltx_text ltx_font_smallcaps">Pylearn2</span> <cite class="ltx_cite">[]</cite>, based on <span class="ltx_text ltx_font_smallcaps">Theano</span> <cite class="ltx_cite">[]</cite>.
We also use this framework to estimate the linear mapping <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p1.m1" class="ltx_Math" alttext="M_{ts}" display="inline"><msub><mi>M</mi><mrow><mi>t</mi><mo>⁢</mo><mi>s</mi></mrow></msub></math> between source and target feature spaces in <span class="ltx_text ltx_font_smallcaps">FRP</span>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p">The 250-dimensional word representations for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS4.p2.m1" class="ltx_Math" alttext="\omega_{1}" display="inline"><msub><mi>ω</mi><mn>1</mn></msub></math> are obtained using <span class="ltx_text ltx_font_smallcaps">Word2vec</span> tool.
Both monolingual data and that from the parallel corpus are included in the training.
In <cite class="ltx_cite"/> the authors consider embeddings of up to 800 dimensions, but we would not expect to benefit as much from larger vectors since we are using a much smaller corpus to train them.
We did not tune the size of the word representation to our task, as this would not be appropriate in a cross-lingual transfer setup, but we observe that the classifier is relatively robust to their dimension when evaluated on source language – in our experiments the performance of the monolingual classifier does not improve significantly if the dimension is increased past 300 and decreases only by a small margin (less than one absolute point) if it is reduced to 100.
It should be noted, however, that the dimension that is optimal in this sense is not necessarily the best choice for <span class="ltx_text ltx_font_smallcaps">FRP</span>, especially if the amount of available parallel data is limited.</p>
</div>
</div>
<div id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.5 </span>Data</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p class="ltx_p">We use two language pairs for evaluation: English-Czech and English-French.
In the first case, the data is converted from Prague Czech-English Dependency Treebank 2.0 <cite class="ltx_cite">[]</cite> using the script from <cite class="ltx_cite"/>.
In the second, we use CoNLL 2009 shared task <cite class="ltx_cite">[]</cite> corpus for English and the manually corrected dataset from <cite class="ltx_cite"/> for French.
Since the size of the latter dataset is relatively small – one thousand sentences – we reserve the whole dataset for testing and only evaluate transfer from English to French, but not the other way around.
Datasets for other languages are sufficiently large, so we take 30 thousand samples for testing and use the rest as training data.
The validation set in each experiment is withheld from the corresponding training corpus and contains 10 thousand samples.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p class="ltx_p">Parallel data for both language pairs is derived from Europarl <cite class="ltx_cite">[]</cite>, which we pre-process using <span class="ltx_text ltx_font_smallcaps">mate-tools</span> <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">The classification error of <span class="ltx_text ltx_font_smallcaps">FRP</span> and the baselines given varying amount of parallel data is reported in figures <a href="#S3.F2" title="Figure 2 ‣ 3 Results ‣ Cross-lingual Model Transfer Using Feature Representation Projection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S3.F3" title="Figure 3 ‣ 3 Results ‣ Cross-lingual Model Transfer Using Feature Representation Projection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 ‣ 3 Results ‣ Cross-lingual Model Transfer Using Feature Representation Projection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The training set for each language is fixed.
We denote the two baselines AP (annotation projection) and DT (direct transfer).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The number of parallel instances in these experiments is shown on a logarithmic scale, the values considered are 2, 5, 10, 20 and 50 thousand pairs.</p>
</div>
<div id="S3.F2" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" xml:id="S3.F2.pic1" class="ltx_centering" fragid="S3.F2.pic1" imagesrc="P14-2095/image003.png" imagewidth="1771" imageheight="24" imagedepth="6" width="50" height="20" viewBox="-5 -10 45 10" overflow="visible"><g transform="translate(0,0)"><g transform="scale(1 -1)"><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><svg height="1" version="1.1" viewBox="0 0 1 1" width="1"><g transform="matrix(1 0 0 -1 0 1)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F2.pic1.m1" class="ltx_Math" alttext="\times{}10^{3}" display="inline"><mrow><mi/><mo>×</mo><msup><mn>10</mn><mn>3</mn></msup></mrow></math></g></g></g></g></g></g></svg></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">{semilogxaxis}</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\legend</span></foreignObject></g></g></g></svg>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>English-Czech transfer results</div>
</div>
<div id="S3.F3" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" xml:id="S3.F3.pic1" class="ltx_centering" fragid="S3.F3.pic1" imagesrc="P14-2095/image002.png" imagewidth="1771" imageheight="24" imagedepth="6" width="50" height="20" viewBox="-5 -10 45 10" overflow="visible"><g transform="translate(0,0)"><g transform="scale(1 -1)"><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><svg height="1" version="1.1" viewBox="0 0 1 1" width="1"><g transform="matrix(1 0 0 -1 0 1)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F3.pic1.m1" class="ltx_Math" alttext="\times{}10^{3}" display="inline"><mrow><mi/><mo>×</mo><msup><mn>10</mn><mn>3</mn></msup></mrow></math></g></g></g></g></g></g></svg></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">{semilogxaxis}</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\legend</span></foreignObject></g></g></g></svg>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Czech-English transfer results</div>
</div>
<div id="S3.F4" class="ltx_figure"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" xml:id="S3.F4.pic1" class="ltx_centering" fragid="S3.F4.pic1" imagesrc="P14-2095/image004.png" imagewidth="1771" imageheight="24" imagedepth="6" width="50" height="20" viewBox="-5 -10 45 10" overflow="visible"><g transform="translate(0,0)"><g transform="scale(1 -1)"><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><svg height="1" version="1.1" viewBox="0 0 1 1" width="1"><g transform="matrix(1 0 0 -1 0 1)"><g><g stroke="#000000"><g fill="#000000"><g stroke-width="0.4pt"><g><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.F4.pic1.m1" class="ltx_Math" alttext="\times{}10^{3}" display="inline"><mrow><mi/><mo>×</mo><msup><mn>10</mn><mn>3</mn></msup></mrow></math></g></g></g></g></g></g></svg></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">{semilogxaxis}</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\addplot</span></foreignObject></g><g transform="scale(1 -1) translate(-5,-10)"><foreignObject width="50" height="20"><span xmlns="http://www.w3.org/1999/xhtml" class="ltx_ERROR undefined">\legend</span></foreignObject></g></g></g></svg>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>English-French transfer results</div>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">Please note that we report only a single value for direct transfer, since this approach does not explicitly rely on parallel data.
Although some of the features – namely, gloss translations and cross-lingual clusters – used in direct transfer are, in fact, derived from parallel data, we consider the effect of this on the performance of direct transfer to be indirect and outside the scope of this work.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">The rather inferior performance of direct transfer baseline on English-French may be partially attributed to the fact that it cannot rely on dependency relation features, as the corpora we consider make use of different dependency relation inventories.
Replacing language-specific dependency annotations with the universal ones <cite class="ltx_cite">[]</cite> may help somewhat, but we would still expect the methods directly relying on parallel data to achieve better results given a sufficiently large parallel corpus.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">Overall, we observe that the proposed method with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p5.m1" class="ltx_Math" alttext="\omega_{1}" display="inline"><msub><mi>ω</mi><mn>1</mn></msub></math> representation demonstrates performance competitive to direct transfer and annotation projection baselines.</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Additional Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">Apart from the work on direct/projected transfer and annotation projection mentioned above, the proposed method can be seen as a more explicit kind of domain adaptation, similar to <cite class="ltx_cite"/> or <cite class="ltx_cite"/>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">It is also somewhat similar in spirit to <cite class="ltx_cite"/>, where a small number of word translation pairs are used to estimate a mapping between distributed representations of words in two different languages and build a word translation model.</p>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this paper we propose a new method of cross-lingual model transfer, report initial evaluation results and highlight directions for its further development.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">We observe that the performance of this method is competitive with that of established cross-lingual transfer approaches and its application requires very little manual adjustment – no heuristics or filtering and no explicit shared feature representation design.
It also retains compatibility with any refinement procedures similar to projected transfer <cite class="ltx_cite">[]</cite> that may have been designed to work in conjunction with direct model transfer.</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">This paper reports work in progress and there is a number of directions we would like to pursue further.</p>
</div>
<div id="S6.SS5.SSS0.P1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Better Monolingual Representations</h4>

<div id="S6.SS5.SSS0.P1.p1" class="ltx_para">
<p class="ltx_p">The representation we used in the initial evaluation does not discriminate between aspects that are relevant for the assignment of semantic roles and those that are not.
Since we are using a relatively small set of features to start with, this does not present much of a problem.
In general, however, retaining only relevant aspects of intermediate monolingual representations would simplify the estimation of mapping between them and make <span class="ltx_text ltx_font_smallcaps">FRP</span> more robust.</p>
</div>
<div id="S6.SS5.SSS0.P1.p2" class="ltx_para">
<p class="ltx_p">For source language, this is relatively straightforward, as the intermediate representation can be directly tuned for the problem in question using labeled training data.
For target language, however, we assume that no labeled data is available and auxiliary tasks have to be used to achieve this.</p>
</div>
</div>
<div id="S6.SS5.SSS0.P2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Alternative Sources of Information</h4>

<div id="S6.SS5.SSS0.P2.p1" class="ltx_para">
<p class="ltx_p">The amount of parallel data available for many language pairs is growing steadily.
However, cross-lingual transfer methods are often applied in cases where parallel resources are scarce or of poor quality and must be used with care.
In such situations an ability to use alternative sources of information may be crucial.
Potential sources of such information include dictionary entries or information about the mapping between certain elements of syntactic structure, for example a known part-of-speech tag mapping.</p>
</div>
<div id="S6.SS5.SSS0.P2.p2" class="ltx_para">
<p class="ltx_p">The available parallel data itself may also be used more comprehensively – aligned arguments of aligned predicates, for example, constitute only a small part of it, while the mapping of vector representations of individual tokens is likely to be the same for all aligned words.</p>
</div>
</div>
<div id="S6.SS5.SSS0.P3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-source Transfer</h4>

<div id="S6.SS5.SSS0.P3.p1" class="ltx_para">
<p class="ltx_p">One of the strong points of direct model transfer is that it naturally fits the multi-source transfer setting.
There are several possible ways of adapting <span class="ltx_text ltx_font_smallcaps">FRP</span> to such a setting.
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer <cite class="ltx_cite">[]</cite>.</p>
</div>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">The authors would like to acknowledge the support of MMCI Cluster of Excellence and Saarbrücken Graduate School of Computer Science and thank the anonymous reviewers for their suggestions.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 18:08:42 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
