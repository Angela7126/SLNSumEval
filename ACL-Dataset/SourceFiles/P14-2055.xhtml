<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization</title>
<!--Generated on Wed Jun 11 17:48:22 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Annie Louis 
<br class="ltx_break"/>ILCC, School of Informatics,
<br class="ltx_break"/>University of Edinburgh, 
<br class="ltx_break"/>Edinburgh EH8 9AB, UK 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">alouis@inf.ed.ac.uk</span> 
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">In order to summarize a document, it is often useful
to have a <em class="ltx_emph">background</em> set of documents from the domain to serve as a reference for
determining new and important information in the input document.
We present a model based on Bayesian surprise which provides an intuitive way
to identify surprising information from a summarization input with respect to
a background corpus. Specifically, the method quantifies the degree to which pieces of information
in the input change one‚Äôs beliefs‚Äô about the world represented in the background. We
develop systems for generic and update summarization based on this idea. Our method provides competitive content selection performance with particular advantages
in the update task where systems are given a small and topical background corpus.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Important facts in a new text are those which deviate from previous knowledge on
the topic. When people create summaries, they
use their knowledge about the world to decide what content
in an input document is informative
to include in a summary.
Understandably in
automatic summarization as well, it is useful to keep a background set of documents
to represent general facts and their frequency in the domain.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">For example, in the simplest setting of multi-document summarization of news, systems are
asked to summarize an <em class="ltx_emph">input set</em> of topically-related news documents to reflect its central content.
In this <span class="ltx_text ltx_font_smallcaps">generic</span> task, some of the best reported results were obtained by a system <cite class="ltx_cite">[]</cite> which
computed importance scores for words in the input by examining if the word occurs with
significantly higher probability in the input compared to a large background collection of news articles.
Other specialized summarization tasks explicitly require the use of background information. In the <span class="ltx_text ltx_font_smallcaps">update</span> summarization task,
a system is given two sets of news documents on the same topic; the second contains articles published later
in time. The system should summarize the important updates from the second set assuming a
user has already read the first set of articles.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this work, we present a Bayesian model for assessing the
novelty of a sentence taken from a summarization input with respect to a background corpus of documents.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">Our model is based on the idea of Bayesian Surprise <cite class="ltx_cite">[]</cite>.
For illustration, assume that a user‚Äôs background knowledge comprises of multiple
hypotheses about the current state of the world and a probability distribution over these
hypotheses indicates his degree of belief in each hypothesis.
For example, one hypothesis may be that <em class="ltx_emph">the
political situation in Ukraine is peaceful</em>, another where <em class="ltx_emph">it is not</em>. Apriori
assume the user favors the hypothesis about a peaceful Ukraine, i.e. the hypothesis has
higher probability in
the prior distribution. Given new data, the evidence can be incorporated using Bayes Rule to
compute the posterior distribution over the hypotheses.
For example, upon viewing news reports about riots in the country, a user would
update his beliefs and the posterior distribution of the user‚Äôs knowledge would have
a higher probability for a riotous Ukraine. Bayesian surprise is the difference
between the prior and posterior distributions over the hypotheses which quantifies the extent
to which the new data (the news report) has changed a user‚Äôs prior beliefs about the world.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">In this work, we exemplify how Bayesian surprise can be used to do
content selection for text summarization.
Here a user‚Äôs prior knowledge is approximated by a background corpus
and we show how to identify sentences from the input set which are most surprising
with respect to this background.
We use the method to do two types of
summarization tasks: a) <span class="ltx_text ltx_font_smallcaps">generic</span> news summarization which uses
a large random collection of news articles as the background, and
b) <span class="ltx_text ltx_font_smallcaps">update</span> summarization where the background is a smaller but specific set of news documents
on the same topic as the input set. We find that our method
performs competitively with a previous log-likelihood ratio approach which identifies words with
significantly higher probability in the input compared to the background.
The Bayesian approach is more advantageous in the update task, where the background corpus is smaller in size.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Computing new information is useful in many applications. The TREC novelty tasks <cite class="ltx_cite">[]</cite>
tested the ability of systems to find novel information in an IR setting. Systems were given a list of documents
ranked according to relevance to a query. The goal is to find sentences in each document which are relevant to the query, and
at the same time is new information given the content of documents higher in the relevance list.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">For update summarization of news, methods range from textual entailment techniques
<cite class="ltx_cite">[]</cite> to find facts in the input
which are not entailed by the background, to
Bayesian topic models <cite class="ltx_cite">[]</cite> which aim to
learn and use topics discussed only in
background, those only in the update input and those that overlap across the two sets.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Even for generic summarization,
some of the best results were obtained by <cite class="ltx_cite"/>
by using a large random corpus of news articles as the background while summarizing a new article,
an idea first proposed by <cite class="ltx_cite"/>.
Central to this approach is the use of a likelihood ratio test to compute <span class="ltx_text ltx_font_italic">topic words</span>, words
that have significantly higher probability in the input compared to the background corpus, and
are hence descriptive of the input‚Äôs topic. In this work, we compare our system to topic word based ones since the latter is also a general
method to find surprising new words in a set of input documents but is not a bayesian approach.
We briefly explain the topic words based approach below.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Computing topic words:</span>
Let us call the input set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m1" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math> and the background <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p4.m2" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math>. The log-likelihood ratio test compares two hypotheses:</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m1" class="ltx_Math" alttext="H_{1}" display="inline"><msub><mi>H</mi><mn>1</mn></msub></math>: A word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is not a topic word and occurs with equal probability in <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m3" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m4" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math>, i.e. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p5.m5" class="ltx_Math" alttext="p(t|I)=p(t|B)=p" display="inline"><mrow><mi>p</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>I</mi><mo>)</mo></mrow><mo>=</mo><mi>p</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>B</mi><mo>)</mo></mrow><mo>=</mo><mi>p</mi></mrow></math></p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m1" class="ltx_Math" alttext="H_{2}" display="inline"><msub><mi>H</mi><mn>2</mn></msub></math>: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m2" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> is a topic word, hence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m3" class="ltx_Math" alttext="p(t|I)=p_{1}" display="inline"><mrow><mi>p</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>I</mi><mo>)</mo></mrow><mo>=</mo><msub><mi>p</mi><mn>1</mn></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m4" class="ltx_Math" alttext="p(t|B)=p_{2}" display="inline"><mrow><mi>p</mi><mrow><mo>(</mo><mi>t</mi><mo>|</mo><mi>B</mi><mo>)</mo></mrow><mo>=</mo><msub><mi>p</mi><mn>2</mn></msub></mrow></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p6.m5" class="ltx_Math" alttext="p_{1}&gt;p_{2}" display="inline"><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>p</mi><mn>2</mn></msub></mrow></math></p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">A set of documents <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m1" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> containing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> tokens
is viewed as a sequence of words <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m3" class="ltx_Math" alttext="w_{1}" display="inline"><msub><mi>w</mi><mn>1</mn></msub></math><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m4" class="ltx_Math" alttext="w_{2}" display="inline"><msub><mi>w</mi><mn>2</mn></msub></math>‚Ä¶<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m5" class="ltx_Math" alttext="w_{N}" display="inline"><msub><mi>w</mi><mi>N</mi></msub></math>. The word in each position <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m6" class="ltx_Math" alttext="i" display="inline"><mi>i</mi></math> is
assumed to be generated by a Bernoulli trial which succeeds when the generated word
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m7" class="ltx_Math" alttext="w_{i}=t" display="inline"><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mi>t</mi></mrow></math> and fails when <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m8" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> is not <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m9" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math>. Suppose that the probability of success is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m10" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>.
Then the probability of a word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m11" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> appearing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m12" class="ltx_Math" alttext="k" display="inline"><mi>k</mi></math> times in a dataset of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p7.m13" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> tokens is
the binomial probability:</p>
</div>
<div id="S2.p8" class="ltx_para">
<table id="S2.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m1" class="ltx_Math" alttext="b(k,N,p)=\dbinom{N}{k}p^{k}(1-p)^{N-k}" display="block"><mrow><mrow><mi>b</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><mi>k</mi><mo>,</mo><mi>N</mi><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mtable columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd><mi>N</mi></mtd></mtr><mtr><mtd><mi>k</mi></mtd></mtr></mtable><mo>)</mo></mrow><mo>‚Å¢</mo><msup><mi>p</mi><mi>k</mi></msup><mo>‚Å¢</mo><msup><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow><mo>)</mo></mrow><mrow><mi>N</mi><mo>-</mo><mi>k</mi></mrow></msup></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
</div>
<div id="S2.p9" class="ltx_para">
<p class="ltx_p">The likelihood ratio compares the likelihood of the data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p9.m1" class="ltx_Math" alttext="D=\{B,I\}" display="inline"><mrow><mi>D</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>B</mi><mo>,</mo><mi>I</mi></mrow><mo>}</mo></mrow></mrow></math> under the two hypotheses.</p>
</div>
<div id="S2.p10" class="ltx_para">
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E2" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m1" class="ltx_Math" alttext="\displaystyle\lambda=\frac{P(D|H_{1})}{P(D|H_{2})}{=}\frac{b(c_{t},N,p)}{b(c_{%&#10;I},N_{I},p_{1})\textrm{ }b(c_{B},N_{B},p_{2})}" display="inline"><mrow><mi>Œª</mi><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>D</mi><mo>|</mo><msub><mi>H</mi><mn>1</mn></msub><mo>)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo>(</mo><mi>D</mi><mo>|</mo><msub><mi>H</mi><mn>2</mn></msub><mo>)</mo></mrow></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>b</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>,</mo><mi>N</mi><mo>,</mo><mi>p</mi></mrow><mo>)</mo></mrow></mrow><mrow><mi>b</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mi>c</mi><mi>I</mi></msub><mo>,</mo><msub><mi>N</mi><mi>I</mi></msub><mo>,</mo><msub><mi>p</mi><mn>1</mn></msub></mrow><mo>)</mo></mrow><mo>‚Å¢</mo><mtext>¬†</mtext><mo>‚Å¢</mo><mi>b</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mi>c</mi><mi>B</mi></msub><mo>,</mo><msub><mi>N</mi><mi>B</mi></msub><mo>,</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
</div>
<div id="S2.p11" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m2" class="ltx_Math" alttext="p_{1}" display="inline"><msub><mi>p</mi><mn>1</mn></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m3" class="ltx_Math" alttext="p_{2}" display="inline"><msub><mi>p</mi><mn>2</mn></msub></math> are estimated by maximum likelihood. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m4" class="ltx_Math" alttext="p=c_{t}/N" display="inline"><mrow><mi>p</mi><mo>=</mo><mrow><msub><mi>c</mi><mi>t</mi></msub><mo>/</mo><mi>N</mi></mrow></mrow></math> where
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m5" class="ltx_Math" alttext="c_{t}" display="inline"><msub><mi>c</mi><mi>t</mi></msub></math> is the number of times word <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m6" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> appears in the total set of tokens comprising <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m7" class="ltx_Math" alttext="\{B,I\}" display="inline"><mrow><mo>{</mo><mrow><mi>B</mi><mo>,</mo><mi>I</mi></mrow><mo>}</mo></mrow></math>. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m8" class="ltx_Math" alttext="p_{1}=c_{t}^{I}/N_{I}" display="inline"><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>=</mo><mrow><msubsup><mi>c</mi><mi>t</mi><mi>I</mi></msubsup><mo>/</mo><msub><mi>N</mi><mi>I</mi></msub></mrow></mrow></math>
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m9" class="ltx_Math" alttext="p_{2}=c_{t}^{B}/N_{B}" display="inline"><mrow><msub><mi>p</mi><mn>2</mn></msub><mo>=</mo><mrow><msubsup><mi>c</mi><mi>t</mi><mi>B</mi></msubsup><mo>/</mo><msub><mi>N</mi><mi>B</mi></msub></mrow></mrow></math> are the probabilities of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p11.m10" class="ltx_Math" alttext="t" display="inline"><mi>t</mi></math> estimated only from the input and only from
the background respectively.</p>
</div>
<div id="S2.p12" class="ltx_para">
<p class="ltx_p">A convenient aspect of this approach is that <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m1" class="ltx_Math" alttext="-2\log\lambda" display="inline"><mrow><mo>-</mo><mrow><mn>2</mn><mo>‚Å¢</mo><mrow><mi>log</mi><mo>‚Å°</mo><mi>Œª</mi></mrow></mrow></mrow></math> is asymptotically <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m2" class="ltx_Math" alttext="\chi^{2}" display="inline"><msup><mi>œá</mi><mn>2</mn></msup></math> distributed.
So for a resulting <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m3" class="ltx_Math" alttext="-2\log\lambda" display="inline"><mrow><mo>-</mo><mrow><mn>2</mn><mo>‚Å¢</mo><mrow><mi>log</mi><mo>‚Å°</mo><mi>Œª</mi></mrow></mrow></mrow></math> value, we can use the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m4" class="ltx_Math" alttext="\chi^{2}" display="inline"><msup><mi>œá</mi><mn>2</mn></msup></math> table to find the
significance level with which the null hypothesis <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m5" class="ltx_Math" alttext="H_{1}" display="inline"><msub><mi>H</mi><mn>1</mn></msub></math> can be rejected.
For example, a value of 10 corresponds to a
significance level of 0.001 and is standardly used as the cutoff. Words
with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.p12.m6" class="ltx_Math" alttext="-2\log\lambda&gt;10" display="inline"><mrow><mrow><mo>-</mo><mrow><mn>2</mn><mo>‚Å¢</mo><mrow><mi>log</mi><mo>‚Å°</mo><mi>Œª</mi></mrow></mrow></mrow><mo>&gt;</mo><mn>10</mn></mrow></math> are considered topic words.
<cite class="ltx_cite"/>‚Äôs system gives a weight of 1 to the topic words and
scores sentences using the number of topic words normalized by sentence length.</p>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Bayesian Surprise</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">First we present the formal definition of Bayesian surprise given by <cite class="ltx_cite"/> without
reference to the summarization task.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Let <em class="ltx_emph ltx_font_bold">H</em> be the space of all hypotheses representing the background knowledge of a user.
The user has a probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m1" class="ltx_Math" alttext="{P(H)}" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> associated with each hypothesis <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m2" class="ltx_Math" alttext="H\in" display="inline"><mrow><mi>H</mi><mo>‚àà</mo><mi/></mrow></math> <em class="ltx_emph ltx_font_bold">H</em>.
Let <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m3" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> be a new observation. The posterior probability of a single hypothesis <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p2.m4" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math>
can be
computed as:</p>
</div>
<div id="S3.p3" class="ltx_para">
<table id="S3.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E3.m1" class="ltx_Math" alttext="P(H|D)=\frac{P(D|H)P(H)}{P(D)}" display="block"><mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>D</mi><mo>)</mo></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>D</mi><mo>|</mo><mi>H</mi><mo>)</mo></mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>D</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">The surprise <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m1" class="ltx_Math" alttext="S(D,{\bf H})" display="inline"><mrow><mi>S</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><mi>D</mi><mo>,</mo><mi>ùêá</mi></mrow><mo>)</mo></mrow></mrow></math> created by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p4.m2" class="ltx_Math" alttext="D" display="inline"><mi>D</mi></math> on hypothesis space <em class="ltx_emph ltx_font_bold">H</em> is defined as
the difference between the prior and posterior distributions over the hypotheses, and is
computed using KL divergence.</p>
</div>
<div id="S3.p5" class="ltx_para">
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S3.E4" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m1" class="ltx_Math" alttext="\displaystyle S(D,{\bf H})" display="inline"><mrow><mi>S</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><mi>D</mi><mo>,</mo><mi>ùêá</mi></mrow><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E4.m3" class="ltx_Math" alttext="\displaystyle\textrm{KL}(P(H|D),P(H)))" display="inline"><mrow><mtext>KL</mtext><mrow><mo>(</mo><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>D</mi><mo>)</mo></mrow><mo>,</mo><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow><mo>)</mo></mrow><mo>)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
<tr id="S3.E5" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.E5.m3" class="ltx_Math" alttext="\displaystyle\int_{\bf H}P(H|D)\log\frac{P(H|D)}{P(H)}" display="inline"><mrow><mstyle displaystyle="true"><msub><mo largeop="true" symmetric="true">‚à´</mo><mi>ùêá</mi></msub></mstyle><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>D</mi><mo>)</mo></mrow><mi>log</mi><mstyle displaystyle="true"><mfrac><mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>D</mi><mo>)</mo></mrow></mrow><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">Note that since KL-divergence is not symmetric, we could also compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S3.p6.m1" class="ltx_Math" alttext="\textrm{KL}(P(H),P(H|D))" display="inline"><mrow><mtext>KL</mtext><mrow><mo>(</mo><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow><mo>,</mo><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>D</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></math> as the surprise value.
In some cases, surprise can be computed analytically, in particular when the prior distribution is
conjugate to the form of the hypothesis, and so the posterior has the same functional form as the prior.
(See <cite class="ltx_cite"/> for the surprise computation for
different families of probability distributions).</p>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Summarization with Bayesian Surprise</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">We consider the hypothesis space <span class="ltx_text ltx_font_bold">H</span> as the set of all the hypotheses
encoding background knowledge. A single hypothesis about the background takes
the form of a multinomial distribution over word unigrams. For example, one
multinomial may have higher word probabilities for ‚ÄòUkraine‚Äô and ‚Äòpeaceful‚Äô and
another multinomial has higher probabilities for ‚ÄòUkraine‚Äô and ‚Äòriots‚Äô.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m1" class="ltx_Math" alttext="P(H)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> gives a prior probability to each hypothesis based on
the information in the background corpus. In our case,
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m2" class="ltx_Math" alttext="P(H)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> is a Dirichlet distribution, the conjugate prior for multinomials.
Suppose that the vocabulary size of the background corpus is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m3" class="ltx_Math" alttext="V" display="inline"><mi>V</mi></math> and we label the word types as
(<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m4" class="ltx_Math" alttext="w_{1}" display="inline"><msub><mi>w</mi><mn>1</mn></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m5" class="ltx_Math" alttext="w_{2}" display="inline"><msub><mi>w</mi><mn>2</mn></msub></math>, ‚Ä¶ <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p1.m6" class="ltx_Math" alttext="w_{V}" display="inline"><msub><mi>w</mi><mi>V</mi></msub></math>). Then,</p>
</div>
<div id="S4.p2" class="ltx_para">
<table id="S4.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E6.m1" class="ltx_Math" alttext="P(H)=Dir(\alpha_{1},\alpha_{2},...\alpha_{V})" display="block"><mrow><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>D</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mi>Œ±</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Œ±</mi><mn>2</mn></msub><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>‚Å¢</mo><msub><mi>Œ±</mi><mi>V</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p3.m1" class="ltx_Math" alttext="\alpha_{1:V}" display="inline"><msub><mi>Œ±</mi><mrow><mn>1</mn><mo>:</mo><mi>V</mi></mrow></msub></math> are the concentration parameters of the Dirichlet distribution (and will be set using
the background corpus as explained in Section <a href="#S4.SS2" title="4.2 Input and background ‚Ä£ 4 Summarization with Bayesian Surprise ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">Now consider a new observation <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m1" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math> (a text, sentence, or paragraph from the <span class="ltx_text ltx_font_italic">summarization input</span>) and the word counts in
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m2" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math> given by (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m3" class="ltx_Math" alttext="c_{1}" display="inline"><msub><mi>c</mi><mn>1</mn></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m4" class="ltx_Math" alttext="c_{2}" display="inline"><msub><mi>c</mi><mn>2</mn></msub></math>, ‚Ä¶, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m5" class="ltx_Math" alttext="c_{V}" display="inline"><msub><mi>c</mi><mi>V</mi></msub></math>). Then the posterior over <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p4.m6" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> is the dirichlet:</p>
</div>
<div id="S4.p5" class="ltx_para">
<table id="S4.E7" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E7.m1" class="ltx_Math" alttext="P(H|I)=Dir(\alpha_{1}+c_{1},\alpha_{2}+c_{2},...,\alpha_{V}+c_{V})" display="block"><mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><mi>I</mi><mo>)</mo></mrow><mo>=</mo><mi>D</mi><mi>i</mi><mi>r</mi><mrow><mo>(</mo><msub><mi>Œ±</mi><mn>1</mn></msub><mo>+</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Œ±</mi><mn>2</mn></msub><mo>+</mo><msub><mi>c</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo>,</mo><msub><mi>Œ±</mi><mi>V</mi></msub><mo>+</mo><msub><mi>c</mi><mi>V</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
</table>
</div>
<div id="S4.p6" class="ltx_para">
<p class="ltx_p">The surprise due to observing <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m1" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p6.m2" class="ltx_Math" alttext="S(I,{\bf H})" display="inline"><mrow><mi>S</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><mi>I</mi><mo>,</mo><mi>ùêá</mi></mrow><mo>)</mo></mrow></mrow></math> is the KL divergence between the two dirichlet distributions. (Details
about computing KL divergence between two dirichlet distributions can be found in <cite class="ltx_cite"/> and <cite class="ltx_cite"/>).</p>
</div>
<div id="S4.p7" class="ltx_para">
<p class="ltx_p">Below we propose a general algorithm for summarization using surprise
computation. Then we define the prior distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.p7.m1" class="ltx_Math" alttext="P(H)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> for each of our two tasks,
<span class="ltx_text ltx_font_smallcaps">generic</span> and <span class="ltx_text ltx_font_smallcaps">update</span> summarization.</p>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Extractive summarization algorithm</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">We first compute a surprise value for each word type
in the summarization input. Word scores are aggregated
to obtain a score for each sentence.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Step 1: Word score.</span> Suppose that word type <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m1" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math> appears <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m2" class="ltx_Math" alttext="c_{i}" display="inline"><msub><mi>c</mi><mi>i</mi></msub></math> times in the summarization input <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m3" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math>. We obtain the
posterior distribution after seeing all instances of this word (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m4" class="ltx_Math" alttext="\bf{w_{i}}" display="inline"><msub><mi>ùê∞</mi><mi>ùê¢</mi></msub></math>) as
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m5" class="ltx_Math" alttext="P(H|{\bf w_{i}})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><msub><mi>ùê∞</mi><mi>ùê¢</mi></msub><mo>)</mo></mrow></mrow></math> = <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m6" class="ltx_Math" alttext="Dir(\alpha_{1},\alpha_{2},...\alpha_{i}+c_{i},...\alpha_{V})" display="inline"><mrow><mi>D</mi><mo>‚Å¢</mo><mi>i</mi><mo>‚Å¢</mo><mi>r</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mrow><msub><mi>Œ±</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Œ±</mi><mn>2</mn></msub><mo>,</mo><mrow><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>‚Å¢</mo><msub><mi>Œ±</mi><mi>i</mi></msub></mrow><mo>+</mo><msub><mi>c</mi><mi>i</mi></msub></mrow><mo>,</mo><mrow><mi mathvariant="normal">‚Ä¶</mi><mo>‚Å¢</mo><msub><mi>Œ±</mi><mi>V</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></math>. The score for <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m7" class="ltx_Math" alttext="w_{i}" display="inline"><msub><mi>w</mi><mi>i</mi></msub></math>
is the surprise computed as KL divergence between <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m8" class="ltx_Math" alttext="P(H|{\bf w_{i}})" display="inline"><mrow><mi>P</mi><mrow><mo>(</mo><mi>H</mi><mo>|</mo><msub><mi>ùê∞</mi><mi>ùê¢</mi></msub><mo>)</mo></mrow></mrow></math> and the prior <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS1.p2.m9" class="ltx_Math" alttext="P(H)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> (eqn. <a href="#S4.E6" title="(6) ‚Ä£ 4 Summarization with Bayesian Surprise ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Step 2: Sentence score.</span> The composition functions to obtain sentence scores
from word scores can impact content selection performance <cite class="ltx_cite">[]</cite>. We experiment
with sum and average value of the word scores.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>An alternative algorithm could
directly compute the surprise of a sentence by incorporating the words from the sentence into the posterior.
However, we found this specific method to not work well probably because the few and unrepeated content words from
a sentence did not change the posterior much. In future, we plan
to use latent topic models to assign a topic to a sentence so that the counts of all the sentence‚Äôs words
can be aggregated into one dimension.
</span></span></span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Step 3: Sentence selection.</span> The goal is to select a subset of sentences with high surprise values.
We follow a greedy approach to optimize the summary surprise by choosing the most surprising sentence, the
next most surprising and so on. At the
same time, we aim to avoid redundancy, i.e. selecting sentences with similar content. After a
sentence is selected for the summary, the surprise for words from this sentence are set to zero. We recompute the surprise
for the remaining sentences using step 2 and the selection process continues until the summary length limit is reached.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p class="ltx_p">The key differences between our Bayesian approach and a method such as topic words are: (i) The Bayesian
approach keeps multiple hypotheses about the background rather than a single one. Surprise is computed
based on the changes in probabilities of all of these hypotheses upon seeing the summarization input. (ii) The computation
of topic words is local, it assumes a binomial distribution and the occurrence of a word is
independent of others. In contrast, word surprise although computed for each word type separately,
quantifies the surprise when incorporating the new counts of this word into the background multinomials.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>Input and background</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Here we describe the input sets and background corpus used for the two summarization tasks and define
the prior distribution for each. We use data from the DUC<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><a href="http://www-nlpir.nist.gov/projects/duc/index.html" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www-nlpir.nist.gov/projects/duc/index.html</span></a></span></span></span> and
TAC<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><a href="http://www.nist.gov/tac/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://www.nist.gov/tac/</span></a></span></span></span> summarization evaluation workshops
conducted by NIST.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Generic summarization.</span>
We use multidocument inputs from DUC 2004.
There were 50 inputs, each contains around 10 documents on a common
topic. Each input is also provided with 4 manually written
summaries created by NIST assessors. We use these manual summaries for evaluation.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">The background corpus is a collection of 5000 randomly selected
articles from the English Gigaword corpus. We use a list of 571 stop words from the
SMART IR system <cite class="ltx_cite">[]</cite> and the remaining
content word vocabulary has 59,497 word types. The count of each word in the background is
calculated and used as the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>Œ±</mi></math> parameters of the prior Dirichlet distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS2.p3.m2" class="ltx_Math" alttext="P(H)" display="inline"><mrow><mi>P</mi><mo>‚Å¢</mo><mrow><mo>(</mo><mi>H</mi><mo>)</mo></mrow></mrow></math> (eqn. <a href="#S4.E6" title="(6) ‚Ä£ 4 Summarization with Bayesian Surprise ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Update summarization.</span>
This task uses data from TAC 2009. An input has two sets of documents,
A and B, each containing 10 documents. Both A and B are on same topic but documents in B were
published at a later time than A (background). There were 44 inputs and 4 manual update summaries
are provided for each.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p class="ltx_p">The prior parameters are the counts of words in A for that input
(using the same stoplist). The vocabulary of these A sets is smaller,
ranging from 400 to 3000 words for the different inputs.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p class="ltx_p">In practice for both tasks, a new summarization input can have words unseen in the
background. So <em class="ltx_emph">new</em> words in an input are added to the
background corpus with a count of 1 and the counts of <em class="ltx_emph">existing</em> words in the background
are incremented by 1 before computing the prior parameters.
The summary length limit is 100 words in both tasks.</p>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Systems for comparison</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">We compare against three types of systems, (i) those which similarly to surprise, use a
background corpus to identify important sentences, (ii) a system that uses information
from the input set only and no background, and (iii) systems that combine scores from
the input and background.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{back}}" display="inline"><msub><mtext>ùêäùêã</mtext><mtext>ùêõùêöùêúùê§</mtext></msub></math><span class="ltx_text ltx_font_bold">:</span> represents a simple baseline for surprise computation from a background corpus.
A <em class="ltx_emph">single</em> unigram probability distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m2" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math> is created from the background using
maximum likelihood. The summary is created by greedily adding sentences which maximize KL divergence between <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m3" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math>
and the current summary. Suppose the set of sentences currently chosen in the
summary is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m4" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>. The next step chooses the sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p2.m5" class="ltx_Math" alttext="s_{l}=\arg\max_{s_{i}}\textrm{KL}(\{S\cup s_{i}\}||B)" display="inline"><mrow><msub><mi>s</mi><mi>l</mi></msub><mo>=</mo><mi>arg</mi><msub><mo>max</mo><msub><mi>s</mi><mi>i</mi></msub></msub><mtext>KL</mtext><mrow><mo>(</mo><mrow><mo>{</mo><mi>S</mi><mo>‚à™</mo><msub><mi>s</mi><mi>i</mi></msub><mo>}</mo></mrow><mo>|</mo><mo>|</mo><mi>B</mi><mo>)</mo></mrow></mrow></math> .</p>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m1" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>ùêìùêí</mtext><mtext>ùê¨ùêÆùê¶</mtext></msub></math><span class="ltx_text ltx_font_bold">, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m2" class="ltx_Math" alttext="\textrm{TS}_{\textrm{avg}}" display="inline"><msub><mtext>ùêìùêí</mtext><mtext>ùêöùêØùê†</mtext></msub></math>:</span> use topic words computed
as described in Section <a href="#S2" title="2 Related work ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and utilizing the same background corpus for the
generic and update tasks as the surprise-based methods. For the generic task, we use a critical value of
10 (0.001 significance level) for the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p3.m3" class="ltx_Math" alttext="\chi^{2}" display="inline"><msup><mi>œá</mi><mn>2</mn></msup></math> distribution during topic word computation. In the update task however, the
background corpus A is smaller and for most inputs, no words exceeded this cutoff.
We lower the significance level to the generally accepted value of
0.05 and take words scoring above this as topic words. The number of topic words is still small (ranging from 1 to 30) for
different inputs.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m1" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>TS</mtext><mtext>sum</mtext></msub></math> system selects sentences with greater counts of topic words
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p4.m2" class="ltx_Math" alttext="\textrm{TS}_{\textrm{avg}}" display="inline"><msub><mtext>TS</mtext><mtext>avg</mtext></msub></math> computes the number of topic words
normalized by sentence length. A greedy selection procedure is used. To reduce redundancy, once a sentence is added,
the topic words contained in it are removed from the topic word list before the next sentence selection.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p5.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>ùêäùêã</mtext><mtext>ùê¢ùêßùê©</mtext></msub></math><span class="ltx_text ltx_font_bold">:</span> represents the system that <span class="ltx_text ltx_font_italic">does not use</span> background information. Rather the
method creates a summary by optimizing for high similarity of the summary with the input word distribution.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p class="ltx_p">Suppose the input unigram distribution is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m1" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math> and the current summary is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m2" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>,
the method chooses the sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m3" class="ltx_Math" alttext="s_{l}=\arg\min_{s_{i}}\textrm{KL}(\{S\cup s_{i}\}||I)" display="inline"><mrow><msub><mi>s</mi><mi>l</mi></msub><mo>=</mo><mi>arg</mi><msub><mo>min</mo><msub><mi>s</mi><mi>i</mi></msub></msub><mtext>KL</mtext><mrow><mo>(</mo><mrow><mo>{</mo><mi>S</mi><mo>‚à™</mo><msub><mi>s</mi><mi>i</mi></msub><mo>}</mo></mrow><mo>|</mo><mo>|</mo><mi>I</mi><mo>)</mo></mrow></mrow></math> at each iteration. Since
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p6.m4" class="ltx_Math" alttext="\{S\cup s_{i}\}" display="inline"><mrow><mo>{</mo><mrow><mi>S</mi><mo>‚à™</mo><msub><mi>s</mi><mi>i</mi></msub></mrow><mo>}</mo></mrow></math> is used to compute divergence, redundancy is implicitly controlled in this approach.
Such a KL objective was used in competitive systems in the past
<cite class="ltx_cite">[]</cite>.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Input <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m1" class="ltx_Math" alttext="+" display="inline"><mo mathvariant="normal">+</mo></math> background:</span> These systems combine (i) a score based on
the background (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m2" class="ltx_Math" alttext="\textrm{KL}_{\textrm{back}}" display="inline"><msub><mtext>KL</mtext><mtext>back</mtext></msub></math>, <span class="ltx_text ltx_markedasmath">TS</span> or <span class="ltx_text ltx_markedasmath">SR</span>) with (ii) the score
based on the input only (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m5" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math>).
For example, to combine <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m6" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>TS</mtext><mtext>sum</mtext></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m7" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math>: for each sentence, we compute its scores
based on the two methods. Then we normalize the two sets of scores for candidate sentences using z-scores
and compute the best sentence as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m8" class="ltx_Math" alttext="\arg\max_{s_{i}}" display="inline"><mrow><mi>arg</mi><mo>‚Å¢</mo><msub><mo>max</mo><msub><mi>s</mi><mi>i</mi></msub></msub></mrow></math> (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m9" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}(s_{i})" display="inline"><mrow><msub><mtext>TS</mtext><mtext>sum</mtext></msub><mo>‚Å¢</mo><mrow><mo>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> - <math xmlns="http://www.w3.org/1998/Math/MathML" id="S5.p7.m10" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}(s_{i})" display="inline"><mrow><msub><mtext>KL</mtext><mtext>inp</mtext></msub><mo>‚Å¢</mo><mrow><mo>(</mo><msub><mi>s</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math>). Redundancy
control is done similarly to the <span class="ltx_text ltx_markedasmath">TS</span> only systems.</p>
</div>
</div>
<div id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">6 </span>Content selection results</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">For evaluation, we compare each summary to the four manual summaries using ROUGE <cite class="ltx_cite">[]</cite>. All
summaries were truncated to 100 words, stemming was performed and stop words were <span class="ltx_text ltx_font_bold">not</span>
removed, as is standard in TAC evaluations. We report the ROUGE-1 and ROUGE-2 recall scores (average over the inputs) for
each system. We use the Wilcoxon signed-rank test to check for significant
differences in mean scores.
Table <a href="#S6.T1" title="Table¬†1 ‚Ä£ 6 Content selection results ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the scores for generic summaries and
<a href="#S6.T2" title="Table¬†2 ‚Ä£ 6 Content selection results ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the update task. For each system, the peer systems with
significantly better scores (p-value <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p1.m1" class="ltx_Math" alttext="&lt;0.05" display="inline"><mrow><mi/><mo>&lt;</mo><mn>0.05</mn></mrow></math>) are indicated within parentheses.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">We refer to the surprise-based summaries
as <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m1" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext>ùêíùêë</mtext><mtext>ùê¨ùêÆùê¶</mtext></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p2.m2" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext>ùêíùêë</mtext><mtext>ùêöùêØùê†</mtext></msub></math> depending on the type of composition function (Section <a href="#S4.SS1" title="4.1 Extractive summarization algorithm ‚Ä£ 4 Summarization with Bayesian Surprise ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<div id="S6.T1" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">ROUGE-1</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">ROUGE-2</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{back}}" display="inline"><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">back</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.2276 (</span><span class="ltx_text ltx_markedasmath ltx_font_tiny">TS</span><span class="ltx_text ltx_font_tiny">, <span class="ltx_text ltx_markedasmath">SR</span></span><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0250 (</span><span class="ltx_text ltx_markedasmath ltx_font_tiny">TS</span><span class="ltx_text ltx_font_tiny">, <span class="ltx_text ltx_markedasmath">SR</span></span><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m6" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3078</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.0616</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m7" class="ltx_Math" alttext="\textrm{TS}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.2841 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m8" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math><span class="ltx_text ltx_font_tiny">, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m9" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math></span><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0493 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m10" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m11" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.3120</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0580</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m12" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3003</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0549</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m13" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub></math></th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">0.3075 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m14" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">0.0684</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m15" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{sum}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3250</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0725</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m16" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.3410</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.0795</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m17" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{sum}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3187 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m18" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0660 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m19" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m20" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3220 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T1.m21" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0696</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table¬†1: </span>Evaluation results for generic summaries. Systems in parentheses are significantly better.</div>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">First, consider <span class="ltx_text ltx_font_smallcaps">generic</span> summarization and the systems which
use the background corpus only (those above the horizontal line). The <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{back}}" display="inline"><msub><mtext>KL</mtext><mtext>back</mtext></msub></math>
baseline performs significantly worse than topic words and surprise summaries. Numerically, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m2" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext>SR</mtext><mtext>sum</mtext></msub></math>
has the highest ROUGE-1 score and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m3" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>TS</mtext><mtext>sum</mtext></msub></math> tops according to ROUGE-2. As per the
Wilcoxon test, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m4" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>TS</mtext><mtext>sum</mtext></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m5" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext>SR</mtext><mtext>sum</mtext></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p3.m6" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext>SR</mtext><mtext>avg</mtext></msub></math> scores
are statistically indistinguishable at 95% confidence
level.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p">Systems below the horizontal line in Table <a href="#S6.T1" title="Table¬†1 ‚Ä£ 6 Content selection results ‚Ä£ A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
use an objective which combines both similarity with the input
and difference from the background. The first line here shows that
a system optimizing only for input similarity, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math>,
by itself has higher scores (though not significant) than
those using background information only. This result is not surprising for
generic summarization where all the topical content is present in the input and the
background is a non-focused random collection. At the same time, adding either <span class="ltx_text ltx_markedasmath">TS</span> or <span class="ltx_text ltx_markedasmath">SR</span> scores to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m4" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math> almost always leads to better
results with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p4.m5" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}+\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext>KL</mtext><mtext>inp</mtext></msub><mo>+</mo><msub><mtext>TS</mtext><mtext>avg</mtext></msub></mrow></math> giving the best score.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_smallcaps">update</span> summarization,
the surprise-based methods have an advantage over the topic word ones. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m1" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext>SR</mtext><mtext>avg</mtext></msub></math>
is significantly better than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m2" class="ltx_Math" alttext="\textrm{TS}_{\textrm{avg}}" display="inline"><msub><mtext>TS</mtext><mtext>avg</mtext></msub></math> for both ROUGE-1 and ROUGE-2 scores and better than
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m3" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext>TS</mtext><mtext>sum</mtext></msub></math> according to ROUGE-1. In fact, the surprise methods have numerically higher ROUGE-1 scores
compared to input similarity (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m4" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math>) in contrast to generic summarization. When combined with <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m5" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math>, the surprise methods
provide improved results, significantly better in terms of ROUGE-1 scores. The <span class="ltx_text ltx_markedasmath">TS</span> methods do not lead to
any improvement, and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m7" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}+\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext>KL</mtext><mtext>inp</mtext></msub><mo>+</mo><msub><mtext>TS</mtext><mtext>avg</mtext></msub></mrow></math> is significantly
worse than <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.p5.m8" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext>KL</mtext><mtext>inp</mtext></msub></math> only.
The limitation of the <span class="ltx_text ltx_markedasmath">TS</span> approach arises from the
paucity of topic words that exceed the significance cutoff applied on the log-likelihood ratio. But Bayesian surprise
is robust on the small background corpus and does not need any tuning for cutoff values depending
on the size of the background set.</p>
</div>
<div id="S6.T2" class="ltx_table">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">ROUGE-1</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">ROUGE-2</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m1" class="ltx_Math" alttext="\textrm{KL}_{\textrm{back}}" display="inline"><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">back</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.2246 (</span><span class="ltx_text ltx_markedasmath ltx_font_tiny">TS</span><span class="ltx_text ltx_font_tiny">, <span class="ltx_text ltx_markedasmath">SR</span></span><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0213 (</span><span class="ltx_text ltx_markedasmath ltx_font_tiny">TS</span><span class="ltx_text ltx_font_tiny">, <span class="ltx_text ltx_markedasmath">SR</span></span><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m6" class="ltx_Math" alttext="\textrm{TS}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3037 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m7" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0563</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m8" class="ltx_Math" alttext="\textrm{TS}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.2909 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m9" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math><span class="ltx_text ltx_font_tiny">, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m10" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></span><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0477 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m11" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math><span class="ltx_text ltx_font_tiny">, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m12" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></span><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m13" class="ltx_Math" alttext="\textrm{SR}_{\textrm{sum}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3201</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.0640</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m14" class="ltx_Math" alttext="\textrm{SR}_{\textrm{avg}}" display="inline"><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.3226</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0639</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m15" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub></math></th>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">0.3098 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m16" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_small">0.0710</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m17" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{sum}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3010 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m18" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{sum, avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum, avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0635</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m19" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{TS}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">TS</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3021 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m20" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{sum, avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum, avg</mtext></msub></mrow></math><span class="ltx_text ltx_font_small">)</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0543 (</span><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m21" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}" display="inline"><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub></math><span class="ltx_text ltx_font_tiny">,</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td"/>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_tiny">¬†¬† <math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m22" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{sum, avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo mathsize="normal" stretchy="false">+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum, avg</mtext></msub></mrow></math></span><span class="ltx_text ltx_font_small">)</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m23" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{sum}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">sum</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.3292</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_small">0.0721</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S6.T2.m24" class="ltx_Math" alttext="\textrm{KL}_{\textrm{inp}}{+}\textrm{SR}_{\textrm{avg}}" display="inline"><mrow><msub><mtext mathsize="small" stretchy="false">KL</mtext><mtext mathsize="small" stretchy="false">inp</mtext></msub><mo>+</mo><msub><mtext mathsize="small" stretchy="false">SR</mtext><mtext mathsize="small" stretchy="false">avg</mtext></msub></mrow></math></th>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.3379</span></td>
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold ltx_font_small">0.0767</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_font_small"><span class="ltx_tag ltx_tag_table">Table¬†2: </span>Evaluation results for update summaries. Systems in parentheses are significantly better. </div>
</div>
<div id="S6.p6" class="ltx_para">
<p class="ltx_p">Note that these models do not perform on par with summarization systems that use multiple
indicators of content importance, involve supervised training and which perform sentence compression.
Rather our goal in this work is to demonstrate a simple and intuitive
unsupervised model.</p>
</div>
</div>
<div id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">We have introduced a Bayesian summarization method that strongly aligns with intuitions
about how people use existing knowledge to identify important events or content
in new observations.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">Our method is especially valuable when a system
must utilize a small background corpus.
While the update task datasets we have used were carefully selected and grouped by NIST assesors into
initial and background sets, for systems on the web, there is little control over the number of
background documents on a particular topic. A system should be able to use smaller amounts of
background information and as new data arrives, be able to incorporate the evidence. Our Bayesian approach is a natural fit in such a setting.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">The author was supported by a Newton International Fellowship (NF120479)
from the Royal Society and the British Academy.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p">/thesis.</p>
</div>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Jun 11 17:48:22 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
