<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization</title>
<!--Generated on Tue Jun 10 19:11:18 2014 by LaTeXML (version 0.8.0) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8"/>
<link rel="stylesheet" href="LaTeXML.css" type="text/css"/>
<link rel="stylesheet" href="ltx-article.css" type="text/css"/>
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Unsupervised Dependency Parsing with Transferring 
<br class="ltx_break"/>Distribution via Parallel Guidance and Entropy Regularization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xuezhe Ma 
<br class="ltx_break"/>Department of Linguistics 
<br class="ltx_break"/>University of Washington 
<br class="ltx_break"/>Seattle, WA 98195, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">xzma@uw.edu</span> 
<br class="ltx_break"/>&amp;Fei Xia 
<br class="ltx_break"/>Department of Linguistics 
<br class="ltx_break"/>University of Washington 
<br class="ltx_break"/>Seattle, WA 98195, USA 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter">fxia@uw.edu</span> 
<br class="ltx_break"/>
</span></span></div>
<div class="ltx_date ltx_role_creation"/>

<div class="ltx_abstract"><h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">We present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data, but have
translated text in a resource-rich language. We train probabilistic parsing models for resource-poor languages by transferring cross-lingual
knowledge from resource-rich language with entropy regularization. Our method can be used as a purely monolingual dependency parser,
requiring no human translations for the test data, thus making it applicable to a wide range of resource-poor languages.
We perform experiments on three Data sets — Version 1.0 and version 2.0 of Google Universal Dependency Treebanks and Treebanks from CoNLL
shared-tasks, across ten languages. We obtain state-of-the art performance of all the three data sets when compared with
previously studied unsupervised and projected parsing systems.</p>
</div>
<div id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">In recent years, dependency parsing has gained universal interest due to its usefulness in a
wide range of applications such as synonym generation <cite class="ltx_cite">[<a href="#bib.bib121" title="Automatic paraphrase acquisition from news articles" class="ltx_ref">43</a>]</cite>, relation extraction <cite class="ltx_cite">[<a href="#bib.bib49" title="Convolution kernels on constituent, dependency and sequential structures for relation extraction" class="ltx_ref">37</a>]</cite>
and machine translation <cite class="ltx_cite">[<a href="#bib.bib47" title="Training a parser for machine translation reordering" class="ltx_ref">21</a>, <a href="#bib.bib48" title="A novel dependency-to-string model for statistical machine translation" class="ltx_ref">51</a>]</cite>. Several supervised dependency parsing algorithms <cite class="ltx_cite">[<a href="#bib.bib186" title="Deterministic dependency parsing of English text" class="ltx_ref">39</a>, <a href="#bib.bib176" title="Online large-margin training of dependency parsers" class="ltx_ref">30</a>, <a href="#bib.bib177" title="Non-projective dependency parsing using spanning tree algorithms" class="ltx_ref">32</a>, <a href="#bib.bib175" title="Online learning of approximate dependency parsing algorithms" class="ltx_ref">33</a>, <a href="#bib.bib80" title="Experiments with a higher-order projective dependency parser" class="ltx_ref">8</a>, <a href="#bib.bib148" title="Efficient third-order dependency parsers" class="ltx_ref">24</a>, <a href="#bib.bib439" title="Fourth-order dependency parsing" class="ltx_ref">27</a>, <a href="#bib.bib7" title="Online learning for inexact hypergraph search" class="ltx_ref">52</a>]</cite>
have been proposed and achieved high parsing accuracies on several treebanks, due in large part to the availability
of dependency treebanks in a number of languages <cite class="ltx_cite">[<a href="#bib.bib35" title="Universal dependency annotation for multilingual parsing" class="ltx_ref">31</a>]</cite>. However, the manually annotated treebanks that
these parsers rely on are highly expensive to create, in particular when we want to build treebanks for resource-poor languages.
This led to a vast amount of research on unsupervised grammar induction  <cite class="ltx_cite">[<a href="#bib.bib9" title="Two experiments on learning probabilistic dependency grammars from corpora" class="ltx_ref">9</a>, <a href="#bib.bib34" title="Corpus-based induction of syntactic structure: models of dependency and constituency" class="ltx_ref">22</a>, <a href="#bib.bib33" title="Contrastive estimation: training log-linear models on unlabeled data" class="ltx_ref">47</a>, <a href="#bib.bib32" title="Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction" class="ltx_ref">12</a>, <a href="#bib.bib29" title="From baby steps to leapfrog: how “less is more” in unsupervised dependency parsing" class="ltx_ref">48</a>, <a href="#bib.bib30" title="Unsupervised induction of tree substitution grammars for dependency parsing" class="ltx_ref">4</a>, <a href="#bib.bib8" title="Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing" class="ltx_ref">29</a>, <a href="#bib.bib28" title="Breaking out of local optima with count transforms and model recombination: a study in grammar induction" class="ltx_ref">49</a>]</cite>, which appears to be a natural solution to this problem, as unsupervised methods require only unannotated text for training parsers.
Unfortunately, the unsupervised grammar induction systems’ parsing accuracies often significantly fall behind those of supervised
systems <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>. Furthermore, from a practical standpoint, it is rarely the case that we are
completely devoid of resources for most languages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">In this paper, we consider a practically motivated scenario, in which we want to build statistical parsers for resource-poor target
languages, using existing resources from a resource-rich source language (like English).<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>For the sake of simplicity,
we refer to the resource-poor language as the “target language”, and resource-rich language as the “source language”. In addition,
in this study we use English as the source resource-rich language, but our methodology can be applied to any resource-rich languages.</span></span></span>
We assume that there are absolutely no labeled training data for the target language, but we have access to parallel data with a
resource-rich language and a sufficient amount of labeled training data to build an accurate parser for the resource-rich language.
This scenario appears similar to the setting in bilingual text parsing. However, most bilingual text parsing approaches require
bilingual treebanks — treebanks that have manually annotated tree structures on both sides of source and target languages <cite class="ltx_cite">[<a href="#bib.bib16" title="Bilingual parsing with factored estimation: using English to parse Korean" class="ltx_ref">45</a>, <a href="#bib.bib24" title="Two languages are better than one (for syntactic parsing)" class="ltx_ref">7</a>]</cite>, or have tree structures on the source side and translated
sentences in the target languages <cite class="ltx_cite">[<a href="#bib.bib26" title="Bilingually-constrained (monolingual) shift-reduce parsing" class="ltx_ref">18</a>, <a href="#bib.bib27" title="Bitext dependency parsing with bilingual subtree constraints" class="ltx_ref">10</a>]</cite>. Obviously, bilingual treebanks
are much more difficult to acquire than the resources required in our scenario, since the labeled training data and the parallel text
in our case are completely separated. What is more important is that most studies on bilingual text parsing assumed that the parser is
applied only on bilingual text. But our goal is to develop a parser that can be used in completely monolingual setting for each
target language of interest.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">This scenario is applicable to a large set of languages and many research studies <cite class="ltx_cite">[<a href="#bib.bib19" title="Bootstrapping parsers via syntactic projection across parallel texts" class="ltx_ref">19</a>]</cite> have
been made on it. Ganchev et al. <cite class="ltx_cite">[<a href="#bib.bib25" title="Dependency grammar induction via bitext projection constraints" class="ltx_ref">14</a>]</cite> presented a parser projection approach
via parallel text using the posterior regularization framework <cite class="ltx_cite">[<a href="#bib.bib22" title="Expectation maximization and posterior constraints" class="ltx_ref">16</a>]</cite>.
McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite> proposed two parser transfer approaches between
two different languages — one is directly transferred parser from delexicalized parsers, and the other parser is transferred using
constraint driven learning algorithm where constraints are drawn from parallel corpora. In that work, they demonstrate that
even the directly transferred delexicalized parser produces significantly higher accuracies than unsupervised parsers.
Cohen et al. <cite class="ltx_cite">[<a href="#bib.bib31" title="Unsupervised structure prediction with non-parallel multilingual guidance" class="ltx_ref">11</a>]</cite> proposed an approach for unsupervised dependency parsing with non-parallel
multilingual guidance from one or more helper languages, in which parallel data is not used.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">In this work, we propose a learning framework for transferring dependency grammars from a resource-rich language to resource-poor
languages via parallel text. We train probabilistic parsing models for resource-poor languages by maximizing a combination of
likelihood on parallel data and confidence on unlabeled data. Our work is based on the learning framework used in
Smith and Eisner <cite class="ltx_cite">[<a href="#bib.bib23" title="Bootstrapping feature-rich dependency parsers with entropic priors" class="ltx_ref">44</a>]</cite>, which is originally designed for parser bootstrapping.
We extend this learning framework so that it can be used to transfer cross-lingual knowledge between different languages.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">Throughout this paper, English is used as the source language and we evaluate our approach on ten target
languages — Danish (da), Dutch (nl), French (fr), German (de), Greek (el), Italian (it), Korean (ko), Portuguese (pt),
Spanish (es) and Swedish (sv).
Our approach achieves significant improvement over previous state-of-the-art unsupervised and projected parsing systems
across all the ten languages, and considerably bridges the gap to fully supervised dependency parsing performance.</p>
</div>
</div>
<div id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">2 </span>Our Approach</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">Dependency trees represent syntactic relationships through labeled directed edges between heads and their dependents.
For example, Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows a dependency tree for the sentence,
<em class="ltx_emph">Economic news had little effect on financial markets</em>, with the sentence’s root-symbol as its root.
The focus of this work is on building dependency parsers for target languages, assuming that an accurate English dependency parser
and some parallel text between the two languages are available. Central to our approach is a maximizing likelihood learning framework,
in which we use an English parser and parallel text to estimate the “transferring distribution” of the target language
parsing model (See Section <a href="#S2.SS2" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> for more details).
Another advantage of the learning framework is that it combines both the likelihood on parallel data and confidence on unlabeled data,
so that both parallel text and unlabeled data can be utilized in our approach.</p>
</div>
<div id="S2.F1" class="ltx_figure"><img src="P14-1126/image001.png" id="S2.F1.g1" class="ltx_graphics" width="338" height="63" alt=""/>
<div class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example dependency tree.</div>
</div>
<div id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.1 </span>Edge-Factored Parsing Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">In this paper, we will use the following notation: <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m1" class="ltx_Math" alttext="\boldsymbol{x}" display="inline"><mi>𝒙</mi></math> represents a generic input sentence,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m2" class="ltx_Math" alttext="\boldsymbol{y}" display="inline"><mi>𝒚</mi></math> represents a generic dependency tree. <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m3" class="ltx_Math" alttext="\mathrm{T}(\boldsymbol{x})" display="inline"><mrow><mi mathvariant="normal">T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> is used to denote the set of
possible dependency trees for sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m4" class="ltx_Math" alttext="\boldsymbol{x}" display="inline"><mi>𝒙</mi></math>. The probabilistic model for dependency parsing defines a family of
conditional probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m5" class="ltx_Math" alttext="p_{\lambda}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> over all <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m6" class="ltx_Math" alttext="\boldsymbol{y}" display="inline"><mi>𝒚</mi></math> given
sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m7" class="ltx_Math" alttext="\boldsymbol{x}" display="inline"><mi>𝒙</mi></math>, with a log-linear form:</p>
<table id="S2.E1" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E1.m1" class="ltx_Math" alttext="p_{\lambda}(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{Z(\boldsymbol{x})}\exp%&#10;\bigg\{\sum_{j}\lambda_{j}F_{j}(\boldsymbol{y},\boldsymbol{x})\bigg\}" display="block"><mrow><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></mfrac><mi>exp</mi><mrow><mo mathsize="2.0em" stretchy="false">{</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder><msub><mi>λ</mi><mi>j</mi></msub><msub><mi>F</mi><mi>j</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>,</mo><mi>𝒙</mi><mo>)</mo></mrow><mo mathsize="2.0em" stretchy="false">}</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(1)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m8" class="ltx_Math" alttext="F_{j}" display="inline"><msub><mi>F</mi><mi>j</mi></msub></math> are feature functions, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m9" class="ltx_Math" alttext="\mathbf{\lambda}=(\lambda_{1},\lambda_{2},\ldots)" display="inline"><mrow><mi>λ</mi><mo>=</mo><mrow><mo>(</mo><mrow><msub><mi>λ</mi><mn>1</mn></msub><mo>,</mo><msub><mi>λ</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi></mrow><mo>)</mo></mrow></mrow></math> are parameters of the model,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m10" class="ltx_Math" alttext="Z(\boldsymbol{x})" display="inline"><mrow><mi>Z</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> is a normalization factor, which is commonly referred to as the <em class="ltx_emph">partition function</em>:</p>
<table id="S2.E2" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E2.m1" class="ltx_Math" alttext="Z(\boldsymbol{x})=\sum_{\boldsymbol{y}\in\mathrm{T}(\boldsymbol{x})}\exp\bigg%&#10;\{\sum_{j}\lambda_{j}F_{j}(\boldsymbol{y},\boldsymbol{x})\bigg\}" display="block"><mrow><mrow><mi>Z</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>𝒚</mi><mo>∈</mo><mrow><mi mathvariant="normal">T</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></mrow></munder><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder><mrow><msub><mi>λ</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>F</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝒚</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(2)</span></td></tr>
</table>
<p class="ltx_p">A common strategy to make this parsing model efficiently computable is to <em class="ltx_emph">factor</em> dependency trees into sets of edges:</p>
<table id="S2.E3" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E3.m1" class="ltx_Math" alttext="F_{j}(\boldsymbol{y},\boldsymbol{x})=\sum_{e\in y}f_{j}(e,\boldsymbol{x})." display="block"><mrow><mrow><mrow><msub><mi>F</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝒚</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>e</mi><mo>∈</mo><mi>y</mi></mrow></munder><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>e</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(3)</span></td></tr>
</table>
<p class="ltx_p">That is, dependency tree <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m11" class="ltx_Math" alttext="y" display="inline"><mi>y</mi></math> is treated as a set of edges <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m12" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> and each feature function
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m13" class="ltx_Math" alttext="F_{j}(\boldsymbol{y},\boldsymbol{x})" display="inline"><mrow><msub><mi>F</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>𝒚</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></math> is equal to the sum of all the features <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p1.m14" class="ltx_Math" alttext="f_{j}(e,\boldsymbol{x})" display="inline"><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>e</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></math>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">We denote the <em class="ltx_emph">weight function</em> of each edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m1" class="ltx_Math" alttext="e" display="inline"><mi>e</mi></math> as follows:</p>
<table id="S2.E4" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E4.m1" class="ltx_Math" alttext="w(e,\boldsymbol{x})=\exp\bigg\{\sum_{j}\lambda_{j}f_{j}(e,\boldsymbol{x})\bigg\}" display="block"><mrow><mrow><mi>w</mi><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>e</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo>{</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>j</mi></munder><mrow><msub><mi>λ</mi><mi>j</mi></msub><mo>⁢</mo><msub><mi>f</mi><mi>j</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>e</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></mrow><mo>}</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(4)</span></td></tr>
</table>
<p class="ltx_p">and the conditional probability <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS1.p2.m2" class="ltx_Math" alttext="p_{\lambda}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> has the following form:</p>
<table id="S2.E5" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E5.m1" class="ltx_Math" alttext="p_{\lambda}(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{Z(\boldsymbol{x})}\prod%&#10;\limits_{e\in y}w(e,\boldsymbol{x})" display="block"><mrow><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Z</mi><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></mfrac><munder><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>e</mi><mo>∈</mo><mi>y</mi></mrow></munder><mi>w</mi><mrow><mo>(</mo><mi>e</mi><mo>,</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(5)</span></td></tr>
</table>
</div>
</div>
<div id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Training</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p">One of the most common model training methods for supervised dependency parser is Maximum conditional likelihood estimation.
For a supervised dependency parser with a set of training data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m1" class="ltx_Math" alttext="\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}" display="inline"><mrow><mo>{</mo><mrow><mo>(</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝒚</mi><mi>i</mi></msub></mrow><mo>)</mo></mrow><mo>}</mo></mrow></math>,
the logarithm of the likelihood (a.k.a. the log-likelihood) is given by:</p>
<table id="S2.E6" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E6.m1" class="ltx_Math" alttext="L(\mathbf{\lambda})=\sum\limits_{i}\log p_{\lambda}(\boldsymbol{y}_{i}|%&#10;\boldsymbol{x}_{i})" display="block"><mrow><mi>L</mi><mrow><mo>(</mo><mi>λ</mi><mo>)</mo></mrow><mo>=</mo><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(6)</span></td></tr>
</table>
<p class="ltx_p">Maximum likelihood training chooses parameters such that the log-likelihood <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p1.m2" class="ltx_Math" alttext="L(\mathbf{\lambda})" display="inline"><mrow><mi>L</mi><mo>⁢</mo><mrow><mo>(</mo><mi>λ</mi><mo>)</mo></mrow></mrow></math> is maximized.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p">However, in our scenario we have no labeled training data for target languages but we have some parallel and unlabeled data plus an
English dependency parser. For the purpose of transferring cross-lingual information from the English parser via parallel text,
we explore the model training method proposed by Smith and Eisner <cite class="ltx_cite">[<a href="#bib.bib23" title="Bootstrapping feature-rich dependency parsers with entropic priors" class="ltx_ref">44</a>]</cite>, which presented
a generalization of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m1" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> function <cite class="ltx_cite">[<a href="#bib.bib17" title="Understanding the Yarowsky algorithm" class="ltx_ref">1</a>]</cite>, and related it to another semi-supervised learning technique,
entropy regularization <cite class="ltx_cite">[<a href="#bib.bib20" title="Semi-supervised conditional random fields for improved sequence segmentation and labeling" class="ltx_ref">20</a>, <a href="#bib.bib21" title="Efficient computation of entropy gradient for semi-supervised conditional random fields" class="ltx_ref">28</a>]</cite>. The objective <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m2" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> function to be minimized is actually the <em class="ltx_emph">expected</em>
negative log-likelihood:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E7" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m1" class="ltx_Math" alttext="\displaystyle K" display="inline"><mi>K</mi></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex1.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{i}\sum_{\boldsymbol{y}_{i}}\tilde{p}(\boldsymbol{y}%&#10;_{i}|\boldsymbol{x}_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder></mstyle><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>𝒚</mi><mi>i</mi></msub></munder></mstyle><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="2" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(7)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E7.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{i}D(\tilde{p}_{i}||p_{\lambda,i})+H(\tilde{p}_{i})" display="inline"><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>i</mi></munder></mstyle><mi>D</mi><mrow><mo>(</mo><msub><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mi>i</mi></msub><mo>|</mo><mo>|</mo><msub><mi>p</mi><mrow><mi>λ</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow><mo>+</mo><mi>H</mi><mrow><mo>(</mo><msub><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m3" class="ltx_Math" alttext="\tilde{p}_{i}(\cdot)\stackrel{def}{=}\tilde{p}(\cdot|\boldsymbol{x}_{i})" display="inline"><mrow><msub><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mi>i</mi></msub><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><mover><mo movablelimits="false">=</mo><mrow><mi>d</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>f</mi></mrow></mover><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mo>⋅</mo><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math> and
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m4" class="ltx_Math" alttext="p_{\lambda,i}(\cdot)\stackrel{def}{=}p_{\lambda}(\cdot|\boldsymbol{x}_{i})" display="inline"><mrow><msub><mi>p</mi><mrow><mi>λ</mi><mo>,</mo><mi>i</mi></mrow></msub><mrow><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><mover><mo movablelimits="false">=</mo><mrow><mi>d</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>f</mi></mrow></mover><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mo>⋅</mo><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math>.
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m5" class="ltx_Math" alttext="\tilde{p}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> is the “transferring distribution” that reflects our uncertainty
about the true labels, and we are trying to learn a parametric model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m6" class="ltx_Math" alttext="p_{\lambda}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> by
minimizing the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p2.m7" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> function.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p class="ltx_p">In our scenario, we have a set of aligned parallel data <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m1" class="ltx_Math" alttext="P=\{\boldsymbol{x}^{s}_{i},\boldsymbol{x}^{t}_{i},a_{i}\}" display="inline"><mrow><mi>P</mi><mo>=</mo><mrow><mo>{</mo><mrow><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup><mo>,</mo><msub><mi>a</mi><mi>i</mi></msub></mrow><mo>}</mo></mrow></mrow></math> where
<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m2" class="ltx_Math" alttext="a_{i}" display="inline"><msub><mi>a</mi><mi>i</mi></msub></math> is the word alignment for the pair of source-target sentences <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m3" class="ltx_Math" alttext="(\boldsymbol{x}^{s}_{i},\boldsymbol{x}^{t}_{i})" display="inline"><mrow><mo>(</mo><mrow><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>)</mo></mrow></math>,
and a set of unlabeled sentences of the target language <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m4" class="ltx_Math" alttext="U=\{\boldsymbol{x}^{t}_{i}\}" display="inline"><mrow><mi>U</mi><mo>=</mo><mrow><mo>{</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup><mo>}</mo></mrow></mrow></math>. We also have a trained English
parsing model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m5" class="ltx_Math" alttext="p_{\lambda_{E}}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><msub><mi>λ</mi><mi>E</mi></msub></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>. Then the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m6" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> in equation (<a href="#S2.E7" title="(7) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) can be divided
into two cases, according to whether <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m7" class="ltx_Math" alttext="\boldsymbol{x}_{i}" display="inline"><msub><mi>𝒙</mi><mi>i</mi></msub></math> belongs to parallel data set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m8" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> or unlabeled data set <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m9" class="ltx_Math" alttext="U" display="inline"><mi>U</mi></math>.
For the unlabeled examples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m10" class="ltx_Math" alttext="\{\boldsymbol{x}_{i}\in U\}" display="inline"><mrow><mo>{</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><mi>U</mi></mrow><mo>}</mo></mrow></math>, some previous studies (e.g., <cite class="ltx_cite">[<a href="#bib.bib17" title="Understanding the Yarowsky algorithm" class="ltx_ref">1</a>]</cite>) simply use a
uniform distribution over labels (e.g., parses), to reflect that the label is unknown. We follow the method in
Smith and Eisner <cite class="ltx_cite">[<a href="#bib.bib23" title="Bootstrapping feature-rich dependency parsers with entropic priors" class="ltx_ref">44</a>]</cite> and take the transferring distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m11" class="ltx_Math" alttext="\tilde{p}_{i}" display="inline"><msub><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mi>i</mi></msub></math> to be
the <em class="ltx_emph">actual</em> current belief <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m12" class="ltx_Math" alttext="p_{\lambda,i}" display="inline"><msub><mi>p</mi><mrow><mi>λ</mi><mo>,</mo><mi>i</mi></mrow></msub></math>. The total contribution of the <em class="ltx_emph">unsupervised</em> examples to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m13" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math> then
simplifies to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m14" class="ltx_Math" alttext="K_{U}=\sum\limits_{\boldsymbol{x}_{i}\in U}H(p_{\lambda,i})" display="inline"><mrow><msub><mi>K</mi><mi>U</mi></msub><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><mi>U</mi></mrow></munder><mrow><mi>H</mi><mo>⁢</mo><mrow><mo>(</mo><msub><mi>p</mi><mrow><mi>λ</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></math>, which may be regarded as the entropy item used
to constrain the model’s uncertainty <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p3.m15" class="ltx_Math" alttext="H" display="inline"><mi>H</mi></math> to be low, as presented in the work on
entropy regularization <cite class="ltx_cite">[<a href="#bib.bib20" title="Semi-supervised conditional random fields for improved sequence segmentation and labeling" class="ltx_ref">20</a>, <a href="#bib.bib21" title="Efficient computation of entropy gradient for semi-supervised conditional random fields" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p class="ltx_p">But how can we define the transferring distribution for the parallel examples <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m1" class="ltx_Math" alttext="\{\boldsymbol{x}^{t}_{i}\in P\}" display="inline"><mrow><mo>{</mo><mrow><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup><mo>∈</mo><mi>P</mi></mrow><mo>}</mo></mrow></math>?
We define the transferring distribution by defining the <em class="ltx_emph">transferring weight</em> utilizing the
English parsing model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m2" class="ltx_Math" alttext="p_{\lambda_{E}}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><msub><mi>λ</mi><mi>E</mi></msub></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> via parallel data with word alignments:</p>
<table id="S2.E8" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E8.m1" class="ltx_Math" alttext="\tilde{w}(e^{t},\boldsymbol{x}^{t}_{i})=\left\{\begin{array}[]{ll}w_{E}(e^{s},%&#10;\boldsymbol{x}^{s}_{i}),&amp;\textrm{if }e^{t}\stackrel{align}{\longrightarrow}e^{%&#10;s}\\&#10;w_{E}(e^{t}_{delex},\boldsymbol{x}^{s}_{i}),&amp;\textrm{otherwise}\end{array}\right." display="block"><mrow><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>e</mi><mi>t</mi></msup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>w</mi><mi>E</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>e</mi><mi>s</mi></msup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if </mtext><mo>⁢</mo><msup><mi>e</mi><mi>t</mi></msup></mrow><mover><mo movablelimits="false">⟶</mo><mrow><mi>a</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>n</mi></mrow></mover><msup><mi>e</mi><mi>s</mi></msup></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>w</mi><mi>E</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi>e</mi><mrow><mi>d</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>x</mi></mrow><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(8)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m3" class="ltx_Math" alttext="w_{E}(\cdot,\cdot)" display="inline"><mrow><msub><mi>w</mi><mi>E</mi></msub><mrow><mo>(</mo><mo>⋅</mo><mo>,</mo><mo>⋅</mo><mo>)</mo></mrow></mrow></math> is the weight function of the English parsing model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m4" class="ltx_Math" alttext="p_{\lambda_{E}}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><msub><mi>λ</mi><mi>E</mi></msub></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>,
and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m5" class="ltx_Math" alttext="e^{t}_{delex}" display="inline"><msubsup><mi>e</mi><mrow><mi>d</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>x</mi></mrow><mi>t</mi></msubsup></math> is the delexicalized form<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>The delexicalized form of an edge is an edge for which only delexicalized features
are considered.</span></span></span> of the edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m6" class="ltx_Math" alttext="e^{t}" display="inline"><msup><mi>e</mi><mi>t</mi></msup></math>.
From the definition of the transferring weight, we can see that, if an edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m7" class="ltx_Math" alttext="e^{t}" display="inline"><msup><mi>e</mi><mi>t</mi></msup></math> of the target language sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m8" class="ltx_Math" alttext="\boldsymbol{x}^{t}_{i}" display="inline"><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup></math>
is aligned to an edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m9" class="ltx_Math" alttext="e^{s}" display="inline"><msup><mi>e</mi><mi>s</mi></msup></math> of the English sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m10" class="ltx_Math" alttext="\boldsymbol{x}^{s}_{i}" display="inline"><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></math>, we transfer the weight of edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m11" class="ltx_Math" alttext="e^{t}" display="inline"><msup><mi>e</mi><mi>t</mi></msup></math> to the
corresponding weight of edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m12" class="ltx_Math" alttext="e^{s}" display="inline"><msup><mi>e</mi><mi>s</mi></msup></math> in the English parsing model <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m13" class="ltx_Math" alttext="p_{\lambda_{E}}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><msub><mi>λ</mi><mi>E</mi></msub></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>.
If the edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m14" class="ltx_Math" alttext="e^{t}" display="inline"><msup><mi>e</mi><mi>t</mi></msup></math> is not aligned to any edges of the English sentence <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m15" class="ltx_Math" alttext="\boldsymbol{x}^{s}_{i}" display="inline"><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></math>, we reduce the edge <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p4.m16" class="ltx_Math" alttext="e^{t}" display="inline"><msup><mi>e</mi><mi>t</mi></msup></math> to
the delexicalized form and calculate the transferring weight in the English parsing model. There are two advantages for this
definition of the transferring weight. First, by transferring the weight function to the corresponding weight in the well-developed
English parsing model, we can project syntactic information across language boundaries.
Second, McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite> demonstrates that parsers with
only delexicalized features produce considerably high parsing performance. By reducing unaligned edges to their delexicalized forms,
we can still use those delexicalized features, such as part-of-speech tags, for those unaligned edges, and can address problem that
automatically generated word alignments include errors.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p class="ltx_p">From the definition of transferring weight in equation (<a href="#S2.E8" title="(8) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), the transferring distribution can be defined in the following way:</p>
<table id="S2.E9" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E9.m1" class="ltx_Math" alttext="\tilde{p}(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{\tilde{Z}(\boldsymbol{x})}%&#10;\prod\limits_{e\in y}\tilde{w}(e,\boldsymbol{x})" display="block"><mrow><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mover accent="true"><mi>Z</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></mfrac><munder><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>e</mi><mo>∈</mo><mi>y</mi></mrow></munder><mover accent="true"><mi>w</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mi>e</mi><mo>,</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(9)</span></td></tr>
</table>
<p class="ltx_p">where</p>
<table id="S2.E10" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E10.m1" class="ltx_Math" alttext="\tilde{Z}(\boldsymbol{x})=\sum\limits_{y}\prod\limits_{e\in y}\tilde{w}(e,%&#10;\boldsymbol{x})" display="block"><mrow><mrow><mover accent="true"><mi>Z</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>y</mi></munder><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∏</mo><mrow><mi>e</mi><mo>∈</mo><mi>y</mi></mrow></munder><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mrow><mi>e</mi><mo>,</mo><mi>𝒙</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(10)</span></td></tr>
</table>
<p class="ltx_p">Due to the normalizing factor <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p5.m1" class="ltx_Math" alttext="\tilde{Z}(\boldsymbol{x})" display="inline"><mrow><mover accent="true"><mi>Z</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>, the transferring distribution is a valid one.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p class="ltx_p">We introduce a multiplier <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m1" class="ltx_Math" alttext="\gamma" display="inline"><mi>γ</mi></math> as a trade-off between the two contributions (parallel and unsupervised) of the objective function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m2" class="ltx_Math" alttext="K" display="inline"><mi>K</mi></math>,
and the final objective function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m3" class="ltx_Math" alttext="K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math> has the following form:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E11" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m1" class="ltx_Math" alttext="\displaystyle K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex2.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{x_{i}\in P}\sum\limits_{y_{i}}\tilde{p}(\boldsymbol%&#10;{y}_{i}|\boldsymbol{x}_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_%&#10;{i})" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>P</mi></mrow></munder></mstyle><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="3" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(11)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex3.m3" class="ltx_Math" alttext="\displaystyle+\quad\gamma\sum\limits_{x_{i}\in U}H(p_{\lambda,i})" display="inline"><mrow><mo>+</mo><mo mathvariant="italic" separator="true"> </mo><mi>γ</mi><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>U</mi></mrow></munder></mstyle><mi>H</mi><mrow><mo>(</mo><msub><mi>p</mi><mrow><mi>λ</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E11.m3" class="ltx_Math" alttext="\displaystyle K_{P}+\gamma K_{U}" display="inline"><mrow><msub><mi>K</mi><mi>P</mi></msub><mo>+</mo><mrow><mi>γ</mi><mo>⁢</mo><msub><mi>K</mi><mi>U</mi></msub></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m4" class="ltx_Math" alttext="K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math> and <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m5" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math> are the contributions of the parallel and unsupervised data, respectively.
One may regard <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS2.p6.m6" class="ltx_Math" alttext="\gamma" display="inline"><mi>γ</mi></math> as a Lagrange multiplier that is used to constrain the parser’s uncertainty H to be low,
as presented in several studies on entropy regularization <cite class="ltx_cite">[<a href="#bib.bib4" title="Structure learning in conditional probability models via an entropic prior and parameter extinction" class="ltx_ref">5</a>, <a href="#bib.bib5" title="Semi-supervised learning by entropy minimization" class="ltx_ref">17</a>, <a href="#bib.bib20" title="Semi-supervised conditional random fields for improved sequence segmentation and labeling" class="ltx_ref">20</a>]</cite>.</p>
</div>
</div>
<div id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.3 </span>Algorithms and Complexity for Model Training</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p">To train our parsing model, we need to find out the parameters <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m1" class="ltx_Math" alttext="\lambda" display="inline"><mi>λ</mi></math> that minimize the objective function <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p1.m2" class="ltx_Math" alttext="K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math> in equation (<a href="#S2.E11" title="(11) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>).
This optimization problem is typically solved using quasi-Newton numerical methods such as L-BFGS <cite class="ltx_cite">[<a href="#bib.bib54" title="A numerical study of the limited memory bfgs method and truncated-newton method for large scale optimization" class="ltx_ref">36</a>]</cite>,
which requires efficient calculation of the objective function and the gradient of the objective function.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p">The first item (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m1" class="ltx_Math" alttext="K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math>) of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m2" class="ltx_Math" alttext="K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math> function in equation (<a href="#S2.E11" title="(11) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) can be rewritten in the following form:</p>
<table id="Sx1.EGx3" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E12" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m1" class="ltx_Math" alttext="\displaystyle K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex4.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{\boldsymbol{x}_{i}\in P}\big[\sum\limits_{y_{i}}%&#10;\tilde{p}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})\sum\limits_{e\in\boldsymbol{y%&#10;}_{i}}\log w(e,\boldsymbol{x}_{i})" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><mi>P</mi></mrow></munder></mstyle><mrow><mo mathsize="1.1em" stretchy="false">[</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>e</mi><mo>∈</mo><msub><mi>𝒚</mi><mi>i</mi></msub></mrow></munder></mstyle><mi>log</mi><mi>w</mi><mrow><mo>(</mo><mi>e</mi><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="2" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(12)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E12.m3" class="ltx_Math" alttext="\displaystyle-\quad\log Z(\boldsymbol{x}_{i})\big]" display="inline"><mrow><mo>-</mo><mo mathvariant="italic" separator="true"> </mo><mi>log</mi><mi>Z</mi><mrow><mo>(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mo mathsize="1.1em" stretchy="false">]</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">and according to equation (<a href="#S2.E1" title="(1) ‣ 2.1 Edge-Factored Parsing Model ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and (<a href="#S2.E3" title="(3) ‣ 2.1 Edge-Factored Parsing Model ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) the gradient of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p2.m3" class="ltx_Math" alttext="K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math> can be written as:</p>
<table id="Sx1.EGx4" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E13" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m1" class="ltx_Math" alttext="\displaystyle\frac{\partial K_{P}}{\partial\lambda_{j}}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>K</mi><mi>P</mi></msub></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mstyle></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex5.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{x_{i}\in P}\frac{\partial\tilde{p}(\boldsymbol{y}_{i%&#10;}|\boldsymbol{x}_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})}{%&#10;\partial\lambda_{j}}" display="inline"><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>P</mi></mrow></munder></mstyle><mstyle displaystyle="true"><mfrac><mrow><mo>∂</mo><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="3" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(13)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex6.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex6.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{x_{i}\in P}\bigg[\sum\limits_{y_{i}}\tilde{p}(%&#10;\boldsymbol{y}_{i}|\boldsymbol{x}_{i})\sum\limits_{e\in y_{i}}f_{j}(e,%&#10;\boldsymbol{x}_{i})" display="inline"><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>P</mi></mrow></munder></mstyle><mrow><mo mathsize="2.0em" stretchy="false">[</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>e</mi><mo>∈</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder></mstyle><msub><mi>f</mi><mi>j</mi></msub><mrow><mo>(</mo><mi>e</mi><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E13.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{y_{i}}p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}%&#10;_{i})\sum\limits_{e\in y_{i}}f_{j}(e,\boldsymbol{x}_{i})\bigg]" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>e</mi><mo>∈</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></munder></mstyle><msub><mi>f</mi><mi>j</mi></msub><mrow><mo>(</mo><mi>e</mi><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mo mathsize="2.0em" stretchy="false">]</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p">According to equation (<a href="#S2.E9" title="(9) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>), <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m1" class="ltx_Math" alttext="\tilde{p}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math> can also be factored into the multiplication of
the weight of each edge, so both <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m2" class="ltx_Math" alttext="K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math> and its gradient can be calculated by running the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m3" class="ltx_Math" alttext="O(n^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow></mrow></math> inside-outside
algorithm <cite class="ltx_cite">[<a href="#bib.bib52" title="Trainable grammars for speech recognition" class="ltx_ref">2</a>, <a href="#bib.bib53" title="Cubic-time parsing and learning algorithms for grammatical bigram models" class="ltx_ref">41</a>]</cite> for projective parsing. For non-projective parsing, the analogy to the inside algorithm
is the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m4" class="ltx_Math" alttext="O(n^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow></mrow></math> matrix-tree algorithm based on Kirchhoff’s Matrix-Tree Theorem, which is dominated asymptotically by a
matrix determinant <cite class="ltx_cite">[<a href="#bib.bib146" title="Structured predicition models via the matrix-tree theorem" class="ltx_ref">25</a>, <a href="#bib.bib147" title="Probabilistic models of nonporjective dependency trees" class="ltx_ref">46</a>]</cite>. The gradient of a determinant may be computed by matrix inversion,
so evaluating the gradient again has the same <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p3.m5" class="ltx_Math" alttext="O(n^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow></mrow></math> complexity as evaluating the function.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p class="ltx_p">The second item (<math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m1" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math>) of the <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m2" class="ltx_Math" alttext="K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math> function in equation (<a href="#S2.E11" title="(11) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>) is the Shannon entropy of the posterior distribution over
parsing trees, and can be written into the following form:</p>
<table id="Sx1.EGx5" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E14" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m1" class="ltx_Math" alttext="\displaystyle K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex7.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{\boldsymbol{x}_{i}\in U}\big[\sum\limits_{y_{i}}p_{%&#10;\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})\sum\limits_{e\in\boldsymbol{y}%&#10;_{i}}\log w(e,\boldsymbol{x}_{i})" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>𝒙</mi><mi>i</mi></msub><mo>∈</mo><mi>U</mi></mrow></munder></mstyle><mrow><mo mathsize="1.1em" stretchy="false">[</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><mi>e</mi><mo>∈</mo><msub><mi>𝒚</mi><mi>i</mi></msub></mrow></munder></mstyle><mi>log</mi><mi>w</mi><mrow><mo>(</mo><mi>e</mi><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="2" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(14)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"/>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E14.m3" class="ltx_Math" alttext="\displaystyle-\quad\log Z(\boldsymbol{x}_{i})\big]" display="inline"><mrow><mo>-</mo><mo mathvariant="italic" separator="true"> </mo><mi>log</mi><mi>Z</mi><mrow><mo>(</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mo mathsize="1.1em" stretchy="false">]</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">and the gradient of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m3" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math> is in the following:</p>
<table id="Sx1.EGx6" class="ltx_equationgroup ltx_eqn_eqnarray">

<tr id="S2.E15" class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m1" class="ltx_Math" alttext="\displaystyle\frac{\partial K_{U}}{\partial\lambda_{j}}" display="inline"><mstyle displaystyle="true"><mfrac><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>K</mi><mi>U</mi></msub></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mstyle></math></td>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex8.m3" class="ltx_Math" alttext="\displaystyle\sum\limits_{x_{i}\in U}\frac{\partial p_{\lambda}(\boldsymbol{y}%&#10;_{i}|\boldsymbol{x}_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i}%&#10;)}{\partial\lambda_{j}}" display="inline"><mrow><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>U</mi></mrow></munder></mstyle><mstyle displaystyle="true"><mfrac><mrow><mo>∂</mo><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow><mrow><mo>∂</mo><mo>⁡</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mstyle></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="4" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(15)</span></td></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m2" class="ltx_Math" alttext="\displaystyle=" display="inline"><mo>=</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex9.m3" class="ltx_Math" alttext="\displaystyle-\sum\limits_{y_{i}}p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}%&#10;_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})F_{j}(\boldsymbol{%&#10;y}_{i},\boldsymbol{x}_{i})" display="inline"><mrow><mo>-</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><msub><mi>F</mi><mi>j</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m2" class="ltx_Math" alttext="\displaystyle+" display="inline"><mo>+</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.Ex10.m3" class="ltx_Math" alttext="\displaystyle\bigg(\sum\limits_{y_{i}}p_{\lambda}(\boldsymbol{y}_{i}|%&#10;\boldsymbol{x}_{i})\log p_{\lambda}(\boldsymbol{y}_{i}|\boldsymbol{x}_{i})\bigg)" display="inline"><mrow><mo mathsize="2.0em" stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mi>log</mi><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mo mathsize="2.0em" stretchy="false">)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
<tr class="ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_td ltx_align_right"/>
<td class="ltx_td ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E15.m2" class="ltx_Math" alttext="\displaystyle\cdot" display="inline"><mo>⋅</mo></math></td>
<td class="ltx_td ltx_align_left"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.E15.m3" class="ltx_Math" alttext="\displaystyle\bigg(\sum\limits_{y_{i}}p_{\lambda}(\boldsymbol{y}_{i}|%&#10;\boldsymbol{x}_{i})F_{j}(\boldsymbol{y}_{i},\boldsymbol{x}_{i})\bigg)" display="inline"><mrow><mo mathsize="2.0em" stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><msub><mi>y</mi><mi>i</mi></msub></munder></mstyle><msub><mi>p</mi><mi>λ</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>|</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><msub><mi>F</mi><mi>j</mi></msub><mrow><mo>(</mo><msub><mi>𝒚</mi><mi>i</mi></msub><mo>,</mo><msub><mi>𝒙</mi><mi>i</mi></msub><mo>)</mo></mrow><mo mathsize="2.0em" stretchy="false">)</mo></mrow></math></td>
<td class="ltx_eqn_center_padright"/></tr>
</table>
<p class="ltx_p">Similar with the calculation of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m4" class="ltx_Math" alttext="K_{P}" display="inline"><msub><mi>K</mi><mi>P</mi></msub></math>, <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m5" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math> can also be computed by running the inside-outside
algorithm <cite class="ltx_cite">[<a href="#bib.bib52" title="Trainable grammars for speech recognition" class="ltx_ref">2</a>, <a href="#bib.bib53" title="Cubic-time parsing and learning algorithms for grammatical bigram models" class="ltx_ref">41</a>]</cite> for projective parsing.
For the gradient of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m6" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math>, both the two multipliers of the second item in equation (<a href="#S2.E15" title="(15) ‣ 2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>) can be computed using the same
inside-outside algorithm. For the first item in equation (<a href="#S2.E15" title="(15) ‣ 2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>), an <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m7" class="ltx_Math" alttext="O(n^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow></mrow></math> dynamic programming algorithm that
is closely related to the forward-backward algorithm <cite class="ltx_cite">[<a href="#bib.bib21" title="Efficient computation of entropy gradient for semi-supervised conditional random fields" class="ltx_ref">28</a>]</cite> for the entropy regularized CRF <cite class="ltx_cite">[<a href="#bib.bib20" title="Semi-supervised conditional random fields for improved sequence segmentation and labeling" class="ltx_ref">20</a>]</cite>
can be used for projective parsing. For non-projective parsing, however, the runtime rises to <math xmlns="http://www.w3.org/1998/Math/MathML" id="S2.SS3.p4.m8" class="ltx_Math" alttext="O(n^{4})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>4</mn></msup><mo>)</mo></mrow></mrow></math>. In this paper, we focus on
projective parsing.</p>
</div>
</div>
<div id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">2.4 </span>Summary of Our Approach</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p">To summarize the description in the previous sections, our approach is performed in the following steps:</p>
<ol id="I1" class="ltx_enumerate">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">1.</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">Train an English parsing model <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i1.p1.m1" class="ltx_Math" alttext="p_{\lambda_{E}}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><msub><mi>p</mi><msub><mi>λ</mi><mi>E</mi></msub></msub><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>, which is used to estimate
the transferring distribution <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i1.p1.m2" class="ltx_Math" alttext="\tilde{p}(\boldsymbol{y}|\boldsymbol{x})" display="inline"><mrow><mover accent="true"><mi>p</mi><mo stretchy="false">~</mo></mover><mrow><mo>(</mo><mi>𝒚</mi><mo>|</mo><mi>𝒙</mi><mo>)</mo></mrow></mrow></math>.</p>
</div></li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">2.</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">Prepare parallel text by running word alignment method to obtain word alignments,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>The word alignment methods
do not require additional resources besides parallel text.</span></span></span> and prepare the unlabeled data.</p>
</div></li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_enumerate">3.</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">Train a parsing model for the target language by minimizing the objective <math xmlns="http://www.w3.org/1998/Math/MathML" id="I1.i3.p1.m1" class="ltx_Math" alttext="K^{{}^{\prime}}" display="inline"><msup><mi>K</mi><msup><mi/><mo>′</mo></msup></msup></math> function which is the
combination of expected negative log-likelihood on parallel and unlabeled data.</p>
</div></li>
</ol>
</div>
<div id="S2.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span class="ltx_text ltx_font_small">#sents/#tokens</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">training</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">dev</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">test</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span class="ltx_text ltx_font_small">Version 1.0</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">de</span></th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">2,200/30,460</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">800/12,215</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">1,000/16,339</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">es</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">3,345/94,232</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">370/10,191</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">300/8,295</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">fr</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">3,312/74,979</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">366/8,071</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">300/6,950</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">ko</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">5,308/62,378</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">588/6,545</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">298/2,917</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">sv</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">4,447/66,631</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">493/9,312</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">1,219/20,376</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span class="ltx_text ltx_font_small">Version 2.0</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">de</span></th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">14,118/26,4906</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">800/12,215</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_small">1,000/16,339</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">es</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">14,138/37,5180</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">1,569/40,950</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">300/8,295</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">fr</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">14,511/35,1233</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">1,611/38,328</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">300/6,950</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">it</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">6,389/14,9145</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">400/9,541</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">400/9,187</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">ko</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">5437/60,621</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">603/6,438</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">299/2,631</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">pt</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">9,600/23,9012</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">1,200/29,873</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_small">1,198/29,438</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_small">sv</span></th>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_small">4,447/66,631</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_small">493/9,312</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_small">1,219/20,376</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data statistics of two versions of Google Universal Treebanks for the target languages.</div>
</div>
</div>
</div>
<div id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">3 </span>Data and Tools</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">In this section, we illustrate the data sets used in our experiments and the tools for data preparation.</p>
</div>
<div id="S3.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<th class="ltx_td ltx_align_center ltx_border_t" colspan="6"><span class="ltx_text ltx_font_script"># sents</span></th></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">500</span></th>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">1000</span></th>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">2000</span></th>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">5000</span></th>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">10000</span></th>
<th class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">20000</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">da</span></th>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">12,568</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">25,225</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">49,889</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">126,623</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">254,565</span></td>
<td class="ltx_td ltx_align_right ltx_border_t"><span class="ltx_text ltx_font_script">509,480</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">de</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">13,548</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">26,663</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">53,170</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">133,596</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">265,589</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">527,407</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">el</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">14,198</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">28,302</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">56,744</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">143,753</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">286,126</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">572,777</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">es</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">15,147</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">29,214</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">57,526</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">144,621</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">290,517</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">579,164</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">fr</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">15,046</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">29,982</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">60,569</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">153,874</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">306,332</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">609,541</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">it</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">15,151</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">29,786</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">57,696</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">145,717</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">288,337</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">573,557</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">ko</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">3,814</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">7,679</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">15,337</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">38,535</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">77,388</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">155,051</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">nl</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">13,234</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">26,777</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">54,570</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">137,277</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">274,692</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">551,463</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">pt</span></th>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">14,346</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">28,109</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">55,998</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">143,221</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">285,590</span></td>
<td class="ltx_td ltx_align_right"><span class="ltx_text ltx_font_script">571,109</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_script">sv</span></th>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">12,242</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">24,897</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">50,047</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">123,069</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">246,619</span></td>
<td class="ltx_td ltx_align_right ltx_border_b"><span class="ltx_text ltx_font_script">490,086</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The number of tokens in parallel data used in our
experiments. For all these corpora, the other language is English.</div>
</div>
<div id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.1 </span>Choosing Target Languages</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p">Our experiments rely on two kinds of data sets: (i) Monolingual Treebanks with consistent annotation schema — English treebank is used
to train the English parsing model, and the Treebanks for target languages are used to evaluate the parsing performance of our approach.
(ii) Large amounts of parallel text with English on one side. We select target languages based on the availability of these resources.
The monolingual treebanks in our experiments are from the Google Universal Dependency Treebanks <cite class="ltx_cite">[<a href="#bib.bib35" title="Universal dependency annotation for multilingual parsing" class="ltx_ref">31</a>]</cite>, for
the reason that the treebanks of different languages in Google Universal Dependency Treebanks have consistent syntactic representations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p">The parallel data come from the Europarl corpus version 7 <cite class="ltx_cite">[<a href="#bib.bib18" title="Europarl: A Parallel Corpus for Statistical Machine Translation" class="ltx_ref">23</a>]</cite> and Kaist Corpus<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="http://semanticweb.kaist.ac.kr/home/index.php/Corpus10" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">http://semanticweb.kaist.ac.kr/home/index.php/Corpus10</span></a></span></span></span>. Taking the intersection of languages in
the two kinds of resources yields the following seven languages: French, German, Italian, Korean, Portuguese, Spanish and
Swedish.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p class="ltx_p">The treebanks from CoNLL shared-tasks on dependency parsing <cite class="ltx_cite">[<a href="#bib.bib417" title="CoNLL-X shared task on multilingual dependency parsing" class="ltx_ref">6</a>, <a href="#bib.bib418" title="The conll 2007 shared task on dependency parsing" class="ltx_ref">38</a>]</cite> appear to be another reasonable choice.
However, previous studies <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>, <a href="#bib.bib35" title="Universal dependency annotation for multilingual parsing" class="ltx_ref">31</a>]</cite> have demonstrated that a homogeneous
representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream
components, and the heterogenous representations used in CoNLL shared-tasks treebanks weaken any conclusion that can be
drawn.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p class="ltx_p">For comparison with previous studies, nevertheless, we also run experiments on
CoNLL treebanks (see Section <a href="#S4.SS4" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> for more details). We evaluate our approach on three target
languages from CoNLL shared task treebanks, which do not appear in Google Universal Treebanks.
The three languages are Danish, Dutch and Greek. So totally we have ten target languages. The parallel data for these three languages
are also from the Europarl corpus version 7.</p>
</div>
</div>
<div id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.2 </span>Word Alignments</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p">In our approach, word alignments for the parallel text are required. We perform word alignments with the open source
GIZA++ toolkit<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><a href="https://code.google.com/p/giza-pp/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://code.google.com/p/giza-pp/</span></a></span></span></span>. The parallel corpus was preprocessed in standard ways, selecting
sentences with the length in the range from 3 to 100. Then we run GIZA++ with the default setting to generate word alignments in both
directions. We then make the intersection of the word alignments of two directions to generate one-to-one alignments.</p>
</div>
<div id="S3.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">DTP</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">DTP†</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">PTP†</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">-U</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">+U</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">OR</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">de</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">58.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">58.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">69.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">73.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">74.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">78.64</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">es</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">68.07</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">68.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">72.57</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">75.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">75.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">82.56</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">fr</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">70.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">71.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">74.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">76.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">76.93</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">83.69</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">ko</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">42.37</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">43.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">53.72</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">59.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">59.94</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">89.85</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">sv</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">70.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">70.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">75.87</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">78.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">79.27</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">85.59</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Ave</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">61.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">62.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">69.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">72.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">73.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">84.67</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>UAS for two versions of our approach, together with baseline and oracle systems on Google Universal Treebanks version 1.0.
“Ave” is the macro-average across the five languages.</div>
</div>
</div>
<div id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">3.3 </span>Part-of-Speech Tagging</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p">Several features in our parsing model involve part-of-speech (POS) tags of the input sentences. The set of POS tags needs to be consistent
across languages and treebanks. For this reason we use the universal POS tag set of Petrov et al. <cite class="ltx_cite">[<a href="#bib.bib11" title="A universal part-of-speech tagset" class="ltx_ref">42</a>]</cite>.
This set consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns),
DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and
X (a catch-all for other categories such as abbreviations or foreign words).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p">POS tags are not available for parallel data in the Europarl and Kaist corpus, so we need to provide the POS tags for these data.
In our experiments, we train a Stanford POS Tagger <cite class="ltx_cite">[<a href="#bib.bib14" title="Feature-rich part-of-speech tagging with a cyclic dependency network" class="ltx_ref">50</a>]</cite> for each language.
The labeled training data for each POS tagger are extracted from the training portion of each Treebanks.
The average tagging accuracy is around 95%.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p class="ltx_p">Undoubtedly, we are primarily interested in applying our approach to build statistical parsers for resource-poor target languages
without any knowledge. For the purpose of evaluation of our approach and comparison with previous work, we need to exploit the gold
POS tags to train the POS taggers. As part-of-speech tags are also a form of syntactic analysis, this assumption weakens the applicability
of our approach. Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and
Petrov <cite class="ltx_cite">[<a href="#bib.bib15" title="Unsupervised part-of-speech tagging with bilingual graph-based projections" class="ltx_ref">13</a>]</cite>, rely only on labeled training data for English and the same kind of parallel
text in our approach. In practice we can use this kind of POS taggers to predict POS tags, whose tagging accuracy is around 85%.</p>
</div>
<div id="S3.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">DTP†</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">PTP†</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">-U</span></th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">+U</span></th>
<th class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">OR</span></th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">de</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">58.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">69.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">73.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">74.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_small">81.65</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">es</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">68.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">73.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">75.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">75.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">83.92</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">fr</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">71.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">74.75</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">76.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">76.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">83.51</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">it</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">70.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">76.08</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">77.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">77.74</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">85.47</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">ko</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">38.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">43.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">59.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">59.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">90.42</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">pt</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">69.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">74.59</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">76.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">76.65</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">85.67</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_small">sv</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">70.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">75.87</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">78.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_small">79.27</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_small">85.59</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">Ave</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">64.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">69.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">73.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_small">74.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_small">85.18</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>UAS for two versions of our approach, together with baseline and oracle systems on Google Universal Treebanks version 2.0.
“Ave” is the macro-average across the seven languages.</div>
</div>
</div>
</div>
<div id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">In this section, we will describe the details of our experiments and compare our results with previous methods.</p>
</div>
<div id="S4.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center" colspan="15"><span class="ltx_text ltx_font_script">Google Universal Treebanks V1.0</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">de</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">es</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">fr</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">ko</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">sv</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script"># sents</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">500</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">63.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">70.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">70.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">70.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">72.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">72.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">72.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">74.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">74.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">47.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">56.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">57.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">71.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">75.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">76.13</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">1000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">65.61</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">71.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">71.86</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">70.90</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.67</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.95</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">47.83</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">57.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">58.15</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.38</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.03</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">2000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">66.52</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">72.48</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.01</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.81</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">48.37</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">58.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">58.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.65</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.86</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">78.12</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">5000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">67.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.31</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.31</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.29</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">53.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">58.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">59.04</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.88</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">78.48</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">78.70</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">10000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">68.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.92</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.48</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.26</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">53.61</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">59.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">59.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">78.78</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">79.08</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">20000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">69.21</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.01</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.57</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.60</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.93</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">53.72</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">59.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">59.94</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.87</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">78.91</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">79.27</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="15"><span class="ltx_text ltx_font_script">Google Universal Treebanks V2.0</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">de</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">es</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">fr</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">ko</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">it</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script"># sents</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">500</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">60.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">71.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">71.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">69.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">72.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">73.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">71.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">74.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">74.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">40.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">56.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">57.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">72.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">75.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">75.94</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">1000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">61.76</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">72.39</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">70.78</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.79</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">40.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">57.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">57.93</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.67</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">2000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">65.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.04</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">71.75</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.35</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.21</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.06</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">40.87</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">58.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">58.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.99</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.39</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">5000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">67.86</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">73.62</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.43</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.83</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.14</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">40.90</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">58.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">58.96</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.07</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.10</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.34</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">10000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">68.70</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.02</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.85</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.95</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.17</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">41.29</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">59.13</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">59.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.65</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.50</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.71</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">20000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">69.77</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">74.30</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.75</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.53</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">43.34</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">59.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">59.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">76.08</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.55</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">77.74</span></td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r ltx_border_t"/>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span class="ltx_text ltx_font_bold ltx_font_script">pt</span></td>
<td class="ltx_td ltx_border_t" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script"># sents</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">PTP†</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">-U</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">+U</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">500</span></th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">71.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_script">74.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_script">74.68</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">1000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">71.91</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.08</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">2000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">72.93</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.32</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">5000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">73.78</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">75.98</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r"><span class="ltx_text ltx_font_script">10000</span></th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">74.40</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_script">75.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_script">76.15</span></td>
<td class="ltx_td" colspan="12"/></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_script">20000</span></th>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_script">74.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_b"><span class="ltx_text ltx_font_script">76.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span class="ltx_text ltx_font_script">76.65</span></td>
<td class="ltx_td" colspan="12"/></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Parsing results of our approach with different amount of parallel data on Google Universal Treebanks version 1.0 and 2.0.
We omit the results of Swedish for treebanks version 2.0 since the data for Swedish from version 2.0 are exactly the same with those
from version 1.0.</div>
</div>
<div id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Sets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">As presented in Section <a href="#S3.SS1" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we evaluate our parsing approach on both version 1.0 and version 2.0 of Google Univereal
Treebanks for seven languages<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup>Japanese and Indonesia are excluded as no practicable parallel data are available.</span></span></span>.
We use the standard splits of the treebank for each language as specified in the release of the
data<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><a href="https://code.google.com/p/uni-dep-tb/" title="" class="ltx_ref ltx_url"><span class="ltx_text ltx_font_typewriter">https://code.google.com/p/uni-dep-tb/</span></a></span></span></span>.
Table <a href="#S2.T1" title="Table 1 ‣ 2.4 Summary of Our Approach ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the statistics of the two versions of Google Universal Treebanks.
We strip all the dependency annotations off the training portion of each treebank, and use that as the unlabeled data for that target language.
We train our parsing model with different numbers of parallel sentences to analyze the influence of the amount of parallel data
on the parsing performance of our approach. The parallel data sets contain 500, 1000, 2000, 5000, 10000 and 20000 parallel sentences, respectively.
We randomly extract parallel sentences from each corpora, and smaller data sets are subsets of larger ones.
Table <a href="#S3.T2" title="Table 2 ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the number of tokens in the parallel data used in the experiments.</p>
</div>
</div>
<div id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.2 </span>System performance and comparison 
<br class="ltx_break"/>on Google Universal Treebanks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">For the comparison of parsing performance, we run experiments on the following systems:</p>
<dl id="I2" class="ltx_description">
<dt id="I2.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_description">DTP:</span></dt>
<dd class="ltx_item">
<div id="I2.ix1.p1" class="ltx_para">
<p class="ltx_p">The direct transfer parser (DTP) proposed by McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>,
who train a delexicalized parser on English labeled training data with no lexical features, then apply this parser to parse target
languages directly. It is based on the transition-based dependency parsing paradigm <cite class="ltx_cite">[<a href="#bib.bib12" title="Algorithms for deterministic incremental dependency parsing" class="ltx_ref">40</a>]</cite>.
We directly cite the results reported in McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib35" title="Universal dependency annotation for multilingual parsing" class="ltx_ref">31</a>]</cite>. In addition to their original results,
we also report results by re-implementing the direct transfer parser based on the first-order projective dependency parsing
model <cite class="ltx_cite">[<a href="#bib.bib176" title="Online large-margin training of dependency parsers" class="ltx_ref">30</a>]</cite> (DTP†).</p>
</div></dd>
<dt id="I2.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_description">PTP</span></dt>
<dd class="ltx_item">
<div id="I2.ix2.p1" class="ltx_para">
<p class="ltx_p">The projected transfer parser (PTP) described in McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>. The results of the
projected transfer parser re-implemented by us is marked as “PTP†”.</p>
</div></dd>
<dt id="I2.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_description">-U:</span></dt>
<dd class="ltx_item">
<div id="I2.ix3.p1" class="ltx_para">
<p class="ltx_p">Our approach training on only parallel data without unlabeled data for the target language. The parallel data set for each language contains 20,000 sentences.</p>
</div></dd>
<dt id="I2.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_description">+U:</span></dt>
<dd class="ltx_item">
<div id="I2.ix4.p1" class="ltx_para">
<p class="ltx_p">Our approach training on both parallel and unlabeled data. The parallel data sets are the ones contains 20,000 sentences.</p>
</div></dd>
<dt id="I2.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_description">OR:</span></dt>
<dd class="ltx_item">
<div id="I2.ix5.p1" class="ltx_para">
<p class="ltx_p">the supervised first-order projective dependency parsing model <cite class="ltx_cite">[<a href="#bib.bib176" title="Online large-margin training of dependency parsers" class="ltx_ref">30</a>]</cite>, trained on
the original treebanks with maximum likelihood estimation (equation <a href="#S2.E6" title="(6) ‣ 2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). One may regard this system as an oracle of transfer parsing.</p>
</div></dd>
</dl>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">Parsing accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ 3.2 Word Alignments ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a href="#S3.T4" title="Table 4 ‣ 3.3 Part-of-Speech Tagging ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the parsing results of our approach, together with the results of the baseline
systems and the oracle, on version 1.0 and version 2.0 of Google Universal Treebanks, respectively.
Our approaches significantly outperform all the baseline systems across all the seven target languages. For the results on
Google Universal Treebanks version 1.0, the improvement on average over the projected transfer paper (PTP†) is 3.96% and up to
6.22% for Korean and 4.80% for German. For the other three languages, the improvements are remarkable,
too — 2.33% for French, 3.03% for Spanish and 3.40% for Swedish.
By adding entropy regularization from unlabeled data, our full model achieves average improvement of 0.29% over the
“-U” setting. Moreover, our approach considerably bridges the gap to fully supervised dependency parsers, whose average UAS is 84.67%.
For the results on treebanks version 2.0, we can get similar observation and draw the same conclusion.</p>
</div>
</div>
<div id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.3 </span>Effect of the Amount of Parallel Text</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the UAS of our approach trained on different amounts of parallel data,
together with the results of the projected transfer parser re-implemented by us (PTP†).
We run two versions of our approach for each of the parallel data sets, one with unlabeled data (+U) and the other without them (-U).
From table <a href="#S4.T5" title="Table 5 ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we can get three observations. First, even the parsers trained with only 500 parallel sentences achieve
considerably high parsing accuracies (average 70.10% for version 1.0 and 71.59% for version 2.0).
This demonstrates that our approach does not rely on a large amount of parallel data.
Second, when gradually increasing the amount of parallel data, the parsing performance continues improving.
Third, entropy regularization with unlabeled data makes modest improvement on parsing performance over the parsers without unlabeled data.
This proves the effectiveness of the entropy regularization from unlabeled data.</p>
</div>
</div>
<div id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.4 </span>Experiments on CoNLL Treebanks</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">To make a thorough empirical comparison with previous studies, we also evaluate our system without unlabeled data (-U)
on treebanks from CoNLL shared task on dependency parsing <cite class="ltx_cite">[<a href="#bib.bib417" title="CoNLL-X shared task on multilingual dependency parsing" class="ltx_ref">6</a>, <a href="#bib.bib418" title="The conll 2007 shared task on dependency parsing" class="ltx_ref">38</a>]</cite>.
To facilitate comparison, we use the same eight Indo-European languages as target
languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish,
and same experimental setup as McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>.
We report both the results of the direct transfer and projected transfer parsers directly cited from McDonald
et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite> (DTP and PTP) and re-implemented by us (DTP†and PTP†).</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> gives the results comparing the model without unlabeled data (-U) presented in this work to those
five baseline systems and the oracle (OR).
The results of unsupervised DMV model <cite class="ltx_cite">[<a href="#bib.bib34" title="Corpus-based induction of syntactic structure: models of dependency and constituency" class="ltx_ref">22</a>]</cite> are from Table 1 of
McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>.
Our approach outperforms all these baseline systems and achieves state-of-the-art performance on all the eight languages.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p class="ltx_p">In order to compare with more previous methods, we also report parsing performance on sentences of length 10 or less after punctuation
has been removed. Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the results of our system and the results of baseline systems. “USR†” is the
weakly supervised system of Naseem et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Using universal linguistic knowledge to guide grammar induction" class="ltx_ref">35</a>]</cite>. “PGI” is the phylogenetic grammar induction
model of Berg-Kirkpatrick and Klein <cite class="ltx_cite">[<a href="#bib.bib3" title="Phylogenetic grammar induction" class="ltx_ref">3</a>]</cite>. Both the results of the two systems are cited from
Table 4 of McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>. We also include the results of the
unsupervised dependency parsing model with non-parallel multilingual guidance (NMG) proposed by
Cohen et al. <cite class="ltx_cite">[<a href="#bib.bib31" title="Unsupervised structure prediction with non-parallel multilingual guidance" class="ltx_ref">11</a>]</cite><span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup>For each language, we use the best result of the four systems in
Table 3 of Cohen et al. <cite class="ltx_cite">[<a href="#bib.bib31" title="Unsupervised structure prediction with non-parallel multilingual guidance" class="ltx_ref">11</a>]</cite></span></span></span>, and “PR” which is the posterior regularization approach presented
in Gillenwater et al. <cite class="ltx_cite">[<a href="#bib.bib2" title="Sparsity in dependency grammar induction" class="ltx_ref">15</a>]</cite>. All the results are shown in Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p class="ltx_p">From Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we can see that among the eight target languages, our approach achieves best parsing
performance on six languages — Danish, German, Greek, Italian, Portuguese and Swedish. It should be noted that
the “NMG” system utilizes more than one helper languages. So it is not directly comparable to our work.</p>
</div>
<div id="S4.T6" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<th class="ltx_td ltx_align_center ltx_border_r">DMV</th>
<th class="ltx_td ltx_align_center">DTP</th>
<th class="ltx_td ltx_align_center">DTP†</th>
<th class="ltx_td ltx_align_center">PTP</th>
<th class="ltx_td ltx_align_center ltx_border_r">PTP†</th>
<th class="ltx_td ltx_align_center ltx_border_r">-U</th>
<th class="ltx_td ltx_align_center">OR</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r ltx_border_t">da</th>
<th class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.4</th>
<td class="ltx_td ltx_align_center ltx_border_t">45.9</td>
<td class="ltx_td ltx_align_center ltx_border_t">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">48.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">87.1</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">de</th>
<th class="ltx_td ltx_align_center ltx_border_r">18.0</th>
<td class="ltx_td ltx_align_center">47.2</td>
<td class="ltx_td ltx_align_center">46.0</td>
<td class="ltx_td ltx_align_center">50.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">52.4</td>
<td class="ltx_td ltx_align_center ltx_border_r">57.3</td>
<td class="ltx_td ltx_align_center">87.0</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">el</th>
<th class="ltx_td ltx_align_center ltx_border_r">39.9</th>
<td class="ltx_td ltx_align_center">63.9</td>
<td class="ltx_td ltx_align_center">62.9</td>
<td class="ltx_td ltx_align_center">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">65.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">67.4</td>
<td class="ltx_td ltx_align_center">82.3</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">es</th>
<th class="ltx_td ltx_align_center ltx_border_r">28.5</th>
<td class="ltx_td ltx_align_center">53.3</td>
<td class="ltx_td ltx_align_center">54.4</td>
<td class="ltx_td ltx_align_center">55.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">59.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">60.3</td>
<td class="ltx_td ltx_align_center">83.6</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">it</th>
<th class="ltx_td ltx_align_center ltx_border_r">43.1</th>
<td class="ltx_td ltx_align_center">57.7</td>
<td class="ltx_td ltx_align_center">59.9</td>
<td class="ltx_td ltx_align_center">60.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">63.4</td>
<td class="ltx_td ltx_align_center ltx_border_r">64.0</td>
<td class="ltx_td ltx_align_center">83.9</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">nl</th>
<th class="ltx_td ltx_align_center ltx_border_r">38.5</th>
<td class="ltx_td ltx_align_center">60.8</td>
<td class="ltx_td ltx_align_center">60.7</td>
<td class="ltx_td ltx_align_center">67.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">66.5</td>
<td class="ltx_td ltx_align_center ltx_border_r">68.2</td>
<td class="ltx_td ltx_align_center">78.2</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">pt</th>
<th class="ltx_td ltx_align_center ltx_border_r">20.1</th>
<td class="ltx_td ltx_align_center">69.2</td>
<td class="ltx_td ltx_align_center">71.1</td>
<td class="ltx_td ltx_align_center">71.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">74.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">75.1</td>
<td class="ltx_td ltx_align_center">87.2</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_r">sv</th>
<th class="ltx_td ltx_align_center ltx_border_r">44.0</th>
<td class="ltx_td ltx_align_center">58.3</td>
<td class="ltx_td ltx_align_center">60.3</td>
<td class="ltx_td ltx_align_center">61.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">62.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">66.7</td>
<td class="ltx_td ltx_align_center">88.0</td></tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Ave</th>
<th class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">33.2</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">63.6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">84.7</td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Parsing results on treebanks from CoNLL shared tasks for eight target languages. The results of unsupervised DMV model are
from Table 1 of McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>.</div>
</div>
<div id="S4.T7" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_top">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_border_r"/>
<th class="ltx_td ltx_align_center">DTP</th>
<th class="ltx_td ltx_align_center">DTP†</th>
<th class="ltx_td ltx_align_center">PTP</th>
<th class="ltx_td ltx_align_center ltx_border_r">PTP†</th>
<th class="ltx_td ltx_align_center">USR†</th>
<th class="ltx_td ltx_align_center">PGI</th>
<th class="ltx_td ltx_align_center">PR</th>
<th class="ltx_td ltx_align_center ltx_border_r">NMG</th>
<th class="ltx_td ltx_align_center">-U</th></tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">da</td>
<td class="ltx_td ltx_align_center ltx_border_t">53.2</td>
<td class="ltx_td ltx_align_center ltx_border_t">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_t">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_t">55.1</td>
<td class="ltx_td ltx_align_center ltx_border_t">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_t">44.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.9</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">60.1</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">de</td>
<td class="ltx_td ltx_align_center">65.9</td>
<td class="ltx_td ltx_align_center">57.9</td>
<td class="ltx_td ltx_align_center">67.0</td>
<td class="ltx_td ltx_align_center ltx_border_r">63.5</td>
<td class="ltx_td ltx_align_center">60.0</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center ltx_border_r">—</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">67.5</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">el</td>
<td class="ltx_td ltx_align_center">73.9</td>
<td class="ltx_td ltx_align_center">70.8</td>
<td class="ltx_td ltx_align_center">73.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">72.3</td>
<td class="ltx_td ltx_align_center">60.3</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center ltx_border_r">73.0</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">74.3</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">es</td>
<td class="ltx_td ltx_align_center">58.0</td>
<td class="ltx_td ltx_align_center">62.3</td>
<td class="ltx_td ltx_align_center">62.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">66.1</td>
<td class="ltx_td ltx_align_center">68.3</td>
<td class="ltx_td ltx_align_center">58.4</td>
<td class="ltx_td ltx_align_center">62.4</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">76.7</span></td>
<td class="ltx_td ltx_align_center">64.6</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">it</td>
<td class="ltx_td ltx_align_center">65.5</td>
<td class="ltx_td ltx_align_center">66.9</td>
<td class="ltx_td ltx_align_center">69.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">71.5</td>
<td class="ltx_td ltx_align_center">47.9</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center">—</td>
<td class="ltx_td ltx_align_center ltx_border_r">—</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">73.6</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">nl</td>
<td class="ltx_td ltx_align_center">67.6</td>
<td class="ltx_td ltx_align_center">66.0</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">72.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r">72.1</td>
<td class="ltx_td ltx_align_center">44.0</td>
<td class="ltx_td ltx_align_center">45.1</td>
<td class="ltx_td ltx_align_center">37.9</td>
<td class="ltx_td ltx_align_center ltx_border_r">50.7</td>
<td class="ltx_td ltx_align_center">70.5</td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">pt</td>
<td class="ltx_td ltx_align_center">77.9</td>
<td class="ltx_td ltx_align_center">79.2</td>
<td class="ltx_td ltx_align_center">80.6</td>
<td class="ltx_td ltx_align_center ltx_border_r">82.9</td>
<td class="ltx_td ltx_align_center">70.9</td>
<td class="ltx_td ltx_align_center">63.0</td>
<td class="ltx_td ltx_align_center">47.8</td>
<td class="ltx_td ltx_align_center ltx_border_r">79.8</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">83.3</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">sv</td>
<td class="ltx_td ltx_align_center">70.4</td>
<td class="ltx_td ltx_align_center">70.2</td>
<td class="ltx_td ltx_align_center">71.3</td>
<td class="ltx_td ltx_align_center ltx_border_r">70.4</td>
<td class="ltx_td ltx_align_center">52.6</td>
<td class="ltx_td ltx_align_center">58.3</td>
<td class="ltx_td ltx_align_center">42.2</td>
<td class="ltx_td ltx_align_center ltx_border_r">74.0</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">75.1</span></td></tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Ave</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">66.6</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">66.1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">69.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">69.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">—</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t">—</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">—</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span class="ltx_text ltx_font_bold">71.1</span></td></tr>
</tbody>
</table>
<div class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>UAS on sentences of length 10 or less without punctuation from CoNLL shared task treebanks.
“USR†” is the weakly supervised system of Naseem et al. <cite class="ltx_cite">[<a href="#bib.bib1" title="Using universal linguistic knowledge to guide grammar induction" class="ltx_ref">35</a>]</cite>.
“PGI” is the phylogenetic grammar induction model of Berg-Kirkpatrick and Klein <cite class="ltx_cite">[<a href="#bib.bib3" title="Phylogenetic grammar induction" class="ltx_ref">3</a>]</cite>.
Both the “USR†” and “PGI” systems are implemented and reported by McDonald et al. <cite class="ltx_cite">[<a href="#bib.bib36" title="Multi-source transfer of delexicalized dependency parsers" class="ltx_ref">34</a>]</cite>.
“NMG” is the unsupervised dependency parsing model with non-parallel multilingual guidance <cite class="ltx_cite">[<a href="#bib.bib31" title="Unsupervised structure prediction with non-parallel multilingual guidance" class="ltx_ref">11</a>]</cite>.
“PR” is the posterior regularization approach presented in Gillenwater et al. <cite class="ltx_cite">[<a href="#bib.bib2" title="Sparsity in dependency grammar induction" class="ltx_ref">15</a>]</cite>.
Some systems’ results for certain target languages are not available as marked by —.
</div>
</div>
</div>
<div id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection"><span class="ltx_tag ltx_tag_subsection">4.5 </span>Extensions</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p">In this section, we briefly outline a few extensions to our approach that we want to explore in future work.</p>
</div>
<div id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Non-Projective Parsing</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p class="ltx_p">As mentioned in section <a href="#S2.SS3" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, the runtime to compute <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS1.p1.m1" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math> and its gradient is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS1.p1.m2" class="ltx_Math" alttext="O(n^{4})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>4</mn></msup><mo>)</mo></mrow></mrow></math>. One reasonable speedup,
as presented in Smith and Eisner <cite class="ltx_cite">[<a href="#bib.bib23" title="Bootstrapping feature-rich dependency parsers with entropic priors" class="ltx_ref">44</a>]</cite>, is to replace Shannon entropy with
Rényi entropy. The <span class="ltx_text ltx_font_bold">Rényi entropy</span> is parameterized by <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS1.p1.m3" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math>:</p>
<table id="S4.E16" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E16.m1" class="ltx_Math" alttext="R_{\alpha}(p)=\frac{1}{1-\alpha}\log\Big(\sum\limits_{y}p(y)^{\alpha}\Big)" display="block"><mrow><mrow><msub><mi>R</mi><mi>α</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mi>p</mi><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mn>1</mn><mo>-</mo><mi>α</mi></mrow></mfrac><mo>⁢</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo>(</mo><mrow><munder><mo largeop="true" movablelimits="false" symmetric="true">∑</mo><mi>y</mi></munder><mrow><mi>p</mi><mo>⁢</mo><msup><mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow><mi>α</mi></msup></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(16)</span></td></tr>
</table>
<p class="ltx_p">With Rényi entropy, the computation of <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS1.p1.m4" class="ltx_Math" alttext="K_{U}" display="inline"><msub><mi>K</mi><mi>U</mi></msub></math> and its gradient is <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS1.p1.m5" class="ltx_Math" alttext="O(n^{3})" display="inline"><mrow><mi>O</mi><mo>⁢</mo><mrow><mo>(</mo><msup><mi>n</mi><mn>3</mn></msup><mo>)</mo></mrow></mrow></math>, even for non-projective case.</p>
</div>
</div>
<div id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Higher-Order Models for Projective Parsing</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p class="ltx_p">Our learning framework can be extended to higher-order dependency parsing models. For example, if we want to make our model capable of
utilizing more contextual information, we can extend our transferring weight to higher-order parts:</p>
<table id="S4.E17" class="ltx_equation">

<tr class="ltx_equation ltx_align_baseline">
<td class="ltx_eqn_center_padleft"/>
<td class="ltx_align_center"><math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.E17.m1" class="ltx_Math" alttext="\tilde{w}(p^{t},\boldsymbol{x}^{t}_{i})=\left\{\begin{array}[]{ll}w_{E}(p^{s},%&#10;\boldsymbol{x}^{s}_{i}),&amp;\textrm{if }p^{t}\stackrel{align}{\longrightarrow}p^{%&#10;s}\\&#10;w_{E}(p^{t}_{delex},\boldsymbol{x}^{s}_{i}),&amp;\textrm{otherwise}\end{array}\right." display="block"><mrow><mrow><mover accent="true"><mi>w</mi><mo stretchy="false">~</mo></mover><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>p</mi><mi>t</mi></msup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>t</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>=</mo><mrow><mo>{</mo><mtable align="center" columnspacing="0.4em" rowspacing="0.2ex"><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>w</mi><mi>E</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msup><mi>p</mi><mi>s</mi></msup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign="left"><mrow><mrow><mtext>if </mtext><mo>⁢</mo><msup><mi>p</mi><mi>t</mi></msup></mrow><mover><mo movablelimits="false">⟶</mo><mrow><mi>a</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>g</mi><mo>⁢</mo><mi>n</mi></mrow></mover><msup><mi>p</mi><mi>s</mi></msup></mrow></mtd></mtr><mtr><mtd columnalign="left"><mrow><mrow><msub><mi>w</mi><mi>E</mi></msub><mo>⁢</mo><mrow><mo>(</mo><mrow><msubsup><mi>p</mi><mrow><mi>d</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>l</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>x</mi></mrow><mi>t</mi></msubsup><mo>,</mo><msubsup><mi>𝒙</mi><mi>i</mi><mi>s</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd columnalign="left"><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mrow></math></td>
<td class="ltx_eqn_center_padright"/>
<td rowspan="1" class="ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation">(17)</span></td></tr>
</table>
<p class="ltx_p">where <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS2.p1.m1" class="ltx_Math" alttext="p" display="inline"><mi>p</mi></math> is a small <em class="ltx_emph">part</em> of tree <math xmlns="http://www.w3.org/1998/Math/MathML" id="S4.SS5.SSS2.p1.m2" class="ltx_Math" alttext="\boldsymbol{y}" display="inline"><mi>𝒚</mi></math> that has limited interactions. For projective parsing,
several algorithms <cite class="ltx_cite">[<a href="#bib.bib175" title="Online learning of approximate dependency parsing algorithms" class="ltx_ref">33</a>, <a href="#bib.bib80" title="Experiments with a higher-order projective dependency parser" class="ltx_ref">8</a>, <a href="#bib.bib148" title="Efficient third-order dependency parsers" class="ltx_ref">24</a>, <a href="#bib.bib439" title="Fourth-order dependency parsing" class="ltx_ref">27</a>]</cite>
have been proposed to solve the model training problems (calculation of objective function and gradient) for different factorizations.</p>
</div>
</div>
<div id="S4.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection"><span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>IGT Data</h4>

<div id="S4.SS5.SSS3.p1" class="ltx_para">
<p class="ltx_p">One possible direction to improve our approach is to replace parallel text with Interlinear Glossed Text (IGT) <cite class="ltx_cite">[<a href="#bib.bib10" title="Developing odin: a multilingual repository of annotated language data for hundreds of the world’s languages." class="ltx_ref">26</a>]</cite>,
which is a semi-structured data type encoding more syntactic information than parallel data. By using IGT Data, not only can we obtain
more accurate word alignments, but also extract useful cross-lingual information for the resource-poor language.</p>
</div>
</div>
</div>
</div>
<div id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section"><span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">In this paper, we propose an unsupervised projective dependency parsing approach for resource-poor languages, using existing resources
from a resource-rich source language. By presenting a model training framework, our approach can utilize parallel text to
estimate transferring distribution
with the help of a well-developed resource-rich language dependency parser, and use unlabeled data as entropy regularization.
The experimental results on three data sets across ten target languages show that our approach achieves significant improvement
over previous studies.</p>
</div>
</div>
<div id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">This material is based upon work supported by the National Science Foundation under Grant No. BCS-0748919. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the
National Science Foundation.</p>
</div>
</div>
<div id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib17" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Abney</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Understanding the Yarowsky algorithm</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">30</span>, <span class="ltx_text ltx_bib_pages"> pp. 2004</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p2" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib52" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. K. Baker</span><span class="ltx_text ltx_bib_year">(1979)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Trainable grammars for speech recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 547–550</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p3" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>,
<a href="#S2.SS3.p4" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Berg-Kirkpatrick and D. Klein</span><span class="ltx_text ltx_bib_year">(2010-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Phylogenetic grammar induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 1288–1297</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P10-1131" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS4.p3" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib30" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Blunsom and T. Cohn</span><span class="ltx_text ltx_bib_year">(2010-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised induction of tree substitution grammars for dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Cambridge, MA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1204–1213</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D10-1117" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Brand</span><span class="ltx_text ltx_bib_year">(1998)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structure learning in conditional probability models via an entropic prior and parameter extinction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Neural Computation</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 1155–1182</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p6" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib417" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Buchholz and E. Marsi</span><span class="ltx_text ltx_bib_year">(2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CoNLL-X shared task on multilingual dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">New York, NY</span>, <span class="ltx_text ltx_bib_pages"> pp. 149–164</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p3" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS4.p1" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
</span></li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Burkett and D. Klein</span><span class="ltx_text ltx_bib_year">(2008-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Two languages are better than one (for syntactic parsing)</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Honolulu, Hawaii</span>, <span class="ltx_text ltx_bib_pages"> pp. 877–886</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D08-1092" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib80" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Carreras</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Experiments with a higher-order projective dependency parser</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 957–961</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS5.SSS2.p1" title="4.5.2 Higher-Order Models for Projective Parsing ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>.
</span></li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Carroll and E. Charniak</span><span class="ltx_text ltx_bib_year">(1992)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Two experiments on learning probabilistic dependency grammars from corpora</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Chen, J. Kazama and K. Torisawa</span><span class="ltx_text ltx_bib_year">(2010-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bitext dependency parsing with bilingual subtree constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 21–29</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P10-1003" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. B. Cohen, D. Das and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2011-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised structure prediction with non-parallel multilingual guidance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, Scotland, UK.</span>, <span class="ltx_text ltx_bib_pages"> pp. 50–61</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D11-1005" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS4.p3" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib32" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Cohen and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2009-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Boulder, Colorado</span>, <span class="ltx_text ltx_bib_pages"> pp. 74–82</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N/N09/N09-1009" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Das and S. Petrov</span><span class="ltx_text ltx_bib_year">(2011-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Unsupervised part-of-speech tagging with bilingual graph-based projections</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Portland, Oregon, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 600–609</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P11-1061" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p3" title="3.3 Part-of-Speech Tagging ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Ganchev, J. Gillenwater and B. Taskar</span><span class="ltx_text ltx_bib_year">(2009-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dependency grammar induction via bitext projection constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Suntec, Singapore</span>, <span class="ltx_text ltx_bib_pages"> pp. 369–377</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P/P09/P09-1042" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Gillenwater, K. Ganchev, J. Graça, F. Pereira and B. Taskar</span><span class="ltx_text ltx_bib_year">(2010-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sparsity in dependency grammar induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 194–199</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P10-2036" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS4.p3" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. V. Graca, L. Inesc-id, K. Ganchev and B. Taskar</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Expectation maximization and posterior constraints</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 569–576</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Grandvalet and Y. Bengio</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semi-supervised learning by entropy minimization</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p6" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
</span></li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Huang, W. Jiang and Q. Liu</span><span class="ltx_text ltx_bib_year">(2009-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bilingually-constrained (monolingual) shift-reduce parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Singapore</span>, <span class="ltx_text ltx_bib_pages"> pp. 1222–1231</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D/D09/D09-1127" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Hwa, P. Resnik, A. Weinberg, C. Cabezas and O. Kolak</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bootstrapping parsers via syntactic projection across parallel texts</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Natural Language Engineering</span> <span class="ltx_text ltx_bib_volume">11</span>, <span class="ltx_text ltx_bib_pages"> pp. 11–311</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Jiao, S. Wang, C. Lee, R. Greiner and D. Schuurmans</span><span class="ltx_text ltx_bib_year">(2006-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Semi-supervised conditional random fields for improved sequence segmentation and labeling</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sydney, Australia</span>, <span class="ltx_text ltx_bib_pages"> pp. 209–216</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P06-1027" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220175.1220202" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p2" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p6" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS3.p4" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Katz-Brown, S. Petrov, R. McDonald, F. Och, D. Talbot, H. Ichikawa, M. Seno and H. Kazawa</span><span class="ltx_text ltx_bib_year">(2011-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Training a parser for machine translation reordering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, Scotland, UK.</span>, <span class="ltx_text ltx_bib_pages"> pp. 183–192</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D11-1017" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Klein and C. Manning</span><span class="ltx_text ltx_bib_year">(2004-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Corpus-based induction of syntactic structure: models of dependency and constituency</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Barcelona, Spain</span>, <span class="ltx_text ltx_bib_pages"> pp. 478–485</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P04-1061" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1218955.1219016" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS4.p2" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
</span></li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Koehn</span><span class="ltx_text ltx_bib_year">(2005)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Europarl: A Parallel Corpus for Statistical Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Phuket, Thailand</span>, <span class="ltx_text ltx_bib_pages"> pp. 79–86</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://mt-archive.info/MTS-2005-Koehn.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p2" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib148" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Koo and M. Collins</span><span class="ltx_text ltx_bib_year">(2010-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient third-order dependency parsers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Uppsala, Sweden</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–11</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS5.SSS2.p1" title="4.5.2 Higher-Order Models for Projective Parsing ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>.
</span></li>
<li id="bib.bib146" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Koo, A. Globerson, X. Carreras and M. Collins</span><span class="ltx_text ltx_bib_year">(2007-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structured predicition models via the matrix-tree theorem</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech</span>, <span class="ltx_text ltx_bib_pages"> pp. 141–150</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p3" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. D. Lewis and F. Xia</span><span class="ltx_text ltx_bib_year">(2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Developing odin: a multilingual repository of annotated language data for hundreds of the world’s languages.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">LLC</span> <span class="ltx_text ltx_bib_volume">25</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 303–319</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dblp.uni-trier.de/db/journals/lalc/lalc25.html#LewisX10" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS5.SSS3.p1" title="4.5.3 IGT Data ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.3</span></a>.
</span></li>
<li id="bib.bib439" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Ma and H. Zhao</span><span class="ltx_text ltx_bib_year">(2012-12)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fourth-order dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Mumbai, India</span>, <span class="ltx_text ltx_bib_pages"> pp. 785–796</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/C12-2077" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS5.SSS2.p1" title="4.5.2 Higher-Order Models for Projective Parsing ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>.
</span></li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. S. Mann and A. McCallum</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient computation of entropy gradient for semi-supervised conditional random fields</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 109–112</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dl.acm.org/citation.cfm?id=1614108.1614136" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS2.p2" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS3.p4" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Mareček and M. Straka</span><span class="ltx_text ltx_bib_year">(2013-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Stop-probability estimates computed on a large corpus improve unsupervised dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sofia, Bulgaria</span>, <span class="ltx_text ltx_bib_pages"> pp. 281–290</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P13-1028" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib176" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald, K. Crammer and F. Pereira</span><span class="ltx_text ltx_bib_year">(2005-June 25-30)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Online large-margin training of dependency parsers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Ann Arbor, Michigan, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 91–98</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.ix1.p1" title="DTP: ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">DTP:</span></a>,
<a href="#I2.ix5.p1" title="OR: ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">OR:</span></a>,
<a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald, J. Nivre, Y. Quirmbach-Brundage, Y. Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov, H. Zhang, O. Täckström, C. Bedini, N. Bertomeu Castelló and J. Lee</span><span class="ltx_text ltx_bib_year">(2013-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Universal dependency annotation for multilingual parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Sofia, Bulgaria</span>, <span class="ltx_text ltx_bib_pages"> pp. 92–97</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P13-2017" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.ix1.p1" title="DTP: ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">DTP:</span></a>,
<a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S3.SS1.p1" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S3.SS1.p3" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
</span></li>
<li id="bib.bib177" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald, F. Pereira, K. Ribarov and J. Hajic</span><span class="ltx_text ltx_bib_year">(2005-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Non-projective dependency parsing using spanning tree algorithms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Vancouver, Canada</span>, <span class="ltx_text ltx_bib_pages"> pp. 523–530</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib175" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald and F. Pereira</span><span class="ltx_text ltx_bib_year">(2006-04)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Online learning of approximate dependency parsing algorithms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Trento, Italy</span>, <span class="ltx_text ltx_bib_pages"> pp. 81–88</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S4.SS5.SSS2.p1" title="4.5.2 Higher-Order Models for Projective Parsing ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>.
</span></li>
<li id="bib.bib36" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. McDonald, S. Petrov and K. Hall</span><span class="ltx_text ltx_bib_year">(2011-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi-source transfer of delexicalized dependency parsers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, Scotland, UK.</span>, <span class="ltx_text ltx_bib_pages"> pp. 62–72</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D11-1006" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.ix1.p1" title="DTP: ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">DTP:</span></a>,
<a href="#I2.ix2.p1" title="PTP ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">PTP</span></a>,
<a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p3" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p4" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S3.SS1.p3" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS4.p1" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.SS4.p2" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.SS4.p3" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.T6" title="Table 6 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>,
<a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Naseem, H. Chen, R. Barzilay and M. Johnson</span><span class="ltx_text ltx_bib_year">(2010-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Using universal linguistic knowledge to guide grammar induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Cambridge, MA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1234–1244</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D10-1120" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS4.p3" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>,
<a href="#S4.T7" title="Table 7 ‣ 4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span></li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. G. Nash and J. Nocedal</span><span class="ltx_text ltx_bib_year">(1991)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A numerical study of the limited memory bfgs method and truncated-newton method for large scale optimization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">SIAM Journal on Optimization</span> <span class="ltx_text ltx_bib_volume">1(2)</span>, <span class="ltx_text ltx_bib_pages"> pp. 358–372</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p1" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. T. Nguyen, A. Moschitti and G. Riccardi</span><span class="ltx_text ltx_bib_year">(2009-08)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Convolution kernels on constituent, dependency and sequential structures for relation extraction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Singapore</span>, <span class="ltx_text ltx_bib_pages"> pp. 1378–1387</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D/D09/D09-1143" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib418" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Nivre, J. Hall, S. Kübler, R. Mcdonald, J. Nilsson, S. Riedel and D. Yuret</span><span class="ltx_text ltx_bib_year">(2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The conll 2007 shared task on dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech</span>, <span class="ltx_text ltx_bib_pages"> pp. 915–932</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS1.p3" title="3.1 Choosing Target Languages ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>,
<a href="#S4.SS4.p1" title="4.4 Experiments on CoNLL Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.
</span></li>
<li id="bib.bib186" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Nivre and M. Scholz</span><span class="ltx_text ltx_bib_year">(2004-August 23-27)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deterministic dependency parsing of English text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Geneva, Switzerland</span>, <span class="ltx_text ltx_bib_pages"> pp. 64–70</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib12" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Nivre</span><span class="ltx_text ltx_bib_year">(2008-12)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Algorithms for deterministic incremental dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Comput. Linguist.</span> <span class="ltx_text ltx_bib_volume">34</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 513–553</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0891-2017</span>,
<a href="http://dx.doi.org/10.1162/coli.07-056-R1-07-027" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1162/coli.07-056-R1-07-027" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I2.ix1.p1" title="DTP: ‣ 4.2 System performance and comparison  on Google Universal Treebanks ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">DTP:</span></a>.
</span></li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_incollection"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. A. Paskin</span><span class="ltx_text ltx_bib_year">(2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cubic-time parsing and learning algorithms for grammatical bigram models</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p3" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>,
<a href="#S2.SS3.p4" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_article"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Petrov, D. Das and R. T. McDonald</span><span class="ltx_text ltx_bib_year">(2011)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A universal part-of-speech tagset</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CoRR</span> <span class="ltx_text ltx_bib_volume">abs/1104.2086</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p1" title="3.3 Part-of-Speech Tagging ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib121" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Shinyama, S. Sekine and K. Sudo</span><span class="ltx_text ltx_bib_year">(2002)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic paraphrase acquisition from news articles</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 313–318</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. A. Smith and J. Eisner</span><span class="ltx_text ltx_bib_year">(2007-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bootstrapping feature-rich dependency parsers with entropic priors</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech Republic</span>, <span class="ltx_text ltx_bib_pages"> pp. 667–677</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D/D07/D07-1070" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p4" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.SS2.p2" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S2.SS2.p3" title="2.2 Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>,
<a href="#S4.SS5.SSS1.p1" title="4.5.1 Non-Projective Parsing ‣ 4.5 Extensions ‣ 4 Experiments ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.1</span></a>.
</span></li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. A. Smith and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bilingual parsing with factored estimation: using English to parse Korean</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 49–56</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib147" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. A. Smith and N. A. Smith</span><span class="ltx_text ltx_bib_year">(2007-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Probabilistic models of nonporjective dependency trees</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech</span>, <span class="ltx_text ltx_bib_pages"> pp. 132–140</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.SS3.p3" title="2.3 Algorithms and Complexity for Model Training ‣ 2 Our Approach ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
</span></li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. A. Smith and J. Eisner</span><span class="ltx_text ltx_bib_year">(2005-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contrastive estimation: training log-linear models on unlabeled data</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Ann Arbor, Michigan</span>, <span class="ltx_text ltx_bib_pages"> pp. 354–362</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/P05-1044" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1219840.1219884" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. I. Spitkovsky, H. Alshawi and D. Jurafsky</span><span class="ltx_text ltx_bib_year">(2010-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">From baby steps to leapfrog: how “less is more” in unsupervised dependency parsing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Los Angeles, California</span>, <span class="ltx_text ltx_bib_pages"> pp. 751–759</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/N10-1116" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib28" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. I. Spitkovsky, H. Alshawi and D. Jurafsky</span><span class="ltx_text ltx_bib_year">(2013-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Breaking out of local optima with count transforms and model recombination: a study in grammar induction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Seattle, Washington, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 1983–1995</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D13-1204" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Toutanova, D. Klein, C. D. Manning and Y. Singer</span><span class="ltx_text ltx_bib_year">(2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Feature-rich part-of-speech tagging with a cyclic dependency network</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 252–259</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.SS3.p2" title="3.3 Part-of-Speech Tagging ‣ 3 Data and Tools ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
</span></li>
<li id="bib.bib48" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Xie, H. Mi and Q. Liu</span><span class="ltx_text ltx_bib_year">(2011-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A novel dependency-to-string model for statistical machine translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Edinburgh, Scotland, UK.</span>, <span class="ltx_text ltx_bib_pages"> pp. 216–226</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D11-1020" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_inproceedings"><span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhang, L. Huang, K. Zhao and R. McDonald</span><span class="ltx_text ltx_bib_year">(2013-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Online learning for inexact hypergraph search</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Seattle, Washington, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 908–913</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D13-1093" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p1" title="1 Introduction ‣ Unsupervised Dependency Parsing with Transferring  Distribution via Parallel Guidance and Entropy Regularization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></li>
</ul>
</div>
</div>
</div>
<div class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jun 10 19:11:18 2014 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"/></a></div></div>
</div>
</body>
</html>
