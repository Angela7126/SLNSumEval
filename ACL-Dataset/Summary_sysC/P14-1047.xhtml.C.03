<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:548">
</head>
<body bgcolor="white">
<a href="#0" id="0">We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.</a>
<a href="#1" id="1">We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.</a>
<a href="#2" id="2">Typically there are three main approaches to the problem of learning dialogue policies using RL.</a>
<a href="#3" id="3">We propose a fourth approach concurrent learning of the system policy and the SU policy using multi-agent RL techniques.</a>
<a href="#4" id="4">So far research on using RL for dialogue policy learning has focused on single-agent RL techniques.</a>
<a href="#5" id="5">In this case the environment of a learning agent is one or more other agents that can also be learning at the same time.</a>
<a href="#6" id="6">As mentioned in section 1 , there are three main approaches to the problem of learning dialogue policies using RL.</a>
<a href="#7" id="7">2008 ) used a combination of RL and supervised learning to learn a dialogue policy in a flight reservation domain, whereas Li et al.</a>
<a href="#8" id="8">Thus the environment of an agent is one or more other agents that continuously change their behavior because they are also learning at the same time.</a>
<a href="#9" id="9">But single-agent RL techniques are not well suited for concurrent learning where each agent is trained against a continuously changing environment.</a>
<a href="#10" id="10">2012 ) proposed a framework for co-adaptation of the dialogue policy and the SU using single-agent RL.</a>
<a href="#11" id="11">On the other hand when the agent is “ losing ” the learning rate Δ L ⁢ F should be high so that the agent has more time to adapt to the other agents ’ policies, which also facilitates convergence.</a>
<a href="#12" id="12">The negotiation finishes when one of the agents accepts the other agent ’ s offer or time runs out.</a>
<a href="#13" id="13">As we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds.</a>
<a href="#14" id="14">Thus the reward for Agent 1 is 1190, the reward for Agent 2 is 1190, and the average reward of the two agents is also 1190.</a>
<a href="#15" id="15">Also, the convergence reward for Agent 1 is 1200 and the convergence reward for Agent 2 is also 1200.</a>
<a href="#16" id="16">Where C ⁢ R 1 is the convergence reward for Agent 1, R 1 ⁢ j is the reward of Agent 1 for run j , C ⁢ R 2 is the convergence reward for Agent 2, and R 2 ⁢ j is the reward of Agent 2 for run j.</a>
<a href="#17" id="17">Moreover, A ⁢ D 1 is the average distance from the convergence reward for Agent 1, A ⁢ D 2 is the average distance from the convergence reward for Agent 2, and A ⁢ D is the average of A ⁢ D 1 and A ⁢ D 2.</a>
<a href="#18" id="18">Figures 2 , 3 , 4 , and 5 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4, 5, 6, and 7 fruits respectively.</a>
<a href="#19" id="19">Figures 6 and 7 show the average distance from the convergence reward as a function of the number of episodes per epoch during training, for 4 and 5 fruits respectively.</a>
</body>
</html>