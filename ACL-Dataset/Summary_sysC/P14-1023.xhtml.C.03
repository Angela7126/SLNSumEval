<html>
<head>
<meta name="TextLength" content="SENT_NUM:26, WORD_NUM:652">
</head>
<body bgcolor="white">
<a href="#0" id="0">Instead of first collecting context vectors and then reweighting these vectors based on various criteria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear.</a>
<a href="#1" id="1">We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their training-based alternative as predict(ive) models.</a>
<a href="#2" id="2">2012 ) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper.</a>
<a href="#3" id="3">Instead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin.</a>
<a href="#4" id="4">2013d ) compare their predict models to “ Latent Semantic Analysis ” (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior.</a>
<a href="#5" id="5">We prepared the count models using the DISSECT toolkit.</a>
<a href="#6" id="6">8 8 http://clic.cimec.unitn.it/composes/toolkit/ We extracted count vectors from symmetric context windows of two and five words to either side of target.</a>
<a href="#7" id="7">In total, 36 count models were evaluated.</a>
<a href="#8" id="8">We trained our predict models with the word2vec toolkit.</a>
<a href="#9" id="9">The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window.</a>
<a href="#10" id="10">In total, we evaluate 48 predict models, a number comparable to that of the count models we consider.</a>
<a href="#11" id="11">We report state-of-the-art performance on the two subsets from the work of Agirre and colleagues, who used different kinds of count vectors extracted from a very large corpus (orders of magnitude larger than ours.</a>
<a href="#12" id="12">Current state of the art was reached by the window-based count model of Baroni and Lenci ( 2010 ).</a>
<a href="#13" id="13">While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al.</a>
<a href="#14" id="14">2013a ) specifically to test predict models.</a>
<a href="#15" id="15">The first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors.</a>
<a href="#16" id="16">The success of the predict models cannot be blamed on poor performance of the count models.</a>
<a href="#17" id="17">Besides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms.</a>
<a href="#18" id="18">Interestingly, count vectors achieve performance comparable to that of predict vectors only on the selectional preference tasks.</a>
<a href="#19" id="19">The up task in particular is also the only benchmark on which predict models are seriously lagging behind state-of-the-art and dm performance.</a>
<a href="#20" id="20">The second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4 , respectively.</a>
<a href="#21" id="21">We see that, for both approaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in performance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem.</a>
<a href="#22" id="22">51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set.</a>
<a href="#23" id="23">The selected count model is the third best overall model of its class as reported in Table 3.</a>
<a href="#24" id="24">Tables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes.</a>
<a href="#25" id="25">Had we also based our systematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors.</a>
</body>
</html>