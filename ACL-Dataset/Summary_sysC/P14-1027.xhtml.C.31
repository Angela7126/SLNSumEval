<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:446">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our extension assumes that the 𝖢𝗈𝗅𝗅𝗈𝖼𝟣 - 𝖢𝗈𝗅𝗅𝗈𝖼𝟥 constituents are in fact phrase-like, so we extend the rules ( 19 – 21 ) to permit an optional sequence of monosyllabic words at the left edge of each of these constituents.</a>
<a href="#1" id="1">Our model thus captures two of the properties of function words discussed in section 1.1 they are monosyllabic (and thus phonologically simple), and they appear on the periphery of phrases.</a>
<a href="#2" id="2">We put “ function words ” in scare quotes below because our model only approximately captures the linguistic properties of function words.</a>
<a href="#3" id="3">This model memoises (i.e.,, learns) both the individual “ function words ” and the sequences of “ function words ” that modify the 𝖢𝗈𝗅𝗅𝗈𝖼𝟣 - 𝖢𝗈𝗅𝗅𝗈𝖼𝟥 constituents.</a>
<a href="#4" id="4">Note also that “ function words ” expand directly to 𝖲𝗒𝗅𝗅𝖺𝖻𝗅𝖾𝖨𝖥 , which in turn expands to a monosyllable with a word-initial onset and word-final coda.</a>
<a href="#5" id="5">This means that “ function words ” are memoised independently of the “ content words ” that 𝖶𝗈𝗋𝖽 expands to; i.e.,, the model learns distinct “ function word ” and “ content word ” vocabularies.</a>
<a href="#6" id="6">Figure 1 depicts a sample parse generated by this grammar.</a>
<a href="#7" id="7">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟥.</a>
<a href="#8" id="8">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥.</a>
<a href="#9" id="9">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥.</a>
<a href="#10" id="10">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟥.</a>
<a href="#11" id="11">𝖶𝗈𝗋𝖽.</a>
<a href="#12" id="12">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝗌𝟣.</a>
<a href="#13" id="13">𝖥𝗎𝗇𝖼𝖶𝗈𝗋𝖽𝟣.</a>
<a href="#14" id="14">𝖶𝗈𝗋𝖽.</a>
<a href="#15" id="15">Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our “ function word ” Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 – 24 ) are replaced with the mirror-image rules in which “ function words ” are attached to the right periphery.</a>
<a href="#16" id="16">Here we evaluate the word segmentations found by the “ function word ” Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from.</a>
<a href="#17" id="17">Figure 2 presents the standard token and lexicon (i.e.,, type) f-score evaluations for word segmentations proposed by these models [] , and Table 1 summarises the token and lexicon f-scores for the major models discussed in this paper.</a>
<a href="#18" id="18">It is interesting to note that adding “ function words ” improves token f-score by more than 4%, corresponding to a 40% reduction in overall error rate.</a>
<a href="#19" id="19">The model that allows “ function words ” only on the left periphery is more accurate than the model that allows them on both the left and right periphery when the input data ranges from about 100 to about 1,000 sentences, but when the training data is larger than about 1,000 sentences both models are equally accurate.</a>
<a href="#20" id="20">As noted earlier, the “ function word ” model generates function words via adapted nonterminals other than the 𝖶𝗈𝗋𝖽 category.</a>
</body>
</html>