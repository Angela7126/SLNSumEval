<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:210">
</head>
<body bgcolor="white">
<a href="#0" id="0">Then the total number of parameters to be learned for this tensor model is H ⁢∑ d = 1 D n d , which is usually much smaller than V = ∏ d = 1 D n d for a traditional vector space model.</a>
<a href="#1" id="1">Therefore we expect the tensor model to be more effective in a low-resource training environment.</a>
<a href="#2" id="2">By contrast, a linear tensor model only needs to learn H ⁢∑ d = 1 D n d “ bases ” of the m feature weights instead of individual weights directly.</a>
<a href="#3" id="3">The size n d of each tensor mode, d = 1 , … , D , determines the structure of feature weights a tensor model can precisely represent, as well as the number of parameters to estimate (we also assume H = 1 in the analysis below.</a>
<a href="#4" id="4">The initial vectors 𝒘 h , 1 i cannot be made all zero, since otherwise the l -mode product in Equation ( 9 ) would yield all zero Φ h , t d ⁢ ( x , y ) and the model would never get a chance to be updated.</a>
<a href="#5" id="5">Therefore, we initialize the entries of 𝒘 h , 1 i uniformly such that the Frobenius-norm of the weight tensor 𝑾 is unity.</a>
</body>
</html>