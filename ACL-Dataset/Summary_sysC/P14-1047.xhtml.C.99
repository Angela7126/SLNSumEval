<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:545">
</head>
<body bgcolor="white">
<a href="#0" id="0">In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.</a>
<a href="#1" id="1">Building a dialogue policy can be a challenging task especially for complex applications.</a>
<a href="#2" id="2">For this reason, recently much attention has been drawn to machine learning approaches to dialogue management and in particular Reinforcement Learning (RL) of dialogue policies [ 40 , 34 , 22 ].</a>
<a href="#3" id="3">1 1 Though corpora or SUs may still be useful for bootstrapping the policies and encoding real user behavior (see section 6.</a>
<a href="#4" id="4">As we discuss below, concurrent learning could potentially be used for learning via live interaction with human users.</a>
<a href="#5" id="5">Moreover, for negotiation in particular there is one more reason in favor of concurrent learning as opposed to learning against a SU.</a>
<a href="#6" id="6">We vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate.</a>
<a href="#7" id="7">Our research contributions are as follows.</a>
<a href="#8" id="8">1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters.</a>
<a href="#9" id="9">Recently, learning of “ full ” dialogue policies (not just choices at specific points in the dialogue) via live interaction with human users has become possible with the use of Gaussian processes [ 10 , 33 ].</a>
<a href="#10" id="10">Space constraints prevent us from providing an exhaustive list of previous work on using RL for dialogue management.</a>
<a href="#11" id="11">Thus below we focus only on research that is directly related to our work, specifically research on concurrent learning of the policies of multiple agents, and the application of RL to negotiation domains.</a>
<a href="#12" id="12">In either case, one can build a user simulation model that is the average of different user behaviors or learn a policy from a corpus that contains a variety of interaction patterns, and thus safely assume that single-agent RL techniques will work.</a>
<a href="#13" id="13">Because it is very difficult for the agent to know what will happen in the rest of the interaction, the agent must select an action based on the average reward it has previously observed after having performed that action in similar contexts.</a>
<a href="#14" id="14">Table 3 also shows the number of state-action pairs (Q-values.</a>
<a href="#15" id="15">As we will see in section 5 , even though the number of states for each agent is not large, it takes many iterations and high exploration rates for convergence due to the fact that both agents are learning at the same time and the assumption of interacting with a stationary environment no longer holds.</a>
</body>
</html>