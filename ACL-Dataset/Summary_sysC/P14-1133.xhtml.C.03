<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:473">
</head>
<body bgcolor="white">
<a href="#0" id="0">Given an input utterance, we first use a simple method to deterministically generate a set of candidate logical forms with a canonical realization in natural language for each.</a>
<a href="#1" id="1">Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable.</a>
<a href="#2" id="2">Next, we heuristically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB.</a>
<a href="#3" id="3">Given (i) a knowledge base ğ’¦ , and (ii) a training set of question-answer pairs { ( x i , y i ) } i = 1 n , output a semantic parser that maps new questions x to answers y via latent logical forms z.</a>
<a href="#4" id="4">Given an utterance x and the KB, we construct a set of candidate logical forms ğ’µ x , and then for each z âˆˆğ’µ x generate a small set of canonical natural language utterances ğ’ z.</a>
<a href="#5" id="5">Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utterance from it.</a>
<a href="#6" id="6">We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances ( c , z ) , given an utterance x.</a>
<a href="#7" id="7">Where Î˜âˆˆâ„ b is the vector of parameters to be learned, and Î¦â¢ ( x , c , z ) is a feature vector extracted from the input utterance x , the canonical utterance c , and the logical form z.</a>
<a href="#8" id="8">Note that the candidate set of logical forms ğ’µ x and canonical utterances ğ’ x are constructed during the canonical utterance construction phase.</a>
<a href="#9" id="9">Where the parameters Î˜ pr define the paraphrase model (Section 5 ), which is based on features extracted from text only (the input and canonical utterance.</a>
<a href="#10" id="10">We approximate the set of pairs of logical forms and canonical utterances with a beam of size 2,000.</a>
<a href="#11" id="11">Given an input utterance x , we first construct a set of logical forms ğ’µ x , and then generate canonical utterances from each z âˆˆğ’µ x.</a>
<a href="#12" id="12">To construct candidate logical forms ğ’µ x for a given utterance x , our strategy is to find an entity in x and grow the logical form from that entity.</a>
<a href="#13" id="13">Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs ( c , z ) based on a paraphrase model.</a>
<a href="#14" id="14">We ablate the association model, the VS model, and the entire paraphrase model (using only logical form features.</a>
<a href="#15" id="15">Our system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions (Section 4.</a>
<a href="#16" id="16">For example, the logical form ğ‘ [PlaceOfBirth].ElvisPresley would generate the utterance â€œ What location Elvis Presley place of birth â€.</a>
</body>
</html>