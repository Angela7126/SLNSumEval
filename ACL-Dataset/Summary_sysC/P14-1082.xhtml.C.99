<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:550">
</head>
<body bgcolor="white">
<a href="#0" id="0">The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models.</a>
<a href="#1" id="1">A second parameter Λ 2 further limits the considered phrase pairs ( f s , f t ) to have the product of their conditional probabilities not not deviate more than a fraction Λ 2 from the joint probability for the strongest possible pairing for f s , the source fragment f s ⁢ t ⁢ r ⁢ o ⁢ n ⁢ g ⁢ e ⁢ s ⁢ t ⁢ _ ⁢ t in Figure 1 corresponds to the best scoring translation for a given source fragment f s.</a>
<a href="#2" id="2">This metric thus effectively prunes weaker alternative translations in the phrase-translation table from being considered if there is a much stronger candidate.</a>
<a href="#3" id="3">Nevertheless, we hope to show that our automated way of test set generation is sufficient to test the feasibility of our core hypothesis that L1 fragments can be translated to L2 using L2 context information.</a>
<a href="#4" id="4">When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table.</a>
<a href="#5" id="5">If so, we are done quickly and need not rely on context information.</a>
<a href="#6" id="6">If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector.</a>
<a href="#7" id="7">The classifier will return a probability distribution of the most likely translations given the context and we can replace the L1 fragment with the highest scoring L2 translation and present it back to the user.</a>
<a href="#8" id="8">In the current study we simply left both weights set to one, thereby assigning equal importance to translation model and language model.</a>
<a href="#9" id="9">We also compute a recall metric that measures the number of fragments that the system provided a translation for as a fraction of the total number of fragments in the input, regardless of whether the fragment is translated correctly or not.</a>
<a href="#10" id="10">In addition to these, the system ’ s output can be compared against the L2 reference translation(s) using established Machine Translation evaluation metrics.</a>
<a href="#11" id="11">A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier.</a>
<a href="#12" id="12">Among the classifier experts are only words and phrases that are ambiguous and may thus map to multiple translations.</a>
<a href="#13" id="13">This implies that such words and phrases must have occurred at least twice in the corpus, though this threshold is made configurable and could have been set higher to limit the number of classifiers.</a>
<a href="#14" id="14">Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2.</a>
<a href="#15" id="15">We observe in this data that the language model often has the added power to choose a correct translation that is not the first prediction of the classifier, but one of the weaker alternatives that nevertheless fits better.</a>
</body>
</html>