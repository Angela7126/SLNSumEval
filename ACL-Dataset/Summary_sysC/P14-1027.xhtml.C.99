<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:452">
</head>
<body bgcolor="white">
<a href="#0" id="0">We show that a learner can use Bayesian model selection to determine the location of function words in their language, even though the input to the model only consists of unsegmented sequences of phones.</a>
<a href="#1" id="1">Thus our computational models support the hypothesis that function words play a special role in word learning.</a>
<a href="#2" id="2">While absolute accuracy is not directly relevant to the main point of the paper, we note that the models that learn generalisations about function words perform unsupervised word segmentation at 92.5% token f-score on the standard corpus, which improves the previous state-of-the-art by more than 4%.</a>
<a href="#3" id="3">As a reviewer points out, the changes we make to our models to incorporate function words can be viewed as “ building in ” substantive information about possible human languages.</a>
<a href="#4" id="4">Adaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints.</a>
<a href="#5" id="5">As section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy.</a>
<a href="#6" id="6">Section 2.3 presents the major novel contribution of this paper by explaining how we modify these adaptor grammars to capture some of the special properties of function words.</a>
<a href="#7" id="7">The rule ( 3 ) models words as sequences of independently generated phones this is what called the “ monkey model ” of word generation (it instantiates the metaphor that word types are generated by a monkey randomly banging on the keys of a typewriter.</a>
<a href="#8" id="8">For comparison purposes we also include results for a mirror-image model that permits “ function words ” on the right periphery, a model which permits “ function words ” on both the left and right periphery (achieved by changing rules 22 – 24 ), as well as a model that analyses all words as monosyllabic.</a>
<a href="#9" id="9">Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our “ function word ” Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 – 24 ) are replaced with the mirror-image rules in which “ function words ” are attached to the right periphery.</a>
<a href="#10" id="10">Thus, the present model, initially aimed at segmenting words from continuous speech, shows three interesting characteristics that are also exhibited by human infants it distinguishes between function words and content words [] , it allows learners to acquire at least some of the function words of their language (e.g., [] ); and furthermore, it may also allow them to start grouping together function words according to their category [].</a>
</body>
</html>