<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:242">
</head>
<body bgcolor="white">
<a href="#0" id="0">A typical method in these studies is to represent each verb as a single data point and apply classification (e.g.,, Joanis et al.</a>
<a href="#1" id="1">First, we make multiple data points for each verb to deal with verb polysemy (cf polysemy-aware previous studies still represented a verb as one data point [ 14 , 23 ].</a>
<a href="#2" id="2">To do that, we induce verb-specific semantic frames by clustering verb uses.</a>
<a href="#3" id="3">Then, we induce verb classes by clustering these verb-specific semantic frames across verbs.</a>
<a href="#4" id="4">Induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1 , and.</a>
<a href="#5" id="5">Induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.</a>
<a href="#6" id="6">To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic frames.</a>
<a href="#7" id="7">We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb.</a>
<a href="#8" id="8">We can see that “ web/SW-S ” achieved the best performance and obtained a higher F 1 than the baselines by more than nine points “ Web/SW-S ” uses the combination of slot-word pair features for clustering verb-specific frames and slot-only features for clustering across verbs.</a>
<a href="#9" id="9">Interestingly, this result indicates that slot distributions are more effective than lexical information in slot-word pairs for inducing verb classes similar to the gold standard.</a>
</body>
</html>