<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:184">
</head>
<body bgcolor="white">
<a href="#0" id="0">These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not necessarily have a natural correspondence (e.g.,,Â text and images.</a>
<a href="#1" id="1">In this work, we introduce a model, illustrated in Figure 1 , which learns grounded meaning representations by mapping words and images into a common embedding space.</a>
<a href="#2" id="2">We show results for three models, using all attributes except those classified as visual (T), only visual attributes (V), and all available attributes (V+T.</a>
<a href="#3" id="3">9 9 Classification of attributes into categories is provided by in their dataset.</a>
<a href="#4" id="4">As baselines, we also report the performance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concatenation (see row Attributes in Table 2 , and columns T, V, and T+V, respectively.</a>
<a href="#5" id="5">The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE.</a>
<a href="#6" id="6">The third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V.</a>
</body>
</html>