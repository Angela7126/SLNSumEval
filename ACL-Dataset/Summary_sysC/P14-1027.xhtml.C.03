<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:475">
</head>
<body bgcolor="white">
<a href="#0" id="0">Inspired by experimental psychological findings suggesting that function words play a special role in word learning, we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic “ function words ” at the beginnings and endings of collocations of (possibly multi-syllabic) words.</a>
<a href="#1" id="1">Our function word model assumes that function words appear at the left periphery, and while this is true of languages such as English, it is not true universally.</a>
<a href="#2" id="2">We do this by comparing two computational models of word segmentation which differ solely in the way that they model function words.</a>
<a href="#3" id="3">We put “ function words ” in scare quotes below because our model only approximately captures the linguistic properties of function words.</a>
<a href="#4" id="4">This model memoises (i.e.,, learns) both the individual “ function words ” and the sequences of “ function words ” that modify the 𝖢𝗈𝗅𝗅𝗈𝖼𝟣 - 𝖢𝗈𝗅𝗅𝗈𝖼𝟥 constituents.</a>
<a href="#5" id="5">This means that “ function words ” are memoised independently of the “ content words ” that 𝖶𝗈𝗋𝖽 expands to; i.e.,, the model learns distinct “ function word ” and “ content word ” vocabularies.</a>
<a href="#6" id="6">Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our “ function word ” Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 – 24 ) are replaced with the mirror-image rules in which “ function words ” are attached to the right periphery.</a>
<a href="#7" id="7">Here we evaluate the word segmentations found by the “ function word ” Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from.</a>
<a href="#8" id="8">It ’ s interesting that after about 1,000 sentences the model that allows “ function words ” only on the right periphery is considerably less accurate than the baseline model.</a>
<a href="#9" id="9">The model that allows “ function words ” only on the left periphery is more accurate than the model that allows them on both the left and right periphery when the input data ranges from about 100 to about 1,000 sentences, but when the training data is larger than about 1,000 sentences both models are equally accurate.</a>
<a href="#10" id="10">As noted earlier, the “ function word ” model generates function words via adapted nonterminals other than the 𝖶𝗈𝗋𝖽 category.</a>
<a href="#11" id="11">In order to better understand just how the model works, we give the 5 most frequent words in each word category found during 8 MCMC runs of the left-peripheral “ function word ” grammar above.</a>
<a href="#12" id="12">For example, we hoped that given an Adaptor Grammar that permits “ function words ” on both the left and right periphery, the inference procedure would decide that the right-periphery rules simply are not used in a language like English.</a>
</body>
</html>