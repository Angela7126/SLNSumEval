<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:162">
</head>
<body bgcolor="white">
<a href="#0" id="0">For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained.</a>
<a href="#1" id="1">From an information-theoretic point of view [ 38 ] , various problems in text analytics can be formulated as estimating the probability mass/density functions of tokens in text.</a>
<a href="#2" id="2">In NLP, many of the probabilistic text models work in the discrete space [ 9 , 2 ] , but our model is different since the text features are sparse, we first perform kernel density estimates to smooth out the zeroing items, and then calculate the empirical cumulative distribution function (CDF) of the random variables.</a>
<a href="#3" id="3">Note that the above step is also known as probability integral transform [ 11 ] , which allows us to convert any given continuous distribution to random variables having a uniform distribution.</a>
</body>
</html>