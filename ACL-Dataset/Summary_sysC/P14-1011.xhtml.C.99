<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:197">
</head>
<body bgcolor="white">
<a href="#0" id="0">With the learned model, we can accurately measure the semantic similarity between a source phrase and a translation candidate.</a>
<a href="#1" id="1">Accordingly, we evaluate the BRAE model on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to check whether a translation candidate and the source phrase are in the same meaning.</a>
<a href="#2" id="2">In phrase table pruning, we discard the phrasal translation rules with low semantic similarity.</a>
<a href="#3" id="3">In decoding with phrasal semantic similarities, we apply the semantic similarities of the phrase pairs as new features during decoding to guide translation candidate selection.</a>
<a href="#4" id="4">The experiments show that up to 72% of the phrase table can be discarded without significant decrease on the translation quality, and in decoding with phrasal semantic similarities up to 1.7 BLEU score improvement over the state-of-the-art baseline can be achieved.</a>
<a href="#5" id="5">We can make inference from this fact that if a model can learn the same embedding for any phrase pair sharing the same meaning, the learned embedding must encode the semantics of the phrases and the corresponding model is our desire.</a>
<a href="#6" id="6">As translation equivalents share the same semantic meaning, we employ high-quality phrase translation pairs as training corpus in this work.</a>
</body>
</html>