<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:144">
</head>
<body bgcolor="white">
<a href="#0" id="0">We propose a new training objective for learning word embeddings that incorporates prior knowledge.</a>
<a href="#1" id="1">Our model builds on word2vec [] , a neural network based language model that learns word embeddings by maximizing the probability of raw text.</a>
<a href="#2" id="2">We extend the objective to include prior knowledge about synonyms from semantic resources; we consider both the Paraphrase Database [] and WordNet [] , which annotate semantic relatedness between words.</a>
<a href="#3" id="3">The latter was also used in [] for training a network for predicting synset relation.</a>
<a href="#4" id="4">The combined objective maximizes both the probability of the raw corpus and encourages embeddings to capture semantic relations from the resources.</a>
<a href="#5" id="5">We demonstrate improvements in our embeddings on three tasks language modeling, measuring word similarity, and predicting human judgements on word pairs.</a>
<a href="#6" id="6">The goal of our experiments is to demonstrate the value of learning semantic embeddings with information from semantic resources.</a>
</body>
</html>