<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:321">
</head>
<body bgcolor="white">
<a href="#0" id="0">We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens.</a>
<a href="#1" id="1">Our hypothesis is that a model that can even weakly identify recurrent motifs such as ‘ water table ’ or ‘ breaking a fall ’ would be helpful in building more effective semantic representations.</a>
<a href="#2" id="2">We briefly discuss data-driven learning of weights for features that define the motif affinity scores and penalties.</a>
<a href="#3" id="3">We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision.</a>
<a href="#4" id="4">Consider the following sentences tagged by the segmentation model, that would correspond to different representations of the token ‘ remains ’ once as a standalone motif, and once as part of an encompassing bigram motif ( ‘ remains classified ’.</a>
<a href="#5" id="5">With such neighbourhood contexts, the distributional paradigm posits that semantic similarity between a pair of motifs can be given by a sense of ‘ distance ’ between the two distributions.</a>
<a href="#6" id="6">Hence, we use the Hellinger measure between neighbourhood motif distributions in learning representations.</a>
<a href="#7" id="7">We first quantitatively and qualitatively analyze the performance of the segmentation model, and then evaluate the distributional motif representations learnt by the model through two downstream applications.</a>
<a href="#8" id="8">Table 2 shows the performance of the segmentation model with the three proposed learning approaches described earlier.</a>
<a href="#9" id="9">In particular, consider the second example, where the model picks ‘ white elephant ’ as a motif.</a>
<a href="#10" id="10">The above table shows some of the top results for the unary token ‘ elephant ’ by frequency, and frequent unary and non-unary motifs for the motif ‘ white elephant ’ retrieved by the segmentation model.</a>
<a href="#11" id="11">For this task, the motif based distributional embeddings vastly outperform a conventional distributional model (DSM) based on token distributions, as well as additive (AVM) and multiplicative (MVM) models of vector compositionality, as proposed by Lapata et al.</a>
</body>
</html>