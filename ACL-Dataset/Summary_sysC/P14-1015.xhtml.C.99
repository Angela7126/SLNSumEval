<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:237">
</head>
<body bgcolor="white">
<a href="#0" id="0">Being supervised methods, the above assumptions are implicitly factored in by including the appropriate feature (e.g.,, post position in thread) in the feature space so that the learner may learn the correlation (e.g.,, solution posts typically are among the first few posts) using the training data.</a>
<a href="#1" id="1">The only unsupervised approach for the task, that from [ 4 ] , uses a graph propagation method on a graph modeled using posts as vertices, and relies on the assumptions that posts that bear high similarity to the problem and other posts and those authored by authoritative users are more likely to be solution posts.</a>
<a href="#2" id="2">Usage of translation models for modeling the correlation between textual problems and solutions have been explored earlier starting from the answer retrieval work in [ 18 ] where new queries were conceptually expanded using the translation model to improve retrieval.</a>
<a href="#3" id="3">In short, each solution word is assumed to be generated from the language model or the translation model (conditioned on the problem words) with a probability of Λ and 1 - Λ respectively, thus accounting for the correlation assumption.</a>
<a href="#4" id="4">The generative model above is similar to the proposal in [ 5 ] , adapted suitably for our scenario.</a>
<a href="#5" id="5">The IBM Model 1 learning process uses an internal EM approach where the E-step estimates the alignment vector for each problem word; this vector indicates the distribution of alignments of the problem word across the solution words.</a>
</body>
</html>