<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:151">
</head>
<body bgcolor="white">
<a href="#0" id="0">We demonstrate that our embeddings improve over those learned solely on raw text in three settings language modeling, measuring semantic similarity, and predicting human judgements.</a>
<a href="#1" id="1">Based on this resource, we learn embeddings that predict one word from another related word.</a>
<a href="#2" id="2">We define 𝐑 as a set of relations between two words w and w ′𝐑 can contain typed relations (e.g.,, w is related to w ′ through a specific type of semantic relation), and relations can have associated scores indicating their strength.</a>
<a href="#3" id="3">In fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling.</a>
<a href="#4" id="4">Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow.</a>
</body>
</html>