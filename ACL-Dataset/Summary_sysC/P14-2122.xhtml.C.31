<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:177">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, bilingual approaches that model word probabilities suffer from computational complexity.</a>
<a href="#1" id="1">Xu et al.</a>
<a href="#2" id="2">[] proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data.</a>
<a href="#3" id="3">Inspired by [] , we employ a Pitman-Yor process model to build the segmentation model ℳ or ℬ.</a>
<a href="#4" id="4">The monolingual model ℳ is.</a>
<a href="#5" id="5">For the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it.</a>
<a href="#6" id="6">Thus its complexity is U 2 times the unigram model ’ s complexity.</a>
<a href="#7" id="7">Table 4 presents the BLEU scores for Moses using different segmentation methods.</a>
<a href="#8" id="8">Each experiment was performed three times.</a>
<a href="#9" id="9">The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested.</a>
<a href="#10" id="10">We intended to test [] , but found it impracticable on large-scale corpora.</a>
</body>
</html>