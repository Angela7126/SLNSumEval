<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:250">
</head>
<body bgcolor="white">
<a href="#0" id="0">Do continuous word embeddings encode any useful information for constituency parsing.</a>
<a href="#1" id="1">We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.</a>
<a href="#2" id="2">We test each of these hypotheses with a targeted change to a state-of-the-art baseline.</a>
<a href="#3" id="3">Despite small gains on extremely small supervised training sets, we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data.</a>
<a href="#4" id="4">Our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways.</a>
<a href="#5" id="5">It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing.</a>
<a href="#6" id="6">For the experiments in this paper, we will use the Berkeley parser [ 12 ] and the related Maryland parser [ 8 ].</a>
<a href="#7" id="7">The Berkeley parser induces a latent, state-split PCFG in which each symbol V of the (observed) X-bar grammar is refined into a set of more specific symbols { V 1 , V 2 , … } which capture more detailed grammatical behavior.</a>
<a href="#8" id="8">This allows the parser to distinguish between words which share the same tag but exhibit very different syntactic behavior — for example, between articles and demonstrative pronouns.</a>
<a href="#9" id="9">The Maryland parser builds on the state-splitting parser, replacing its basic word emission model with a feature-rich, log-linear representation of the lexicon.</a>
</body>
</html>