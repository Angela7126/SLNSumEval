<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:263">
</head>
<body bgcolor="white">
<a href="#0" id="0">So keeping the verb ’ s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word ’ s occurrence in multiple constructions.</a>
<a href="#1" id="1">Indeed, if we encounter a verb used intransitively which was only attested as transitive in the training corpus, we can simply omit the object matrix to obtain a type-appropriate representation.</a>
<a href="#2" id="2">The plf model performs very well on both anvan benchmarks, outperforming not only add and mult, but also the full-fledged lf model.</a>
<a href="#3" id="3">Given that these data sets contain, systematically, transitive verbs, the major difference between plf and lf lies in their representation of the latter.</a>
<a href="#4" id="4">Evidently, the separately-trained subject and object matrices of plf, being less affected by data sparseness than the 3-way tensors of lf, are better able to capture how verbs interact with their arguments.</a>
<a href="#5" id="5">For anvan1, plf is just below the state of the art, which is based on disambiguating the verb vector in context [ 18 ] , and lf outperforms the baseline, which consists in using the verb vector only as a proxy to sentence similarity.</a>
<a href="#6" id="6">5 5 We report state of the art from Kartsaklis and Sadrzadeh ( 2013 ) rather than Kartsaklis et al.</a>
<a href="#7" id="7">2013 ) , since only the former used a source corpus that is comparable to ours.</a>
<a href="#8" id="8">On anvan2, plf outperforms the best model reported by Grefenstette ( 2013 ) (an implementation of the lexical function ideas along the lines of Grefenstette and Sadrzadeh [ 12 , 13 ].</a>
</body>
</html>