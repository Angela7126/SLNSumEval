<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:139">
</head>
<body bgcolor="white">
<a href="#0" id="0">Previous work on neural word embeddings take the contexts of a word to be its linear context – words that precede and follow the target word, typically in a window of k tokens to each side.</a>
<a href="#1" id="1">In Section 5 we show that the SkipGram model does allow for some introspection by querying it for contexts that are “ activated by ” a target word.</a>
<a href="#2" id="2">In the SkipGram embedding algorithm, the contexts of a word w are the words surrounding it in the text.</a>
<a href="#3" id="3">Using a window of size k around the target word w , 2 ⁢ k contexts are produced the k words before and the k words after w.</a>
<a href="#4" id="4">For k = 2 , the contexts of the target word w are w - 2 , w - 1 , w + 1 , w + 2.</a>
</body>
</html>