<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:452">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this work we present a chance-corrected metric based on Krippendorff ’ s Α , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.</a>
<a href="#1" id="1">To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric ’ s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.</a>
<a href="#2" id="2">1 1 The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/.</a>
<a href="#3" id="3">The definitive reference for agreement measures in computational linguistics is \citeN Art:Poe08, who argue forcefully in favour of the use of chance-corrected measures of agreement over simple accuracy measures.</a>
<a href="#4" id="4">However, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F 1 scores for constituent trees (NEGRA, [] ; TIGER, [] ; Cat3LB, [] ; The Arabic Treebank, [] ) or labelled or unlabelled attachment scores for dependency syntax (PDT, [] ; PCEDT [] ; Norwegian Dependency Treebank, [].</a>
<a href="#5" id="5">The only work we know of using chance-corrected metrics is \citeN Rag:Dic13, who use MASI [] to measure agreement on dependency relations and head selection in multi-headed dependency syntax, and \citeN Bha:Sha12, who compute Cohen ’ s Κ [] on dependency relations in single-headed dependency syntax.</a>
<a href="#6" id="6">For dependency syntax we could generalise these metrics similarly to how Κ is generalised to Κ w to handle partial credit for overlapping annotations.</a>
<a href="#7" id="7">Let the function LAS ⁢ ( t 1 , t 2 ) be the number of tokens with the same head and label in the two trees t 1 and t 2 , T ⁢ ( i ) the set of trees possible for an item i ∈ I , and tokens the number of tokens in the corpus.</a>
<a href="#8" id="8">Then we can compute an expected agreement as follows.</a>
<a href="#9" id="9">∀ x , y Δ⁢ ( x , y ) ≥ 0.</a>
<a href="#10" id="10">∀ x , y Δ⁢ ( x , y ) = Δ⁢ ( x , y.</a>
<a href="#11" id="11">∀ x , y , z Δ⁢ ( x , y ) + Δ⁢ ( y , z ) ≥Δ⁢ ( x , z.</a>
<a href="#12" id="12">When there are more than two annotators, we generalise the metric to be the average pairwise LAS for each sentence, weighted by the length of the sentence.</a>
<a href="#13" id="13">Let LAS ⁢ ( t 1 , t 2 ) be the fraction of tokens with identical head and label in the trees t 1 and t 2 ; the pairwise labelled accuracy LAS p ⁢ ( X ) of a set of annotations X as described in section 1.2 is.</a>
</body>
</html>