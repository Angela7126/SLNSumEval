<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:195">
</head>
<body bgcolor="white">
<a href="#0" id="0">Therefore, we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases.</a>
<a href="#1" id="1">However, no gold semantic phrase embedding exists.</a>
<a href="#2" id="2">1 reconstruction error E r ⁢ e ⁢ c ⁢ ( s , t ; Θ how well the learned vector representations p s and p t represent the phrase s and t respectively.</a>
<a href="#3" id="3">With the semantic phrase embeddings and the vector space transformation function, we apply the BRAE to measure the semantic similarity between a source phrase and its translation candidates in the phrase-based SMT.</a>
<a href="#4" id="4">Given a phrase pair ( s , t ) , the BRAE model first obtains their semantic phrase representations ( p s , p t ) , and then transforms p s into target semantic space p s * , p t into source semantic space p t *.</a>
<a href="#5" id="5">To have a better intuition about the power of the BRAE model at learning semantic phrase embeddings, we show some examples in Table 3.</a>
<a href="#6" id="6">This indicates that the proposed BRAE model is effective at learning semantic phrase embeddings.</a>
<a href="#7" id="7">Therefore, our model can be easily adapted to learn semantic phrase embeddings using paraphrases.</a>
</body>
</html>