<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:630">
</head>
<body bgcolor="white">
<a href="#0" id="0">This approach to filler-gap comprehension is supported by findings that show people do not actually link fillers to gap positions but instead link the filler to a verb with missing arguments [] Further, the model presented here is also shown to initially reflect an idiosyncratic role assignment error observed in development (e.g., A and B kradded interpreted as A kradded B ; Gertner and Fisher, 2012 ), though after training, the model is able to avoid the error.</a>
<a href="#1" id="1">BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work.</a>
<a href="#2" id="2">The model presented here learns a single, non-recursive ordering for the semantic roles in each sentence relative to the verb since several studies have suggested that early child grammars may consist of simple linear grammars that are dictated by semantic roles [].</a>
<a href="#3" id="3">The model represents the preferred locations of semantic roles relative to the verb as distributions over real numbers.</a>
<a href="#4" id="4">Finally, the one-to-one role bias is explicitly encoded such that the model cannot use a label that has already been used elsewhere in the sentence.</a>
<a href="#5" id="5">The model in this work is trained using transcribed child-directed speech (CDS) from the BabySRL portions [] of CHILDES [].</a>
<a href="#6" id="6">During the Expectation step, the model uses the current Gaussian parameters to label the nouns in each sentence with argument roles.</a>
<a href="#7" id="7">Since the model is not lexicalized, these roles correspond to the semantic roles most commonly associated with subject and object.</a>
<a href="#8" id="8">Since the model is unsupervised, it is trained on a given corpus (e.g., Eve) before being tested on the role annotations of that same corpus.</a>
<a href="#9" id="9">The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles.</a>
<a href="#10" id="10">The overall results of the filler-gap evaluation (see Table 4 ) indicate that the model improves significantly at parsing filler-gap constructions after training.</a>
<a href="#11" id="11">The acquisition of semantic role labelling (SRL) by the BabySRL model [] bears many similarities to the current work and is, to our knowledge, the only comparable line of inquiry to the current one.</a>
<a href="#12" id="12">The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary.</a>
<a href="#13" id="13">Similar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments.</a>
<a href="#14" id="14">This section will demonstrate that the model in this paper initially reflects 1-1 role bias comparably to BabySRL, though it progresses beyond this bias after training.</a>
<a href="#15" id="15">Further, the model in this paper is able to reflect the concurrent acquisition of filler-gap whereas BabySRL does not seem well-suited to such a task.</a>
<a href="#16" id="16">Finally, BabySRL performs undesirably in intransitive settings whereas the model in this paper does not.</a>
<a href="#17" id="17">The results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model.</a>
<a href="#18" id="18">The 1-1 role bias error rate (before training) of the model presented in this paper is comparable to that of and , which shows that the current model provides comparable developmental modeling benefits to the BabySRL models.</a>
<a href="#19" id="19">Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected.</a>
<a href="#20" id="20">Further, the unsupervised model in this paper initially reflects developmental 1-1 role bias as well as the supervised BabySRL models, and it is able to progress beyond this bias.</a>
</body>
</html>