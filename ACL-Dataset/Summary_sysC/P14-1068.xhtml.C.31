<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:182">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper we address the problem of grounding distributional representations of lexical meaning.</a>
<a href="#1" id="1">We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input.</a>
<a href="#2" id="2">The two modalities are encoded as vectors of attributes and are obtained automatically from text and images, respectively.</a>
<a href="#3" id="3">We evaluate our model on its ability to simulate similarity judgments and concept categorization.</a>
<a href="#4" id="4">Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1 , respectively.</a>
<a href="#5" id="5">We also compare our model against two approaches that differ in their fusion mechanisms.</a>
<a href="#6" id="6">The first one is based on kernelized canonical correlation (kCCA, ) with a linear kernel which was the best performing model in.</a>
<a href="#7" id="7">The second one emulates Bruni et al â€™ s ( ) fusion mechanism.</a>
<a href="#8" id="8">Specifically, we concatenate the textual and visual vectors and project them onto a lower dimensional latent space using SVD.</a>
<a href="#9" id="9">All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations.</a>
</body>
</html>