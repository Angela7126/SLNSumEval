<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:235">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this section we analyze the games in terms of participation and player ’ s ability to correctly play.</a>
<a href="#1" id="1">Players completed over 1388 games during the study period.</a>
<a href="#2" id="2">The paid and free versions of TKT had similar numbers of players, while the paid version of Infection attracted nearly twice the players compared to the free version, shown in Table 1 , Column 1.</a>
<a href="#3" id="3">However, both versions created approximately the same number of annotations, shown in Column 2.</a>
<a href="#4" id="4">Surprisingly, SuchGame received little attention, with only a few players completing a full round of game play.</a>
<a href="#5" id="5">We believe this emphasizes the strength of video game-based annotation; adding incentives and game-like features to an annotation task will not necessarily increase its appeal.</a>
<a href="#6" id="6">This section assesses the annotation quality of both games and of CrowdFlower in terms of (1) the IAA of the participants, measured using Krippendorff ’ s Α , and (2) the percentage agreement of the resulting annotations with the gold standard.</a>
<a href="#7" id="7">Players in both free and paid games had similar IAA, though the free version is consistently higher (Table 1 , Col. 4.</a>
<a href="#8" id="8">3 3 In conversations with players after the contest ended, several mentioned that being aware their play was contributing to research motivated them to play more accurately.</a>
<a href="#9" id="9">For images, crowdsourcing workers have a higher IAA than game players; however, this increased agreement is due to adversarial workers consistently selecting the same, incorrect answer.</a>
</body>
</html>