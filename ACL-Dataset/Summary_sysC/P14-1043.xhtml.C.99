<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:192">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training.</a>
<a href="#1" id="1">Instead of only using 1-best parse trees in previous work, our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data.</a>
<a href="#2" id="2">The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data apply a variant of co-training to dependency parsing and report positive results on out-of-domain text combine tri-training and parser ensemble to boost parsing accuracy.</a>
<a href="#3" id="3">Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical.</a>
<a href="#4" id="4">During experimental trials, we find that “ Unlabeled ← B+(Z ∩ G) ” can further boost performance on Chinese.</a>
<a href="#5" id="5">A possible explanation is that by using the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser.</a>
</body>
</html>