<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:193">
</head>
<body bgcolor="white">
<a href="#0" id="0">To the best of our knowledge, we are the first to apply this technique on relation extraction with distant supervision.</a>
<a href="#1" id="1">More specifically, as shown in Figure 2, we model the task with a sparse matrix whose rows present items (entity pairs) and columns contain noisy textual features and incomplete relation labels.</a>
<a href="#2" id="2">In such a way, relation classification is transformed into a problem of completing the unknown labels for testing items in the sparse matrix that concatenates training and testing textual features with training labels, based on the assumption that the item-by-feature and item-by-label joint matrix is of low rank.</a>
<a href="#3" id="3">In practice, we record the rank of matrix Z at each round of iteration until it converges at a rather small threshold Î• = 10 - 4.</a>
<a href="#4" id="4">The reason is that we suppose the optimal low-rank representation of the matrix Z conveys the truly effective information about underlying semantic correlation between the features and the corresponding labels.</a>
<a href="#5" id="5">In other words, for each approach, the amount of truly effective information about underlying semantic correlation keeps constant for the same dataset, which, to some extent, explains the reason why our approaches are robust to sparse features.</a>
</body>
</html>