<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:130">
</head>
<body bgcolor="white">
<a href="#0" id="0">This is the context used by word2vec and many other neural embeddings.</a>
<a href="#1" id="1">Using a window of size k around the target word w , 2 ⁢ k contexts are produced the k words before and the k words after w.</a>
<a href="#2" id="2">Notice that syntactic dependencies are both more inclusive and more focused than bag-of-words.</a>
<a href="#3" id="3">They capture relations to words that are far apart and thus “ out-of-reach ” with small window bag-of-words (e.g., the instrument of discover is telescope/prep_with ), and also filter out “ coincidental ” contexts which are within the window but not directly related to the target word (e.g., Australian is not used as the context for discovers.</a>
<a href="#4" id="4">In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientist s are subjects.</a>
</body>
</html>