<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:455">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, no such measure is in widespread use for the task of syntactic annotation.</a>
<a href="#1" id="1">This is due to a mismatch between the formulation of the agreement measures, which assumes that the annotations have no or relatively little internal structure, and syntactic annotation where structure is the entire point of the annotation.</a>
<a href="#2" id="2">For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures.</a>
<a href="#3" id="3">As shown in \citeN Art:Poe08, such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent.</a>
<a href="#4" id="4">These metrics express agreement on a nominal coding task as the ratio Κ , Π = A o - A e / 1 - A e where A o is the observed agreement and A e the expected agreement according to some model of “ random ” annotation.</a>
<a href="#5" id="5">Tree edit distance has previously been used in the TedEval software [] for parser evaluation agnostic to both annotation scheme and theoretical framework, but this by itself is still an uncorrected accuracy measure and thus unsuitable for our purposes.</a>
<a href="#6" id="6">3 3 While it is quite different from other parser evaluation schemes, TedEval does not correct for chance agreement and is thus an uncorrected metric.</a>
<a href="#7" id="7">It could of course form the basis for a corrected metric, given a suitable measure of expected agreement.</a>
<a href="#8" id="8">The results of these experiments are shown in Figure 3 , with the labelled attachment score 6 6 The de facto standard parser evaluation metric in dependency parsing the percentage of tokens that receive the correct head and dependency relation.</a>
<a href="#9" id="9">A ∪ B and we use the Jaccard similarity of the sets of labelled bracketings of two trees as our uncorrected measure.</a>
<a href="#10" id="10">To compute the similarity for a complete set of annotations we use the mean pairwise Jaccard similarity weighted by sentence length; that is, the same procedure as in 3 , but using Jaccard similarity rather than LAS.</a>
<a href="#11" id="11">Since LAS assumes that both of the sentences compared have identical sets of tokens, we had to exclude a number of sentences from the LAS computation in the cases of the English and Italian CDT corpora, and especially the PCEDT.</a>
<a href="#12" id="12">The reason the PCEDT gets such low LAS is essentially the same as the reason many sentences had to be excluded from the computation in the first place; since inserting and deleting nodes is an integral part of the tectogrammatical annotation task, the assumption implicit in the LAS computation that sentences with the same number of nodes have the same nodes in the same order is obviously false, resulting in a very low LAS.</a>
</body>
</html>