<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:166">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples.</a>
<a href="#1" id="1">This accuracy is so high that current research has shifted to related more challenging problems language variety identification [ 26 ] , native language identification [ 24 ] and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths [ 1 ].</a>
<a href="#2" id="2">More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length.</a>
<a href="#3" id="3">Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing.</a>
</body>
</html>