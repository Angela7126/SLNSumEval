<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:650">
</head>
<body bgcolor="white">
<a href="#0" id="0">The closest work to that presented here is the work on BabySRL [].</a>
<a href="#1" id="1">BabySRL is a computational model of semantic role acquistion using a similar set of assumptions to the current work.</a>
<a href="#2" id="2">BabySRL learns weights over ordering constraints (e.g., preverbal, second noun, etc.) to acquire semantic role labelling while still exhibiting 1-1 role bias.</a>
<a href="#3" id="3">However, no analysis has evaluated the ability of BabySRL to acquire filler-gap constructions.</a>
<a href="#4" id="4">Further comparison to BabySRL may be found in Section 6.</a>
<a href="#5" id="5">For testing, this study uses the semantic role annotations in the BabySRL corpus.</a>
<a href="#6" id="6">These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of before roughly hand-correcting them [].</a>
<a href="#7" id="7">The BabySRL corpus is annotated with 5 different roles, but the model described in this paper only uses 2 roles.</a>
<a href="#8" id="8">Therefore, overall accuracy results (see Table 3 ) are presented both for the raw BabySRL corpus and for a collapsed BabySRL corpus where all non-agent roles are collapsed into a single role (denoted by a subscript c in all tables.</a>
<a href="#9" id="9">The acquisition of semantic role labelling (SRL) by the BabySRL model [] bears many similarities to the current work and is, to our knowledge, the only comparable line of inquiry to the current one.</a>
<a href="#10" id="10">The primary function of BabySRL is to model the acquisition of semantic role labelling while making an idiosyncratic error which infants also make [] , the 1-1 role bias error ( John and Mary gorped interpreted as John gorped Mary.</a>
<a href="#11" id="11">Similar to the model presented in this paper, BabySRL is based on simple ordering features such as argument position relative to the verb and argument position relative to the other arguments.</a>
<a href="#12" id="12">And run direct analyses of how frequently their models make 1-1 role bias errors.</a>
<a href="#13" id="13">A comparable evaluation may be run on the current model by generating 1000 sentences with a structure of NNV and reporting how many times the model chooses a subject-first labelling (see Table 6.</a>
<a href="#14" id="14">11 11 While Table 6 analyzes erroneous labellings of NNV structure, the ‘ Obj ’ column of Table 5 (Left) shows model accuracy on NNV structures.</a>
<a href="#15" id="15">The results of and depend on whether BabySRL uses argument-argument relative position as a feature or argument-verb relative position as a feature (there is no combined model.</a>
<a href="#16" id="16">Further, the model presented here from has a unique argument constraint, similar to the model in this paper, in order to make comparison as direct as possible.</a>
<a href="#17" id="17">The 1-1 role bias error rate (before training) of the model presented in this paper is comparable to that of and , which shows that the current model provides comparable developmental modeling benefits to the BabySRL models.</a>
<a href="#18" id="18">Further, similar to real children (see Figure 1 ) the model presented in this paper develops beyond this error by the end of its training, 12 12 It is important to note that the unique argument constraint prevents the current model from actually getting the correct, conjoined-subject parse, but it no longer exhibits agent-first bias, an important step for acquiring passives, which occurs between 3 and 4 years [] whereas the BabySRL models still make this error after training.</a>
<a href="#19" id="19">The overall reason for the different results between the current work and BabySRL is that BabySRL relies on positional features that measure the relative position of two individual elements (e.g., where a given noun is relative to the verb.</a>
<a href="#20" id="20">Since the model in this paper operates over global orderings, it implicitly takes into account the positions of other nouns as it models argument position relative to the verb; object and subject are in competition as labels for preverbal nouns, so a preverbal object is usually only assigned once a subject has already been detected.</a>
<a href="#21" id="21">In sum, the unlexicalized model presented in this paper is able to achieve greater labelling accuracy than the lexicalized BabySRL models in intransitive settings, though this model does perform slightly worse in the less common transitive setting.</a>
</body>
</html>