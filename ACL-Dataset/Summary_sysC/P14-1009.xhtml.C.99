<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:244">
</head>
<body bgcolor="white">
<a href="#0" id="0">2010 ) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model.</a>
<a href="#1" id="1">A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.</a>
<a href="#2" id="2">The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ].</a>
<a href="#3" id="3">In lf, arguments are vectors and functions taking arguments (e.g.,, adjectives that combine with nouns) are tensors, with the number of arguments (n) determining the order of tensor (n+1.</a>
<a href="#4" id="4">To model passive usages, we insert the object matrix of the verb only, which will be multiplied by the syntactic subject vector, capturing the similarity between eat meat and meat is eaten.</a>
<a href="#5" id="5">So keeping the verb ’ s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word ’ s occurrence in multiple constructions.</a>
<a href="#6" id="6">Following standard practice in paraphrase detection studies (e.g.,, Blacoe and Lapata ( 2012 ) ), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues word overlap between the two sentences and difference in sentence length.</a>
</body>
</html>