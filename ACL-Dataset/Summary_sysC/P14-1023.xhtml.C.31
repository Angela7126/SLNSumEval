<html>
<head>
<meta name="TextLength" content="SENT_NUM:28, WORD_NUM:651">
</head>
<body bgcolor="white">
<a href="#0" id="0">The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window.</a>
<a href="#1" id="1">We considered context windows of 2 and 5 words to either side of the central element.</a>
<a href="#2" id="2">We vary vector dimensionality within the 200 to 500 range in steps of 100.</a>
<a href="#3" id="3">The word2vec toolkit implements two efficient alternatives to the standard computation of the output word probability distributions by a softmax classifier.</a>
<a href="#4" id="4">Hierarchical softmax is a computationally efficient way to estimate the overall probability distribution using an output layer that is proportional to l o g ( u n i g r a m p e r p l e x i t y ( W ) ) instead of W (for W the vocabulary size.</a>
<a href="#5" id="5">As an alternative, negative sampling estimates the probability of an output word by learning to distinguish it from draws from a noise distribution.</a>
<a href="#6" id="6">The number of these draws (number of negative samples ) is given by a parameter k.</a>
<a href="#7" id="7">We test both hierarchical softmax and negative sampling with k values of 5 and 10.</a>
<a href="#8" id="8">Very frequent words such as the or a are not very informative as context features.</a>
<a href="#9" id="9">The word2vec toolkit implements a method to downsize their effect (and simultaneously improve speed performance.</a>
<a href="#10" id="10">More precisely, words in the training data are discarded with a probability that is proportional to their frequency (capturing the same intuition that motivates traditional count vector weighting measures such as PMI.</a>
<a href="#11" id="11">This is controlled by a parameter t and words that occur with higher frequency than t are aggressively subsampled.</a>
<a href="#12" id="12">We train models without subsampling and with subsampling at t = 1 ⁢ e - 5 (the toolkit page suggests 1 ⁢ e - 3 - 1 ⁢ e - 5 as a useful range based on empirical observations.</a>
<a href="#13" id="13">Table 2 summarizes the evaluation results.</a>
<a href="#14" id="14">The first block of the table reports the maximum per-task performance (across all considered parameter settings) for count and predict vectors.</a>
<a href="#15" id="15">The latter emerge as clear winners, with a large margin over count vectors in most tasks.</a>
<a href="#16" id="16">Indeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more.</a>
<a href="#17" id="17">It is worth stressing that, as reviewed in Section 3 , the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on external knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning.</a>
<a href="#18" id="18">Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers.</a>
<a href="#19" id="19">The success of the predict models cannot be blamed on poor performance of the count models.</a>
<a href="#20" id="20">Besides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms.</a>
<a href="#21" id="21">Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count-based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci ( 2010 ).</a>
<a href="#22" id="22">Finally, we go back to Table 2 to point out the poor performance of the out-of-the-box cw model.</a>
<a href="#23" id="23">We must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus window size objective function being used subsampling … ) to further work.</a>
<a href="#24" id="24">Still, our results show that it ’ s not just training by context prediction that ensures good performance.</a>
<a href="#25" id="25">The cw approach is very popular (for example both Huang et al.</a>
<a href="#26" id="26">2012 ) and Blacoe and Lapata ( 2012 ) used it in the studies we discussed in Section 1.</a>
<a href="#27" id="27">Had we also based our systematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors.</a>
</body>
</html>