<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:534">
</head>
<body bgcolor="white">
<a href="#0" id="0">We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario.</a>
<a href="#1" id="1">Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from.</a>
<a href="#2" id="2">In particular, we compare the Q-learning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate.</a>
<a href="#3" id="3">Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly.</a>
<a href="#4" id="4">We also show that very high gradually decreasing exploration rates are required for convergence.</a>
<a href="#5" id="5">We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.</a>
<a href="#6" id="6">Multi-agent RL is designed to work for non-stationary environments.</a>
<a href="#7" id="7">We apply multi-agent RL to a resource allocation negotiation scenario.</a>
<a href="#8" id="8">Two agents with different preferences negotiate about how to share resources.</a>
<a href="#9" id="9">We compare Q-learning (a single-agent RL algorithm) with two multi-agent RL algorithms.</a>
<a href="#10" id="10">Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) [ 3 ].</a>
<a href="#11" id="11">We vary the scenario complexity (i.e.,, the quantity of resources to be shared and consequently the state space size), the number of training episodes, the learning rate, and the exploration rate.</a>
<a href="#12" id="12">Our research contributions are as follows.</a>
<a href="#13" id="13">1) we propose concurrent learning using multi-agent RL as a way to deal with some of the issues of current approaches to dialogue policy learning (i.e.,, the need for SUs and corpora), which may also potentially prove useful for learning via live interaction with human users; (2) we show that concurrent learning can address changes in user behavior over time, and requires multi-agent RL techniques and variable exploration rates; (3) to our knowledge this is the first time that PHC and PHC-WoLF are used for learning dialogue policies; (4) for the first time, the above techniques are applied to a negotiation domain; and (5) this is the first study that compares Q-learning, PHC, and PHC-WoLF in such a variety of situations (varying a large number of parameters.</a>
<a href="#14" id="14">Agent 1 offer-2-2 (I offer you 2 A and 2 O.</a>
<a href="#15" id="15">[1ex] Agent 2 offer-3-0 (I offer you 3 A and 0 O.</a>
<a href="#16" id="16">[1ex] Agent 1 offer-0-3 (I offer you 0 A and 3 O.</a>
<a href="#17" id="17">In this section we report results with different exploration rates per training epoch (see section 4.</a>
<a href="#18" id="18">Table 4 shows the average distance from the convergence reward over 20 runs for 100,000 episodes per epoch, for different numbers of fruits, and for all four methods (Q-learning, PHC-LF, PHC-W, and PHC-WoLF.</a>
<a href="#19" id="19">It is clear that as the state space becomes larger 100,000 training episodes per epoch are not enough for convergence.</a>
<a href="#20" id="20">Also, for 1, 2, and 3 fruits all algorithms converge and perform comparably.</a>
<a href="#21" id="21">As the number of fruits increases, Q-learning starts performing worse than the multi-agent RL algorithms.</a>
<a href="#22" id="22">For 7 fruits PHC-W appears to perform worse than Q-learning but this is because, as we can see in Figure 5 , in this case more than 400,000 episodes per epoch are required for convergence.</a>
<a href="#23" id="23">Thus after only 100,000 episodes per epoch all policies still behave somewhat randomly.</a>
</body>
</html>