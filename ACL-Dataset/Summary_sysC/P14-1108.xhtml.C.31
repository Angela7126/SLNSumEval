<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:153">
</head>
<body bgcolor="white">
<a href="#0" id="0">Interpolation with lower order models is motivated by the problem of data sparsity in higher order models.</a>
<a href="#1" id="1">However, lower order models omit only the first word in the local context, which might not necessarily be the cause for the overall n -gram to be rare.</a>
<a href="#2" id="2">This is the motivation for our generalized language models to not only interpolate with one lower order model, where the first word in a sequence is omitted, but also with all other skip n -gram models, where one word is left out.</a>
<a href="#3" id="3">We see that the GLM performs particularly well on small training data.</a>
<a href="#4" id="4">As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 ‚Å¢ % compared to language models with modified Kneser-Ney smoothing on the same data set.</a>
<a href="#5" id="5">The absolute perplexity values for this experiment are presented in Table 4.</a>
</body>
</html>