<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:147">
</head>
<body bgcolor="white">
<a href="#0" id="0">Word-level vector space embeddings have so far had limited impact on parsing performance.</a>
<a href="#1" id="1">From a computational perspective, adding non-sparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g.,, dependency arc.</a>
<a href="#2" id="2">Because of this issue, Cirik and Ş ensoy ( 2013 ) used word vectors only as unigram features (without combinations) as part of a shift reduce parser [ 32 ].</a>
<a href="#3" id="3">We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U ⁢Φ h , V ⁢Φ m and W ⁢Φ h , m rather than explicitly calculate the feature tensor Φ h ⊗Φ m ⊗Φ h , m.</a>
<a href="#4" id="4">Therefore updating parameters and decoding a sentence is still efficient, i.e.,, linear in the number of values of the feature vector.</a>
</body>
</html>