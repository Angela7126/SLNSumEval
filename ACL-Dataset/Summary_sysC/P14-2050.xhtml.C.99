<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:132">
</head>
<body bgcolor="white">
<a href="#0" id="0">Previous work on neural word embeddings take the contexts of a word to be its linear context â€“ words that precede and follow the target word, typically in a window of k tokens to each side.</a>
<a href="#1" id="1">The context vocabulary C is thus identical to the word vocabulary W.</a>
<a href="#2" id="2">However, this restriction is not required by the model; contexts need not correspond to words, and the number of context-types can be substantially larger than the number of word-types.</a>
<a href="#3" id="3">In particular, we found that Deps perform dramatically worse than BoW contexts on analogy tasks as in [ 22 , 17 ].</a>
<a href="#4" id="4">Neural word embeddings are often considered opaque and uninterpretable, unlike sparse vector space representations in which each dimension corresponds to a particular known context, or LDA models where dimensions correspond to latent topics.</a>
</body>
</html>