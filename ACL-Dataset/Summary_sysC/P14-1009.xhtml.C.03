<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:262">
</head>
<body bgcolor="white">
<a href="#0" id="0">A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.</a>
<a href="#1" id="1">The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ].</a>
<a href="#2" id="2">For example, if noun meanings are encoded in vectors of 300 dimensions, adjectives become matrices of 300 2 cells, and transitive verbs are represented as tensors with 300 3 = 27 , 000 , 000 dimensions.</a>
<a href="#3" id="3">Indeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al.</a>
<a href="#4" id="4">In plf, a functional word is not represented by a single tensor of arity-dependent order, but by a vector plus an ordered set of matrices, with one matrix for each argument the function takes.</a>
<a href="#5" id="5">For transitive verbs semantic composition applies iteratively as shown in the derivation of Figure 2.</a>
<a href="#6" id="6">So keeping the verb ’ s interaction with subject and object encoded in distinct matrices not only solves the issues of representation size for arbitrary semantic types, but also provides a sensible built-in strategy for handling a word ’ s occurrence in multiple constructions.</a>
<a href="#7" id="7">The add (additive) model produces the vector of a sentence by summing the vectors of all content words in it.</a>
<a href="#8" id="8">We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences.</a>
</body>
</html>