<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:216">
</head>
<body bgcolor="white">
<a href="#0" id="0">Models that learn semantic representations from both linguistic and perceptual input outperform text-only models in many contexts and better reflect human concept acquisition.</a>
<a href="#1" id="1">Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system [ 3 ].</a>
<a href="#2" id="2">Multi-modal models in which perceptual input is filtered according to our algorithm learn higher-quality semantic representations than previous approaches, resulting in a significant performance improvement of up to 17% in capturing the semantic similarity of concepts.</a>
<a href="#3" id="3">The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for conceptual concreteness by at least 10 human annotators.</a>
<a href="#4" id="4">We apply image dispersion-based filtering as follows if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included.</a>
<a href="#5" id="5">The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts.</a>
<a href="#6" id="6">To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A âˆª C introduced in Section 2.</a>
</body>
</html>