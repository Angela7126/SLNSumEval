<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:270">
</head>
<body bgcolor="white">
<a href="#0" id="0">Since the quality standards of individual users may vary considerably ( e.g., according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user;.</a>
<a href="#1" id="1">On one side, the QE models obtained with batch methods are learned exclusively from a predefined set of training examples under the assumption that they have similar characteristics with respect to the test data.</a>
<a href="#2" id="2">Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework.</a>
<a href="#3" id="3">Among these, the necessity to model the diversity of human quality judgements and correction strategies [ 16 , 15 ] calls for solutions that i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items.</a>
<a href="#4" id="4">This suggests that, although PA is potentially capable of achieving higher results and better adapt to the new test points, its instability makes it less reliable for practical use.</a>
<a href="#5" id="5">As a final analysis of our results, we investigated how the performance of the different types of models ( batch , adaptive , empty ) relates to the distance between training and test sets.</a>
<a href="#6" id="6">When this distance is minimal, batch models can be a reasonable option, but when the gap between training and test data increases, adaptive or empty models are a preferable choice to achieve good results.</a>
</body>
</html>