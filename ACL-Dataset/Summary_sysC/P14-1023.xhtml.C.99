<html>
<head>
<meta name="TextLength" content="SENT_NUM:18, WORD_NUM:650">
</head>
<body bgcolor="white">
<a href="#0" id="0">In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,, co-occurring words) in which target terms appear in a large corpus as proxies for meaning representations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words [ 13 , 16 , 45 ].</a>
<a href="#1" id="1">1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet Allocation (LDA) models [ 8 , 21 ] , where parameters are set to optimize the joint probability distribution of words and documents.</a>
<a href="#2" id="2">This is in part due to the fact that context-predicting vectors were first developed as an approach to language modeling and/or as a way to initialize feature vectors in neural-network-based “ deep learning ” NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesting side effect.</a>
<a href="#3" id="3">Blacoe and Lapata ( 2012 ) compare count and predict representations as input to composition functions.</a>
<a href="#4" id="4">Count vectors make for better inputs in a phrase similarity task, whereas the two representations are comparable in a paraphrase classification experiment.</a>
<a href="#5" id="5">In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks.</a>
<a href="#6" id="6">Count models have such a long and rich history that we can only explore a small subset of the counting, weighting and compressing methods proposed in the literature.</a>
<a href="#7" id="7">However, it is worth pointing out that the evaluated parameter subset encompasses settings (narrow context window, positive PMI, SVD reduction) that have been found to be most effective in the systematic explorations of the parameter space conducted by Bullinaria and Levy [ 10 , 11 ].</a>
<a href="#8" id="8">Following previous art, we tackle categorization as an unsupervised clustering task.</a>
<a href="#9" id="9">The vectors produced by a model are clustered into n groups (with n determined by the gold standard partition) using the CLUTO toolkit [ 26 ] , with the repeated bisections with global optimization method and CLUTO ’ s default settings otherwise (these are standard choices in the literature.</a>
<a href="#10" id="10">The latter emerge as clear winners, with a large margin over count vectors in most tasks.</a>
<a href="#11" id="11">Indeed, the predictive models achieve an impressive overall performance, beating the current state of the art in several cases, and approaching it in many more.</a>
<a href="#12" id="12">Besides the fact that this would not explain the near-state-of-the-art performance of the predict vectors, the count model results are actually quite good in absolute terms.</a>
<a href="#13" id="13">A more cogent and interesting evaluation is reported in the third block of Table 2 , where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an experimenter might be tempted to choose without tuning.</a>
<a href="#14" id="14">51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models performs much better than the best one, confirming our suspicion about the brittleness of this small data set.</a>
<a href="#15" id="15">Tables 3 and 4 let us take a closer look at the most important count and predict parameters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes.</a>
<a href="#16" id="16">For the predict models, we observe in Table 4 that negative sampling, where the task is to distinguish the target output word from samples drawn from the noise distribution, outperforms the more costly hierarchical softmax method.</a>
<a href="#17" id="17">We must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus window size objective function being used subsampling … ) to further work.</a>
</body>
</html>