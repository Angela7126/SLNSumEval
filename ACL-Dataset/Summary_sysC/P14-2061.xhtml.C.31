<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:168">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our contributions a ) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages.</a>
<a href="#1" id="1">More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification b ) demonstrate the impact on performance of varying the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length.</a>
<a href="#2" id="2">Our method performs two important tasks.</a>
<a href="#3" id="3">First, it learns a feature representation from patches of unlabelled raw video data [ 12 , 4 ].</a>
<a href="#4" id="4">Second, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages.</a>
<a href="#5" id="5">Given samples of sign language videos (unknown sign language with one signer per video), our system performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing.</a>
</body>
</html>