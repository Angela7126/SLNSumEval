<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:248">
</head>
<body bgcolor="white">
<a href="#0" id="0">Do continuous word embeddings encode any useful information for constituency parsing.</a>
<a href="#1" id="1">We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.</a>
<a href="#2" id="2">This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.</a>
<a href="#3" id="3">It has been less clear how (and indeed whether) word embeddings in and of themselves are useful for constituency parsing.</a>
<a href="#4" id="4">There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson ( 2004 ) , and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al.</a>
<a href="#5" id="5">With extremely limited training data, parser extensions using word embeddings give modest improvements in accuracy (relative error reduction on the order of 1.5%.</a>
<a href="#6" id="6">It seems clear that word embeddings exhibit some syntactic structure.</a>
<a href="#7" id="7">How much data is needed to learn them without word embeddings.</a>
<a href="#8" id="8">For OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser.</a>
<a href="#9" id="9">Each Α t , w is learned in the same way as its corresponding probability in the original parser model — during each M step of the training procedure, Α w , t is set to the expected number of times the word w appears under the refined tag t.</a>
</body>
</html>