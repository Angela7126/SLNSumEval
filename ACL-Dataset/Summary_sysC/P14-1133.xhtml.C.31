<html>
<head>
<meta name="TextLength" content="SENT_NUM:19, WORD_NUM:476">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1.</a>
<a href="#1" id="1">Our approach targets factoid questions with a modest amount of compositionality.</a>
<a href="#2" id="2">Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable.</a>
<a href="#3" id="3">Next, we heuristically generate canonical utterances for each logical form based on the text descriptions of predicates from the KB.</a>
<a href="#4" id="4">Finally, we choose the canonical utterance that best paraphrases the input utterance, and thereby the logical form that generated it.</a>
<a href="#5" id="5">We use two complementary paraphrase models an association model based on aligned phrase pairs extracted from a monolingual parallel corpus, and a vector space model , which represents each utterance as a vector and learns a similarity score between them.</a>
<a href="#6" id="6">The entire system is trained jointly from question-answer pairs only.</a>
<a href="#7" id="7">Given an utterance x and the KB, we construct a set of candidate logical forms 𝒵 x , and then for each z ∈𝒵 x generate a small set of canonical natural language utterances 𝒞 z.</a>
<a href="#8" id="8">Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utterance from it.</a>
<a href="#9" id="9">This strategy is feasible in factoid QA where compositionality is low, and so the size of 𝒵 x is limited (Section 4.</a>
<a href="#10" id="10">Where Θ∈ℝ b is the vector of parameters to be learned, and Φ⁢ ( x , c , z ) is a feature vector extracted from the input utterance x , the canonical utterance c , and the logical form z.</a>
<a href="#11" id="11">Note that the candidate set of logical forms 𝒵 x and canonical utterances 𝒞 x are constructed during the canonical utterance construction phase.</a>
<a href="#12" id="12">Where the parameters Θ pr define the paraphrase model (Section 5 ), which is based on features extracted from text only (the input and canonical utterance.</a>
<a href="#13" id="13">The parameters Θ lf correspond to semantic parsing features based on the logical form and input utterance, and are briefly described in this section.</a>
<a href="#14" id="14">Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs ( c , z ) based on a paraphrase model.</a>
<a href="#15" id="15">The NLP paraphrase literature is vast and ranges from simple methods employing surface features [ 32 ] , through vector space models [ 28 ] , to latent variable models [ 6 , 33 , 29 ].</a>
<a href="#16" id="16">Our system generates relatively natural utterances from logical forms using simple rules based on Freebase descriptions (Section 4.</a>
<a href="#17" id="17">We now consider simply concatenating Freebase descriptions.</a>
<a href="#18" id="18">For example, the logical form 𝐑 [PlaceOfBirth].ElvisPresley would generate the utterance “ What location Elvis Presley place of birth ”.</a>
</body>
</html>