<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:277">
</head>
<body bgcolor="white">
<a href="#0" id="0">On one side, the QE models obtained with batch methods are learned exclusively from a predefined set of training examples under the assumption that they have similar characteristics with respect to the test data.</a>
<a href="#1" id="1">Our results show that the sensitivity of online QE models to different distributions of training and test instances makes them more suitable than batch methods for integration in a CAT framework.</a>
<a href="#2" id="2">The batch model is built by learning only from the training data and is evaluated on the test set without exploiting information from the test instances.</a>
<a href="#3" id="3">On each combination of training and test sets, the batch , adaptive , and empty models are trained and evaluated in terms of global MAE scores on the test set.</a>
<a href="#4" id="4">To this aim, our QE models are created using a training set coming from one domain (L or IT), and then used to predict the HTER labels for the test instances coming from the other domain ( e.g., training on L, testing on IT.</a>
<a href="#5" id="5">Although in these cases the distance between training and test data is comparable to the experiments with similar post-editors working in the same domain ( sim1 and sim2 ), here the predictive power of the batch models seems in fact to be lower.</a>
<a href="#6" id="6">This is a strong evidence of the fact that, in case of domain changes, online models can still learn from new test instances even if they have a label distribution similar to the training set.</a>
<a href="#7" id="7">When this distance is minimal, batch models can be a reasonable option, but when the gap between training and test data increases, adaptive or empty models are a preferable choice to achieve good results.</a>
</body>
</html>