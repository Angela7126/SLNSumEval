<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:178">
</head>
<body bgcolor="white">
<a href="#0" id="0">We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input.</a>
<a href="#1" id="1">Our model uses stacked autoencoders () to induce semantic representations integrating visual and textual information.</a>
<a href="#2" id="2">Our model learns higher-level meaning representations for single words from textual and visual input in a joint fashion.</a>
<a href="#3" id="3">To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs.</a>
<a href="#4" id="4">As our input consists of natural language attributes, the model would infer textual attributes given visual attributes and vice versa.</a>
<a href="#5" id="5">As shown in Figure 1 , our model takes as input two (real-valued) vectors representing the visual and textual modalities.</a>
<a href="#6" id="6">Throughout our experiments we compare a bimodal stacked autoencoder against unimodal autoencoders based solely on textual and visual input (left- and right-hand sides in Figure 1 , respectively.</a>
<a href="#7" id="7">All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations.</a>
<a href="#8" id="8">The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoencoder (SAE.</a>
</body>
</html>