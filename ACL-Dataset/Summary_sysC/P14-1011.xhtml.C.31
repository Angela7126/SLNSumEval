<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:202">
</head>
<body bgcolor="white">
<a href="#0" id="0">In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error [ 22 ] , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs.</a>
<a href="#1" id="1">We know from the semi-supervised phrase embedding that the learned vector representation can be well adapted to the given label.</a>
<a href="#2" id="2">Therefore, we can imagine that learning semantic phrase embedding is reasonable if we are given gold vector representations of the phrases.</a>
<a href="#3" id="3">1 reconstruction error E r ⁢ e ⁢ c ⁢ ( s , t ; Θ how well the learned vector representations p s and p t represent the phrase s and t respectively.</a>
<a href="#4" id="4">Ideally, we want the learned BRAE model can make sure that the semantic error for the positive example (a source phrase s and its correct translation t ) is much smaller than that for the negative example (the source phrase s and a bad translation t ′.</a>
<a href="#5" id="5">However, the current model cannot guarantee this since the above semantic error E s ⁢ e ⁢ m ( s t , Θ ) only accounts for positive ones.</a>
</body>
</html>