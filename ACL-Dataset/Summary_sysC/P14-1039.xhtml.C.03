<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:299">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using parse accuracy in a simple reranking strategy for self-monitoring, we find that with a state-of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make.</a>
<a href="#1" id="1">However, by using an SVM ranker to combine the realizer ’ s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved.</a>
<a href="#2" id="2">With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with White Rajkumar ’ s [ 28 , 36 ] baseline generative model, but not with their averaged perceptron model.</a>
<a href="#3" id="3">Therefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n -best parsing results, and per-label precision and recall for each type of dependency, together with the realizer ’ s normalized perceptron model score as a feature.</a>
<a href="#4" id="4">With the SVM reranker, we obtain a significant improvement in BLEU scores over White Rajkumar ’ s averaged perceptron model on both development and test data.</a>
<a href="#5" id="5">2 2 Note that the features from the local classification model for that -complementizer choice have not yet been incorporated into OpenCCG ’ s global realization ranking model, and thus do not inform the baseline realization choices in this work.</a>
<a href="#6" id="6">Simple ranking with the Berkeley parser of the generative model ’ s n -best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model ’ s BLEU score of 87.93.</a>
</body>
</html>