<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:178">
</head>
<body bgcolor="white">
<a href="#0" id="0">The evaluation of computer-generated text is a notoriously difficult problem, however, the quality of image descriptions has typically been measured using unigram bleu and human judgements.</a>
<a href="#1" id="1">The focus of this paper is to determine the correlation of automatic measures with human judgements for this task.</a>
<a href="#2" id="2">We estimate the correlation of unigram and Smoothed bleu , ter , rouge-su4 , and Meteor against human judgements on two data sets.</a>
<a href="#3" id="3">The main finding is that unigram bleu has a weak correlation, and Meteor has the strongest correlation with human judgements.</a>
<a href="#4" id="4">In this paper we estimate the correlation of human judgements with five automatic evaluation measures on two image description data sets.</a>
<a href="#5" id="5">The main finding of our analysis is that ter and unigram bleu are weakly correlated against human judgements, rouge-su4 and Smoothed bleu are moderately correlated, and the strongest correlation is found with Meteor.</a>
<a href="#6" id="6">We estimate Spearman ’ s Ρ for five different automatic evaluation measures against human judgements for the automatic image description task.</a>
<a href="#7" id="7">Bleu measures the effective overlap between a reference sentence X and a candidate sentence Y.</a>
</body>
</html>