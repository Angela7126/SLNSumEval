<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:175">
</head>
<body bgcolor="white">
<a href="#0" id="0">We performed the correlation analysis as follows.</a>
<a href="#1" id="1">The sentence-level evaluation measures were calculated for each image – description – reference tuple.</a>
<a href="#2" id="2">We collected the bleu , ter , and Meteor scores using MultEval [ 1 ] , and the rouge-su4 scores using the RELEASE-1.5.5.pl script.</a>
<a href="#3" id="3">The evaluation measure scores were then compared with the human judgements using Spearman ’ s correlation estimated at the sentence-level.</a>
<a href="#4" id="4">On the Flickr8k data set, all evaluation measures can be classified as either weakly correlated or moderately correlated with human judgements and all results are significant ter is only weakly correlated with human judgements but could prove useful in comparing the types of differences between models.</a>
<a href="#5" id="5">An analysis of the distribution of ter scores in Figure 2 (a) shows that differences in candidate and reference length are prevalent in the image description task.</a>
<a href="#6" id="6">Unigram bleu is also only weakly correlated against human judgements, even though it has been reported extensively for this task.</a>
<a href="#7" id="7">Figure 2 (a) shows an almost uniform distribution of unigram bleu scores, regardless of the human judgement.</a>
</body>
</html>