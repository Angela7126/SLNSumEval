<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:536">
</head>
<body bgcolor="white">
<a href="#0" id="0">The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models.</a>
<a href="#1" id="1">The output of the algorithm in Figure 1 is a modified set of sentence pairs ( s ⁢ e ⁢ n ⁢ t ⁢ e ⁢ n ⁢ c ⁢ e t ′ , s ⁢ e ⁢ n ⁢ t ⁢ e ⁢ n ⁢ c ⁢ e t ) , in which the same sentence pair may be used multiple times with different L1 substitutions for different fragments.</a>
<a href="#2" id="2">The final test set is created by randomly sampling the desired number of test instances.</a>
<a href="#3" id="3">We develop a classifier-based system composed of so-called “ classifier experts ”.</a>
<a href="#4" id="4">Numerous classifiers are trained and each is an expert in translating a single word or phrase.</a>
<a href="#5" id="5">In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained.</a>
<a href="#6" id="6">A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier.</a>
<a href="#7" id="7">It adds a LM component to the MLF baseline.</a>
<a href="#8" id="8">This LM baseline allows the comparison of classification through L1 fragments in an L2 context, with a more traditional L2 context modelling (i.e., target language modelling) which is also customary in MT decoders.</a>
<a href="#9" id="9">Computing this baseline is done in the same fashion as previously illustrated in Equation 1 , where s ⁢ c ⁢ o ⁢ r ⁢ e T then represents the normalised p ( t s ) score from the phrase-translation table rather than the class probability from the classifier.</a>
<a href="#10" id="10">Likewise, Table 4 exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model.</a>
<a href="#11" id="11">We observe in this data that the language model often has the added power to choose a correct translation that is not the first prediction of the classifier, but one of the weaker alternatives that nevertheless fits better.</a>
<a href="#12" id="12">Though the classifier generally works best in the l1r1 configuration, i.e., with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives.</a>
<a href="#13" id="13">This combination of a classifier with context size one and trigram-based language model proves to be most effective and reaches the best results so far.</a>
<a href="#14" id="14">We have not conducted experiments with language models of other orders.</a>
<a href="#15" id="15">Automatic configuration selection was done by performing leave-one-out testing (for small number of instances) or 10-fold-cross validation (for larger number of instances, n ≥ 20 ) on the training data per classifier expert.</a>
<a href="#16" id="16">Various configurations were tested.</a>
<a href="#17" id="17">Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in Table 2.</a>
<a href="#18" id="18">The auto configuration improves results over the uniformly applied feature selection.</a>
<a href="#19" id="19">However, if we enable the language model as we do in the auto + LM configuration we do not notice an improvement over l1r1 + LM , surprisingly.</a>
</body>
</html>