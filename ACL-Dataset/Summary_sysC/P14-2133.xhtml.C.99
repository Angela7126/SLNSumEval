<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:229">
</head>
<body bgcolor="white">
<a href="#0" id="0">We isolate three ways in which word embeddings might augment a state-of-the-art statistical parser by connecting out-of-vocabulary words to known ones, by encouraging common behavior among related in-vocabulary words, and by directly providing features for the lexicon.</a>
<a href="#1" id="1">This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.</a>
<a href="#2" id="2">Word embeddings — representations of lexical items as points in a real vector space — have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ].</a>
<a href="#3" id="3">Second, and more importantly, the fact that they use no continuous state representations internally makes it easy to design experiments that isolate the contributions of word vectors, without worrying about effects from real-valued operators higher up in the model.</a>
<a href="#4" id="4">Each Α t , w is learned in the same way as its corresponding probability in the original parser model — during each M step of the training procedure, Α w , t is set to the expected number of times the word w appears under the refined tag t.</a>
<a href="#5" id="5">Intuitively, as Β grows small groups of related words will be assigned increasingly similar probabilities of being generated from the same tag (in the limit where Β = 0 , Equation 1 is a uniform distribution over the entire vocabulary.</a>
</body>
</html>