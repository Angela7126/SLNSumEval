<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:186">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our problem fits well into the neural network framework and we expect that it can further improve inferring the topic representations for sentences.</a>
<a href="#1" id="1">To incorporate topic representations as translation knowledge into SMT, our neural network based approach directly optimizes similarities between the source language and target language in a compact topic space.</a>
<a href="#2" id="2">This is not simply coincidence since we can interpret their approach as a special case in our neural network method when a parallel sentence pair has document-level information, that document will be retrieved for training; otherwise, the most relevant document will be retrieved from the monolingual data.</a>
<a href="#3" id="3">Therefore, our method can be viewed as a more general framework than previous LDA-based approaches.</a>
<a href="#4" id="4">In contrast, with our neural network based approach, the learned topic distributions of “ deliver X ” or “ distribute X ” are more similar with the input sentence than “ send X ” , which is shown in Figure 4.</a>
<a href="#5" id="5">We directly optimized bilingual topic similarity in the deep learning framework with the help of sentence-level parallel data, so that the learned representation could be easily used in SMT decoding procedure.</a>
</body>
</html>