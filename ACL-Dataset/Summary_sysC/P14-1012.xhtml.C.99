<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:262">
</head>
<body bgcolor="white">
<a href="#0" id="0">To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity [ Zhao et al.2004 ] , phrase frequency, phrase length [ Hopkins and May2011 ] , and phrase generative probability [ Foster et al.2010 ] , which also show further improvement for new phrase feature learning in our experiments.</a>
<a href="#1" id="1">2011), which successfully capture both the preceding and succeeding contexts of the current word, and we estimate the backward LM by inverting the order in each sentence in the training data from the original order to the reverse order background 4-gram LMs are trained by the corresponding side of bilingual corpus 2 2 This corpus is used to train the translation model in our experiments, and we will describe it in detail in section 5.1.</a>
<a href="#2" id="2">Moreover, although we have introduced another four types of phrase features ( X 2 , X 3 , X 4 and X 5 ), the only 16 features in X are a bottleneck for learning large hidden layers feature representation, because it has limited information, the performance of the high-dimensional DAE features which are directly learned from single DAE is not very satisfactory.</a>
<a href="#3" id="3">To learn high-dimensional feature representation and to further improve the performance, we introduce a natural horizontal composition for DAEs that can be used to create large hidden layer representations simply by horizontally combining two (or more) DAEs [ Baldi2012 ] , as shown in Figure 3.</a>
</body>
</html>