<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:172">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, we further observe that hypernym â€“ hyponym relations are more complicated than a single offset can represent.</a>
<a href="#1" id="1">To address this challenge, we propose a more sophisticated and general method â€” learning a linear projection which maps words to their hypernyms (Section 3.3.1.</a>
<a href="#2" id="2">Furthermore, we propose a piecewise linear projection method based on relation clustering to better model hypernym â€“ hyponym relations (Section 3.3.2.</a>
<a href="#3" id="3">The training data for projection learning is collected from CilinE (Section 3.3.3.</a>
<a href="#4" id="4">We obtain 15,247 word pairs of hypernym â€“ hyponym relations (9,288 for direct relations and 5,959 for indirect relations.</a>
<a href="#5" id="5">We analyze error cases after experiments.</a>
<a href="#6" id="6">Some cases are shown in Figure 8.</a>
<a href="#7" id="7">We can see that there is only one general relation â€œ Ã¦Â¤ÂÃ§Â‰Â© ( plant ) â€â†’ğ»â€œ Ã§Â”ÂŸÃ§Â‰Â© ( organism ) â€ existing in CilinE.</a>
<a href="#8" id="8">Some fine-grained relations exist in Wikipedia, but the coverage is limited.</a>
<a href="#9" id="9">Our method based on word embeddings can discover more hypernym â€“ hyponym relations than the previous methods can.</a>
<a href="#10" id="10">When we combine the methods together, we get the correct hierarchy.</a>
</body>
</html>