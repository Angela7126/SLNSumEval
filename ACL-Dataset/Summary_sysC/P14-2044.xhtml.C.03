<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:117">
</head>
<body bgcolor="white">
<a href="#0" id="0">The prior distribution over word clusterings uses a log-linear model of morphological similarity; the likelihood function is the probability of generating vector word embeddings.</a>
<a href="#1" id="1">1) the distributional similarity between all words in the proposed partition containing w 1 and w 2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w 1 and w 2 , which acts as a prior distribution on the induced clustering.</a>
<a href="#2" id="2">In contrast, we use pairwise morphological similarity as a prior in a non-parametric clustering model.</a>
<a href="#3" id="3">We can create an infinite mixture model by combining the ddCRP prior with a likelihood function defining the probability of the data given the cluster assignments.</a>
</body>
</html>