<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:166">
</head>
<body bgcolor="white">
<a href="#0" id="0">To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser [] , and a generative constituent parser [].</a>
<a href="#1" id="1">The 1-best parse trees of these three parsers are aggregated in different ways.</a>
<a href="#2" id="2">Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3.</a>
<a href="#3" id="3">Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings.</a>
<a href="#4" id="4">Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods such as self/co/tri-training.</a>
<a href="#5" id="5">In summary, we make following contributions.</a>
<a href="#6" id="6">The second major row shows the results when we use single 1-best parse trees on unlabeled data.</a>
<a href="#7" id="7">When using the outputs of GParser itself ( “ Unlabeled ← G ” ), the experiment reproduces traditional self-training.</a>
</body>
</html>