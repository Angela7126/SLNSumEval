<html>
<head>
<meta name="TextLength" content="SENT_NUM:19, WORD_NUM:546">
</head>
<body bgcolor="white">
<a href="#0" id="0">We describe a system capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context.</a>
<a href="#1" id="1">The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to standard language models.</a>
<a href="#2" id="2">Generating test data using the same phrase-translation table as the training data would introduce a bias.</a>
<a href="#3" id="3">The classifier maps the L1 word or phrase in its L2 context to its L2 translation.</a>
<a href="#4" id="4">It has also been used in machine translation studies in which local source context is used to classify source phrases into target phrases, rather than looking them up in a phrase table [].</a>
<a href="#5" id="5">If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this fragment and adding those to the feature vector.</a>
<a href="#6" id="6">The classifier will return a probability distribution of the most likely translations given the context and we can replace the L1 fragment with the highest scoring L2 translation and present it back to the user.</a>
<a href="#7" id="7">The language model is a trigram-based back-off language model with Kneser-Ney smoothing, computed using SRILM [] and trained on the same training data as the translation model.</a>
<a href="#8" id="8">For any given hypothesis H , results from the L1 to L2 classifier are combined with results from the L2 language model.</a>
<a href="#9" id="9">We do so by normalising the class probability from the classifier ( s ⁢ c ⁢ o ⁢ r ⁢ e T ⁢ ( H ) ), which is our translation model, and the language model ( s ⁢ c ⁢ o ⁢ r ⁢ e l ⁢ m ⁢ ( H ) ), in such a way that the highest classifier score for the alternatives under consideration is always 1.0 , and the highest language model score of the sentence is always 1.0.</a>
<a href="#10" id="10">Several automated metrics exist for the evaluation of L2 system output against the L2 reference output in the test set.</a>
<a href="#11" id="11">A context-insensitive yet informed baseline was constructed to assess the impact of L2 context information in translating L1 fragments.</a>
<a href="#12" id="12">The baseline selects the most probable L1 fragment per L2 fragment according to the phrase-translation table.</a>
<a href="#13" id="13">A second baseline was constructed by weighing the probabilities from the translation table directly with the L2 language model described earlier.</a>
<a href="#14" id="14">This LM baseline allows the comparison of classification through L1 fragments in an L2 context, with a more traditional L2 context modelling (i.e., target language modelling) which is also customary in MT decoders.</a>
<a href="#15" id="15">Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2.</a>
<a href="#16" id="16">It appears that the classifier approach and the L2 language model are able to complement each other.</a>
<a href="#17" id="17">Likewise, Table 4 exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model.</a>
<a href="#18" id="18">This combination of a classifier with context size one and trigram-based language model proves to be most effective and reaches the best results so far.</a>
</body>
</html>