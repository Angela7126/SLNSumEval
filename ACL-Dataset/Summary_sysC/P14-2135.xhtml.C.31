<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:208">
</head>
<body bgcolor="white">
<a href="#0" id="0">Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system [ 3 ].</a>
<a href="#1" id="1">Such models extract information about the perceptible characteristics of words from data collected in property norming experiments [ 22 , 24 ] or directly from ‘ raw ’ data sources such as images [ 11 , 6 ].</a>
<a href="#2" id="2">This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning.</a>
<a href="#3" id="3">Multi-modal models outperform language-only models on a range of tasks, including modelling conceptual association and predicting compositionality [ 6 , 24 , 22 ].</a>
<a href="#4" id="4">We apply image dispersion-based filtering as follows if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included.</a>
<a href="#5" id="5">If not, in accordance with the Dual Coding Theory of human concept processing [ 21 ] , only the linguistic representation is used.</a>
<a href="#6" id="6">For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter.</a>
<a href="#7" id="7">We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations.</a>
</body>
</html>