<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:128">
</head>
<body bgcolor="white">
<a href="#0" id="0">Finally, we selected a certain number of sentences according to the rank score into a summary.</a>
<a href="#1" id="1">LDA presented by [ 4 ] models each document as a mixture of topics (we call it lda_topic to discriminate our t ⁢ o ⁢ p ⁢ i ⁢ c candidates), and generates a discrete probability distribution over words for each lda_topic.</a>
<a href="#2" id="2">After a sufficient number of sampling iterations, the approximated posterior can be used to estimate Φ and Θ by examining the counts of word assignments to topics and topic occurrences in documents.</a>
<a href="#3" id="3">For each lda_topic, we extracted words whose probabilities are larger than zero, and regarded these as topic candidates.</a>
<a href="#4" id="4">The proposed method does not simply use MACD to find bursts, but instead determines topic words in series of documents.</a>
</body>
</html>