<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:185">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we propose learning sentiment-specific word embedding ( SSWE ) for sentiment analysis.</a>
<a href="#1" id="1">In the accuracy of polarity consistency between each sentiment word and its top N closest words, SSWE outperforms existing word embedding learning algorithms.</a>
<a href="#2" id="2">To our knowledge, this is the first work that exploits word embedding for Twitter sentiment classification.</a>
<a href="#3" id="3">In this section, we present the details of learning sentiment-specific word embedding ( SSWE ) for Twitter sentiment classification.</a>
<a href="#4" id="4">We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification.</a>
<a href="#5" id="5">By contrast, SSWE h and SSWE r learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words.</a>
<a href="#6" id="6">We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ].</a>
<a href="#7" id="7">We compare sentiment-specific word embedding (SSWE h , SSWE r , SSWE u ) with baseline embedding learning algorithms by only using word embedding as features for Twitter sentiment classification.</a>
<a href="#8" id="8">We show that the word embedding learned by traditional neural networks are not effective enough for Twitter sentiment classification.</a>
</body>
</html>