<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:149">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this section, we explain our neural network based topic similarity model in detail, as well as how to incorporate the topic similarity features into SMT decoding procedure.</a>
<a href="#1" id="1">Figure 1 sketches the high-level overview which illustrates how to learn topic representations using sentence-level parallel data.</a>
<a href="#2" id="2">Given a parallel sentence pair ⟨ f , e ⟩ , the first step is to treat f and e as queries, and use IR methods to retrieve relevant documents to enrich contextual information for them.</a>
<a href="#3" id="3">Specifically, the ranking model we used is a Vector Space Model (VSM), where the query and document are converted into tf-idf weighted vectors.</a>
<a href="#4" id="4">The most relevant N documents 𝐝 f and 𝐝 e are retrieved and converted to a high-dimensional, bag-of-words input f and e for the representation learning 1 1 We use f and e to denote the n -of- V vector converted from the retrieved documents.</a>
</body>
</html>