<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:186">
</head>
<body bgcolor="white">
<a href="#0" id="0">This model memoises (i.e.,, learns) both the individual â€œ function words â€ and the sequences of â€œ function words â€ that modify the ğ–¢ğ—ˆğ—…ğ—…ğ—ˆğ–¼ğŸ£ - ğ–¢ğ—ˆğ—…ğ—…ğ—ˆğ–¼ğŸ¥ constituents.</a>
<a href="#1" id="1">This means that â€œ function words â€ are memoised independently of the â€œ content words â€ that ğ–¶ğ—ˆğ—‹ğ–½ expands to; i.e.,, the model learns distinct â€œ function word â€ and â€œ content word â€ vocabularies.</a>
<a href="#2" id="2">Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our â€œ function word â€ Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 â€“ 24 ) are replaced with the mirror-image rules in which â€œ function words â€ are attached to the right periphery.</a>
<a href="#3" id="3">As noted earlier, the â€œ function word â€ model generates function words via adapted nonterminals other than the ğ–¶ğ—ˆğ—‹ğ–½ category.</a>
<a href="#4" id="4">This paper showed that the word segmentation accuracy of a state-of-the-art Adaptor Grammar model is significantly improved by extending it so that it explicitly models some properties of function words.</a>
</body>
</html>