<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:104">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.</a>
<a href="#1" id="1">It seems clear that word embeddings exhibit some syntactic structure.</a>
<a href="#2" id="2">For OOV words which are not in the dictionary of embeddings, we back off to the unknown word model for the underlying parser.</a>
<a href="#3" id="3">Each Α t , w is learned in the same way as its corresponding probability in the original parser model — during each M step of the training procedure, Α w , t is set to the expected number of times the word w appears under the refined tag t.</a>
</body>
</html>