<html>
<head>
<meta name="TextLength" content="SENT_NUM:2, WORD_NUM:58">
</head>
<body bgcolor="white">
<a href="#0" id="0">Previous work on neural word embeddings take the contexts of a word to be its linear context â€“ words that precede and follow the target word, typically in a window of k tokens to each side.</a>
<a href="#1" id="1">In particular, we found that Deps perform dramatically worse than BoW contexts on analogy tasks as in [ 22 , 17 ].</a>
</body>
</html>