<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:183">
</head>
<body bgcolor="white">
<a href="#0" id="0">Adaptor grammars are useful when the goal is to learn a potentially unbounded set of entities that need to satisfy hierarchical constraints.</a>
<a href="#1" id="1">As section 2 explains in more detail, word segmentation is such a case words are composed of syllables and belong to phrases or collocations, and modelling this structure improves word segmentation accuracy.</a>
<a href="#2" id="2">For comparison purposes we also include results for a mirror-image model that permits “ function words ” on the right periphery, a model which permits “ function words ” on both the left and right periphery (achieved by changing rules 22 – 24 ), as well as a model that analyses all words as monosyllabic.</a>
<a href="#3" id="3">Section 4 explains how a learner could use Bayesian model selection to determine that function words appear on the left periphery in English by comparing the posterior probability of the data under our “ function word ” Adaptor Grammar to that obtained using a grammar which is identical except that rules ( 22 – 24 ) are replaced with the mirror-image rules in which “ function words ” are attached to the right periphery.</a>
</body>
</html>