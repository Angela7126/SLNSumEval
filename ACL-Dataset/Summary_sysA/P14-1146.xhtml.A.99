<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:162">
</head>
<body bgcolor="white">
<a href="#0" id="0">The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text.</a>
<a href="#1" id="1">As a result, words with opposite polarity, such as good and bad , are mapped into close vectors.</a>
<a href="#2" id="2">To this end, we extend the existing word embedding learning algorithm [ 9 ] and develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g., sentences or tweets) in their loss functions.</a>
<a href="#3" id="3">We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work [ 33 ].</a>
<a href="#4" id="4">Instead of hand-crafting features, we incorporate the continuous representation of words and phrases as the feature of a tweet.</a>
<a href="#5" id="5">The sentiment classifier is built from tweets with manually annotated sentiment polarity.</a>
<a href="#6" id="6">From the first column of Table 3 , we can see that the performance of C W and word2vec are obviously lower than sentiment-specific word embeddings by only using unigram embedding as features.</a>
</body>
</html>