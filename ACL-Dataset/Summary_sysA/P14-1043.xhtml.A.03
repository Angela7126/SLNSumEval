<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:173">
</head>
<body bgcolor="white">
<a href="#0" id="0">Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical.</a>
<a href="#1" id="1">The auto-parsed unlabeled data with ambiguous labelings is denoted as ğ’Ÿâ€² = { ( ğ® i , ğ’± i ) } i = 1 M , where ğ® i is an unlabeled sentence, and ğ’± i is the corresponding parse forest.</a>
<a href="#2" id="2">These three parsers are trained on labeled data and then used to parse each unlabeled sentence.</a>
<a href="#3" id="3">The second major row shows the results when we use single 1-best parse trees on unlabeled data.</a>
<a href="#4" id="4">The reason may be that dependency parsers are prone to amplify previous mistakes on unlabeled data during training.</a>
<a href="#5" id="5">Using unlabeled data with the results of Berkeley Parser ( â€œ Unlabeled â† B â€ ) significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese.</a>
<a href="#6" id="6">We divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar.</a>
</body>
</html>