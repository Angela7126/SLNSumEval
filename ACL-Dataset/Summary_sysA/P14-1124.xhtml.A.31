<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:144">
</head>
<body bgcolor="white">
<a href="#0" id="0">For the Tagalog conversations, as with English newswire, we observe that the document frequency, DF w , of a word w is not a linear function of word frequency f w in the log domain, as would be expected under a naive Poisson generative assumption.</a>
<a href="#1" id="1">The implication of deviations from a Poisson model is that words tend to be concentrated in a small number of documents rather than occurring uniformly across the corpus.</a>
<a href="#2" id="2">This is the burstiness we leverage to improve term detection.</a>
<a href="#3" id="3">In Figure 9 we see two classes of words emerge.</a>
<a href="#4" id="4">A similar phenomenon is observed concerning adaptive language models [].</a>
<a href="#5" id="5">In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model [].</a>
<a href="#6" id="6">Likewise, Katz attempts to capture these two classes in his G model of word frequencies (1996.</a>
</body>
</html>