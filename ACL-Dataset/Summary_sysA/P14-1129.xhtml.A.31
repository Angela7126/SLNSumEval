<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:165">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper we use a basic neural network architecture and a lexicalized probability model to create a powerful MT decoding feature.</a>
<a href="#1" id="1">Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window.</a>
<a href="#2" id="2">Unlike previous approaches to joint modeling [ 13 ] , our feature can be easily integrated into any statistical machine translation (SMT) decoder, which leads to substantially larger improvements than k -best rescoring only.</a>
<a href="#3" id="3">Formally, our model approximates the probability of target hypothesis T conditioned on source sentence S.</a>
<a href="#4" id="4">We follow the standard n -gram LM decomposition of the target, where each target word t i is conditioned on the previous n - 1 target words.</a>
<a href="#5" id="5">To make this a joint model, we also condition on source context vector ùíÆ i.</a>
<a href="#6" id="6">One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words.</a>
</body>
</html>