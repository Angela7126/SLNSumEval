<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:84">
</head>
<body bgcolor="white">
<a href="#0" id="0">Based on this resource, we learn embeddings that predict one word from another related word.</a>
<a href="#1" id="1">In fact, RCM does not even observe all the words that appear in the training set, so it makes little sense to use the RCM embeddings directly for language modeling.</a>
<a href="#2" id="2">Therefore, in order to make fair comparison, for every set of trained embeddings, we fix them as input embedding for word2vec, then learn the remaining input embeddings (words not in the relations) and all the output embeddings using cbow.</a>
</body>
</html>