<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:168">
</head>
<body bgcolor="white">
<a href="#0" id="0">Interpolation with lower order models is motivated by the problem of data sparsity in higher order models.</a>
<a href="#1" id="1">We see that the GLM performs particularly well on small training data.</a>
<a href="#2" id="2">As the size of the training data set becomes smaller (even smaller than the evaluation data), the GLM achieves a reduction of perplexity of up to 25.7 ⁢ % compared to language models with modified Kneser-Ney smoothing on the same data set.</a>
<a href="#3" id="3">The absolute perplexity values for this experiment are presented in Table 4.</a>
<a href="#4" id="4">In an empirical evaluation, we have demonstrated that for higher orders the GLM outperforms MKN for all test cases.</a>
<a href="#5" id="5">The relative improvement in perplexity is up to 12.7 ⁢ % for large data sets.</a>
<a href="#6" id="6">GLMs also performs particularly well on small and sparse sets of training data.</a>
<a href="#7" id="7">On a very small training data set we observed a reduction of perplexity by 25.7 ⁢ %.</a>
<a href="#8" id="8">Our experiments underline that the generalized language models overcome in particular the weaknesses of modified Kneser-Ney smoothing on sparse training data.</a>
</body>
</html>