<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:127">
</head>
<body bgcolor="white">
<a href="#0" id="0">In our method, the standard recursive auto-encoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error [ 22 ] , while the bilingually-constrained model learns to fine-tune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs.</a>
<a href="#1" id="1">Ideally, we want the learned BRAE model can make sure that the semantic error for the positive example (a source phrase s and its correct translation t ) is much smaller than that for the negative example (the source phrase s and a bad translation t ′.</a>
<a href="#2" id="2">However, the current model cannot guarantee this since the above semantic error E s ⁢ e ⁢ m ( s t , Θ ) only accounts for positive ones.</a>
</body>
</html>