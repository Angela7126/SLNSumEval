<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:117">
</head>
<body bgcolor="white">
<a href="#0" id="0">We then collect a corpus of such disagreements and have experts mark which ones are due to actual annotation errors , and which ones reflect linguistically hard cases (Section 3.</a>
<a href="#1" id="1">The results show that the majority of disagreements are due to hard cases, and only about 20% of conflicting annotations are actual errors.</a>
<a href="#2" id="2">This suggests that inter-annotator agreement scores often hide the fact that the vast majority of annotations are actually linguistically motivated.</a>
<a href="#3" id="3">In our case, less than 2% of the overall annotations are linguistically unmotivated.</a>
<a href="#4" id="4">Presents work on detecting linguistically hard cases in the context of word sense annotations, e.g.,, cases where expert annotators will disagree, as well as differentiating between underspecified, overspecified and metaphoric cases.</a>
</body>
</html>