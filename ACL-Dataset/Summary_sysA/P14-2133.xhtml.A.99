<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:117">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper investigates a variety of ways in which word embeddings might augment a constituency parser with a discrete state space.</a>
<a href="#1" id="1">Word embeddings — representations of lexical items as points in a real vector space — have a long history in natural language processing, going back at least as far as work on latent semantic analysis (LSA) for information retrieval [ 4 ].</a>
<a href="#2" id="2">However, the failure to uncover gains when searching across a variety of possible mechanisms for improvement, training procedures for embeddings, hyperparameter settings, tasks, and resource scenarios suggests that these gains (if they do exist) are extremely sensitive to these training conditions, and not nearly as accessible as they seem to be in dependency parsers.</a>
</body>
</html>