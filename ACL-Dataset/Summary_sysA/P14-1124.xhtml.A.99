<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:136">
</head>
<body bgcolor="white">
<a href="#0" id="0">The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary.</a>
<a href="#1" id="1">In applying the burstiness quantity to term detection, we recall that the task requires us to locate a particular instance of a term, not estimate a count, hence the utility of N-gram language models predicting words in sequence.</a>
<a href="#2" id="2">Lastly, the reductions in P ‚Å¢ ( Miss ) suggests that we are improving the term detection metric, which is sensitive to threshold changes, by doing what we set out to do, which is to boost lower confidence repeated words and correctly asserting them as true hits.</a>
<a href="#3" id="3">Moreover, we are able to accomplish this in a wide variety of languages.</a>
</body>
</html>