<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:184">
</head>
<body bgcolor="white">
<a href="#0" id="0">The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data apply a variant of co-training to dependency parsing and report positive results on out-of-domain text combine tri-training and parser ensemble to boost parsing accuracy.</a>
<a href="#1" id="1">Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical.</a>
<a href="#2" id="2">In this way, the auto-parsed unlabeled data becomes more reliable.</a>
<a href="#3" id="3">During experimental trials, we find that “ Unlabeled ← B+(Z ∩ G) ” can further boost performance on Chinese.</a>
<a href="#4" id="4">A possible explanation is that by using the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser.</a>
<a href="#5" id="5">Specifically, we find that our approach is very effective when using divergent parsers such as the generative parser, and it is also helpful to properly balance the size and oracle accuracy of the parse forest of the unlabeled data.</a>
</body>
</html>