<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:113">
</head>
<body bgcolor="white">
<a href="#0" id="0">A related approach [ 24 ] assumes richer lexical representations where each word is represented with a vector and a matrix that encodes its interaction with its syntactic sister.</a>
<a href="#1" id="1">The lf model can be seen as a projection of the symbolic Montagovian approach to semantic composition in natural language onto the domain of vector spaces and linear operations on them [ 3 ].</a>
<a href="#2" id="2">Indeed, in order to train a transitive verb tensor (e.g.,, eat ), the method of Grefenstette et al.</a>
<a href="#3" id="3">We conjecture that the lf 3-way tensor representation of transitive verbs leads to a stronger asymmetry between sentences with inverted arguments, and thus makes this model particularly sensitive to word order differences.</a>
</body>
</html>