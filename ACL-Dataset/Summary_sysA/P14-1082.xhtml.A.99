<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:154">
</head>
<body bgcolor="white">
<a href="#0" id="0">When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table.</a>
<a href="#1" id="1">If so, we are done quickly and need not rely on context information.</a>
<a href="#2" id="2">Third, we observe that adding the language model to our classifier leads to another significant gain (configuration l1r1 + LM in the results in Table 2.</a>
<a href="#3" id="3">We observe in this data that the language model often has the added power to choose a correct translation that is not the first prediction of the classifier, but one of the weaker alternatives that nevertheless fits better.</a>
<a href="#4" id="4">Though the classifier generally works best in the l1r1 configuration, i.e., with context size one, the trigram-based language model allows further left-context information to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives.</a>
</body>
</html>