<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:179">
</head>
<body bgcolor="white">
<a href="#0" id="0">One word that appears at the end of a local context w i - n + 1 i - 1 and for which only a poor approximation exists may adversely affect the conditional probabilities in language models of all lengths — leading to severe errors even for smoothed language models.</a>
<a href="#1" id="1">Among other techniques, skip n -grams have also been considered as an approach to overcome problems of data sparsity [].</a>
<a href="#2" id="2">However, to best of our knowledge, language models making use of skip n -grams models have never been investigated to their full extent and over different levels of lower order models.</a>
<a href="#3" id="3">Where c ⁢ ( w i - n + 1 i ) provides the frequency count that sequence w i - n + 1 i occurs in training data, D is a discount value (which depends on the frequency of the sequence) and Γ h ⁢ i ⁢ g ⁢ h depends on D and is the interpolation factor to mix in the lower order distribution 1 1 The factors Γ and D are quite technical and lengthy.</a>
</body>
</html>