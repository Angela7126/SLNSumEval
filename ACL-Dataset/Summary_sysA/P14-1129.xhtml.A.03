<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:179">
</head>
<body bgcolor="white">
<a href="#0" id="0">Specifically, we introduce a novel formulation for a neural network joint model (NNJM), which augments an n -gram target language model with an m -word source window.</a>
<a href="#1" id="1">An example of the NNJM context model for a Chinese-English parallel sentence is given in Figure 1.</a>
<a href="#2" id="2">However, note that there are only 3 possible positions for each target word, and 11 for each source word.</a>
<a href="#3" id="3">The decoding cost is based on a measurement of ∼ 1200 unique NNJM lookups per source word for our Arabic-English system.</a>
<a href="#4" id="4">One issue with the S2T NNJM is that the probability is computed over every target word, so it does not explicitly model NULL-aligned source words.</a>
<a href="#5" id="5">In order to assign a probability to every source word during decoding, we also train a neural network lexical translation model (NNLMT.</a>
<a href="#6" id="6">It is easy and computationally inexpensive to use this model in decoding, since only one neural network computation must be made for each source word.</a>
<a href="#7" id="7">In Table 2 we showed that the cost of unique lookups for the pre-computed NNJM is only ∼ 0.001 seconds per source word.</a>
</body>
</html>