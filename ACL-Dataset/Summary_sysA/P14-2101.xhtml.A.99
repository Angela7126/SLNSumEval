<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:167">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our research task of automatic labelling a topic consists on selecting a set of words that best describes the semantics of the terms involved in this topic.</a>
<a href="#1" id="1">The most generic approach to automatic labelling has been to use as primitive labels the top- n words in a topic distribution learned by a topic model such as LDA [ 9 , 2 ].</a>
<a href="#2" id="2">Such top words are usually ranked using the marginal probabilities P ( w i t j ) associated with each word w i for a given topic t j.</a>
<a href="#3" id="3">We compared the results of the summarisation techniques with the top terms ( TT ) of a topic as our baseline.</a>
<a href="#4" id="4">These TT set corresponds to the top x terms ranked based on the probability of the word given the topic ( p ( w k ) ) from the topic model.</a>
<a href="#5" id="5">Different summarisation techniques reveal words which do not appear in the top terms but which are relevant to the information clustered by the topic.</a>
</body>
</html>