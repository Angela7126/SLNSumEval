<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 85" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Recently, researchers have begun exploring methods of scoring student essays with respect to particular dimensions of quality such as coherence, technical errors, and prompt adherence.
  </a>
  <a href="#1" id="1">
   The work on modeling prompt adherence, however, has been focused mainly on whether individual sentences adhere to the prompt.
  </a>
  <a href="#2" id="2">
   We present a new annotated corpus of essay-level prompt adherence scores and propose a feature-rich approach to scoring essays along the prompt adherence dimension.
  </a>
  <a href="#3" id="3">
   Our approach significantly outperforms a knowledge-lean baseline prompt adherence scoring system yielding improvements of up to 16.6%.
  </a>
 </body>
</html>