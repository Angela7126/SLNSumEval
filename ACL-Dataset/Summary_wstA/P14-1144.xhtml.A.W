<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 93" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block.
  </a>
  <a href="#1" id="1">
   Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches.
  </a>
  <a href="#2" id="2">
   In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings.
  </a>
  <a href="#3" id="3">
   The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.
  </a>
 </body>
</html>