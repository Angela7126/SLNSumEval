<html>
 <head>
  <meta content="SENT_NUM: 5, WORD_NUM: 170" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   Following the works of \citeN Carletta96 and \citeN Art:Poe08, there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used.
  </a>
  <a href="#1" id="1">
   With this in mind, it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F 1 (for phrase structure) and accuracy scores (for dependencies).
  </a>
  <a href="#2" id="2">
   In this work we present a chance-corrected metric based on Krippendorffs , adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications.
  </a>
  <a href="#3" id="3">
   To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric s responses, before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.
  </a>
  <a href="#4" id="4">
   The code used to produce the data in this paper, and some of the datasets used, are available to download at https://github.com/arnsholt/syn-agreement/.
  </a>
 </body>
</html>