<html>
 <head>
  <meta content="SENT_NUM: 4, WORD_NUM: 127" name="TextLength"/>
 </head>
 <body bgcolor="white">
  <a href="#0" id="0">
   In this paper, instead of designing new features based on intuition, linguistic knowledge and domain, we learn some new and effective features using the deep auto-encoder (DAE) paradigm for phrase-based translation model.
  </a>
  <a href="#1" id="1">
   Using the unsupervised pre-trained deep belief net (DBN) to initialize DAE s parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning, we learn new semi-supervised DAE features, which are more effective and stable than the unsupervised DBN features.
  </a>
  <a href="#2" id="2">
   Moreover, to learn high dimensional feature representation, we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning.
  </a>
  <a href="#3" id="3">
   On two Chinese-English tasks, our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 (IWSLT) and 0.82/1.52 (NIST) BLEU points over the unsupervised DBN features and the baseline features, respectively.
  </a>
 </body>
</html>