<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:177">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we proposed an efficient kernel low-rank decomposition algorithm called the generalized Nystr√∂m method.</a>
<a href="#1" id="1">Our approach inherits the advantages from both low-rank matrix decomposition and kernel learning.</a>
<a href="#2" id="2">Our approach is computationally very efficient and the space and time complexity is both linear in the sample size and dimension.</a>
<a href="#3" id="3">Our approach can also incorporate side information to compute low-rank decomposition that aligns well with the learning target, leading to improved generalization performance.</a>
<a href="#4" id="4">Our approach is endowed with a flexible non-parametric structure and can extend naturally to new data.</a>
<a href="#5" id="5">It shows significant performance gains in benchmark learning tasks.</a>
<a href="#6" id="6">In the future, we will consider learning a sparse dictionary kernel in our method.</a>
<a href="#7" id="7">In case a large number of landmark points are selected, sparse dictionary kernel can effectively reduce the computational cost while preserving the model capacity.</a>
<a href="#8" id="8">We are also interested in incorporating do-not-link constraints in the loss function for improved generalization performance.</a>
<a href="#9" id="9">Finally, we are also pursuing more principled ways of determining the rank parameter by achieving a balanced tradeoff between computational savings and the generalization performance.</a>
</body>
</html>