<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:166">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper presents new non-trivial pruning rules to be used with the Bayesian Information Criterion (BIC) score for learning the structure of Bayesian networks.</a>
<a href="#1" id="1">The derived theoretical bounds extend previous results in the literature and can be promptly integrated into existing solvers with minimal effort and computational costs.</a>
<a href="#2" id="2">They imply faster computations without losing optimality.</a>
<a href="#3" id="3">The very computationally efficient version of the new rules imply gains of around 20% with respect to previous work, according to our experiments, while the most computationally demanding pruning achieves around 50% more pruning than before.</a>
<a href="#4" id="4">Pruning rules for other widely used scores such as the Bayesian Dirichlet equivalent uniform (BDeu) have been devised [13] and some researchers conjecture that they cannot be improved.</a>
<a href="#5" id="5">Similarly, we conjecture that further bounds for the BIC score are unlikely to exist unless for some particular cases and situations.</a>
<a href="#6" id="6">This can be studied in a future work, as well as means to devise smart strategies to tune the theorem parameters and improve their pruning capabilities.</a>
</body>
</html>