<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:627">
</head>
<body bgcolor="white">
<a href="#0" id="0">Although several competing human behavior models have been proposed to model and protect against boundedly rational adversaries in repeated Stackelberg security games, no study has yet been conducted against actual human subjects to show which is the best model in such repeated settings.</a>
<a href="#1" id="1">This article provides three major contributions towards answering that question and therefore, provides an advancement to the field of “security games”, a key area of research in Artificial Intelligence.</a>
<a href="#2" id="2">Given the important applications of security games in areas such as protecting wildlife and fisheries, where there are repeated interactions between the defenders and adversaries, our contributions are critical for such domains.</a>
<a href="#3" id="3">First, we introduce a novel human behavior model called SHARP for repeated SSG settings.</a>
<a href="#4" id="4">SHARP has three major novelties: (i) It models the adversary's adaptive decision making process by reasoning based on success or failure of the adversary's past actions on exposed portions of the attack surface.</a>
<a href="#5" id="5">(ii) It accounts for lack of information about the adversary's preferences due to insufficient exposure to attack surface by reasoning about similarity between exposed and unexposed areas of the attack surface, and also incorporating a confidence based discounting parameter to model the learner's trust in the available data.</a>
<a href="#6" id="6">(iii) It integrates a non-linear probability weighting function to model the adversary's perception of probabilities.</a>
<a href="#7" id="7">Second, we provide empirically supported methodological contributions towards conducting human subjects experiments in repeated measures settings on AMT.</a>
<a href="#8" id="8">We provide analysis of the contribution of our approaches that help maintain high participant retention rates throughout the course of 2.3 week-long studies (46 weeks in total) and thus providing the grounds for fair comparison of various human behavior models in repeated Stackelberg Security Games.</a>
<a href="#9" id="9">Third, we provided results from the first repeated measures study of competing models in repeated SSGs to test the performance of SHARP along with existing approaches.</a>
<a href="#10" id="10">We conducted experiments on four different payoff structures on the Amazon Mechanical Turk platform and also on one payoff structure in the real world, at the Bukit Barisan Seletan National Park in Indonesia, with security experts from the local government as well as various NGOs (WWF, WCS, YABI, etc.)</a>
<a href="#11" id="11">who are in charge of protecting wildlife in the national park.</a>
<a href="#12" id="12">Our results show that: (i) Human perceptions of probability are S-shaped, contradicting the inverse S-shaped observation from prospect theory.</a>
<a href="#13" id="13">(ii) Existing human behavior models and algorithms perform poorly in initial rounds of repeated SSGs, with P-BSUQR which was earlier proposed as the basis for wildlife security application PAWS, performing poorly throughout all the rounds; (iii) a simpler model, P-SUQR, which was originally proposed for single-shot games recovers significantly after initial round losses; and (iv) SHARP performs significantly better than existing approaches consistently over all the rounds, most notably in the initial rounds.</a>
<a href="#14" id="14">Whereas in our work we have found support for S-shaped probability weighting curves, there are existing works that demonstrate inverse S-shaped curves in their domains [48], [77].</a>
<a href="#15" id="15">As mentioned earlier in Section 7.1, one hypothesis is that human probability weighting could be context dependent, for example, results may vary depending on the number of alternatives available to the decision maker in the domain under consideration.</a>
<a href="#16" id="16">Therefore, in the future, we would like to conduct further studies to identify settings when people may exhibit an S-shaped probability weighting curve as opposed to an inverse S-shaped one.</a>
<a href="#17" id="17">We would also want to explore existing models of human discounting as we mentioned earlier in Section 8.1.</a>
<a href="#18" id="18">Furthermore, while we have treated the reward to the poacher as a fixed reward (see Section 4.3, footnote 3) if the ranger does not capture the poacher, in reality, the poacher has to further take into account the uncertainty over capturing a hippo.</a>
<a href="#19" id="19">This is a further layer of uncertainty that may need to be investigated in the future.</a>
</body>
</html>