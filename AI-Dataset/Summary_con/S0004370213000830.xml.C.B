<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:392">
</head>
<body bgcolor="white">
<a href="#0" id="0">This work has investigated the viability of both supervised and semi-supervised feature labeling in real circumstances, with end users freely choosing features to label directly from text documents.</a>
<a href="#1" id="1">Our new supervised LWLR-FL algorithm expands LWLR to take feature labeling into account.</a>
<a href="#2" id="2">Our results show that LWLR-FL was among the best performing supervised feature labeling algorithms under ideal conditions in an oracle study.</a>
<a href="#3" id="3">In our user study, we allowed ordinary end users to select any features for labeling directly from text documents.</a>
<a href="#4" id="4">LWLR-FL and MNB/Priors both were robust against lower quality feature labels in this more realistic setting, with MNB/Priors being the best performing algorithm overall.</a>
<a href="#5" id="5">Furthermore, our sensitivity analysis showed that LWLR-FL was robust to different parameter settings.</a>
<a href="#6" id="6">As to the end-user labels themselves, we showed that real end usersʼ feature labels helped on average for all algorithms, with the features end users chose for labeling to be conceptually related to the class labels, although with moderately lower information gains compared to those of the oracleʼs.</a>
<a href="#7" id="7">These results are promising, as they show that end users with no background in machine learning can use feature labeling to significantly improve machine learning algorithms trained on small data sets.</a>
<a href="#8" id="8">We also proposed a new semi-supervised LWLR-SS-FL algorithm, which extends LWLR-FL to incorporate information from a pool of unlabeled data.</a>
<a href="#9" id="9">With oracle feature labels, LWLR-SS-FL and GE were the best performing algorithms over the six datasets in our evaluation.</a>
<a href="#10" id="10">With end user feature labels, LWLR-SS-FL outperformed all other algorithms.</a>
<a href="#11" id="11">Even in situations where the unlabeled data (without feature labeling) degraded performance below the supervised learning LWLR baseline, semi-supervised learning in combination with feature labeling was able to overcome this deficit and outperform this baseline.</a>
<a href="#12" id="12">Finally, our results point to promising future research.</a>
<a href="#13" id="13">First, we intend to design suitable user interfaces to help end users choose and create features to label.</a>
<a href="#14" id="14">Second, we would like to improve the LWLR-SS-FL algorithm by making it more scalable to large datasets and more robust to its diffusion kernel width parameter setting.</a>
<a href="#15" id="15">Taken together, these results demonstrate that feature labeling by end users, especially in the supervised learning setting, is an overall effective solution for augmenting the learning process to use knowledge beyond labeled training instances.</a>
<a href="#16" id="16">Semi-supervised feature labeling can be effective in some cases, but the unlabeled data may degrade performance if it does not match the algorithmʼs assumptions.</a>
</body>
</html>