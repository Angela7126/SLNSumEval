<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:690">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper was motivated by an observed explosion of the number of papers featuring computer models addressing intelligence test problems.</a>
<a href="#1" id="1">Among the around 30 papers we have analysed, half of them have appeared since 2011.</a>
<a href="#2" id="2">We wanted to investigate whether this increase was casual or was motivated by an increasing need of these tests and the computer models solving them.</a>
<a href="#3" id="3">When we began our investigation we soon realised that computer models addressing intelligence tests have different purposes and applications: to advance AI by the use of challenging problems (this is the Psychometric AI approach), to use them for the evaluation of AI systems, to better understand intelligence tests and what they measure (including item difficulty), and, finally, to better understand what (human) intelligence is.</a>
<a href="#4" id="4">Note that if any or all of the above reasons were spurious, its negation would still be most interesting.</a>
<a href="#5" id="5">Namely, if intelligence tests were not challenging, were not useful for the evaluation of AI systems, did not measure the purported abilities beyond humans, or were useless to understand what human intelligence is, then this would represent important insights for psychometrics, cognitive science, and artificial intelligence.</a>
<a href="#6" id="6">In fact, the authors of this paper may have different stances on some of these issues, which partially explains why all of us agree that this area of research is worth being pursued and empowered in the near future.</a>
<a href="#7" id="7">Another thing that motivated this paper is that there seems to be a limited connection between these works, as many of them seem to ignore results and ideas already present in previous approaches.</a>
<a href="#8" id="8">This includes the (mis)understanding of what a computer model passing an intelligence test really means.</a>
<a href="#9" id="9">We hope that this work could facilitate and encourage any future computer model taking intelligence test problems to link with and build upon previous research.</a>
<a href="#10" id="10">This is, in fact, a lesson learned for the field of AI.</a>
<a href="#11" id="11">We have seen that even for supposedly general tasks that are designed for evaluation, many approaches have the (understandable) tendency to specialise to the task and hard-wire parts (or most) of the solution.</a>
<a href="#12" id="12">This is yet another indication of a discipline like AI that is being extremely successful in task-specific applications—from playing chess to driving a car—but yet of limited success in general-purpose systems.</a>
<a href="#13" id="13">In a nutshell, we could say that AI has become a big switch discipline.</a>
<a href="#14" id="14">This problem is being recognised in AI itself, as several benchmarks and competitions are now aiming at more general classes of problems (e.g., the general game playing competition [6]) and we hope that this paper contributes to a more widespread realisation of this when constructing new benchmarks, as relevant things are happening in AI evaluation [157], such as the proposal of a Turing championship[156].</a>
<a href="#15" id="15">As mentioned in the previous section, a very ambitious goal would be to create a repository or generator of all these problems.</a>
<a href="#16" id="16">We know that many intelligence tests are not publicly available, and many of the approaches we have surveyed here have used alternative formulations because of this.</a>
<a href="#17" id="17">It would be very useful for AI to arrange these problems, record the results of computer models and humans over them and organise competitions.</a>
<a href="#18" id="18">This first stage of the incremental roadmap suggested in Fig. 6 should have some conditions.</a>
<a href="#19" id="19">The benchmark should be broad (including a wide range of tests), standard (using some kind of general protocol for inputs and outputs), characterised (accompanied with a catalogue of information about their difficulty, the abilities they cover, etc.</a>
<a href="#20" id="20">), available (on a web or problem library) and renewed (new items are generated or disclosed so that systems cannot rote-learn them).</a>
<a href="#21" id="21">The General Game Playing competition can be used as a good example that follows all these conditions.</a>
<a href="#22" id="22">Finally, a continued and enlarged research program on computer models taking intelligence tests would help with the delimiting and, when possible, integration of the approaches discussed in Section 2 and illustrated in Fig. 5.</a>
<a href="#23" id="23">One of the key issues about these tests is to ascertain to what degree they are anthropocentric and what parts in them (or which of them) could remain as universal, hence applicable to AI systems and humans alike.</a>
</body>
</html>