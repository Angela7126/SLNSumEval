<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:188">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper discusses two novel sparse group feature selection methods.</a>
<a href="#1" id="1">Both of them are motivated from the ideal formulation of discrete feature and group selection.</a>
<a href="#2" id="2">For the first method, an efficient optimization scheme is developed based on the DC programming, accelerated gradient method and efficient projection.</a>
<a href="#3" id="3">In addition, theoretical properties on the accuracy of selection and parameter estimation are analyzed.</a>
<a href="#4" id="4">For the second method, we transform the proximal part to the sparse group subset selection problem and present a dynamic programming algorithm which is capable of finding a global optimum.</a>
<a href="#5" id="5">The projection is then fed into the Iterative Shrinkage and Thresholding Algorithm (ISTA) framework to produce a local solution for the original problem.</a>
<a href="#6" id="6">The efficiency and efficacy of the two proposed methods are validated on both synthetic data and real-world applications.</a>
<a href="#7" id="7">In the future, we plan to apply the proposed methods to other real-world applications involving the group structure.</a>
<a href="#8" id="8">Moreover, we are also interested in exploring the statistical properties, such as the error bound on parameter estimation, for the discrete optimization approach.</a>
<a href="#9" id="9">Finally, extending our approach to multi-source learning with block-wise missing data [46] is another promising direction.</a>
</body>
</html>