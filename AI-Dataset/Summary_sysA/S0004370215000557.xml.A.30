<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:361">
</head>
<body bgcolor="white">
<a href="#0" id="0">For the experiments described in this article, a Markovian model of human reward, {a mathematical formula}R ˆ H, is learned from human reward instances.</a>
<a href="#1" id="1">This model completes an MDP specification for the agent to plan in, {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#2" id="2">Thus, the output of {a mathematical formula}R ˆ H(s,a) for an agent's current state s and action a is the reward directly experienced by the learning agent.</a>
<a href="#3" id="3">In our experiments, a predictive model of human reward, {a mathematical formula}R ˆ H, is learned and provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#4" id="4">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#5" id="5">We first describe the results of the episodic-task version of the experiment specified in Section 6.1, using 19 fixed {a mathematical formula}R ˆ Hs that were pre-trained during a previous user study [13].</a>
<a href="#6" id="6">Recall that during training for this previous study, agents acted to maximize reward under {a mathematical formula} Γ =0 discounting; we address the consequences of this experiment's off-discounting character at the end of this subsection.</a>
<a href="#7" id="7">Mean task performance and mean total reward per episode for each tested discount factor across all 19 {a mathematical formula}R ˆ H models are displayed in Fig.</a>
<a href="#8" id="8">In the episodic-task analysis described here in Section 7.2.2 — as in the off-discounting analysis in the previous Section 7.2.1 — the human reward model {a mathematical formula}R ˆ H is learned by tamer and provides predictions that are interpreted as reward by an RL algorithm.</a>
<a href="#9" id="9">But unlike the previous analysis, {a mathematical formula}R ˆ H is learned while performing reinforcement learning, and the RL algorithm — not tamer — selects actions while learning {a mathematical formula}R ˆ H.</a>
<a href="#10" id="10">Thus this experiment is on-discounting, and the human trainer will be adapting to the same algorithm, with the same Γ , that is being tested.</a>
<a href="#11" id="11">The agent, task, and experiment conform to the baseline specified in Section 6.2.</a>
</body>
</html>