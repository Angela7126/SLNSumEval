<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:363">
</head>
<body bgcolor="white">
<a href="#0" id="0">RL provided several advantages over previous work with dynamic programming [9]: a single learning phase leading to the generation of multiple trajectories, better compensation for accumulated error resulting from model approximation, and lack of knowledge of the detailed system dynamics.</a>
<a href="#1" id="1">To keep the system in the proximity of the reference path, we restrict the action space {a mathematical formula}As ⊂ A to only the actions that transition the system in the proximity of the reference path, using proximity constant {a mathematical formula} Δ >0{a mathematical formula} In the case {a mathematical formula}As= ∅ , we use an alternative action subset that transitions the system to the k closest position to the reference trajectory,{a mathematical formula} where {a mathematical formula}argmink denotes k smallest elements.</a>
<a href="#2" id="2">The action selection step, or the policy, becomes the search for action that transitions the system to the highest valued state chosen from the {a mathematical formula}As subset,{a mathematical formula} The policy given in (7) ensures state transitions in the vicinity of the reference path P. If it is not possible to transition the system within given ideal proximity Δ , the algorithm selects k closest positions and selects an action that produces the best minimal residual oscillations characteristics upon transition (see Algorithm 1).</a>
<a href="#3" id="3">When the reference path is a line segment with start in {a mathematical formula}s0 and end in the origin, such as in this setup, the distance d calculation defined in (4), and needed to calculate the action space subsets (5), and (6), can be simplified and depends only the start state {a mathematical formula}s0:{a mathematical formula}</a>
<a href="#4" id="4">Both configurations use the same discount parameter {a mathematical formula} Γ <1 to ensure that the value function is finite and are learning in the sampling box 1 m around the goal state with the load displacements under {a mathematical formula}10 ∘ .</a>
<a href="#5" id="5">The section also compares experimentally the trajectory created using the learned policy to two other methods: a cubic spline trajectory, which is a minimum time {a mathematical formula}C3-class trajectory without any pre-assumptions about the load swing, and, to a dynamic programming trajectory [9], an optimal trajectory for a fixed start position with respect to its MDP setup.</a>
</body>
</html>