<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:285">
</head>
<body bgcolor="white">
<a href="#0" id="0">Given a set of reward weights the expert's MDP is completed and solved optimally to produce {a mathematical formula} Π I ⁎ .</a>
<a href="#1" id="1">Therefore, the optimal policy {a mathematical formula} Π I ⁎ under the reward function with weights Θ has the highest probability.</a>
<a href="#2" id="2">{a mathematical formula}</a>
<a href="#3" id="3">Therefore, {a mathematical formula} Μ Π I over all states is computed.</a>
<a href="#4" id="4">We may then replace {a mathematical formula} Μ Π I in Eq.</a>
<a href="#5" id="5">{a mathematical formula}</a>
<a href="#6" id="6">Let Ψ : {a mathematical formula}S×A → S map an observed robot's transition from state s given action a to a particular next state, {a mathematical formula}s ′ .</a>
<a href="#7" id="7">Therefore, the patroller may reveal its intended next state given by {a mathematical formula} Ψ (s,a) during this movement.</a>
<a href="#8" id="8">We therefore focus on learning the probability of transitioning to the intended next state given a state-action pair for an observed robot I, {a mathematical formula}TI(s,a, Ψ (s,a)), and this probability is estimated from the available trajectory data.</a>
<a href="#9" id="9">Here, {a mathematical formula}V Π I ⁎ is the optimal value function of I's MDP and {a mathematical formula}V Π IL is the value function due to utilizing the learned policy {a mathematical formula} Π IL in I's MDP.</a>
<a href="#10" id="10">Notice that when the learned reward function results in an identical optimal policy to I's optimal policy, {a mathematical formula} Π I ⁎ = Π IL, ILE will be zero; it increases monotonically as the two policies increasingly diverge in value.</a>
<a href="#11" id="11">We evaluate mIRL{a mathematical formula}/T ⁎ +Int for learning the experts' transition functions as previously described in Section 5. mIRL{a mathematical formula}/T ⁎ +Int begins by estimating the transition functions of others, after which it collapses into mIRL{sup: ⁎ }+Int learning reward weights Θ .</a>
</body>
</html>