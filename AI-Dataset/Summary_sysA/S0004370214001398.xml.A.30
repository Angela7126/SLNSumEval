<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:322">
</head>
<body bgcolor="white">
<a href="#0" id="0">Furthermore, the outcome of executing a selected action, e.g., the robot's success of returning the ball to the opponent's court with the chosen hitting movement, is not deterministic as the underlying dynamics of the robot arm are often too complicated to be modeled precisely at high speed.</a>
<a href="#1" id="1">In the policy improvement step, we improve the policy Π by a new policy {a mathematical formula} Π ′ that maximizes the expected reward according to the estimated value function {a mathematical formula}Q ˆ Π .</a>
<a href="#2" id="2">The optimal policy {a mathematical formula} Π ′ greedily chooses the action that maximizes the corresponding value function {a mathematical formula}Q ˆ Π .</a>
<a href="#3" id="3">Therefore, we obtain an improved policy{a mathematical formula} We can obtain the optimal policy by iteratively executing the policy evaluation and improvement steps, as summarized in Algorithm 1.</a>
<a href="#4" id="4">The LSPI algorithm as described above employs a model-free approach to policy learning, using the Intention-Driven Dynamics Model as a black box for updating belief {a mathematical formula} Θ t given a history of observations {a mathematical formula}z1:t. Besides the capability of belief update, the IDDM in fact provides a transition model in state space, estimated from its training data, which has not been exploited by the LSPI algorithm.</a>
<a href="#5" id="5">A typical application would be one where the robot observes the actions of a human, makes predictions about the human's intention with the IDDM, and reacts accordingly by either waiting for more observations or by executing a physical action, where the uncertainties in prediction needs to be considered in deciding when and how to plan the robot's own actions.</a>
<a href="#6" id="6">In this article, we motivated and evaluated the proposed framework with human – robot table tennis games, where a bottleneck is the limited amount of time for the robot to execute a hitting movement.</a>
<a href="#7" id="7">We concluded that anticipatory action selection substantially improves the robot's performance, and that uncertainties in prediction needs to be taken into account for optimal stopping.</a>
</body>
</html>