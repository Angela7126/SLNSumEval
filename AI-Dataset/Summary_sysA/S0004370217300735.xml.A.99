<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:199">
</head>
<body bgcolor="white">
<a href="#0" id="0">The projection of a dataset onto 3 or 4 binary variables consists of only 8 or 16 distinct cases no matter how large the original sample size is.</a>
<a href="#1" id="1">Hence progressive EM takes constant time, which we denote by {a mathematical formula}c1, on each submodel.</a>
<a href="#2" id="2">This is the key reason why HLTA can scale up.</a>
<a href="#3" id="3">The data projection takes {a mathematical formula}O(N) time for each submodel.</a>
<a href="#4" id="4">Hence the total time for island building is {a mathematical formula}O(2n(N+c1)).</a>
<a href="#5" id="5">Let {a mathematical formula}c2 be the time for each propagation step.</a>
<a href="#6" id="6">Then the hard assignment step takes {a mathematical formula}O(4nc2N) time.</a>
<a href="#7" id="7">So, the total time for the first pass through the loop in HLTA is {a mathematical formula}O(2n2N+3n(N+c1)+4nc2N)=O(2n2N+3nc1+4nc2N), where the term {a mathematical formula}3nN is ignored because it is dominated by the term {a mathematical formula}4nc2N.</a>
<a href="#8" id="8">As we move up one level, the number of “ observed ” variables is decreased by at least half.</a>
<a href="#9" id="9">Such topics can be obtained as follows: First, pick a list of words to characterize a topic using the method described in Section 4; then, form a latent class model using those words as observed variables; and finally, use the model to partition all documents into two clusters.</a>
</body>
</html>