<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:221">
</head>
<body bgcolor="white">
<a href="#0" id="0">Before evaluating the successors, the algorithm determines whether the current action {a mathematical formula}ai of the searching player i can still be the best response action against the strategy of the opponent {a mathematical formula} Σ − i ′ .</a>
<a href="#1" id="1">Alternatively, the action to play in each state can be determined based on the mixed strategy obtained by normalizing the visit counts of each action{a mathematical formula} Using the first method certainly makes the algorithm not converge to a Nash equilibrium, because the game may require a mixed strategy.</a>
<a href="#2" id="2">In order to obtain positive formal results about the convergence of SM-MCTS-like algorithms, the authors in [59] either add an additional averaging step to the algorithm (that makes it significantly slower in practical games used in benchmarks), or assume additional non-trivial technical properties about the selection function, which are not known to hold for any of the selection functions above.</a>
<a href="#3" id="3">In our implementation of iterative deepening we follow a natural observation that saves the computation time between different searches: a solution computed in state s by player i to depth d contains an optimal solution on {a mathematical formula}d − 1 approximation of subgames starting in possible next states {a mathematical formula}T(s,r,c), where r is the action selected for the player performing the search and c is the action of the opponent.</a>
</body>
</html>