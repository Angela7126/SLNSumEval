<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:278">
</head>
<body bgcolor="white">
<a href="#0" id="0">In the presence of occlusion, this equation may not be computable as the proportion of interacting states may be unknown to L. Instead, we empirically compute the state-visitation frequencies by projecting in the full environment the policy under consideration for each robot for a large number of time steps while utilizing the equilibrium behavior {a mathematical formula} Σ e when the robots interact.</a>
<a href="#1" id="1">Despite this, not knowing the probability of the expert reaching its intended state is a significant relaxation of the requirement by existing IRL methods that the full stochastic transition function inclusive of all probabilities be known.</a>
<a href="#2" id="2">The mass {a mathematical formula}1 − TI(s,a, Ψ (s,a)) could be distributed according to many possible models: uniformly across all next states excepting the intended state; to a dedicated error state; or among the intended states that would result due to performing actions other than a from s. While one could be chosen based on knowledge of the agent or robot being modeled, a general way is to choose the most likely model given the data on observed unintended transitions.</a>
<a href="#3" id="3">High occlusion presents a difficult challenge for this method: instead of randomly missing some data, much of the trajectory is missing, and furthermore the patrollers do not explore all states and actions resulting in the uniform probability distribution being assigned for most transitions.</a>
<a href="#4" id="4">Significant milestones in this rapidly emerging research area include modeling the reward function as a linear combination of features [1], utilizing the principle of maximum entropy to eliminate bias from the learned reward function [37], [8], using a Bayesian framework [32], and employing the structure of the MDP to help learn under noisy feature functions [8].</a>
</body>
</html>