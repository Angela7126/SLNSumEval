<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:313">
</head>
<body bgcolor="white">
<a href="#0" id="0">This section shows the results of our human subjects experiments on AMT, while the next section shows results against security experts, based on the experimental setting discussed in Sections 4, 5 and 6.</a>
<a href="#1" id="1">In Section 12.1 we show average defender expected utilities for five models (P-SUQR, P-BSUQR, P-RSUQR, SHARP and Maximin) against actual human subjects, for various rounds of our experiment on two payoff structures ({a mathematical formula}ADS1 and {a mathematical formula}ADS2).</a>
<a href="#2" id="2">Comparison with SUQR ({a mathematical formula}w1>0): As mentioned earlier in Section 7.1, we conduct additional human subjects experiments on {a mathematical formula}ADS1 to show that the performance of SUQR without probability weighting is worse than any of the other models.</a>
<a href="#3" id="3">We deployed an experiment on AMT with the defender strategy computed based on the SUQR model learned from round 1 data of {a mathematical formula}ADS1.</a>
<a href="#4" id="4">The resulting SUQR weight vector had a positive weight on coverage probability and thus resulted in a defender pure strategy.</a>
<a href="#5" id="5">The game was deployed with this strategy on AMT.</a>
<a href="#6" id="6">60 people played the game, and out of them 48 participants passed the validation test.</a>
<a href="#7" id="7">For our experimental results, we considered the data from only the participants who passed the validation test.</a>
<a href="#8" id="8">The average expected defender utility obtained was − 4.75.</a>
<a href="#9" id="9">This average expected defender utility obtained by deploying a pure strategy based on a learned SUQR model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#10" id="10">Comparison with RL based approach: We conducted human subjects experiments on {a mathematical formula}ADS1 with the RL based approach (Algorithm 1) to compare its performance against our behavioral models.</a>
<a href="#11" id="11">We deployed an experiment on AMT with the defender strategy computed based on the RL model learned from round 1 data of Maximin on {a mathematical formula}ADS1 (as explained earlier in Section 11).</a>
<a href="#12" id="12">The average expected defender utility obtained was − 4.139.</a>
</body>
</html>