<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:210">
</head>
<body bgcolor="white">
<a href="#0" id="0">kLog generates a set of features starting from a logical and relational learning problem and uses these features for learning a (linear) statistical model.</a>
<a href="#1" id="1">kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.</a>
<a href="#2" id="2">Having specified a target relation r, kLog is able to infer the partition {a mathematical formula}x ∪ y of ground atoms into inputs and outputs in the supervised learning setting.</a>
<a href="#3" id="3">Learning in kLog is performed using a suitable graph kernel on the graphicalized interpretations.</a>
<a href="#4" id="4">As kLog is a language for logical and relational learning with kernels it is related to work on inductive logic programming, to statistical relational learning, to graph kernels, and to propositionalization.</a>
<a href="#5" id="5">One difference between these statistical relational learning models and kLog is that the former do not really have a second level as does kLog.</a>
<a href="#6" id="6">kLog builds also upon the many results on learning with graph kernels, see [73] for an overview.</a>
<a href="#7" id="7">Structured output learning problems can be naturally defined within kLog's semantics: graphicalization followed by a graph kernel yields a joint feature vector {a mathematical formula} Φ (x,y) where y are the groundings of the output predicates.</a>
</body>
</html>