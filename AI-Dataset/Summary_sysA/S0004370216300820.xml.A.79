<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:219">
</head>
<body bgcolor="white">
<a href="#0" id="0">In our recent work [16], we proposed a method that exploits the structural knowledge derived from semantic networks, together with distributional statistics from text corpora, to produce effective representations of individual word senses or concepts.</a>
<a href="#1" id="1">We put forward an approach that allows us to plug in an arbitrary word embedding representation with that of our lexical vector representations, providing three main advantages: (1) benefiting from the word-based knowledge derived as a result of learning from massive corpora for our sense-level representation; (2) reducing the dimensionality of our lexical space to a fixed-size continuous space; and (3) providing a shared semantic space between words and synsets (more details in Section 4), hence enabling a direct comparison of words and synsets.</a>
<a href="#2" id="2">Specifically, given an input text {a mathematical formula}T and a space of word embeddings E, we first calculate the lexical vector of {a mathematical formula}T (i.e., {a mathematical formula}v → lex(T)) as explained in Section 3.2 and then map our lexical vector to the semantic space E as follows:{a mathematical formula} where {a mathematical formula}E(w) is the embedding-based representation of the word w in E, and {a mathematical formula}rank(w,v → lex(T)) is the rank of the dimension corresponding to the word w in the lexical vector {a mathematical formula}v → lex(T), thus giving more importance to the higher weighted dimensions.</a>
</body>
</html>