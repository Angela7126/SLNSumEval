<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:203">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper proposes a middle ground between LfD and RL, introducing abstraction from demonstration algorithms, which take a small set of demonstrations from human teachers in order to infer the relevant features of the state space at each moment.</a>
<a href="#1" id="1">Then using this abstracted state space allows significant speed-ups in reinforcement learning algorithms with respect to the number of state features in complex domains.</a>
<a href="#2" id="2">Our experiments show that these speed-ups are exponential when using table-based representations and polynomial when used in conjunction with function approximation-based RL algorithms such as fitted Q-learning or LSPI.</a>
<a href="#3" id="3">Our first algorithm, state abstraction from demonstration (AfD) learns a policy for an MDP by building an abstract space {a mathematical formula}S 품 and using reinforcement learning to find an optimal policy that can be represented in {a mathematical formula}S 품 .</a>
<a href="#4" id="4">AfD obtains {a mathematical formula}S 품 by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S 품 can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
</body>
</html>