<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:202">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, for {a mathematical formula}I={s0}, as considered in this paper, where only a subset of S is reachable from {a mathematical formula}s0, we cannot guarantee that all states are updated infinitely often, and therefore Theorem 2.9 cannot be directly employed to prove the convergence of RTDP.</a>
<a href="#1" id="1">Thus, in order to prove the convergence of RTDP with {a mathematical formula}I={s0}, we have to guarantee that asynchronous DP is performed for the set of relevant states (Definition 2.7).</a>
<a href="#2" id="2">Note that the maximization {a mathematical formula}maxp → ∈ Ka in Equation (21) is performed by applying the MaxParameterOut operation on a PADD, i.e., by calling a nonlinear solver to maximize each PADD leaf, which results in an ADD (this operation corresponds to choosing the worst model for an abstract state and action pair).</a>
<a href="#3" id="3">In order to sample the next state, three different methods are considered for choosing a value for each probability parameter {a mathematical formula}pi, subject to a set of constraints Φ : (1) the use of the same parameter values computed in the Bellman update called minimax_parameter_choice; (2) computing a random legal parameter value at each step during the trial called rand_parameter_choice; (3) computing a predefined legal parameter value only once called predefined_parameter_choice.</a>
</body>
</html>