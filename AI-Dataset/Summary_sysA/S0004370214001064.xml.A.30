<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:205">
</head>
<body bgcolor="white">
<a href="#0" id="0">The key contributions of this framework are threefold: (1) kLog is a language that allows users to declaratively specify relational learning tasks in a similar way as statistical relational learning and inductive logic programming approaches but it is based on kernel methods rather than on probabilistic modeling; (2) kLog compiles the relational domain and learning task into a graph-based representation using a technique called graphicalization; and (3) kLog uses a graph kernel to construct the feature space where eventually the learning takes place.</a>
<a href="#1" id="1">When {a mathematical formula}n=0, the declared job consists of predicting one or more properties of an entire interpretation, when {a mathematical formula}n=1 one or more properties of certain entities, when {a mathematical formula}n=2 one or more properties of pairs of entities, and so on.</a>
<a href="#2" id="2">In spite of the advantage of MLN for using a collective inference approach, results (Table 7) are comparable to those obtained with kLog (MLN tends to overpredict the class “ student ” , resulting in a slightly lower average {a mathematical formula}F1 measure, but accuracies are identical).</a>
<a href="#3" id="3">Thus the feature extracted by kLog using the graph kernel are capable of capturing enough contextual information from the input portion of the data to obviate the lack of collective inference.</a>
</body>
</html>