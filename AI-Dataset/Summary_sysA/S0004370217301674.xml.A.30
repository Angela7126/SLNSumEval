<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:177">
</head>
<body bgcolor="white">
<a href="#0" id="0">For instance, by considering a system with M actuators (an M-dimensional action space) and N discrete actions in each one, a DRL modeling leads to evaluating and storing NM values per state instead of {a mathematical formula}NM as a centralized RL does.</a>
<a href="#1" id="1">Algorithm 1 is described for the most general case of a fully-decentralized system with individual rewards, where states and rewards are annotated as {a mathematical formula}sm and {a mathematical formula}rm respectively, but it is also possible to implement a joint state vector or common reward DRL systems.</a>
<a href="#2" id="2">In addition, note that RL parameters could have been defined separately per agent ({a mathematical formula} Α m, Γ m), which is one of the DRL properties remarked in Section 2.3, but in Algorithm 1 they appear unified just for the sake of simplicity.</a>
<a href="#3" id="3">Since 17 discrete actions per agent are implemented for the DRL system, if an equivalent CRL system were implemented, that centralized agent would search in an action space of {a mathematical formula}173=4913 possible actions, which would be enormous for most of the RL algorithms.</a>
</body>
</html>