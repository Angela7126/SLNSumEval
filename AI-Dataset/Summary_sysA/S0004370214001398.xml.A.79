<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:349">
</head>
<body bgcolor="white">
<a href="#0" id="0">In the table tennis scenario, this Intention-Driven Dynamics Model (IDDM) leads to an online algorithm that, given a time series of observations, continually predicts the human player's intended target, i.e., where the human intends to shoot the ball [53], as shown in Fig.</a>
<a href="#1" id="1">Furthermore, the outcome of executing a selected action, e.g., the robot's success of returning the ball to the opponent's court with the chosen hitting movement, is not deterministic as the underlying dynamics of the robot arm are often too complicated to be modeled precisely at high speed.</a>
<a href="#2" id="2">The states evolve following a Markov transition model, governed by {a mathematical formula}P, where {a mathematical formula}P(s ′ |s,a) represents the probability of going to state {a mathematical formula}s ′ from state s when taking action a.</a>
<a href="#3" id="3">The observations are generated from the states following the observation model Ω , where {a mathematical formula} Ω (z|s) is the probability of observing z in state s. The reward function {a mathematical formula}R(s,a) represents the expected immediate reward obtained for taking action a in state s.</a>
<a href="#4" id="4">We, hence, reuse the estimated value function for stopping actions by LSPI, and focus on the value function for the waiting action {a mathematical formula}Q( Θ t,at=waiting), given by the expected future reward{a mathematical formula} with respect to the subsequent belief state {a mathematical formula} Θ t+1.</a>
<a href="#5" id="5">Nevertheless, consider the online planning that finds the optimal action{a mathematical formula} where one only needs to know if the waiting action leads to higher expected reward than all the stopping actions, rather than the actual expected reward of waiting.</a>
<a href="#6" id="6">We can terminate the particle projection routine if the expected reward of waiting is very likely to be higher or lower than that of the optimal stopping action {a mathematical formula}maxa ≠ 0 ⁡ Q( Θ t,a).</a>
<a href="#7" id="7">Experimental results using real data and a simulated environment showed that the anticipatory action selection can be used for a robot table tennis player to enhance its performance against human players, where the robot decided the timing to initiate a selected hitting movement according to the prediction of the human opponent.</a>
</body>
</html>