<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:232">
</head>
<body bgcolor="white">
<a href="#0" id="0">All minimal plans that pass the previous filtering tests are merged into a partial policy.</a>
<a href="#1" id="1">Recall that a policy in an MDP is a function that returns the action to execute for each state (cf.</a>
<a href="#2" id="2">Section 2.1).</a>
<a href="#3" id="3">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#4" id="4">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#5" id="5">The function {a mathematical formula}o:S → Sm is the mapping from the states of the MDP to the states of the model introduced at the beginning of this section.</a>
<a href="#6" id="6">This partial policy is used in the reinforcement learning phase to define an MDP on which the agent learns the optimal policy.</a>
<a href="#7" id="7">The partial policy is defined over the state space of the original MDP, to allow the learning algorithm to adapt to the real environment, and not to the model.</a>
<a href="#8" id="8">In this section, we analyze how the model {a mathematical formula}Dm affects the ability of the agent to learn an optimal policy.</a>
</body>
</html>