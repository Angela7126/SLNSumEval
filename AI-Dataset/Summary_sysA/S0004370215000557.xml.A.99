<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:389">
</head>
<body bgcolor="white">
<a href="#0" id="0">Second, to account for delays in giving feedback, the causal attribution of each reward is distributed across multiple recent time steps by tamer's credit assignment module [11], further adding variety to the label values of samples for training {a mathematical formula}R ˆ H.</a>
<a href="#1" id="1">Unlike with the mountain-car task, these experiments are on-discounting, involving an agent being trained with the same discount factor by which it calculates return when learning a value function (and thus a policy).</a>
<a href="#2" id="2">In other words, the agent's behavior during training reflects its discount factor, removing the bias inherent in the mountain-car task.</a>
<a href="#3" id="3">Additionally, these results provide initial evidence of the presence of positive circuits: compared with agents optimizing for low Γ values, agents optimizing for high Γ values learn policies that accrue more human reward but do not reach the goal, and a sampling of the learned policies for {a mathematical formula} Γ =0 showed consistently circuitous behavior.</a>
<a href="#4" id="4">At {a mathematical formula} Γ =1, if the RL algorithm learns an infinitely repeatable sequence of actions with positive net reward, then the disastrous policy that repeats that sequence is necessarily within the set of MDP-optimal policies.</a>
<a href="#5" id="5">As mentioned above, we visually checked the behavior of five human models' corresponding algorithms while they exhibited the worst possible performance, and each agent repeatedly visited roughly the same circuit of states until the episode limit was reached.</a>
<a href="#6" id="6">We do not directly investigate the effects of giving positive reward from absorbing state, but the strategy in the following section — making the task appear continuing — is equivalent to setting the absorbing-state reward such that its resulting return is equal to the agent's discounted expectation of return from the distribution of starting states, assuming the value function is accurate for the policy it assesses.</a>
<a href="#7" id="7">From analyzing these results, we believe that adding the failure state affected the ease of training in both positive and negative ways.</a>
<a href="#8" id="8">In comparison to the goal-only task, on the other hand, the avi-tamer algorithm performed better overall in both the continuing and episodic versions of the failure-task; this increase might be due in part to the failure state being used as an intermediate “ goal ” that the learner makes updates for, goes to, and then gets experience and reward for those states near it, which then help the agent go to the real goal.</a>
</body>
</html>