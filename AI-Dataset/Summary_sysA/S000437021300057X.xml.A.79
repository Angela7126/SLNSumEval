<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:207">
</head>
<body bgcolor="white">
<a href="#0" id="0">We see that a POMDP extends an MDP by adding a finite set of observations {a mathematical formula}O and a corresponding observation model {a mathematical formula}P. Since the agent is unable to directly observe the world ʼ s current state, it maintains a probability distribution over states, called a belief stateb, reflecting its estimate of their likelihood:{a mathematical formula} where {a mathematical formula}b(s) is the probability that the current state is s, inferred from the previous belief state, the most recent action, and the resulting observation.</a>
<a href="#1" id="1">Suppose the quality of the current artifact is q, we define the conditional distribution {a mathematical formula}P(q ′ |q,x) to be {a mathematical formula}Beta(10 Μ Q ′ |q,x,10(1 − Μ Q ′ |q,x)), with mean {a mathematical formula} Μ Q ′ |q,x, where{a mathematical formula} Thus, the resulting distribution of qualities is influenced by the worker ʼ s accuracy and the hardness of an improvement, indicated by the quality of the original artifact, q.</a>
<a href="#2" id="2">{sup:15} By introducing {a mathematical formula} Μ Qw ′ , we separate the variance in a worker ʼ s ability in improving all artifacts of the same quality from the variance in our training data, which is due to her starting out from artifacts of different qualities.</a>
</body>
</html>