<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:323">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}S is the finite set of states;</a>
<a href="#1" id="1">{a mathematical formula}P(s ′ |s,a) represents the probability that {a mathematical formula}s ′ ∈ S is reached after applying action {a mathematical formula}a ∈ A in state {a mathematical formula}s ∈ S; and</a>
<a href="#2" id="2">Assumption 2</a>
<a href="#3" id="3">Given a closed policy Π , {a mathematical formula}V Π (s) is the expected accumulated cost to reach a goal state from state {a mathematical formula}s ∈ S Π .</a>
<a href="#4" id="4">Definition 3</a>
<a href="#5" id="5">Ε -consistentGiven an SSP {a mathematical formula}S, a value function V for {a mathematical formula}S is Ε -consistent if{a mathematical formula} where {a mathematical formula}S ′ =S Π V, i.e., the states reachable from {a mathematical formula}s0 when following a greedy policy {a mathematical formula} Π V.</a>
<a href="#6" id="6">The functions {a mathematical formula}R(s,V) and {a mathematical formula}R(S,V) are known as the Bellman residual w.r.t.</a>
<a href="#7" id="7">V of the state s and the SSP {a mathematical formula}S, respectively.</a>
<a href="#8" id="8">By (2), if V is 0-consistent, then V equals {a mathematical formula}V ⁎ .</a>
<a href="#9" id="9">Definition 8</a>
<a href="#10" id="10">Another important relationship between SSPs and short-sighted SSPs is through their policies.</a>
<a href="#11" id="11">To formalize this relationship, we first define the concept of t-closed policy w.r.t.</a>
<a href="#12" id="12">s, i.e., policies that can be executed from s independent of the probabilistic outcome of actions for at least t actions without replanning: t-closed policyA policy Π for an SSP {a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 is t-closed w.r.t.</a>
<a href="#13" id="13">a state {a mathematical formula}s ∈ S if, for all {a mathematical formula}s ′ ∈ R Π ∩ S Π , {a mathematical formula} Δ (s,s ′ ) ≥ t.</a>
<a href="#14" id="14">SSiPP obtains the next state {a mathematical formula}s ′ from the current state s by either executing or sampling one outcome of the optimal policy {a mathematical formula} Π Ss,t ⁎ of the current short-sighted SSP (Algorithm 1 line 11).</a>
<a href="#15" id="15">This procedure is repeated until {a mathematical formula}s ′ is a goal state, either from the original SSP or an artificial goal.</a>
</body>
</html>