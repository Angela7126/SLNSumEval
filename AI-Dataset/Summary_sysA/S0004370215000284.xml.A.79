<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:246">
</head>
<body bgcolor="white">
<a href="#0" id="0">In summary, we propose a RL algorithm that can request demonstrations from a teacher whenever it requires new unknown actions to complete a task, which significantly reduces the number of experiences required to learn.</a>
<a href="#1" id="1">Experiences comprises a set of triples {a mathematical formula}E=(s,a,s ′ ), which represent the new state s obtained after executing the action a in state s. We say that a rule covers an experience when the rule preconditions are satisfied by s, the rule represents the action executed {a mathematical formula}ar=a, and the effects of the rule are the changes between the initial and the final state for the experience {a mathematical formula} Ω r,i=s ′ ∖ s.</a>
<a href="#2" id="2">In addition, even if the teacher knows the optimal (or near optimal) policy, implementing a policy in a robot is a very tedious task, which may take a much longer time to model and to test properly compared with learning it using the proposed algorithm.</a>
<a href="#3" id="3">Although the actual number of experiences required to learn this domain is much lower (the rule only has four preconditions and the initial state also limits the number of incorrect experiences that can be performed), these theoretical numbers reflect the reduction in complexity obtained through demonstrations.</a>
<a href="#4" id="4">We combined the generalization capabilities of learning in relational domains, where the same type of objects have to be learned only once, with teacher demonstrations, thereby significantly reducing the number of exploration actions required, which is the longest part of the learning process.</a>
</body>
</html>