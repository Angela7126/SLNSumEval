<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:189">
</head>
<body bgcolor="white">
<a href="#0" id="0">This section compares 8 algorithms on learning low-rank kernel: (1) LibSVM [46]: support vector machine, which uses only the small number of labeled samples for training; (2) Nyström: standard Nyström method; (3) CSI: Cholesky with Side Information [2]; (4) Cluster: cluster kernel [47]; (5) Spectral: non-parametric spectral graph kernel [17]; (6) Breg: low-rank kernel learning with Bregman divergence [37]; (7) NCNM [44]: Sparse Gaussian Process for Semi-supervised Learning; and (8) Our proposed method.</a>
<a href="#1" id="1">Most algorithms can learn the {a mathematical formula}n×n low-rank kernel{sup:2} matrix on labeled and unlabeled samples{sup:3} in the form of {a mathematical formula}K=GG ⊤ , which can then be fed into an SVM for classification.</a>
<a href="#2" id="2">The resultant problem will be a linear SVM using G as training/testing samples [34].</a>
<a href="#3" id="3">Method (1) and (2) are baseline methods that do not involve any kernel learning, therefore they are computationally very efficient.</a>
<a href="#4" id="4">Note that Method (1) only uses labeled samples for training, so it has higher error rates.</a>
<a href="#5" id="5">Method (3) is computationally efficient as well because it only uses labeled samples for training.</a>
<a href="#6" id="6">In this paper, we proposed an efficient kernel low-rank decomposition algorithm called the generalized Nyström method.</a>
</body>
</html>