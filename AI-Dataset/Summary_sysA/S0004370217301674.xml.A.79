<html>
<head>
<meta name="TextLength" content="SENT_NUM:2, WORD_NUM:145">
</head>
<body bgcolor="white">
<a href="#0" id="0">Since from a computational point of view, all the individual agents in a DRL system can operate in parallel acting upon their individual and reduced action spaces, the learning speed is typically higher compared to a centralized agent which searches an exponentially larger action space {a mathematical formula}N=N1× ⋯ ×NM, as expressed in (2)[39].</a>
<a href="#1" id="1">Stage3.5 — Completing RL single modelings the learning procedure is defined as follows: goal positions are defined in such a way that they are always reachable for the robot; thus, the learning process needs to develop an internal model of the inverse kinematics of the robot which is not directly injected by the designer; through the different trials, a model of the robot inverse kinematics is learned by the system; when a goal position is generated, the robot tries to reach it; each trial can finish as a success episode, i.e.</a>
</body>
</html>