<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:254">
</head>
<body bgcolor="white">
<a href="#0" id="0">We now consider a single linear layer with k output units{a mathematical formula} In this case, dropout applied to input units is slightly different from dropout applied to the connections.</a>
<a href="#1" id="1">Dropout applied to the input units leads to the random variables{a mathematical formula} whereas dropout applied to the connections leads to the random variables{a mathematical formula} In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectation and the independence assumption.</a>
<a href="#2" id="2">When dropout is applied to the input units, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula}</a>
<a href="#3" id="3">When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or the weights, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#4" id="4">This formula can be applied recursively across the entire network, starting from the input layer.</a>
<a href="#5" id="5">The proof is similar to the previous case, interchanging x and {a mathematical formula}1 − x.</a>
<a href="#6" id="6">Throughout the rest of this article, we let {a mathematical formula}Wil= Σ (Uil) denote the deterministic variables of the dropout approximation (or ensemble network) with{a mathematical formula} in the case of dropout applied to the nodes.</a>
<a href="#7" id="7">For neurons whose activities are close to 0 or 1, and thus in general for neurons towards the end of learning, these two bounds are similar to each other.</a>
<a href="#8" id="8">This is not the case at the beginning of learning when, with very small weights and a standard logistic transfer function, {a mathematical formula}Wil=0.5 and {a mathematical formula}Var(Oil) ≈ 0 (Fig.</a>
</body>
</html>