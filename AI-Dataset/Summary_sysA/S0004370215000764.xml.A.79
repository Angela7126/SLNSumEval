<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:262">
</head>
<body bgcolor="white">
<a href="#0" id="0">For two different algorithms and tasks, they search over a broad set of possible task and agent specific intrinsic rewards and find rewards that make the agent learn faster than if it solely used external rewards.</a>
<a href="#1" id="1">These methods affect the agent's exploration in the domain to speed up learning, but they typically require the user to have specific knowledge about what constitutes improvement in the task.</a>
<a href="#2" id="2">texplore's model learning algorithm starts by calculating the relative change in the state ({a mathematical formula}srel), then it updates the model for each feature with the new transition and updates the reward model.</a>
<a href="#3" id="3">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#4" id="4">For particularly difficult state transitions, the agent's model may make an incorrect prediction in all of its forest trees, and as some of the trees start to learn the transition, the variance increases and the agent moves to explore the true dynamics of how it works.</a>
<a href="#5" id="5">In the light world domain, by the time the algorithm has determined error is improving in a region, the agent has already learned a model of that region and no longer needs to explore there.</a>
<a href="#6" id="6">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
</body>
</html>