<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:223">
</head>
<body bgcolor="white">
<a href="#0" id="0">In the model, the actions are represented as a set of action rules, which can be applied over a state space.</a>
<a href="#1" id="1">Therefore, we propose the use of a rule analysis approach to provide some guidance to the teacher so he can demonstrate the unknown parts needed by the decision maker.</a>
<a href="#2" id="2">In summary, we propose a RL algorithm that can request demonstrations from a teacher whenever it requires new unknown actions to complete a task, which significantly reduces the number of experiences required to learn.</a>
<a href="#3" id="3">The main problem for algorithms without demonstrations is finding positive examples.</a>
<a href="#4" id="4">As explained in Proposition 1, we can calculate the number of experiences that would be required to obtain a positive example with the pick-up(X,Y) action in the worst case.</a>
<a href="#5" id="5">Although the actual number of experiences required to learn this domain is much lower (the rule only has four preconditions and the initial state also limits the number of incorrect experiences that can be performed), these theoretical numbers reflect the reduction in complexity obtained through demonstrations.</a>
<a href="#6" id="6">These experiments were performed in the VR system since a very large number of teacher requests were required to obtain statistically significant results, which would have been tedious for a human teacher.</a>
<a href="#7" id="7">Therefore, an automated teacher that planned with the correct rule set was used to perform the experiments described in this section.</a>
</body>
</html>