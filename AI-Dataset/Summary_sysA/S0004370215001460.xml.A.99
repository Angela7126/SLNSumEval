<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:216">
</head>
<body bgcolor="white">
<a href="#0" id="0">Once Θ is learned, and given expert predictions {a mathematical formula}y ˆ i, aggregated prediction {a mathematical formula}yi for the ith data point can be found as a mean of the posterior distribution {a mathematical formula}yi|xi,y ˆ iK ∼ N(y ‾ i,(1T Σ − 11) − 1), where mean {a mathematical formula}y ‾ i is computed as{a mathematical formula} with {a mathematical formula}y ˆ i=[(y ˆ iK)T, Μ i]T, and Σ is a {a mathematical formula}(K+1)×(K+1) block matrix equal to{a mathematical formula} Interestingly, we can see that the prior mean {a mathematical formula} Μ i can be viewed as an additional expert with variance {a mathematical formula} Σ 2, which is independent from the original K experts.</a>
<a href="#1" id="1">Thus, to simplify the presentation, in the following we consider the prior mean as the {a mathematical formula}(K+1)th expert that is always available to the aggregation algorithm (i.e., it can never be missing).</a>
<a href="#2" id="2">Interestingly, for purely unlabeled data set, the method with informative prior obtained slightly worse result than the method without, which is due to overfitting to the estimated ground-truth values for unlabeled data points (i.e., the prior model learns to predict other experts' predictions), and can be mitigated with stronger regularization when number of labeled data points is small or by increasing size of the training set.</a>
</body>
</html>