<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:366">
</head>
<body bgcolor="white">
<a href="#0" id="0">We then repeat one of the episodic-task experiments in a continuing version of the same task, in what we call the continuing-task experiment.</a>
<a href="#1" id="1">We make the following four observations.</a>
<a href="#2" id="2">First, we find for discount factors {a mathematical formula} Γ <1 that task performance of MDP-optimal policies during training is generally high and independent of discounting, a pattern that is quite different from that seen in the episodic setting.</a>
<a href="#3" id="3">Second, several strong correlations observed in the episodic-task grid-world experiment disappear, including the relationships between Γ and both reward positivity and task performance.</a>
<a href="#4" id="4">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#5" id="5">Fourth, in two additional tests using the training data from this continuing-task experiment, we find evidence for the theoretically based conjecture that low discount rates (high Γ s) facilitate the communication of higher-level task information — e.g., the location of the goal rather than the exact sequence of steps to reach it.</a>
<a href="#6" id="6">Such higher-level information enables learning that is more robust to environmental changes, better guides action in unexperienced states, and has the potential to lead the agent to learn policies that surpass those known to the trainer.</a>
<a href="#7" id="7">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#8" id="8">Fig.</a>
<a href="#9" id="9">2 illustrates this scenario.</a>
<a href="#10" id="10">During training, human reward signals form labels for learning samples that have state-action pairs as features; a regression algorithm continuously updates {a mathematical formula}R ˆ H with new training samples.</a>
<a href="#11" id="11">Additionally, all {a mathematical formula}R ˆ H models and all value functions are initialized to 0.</a>
<a href="#12" id="12">We observe that the results for the VI-cont condition are similar to that of the equivalent {a mathematical formula} Γ =0.99 condition in the continuing-task experiment (shown in Fig.</a>
<a href="#13" id="13">10, Fig.</a>
<a href="#14" id="14">14).</a>
<a href="#15" id="15">The performance is insignificantly higher in this experiment by Fisher's exact test of whether the agent reached the goal 10 times ({a mathematical formula}p=0.1085).</a>
</body>
</html>