<html>
<head>
<meta name="TextLength" content="SENT_NUM:3, WORD_NUM:268">
</head>
<body bgcolor="white">
<a href="#0" id="0">Instead, we follow a hierarchical approach, where we learn an upper-level policy {a mathematical formula} Π ( Ω |s), which provides the lower-level controller parametrization Ω given the context s. Our goal is to find the optimal policy {a mathematical formula} Π ⁎ ( Ω |s), such that it maximizes the expected reward{a mathematical formula} where {a mathematical formula}Rs Ω denotes the expected reward when executing the lower-level policy with parameter Ω in context s. As the dynamics of the robot and its environment are stochastic, the reward {a mathematical formula}Rs Ω is given by the expected reward over all trajectories{a mathematical formula} The probability of a trajectory {a mathematical formula}p( Τ |s, Ω ) now also depends on the context and the reward function {a mathematical formula}R( Τ ,s) now also depend on the context s. Using our motivating example, for the throwing task, we might define the reward function as the minimal distance to the target that is now defined by the context, i.e., {a mathematical formula}R( Τ ,s)= − mint ⁡ ‖ s − bt ‖ 2, where now the context defines the target position to throw to, {a mathematical formula}s=[px,py]T. We also illustrate the basic concept of (model-free) contextual policy search in Fig.</a>
<a href="#1" id="1">A smaller value of Ε results in more conservative policy updates, while a higher Ε leads to faster converging policies.</a>
<a href="#2" id="2">As the context distribution {a mathematical formula} Μ (s) is defined by the learning problem and cannot be chosen by the learning algorithm, the constraints {a mathematical formula} ∫ Ω p(s, Ω )= Μ (s), ∀ s must also be satisfied.</a>
</body>
</html>