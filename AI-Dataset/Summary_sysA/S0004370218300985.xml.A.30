<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:275">
</head>
<body bgcolor="white">
<a href="#0" id="0">Updates to a deep layer with RBP or SRBP appear to require updates in the precedent layers in the learning channel.</a>
<a href="#1" id="1">If we fix the weights in layer h, while updating the rest of the layers with SRBP, performance is often worse than if we fix layers {a mathematical formula}l ≤ h.</a>
<a href="#2" id="2">In all these RBPs algorithms, the L-layer at the top with parameters {a mathematical formula}wijL follows the gradient, as it is trained just like BP, since there are no random feedback weights used for learning in the top layer.</a>
<a href="#3" id="3">In other words, BP = RBP = SRBP for the top layer.</a>
<a href="#4" id="4">Let us denote by {a mathematical formula}a1 and {a mathematical formula}a2 the weights in the first and second layer, and by {a mathematical formula}c1 the random weight of the learning channel.</a>
<a href="#5" id="5">Consider a linear {a mathematical formula}A[1,N,1] architecture (Fig.</a>
<a href="#6" id="6">The main conclusion is that the postsynaptic terms must: (1) implement gradient descent for the top layer (i.e.</a>
<a href="#7" id="7">random weights in the learning channel for the top layer do not work at all); (2) for any other deep layer h it should be of the form {a mathematical formula}f ′ F(T − O), where {a mathematical formula}f ′ represents the local derivatives of the activations of the units in layer h (the derivatives for the layers above are not necessary) and F is some reasonable function of the error {a mathematical formula}T − O.</a>
<a href="#8" id="8">By reasonable, we mean that the function F can be linear, or a composition of linear propagation with non-linear activation functions, it can be fixed or slowly varying, and when matrices are involved these can be random, sparse, etc.</a>
</body>
</html>