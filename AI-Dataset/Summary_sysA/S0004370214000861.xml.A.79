<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:181">
</head>
<body bgcolor="white">
<a href="#0" id="0">AfD obtains {a mathematical formula}S Α by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S Α can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
<a href="#1" id="1">ADA+RL adds another step, policy improvement, in which we use reinforcement learning techniques to find the optimal policy that can be represented in the abstract state space {a mathematical formula}S Α .</a>
<a href="#2" id="2">Recent TD methods [58] offer attractive theoretical properties, but so far these methods are limited to prediction, i.e., determining the value function {a mathematical formula}V Π of a given policy Π , as opposed to finding the optimal policy {a mathematical formula} Π ⁎ .</a>
<a href="#3" id="3">Unfortunately, in complex domains, a random behavior policy does not visit the interesting parts of the state space often enough for effective learning, and the algorithm is no longer stable with a varying behavior policy.</a>
</body>
</html>