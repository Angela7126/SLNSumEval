<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:166">
</head>
<body bgcolor="white">
<a href="#0" id="0">The Random SAT+UNSAT track consisted of three random benchmarks detailed in Appendix A.3: 3cnf, K3, and unif-k5.</a>
<a href="#1" id="1">The instances in unif-k5 are all unsatisfiable, while the other two sets contain both satisfiable and unsatisfiable instances.</a>
<a href="#2" id="2">Fig.</a>
<a href="#3" id="3">6 visualizes the improvements achieved by configuration on these benchmarks for the best-performing solver Clasp-3.0.4-p8.</a>
<a href="#4" id="4">Clasp-3.0.4-p8 benefited most from configuration on benchmark 3cnf, where it reduced the number of timeouts from 18 to 0.</a>
<a href="#5" id="5">For the other benchmarks, it could already solve all instances in its default parameter configuration, but configuration helped reduce its average runtime by factors of 3 (K3) and 2 (unif-k5), respectively.</a>
<a href="#6" id="6">5sat500 This set contains 500 5-SAT instances generated uniformly at random with a clause-to-variable ratio of 20.</a>
<a href="#7" id="7">Each instance is satisfiable and has 500 variables and 10000 clauses.</a>
<a href="#8" id="8">This set was first used for tuning the SAT solver Captain Jack and other SLS solvers [81].</a>
<a href="#9" id="9">We used the original uniform random split into 250 training and test instances in the 2014 CSSC (random satisfiable track).</a>
</body>
</html>