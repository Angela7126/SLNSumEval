<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:253">
</head>
<body bgcolor="white">
<a href="#0" id="0">When dropout is applied to the connections, under similar independence assumptions, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#1" id="1">This formula can be applied recursively across the entire network.</a>
<a href="#2" id="2">Note again that although the expectation {a mathematical formula}E(Sih) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the previous layers ({a mathematical formula}l<h) matter.</a>
<a href="#3" id="3">Therefore it is also the expectation taken over only all the induced subnetworks of node i (corresponding to all the ancestors of node i).</a>
<a href="#4" id="4">Furthermore, using these expectations, all the covariances can also be computed recursively from the input layer to the output layer using a similar analysis to the one given above for the case of dropout applied to the units of a general linear network.</a>
<a href="#5" id="5">Consider unit i in the output layer T receiving a connection from unit j in a layer l (typically {a mathematical formula}l=T − 1) with weight {a mathematical formula}wijTl.</a>
<a href="#6" id="6">The gradient of the error function in the dropout network is given by{a mathematical formula} using the notation of Section 9.5: {a mathematical formula}SijTl=Sil − wijTl Δ jlOjl.</a>
<a href="#7" id="7">Using a first order Taylor expansion to separate out independent terms gives:{a mathematical formula} We can now take the expectation of the gradient{a mathematical formula} Now, using the NWGM approximation {a mathematical formula}E( Σ (SijTl)) ≈ Σ (E(SijTl))= Σ (UijTl)=WijTl ≈ WiT − Σ ′ (UiT)wijTlpjlWjl{a mathematical formula} which has the form{a mathematical formula} where A has the complex expression given by Eq.</a>
<a href="#8" id="8">(179).</a>
</body>
</html>