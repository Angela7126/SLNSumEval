<html>
<head>
<meta name="TextLength" content="SENT_NUM:2, WORD_NUM:272">
</head>
<body bgcolor="white">
<a href="#0" id="0">Instead, we follow a hierarchical approach, where we learn an upper-level policy {a mathematical formula} Π ( Ω |s), which provides the lower-level controller parametrization Ω given the context s. Our goal is to find the optimal policy {a mathematical formula} Π ⁎ ( Ω |s), such that it maximizes the expected reward{a mathematical formula} where {a mathematical formula}Rs Ω denotes the expected reward when executing the lower-level policy with parameter Ω in context s. As the dynamics of the robot and its environment are stochastic, the reward {a mathematical formula}Rs Ω is given by the expected reward over all trajectories{a mathematical formula} The probability of a trajectory {a mathematical formula}p( Τ |s, Ω ) now also depends on the context and the reward function {a mathematical formula}R( Τ ,s) now also depend on the context s. Using our motivating example, for the throwing task, we might define the reward function as the minimal distance to the target that is now defined by the context, i.e., {a mathematical formula}R( Τ ,s)= − mint ⁡ ‖ s − bt ‖ 2, where now the context defines the target position to throw to, {a mathematical formula}s=[px,py]T. We also illustrate the basic concept of (model-free) contextual policy search in Fig.</a>
<a href="#1" id="1">The expected reward {a mathematical formula}Rs Ω =E Τ [R( Τ ,s)|s, Ω ] can be estimated by multiple samples from the trajectory distribution {a mathematical formula}p( Τ |s, Ω ), that is,{a mathematical formula} where the trajectories are now generated using the learned forward models in computer simulation and we will assume that the trajectory-dependent reward function {a mathematical formula}R( Τ ,s) is known.</a>
</body>
</html>