<html>
<head>
<meta name="TextLength" content="SENT_NUM:2, WORD_NUM:209">
</head>
<body bgcolor="white">
<a href="#0" id="0">First, as long as the inverse dictionary kernel S is PSD, the resultant kernel {a mathematical formula}ESE ⊤ will also be PSD; second, the rank of the kernel matrix can be easily controlled by the landmark size; this can be computationally much more efficient than learning a full kernel matrix subject to rank constraint; third, the extrapolation (4) is “ generative ” and allows us to compute the similarity between any pair of samples; this means the learned kernel matrix generalizes easily to new unseen samples.</a>
<a href="#1" id="1">The criterion (15) has the following properties: (1) it is scale invariant, and does not require any extra trade-off parameter due to its multiplicative form; (2) the first term measures the closeness between S and {a mathematical formula}S0, which is related to unsupervised structures of kernel matrix; the second term measures the closeness between {a mathematical formula}ElSEl ⊤ and {a mathematical formula}Kl ⁎ , which is related to side information; therefore the criterion is consistent with the objective (7) but is numerically different; (3) a higher alignment (second term in (15)) indicates existence of a good predictor with higher probability [16]; (4) computation of the criterion does not require any extra validation set, which is convenient if only limited labeled samples are available.</a>
</body>
</html>