<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:195">
</head>
<body bgcolor="white">
<a href="#0" id="0">In [27] we show that the precise information reuse method has an impact on the performance of an enhancement, and in particular we show that policies designed to balance exploitation and exploration, such as Ε -greedy and UCB1 [6], produce strong simulation policies.</a>
<a href="#1" id="1">In the case of UCB1, this leads to an elegant MCTS algorithm which uses a bandit algorithm to select all moves in the playout, where in the MCTS tree the action value estimates correspond to information about a single state and in simulations the action value estimates correspond to information reused between many states.</a>
<a href="#2" id="2">Thus the only difference between the “ in tree ” (selection) and “ out of tree ” (simulation) modes of MCTS is whether the context in which the bandit algorithm executes is specific to a single state or general across a larger collection of states.</a>
<a href="#3" id="3">Each step of the playout uses the policy function Α to choose an action a, depending on the current move history {a mathematical formula}[h] ⌣ Ρ (s) for the player about to act from state s, the current information mapping Θ , and the set of available actions {a mathematical formula}A(s) (line 11).</a>
</body>
</html>