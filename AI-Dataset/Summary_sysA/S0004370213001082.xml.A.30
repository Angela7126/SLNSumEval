<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:239">
</head>
<body bgcolor="white">
<a href="#0" id="0">Specifically, we evaluated all methods of which we are aware on performance data for 11 algorithms and 35 instance distributions spanning SAT, TSP and MIP and considering three different problems: predicting runtime on novel instances (Section 6), novel parameter configurations (Section 7), and both novel instances and configurations (Section 8).</a>
<a href="#1" id="1">Bartz-Beielstein & Markon [8] support categorical algorithm parameters (using regression tree models), and two existing methods consider predictions across both different instances and parameter settings.</a>
<a href="#2" id="2">First, Ridge & Kudenko [96] applied an analysis of variance (ANOVA) approach to detect important parameters, using linear and quadratic models.</a>
<a href="#3" id="3">Second, Chiarandini & Goegebeur [20] noted that in contrast to algorithm parameters, instance characteristics cannot be controlled and should be treated as so-called random effects.</a>
<a href="#4" id="4">Their resulting mixed-effects models are linear and, like Ridge & Kudenko Ê¼ s ANOVA model, assume Gaussian performance distributions.</a>
<a href="#5" id="5">We now study the performance of the models described in Sections 3 and 4 (see Table 1 for an overview), based on (various subsets of) the features described in Section 5.</a>
<a href="#6" id="6">In this section, we consider the (only) problem considered by most past work: predicting the performance achieved by the default configuration of a given algorithm on new instances.</a>
<a href="#7" id="7">Predictions for training configurations on training instances.</a>
<a href="#8" id="8">Predictions for this most basic case are useful for succinctly modelling known algorithm performance data.</a>
<a href="#9" id="9">Table 7 provides quantitative results of model performance based on {a mathematical formula}n=10000 training data points, and Fig.</a>
</body>
</html>