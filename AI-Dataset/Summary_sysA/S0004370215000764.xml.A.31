<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:247">
</head>
<body bgcolor="white">
<a href="#0" id="0">This article presents an intrinsically motivated model-based RL algorithm, called texplore with Variance-And-Novelty-Intrinsic-Rewards (texplore-vanir), that uses intrinsic motivation both for improved sample efficiency and to give the agent a curiosity drive.</a>
<a href="#1" id="1">Next, we evaluate the ability of texplore-vanir to learn to control the arm of a humanoid robot.</a>
<a href="#2" id="2">Due to the difficulty of performing experiments on a real robot, instead of the detailed evaluations we performed on the light world task, we only examine two aspects of the learning agent on the robot.</a>
<a href="#3" id="3">First, we look at the amount of the state space that the agent was able to explore.</a>
<a href="#4" id="4">Second, we test whether the learned model can be used to perform tasks in the domain when given a reward function.</a>
<a href="#5" id="5">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#6" id="6">These results show that texplore-vanir's intrinsic rewards drive the agent to explore more of the state space and discover more affordances in the task than if the agent explored randomly.</a>
<a href="#7" id="7">Exploring with texplore-vanir enables the robot to learn a more accurate model of more of the state space and perform better on possible tasks given to the robot after exploring the domain.</a>
<a href="#8" id="8">This article presents the texplore-vanir algorithm for intrinsically motivated learning.</a>
<a href="#9" id="9">This algorithm combines random forest based model learning with two novel intrinsic rewards.</a>
</body>
</html>