<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:199">
</head>
<body bgcolor="white">
<a href="#0" id="0">kLog generates a set of features starting from a logical and relational learning problem and uses these features for learning a (linear) statistical model.</a>
<a href="#1" id="1">Finally, the graphs produced by kLog are turned into feature vectors using a graph kernel, which leads to a statistical learning problem at the third level.</a>
<a href="#2" id="2">{sup:2} It is from this graph that kLog will generate propositional features (based on a graph kernel) for use in the learning procedure.</a>
<a href="#3" id="3">The details of the graphicalization procedure and the kernel are given in Sections 6 and 6.2, respectively.</a>
<a href="#4" id="4">kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.</a>
<a href="#5" id="5">Having specified a target relation r, kLog is able to infer the partition {a mathematical formula}x âˆª y of ground atoms into inputs and outputs in the supervised learning setting.</a>
<a href="#6" id="6">As kLog is a language for logical and relational learning with kernels it is related to work on inductive logic programming, to statistical relational learning, to graph kernels, and to propositionalization.</a>
<a href="#7" id="7">We now discuss each of these lines of work and their relation to kLog.</a>
</body>
</html>