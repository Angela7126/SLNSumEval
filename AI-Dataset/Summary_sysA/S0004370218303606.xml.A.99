<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:274">
</head>
<body bgcolor="white">
<a href="#0" id="0">Some features critical to the demonstrated behavior may be active entirely in the occluded portion of the state space, which will result in {a mathematical formula} Φ ˆ k=0 for this feature.</a>
<a href="#1" id="1">As a result, we can no longer utilize the gradient of the feature expectation to find the reward weights.</a>
<a href="#2" id="2">Thus, a numerical solving method that does not use the gradient function such as Nelder-Mead's simplex technique [27] is necessary when some states are occluded; these tend to converge slower in the absence of a guiding gradient.</a>
<a href="#3" id="3">In the presence of occlusion, this equation may not be computable as the proportion of interacting states may be unknown to L. Instead, we empirically compute the state-visitation frequencies by projecting in the full environment the policy under consideration for each robot for a large number of time steps while utilizing the equilibrium behavior {a mathematical formula} Σ e when the robots interact.</a>
<a href="#4" id="4">The mass {a mathematical formula}1 − TI(s,a, Ψ (s,a)) could be distributed according to many possible models: uniformly across all next states excepting the intended state; to a dedicated error state; or among the intended states that would result due to performing actions other than a from s. While one could be chosen based on knowledge of the agent or robot being modeled, a general way is to choose the most likely model given the data on observed unintended transitions.</a>
<a href="#5" id="5">High occlusion presents a difficult challenge for this method: instead of randomly missing some data, much of the trajectory is missing, and furthermore the patrollers do not explore all states and actions resulting in the uniform probability distribution being assigned for most transitions.</a>
</body>
</html>