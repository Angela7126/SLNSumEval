<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:257">
</head>
<body bgcolor="white">
<a href="#0" id="0">For two different algorithms and tasks, they search over a broad set of possible task and agent specific intrinsic rewards and find rewards that make the agent learn faster than if it solely used external rewards.</a>
<a href="#1" id="1">The Policy Gradient Reward Design algorithm (pgrd) learns the best intrinsic rewards on-line for cases where the true reward function is given and the agent is limited in some way [15], [16], [17].</a>
<a href="#2" id="2">pgrd uses its knowledge of the true reward function to calculate the gradient of intrinsic rewards to agent return.</a>
<a href="#3" id="3">Using this gradient, intrinsic rewards are found that enable the best agent performance given its limitations.</a>
<a href="#4" id="4">For example, if the agent has a limited planning depth, then even with the true reward function, it cannot perform well.</a>
<a href="#5" id="5">This generalization enables the model to make predictions about unseen or infrequently visited state-actions, and therefore the agent does not have to visit every state-action.</a>
<a href="#6" id="6">Thus, texplore-vanir approaches the model learning task as a supervised learning problem, with the current state and action as the input, and the next state as the output to be predicted.</a>
<a href="#7" id="7">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#8" id="8">In the light world domain, by the time the algorithm has determined error is improving in a region, the agent has already learned a model of that region and no longer needs to explore there.</a>
</body>
</html>