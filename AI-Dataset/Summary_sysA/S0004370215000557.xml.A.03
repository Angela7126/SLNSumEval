<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:384">
</head>
<body bgcolor="white">
<a href="#0" id="0">At the other extreme, an agent with a {a mathematical formula} Γ =1 objective values near-term reward equally as reward infinitely far in the future.</a>
<a href="#1" id="1">In our experiments, a predictive model of human reward, {a mathematical formula}R ˆ H, is learned and provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#2" id="2">The tamer framework is a fully myopic (i.e., {a mathematical formula} Γ =0), high-level algorithm for learning from human reward.</a>
<a href="#3" id="3">At the extreme of this trend, the tamer framework discounts by {a mathematical formula} Γ =0, learning a model of human reward that is (because of this discounting) also an action-value function.</a>
<a href="#4" id="4">Thus, setting {a mathematical formula} Γ =0 effectively reduces reinforcement learning of a value function to supervised learning of a reward function.</a>
<a href="#5" id="5">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#6" id="6">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#7" id="7">We call these experiments off-discounting{sup:3} because the human reward data was gathered under {a mathematical formula} Γ =0 discounting, which usually differs from the discounting used during evaluation, when the agent learns a value function from the learned reward model.</a>
<a href="#8" id="8">Thus, at {a mathematical formula} Γ =1{a mathematical formula}R ˆ H is being used as if it were interchangeable with a conventional MDP reward function.</a>
<a href="#9" id="9">Mean task performance and mean total reward per episode for each tested discount factor across all 19 {a mathematical formula}R ˆ H models are displayed in Fig.</a>
<a href="#10" id="10">In the episodic-task analysis described here in Section 7.2.2 — as in the off-discounting analysis in the previous Section 7.2.1 — the human reward model {a mathematical formula}R ˆ H is learned by tamer and provides predictions that are interpreted as reward by an RL algorithm.</a>
<a href="#11" id="11">as mentioned in Section 7.2.1, the {a mathematical formula}R ˆ Hs likely reflect trainer adaptation to the {a mathematical formula} Γ =0 discount factor under which these human reward models were trained;</a>
</body>
</html>