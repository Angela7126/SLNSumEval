<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:252">
</head>
<body bgcolor="white">
<a href="#0" id="0">Second, we want to obtain an accurate estimate of the expected reward {a mathematical formula}Rs Ω for a given context-policy parameter pair to avoid the bias in the sample-based REPS formulation.</a>
<a href="#1" id="1">The expected reward {a mathematical formula}Rs Ω =E Τ [R( Τ ,s)|s, Ω ] can be estimated by multiple samples from the trajectory distribution {a mathematical formula}p( Τ |s, Ω ), that is,{a mathematical formula} where the trajectories are now generated using the learned forward models in computer simulation and we will assume that the trajectory-dependent reward function {a mathematical formula}R( Τ ,s) is known.</a>
<a href="#2" id="2">Due to the recent success of using Gaussian Process models to reduce the model bias when learning complex system dynamics [9], we use GP models to learn the forward models of the robot and its environment.</a>
<a href="#3" id="3">Therefore, our method is called Gaussian Process Relative Entropy Policy Search (GPREPS).</a>
<a href="#4" id="4">For data collection, we observe the context {a mathematical formula}s[i] and sample the parameters {a mathematical formula} Ω [i] using the upper-level policy {a mathematical formula} Π ( Ω |s[i]).</a>
<a href="#5" id="5">Subsequently, we use the lower-level control policy {a mathematical formula} Π (x; Ω [i]) to obtain the trajectory sample {a mathematical formula} Τ [i].</a>
<a href="#6" id="6">Using the learned GP forward model, we need to predict the expected reward{a mathematical formula} for a given parameter vector Ω executed in context s. The expectation over the trajectories is now estimated using the learned forward models.</a>
<a href="#7" id="7">We compared GPREPS to model-free REPS and CrKR [20], a state-of-the-art model-free contextual policy search method.</a>
</body>
</html>