<html>
<head>
<meta name="TextLength" content="SENT_NUM:2, WORD_NUM:194">
</head>
<body bgcolor="white">
<a href="#0" id="0">Once Θ is learned, and given expert predictions {a mathematical formula}y ˆ i, aggregated prediction {a mathematical formula}yi for the ith data point can be found as a mean of the posterior distribution {a mathematical formula}yi|xi,y ˆ iK ∼ N(y ‾ i,(1T Σ − 11) − 1), where mean {a mathematical formula}y ‾ i is computed as{a mathematical formula} with {a mathematical formula}y ˆ i=[(y ˆ iK)T, Μ i]T, and Σ is a {a mathematical formula}(K+1)×(K+1) block matrix equal to{a mathematical formula} Interestingly, we can see that the prior mean {a mathematical formula} Μ i can be viewed as an additional expert with variance {a mathematical formula} Σ 2, which is independent from the original K experts.</a>
<a href="#1" id="1">Next, we maximize the log-likelihood of data set {a mathematical formula}D with respect to w. The log-likelihood, after removing all elements not dependent on w from equations (8) and (9), is equal to{a mathematical formula} We can see from (11) that, in order to maximize {a mathematical formula}L with respect to w, we need to solve linear regression where for unlabeled points we use an estimate of a ground truth equal to {a mathematical formula}y ‾ i.</a>
</body>
</html>