<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:197">
</head>
<body bgcolor="white">
<a href="#0" id="0">[25] presents a general theoretical framework for statistical logical learning based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space.</a>
<a href="#1" id="1">This definition allows faster convergence during the training of the model: the classical Fuzzy FOL formulation directly depends only on one item over the set of groundings (the argmin element), whereas the average depends on all elements and allows parallel optimization during training.</a>
<a href="#2" id="2">We assume that a set of H functional constraints in the form {a mathematical formula}1 − Φ h(f)=0,0 ≤ Φ h(f) ≤ 1,h=1, … ,H are provided in order to describe how the query functions should behave.</a>
<a href="#3" id="3">However, whereas the gradient was computed with respect to the functions' weights in the training phase, only the first terms of the chain rule have to be taken into account during collective classification:{a mathematical formula} Indeed, since the input weights are now fixed, no back-propagation of the derivative of the error down to the weights is needed.</a>
<a href="#4" id="4">It is clear from the results that the dom heuristic effectively breaks the learning complexity, allowing to find better solutions.</a>
</body>
</html>