<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:170">
</head>
<body bgcolor="white">
<a href="#0" id="0">The online POMDP approach that we propose allows for long planning horizons by using a compact, fixed-size policy graph [17].</a>
<a href="#1" id="1">Following this, our probability model uses Beta distributions to model uncertainty in object specific grasp probabilities.</a>
<a href="#2" id="2">At each time step the agent observes whether the grasp succeeded and makes an observation about object attributes.</a>
<a href="#3" id="3">Note that if grasp success or semantic locations are not fully observed, then we cannot estimate the initial POMDP belief directly using grasp success and object attribute observations.</a>
<a href="#4" id="4">Instead, we could update at each time step an (approximate) belief according to the current action and observation and use that as the initial POMDP belief.</a>
<a href="#5" id="5">Note that the magnitude of {a mathematical formula}ni determines how much object specific grasp properties affect the grasp success probability compared to occlusion.</a>
<a href="#6" id="6">5 shows a compact policy graph computed by the POMDP method for the first scene in Fig.</a>
<a href="#7" id="7">6 shows performance for the heuristic manipulation method and the POMDP method with a planning horizon of three for different reward choices.</a>
</body>
</html>