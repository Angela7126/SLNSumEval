<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:227">
</head>
<body bgcolor="white">
<a href="#0" id="0">To construct an EPM for an algorithm {a mathematical formula}A with configuration space Θ on an instance set Π , we run {a mathematical formula}A on various combinations of configurations {a mathematical formula} Θ i ∈ Θ and instances {a mathematical formula} Π i ∈ Π , and record the resulting performance values {a mathematical formula}yi.</a>
<a href="#1" id="1">We now examine the most interesting case, where test instances and configurations were both previously unseen.</a>
<a href="#2" id="2">Table 7 provides quantitative results of model performance based on {a mathematical formula}n=10000 training data points, and Fig.</a>
<a href="#3" id="3">8 visualizes performance.</a>
<a href="#4" id="4">Overall, we note that the best models generalized to new configurations and to new instances almost as well as to either alone (compare to Sections 6 and 7, respectively).</a>
<a href="#5" id="5">On the most heterogeneous data set, CPLEX-BIGMIX, we once again witnessed extremely poorly predicted outliers for the ridge regression variants, but in all other cases, the models captured the large spread in runtimes (above 5 orders of magnitude) quite well.</a>
<a href="#6" id="6">As in the experiments in Section 6.3, the tree-based approaches, which are able to model different regions of the input space independently, performed best on the most heterogeneous data sets.</a>
<a href="#7" id="7">Fig.</a>
<a href="#8" id="8">8 also shows some qualitative differences in predictions: for example, ridge regression, neural networks, and projected processes sometimes overpredicted the runtime of the shortest runs, while the tree-based methods did not have this problem.</a>
</body>
</html>