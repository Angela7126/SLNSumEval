<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:388">
</head>
<body bgcolor="white">
<a href="#0" id="0">These simpler reward functions with higher-level information could reduce the need for training, allow the agent to find behaviors that are more effective than those known by the trainer, and make the agent's learned behavior robust to environment changes that render ineffective a previously effective policy but leave the purpose of the task unchanged (e.g., when the MDP-optimal path to a goal becomes blocked, but the goal remains unchanged).</a>
<a href="#1" id="1">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#2" id="2">Second, to account for delays in giving feedback, the causal attribution of each reward is distributed across multiple recent time steps by tamer's credit assignment module [11], further adding variety to the label values of samples for training {a mathematical formula}R ˆ H.</a>
<a href="#3" id="3">Additionally, these results provide initial evidence of the presence of positive circuits: compared with agents optimizing for low Γ values, agents optimizing for high Γ values learn policies that accrue more human reward but do not reach the goal, and a sampling of the learned policies for {a mathematical formula} Γ =0 showed consistently circuitous behavior.</a>
<a href="#4" id="4">We do not directly investigate the effects of giving positive reward from absorbing state, but the strategy in the following section — making the task appear continuing — is equivalent to setting the absorbing-state reward such that its resulting return is equal to the agent's discounted expectation of return from the distribution of starting states, assuming the value function is accurate for the policy it assesses.</a>
<a href="#5" id="5">For episodic tasks, we argued in Section 7.1 that a positive reward bias among human trainers combined with high discount factors can lead to infinite behavioral circuits — created by what we call “ positive circuits ” — and consequently minimal task performance.</a>
<a href="#6" id="6">In comparison to the goal-only task, on the other hand, the avi-tamer algorithm performed better overall in both the continuing and episodic versions of the failure-task; this increase might be due in part to the failure state being used as an intermediate “ goal ” that the learner makes updates for, goes to, and then gets experience and reward for those states near it, which then help the agent go to the real goal.</a>
</body>
</html>