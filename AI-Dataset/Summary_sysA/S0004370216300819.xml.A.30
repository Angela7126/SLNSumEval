<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:269">
</head>
<body bgcolor="white">
<a href="#0" id="0">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#1" id="1">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#2" id="2">The function {a mathematical formula}o:S → Sm is the mapping from the states of the MDP to the states of the model introduced at the beginning of this section.</a>
<a href="#3" id="3">This partial policy is used in the reinforcement learning phase to define an MDP on which the agent learns the optimal policy.</a>
<a href="#4" id="4">The partial policy is defined over the state space of the original MDP, to allow the learning algorithm to adapt to the real environment, and not to the model.</a>
<a href="#5" id="5">We carried out three experiments, in order to demonstrate different characteristics of DARLING.</a>
<a href="#6" id="6">We compare three agents across all experiments, for each domain D and model {a mathematical formula}Dm: an agent that makes decisions by planning on {a mathematical formula}Dm only, an agent that makes decisions by reinforcement learning on D, and DARLING that uses {a mathematical formula}Dm to compute a partial policy, and limits its exploration to {a mathematical formula}Dr. We refer to the first agent as the P agent, to the second as the RL agent, and to the third as the PRL agent.</a>
</body>
</html>