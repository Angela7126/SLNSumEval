<html>
<head>
<meta name="TextLength" content="SENT_NUM:18, WORD_NUM:389">
</head>
<body bgcolor="white">
<a href="#0" id="0">These optimisation problems consider settings where actions (i.e., the pulling of a particular arm) have initially unknown rewards that have to be learnt through noisy observations, and the goal is to maximise the total amount of rewards by sequentially choosing different actions over time.</a>
<a href="#1" id="1">This corresponds exactly to choosing initially unknown workers in an expert crowdsourcing setting.</a>
<a href="#2" id="2">However, as we discuss in Section 2, no existing MAB model considers the specific constraints of the expert crowdsourcing setting.</a>
<a href="#3" id="3">While some work considers MAB problems with a fixed budget, termed budget-limited MABs [33], and proposes a budget-limited Ε -first algorithm for this, their model does not consider task limits per worker.</a>
<a href="#4" id="4">Theorem 6</a>
<a href="#5" id="5">Now, by using elementary algebra, we can show that by setting{a mathematical formula} the upper bound given in Theorem 1 is minimised.</a>
<a href="#6" id="6">Thus, we get: Let{a mathematical formula} Ε optdenote the abovementioned value that minimises Eq.</a>
<a href="#7" id="7">(10)and{a mathematical formula}0< Β <1.</a>
<a href="#8" id="8">By setting the exploration budget to be{a mathematical formula}B Ε opt, with at least probability Β , the regret of the bounded Ε -first algorithm is at most{a mathematical formula}</a>
<a href="#9" id="9">Let{a mathematical formula}0< Ε , Β <1.</a>
<a href="#10" id="10">Suppose that{a mathematical formula} Ε B ≥ ∑ j=1Ncj.</a>
<a href="#11" id="11">With at least probability Β , the performance regret of the bounded Ε -first with SR exploration approach is at most{a mathematical formula}In addition, by optimally tuning Ε , we can show that the regret is at most{a mathematical formula}</a>
<a href="#12" id="12">Here, the significantly higher performance compared to the benchmarks is due to the ability of our algorithm to take into account the workers' task limits and divide the high budget between several workers.</a>
<a href="#13" id="13">In addition, our algorithm outperforms the others by up to {a mathematical formula}162% (for the case of budget {a mathematical formula}B=$10,000).</a>
<a href="#14" id="14">We can also see that when the budget is sufficiently large, bounded KUBE achieves a higher performance, compared to other benchmarks.</a>
<a href="#15" id="15">However, it can still only achieve less than {a mathematical formula}60% of the bounded Ε -first.</a>
<a href="#16" id="16">We now turn to the investigation of whether we can improve the performance of the bounded Ε -first algorithm by replacing the uniform exploration approach with other policies.</a>
<a href="#17" id="17">Recall that in Section 5, we have proved that by replacing the uniform approach with Successive Rejects (SR), the theoretical regret bound, that the bounded Ε -first approach can achieve, is increased.</a>
</body>
</html>