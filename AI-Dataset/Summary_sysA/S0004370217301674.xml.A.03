<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:163">
</head>
<body bgcolor="white">
<a href="#0" id="0">Algorithm 1 is described for the most general case of a fully-decentralized system with individual rewards, where states and rewards are annotated as {a mathematical formula}sm and {a mathematical formula}rm respectively, but it is also possible to implement a joint state vector or common reward DRL systems.</a>
<a href="#1" id="1">In this case, we have that {a mathematical formula} Σ → 1 and the learning rate tends to Α .</a>
<a href="#2" id="2">To keep parity with the centralized model, our decentralized modeling considers two individual agents for learning {a mathematical formula}vl and {a mathematical formula}aw in parallel as is shown in Table 3.</a>
<a href="#3" id="3">All the DRL systems implemented improved the {a mathematical formula}%ofScoredGoals of the centralized one as in the learning evolution traces (Fig.</a>
<a href="#4" id="4">Since 17 discrete actions per agent are implemented for the DRL system, if an equivalent CRL system were implemented, that centralized agent would search in an action space of {a mathematical formula}173=4913 possible actions, which would be enormous for most of the RL algorithms.</a>
</body>
</html>