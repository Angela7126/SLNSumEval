<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:263">
</head>
<body bgcolor="white">
<a href="#0" id="0">Note that the goal of dropout training is precisely to make {a mathematical formula} Σ S small, that is to make the output of each unit robust, independent of the details of the activities of the other units, and thus roughly constant over all possible dropout subnetworks.</a>
<a href="#1" id="1">To further understand dropout, one must better understand the properties and relationships of the weighted arithmetic, geometric, and normalized geometric means and specifically how well the NWGM of a sigmoidal unit approximates its expectation ({a mathematical formula}E( Σ ) ≈ NWGMS( Σ )).</a>
<a href="#2" id="2">Thus consider that we have m numbers {a mathematical formula}O1, … ,Om with corresponding probabilities {a mathematical formula}P1, … ,Pm ({a mathematical formula} ∑ i=1mPi=1).</a>
<a href="#3" id="3">This is because it can be viewed as a form of on-line gradient descent with respect to the error function{a mathematical formula} of the true ensemble, where {a mathematical formula}t(I) is the target value for input I and {a mathematical formula}fw is the elementary error function, typically the squared error in regression, or the relative entropy error in classification, which depends on the weights w. In the case of dropout, the probability {a mathematical formula}P(N) of the network {a mathematical formula}N is factorial and associated with the product of the underlying Bernoulli selector variables.</a>
<a href="#4" id="4">Thus dropout is “ on-line ” with respect to both the input examples I and the networks {a mathematical formula}N, or alternatively one can form a new set of training examples, where the examples are formed by taking the cartesian product of the set of original examples with the set of all possible subnetworks.</a>
</body>
</html>