<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:234">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, such a simple solution suffers from some critical drawbacks: (i) when studying OTL on homogeneous domains, it could suffer from negative transfer (transferred knowledge is harmful to learning target task) whenever there exists much significant difference between two conditional probabilities; and (ii) when studying OTL across heterogeneous domains, the old classifiers cannot be trained continuously with the new features because of the inconsistence of the two feature spaces.</a>
<a href="#1" id="1">In addition to these two challenges, we note that online transfer learning is in general more challenging than a classical batch transfer learning task.</a>
<a href="#2" id="2">To overcome the challenge of parameter selection for {a mathematical formula}Pi, in this paper, we propose an automated parameter selection technique, an Online Window Adjustment (OWA) algorithm as shown in Algorithm 5, which can automatically determine a proper value for window size parameter {a mathematical formula}Pi during the online learning process.</a>
<a href="#3" id="3">In addition, the proposed HomOTL-I and HomOTL-II algorithms achieve the best performance among all datasets, which implies that the exploiting learnt knowledge from source domain is able to boost the performance of traditional online learning algorithms, and the two kinds of weight updating methods are generally comparable.</a>
<a href="#4" id="4">Similar observations show that the two OTL algorithms achieve the best performance after receiving a small number of examples (e.g., less than 100 examples), which implies these two strategies can efficiently transfer the well-learnt knowledge from the source task to the target task.</a>
</body>
</html>