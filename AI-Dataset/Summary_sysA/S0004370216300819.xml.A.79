<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:223">
</head>
<body bgcolor="white">
<a href="#0" id="0">The behavior of the agent is represented as a function {a mathematical formula} Π :S×A → [0,1] called a (stationary) policy, where {a mathematical formula} Π (s,a)=Pr(At=a|St=s) is the probability of selecting action a in state s at time t. If {a mathematical formula} Π (s,a)={0,1}{a mathematical formula} ∀ s ∈ S and {a mathematical formula}a ∈ A the policy is deterministic, in which case we will denote the action chosen by the policy as {a mathematical formula}a= Π (s).</a>
<a href="#1" id="1">[1] proposed a method close in motivation to ours, in which the agent uses a crude model of a deterministic dynamical system to identify a direction for policy improvement, and then executes trials in the real world along that direction only, updating the model with the data from the environment.</a>
<a href="#2" id="2">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#3" id="3">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
</body>
</html>