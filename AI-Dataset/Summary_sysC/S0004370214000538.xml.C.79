<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:941">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using this technique allows us to derive an O(B23) upper bound on its performance regret (i.e., the expected difference in utility between our algorithm and the optimum), which means that as the budget B increases, the regret tends to 0.</a>
<a href="#1" id="1">More specifically, it operates as follows: To deal with the unknown performance characteristics of workers, our algorithm divides its budget into two amounts (as dictated by an Ε parameter) to be used in two sequential phases — an initial exploration phase, during which it uniformly samples the performance of a wide range of workers using the first part of its budget, and an exploitation phase, during which it selects only the best workers using its remaining budget.</a>
<a href="#2" id="2">More specifically, we prove that the performance regret (i.e., the difference between the performance of a particular algorithm and that of the optimal solution) of the bounded Ε -first approach is at most {a mathematical formula}O(B23) with a high probability, where B is the total budget.</a>
<a href="#3" id="3">However, the budget-limited MAB model is not directly applicable to the expert crowdsourcing setting, because it is assumed that individual workers can perform an unlimited amount of tasks and indeed the optimal solution of the budget-limited MAB often assigns most tasks to a single worker.</a>
<a href="#4" id="4">However, since their model does not include a total budget limit (only a limitation in the number of pulls per arm), it requires a different underlying solution technique (i.e., not the bounded knapsack model), and thus, it is not feasible for our setting.</a>
<a href="#5" id="5">As we will explain later in Section 4.2, within the exploration phase, our bounded Ε -first approach relies on an approximation method that aims to choose arms with highest reward-cost density values.</a>
<a href="#6" id="6">Thus, we have:{a mathematical formula} where {a mathematical formula}SA(t) is the subset that A chooses to pull at time step t and {a mathematical formula}I{i ∈ SA(t)} denotes the indicator function whether arm i is chosen to be pulled at t. To guarantee that the total cost of the sequence A cannot exceed B, we have:{a mathematical formula} where {a mathematical formula}P( ⋅ ) denotes the probability of an event.</a>
<a href="#7" id="7">Note that if we set the limits {a mathematical formula}Li= ∞ for each arm i (i.e., there is no pull limit) and we restrict {a mathematical formula}|S(t)|=1 for each t (i.e., the agent can only pull a single arm at each time step), we get the budget-limited MAB, and in addition, if we set {a mathematical formula}B= ∞ (there is no budget limit either), we get the standard MAB model (for more details, see [33], [36]).</a>
<a href="#8" id="8">However, workers charge different prices per hour, {a mathematical formula}ci, and have different skill levels, represented by their expected number of working features they can implement per hour, {a mathematical formula} Μ i.</a>
<a href="#9" id="9">Clearly, the maximal number of rounds is N. The reason for choosing this algorithm is that it provides a well-behaved sequence of items (i.e., they are ordered by density), that can be efficiently exploited in the theoretical performance analysis.</a>
<a href="#10" id="10">Given this, we aim to solve the following integer program:{a mathematical formula} where {a mathematical formula}xiexploit are the decision variables, representing the number of times we pull arm i in the exploitation phase.</a>
<a href="#11" id="11">Having the value of each {a mathematical formula}xiexploit, we now run the exploitation algorithm as follows: At each subsequent time step t, if the number of times arm i has been pulled does not exceed {a mathematical formula}xiexploit, then we pull that arm at t. Hereafter we refer to this exploitation approach as {a mathematical formula}Agreedy.</a>
<a href="#12" id="12">With at least probability Β , the performance regret of the bounded Ε -first with SR exploration approach is at most{a mathematical formula}In addition, by optimally tuning Ε , we can show that the regret is at most{a mathematical formula}</a>
<a href="#13" id="13">Note that for {a mathematical formula}N ≥ 9, this regret bound is clearly worse than that of the Ε -first approach with uniform exploration (see Eq.</a>
<a href="#14" id="14">Specifically, the quality distribution is the sum of two random variables, {a mathematical formula}0.9 ⋅ Ri+0.1 ⋅ U(0,1), where {a mathematical formula}Ri is the empirical distribution of the user's actual ratings obtained on previous jobs{sup:13} and {a mathematical formula}U(0,1) is the continuous uniform distribution on the interval {a mathematical formula}[0,1] (to add a small amount of noise).</a>
<a href="#15" id="15">Budget-limited Ε -first: a practically efficient budget-limited MAB algorithm that assigns all tasks to a single expert, that can provide the highest total quality with respect to his task limit, during the exploitation phase [33].</a>
<a href="#16" id="16">Note that our algorithm approaches the theoretical optimum by up to {a mathematical formula}75% (in the cases of moderate, large and extreme budgets), while it achieves {a mathematical formula}61% of the optimal solution's performance in the scenario with small budgets.</a>
<a href="#17" id="17">We can also observe that the uniform and random algorithms are clearly worse than our approach for any budget size, as they do not take into account the workers' performance characteristics at all.</a>
<a href="#18" id="18">This significant increase in relative performance to the other benchmarks is again due to the ability of our algorithm to rely on several high-quality workers within their respective task limits, while most of the other benchmarks rely on a single worker that eventually hits its task limit.</a>
<a href="#19" id="19">To achieve this, we use the advertised cost of a worker, {a mathematical formula}ci, and determine its mean quality as {a mathematical formula} Μ i=D ⋅ ci, where D is a random variable representing the worker's quality-cost density.</a>
<a href="#20" id="20">This is due to the fact that with a sufficiently large budget size, our algorithm can efficiently explore the quality of each worker, and thus, it can achieve a high performance within the exploitation phase.</a>
</body>
</html>