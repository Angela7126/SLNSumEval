<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:382">
</head>
<body bgcolor="white">
<a href="#0" id="0">This fact has incited many researchers to design comprehensive algorithm portfolios, such that a problem instance would be properly handled by at least one algorithm in the portfolio.</a>
<a href="#1" id="1">Algorithm portfolios thus give rise to the algorithm selection issue, aimed at selecting the algorithm best suited to a particular problem instance.</a>
<a href="#2" id="2">First formalized by Rice [40], algorithm selection involves: i) a problem space {a mathematical formula}P; ii) an algorithm space {a mathematical formula}A; iii) a mapping from {a mathematical formula}P×A onto {a mathematical formula}R, referred to as performance model, estimating the performance of any algorithm on any problem instance.</a>
<a href="#3" id="3">The performance model thus naturally supports algorithm selection, by selecting the algorithm with best estimated performance on the current problem instance.</a>
<a href="#4" id="4">From the training set{a mathematical formula} supervised machine learning algorithms – most often regression algorithms although classification algorithms have also been considered [25] – derive an estimate of the computational cost of any algorithm a on any problem instance described by its feature vector x,{a mathematical formula} Algorithm selection proceeds by selecting the algorithm with optimal estimated performance on the current problem instance x:{a mathematical formula}</a>
<a href="#5" id="5">If the latent factors capture the information in the collaborative filtering matrix, and the initial representation efficiently estimates the latent representation (not necessarily through linear combinations), then algorithm selection is enabled via sufficient benchmark data and relevant representation.</a>
<a href="#6" id="6">The first indicator, noted cf, measures the performance of algorithm selection on known problem instances based on an excerpt of matrix {a mathematical formula}M. After the above discussion, the cf performance reflects the quality of the set of problem instances, called benchmark in the following.</a>
<a href="#7" id="7">The second performance indicator, noted cs, measures the quality of algorithm selection for new problem instances.</a>
<a href="#8" id="8">In the general case where the quality of the initial features is unknown, the question is whether one should consider a rich set of features and search for linear latent factors, or build latent factors from the collaborative filtering matrix and model them as (not necessarily linear) functions of the features.</a>
<a href="#9" id="9">5 depicts the distortion of the initial 11 features of the OpenML benchmark (left: {a mathematical formula} Κ =5; right: {a mathematical formula} Κ =7), suggesting that the 7-th feature (the fraction of missing values in the dataset) is more relevant to algorithm selection than other features.</a>
</body>
</html>