<html>
<head>
<meta name="TextLength" content="SENT_NUM:28, WORD_NUM:681">
</head>
<body bgcolor="white">
<a href="#0" id="0">The reward function is modeled as a linear combination of K binary features, each of which, {a mathematical formula} Φ k: {a mathematical formula}S×AI → {a mathematical formula}{0,1}, maps a state from the set of states S and an action from the set of I's actions {a mathematical formula}AI to either a 0 or 1.</a>
<a href="#1" id="1">The reward function for expert I is then defined as {a mathematical formula}RI(s,a)= ∑ k=1K Θ k ⋅ Φ k(s,a), where {a mathematical formula} Θ k are the weights.</a>
<a href="#2" id="2">Given a set of reward weights the expert's MDP is completed and solved optimally to produce {a mathematical formula} Π I ⁎ .</a>
<a href="#3" id="3">The difference {a mathematical formula} Φ ˆ − Φ Π I ⁎ provides a gradient with respect to the reward weights for a numerical solver.</a>
<a href="#4" id="4">The revised objective function is {a mathematical formula} ∂ L ∂ Pr( Π I)2+ ∂ L ∂ Θ 2+ ∂ L ∂ Η 2.</a>
<a href="#5" id="5">Therefore, the optimal policy {a mathematical formula} Π I ⁎ under the reward function with weights Θ has the highest probability.</a>
<a href="#6" id="6">We may thus replace {a mathematical formula}Pr( Π I) as given in Eq.</a>
<a href="#7" id="7">Furthermore, {a mathematical formula}Pr( Π I) as calculated by Eq.</a>
<a href="#8" id="8">5, and retain {a mathematical formula} Π I that is optimal for the weights Θ in the current iteration of the optimization.</a>
<a href="#9" id="9">{a mathematical formula} The revised objective function for minimization is,{a mathematical formula} As we seek to minimize w.r.t.</a>
<a href="#10" id="10">{a mathematical formula}</a>
<a href="#11" id="11">{a mathematical formula} Π I may be found using value iteration performed inline during each iteration of the objective function maximization procedure.</a>
<a href="#12" id="12">The empirical feature expectation of the expert {a mathematical formula} Φ ˆ k will therefore exclude the occluded states (and actions taken in those states).</a>
<a href="#13" id="13">We revise the constraint in (2) to:{a mathematical formula} The distribution as given by {a mathematical formula} Μ Π I over observed states flows from distributions over other states.</a>
<a href="#14" id="14">Therefore, {a mathematical formula} Μ Π I over all states is computed.</a>
<a href="#15" id="15">We may then replace {a mathematical formula} Μ Π I in Eq.</a>
<a href="#16" id="16">{a mathematical formula}</a>
<a href="#17" id="17">Let Ψ : {a mathematical formula}S×A → S map an observed robot's transition from state s given action a to a particular next state, {a mathematical formula}s ′ .</a>
<a href="#18" id="18">Therefore, the patroller may reveal its intended next state given by {a mathematical formula} Ψ (s,a) during this movement.</a>
<a href="#19" id="19">We therefore focus on learning the probability of transitioning to the intended next state given a state-action pair for an observed robot I, {a mathematical formula}TI(s,a, Ψ (s,a)), and this probability is estimated from the available trajectory data.</a>
<a href="#20" id="20">Equation 20 is loosely analogous to using feature functions in the reward function with the difference being that {a mathematical formula}TI is probabilistic due to which features are random variables.</a>
<a href="#21" id="21">20 to relate the observed transition probability to the transitions features:{a mathematical formula}</a>
<a href="#22" id="22">The novel nonlinear, non-convex program for finding the transition feature probabilities of N robots is:{a mathematical formula}</a>
<a href="#23" id="23">The approximating Bernstein polynomial for {a mathematical formula}d=4 using the objective function over {a mathematical formula}Pr( Τ ) as f is:{a mathematical formula}</a>
<a href="#24" id="24">Let {a mathematical formula}Pr(st+1|st,at,m) be the distribution over next states {a mathematical formula}st+1 conditioned on error model m. The observed trajectories of the expert may yield several triples {a mathematical formula} 〈 st,at,st+1 〉 where {a mathematical formula}st+1 ≠ Ψ (st,at).</a>
<a href="#25" id="25">Here, {a mathematical formula}V Π I ⁎ is the optimal value function of I's MDP and {a mathematical formula}V Π IL is the value function due to utilizing the learned policy {a mathematical formula} Π IL in I's MDP.</a>
<a href="#26" id="26">Notice that when the learned reward function results in an identical optimal policy to I's optimal policy, {a mathematical formula} Π I ⁎ = Π IL, ILE will be zero; it increases monotonically as the two policies increasingly diverge in value.</a>
<a href="#27" id="27">We evaluate mIRL{a mathematical formula}/T ⁎ +Int for learning the experts' transition functions as previously described in Section 5. mIRL{a mathematical formula}/T ⁎ +Int begins by estimating the transition functions of others, after which it collapses into mIRL{sup: ⁎ }+Int learning reward weights Θ .</a>
</body>
</html>