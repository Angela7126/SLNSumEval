<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:460">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively.</a>
<a href="#1" id="1">In our recent work [16], we proposed a method that exploits the structural knowledge derived from semantic networks, together with distributional statistics from text corpora, to produce effective representations of individual word senses or concepts.</a>
<a href="#2" id="2">Our lexical vectors have individual words as their dimensions, therefore, in our lexical semantic space, a text is represented on the basis of its association with a set of lexical items, i.e., words.</a>
<a href="#3" id="3">We put forward an approach that allows us to plug in an arbitrary word embedding representation with that of our lexical vector representations, providing three main advantages: (1) benefiting from the word-based knowledge derived as a result of learning from massive corpora for our sense-level representation; (2) reducing the dimensionality of our lexical space to a fixed-size continuous space; and (3) providing a shared semantic space between words and synsets (more details in Section 4), hence enabling a direct comparison of words and synsets.</a>
<a href="#4" id="4">Specifically, given an input text {a mathematical formula}T and a space of word embeddings E, we first calculate the lexical vector of {a mathematical formula}T (i.e., {a mathematical formula}v → lex(T)) as explained in Section 3.2 and then map our lexical vector to the semantic space E as follows:{a mathematical formula} where {a mathematical formula}E(w) is the embedding-based representation of the word w in E, and {a mathematical formula}rank(w,v → lex(T)) is the rank of the dimension corresponding to the word w in the lexical vector {a mathematical formula}v → lex(T), thus giving more importance to the higher weighted dimensions.</a>
<a href="#5" id="5">Most of these techniques view sense representation as a specific type of word representation and try to adapt the existing distributional word modeling techniques to the sense level, usually through clustering the contexts in which a word appears [138], [47], [99].</a>
<a href="#6" id="6">In contrast, our method provides a multilingual representation of word senses on the basis of the complementary knowledge of two different resources, enabling a significantly higher coverage of specific domains and named entities.</a>
<a href="#7" id="7">In contrast, our approach combines expert-based and encyclopedic knowledge from two different types of resource, providing three advantages: (1) more effective measurement of similarity based on rich semantic representations, (2) the possibility of measuring cross-resource semantic similarity, i.e., between Wikipedia pages and WordNet synsets, and (3) the possibility of comparing the semantics of word senses across different languages.</a>
</body>
</html>