<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:517">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using model-based reinforcement learning from human reward, this article investigates the problem of learning from human reward through six experiments, focusing on the relationships between reward positivity, which is how generally positive a trainer's reward values are; temporal discounting, the extent to which future reward is discounted in value; episodicity, whether task learning occurs in discrete learning episodes instead of one continuing session; and task performance, the agent's performance on the task the trainer intends to teach.</a>
<a href="#1" id="1">These simpler reward functions with higher-level information could reduce the need for training, allow the agent to find behaviors that are more effective than those known by the trainer, and make the agent's learned behavior robust to environment changes that render ineffective a previously effective policy but leave the purpose of the task unchanged (e.g., when the MDP-optimal path to a goal becomes blocked, but the goal remains unchanged).</a>
<a href="#2" id="2">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#3" id="3">Fourth, in two additional tests using the training data from this continuing-task experiment, we find evidence for the theoretically based conjecture that low discount rates (high Γ s) facilitate the communication of higher-level task information — e.g., the location of the goal rather than the exact sequence of steps to reach it.</a>
<a href="#4" id="4">Second, to account for delays in giving feedback, the causal attribution of each reward is distributed across multiple recent time steps by tamer's credit assignment module [11], further adding variety to the label values of samples for training {a mathematical formula}R ˆ H.</a>
<a href="#5" id="5">Additionally, these results provide initial evidence of the presence of positive circuits: compared with agents optimizing for low Γ values, agents optimizing for high Γ values learn policies that accrue more human reward but do not reach the goal, and a sampling of the learned policies for {a mathematical formula} Γ =0 showed consistently circuitous behavior.</a>
<a href="#6" id="6">We do not directly investigate the effects of giving positive reward from absorbing state, but the strategy in the following section — making the task appear continuing — is equivalent to setting the absorbing-state reward such that its resulting return is equal to the agent's discounted expectation of return from the distribution of starting states, assuming the value function is accurate for the policy it assesses.</a>
<a href="#7" id="7">For episodic tasks, we argued in Section 7.1 that a positive reward bias among human trainers combined with high discount factors can lead to infinite behavioral circuits — created by what we call “ positive circuits ” — and consequently minimal task performance.</a>
<a href="#8" id="8">In comparison to the goal-only task, on the other hand, the avi-tamer algorithm performed better overall in both the continuing and episodic versions of the failure-task; this increase might be due in part to the failure state being used as an intermediate “ goal ” that the learner makes updates for, goes to, and then gets experience and reward for those states near it, which then help the agent go to the real goal.</a>
</body>
</html>