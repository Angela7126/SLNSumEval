<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:393">
</head>
<body bgcolor="white">
<a href="#0" id="0">The parameterization of {a mathematical formula}M Θ include therefore parameters for the grammar, parameters for the MH algorithm, parameters for the one-class SVM estimator and parameters for the graph kernel.</a>
<a href="#1" id="1">The optimization of the compound system is difficult; many components in fact have an algorithmic nature that cannot be reduced to a functional description which prevents us from using derivative based optimization methods.</a>
<a href="#2" id="2">For this reasons we proceed in stages: first the parameters for the grammar, for the MH algorithm and for the graph kernel are fixed, then the grammar is induced and the probability estimators are optimized using classical SVM optimization schemes; then these parameters are changed uniformly at random within user defined parameters ranges and the procedure is iterated.</a>
<a href="#3" id="3">The solution of the one class problem defines a region of the feature space where the probability distribution that has generated the data has high support.</a>
<a href="#4" id="4">We can therefore use the induced model to find whether a test point {a mathematical formula}x ′ has a probability higher than a given threshold, but not to find its actual probability.</a>
<a href="#5" id="5">To convince oneself that the substitutable graph grammar yields a proposal distribution that is in first approximation consider how the inner core replacement procedure works: starting from a graph G we select an inner core graph C and an interface graph I, we then identify all core/interface pairs where the interface matches I up to isomorphism and uniformly at random choose one core {a mathematical formula}C ′ for the replacement to yield graph {a mathematical formula}G ′ .</a>
<a href="#6" id="6">As an illustrative example consider the substitution of a small core with a large core; in this case a greater number of nodes exist after the substitution and hence the exact inverse transition {a mathematical formula}g(G ′ → G) is less probable since there exist more roots to sample from for the core replacement.</a>
<a href="#7" id="7">The two approaches complement each other: the grammar is a flexible non-parametric approach that can model complex dependencies between vertices that are within a short path distance from each other; the statistical model, instead, can employ the bias derived from the particular functional family (linear) and the type of regularization used (a penalty over the {a mathematical formula}L2 norm of the parameters) to generalize long range dependencies to cases that are similar but not identical to those observed in the training set.</a>
</body>
</html>