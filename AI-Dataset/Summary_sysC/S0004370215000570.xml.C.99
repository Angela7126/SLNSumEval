<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:314">
</head>
<body bgcolor="white">
<a href="#0" id="0">This is partly because the manipulation planning problems have intrinsic characteristics such as continuous state spaces which make the application of POMDP solvers less straightforward, and partly because the manipulation planning approaches have only recently advanced to the point where the explicit modeling of uncertainty becomes tractable.</a>
<a href="#1" id="1">In particular, POMDPs are beneficial if a particular problem has some of the following characteristics: 1) the problem requires weighting the value of information gathering versus collecting immediate rewards such as lifting objects to get a better view on other objects, 2) the world model is uncertain and thus it should be updated, for example when some objects are harder to grasp than others, or 3) the sequence of actions matters such as when objects occlude each other even partially.</a>
<a href="#2" id="2">In a POMDP, the transition probability {a mathematical formula}P(s ′ |s,a) models the uncertainty in action effects: what is the probability to move a cup successfully from a table (part of state s) into a dishwasher (part of {a mathematical formula}s ′ ), when the action a is “ move cup into dishwasher ” ?</a>
<a href="#3" id="3">The observation probability {a mathematical formula}P(o|s ′ ,a) models the uncertainty in observations: what is the probability of observing a cup as dirty (observation o), when it is dirty (part of state {a mathematical formula}s ′ ) and we are executing action a “ look at cup ” ?</a>
<a href="#4" id="4">The robot has to consider at each time step, whether the information gain from lifting a cup yields more reward in the long run than executing an action which may yield higher immediate reward.</a>
<a href="#5" id="5">Of course, because of the uncertainty in actions and observations, the real decision making problem can be even more complicated than this simple example implies.</a>
<a href="#6" id="6">Consequently, our hypothesis is that a heuristic greedy manipulation approach is not sufficient and that planning several time steps into the future is needed.</a>
</body>
</html>