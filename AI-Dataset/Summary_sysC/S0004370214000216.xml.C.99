<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:474">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this case, the weighted geometric means are defined by:{a mathematical formula} and{a mathematical formula} and similarly for the normalized weighted geometric mean (NWGM){a mathematical formula} Using the same calculation as above in the uniform case, we can then compute the normalized weighted geometric mean NWGM in the form{a mathematical formula}{a mathematical formula} where here {a mathematical formula}E(S)= ∑ NP(N)S(N).</a>
<a href="#1" id="1">Thus in summary with any distribution{a mathematical formula}P(N)over all possible sub-networks{a mathematical formula}N, including the case of independent but not identically distributed input unit selector variables{a mathematical formula} Δ iwith probability{a mathematical formula}pi, the NWGM is simply obtained by applying the logistic function to the expectation of the linear input S. In the case of independent but not necessarily identically distributed selector variables{a mathematical formula} Δ i, each with a probability{a mathematical formula}piof being equal to one, the expectation of S can be computed simply by keeping the same overall network but multiplying each weight{a mathematical formula}wiby{a mathematical formula}piso that{a mathematical formula}E(S)= ∑ i=1npiwiIi.</a>
<a href="#2" id="2">Note that the goal of dropout training is precisely to make {a mathematical formula} Σ S small, that is to make the output of each unit robust, independent of the details of the activities of the other units, and thus roughly constant over all possible dropout subnetworks.</a>
<a href="#3" id="3">To further understand dropout, one must better understand the properties and relationships of the weighted arithmetic, geometric, and normalized geometric means and specifically how well the NWGM of a sigmoidal unit approximates its expectation ({a mathematical formula}E( Σ ) ≈ NWGMS( Σ )).</a>
<a href="#4" id="4">Thus consider that we have m numbers {a mathematical formula}O1, … ,Om with corresponding probabilities {a mathematical formula}P1, … ,Pm ({a mathematical formula} ∑ i=1mPi=1).</a>
<a href="#5" id="5">By isolating the contribution of {a mathematical formula}Ojh, we have{a mathematical formula} with a first order Taylor approximation which is more accurate when {a mathematical formula}wijlh or {a mathematical formula}Ojh are small (conditions that are particularly well satisfied at the beginning of learning or with sparse coding).</a>
<a href="#6" id="6">This is because it can be viewed as a form of on-line gradient descent with respect to the error function{a mathematical formula} of the true ensemble, where {a mathematical formula}t(I) is the target value for input I and {a mathematical formula}fw is the elementary error function, typically the squared error in regression, or the relative entropy error in classification, which depends on the weights w. In the case of dropout, the probability {a mathematical formula}P(N) of the network {a mathematical formula}N is factorial and associated with the product of the underlying Bernoulli selector variables.</a>
<a href="#7" id="7">Thus dropout is “ on-line ” with respect to both the input examples I and the networks {a mathematical formula}N, or alternatively one can form a new set of training examples, where the examples are formed by taking the cartesian product of the set of original examples with the set of all possible subnetworks.</a>
</body>
</html>