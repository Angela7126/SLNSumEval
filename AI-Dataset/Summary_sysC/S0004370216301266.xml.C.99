<html>
<head>
<meta name="TextLength" content="SENT_NUM:23, WORD_NUM:620">
</head>
<body bgcolor="white">
<a href="#0" id="0">We define the ad hoc agent's knowledge about its teammates' behaviors as{a mathematical formula} where {a mathematical formula}1 ≤ s ≤ n is the state, {a mathematical formula}1 ≤ t ≤ k specifies a teammate, {a mathematical formula}TrueActiont(s) is the ground truth action distribution for teammate t for state s, and {a mathematical formula}PredActiont(s) is the action distribution that the ad hoc agent predicts that teammate t will select for state s.</a>
<a href="#1" id="1">We assume that {a mathematical formula}PredActiont(s) is the uniform distribution if the agent has no information about teammate t's actions in state s. Thus, if the ad hoc agent has better information about its teammates' behaviors, the distance between the distributions will be smaller and TeamK will be higher.</a>
<a href="#2" id="2">If the ad hoc agent can limit possible actions that its teammates may take or bias its predictions towards more likely actions, TeamK will be higher.</a>
<a href="#3" id="3">For instance, if there are 100 possible actions and the ad hoc agent can narrow a teammate's action to two choices, TeamK will be relatively large.</a>
<a href="#4" id="4">Finally, we assume that the teammates' behaviors from A and P are stationary; the teammates only respond to the ad hoc agent's immediate actions and do not change their behaviors and learn over time.</a>
<a href="#5" id="5">As such, the algorithms put forth in this article address only a small subset of possible ad hoc teamwork scenarios leaving much room for fruitful future research.</a>
<a href="#6" id="6">The difficulty of the problem we do consider is that the ad hoc agent does not have full knowledge of its teammates' behaviors, though it does have prior experiences with other teammates.</a>
<a href="#7" id="7">Nor does the agent have a shared communication protocol that allows those teammates to explicitly communicate their intentions.</a>
<a href="#8" id="8">Therefore, the ad hoc agent must observe its teammates to determine their behaviors.</a>
<a href="#9" id="9">Once it knows the behaviors its teammates exhibit, the ad hoc agent can adapt accordingly.</a>
<a href="#10" id="10">To speed up the process of determining the teammates' behaviors, the ad hoc agent can draw upon its observations of past teammates, exploiting similarities between the current and past teammates' behaviors.</a>
<a href="#11" id="11">In this article, we consider scenarios in which the ad hoc agent has different amounts of prior knowledge of its teammates.</a>
<a href="#12" id="12">Metrics other than the sum of the rewards can be used depending on the domain, such as the worst-case performance.</a>
<a href="#13" id="13">As discussed in the previous section, the evaluation of ad hoc team agents depends strongly on the domains that they may encounter.</a>
<a href="#14" id="14">Therefore, this section describes the first domain that is used for evaluating ad hoc team agents in this article.</a>
<a href="#15" id="15">The pursuit domain, also known as the predator-prey domain, is a popular problem in multiagent systems literature as it requires cooperation between all of the teammates to capture the prey while remaining simple enough to evaluate approaches well [66].</a>
<a href="#16" id="16">The ad hoc agent's knowledge about its team (TeamK) varies in the different tests, as does the reactivity of the teammates.</a>
<a href="#17" id="17">When the ad hoc agent knows its teammates' behaviors, {a mathematical formula}TeamK=1.</a>
<a href="#18" id="18">A variety of other scenarios are summarized in Table 1, where we vary the type of teammates as well as the prior knowledge the ad hoc agent has about its teammates.</a>
<a href="#19" id="19">The ad hoc agent completely knows the environment dynamics, leading to {a mathematical formula}EnvK=(1,1).</a>
<a href="#20" id="20">The moderate reactivity values for most teammate types implies that it is vital to understand and model the ad hoc agent's teammates.</a>
<a href="#21" id="21">However, the teammates do not learn over time from the ad hoc agent's actions.</a>
<a href="#22" id="22">To capture the notion that the ad hoc agent is expected to have extensive prior general domain expertise (as is assumed in the ad hoc teamwork setting), though not with the specific teammates at hand, PLASTIC-Model observes a number of past teammates.</a>
</body>
</html>