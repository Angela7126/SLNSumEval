<html>
<head>
<meta name="TextLength" content="SENT_NUM:29, WORD_NUM:613">
</head>
<body bgcolor="white">
<a href="#0" id="0">When an agent has a good model of its environment, it can use this model to plan good actions using a limited number of interactions with the environment.</a>
<a href="#1" id="1">For an ad hoc agent to plan, it also needs to model its teammates; therefore, it is useful for the ad hoc agent to build models of its teammates' behaviors.</a>
<a href="#2" id="2">Given that learning new models online takes many samples, it is useful to reuse information learned from past teammates.</a>
<a href="#3" id="3">This section describes PLASTIC-Model, a variant of the PLASTIC approach that learns models of prior teammates and selects which models best predict its current teammates.</a>
<a href="#4" id="4">An overview of this approach is given in Fig.</a>
<a href="#5" id="5">5 and the specification of the LearnAboutPriorTeammate, SelectAction, and UpdateBeliefs functions are given in Algorithm 3.</a>
<a href="#6" id="6">These functions are described in depth in the remainder of this section.</a>
<a href="#7" id="7">This section describes how teammate models are learned in the LearnAboutPriorTeammate function of Algorithm 3.</a>
<a href="#8" id="8">The previous sections described how the ad hoc agent can select the most accurate model and use it for planning, but they did not specify the source of these models.</a>
<a href="#9" id="9">One option is that the ad hoc agent is given hand-coded models from human experts, as shown in Line 2 of Algorithm 2 and in Fig.</a>
<a href="#10" id="10">5.</a>
<a href="#11" id="11">However, there may not always be a source of these models, or the models may be imperfect.</a>
<a href="#12" id="12">Therefore, a more general solution is for the ad hoc agent to learn the models.</a>
<a href="#13" id="13">Learning allows the agent to gain a good set of diverse models over its lifespan, allowing better performance with arbitrary new teammates.</a>
<a href="#14" id="14">The ad hoc agent builds models of past teammates' behaviors offline and then selects from these learned models online while cooperating with new teammates.</a>
<a href="#15" id="15">It is expected that the past teammates are representative of the distribution of future teammates, though the future teammates have not yet been seen.</a>
<a href="#16" id="16">In this section, we explore the scenario where the ad hoc agent has previously observed a number of past teammates.</a>
<a href="#17" id="17">Once again, strategies 1 and 2 serve as baselines and require knowledge of the current teammates true behaviors.</a>
<a href="#18" id="18">PLASTIC-Model(CorrectLearned) evaluates the performance of the learning algorithm, where PLASTIC-Model knows which teammates the agent is cooperating with and uses its past observations of these teammates to learn a model of them.</a>
<a href="#19" id="19">PLASTIC-Model(SetIncluding) evaluates the more general ad hoc teamwork scenario where the current type of teammate is unknown, but the current teammates have been previously observed.</a>
<a href="#20" id="20">Finally, PLASTIC-Model(SetExcluding) shows the true ad hoc teamwork scenario, when the ad hoc agent has never seen the current teammates, but uses PLASTIC-Model to reuse knowledge it has learned from previous teammates.</a>
<a href="#21" id="21">This section showed that PLASTIC-Model enables ad hoc team agents to cooperate with a variety of hand-coded and externally-created teammates in the pursuit domain.</a>
<a href="#22" id="22">PLASTIC-Model gets good results when given a set of hand-coded behaviors as HandCodedKnowledge or when it has experienced a number of previous teammates as PriorTeammates.</a>
<a href="#23" id="23">PLASTIC-Model performs well even when it has never seen the current teammates before if the ad hoc agent has experience with previous teammates that exhibit similar behaviors.</a>
<a href="#24" id="24">Furthermore, transfer learning can learn new models quickly, allowing PLASTIC-Model to quickly adapt to new teammates.</a>
<a href="#25" id="25">Specifically, combining models learned from TwoStageTransfer with the models of past teammates outperforms the other approaches considered.</a>
<a href="#26" id="26">PLASTIC-Model allows the ad hoc agent to adapt to a variety of teammates.</a>
<a href="#27" id="27">Also, the agent's behavior differs greatly when cooperating with different teammates, indicating that it is not just following a single policy with all teammates.</a>
<a href="#28" id="28">These results show that PLASTIC-Model allows agents to reuse knowledge about previous teammates to quickly adapt by exploiting similarities to observed teammates' behaviors in the pursuit domain.</a>
</body>
</html>