<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:642">
</head>
<body bgcolor="white">
<a href="#0" id="0">The key contributions of this framework are threefold: (1) kLog is a language that allows users to declaratively specify relational learning tasks in a similar way as statistical relational learning and inductive logic programming approaches but it is based on kernel methods rather than on probabilistic modeling; (2) kLog compiles the relational domain and learning task into a graph-based representation using a technique called graphicalization; and (3) kLog uses a graph kernel to construct the feature space where eventually the learning takes place.</a>
<a href="#1" id="1">Second, the feature space is not immediately defined by the declared logical formulae but it is constructed by a graph kernel, where nodes in the graph correspond to entities and relations, some given in the data, and some (expressing background knowledge) defined declaratively in Prolog.</a>
<a href="#2" id="2">In Section 3, we formalize the assumed statistical setting for supervised learning from interpretations, provide some background on statistical modeling from a relational learning point of view, and position kLog more clearly in the context of related systems such as Markov logic, M{sup:3}N [6], etc.</a>
<a href="#3" id="3">We assume that interpretations are sampled identically and independently from a fixed and unknown distribution D. We denote by {a mathematical formula}{zi;i ∈ I} the resulting data set, where {a mathematical formula}I is a given index set (e.g., the first n natural numbers) that can be thought of as interpretation identifiers.</a>
<a href="#4" id="4">The goal of supervised learning in this setting is to construct a prediction function f that maps the set of input atoms x (a partial interpretation) into a set of output atoms {a mathematical formula}f(x).</a>
<a href="#5" id="5">kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.</a>
<a href="#6" id="6">As explained in Section 6.2, features for the learning process are derived from a graph whose vertices are ground facts in the database; hence the ability to declare rules that specify relations directly translates into the ability to design and maintain features in a declarative fashion.</a>
<a href="#7" id="7">When {a mathematical formula}m=0 (no properties) we have a binary classification task (where positive cases are ground atoms that belong to the complete interpretation).</a>
<a href="#8" id="8">This becomes illegal in the second approach since otherwise there would be two or more atoms of the E-relation page with the same identifier.From a statistical point of view, since pages for the same department are part of the same interpretation and connected by hyperlinks, the corresponding category labels are interdependent random variables and we formally have an instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification[18].</a>
<a href="#9" id="9">This enables the application of several supervised learning algorithms that construct linear functions in the feature space {a mathematical formula}F. In this context, {a mathematical formula} Φ (z) can be either computed explicitly or defined implicitly, via a kernel function {a mathematical formula}K(z,z ′ )= 〈 Φ (z), Φ (z ′ ) 〉 .</a>
<a href="#10" id="10">However, since it is hard to limit the type of graph produced by the graphicalization procedure (e.g., cases with very high vertex degree are possible as in general an entity atom may play a role in an arbitrary number of relationship atoms), we prefer an approximate solution with efficiency guarantees based on topological distances similar in spirit to [46].</a>
<a href="#11" id="11">When dealing with tuples of mixed discrete and real values, the contribution of the kernels on the separate collections of discrete and real attributes are combined via summation:{a mathematical formula} where indices d and c run exclusively over the discrete and continuous properties respectively.</a>
<a href="#12" id="12">When moving to more complex jobs involving, e.g., classification of entities or tuples of entities, the kernel induces a feature vector {a mathematical formula} Φ (x,y) suitable for the application of a structured output technique where {a mathematical formula}f(x)=argmaxy ⁡ w ⊤ Φ (x,y).</a>
</body>
</html>