<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:307">
</head>
<body bgcolor="white">
<a href="#0" id="0">This extension has been defined within the L3 meta-algorithm (Algorithm 3), which works in three stages: first, the algorithm learns how to perform a task in the source domain, storing the optimal policy for this problem as a case base; second, it maps actions from the source domain to actions in the target domain; and third, it uses the case base learned in the first stage as heuristics in a CB-HARL algorithm.</a>
<a href="#1" id="1">To implement the case retrieval procedure, for each action selected, Equation (8) was used to compute the similarity between the current state of the car and each case in the case base.</a>
<a href="#2" id="2">As the source task has only two attributes (horizontal position x and velocity {a mathematical formula}x ˙ ) in the case base, and the problem has four attributes (x, {a mathematical formula}x ˙ , y, {a mathematical formula}y ˙ ), we used Equation (8) to find the most similar case between each degree of freedom of the 3D problem and the 2D problem, and computed the similarity of a case as the minimum value between these two results, as stated in Equation (11):{a mathematical formula} where {a mathematical formula}dist(a,b)=|a − b| is the Manhattan Distance between two points.</a>
<a href="#3" id="3">This characteristic makes L3 robust to negative transfers: if the cases acquired in the source domain are not useful in the target domain, assuming them as heuristics will not speed up the learning procedure but, in the worst case (when every case in the case base is not applicable to the target domain), L3 will be as efficient as the original RL algorithm that it is based.</a>
<a href="#4" id="4">In other words, if the case base contains no useful (or even misleading) information for the target domain, the agent is still able to learn the optimal policy for the domain using the RL component of the algorithm.</a>
</body>
</html>