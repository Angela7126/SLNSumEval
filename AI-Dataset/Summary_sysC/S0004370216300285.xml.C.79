<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:535">
</head>
<body bgcolor="white">
<a href="#0" id="0">These games differ in the number of possible actions, the number of moves before the game ends, the variance of the utility values, and the proportion of states in which mixed strategies are required for optimal play.</a>
<a href="#1" id="1">Before evaluating the successors, the algorithm determines whether the current action {a mathematical formula}ai of the searching player i can still be the best response action against the strategy of the opponent {a mathematical formula} Σ − i ′ .</a>
<a href="#2" id="2">In order to determine this, the algorithm computes value {a mathematical formula} Λ ai that represents the lower bound on the expected utility this action must gain against the current action of the opponent {a mathematical formula}a − i in order for action {a mathematical formula}ai to be a best response.</a>
<a href="#3" id="3">{a mathematical formula} Λ ai is calculated (line 7) by subtracting the upper bound of the expected value against all other actions of the opponent ({a mathematical formula}vT(s,ai,a − i ′ )i) from the current best response value ({a mathematical formula}viBR) and normalizing with the probability that the action {a mathematical formula}a − i is played by the opponent ({a mathematical formula} Σ − i ′ (a − i)).</a>
<a href="#4" id="4">This calculation corresponds to a situation where player i achieves the best possible utility by playing action {a mathematical formula}ai against all other actions from the strategy of the opponent and it needs to achieve at least {a mathematical formula} Λ ai against {a mathematical formula}a − i so that the expected value for playing {a mathematical formula}ai is at least {a mathematical formula}viBR.</a>
<a href="#5" id="5">If {a mathematical formula} Λ ai is strictly higher than the upper bound on the value of the subgame rooted in the successor (i.e., {a mathematical formula}vT(s,ai,a − i)i) then the algorithm knows that the action {a mathematical formula}ai can never be the best response action, and can proceed with the next action (line 9).</a>
<a href="#6" id="6">Note that {a mathematical formula} Λ ai is recalculated for each action of the opponent since the upper bound values can become tighter when the exact values are computed for successor nodes {a mathematical formula}s ′ (line 13).</a>
<a href="#7" id="7">Alternatively, the action to play in each state can be determined based on the mixed strategy obtained by normalizing the visit counts of each action{a mathematical formula} Using the first method certainly makes the algorithm not converge to a Nash equilibrium, because the game may require a mixed strategy.</a>
<a href="#8" id="8">In order to obtain positive formal results about the convergence of SM-MCTS-like algorithms, the authors in [59] either add an additional averaging step to the algorithm (that makes it significantly slower in practical games used in benchmarks), or assume additional non-trivial technical properties about the selection function, which are not known to hold for any of the selection functions above.</a>
<a href="#9" id="9">In our implementation of iterative deepening we follow a natural observation that saves the computation time between different searches: a solution computed in state s by player i to depth d contains an optimal solution on {a mathematical formula}d − 1 approximation of subgames starting in possible next states {a mathematical formula}T(s,r,c), where r is the action selected for the player performing the search and c is the action of the opponent.</a>
</body>
</html>