<html>
<head>
<meta name="TextLength" content="SENT_NUM:25, WORD_NUM:660">
</head>
<body bgcolor="white">
<a href="#0" id="0">Graphicalization is our approach to capture the relational structure of the data by means of a graph.</a>
<a href="#1" id="1">It enables the use of kernel methods in SRL via a graph kernel, which measures the similarity between two graphs.</a>
<a href="#2" id="2">The procedure maps a set of ground atoms into a bipartite undirected graph whose nodes are true ground atoms and whose edges connect an entity atom to a relationship atom if the identifier of the former appears as an argument in the latter.</a>
<a href="#3" id="3">The graph resulting from the graphicalization of the ai interpretation is shown in Fig.</a>
<a href="#4" id="4">2.</a>
<a href="#5" id="5">{sup:2} It is from this graph that kLog will generate propositional features (based on a graph kernel) for use in the learning procedure.</a>
<a href="#6" id="6">The details of the graphicalization procedure and the kernel are given in Sections 6 and 6.2, respectively.</a>
<a href="#7" id="7">Having specified a target relation r, kLog is able to infer the partition {a mathematical formula}x ∪ y of ground atoms into inputs and outputs in the supervised learning setting.</a>
<a href="#8" id="8">The output y consists of all ground atoms of r and all ground atoms of any intensional relation {a mathematical formula}r ′ which depends on r. The partition is inferred by analyzing the dependency graphs of Prolog predicates defining intensional relations, using an algorithm reminiscent of the call graph computation in ViPReSS [19].</a>
<a href="#9" id="9">The above approach effectively defines a kernel over interpretations{a mathematical formula} where {a mathematical formula}Gz is the result of graphicalization applied to interpretation z.</a>
<a href="#10" id="10">For learning jobs such as classification or regression on interpretations (see Table 2), this kernel is directly usable in conjunction with plain kernel machines like SVM.</a>
<a href="#11" id="11">When moving to more complex jobs involving, e.g., classification of entities or tuples of entities, the kernel induces a feature vector {a mathematical formula} Φ (x,y) suitable for the application of a structured output technique where {a mathematical formula}f(x)=argmaxy ⁡ w ⊤ Φ (x,y).</a>
<a href="#12" id="12">Alternatively, we may convert the structured output problem into a set of independent subproblems as follows.</a>
<a href="#13" id="13">For simplicity, assume the learning job consists of a single relation r of relational arity n. We call each ground atom of r a case.</a>
<a href="#14" id="14">Intuitively, cases correspond to training targets or prediction-time queries in supervised learning.</a>
<a href="#15" id="15">Usually an interpretation contains several cases corresponding to specific entities such as individual Web pages (as in Section 5.3) or movies (as in Section 5.4), or tuples of entities for link prediction problems (as in Section 2).</a>
<a href="#16" id="16">Given a case {a mathematical formula}c ∈ y, the viewpoint of c, {a mathematical formula}Wc, is the set of vertices that are adjacent to c in the graph.</a>
<a href="#17" id="17">In order to define the kernel, we first consider the mutilated graph {a mathematical formula}Gc where all vertices in y, except c, are removed (see Fig.</a>
<a href="#18" id="18">5 for an illustration).</a>
<a href="#19" id="19">We then define a kernel {a mathematical formula} Κ ˆ on mutilated graphs, following the same approach of the NSPDK, but with the additional constraint that the first endpoint must be in {a mathematical formula}Wc.</a>
<a href="#20" id="20">The decomposition is thus defined as{a mathematical formula} We obtain in this way a kernel “ centered ” around case c:{a mathematical formula} and finally we let{a mathematical formula} This kernel corresponds to the potential{a mathematical formula} which is clearly maximized by maximizing, independently, all sub-potentials {a mathematical formula}w ⊤ Φ ˆ (x,c) with respect to c.</a>
<a href="#21" id="21">We compared kLog to MLN and to Tilde on the same task.</a>
<a href="#22" id="22">For MLN we used the Alchemy system and the following set of formulae, which essentially encode the same domain knowledge exploited in kLog:{a mathematical formula} Ground atoms for the predicate LinkTo were actually precalculated externally in Prolog (same code as for the kLog's intensional signature) since regular expressions are not available in Alchemy.</a>
<a href="#23" id="23">We did not enforce mutually exclusive categories since results tended to be worse.</a>
<a href="#24" id="24">For learning we used the preconditioned scaled conjugate gradient approach described in [63] and we tried a wide range of values for the learning rate and the number of iterations.</a>
</body>
</html>