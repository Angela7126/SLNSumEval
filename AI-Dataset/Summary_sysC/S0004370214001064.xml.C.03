<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:671">
</head>
<body bgcolor="white">
<a href="#0" id="0">kLog adopts, as many other logical and relational learning systems, the learning from interpretations framework [9].</a>
<a href="#1" id="1">However, unlike typical statistical relational learning frameworks, kLog does not employ a probabilistic framework but is rather based on linear modeling in a kernel-defined feature space.</a>
<a href="#2" id="2">kLog generates a set of features starting from a logical and relational learning problem and uses these features for learning a (linear) statistical model.</a>
<a href="#3" id="3">Finally, the graphs produced by kLog are turned into feature vectors using a graph kernel, which leads to a statistical learning problem at the third level.</a>
<a href="#4" id="4">The goal of supervised learning in this setting is to construct a prediction function f that maps the set of input atoms x (a partial interpretation) into a set of output atoms {a mathematical formula}f(x).</a>
<a href="#5" id="5">kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.</a>
<a href="#6" id="6">A supervised learning job in kLog is specified as a set of relations.</a>
<a href="#7" id="7">Having specified a target relation r, kLog is able to infer the partition {a mathematical formula}x ∪ y of ground atoms into inputs and outputs in the supervised learning setting.</a>
<a href="#8" id="8">The goal is to map an interpretation {a mathematical formula}z=(x,y) into a feature vector {a mathematical formula} Φ (z)= Φ (x,y) ∈ F (see Section 3).</a>
<a href="#9" id="9">Note that, because of our data modeling assumptions (see Section 4.1), the degree of every vertex {a mathematical formula}v ∈ Fz equals the relational arity of the corresponding R-relation.</a>
<a href="#10" id="10">The semantics of kLog graphs for the learning procedure is however quite different and intimately related to the concept of graph kernels, as detailed in the following section.</a>
<a href="#11" id="11">Learning in kLog is performed using a suitable graph kernel on the graphicalized interpretations.</a>
<a href="#12" id="12">In the following sections, we introduce variants of {a mathematical formula} Κ subgraph to be used when the atoms in the relational data set can maximally have a single discrete or continuous property, or when more general tuples of properties are allowed.</a>
<a href="#13" id="13">We provide a way to do so, declaratively, by introducing the set of kernel points, a subset of {a mathematical formula}V(G) which includes all vertices associated with ground atoms of some specially marked signatures.</a>
<a href="#14" id="14">As kLog is a language for logical and relational learning with kernels it is related to work on inductive logic programming, to statistical relational learning, to graph kernels, and to propositionalization.</a>
<a href="#15" id="15">What kLog and statistical relational learning techniques have in common is that they both construct (implicitly or explicitly) graphs representing the instances.</a>
<a href="#16" id="16">Statistical relational learning systems then learn a probability distribution using the features and parameters in the graphical model, while kLog learns a function using the features derived by the kernel from the graphs.</a>
<a href="#17" id="17">As each such feature has a single weight, kLog also realizes parameter tying in a similar way as statistical relational learning methods.</a>
<a href="#18" id="18">One difference between these statistical relational learning models and kLog is that the former do not really have a second level as does kLog.</a>
<a href="#19" id="19">kLog builds also upon the many results on learning with graph kernels, see [73] for an overview.</a>
<a href="#20" id="20">A distinctive feature of kLog is automatic graphicalization of relational representations, which also allows users to naturally specify multitask and collective learning tasks.</a>
<a href="#21" id="21">The neighborhood of a vertex v is the set of vertices that are adjacent to v and is indicated with {a mathematical formula}N(v).</a>
<a href="#22" id="22">The neighborhood of radius r of a vertex v is the set of vertices at a distance less than or equal to r from v and is denoted by {a mathematical formula}Nr(v).</a>
<a href="#23" id="23">In a graph G, the induced subgraph on a set of vertices {a mathematical formula}W={w1, … ,wk} is a graph that has W as its vertex set and it contains every edge of G whose endpoints are in W. The neighborhood subgraph of radius r of vertex v is the subgraph induced by the neighborhood of radius r of v and is denoted by {a mathematical formula}Nrv.</a>
</body>
</html>