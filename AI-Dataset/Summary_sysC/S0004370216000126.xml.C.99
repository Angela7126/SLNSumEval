<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:471">
</head>
<body bgcolor="white">
<a href="#0" id="0">Conversely, in EML we aim at enabling the use of as many Machine Learning techniques as possible, within as many optimization methods as possible: the goal is having the ability to choose the most adequate solution approach for each problem.</a>
<a href="#1" id="1">As a consequence, our focus is mostly on handling the integration of Machine Learning models in optimization: for example, we emphasize the importance of model embedding techniques (in Section 5), and of methods to exploit the structure of the extracted model for boosting the search process.</a>
<a href="#2" id="2">Black box optimization Black-box optimization approaches are concerned with finding solutions for optimization problems having cost or constraint functions with unknown structure: the typical case is that of systems lacking a declarative model, but for which a simulator is available.</a>
<a href="#3" id="3">This is done by relying on a set of functions {a mathematical formula}hkbal:{0,1}n â†’ (0,1], each of which associates a mapping of all jobs to a value for the {a mathematical formula}effk variable, representing the efficiency of core k. A base model for the second problem variant can be formulated as follows:</a>
<a href="#4" id="4">LS approaches require only the ability to evaluate the cost and constraint functions, and they always manipulate fully-instantiated solutions.</a>
<a href="#5" id="5">For these reasons, embedding a Machine Learning model requires simply to implement a function evaluator.</a>
<a href="#6" id="6">As a downside, LS approaches are limited in their ability to exploit the model structure for boosting the search process.</a>
<a href="#7" id="7">In EML, this means that the Empirical Model constraints in the formulation from Section 4 are in fact a combination of two relations:{a mathematical formula} where y is a vector of variables representing the features, {a mathematical formula}hEM is the encoding of the Machine Learning model, and {a mathematical formula}hfeat are feature extraction constraints.</a>
<a href="#8" id="8">If the same values happen to be favorable in terms of cost in the problem model, then such input configurations, despite being uncommon in the training set, will be actively sought by the optimizer and will be much more likely to appear at search time.</a>
<a href="#9" id="9">Because of this, the actual error in the final system has a chance to be similar to the maximum error on the test set.</a>
<a href="#10" id="10">the authors of the platform simulator) have suggested to employ the average job CPI on each core as a proxy for its efficiency in the {a mathematical formula}WDPbal.</a>
<a href="#11" id="11">The resulting system model is:{a mathematical formula} where avgcpi is the vector of the {a mathematical formula}avgcpik values, as specified in the feature definitions (i.e.</a>
<a href="#12" id="12">Linear regression makes it feasible to obtain values for all the parameters; however, it also requires one to build a training set, and therefore an overall design effort much closer to that of extracting an Artificial Neural Network.</a>
<a href="#13" id="13">Still, in the FEAT model the cost function is linear, which may allow the optimization approach to get considerably closer to the optimum (predicted) cost.</a>
</body>
</html>