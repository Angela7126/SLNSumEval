<html>
<head>
<meta name="TextLength" content="SENT_NUM:23, WORD_NUM:652">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper introduces the kLog language and framework for kernel-based logical and relational learning.</a>
<a href="#1" id="1">The key contributions of this framework are threefold: (1) kLog is a language that allows users to declaratively specify relational learning tasks in a similar way as statistical relational learning and inductive logic programming approaches but it is based on kernel methods rather than on probabilistic modeling; (2) kLog compiles the relational domain and learning task into a graph-based representation using a technique called graphicalization; and (3) kLog uses a graph kernel to construct the feature space where eventually the learning takes place.</a>
<a href="#2" id="2">This whole process is reminiscent of knowledge-based model construction in statistical relational learning.</a>
<a href="#3" id="3">We now sketch these contributions in more detail and discuss the relationships with statistical relational learning.</a>
<a href="#4" id="4">kLog generates a set of features starting from a logical and relational learning problem and uses these features for learning a (linear) statistical model.</a>
<a href="#5" id="5">Finally, the graphs produced by kLog are turned into feature vectors using a graph kernel, which leads to a statistical learning problem at the third level.</a>
<a href="#6" id="6">In the next section, we illustrate the main steps of kLog modeling with a complete example in a real world domain.</a>
<a href="#7" id="7">In Section 3, we formalize the assumed statistical setting for supervised learning from interpretations, provide some background on statistical modeling from a relational learning point of view, and position kLog more clearly in the context of related systems such as Markov logic, M{sup:3}N [6], etc.</a>
<a href="#8" id="8">In Section 4, we formalize the semantics of the language and illustrate what types of learning problems can be formulated in the framework.</a>
<a href="#9" id="9">{sup:2} It is from this graph that kLog will generate propositional features (based on a graph kernel) for use in the learning procedure.</a>
<a href="#10" id="10">The details of the graphicalization procedure and the kernel are given in Sections 6 and 6.2, respectively.</a>
<a href="#11" id="11">kLog contributes to this perspective as it is a language for generating a set of features starting from a logical and relational learning problem and using these features for learning a (linear) statistical model.</a>
<a href="#12" id="12">a set of ground facts, embedded in a standard Prolog database, representing the data of the learning problem (see, e.g., Listing 1);</a>
<a href="#13" id="13">Having specified a target relation r, kLog is able to infer the partition {a mathematical formula}x ∪ y of ground atoms into inputs and outputs in the supervised learning setting.</a>
<a href="#14" id="14">The output y consists of all ground atoms of r and all ground atoms of any intensional relation {a mathematical formula}r ′ which depends on r. The partition is inferred by analyzing the dependency graphs of Prolog predicates defining intensional relations, using an algorithm reminiscent of the call graph computation in ViPReSS [19].</a>
<a href="#15" id="15">In the following sections, we introduce variants of {a mathematical formula} Κ subgraph to be used when the atoms in the relational data set can maximally have a single discrete or continuous property, or when more general tuples of properties are allowed.</a>
<a href="#16" id="16">As kLog is a language for logical and relational learning with kernels it is related to work on inductive logic programming, to statistical relational learning, to graph kernels, and to propositionalization.</a>
<a href="#17" id="17">We now discuss each of these lines of work and their relation to kLog.</a>
<a href="#18" id="18">kLog features correspond to subgraphs that represent relational templates and that may match (and hence be grounded) multiple times in the graphicalization.</a>
<a href="#19" id="19">As each such feature has a single weight, kLog also realizes parameter tying in a similar way as statistical relational learning methods.</a>
<a href="#20" id="20">One difference between these statistical relational learning models and kLog is that the former do not really have a second level as does kLog.</a>
<a href="#21" id="21">Indeed, the knowledge base model construction process directly generates the graphical model that includes all the features used for learning, while in kLog these features are derived from the graph kernel.</a>
<a href="#22" id="22">While statistical relational learning systems have been commonly used for collective learning, this is still a question for further research within kLog.</a>
</body>
</html>