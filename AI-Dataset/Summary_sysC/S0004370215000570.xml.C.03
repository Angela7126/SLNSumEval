<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:302">
</head>
<body bgcolor="white">
<a href="#0" id="0">In addition to occlusion dependent observation and action success probabilities, our POMDP model also automatically adapts object specific action success probabilities.</a>
<a href="#1" id="1">The online POMDP approach that we propose allows for long planning horizons by using a compact, fixed-size policy graph [17].</a>
<a href="#2" id="2">Following this, our probability model uses Beta distributions to model uncertainty in object specific grasp probabilities.</a>
<a href="#3" id="3">{a mathematical formula}P(s ′ |s,a) is the transition probability to move from state s to the next time step state {a mathematical formula}s ′ , when action a is executed.</a>
<a href="#4" id="4">The action success probability and the observation probability of an object depend on how occluded the object is.</a>
<a href="#5" id="5">At each time step the agent observes whether the grasp succeeded and makes an observation about object attributes.</a>
<a href="#6" id="6">Note that if grasp success or semantic locations are not fully observed, then we cannot estimate the initial POMDP belief directly using grasp success and object attribute observations.</a>
<a href="#7" id="7">Instead, we could update at each time step an (approximate) belief according to the current action and observation and use that as the initial POMDP belief.</a>
<a href="#8" id="8">In the experiments, we model the conditional probability of observing cup i as dirty when it is dirty with{a mathematical formula} where {a mathematical formula} Θ D1 and {a mathematical formula} Θ D2 are parameters that can be, similarly to the grasp probability, experimentally estimated from captured point clouds and object labels.</a>
<a href="#9" id="9">Note that the magnitude of {a mathematical formula}ni determines how much object specific grasp properties affect the grasp success probability compared to occlusion.</a>
<a href="#10" id="10">Overall, POMDP planning achieves higher reward than the heuristic manipulation approach.</a>
<a href="#11" id="11">5 shows a compact policy graph computed by the POMDP method for the first scene in Fig.</a>
<a href="#12" id="12">6 shows performance for the heuristic manipulation method and the POMDP method with a planning horizon of three for different reward choices.</a>
</body>
</html>