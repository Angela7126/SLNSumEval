<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:613">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this case, the teammates are not programmed to cooperate with this specific ad hoc agent, and they must be treated as given and inalterable.</a>
<a href="#1" id="1">Instead, this research focuses on enabling the ad hoc agent to cooperate with a variety of teammates in a range of possible environments.</a>
<a href="#2" id="2">In an ad hoc team, agents need to be able to cooperate with a variety of previously unseen teammates.</a>
<a href="#3" id="3">Rather than developing protocols for coordinating an entire team, ad hoc team research focuses on developing agents that cooperate with teammates in the absence of such explicit protocols.</a>
<a href="#4" id="4">We define the ad hoc agent's knowledge about its teammates' behaviors as{a mathematical formula} where {a mathematical formula}1 ≤ s ≤ n is the state, {a mathematical formula}1 ≤ t ≤ k specifies a teammate, {a mathematical formula}TrueActiont(s) is the ground truth action distribution for teammate t for state s, and {a mathematical formula}PredActiont(s) is the action distribution that the ad hoc agent predicts that teammate t will select for state s.</a>
<a href="#5" id="5">We assume that {a mathematical formula}PredActiont(s) is the uniform distribution if the agent has no information about teammate t's actions in state s. Thus, if the ad hoc agent has better information about its teammates' behaviors, the distance between the distributions will be smaller and TeamK will be higher.</a>
<a href="#6" id="6">Similarly to teammate knowledge, we formally define the ad hoc agent's knowledge about the environment's transitions as{a mathematical formula} where {a mathematical formula}1 ≤ s ≤ n is the state, {a mathematical formula}1 ≤ j ≤ m is a joint action, K is taken from Equation (1), {a mathematical formula}TrueTrans(s,j) is the ground truth transition distribution from state s given joint action j, and PredTrans is the ad hoc agent's predicted transition distribution.</a>
<a href="#7" id="7">Therefore, the ad hoc agent must observe its teammates to determine their behaviors.</a>
<a href="#8" id="8">Once it knows the behaviors its teammates exhibit, the ad hoc agent can adapt accordingly.</a>
<a href="#9" id="9">To speed up the process of determining the teammates' behaviors, the ad hoc agent can draw upon its observations of past teammates, exploiting similarities between the current and past teammates' behaviors.</a>
<a href="#10" id="10">In this article, we consider scenarios in which the ad hoc agent has different amounts of prior knowledge of its teammates.</a>
<a href="#11" id="11">In this setting, the ad hoc agent is assumed to have observed some past teammates' actions, in the form of tuples {a mathematical formula}(s,ai), where {a mathematical formula}ai is the action that teammate i from the MDP state s. The learning problem is to build a model m that captures {a mathematical formula}P(ai|s) from this data.</a>
<a href="#12" id="12">To capture the notion that the ad hoc agent is expected to have extensive prior general domain expertise (as is assumed in the ad hoc teamwork setting), though not with the specific teammates at hand, we pre-train the ad hoc agent with observations of a pool of past teammates.</a>
<a href="#13" id="13">A common approach to the problem is to determine which source data set (previous teammate) is the most similar to the target data set (current teammate) and transfer knowledge from only this source data set, where the similarity refers to the probability of the teammates taking the same actions from the same states.</a>
<a href="#14" id="14">To capture the notion that the ad hoc agent is expected to have extensive prior general domain expertise (as is assumed in the ad hoc teamwork setting), though not with the specific teammates at hand, PLASTIC-Model observes a number of past teammates.</a>
<a href="#15" id="15">The Combined Policy line shows the performance if the agent learns a single policy using the data collected from all possible teammates, representing what an agent might do if treating this as a single agent learning problem instead of an ad hoc teamwork problem.</a>
</body>
</html>