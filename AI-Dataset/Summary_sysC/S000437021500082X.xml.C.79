<html>
<head>
<meta name="TextLength" content="SENT_NUM:4, WORD_NUM:303">
</head>
<body bgcolor="white">
<a href="#0" id="0">The L3 meta-algorithm works in 3 stages: first, it uses an RL algorithm to learn how to perform one task, storing the solution for this problem as a case in a case base; in the second stage, it maps actions of the source domain to actions of the target domain; and, in the last stage, it uses the stored cases as heuristics to speed up the Reinforcement Learning process in the target domain.</a>
<a href="#1" id="1">There are several possibilities to compute {a mathematical formula}Ht(st,at), from using a large value that is lower than {a mathematical formula}rn/(1 − Γ ), where {a mathematical formula}rn is the negative reward the agent receives in each time step (see [38] for a discussion on this value), to using a small value that depends on the instant values of the value function approximation, that can be defined as:{a mathematical formula} where Η is a small real value and {a mathematical formula} Π H(st) is the action suggested by the heuristic H.</a>
<a href="#2" id="2">This extension has been defined within the L3 meta-algorithm (Algorithm 3), which works in three stages: first, the algorithm learns how to perform a task in the source domain, storing the optimal policy for this problem as a case base; second, it maps actions from the source domain to actions in the target domain; and third, it uses the case base learned in the first stage as heuristics in a CB-HARL algorithm.</a>
<a href="#3" id="3">This characteristic makes L3 robust to negative transfers: if the cases acquired in the source domain are not useful in the target domain, assuming them as heuristics will not speed up the learning procedure but, in the worst case (when every case in the case base is not applicable to the target domain), L3 will be as efficient as the original RL algorithm that it is based.</a>
</body>
</html>