<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:844">
</head>
<body bgcolor="white">
<a href="#0" id="0">Additionally we show that enhancements developed for games of perfect information (where the state is fully observable to all players and state transitions are deterministic) can also be effective in games of imperfect information (where the state is partially observable with different observations for different players, and state transitions may be stochastic).</a>
<a href="#1" id="1">Although the actual methods used are different, the idea of a learning system acquiring knowledge over its lifetime as it is confronted by different problems is similar to the idea of a tree search algorithm transferring knowledge from one part of the game tree to another over the “ lifetime ” of a single search.</a>
<a href="#2" id="2">In this paper we present a new enhancement, EPisodic Information Capture and reuse (EPIC), that was designed by considering correlation between states in the card game Dou Di Zhu.</a>
<a href="#3" id="3">Dou Di Zhu has an episodic structure, where a game consists of a sequence of somewhat independent rounds, and EPIC is designed to correlate states in analogous positions within different episodes.</a>
<a href="#4" id="4">In [27] we show that the precise information reuse method has an impact on the performance of an enhancement, and in particular we show that policies designed to balance exploitation and exploration, such as Ε -greedy and UCB1 [6], produce strong simulation policies.</a>
<a href="#5" id="5">In the case of UCB1, this leads to an elegant MCTS algorithm which uses a bandit algorithm to select all moves in the playout, where in the MCTS tree the action value estimates correspond to information about a single state and in simulations the action value estimates correspond to information reused between many states.</a>
<a href="#6" id="6">Thus the only difference between the “ in tree ” (selection) and “ out of tree ” (simulation) modes of MCTS is whether the context in which the bandit algorithm executes is specific to a single state or general across a larger collection of states.</a>
<a href="#7" id="7">This is achieved by building a tree of information sets (sets of states indistinguishable from one player's view point) rather than individual states, and dealing with the increased branching factor by restricting each MCTS iteration to a random determinization (a state sampled at random from the current information set).</a>
<a href="#8" id="8">In this paper we use the MO-ISMCTS version of the algorithm, which deals with games that have partially observable moves by constructing a separate search tree (a “ projection ” of the underlying game tree) to reflect each player's observation of the game.</a>
<a href="#9" id="9">{a mathematical formula} ⌣ i, for each player{a mathematical formula}i=0,1, … , Κ , is an equivalence relation on Λ , whose classes are moves as observed by player i, such that for all{a mathematical formula}q,r,s ∈ S,{a mathematical formula}(q,r) ⌣ Ρ (q)(q,s)implies{a mathematical formula}r=s.</a>
<a href="#10" id="10">A game can be described as a sequential decision problem, where the players collectively choose a path through {a mathematical formula}(S, Λ ) from {a mathematical formula}s0 to a terminal state.</a>
<a href="#11" id="11">This function takes two arguments (the root game state and the current move history) and maps them to a sequence of (record, capture context) pairs which are to be updated following a playout.</a>
<a href="#12" id="12">Each step of the playout uses the policy function Α to choose an action a, depending on the current move history {a mathematical formula}[h] ⌣ Ρ (s) for the player about to act from state s, the current information mapping Θ , and the set of available actions {a mathematical formula}A(s) (line 11).</a>
<a href="#13" id="13">After the playout has reached a terminal state, the capture function is applied to the root determinization {a mathematical formula}sroot and the terminal history h to obtain the sequence of (record, context) pairs to be updated (line 16).</a>
<a href="#14" id="14">The idea is to maintain average reward statistics for each action independently of where it occurs in the game tree, and use these statistics to bias the simulation policy.</a>
<a href="#15" id="15">In [50] we show that NAST works for the three imperfect information games studied in the present paper (Section 7), with {a mathematical formula}n=2 typically giving the strongest performance.</a>
<a href="#16" id="16">All ways of combining ICARUSes make use of information from the baseline definition (Section 4.2) in some way, whether to determine the current stage (selection, expansion or simulation) of the playout or to vary the combination coefficient.</a>
<a href="#17" id="17">Thus for a combination to make sense, it must incorporate the baseline ICARUS.</a>
<a href="#18" id="18">If the episodes truly are independent, this implies that the strength of a policy for a particular episode does not depend on the context of where that episode occurs in the game.</a>
<a href="#19" id="19">Thus strong play overall can be achieved by constructing a good policy for each episode, and combining these policies to obtain a policy for the full game.</a>
<a href="#20" id="20">The fact that the same episode occurs in several different parts of the game tree implies that a naïve tree search algorithm must rediscover the strong episode policy many times.</a>
<a href="#21" id="21">If EPIC is combined with the baseline algorithm using sequential combination then during simulation, subset-armed UCB1 selection is applied according to the current position-in-episode (EPIC-4): effectively this means that the simulation policy for the overall search is provided by the tree policy in the episode trees.</a>
</body>
</html>