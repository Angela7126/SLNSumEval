<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:517">
</head>
<body bgcolor="white">
<a href="#0" id="0">For the experiments described in this article, a Markovian model of human reward, {a mathematical formula}R ˆ H, is learned from human reward instances.</a>
<a href="#1" id="1">This model completes an MDP specification for the agent to plan in, {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#2" id="2">Thus, the output of {a mathematical formula}R ˆ H(s,a) for an agent's current state s and action a is the reward directly experienced by the learning agent.</a>
<a href="#3" id="3">In our experiments, a predictive model of human reward, {a mathematical formula}R ˆ H, is learned and provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#4" id="4">and when the agent acts non-myopically ({a mathematical formula} Γ =0.99) in a task that has both goal and failure absorbing states, varying both episodicity and the agent's distance from MDP-optimality (Section 10).</a>
<a href="#5" id="5">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#6" id="6">We first describe the results of the episodic-task version of the experiment specified in Section 6.1, using 19 fixed {a mathematical formula}R ˆ Hs that were pre-trained during a previous user study [13].</a>
<a href="#7" id="7">Recall that during training for this previous study, agents acted to maximize reward under {a mathematical formula} Γ =0 discounting; we address the consequences of this experiment's off-discounting character at the end of this subsection.</a>
<a href="#8" id="8">Mean task performance and mean total reward per episode for each tested discount factor across all 19 {a mathematical formula}R ˆ H models are displayed in Fig.</a>
<a href="#9" id="9">In the episodic-task analysis described here in Section 7.2.2 — as in the off-discounting analysis in the previous Section 7.2.1 — the human reward model {a mathematical formula}R ˆ H is learned by tamer and provides predictions that are interpreted as reward by an RL algorithm.</a>
<a href="#10" id="10">But unlike the previous analysis, {a mathematical formula}R ˆ H is learned while performing reinforcement learning, and the RL algorithm — not tamer — selects actions while learning {a mathematical formula}R ˆ H.</a>
<a href="#11" id="11">Thus this experiment is on-discounting, and the human trainer will be adapting to the same algorithm, with the same Γ , that is being tested.</a>
<a href="#12" id="12">The agent, task, and experiment conform to the baseline specified in Section 6.2.</a>
<a href="#13" id="13">Reward is predominately positive (a ratio greater than 1) for 66.7% of trainers in this experiment, which supports the conjecture that human reward generally has a positive bias.</a>
<a href="#14" id="14">At {a mathematical formula} Γ =0, all trainers are predominately positive, fitting results from tamer experiments (Section 7.1.1).</a>
<a href="#15" id="15">At {a mathematical formula} Γ =0.7, the closest condition to that of the {a mathematical formula} Γ =0.75 used in Thomaz and Breazeal, 5 of 8 trainers are predominately positive, 2 are predominately negative, and 1 trainer's ratio is exactly 1, roughly fitting Thomaz and Breazeal's observations [30].</a>
<a href="#16" id="16">However, for {a mathematical formula} Γ ≥ 0.99 most trainers are predominately negative, contrasting with what has been reported in past work, none of which includes such non-myopic learning.</a>
</body>
</html>