<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:251">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper proposes a middle ground between LfD and RL, introducing abstraction from demonstration algorithms, which take a small set of demonstrations from human teachers in order to infer the relevant features of the state space at each moment.</a>
<a href="#1" id="1">Then using this abstracted state space allows significant speed-ups in reinforcement learning algorithms with respect to the number of state features in complex domains.</a>
<a href="#2" id="2">Our experiments show that these speed-ups are exponential when using table-based representations and polynomial when used in conjunction with function approximation-based RL algorithms such as fitted Q-learning or LSPI.</a>
<a href="#3" id="3">Compared with learning a policy with LfD, our approach can obtain significantly better performance with orders of magnitude fewer samples.</a>
<a href="#4" id="4">Our first algorithm, state abstraction from demonstration (AfD) learns a policy for an MDP by building an abstract space {a mathematical formula}S 품 and using reinforcement learning to find an optimal policy that can be represented in {a mathematical formula}S 품 .</a>
<a href="#5" id="5">AfD obtains {a mathematical formula}S 품 by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S 품 can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
<a href="#6" id="6">AfD, shown in Algorithm 1, is composed of two steps.</a>
<a href="#7" id="7">First, a feature selection algorithm is applied to human demonstrations to choose the subset of features to use.</a>
</body>
</html>