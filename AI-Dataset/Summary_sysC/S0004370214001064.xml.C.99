<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:638">
</head>
<body bgcolor="white">
<a href="#0" id="0">The key contributions of this framework are threefold: (1) kLog is a language that allows users to declaratively specify relational learning tasks in a similar way as statistical relational learning and inductive logic programming approaches but it is based on kernel methods rather than on probabilistic modeling; (2) kLog compiles the relational domain and learning task into a graph-based representation using a technique called graphicalization; and (3) kLog uses a graph kernel to construct the feature space where eventually the learning takes place.</a>
<a href="#1" id="1">The goal of supervised learning in this setting is to construct a prediction function f that maps the set of input atoms x (a partial interpretation) into a set of output atoms {a mathematical formula}f(x).</a>
<a href="#2" id="2">The ability to specify intensional predicates through clauses (see an example in Listing 3) is most useful for introducing background knowledge in the learning process and common practice in inductive logic programming [9].</a>
<a href="#3" id="3">As explained in Section 6.2, features for the learning process are derived from a graph whose vertices are ground facts in the database; hence the ability to declare rules that specify relations directly translates into the ability to design and maintain features in a declarative fashion.</a>
<a href="#4" id="4">When {a mathematical formula}n=0, the declared job consists of predicting one or more properties of an entire interpretation, when {a mathematical formula}n=1 one or more properties of certain entities, when {a mathematical formula}n=2 one or more properties of pairs of entities, and so on.</a>
<a href="#5" id="5">When {a mathematical formula}m=0 (no properties) we have a binary classification task (where positive cases are ground atoms that belong to the complete interpretation).</a>
<a href="#6" id="6">This enables the application of several supervised learning algorithms that construct linear functions in the feature space {a mathematical formula}F. In this context, {a mathematical formula} Φ (z) can be either computed explicitly or defined implicitly, via a kernel function {a mathematical formula}K(z,z ′ )= 〈 Φ (z), Φ (z ′ ) 〉 .</a>
<a href="#7" id="7">However, since it is hard to limit the type of graph produced by the graphicalization procedure (e.g., cases with very high vertex degree are possible as in general an entity atom may play a role in an arbitrary number of relationship atoms), we prefer an approximate solution with efficiency guarantees based on topological distances similar in spirit to [46].</a>
<a href="#8" id="8">When moving to more complex jobs involving, e.g., classification of entities or tuples of entities, the kernel induces a feature vector {a mathematical formula} Φ (x,y) suitable for the application of a structured output technique where {a mathematical formula}f(x)=argmaxy ⁡ w ⊤ Φ (x,y).</a>
<a href="#9" id="9">After graphicalization, vertices representing webpages have large degree (at least the number of words), making the standard NSPDK of [11] totally inadequate: even by setting the maximum distance {a mathematical formula}d ⁎ =1 and the maximum radius {a mathematical formula}r ⁎ =2, the hard match would essentially create a distinct feature for every page.</a>
<a href="#10" id="10">In this domain we can therefore appreciate the flexibility of the kernel defined in Section 6.2.</a>
<a href="#11" id="11">In spite of the advantage of MLN for using a collective inference approach, results (Table 7) are comparable to those obtained with kLog (MLN tends to overpredict the class “ student ” , resulting in a slightly lower average {a mathematical formula}F1 measure, but accuracies are identical).</a>
<a href="#12" id="12">Thus the feature extracted by kLog using the graph kernel are capable of capturing enough contextual information from the input portion of the data to obviate the lack of collective inference.</a>
<a href="#13" id="13">kLog features correspond to subgraphs that represent relational templates and that may match (and hence be grounded) multiple times in the graphicalization.</a>
<a href="#14" id="14">As each such feature has a single weight, kLog also realizes parameter tying in a similar way as statistical relational learning methods.</a>
<a href="#15" id="15">One difference between these statistical relational learning models and kLog is that the former do not really have a second level as does kLog.</a>
</body>
</html>