<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:543">
</head>
<body bgcolor="white">
<a href="#0" id="0">Before evaluating the successors, the algorithm determines whether the current action {a mathematical formula}ai of the searching player i can still be the best response action against the strategy of the opponent {a mathematical formula} Σ − i ′ .</a>
<a href="#1" id="1">In order to determine this, the algorithm computes value {a mathematical formula} Λ ai that represents the lower bound on the expected utility this action must gain against the current action of the opponent {a mathematical formula}a − i in order for action {a mathematical formula}ai to be a best response.</a>
<a href="#2" id="2">Alternatively, the action to play in each state can be determined based on the mixed strategy obtained by normalizing the visit counts of each action{a mathematical formula} Using the first method certainly makes the algorithm not converge to a Nash equilibrium, because the game may require a mixed strategy.</a>
<a href="#3" id="3">Therefore, unless stated otherwise, we only use the mixed form in Equation (7), which was called DUCT(mix) in [11], [61].</a>
<a href="#4" id="4">In order to obtain positive formal results about the convergence of SM-MCTS-like algorithms, the authors in [59] either add an additional averaging step to the algorithm (that makes it significantly slower in practical games used in benchmarks), or assume additional non-trivial technical properties about the selection function, which are not known to hold for any of the selection functions above.</a>
<a href="#5" id="5">As for computational complexity, the time cost per node is linear in {a mathematical formula}|Ai| for UCT and RM.</a>
<a href="#6" id="6">Since CFR minimizes the counterfactual regret in an individual information set regardless of the strategies in other information sets, the samples acquired during the tree building cannot have a negative impact on the rate of regret minimization in individual states.</a>
<a href="#7" id="7">Therefore, we can use [74, Theorem 4] that bounds the number of iterations needed for OS as an offline solver with the complete game in the memory, starting after the tree has been built with a high probability.</a>
<a href="#8" id="8">In iterative deepening, the algorithm by default starts at depth {a mathematical formula}d=1 and gradually increases d until there is no more time.</a>
<a href="#9" id="9">In our implementation of iterative deepening we follow a natural observation that saves the computation time between different searches: a solution computed in state s by player i to depth d contains an optimal solution on {a mathematical formula}d − 1 approximation of subgames starting in possible next states {a mathematical formula}T(s,r,c), where r is the action selected for the player performing the search and c is the action of the opponent.</a>
<a href="#10" id="10">Therefore, when the iterative deepening algorithm starts a new search in state {a mathematical formula}s ′ ∈ T(s,r,c), it can often begin at depth d. This can require space exponential in the depth d in the worst case, but it is beneficial in practical experiments.</a>
<a href="#11" id="11">When information is missing due to pruning, then a search starts with the lowest possible depth {a mathematical formula}d=1.</a>
<a href="#12" id="12">With the increasing number of the coins the players need to use mixed strategies, however, mixing is typically required only at the beginning of the game.</a>
<a href="#13" id="13">As an example, the following table depicts the number of states with pure strategies and mixed strategies in a subgame-perfect NE calculated by backward induction for Oshi-Zumo with {a mathematical formula}N=10 coins, {a mathematical formula}K=3, and minimal bid {a mathematical formula}M=1.</a>
</body>
</html>