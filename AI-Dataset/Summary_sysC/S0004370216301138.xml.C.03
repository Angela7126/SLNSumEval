<html>
<head>
<meta name="TextLength" content="SENT_NUM:15, WORD_NUM:370">
</head>
<body bgcolor="white">
<a href="#0" id="0">In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track.</a>
<a href="#1" id="1">The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step.</a>
<a href="#2" id="2">Specifically, for each type of instances T and each SAT solver S, an automated fixed-time offline configuration phase determines parameter settings of S optimized for high performance on T. Then, the performance of S on T is evaluated with these settings, and the solver with the best performance wins.</a>
<a href="#3" id="3">Our approach to evaluating solver performance after configuration could of course be transferred to any other competition.</a>
<a href="#4" id="4">We then executed only this configuration on the test set to determine the performance of the configured solver.</a>
<a href="#5" id="5">Lingeling[16] is a CDCL solver with 241 parameters (making it the solver with the largest configuration space in the CSSC 2013).</a>
<a href="#6" id="6">We then executed only this configuration on the test set to assess the performance of the configured solver.</a>
<a href="#7" id="7">1 visualizes the results of the configuration process for the winning solver Lingeling on these four benchmark sets.</a>
<a href="#8" id="8">2 visualizes the improvements algorithm configuration yielded for the best-performing solver Clasp-3.0.4-p8 on these benchmarks.</a>
<a href="#9" id="9">Table 5 summarizes the results we obtained for all solvers on these benchmarks, showing that configuration also substantially improved the performance of many other solvers.</a>
<a href="#10" id="10">We then executed only this configuration on the benchmark's test set to determine the performance of the configured solver.</a>
<a href="#11" id="11">For each of the four tracks of CSSC 2014, we configured the solvers submitted to the track on each of the three benchmark families from that track and aggregated results across the respective test instances.</a>
<a href="#12" id="12">4 visualizes the results of applying algorithm configuration to the winning solver Lingeling on these three benchmark sets.</a>
<a href="#13" id="13">We then examine the relationship between configurability and number of parameters to determine whether solvers with many parameters consistently benefited more or less from configuration than solvers with few parameters.</a>
<a href="#14" id="14">For each of these categories, we used various benchmark sets, each of them split into a training set to be used for algorithm configuration and a disjoint test set.</a>
</body>
</html>