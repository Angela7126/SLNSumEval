<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:500">
</head>
<body bgcolor="white">
<a href="#0" id="0">Reinforcement Learning (RL) agents are typically deployed to learn a specific, concrete task based on a pre-defined reward function.</a>
<a href="#1" id="1">However, in some cases an agent may be able to gain experience in the domain prior to being given a task.</a>
<a href="#2" id="2">In such cases, intrinsic motivation can be used to enable the agent to learn a useful model of the environment that is likely to help it learn its eventual tasks more efficiently.</a>
<a href="#3" id="3">This paradigm fits robots particularly well, as they need to learn about their own dynamics and affordances which can be applied to many different tasks.</a>
<a href="#4" id="4">This article presents the texplore with Variance-And-Novelty-Intrinsic-Rewards algorithm (texplore-vanir), an intrinsically motivated model-based RL algorithm.</a>
<a href="#5" id="5">The algorithm learns models of the transition dynamics of a domain using random forests.</a>
<a href="#6" id="6">It calculates two different intrinsic motivations from this model: one to explore where the model is uncertain, and one to acquire novel experiences that the model has not yet been trained on.</a>
<a href="#7" id="7">This article presents experiments demonstrating that the combination of these two intrinsic rewards enables the algorithm to learn an accurate model of a domain with no external rewards and that the learned model can be used afterward to perform tasks in the domain.</a>
<a href="#8" id="8">While learning the model, the agent explores the domain in a developing and curious way, progressively learning more complex skills.</a>
<a href="#9" id="9">In addition, the experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone.</a>
<a href="#10" id="10">We also present results demonstrating the applicability of this approach to learning on robots.</a>
<a href="#11" id="11">Evaluating the benefits of intrinsic motivation is not as straightforward as evaluating a standard RL agent on a specific task.</a>
<a href="#12" id="12">Rather than attempting to accrue reward on a given task, a curious agent's goal is better stated as preparing itself for any task.</a>
<a href="#13" id="13">We therefore evaluate texplore-vanir in four ways on a complex domain with no external rewards.</a>
<a href="#14" id="14">First, we measure the accuracy of the agent's learned model in predicting the domain's transition dynamics.</a>
<a href="#15" id="15">Second, we test whether the learned model can be used to perform tasks in the domain when given a reward function.</a>
<a href="#16" id="16">Third, we examine the agent's exploration to see if it is exploring in a developing, curious way.</a>
<a href="#17" id="17">Finally, we demonstrate that texplore-vanir can combine its intrinsic rewards with external rewards to learn faster than if it was given only external rewards.</a>
<a href="#18" id="18">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#19" id="19">These results show that texplore-vanir's intrinsic rewards drive the agent to explore more of the state space and discover more affordances in the task than if the agent explored randomly.</a>
<a href="#20" id="20">Exploring with texplore-vanir enables the robot to learn a more accurate model of more of the state space and perform better on possible tasks given to the robot after exploring the domain.</a>
</body>
</html>