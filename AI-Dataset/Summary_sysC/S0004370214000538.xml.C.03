<html>
<head>
<meta name="TextLength" content="SENT_NUM:36, WORD_NUM:909">
</head>
<body bgcolor="white">
<a href="#0" id="0">More specifically, we prove that the performance regret (i.e., the difference between the performance of a particular algorithm and that of the optimal solution) of the bounded Ε -first approach is at most {a mathematical formula}O(B23) with a high probability, where B is the total budget.</a>
<a href="#1" id="1">Let {a mathematical formula} Μ i denote the mean value of the rewards that the agent receives from pulling arm i.</a>
<a href="#2" id="2">Let {a mathematical formula}NiB(A) be the random variable that represents the total number of pulls of arm i by A, with respect to the budget limit B.</a>
<a href="#3" id="3">That is:{a mathematical formula} Now, let {a mathematical formula}GB(A) be the total reward earned by using A to pull the arms within budget limit B.</a>
<a href="#4" id="4">As described in Section 1, each worker i has a different maximum number of tasks {a mathematical formula}Li that can be assigned to it.</a>
<a href="#5" id="5">The employer has a set budget to spend on developers, e.g., {a mathematical formula}B=$5000, and wishes to maximise the total number of working features.</a>
<a href="#6" id="6">Recall that within our setting, {a mathematical formula} Μ i are unknown a priori.</a>
<a href="#7" id="7">First, we repeatedly pull all arms in the first {a mathematical formula} ⌊ Ε B ∑ i=1Nci ⌋ time steps.</a>
<a href="#8" id="8">Given this, if {a mathematical formula}xiexplore denotes the number of times we pull arm i within the exploration phase, we have {a mathematical formula} ⌊ Ε B ∑ i=1Nci ⌋ ≤ xiexplore.</a>
<a href="#9" id="9">Hereafter we refer to the allocation sequence performed by the uniform algorithm as {a mathematical formula}Auni.</a>
<a href="#10" id="10">The bounded greedy algorithm works as follows: Let {a mathematical formula}viwi denote the density of type i.</a>
<a href="#11" id="11">Let {a mathematical formula} Μ ˆ i denote the estimate of {a mathematical formula} Μ i after the exploration phase.</a>
<a href="#12" id="12">When used together with the uniform exploration technique described above, we refer to this algorithm as bounded Ε -first, or {a mathematical formula}A Ε -first.</a>
<a href="#13" id="13">Recall that both {a mathematical formula}Auni and {a mathematical formula}Agreedy together form sequence {a mathematical formula}A Ε -first, which is the policy generated by the bounded Ε -first algorithm.</a>
<a href="#14" id="14">{a mathematical formula}E[G(1 − Ε )B(Agreedy)] ≥ ∑ j=1Nx ˆ j Μ j − 1.</a>
<a href="#15" id="15">Note that for any arm j, {a mathematical formula} ∑ i=1Ncixiexplore ≥ Ε B − cj, since none of the arms can be pulled after the stop of {a mathematical formula}Auni without exceeding Ε B.</a>
<a href="#16" id="16">Furthermore,{a mathematical formula} Recall that {a mathematical formula} Μ i ≤ 1.</a>
<a href="#17" id="17">Without loss of generality, assume that the bounded greedy chooses the arms to pull in the order of {a mathematical formula}1,2, … ,N.</a>
<a href="#18" id="18">Thus{a mathematical formula} which concludes the proof, since {a mathematical formula} Μ b ≤ 1.</a>
<a href="#19" id="19">We first show that{a mathematical formula} In particular, let {a mathematical formula} Σ i be the difference between the number of pulls of arm i within the optimal solution of {a mathematical formula}GB(A ⁎ ) and that of {a mathematical formula}G(1 − Ε )B(A ⁎ ).</a>
<a href="#20" id="20">Note that {a mathematical formula} Σ i can be negative.</a>
<a href="#21" id="21">This implies that{a mathematical formula} Note that {a mathematical formula}x ˆ j ≤ (1 − Ε )Bcj ≤ (1 − Ε )B.</a>
<a href="#22" id="22">By setting the exploration budget to be{a mathematical formula}B Ε opt, with at least probability Β , the regret of the bounded Ε -first algorithm is at most{a mathematical formula}</a>
<a href="#23" id="23">This result implies that the regret bound is guaranteed to be sub-linear (i.e., less than {a mathematical formula}O(B)), and thus, our algorithm converges to the optimal solution in an asymptotic manner.</a>
<a href="#24" id="24">In particular, for any {a mathematical formula}0< Α <1, there is a sufficiently large {a mathematical formula}B0 such that for any budget size {a mathematical formula}B>B0, the performance of our algorithm for that budget size is guaranteed to be better than an Α -ratio of the optimal solution.</a>
<a href="#25" id="25">With at least probability Β , the performance regret of the bounded Ε -first with SR exploration approach is at most{a mathematical formula}In addition, by optimally tuning Ε , we can show that the regret is at most{a mathematical formula}</a>
<a href="#26" id="26">Note that for {a mathematical formula}N ≥ 9, this regret bound is clearly worse than that of the Ε -first approach with uniform exploration (see Eq.</a>
<a href="#27" id="27">Optimal: this is a hypothetical optimal algorithm with full knowledge of each worker's mean quality {a mathematical formula} Μ i.</a>
<a href="#28" id="28">As we can see from the results, our algorithm typically outperforms the existing algorithms by up to {a mathematical formula}78%.</a>
<a href="#29" id="29">In particular, it outperforms the budget-limited Ε -first by {a mathematical formula}23% in the case of a small budget ({a mathematical formula} Ε =0.1 for the budget-limited algorithm).</a>
<a href="#30" id="30">However, if the budget is higher than $100, our algorithm clearly outperforms the others by up to {a mathematical formula}67%.</a>
<a href="#31" id="31">Thus, we also vary the budget B from $5000 to {a mathematical formula}$20,000, to analyse the performance of the algorithms (for consistency fixing the set of candidates to those that charge at most $50 per hour).</a>
<a href="#32" id="32">3, we can see that our algorithm typically outperforms the others by up to {a mathematical formula}200%, and it achieves around {a mathematical formula}95% of the optimum.</a>
<a href="#33" id="33">In addition, our algorithm outperforms the others by up to {a mathematical formula}162% (for the case of budget {a mathematical formula}B=$10,000).</a>
<a href="#34" id="34">However, it can still only achieve less than {a mathematical formula}60% of the bounded Ε -first.</a>
<a href="#35" id="35">We now turn to the investigation of whether we can improve the performance of the bounded Ε -first algorithm by replacing the uniform exploration approach with other policies.</a>
</body>
</html>