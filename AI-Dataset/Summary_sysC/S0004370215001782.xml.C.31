<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:399">
</head>
<body bgcolor="white">
<a href="#0" id="0">Term Unithood Model: We explore rich contextual information in the term unithood model to calculate how likely a source term should remain contiguous after translation.</a>
<a href="#1" id="1">We use this unithood model to reward translation hypotheses where multi-word terms are translated as a whole unit.</a>
<a href="#2" id="2">For the bilingual term bank created from training data, we first group each source term and all its corresponding target terms into a 2-tuple {a mathematical formula}G 〈 tf,Set(te) 〉 , where {a mathematical formula}tf is the source term and {a mathematical formula}Set(te) is the set of {a mathematical formula}tf's corresponding target terms.</a>
<a href="#3" id="3">We maintain a K-dimension vector for each 2-tuple {a mathematical formula}G 〈 tf,Set(te) 〉 .</a>
<a href="#4" id="4">The k-th component measures the translation consistency strength {a mathematical formula}cons(tf,k) of the source term {a mathematical formula}tf given the topic k.</a>
<a href="#5" id="5">These suggest that the consistency of term translation is topic-sensitive.</a>
<a href="#6" id="6">The same terms may be translated in a different consistency pattern in different topics.</a>
<a href="#7" id="7">Therefore we calculate the translation consistency strength of a source term {a mathematical formula}tf based on topic distributions.</a>
<a href="#8" id="8">What's the impact of the topic number k on the term translation disambiguation and consistency models that explore document-level topic information?</a>
<a href="#9" id="9">We derived new variations of the term translation consistency and unithood models with or without topic information to study the impact of topic information on these two models.</a>
<a href="#10" id="10">We compared our term translation consistency model against an alternative consistency method that rewards hypotheses with repeated terms from previously translated sentences.</a>
<a href="#11" id="11">“ Cons-No-Topic ” : This model is a simplified version of our term translation consistency model, which does not use any topic information.</a>
<a href="#12" id="12">In this model, the term translation consistency strength {a mathematical formula}cons(tf) is calculated as follows:{a mathematical formula} where {a mathematical formula}M,Nm,qmn are the same as defined in the equation (10).</a>
<a href="#13" id="13">Although these two models are better than the baseline and the “ CountFeat ” method as shown in Table 12, they are worse than our term translation consistency model.</a>
<a href="#14" id="14">Our model outperforms the “ Cons-Count ” model by up to 0.33 BLEU points.</a>
<a href="#15" id="15">This suggests that the strategy of capturing information of how terms are consistently translated in the training data is better than the strategy of counting the times of a term being consistently translated in a test set on the fly.</a>
<a href="#16" id="16">Our model is also better than the “ Cons-No-Topic ” model by 0.25 BLEU points on average.</a>
</body>
</html>