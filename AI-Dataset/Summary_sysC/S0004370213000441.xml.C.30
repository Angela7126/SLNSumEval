<html>
<head>
<meta name="TextLength" content="SENT_NUM:31, WORD_NUM:912">
</head>
<body bgcolor="white">
<a href="#0" id="0">In contrast to a {a mathematical formula}ToM0 agent, a first-order theory of mind ({a mathematical formula}ToM1) agent considers the possibility that his opponent is trying to win the game for herself, and that she reacts to the choices made by the {a mathematical formula}ToM1 agent.</a>
<a href="#1" id="1">To predict his opponent ʼ s behaviour, the {a mathematical formula}ToM1 agent puts himself in the position of his opponent, and considers the information available to him from her perspective.</a>
<a href="#2" id="2">(3) After integrating his zero-order beliefs {a mathematical formula}b(0) and his prediction of opponent behaviour {a mathematical formula}a ˆ j(1) based on first-order theory of mind, the agent chooses what action to play.</a>
<a href="#3" id="3">This decision is made analogously to the way a {a mathematical formula}ToM0 agent decides (Eq.</a>
<a href="#4" id="4">However, the {a mathematical formula}ToM1 agent decides based on his integrated beliefs {a mathematical formula}U(b(0),a ˆ j(1),c1) of opponent behaviour, instead of his zero-order beliefs {a mathematical formula}b(0) directly.</a>
<a href="#5" id="5">That is, a {a mathematical formula}ToM1 agent chooses to play the action given by{a mathematical formula} In the special case where the agent has no confidence in first-order theory of mind, {a mathematical formula}c1=0, the {a mathematical formula}ToM1 agent ʼ s decision is only influenced by his zero-order beliefs.</a>
<a href="#6" id="6">In this case, the agent chooses as if he were a {a mathematical formula}ToM0 agent.</a>
<a href="#7" id="7">6a shows that a {a mathematical formula}ToM1 agent that has a learning speed {a mathematical formula} Λ i=0 cannot compete with his opponent.</a>
<a href="#8" id="8">When the agent does not learn from his opponent ʼ s behaviour, he loses nearly all rounds.</a>
<a href="#9" id="9">Similarly, his opponent loses nearly all rounds when she does not learn at all ({a mathematical formula} Λ j=0).</a>
<a href="#10" id="10">This shows that both zero-order theory of mind agents and first-order theory of mind agents can successfully model an opponent that always performs the same action.</a>
<a href="#11" id="11">6b shows the performance of a {a mathematical formula}ToM2 agent playing RPS against a {a mathematical formula}ToM1 opponent.</a>
<a href="#12" id="12">As for the {a mathematical formula}ToM1 agent, a {a mathematical formula}ToM2 agent performs best when playing RPS against a {a mathematical formula}ToM1 opponent when both he and his opponent learn at a high speed, while the {a mathematical formula}ToM2 agent has more difficulty modeling a {a mathematical formula}ToM1 agent that learns slowly.</a>
<a href="#13" id="13">This shows that application of higher-order theory of mind can benefit an agent when playing RPS.</a>
<a href="#14" id="14">Performance of the {a mathematical formula}ToM2 agent playing RPS against a {a mathematical formula}ToM1 opponent is nonetheless slightly lower than that of a {a mathematical formula}ToM1 agent playing RPS against a {a mathematical formula}ToM0 opponent.</a>
<a href="#15" id="15">7c shows that the {a mathematical formula}ToM3 agent does not have this difficulty when playing ERPS against a similar opponent.</a>
<a href="#16" id="16">Since the richer action space of ERPS increases performance of the {a mathematical formula}ToM3 agent when playing against an opponent that does not learn, the structure of the game influences the effectiveness of theory of mind.</a>
<a href="#17" id="17">However, performance of a {a mathematical formula}ToM3 agent playing ERPS against a {a mathematical formula}ToM2 opponent is still poor in comparison to performance of the {a mathematical formula}ToM1 and {a mathematical formula}ToM2 agents playing ERPS against opponents of a lower order of theory of mind.</a>
<a href="#18" id="18">Although the {a mathematical formula}ToM1 and {a mathematical formula}ToM2 agents clearly outperform opponents of a lower order of theory of mind, the {a mathematical formula}ToM3 agent outperforms the {a mathematical formula}ToM2 agent at a small margin only.</a>
<a href="#19" id="19">7d shows the performance of a {a mathematical formula}ToM4 agent playing ERPS against a {a mathematical formula}ToM3 opponent.</a>
<a href="#20" id="20">Like the {a mathematical formula}ToM3 agent, the peak performance of the {a mathematical formula}ToM4 agent when playing against an opponent with learning speed {a mathematical formula} Λ j=0 shown in the figure indicates that the {a mathematical formula}ToM4 agent has no difficulty distinguishing agents that have learning speed zero from agents of a lower order of theory of mind.</a>
<a href="#21" id="21">hypothesis {a mathematical formula}HERPS, Section 2.4) that when agents choose from a limited action space, higher orders of theory of mind may experience difficulty modeling their opponent.</a>
<a href="#22" id="22">However, the limited action space does not explain the relatively poor performance of a {a mathematical formula}ToM3 agent when playing against a {a mathematical formula}ToM2 opponent, which was found both in RPS and ERPS.</a>
<a href="#23" id="23">8b shows the performance of a {a mathematical formula}ToM2 agent when playing RPSLS against a {a mathematical formula}ToM1 opponent.</a>
<a href="#24" id="24">Similar to the {a mathematical formula}ToM1 agent, performance of the {a mathematical formula}ToM2 agent is low when playing RPSLS against an opponent that learns at maximum speed, {a mathematical formula} Λ o=1.</a>
<a href="#25" id="25">The {a mathematical formula}ToM2 agent also has particular difficulties modeling a {a mathematical formula}ToM1 opponent in RPSLS when his own learning speed {a mathematical formula} Λ i is low.</a>
<a href="#26" id="26">In this case, the {a mathematical formula}ToM2 agent is outperformed by an opponent of lower order of theory of mind.</a>
<a href="#27" id="27">However, the {a mathematical formula}ToM2 agent will on average win when his learning speed {a mathematical formula} Λ i is over 0.7.</a>
<a href="#28" id="28">Similar to the games of RPS and ERPS, performance of a {a mathematical formula}ToM4 agent playing RPSLS against a {a mathematical formula}ToM3 opponent is mostly determined by which player has the highest learning speed, as shown in Fig.</a>
<a href="#29" id="29">Similar to the results found for the variations on RPS, the application of first-order and second-order theory of mind present an agent with a clear advantage over opponents of a lower order of theory of mind.</a>
<a href="#30" id="30">However, the advantage of a {a mathematical formula}ToM3 agent over a {a mathematical formula}ToM2 opponent is only marginal.</a>
</body>
</html>