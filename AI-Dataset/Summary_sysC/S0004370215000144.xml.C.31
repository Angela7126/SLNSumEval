<html>
<head>
<meta name="TextLength" content="SENT_NUM:52, WORD_NUM:1221">
</head>
<body bgcolor="white">
<a href="#0" id="0">Fig.</a>
<a href="#1" id="1">1 shows the number of planners that solved a particular number of problems considering the whole benchmark, that is, those that were reused according to the procedure described in Appendix D and the new problems.</a>
<a href="#2" id="2">As a result of the methodology used, we expected the results to fit a normal distribution.</a>
<a href="#3" id="3">In other words, we expected many problems to be solved by several planners and fewer problems to be solvable by only a few planners or all of them, corresponding to the two tails of a normal distribution.</a>
<a href="#4" id="4">These desirable properties were observed for all the IPC-2011 tracks except the sequential optimal track.</a>
<a href="#5" id="5">In this case, many problems were not solved by any competing planner, and many problems were solved by many entrants.</a>
<a href="#6" id="6">In our experience, selecting problems for this track seems to be very challenging, mainly because planners scale worse in this track, which reduces the maximum level of problem complexity they can handle.</a>
<a href="#7" id="7">Thus, selection of structurally different problems becomes harder and problems tend in general to be more similar in terms of difficulty than in the other tracks: either they are not solved at all or a large number of planners solve them, as evidenced, for example, by the results for the barman domain.</a>
<a href="#8" id="8">Fig.</a>
<a href="#9" id="9">8 shows the partial order according to the binomial test results for coverage.</a>
<a href="#10" id="10">Likewise, Fig.</a>
<a href="#11" id="11">9 (page 98) shows the partial order according to the Wilcoxon signed-rank test for plan quality.</a>
<a href="#12" id="12">Recall from the beginning of this section that only double hits are considered in the second case.</a>
<a href="#13" id="13">It should be noted that some results suggest a significant difference in favor of a planner that solves far fewer problems.</a>
<a href="#14" id="14">This is reasonable since use of only double hits completely ignores the overall performance of each planner.</a>
<a href="#15" id="15">A plausible interpretation of this observation is that some planners might solve problems with higher quality solutions but that the price they pay in searching to find these solutions is so high that they solve a much smaller set of problems.</a>
<a href="#16" id="16">Fig.</a>
<a href="#17" id="17">13 (page 101) shows the maximum memory required by all entrants in this track as a function of the number of problems solved by each of them.</a>
<a href="#18" id="18">In particular, brt takes less than 6 GB to solve 127 planning tasks, but it exceeds this limit to solve 30 additional tasks.</a>
<a href="#19" id="19">brt uses only a few hundreds of MB most of the time, but it can take a few GB when loading the results of a SAT compilation into memory.</a>
<a href="#20" id="20">It has been observed that this task consistently lasts for less than 5 s and is performed by a subprocess that never uses 6 GB on its own.</a>
<a href="#21" id="21">Remarkably, the winner of this track, arvandherd, did not outperform the winner of the sequential satisficing track and it solved 14 fewer problems than lama-2011 did, despite being allowed more computational resources.</a>
<a href="#22" id="22">A number of planners that entered this track also participated in the sequential satisficing track: yahsp2-mt, madagascar-p, madagascar, and acoplan.</a>
<a href="#23" id="23">yahsp2-mt was able to solve more problems when allowed to run the subprocesses for longer, solving 18 additional problems.</a>
<a href="#24" id="24">madagascar and madagascar-p solved exactly the same number of problems, 88 and 67, respectively, and acoplan, surprisingly, solved two fewer problems.</a>
<a href="#25" id="25">The performance of madagascar and madagascar-p was as expected, since these are the same planners as entered in the sequential satisficing track.</a>
<a href="#26" id="26">However, the case of acoplan seems more difficult to account for.</a>
<a href="#27" id="27">A plausible explanation is that this planner uses a stochastic algorithm so that small differences in coverage can easily be attributed to chance, even if more computational resources are allowed.</a>
<a href="#28" id="28">Fig.</a>
<a href="#29" id="29">17 shows the evolution of the number of problems solved in the interval {a mathematical formula}(0,1800]s. phsff and yahsp2-mt are the fastest algorithms in the short term, with phsff always solving more problems than yahsp2-mt up to the end of the interval.</a>
<a href="#30" id="30">phsff solved 120 problems in the first 13 s (and yahsp2-mt solved 109), whereas arvandherd solved 119 problems.</a>
<a href="#31" id="31">From this point on, the winner of this track progressed much faster and had already solved 123 problems at {a mathematical formula}t=14s, and yahsp2-mt solved only one additional problem.</a>
<a href="#32" id="32">However, these planners are still among the fastest up to {a mathematical formula}t=58s, at which point ayalsoplan catches yahsp2-mt, with both planners solving 123 problems by this time.</a>
<a href="#33" id="33">A couple of minutes later, at {a mathematical formula}t=219s, the coverage of ayalsoplan equals that of phsff, with 153 problems solved, and it finally solves more problems, as shown in Table 6 (page 101).</a>
<a href="#34" id="34">According to the results in Table 6 (page 101), arvandherd and ayalsoplan solved more problems with the highest score.</a>
<a href="#35" id="35">In particular, the difference between arvandherd and all the other entrants, as evidenced by the statistical test performed in Section 4.4.1 for which results are summarized in Fig.</a>
<a href="#36" id="36">16 (page 103), is deemed statistically significant.</a>
<a href="#37" id="37">ayalsoplan appears to be dominated by roamer-p and has indistinguishable performance from phsff.</a>
<a href="#38" id="38">However, these two planners were ranked far behind ayalsoplan.</a>
<a href="#39" id="39">Thus, the following planners were distinguished by their performance in the sequential multi-core track of IPC-2011:</a>
<a href="#40" id="40">In 2008 the winner of the sequential satisficing track was lama-2008 and a refined version of this planner was entered in IPC-2011.</a>
<a href="#41" id="41">Thus, this version was compared with the current winner, lama-2011, instead of the version submitted in 2008.</a>
<a href="#42" id="42">Again, the analysis in this section shows that, overall, the planner that won IPC-2011 improves on the performance of the IPC-2008 winner for the problems chosen for IPC-2011 in terms of coverage, quality, and raw speed.</a>
<a href="#43" id="43">Unfortunately, it is not as straightforward to compare the plan quality for these two planners because the scoring schema used in IPC-2011 may assign a planner a better score simply because the other planners in the same track were worse (Sections 2.3 and 6).</a>
<a href="#44" id="44">Thus, to provide a better assessment of differences in plan quality between lama-2011 and arvandherd, an alternative competition was simulated with all entrants in both the sequential multi-core and satisficing tracks, yielding a total number of 35 entrants.</a>
<a href="#45" id="45">In this way, both planners were compared with regard to the solutions provided by the same set of planners.</a>
<a href="#46" id="46">The results indicate that the score for lama-2011 slightly decreased from 216.33 to 215.51, while the score for arvandherd decreased even more, from 227.07 to 210.41.</a>
<a href="#47" id="47">The most prominent differences were observed for the nomystery and parking domains; arvandherd showed superior performance in the former but worse performance in the latter.</a>
<a href="#48" id="48">Overall, lama-2011 would have been declared the winner and arvandherd would have been selected as the best runner-up; its difference from the third-ranked planner, fdss-1, in this simulated competition is even greater, since fdss-1 was assigned 201.47 points.</a>
<a href="#49" id="49">In the final ranking, up to 10 classical planners obtain a better score than the second-ranked multi-core planner, which shows clear superiority of single-core over multi-core planners at the time of writing.</a>
<a href="#50" id="50">The fact that the winner of the sequential satisficing track surpassed the performance of the winner of the sequential multi-core track (at least in terms of both coverage and the official IPC score, and in terms of plan quality when considering the tasks solved by both planners) is not of great concern.</a>
<a href="#51" id="51">The history of the IPC shows that classical planners are highly engineered in terms of data structures and are difficult to beat in the first editions of new tracks.</a>
</body>
</html>