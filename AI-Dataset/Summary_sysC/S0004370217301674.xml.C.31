<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:621">
</head>
<body bgcolor="white">
<a href="#0" id="0">Algorithm 1 is described for the most general case of a fully-decentralized system with individual rewards, where states and rewards are annotated as {a mathematical formula}sm and {a mathematical formula}rm respectively, but it is also possible to implement a joint state vector or common reward DRL systems.</a>
<a href="#1" id="1">In addition, note that RL parameters could have been defined separately per agent ({a mathematical formula} Α m, Γ m), which is one of the DRL properties remarked in Section 2.3, but in Algorithm 1 they appear unified just for the sake of simplicity.</a>
<a href="#2" id="2">The problem descriptions and results are presented in a manner of increasing complexity.</a>
<a href="#3" id="3">3DMC is a canonical RL test-bed; it allows splitting the action space, as well as the state space for evaluating from a centralized system, up to a fully decentralized system with limited observability of the state space.</a>
<a href="#4" id="4">The Ball-Pushing problem also allows carrying out a performance comparison between a centralized and a decentralized scheme.</a>
<a href="#5" id="5">The best CRL and DRL learned policies are transferred and tested with a physical robot.</a>
<a href="#6" id="6">The Ball-Dribbling and SCARA-RTG problems are more complex systems (implemented with 3 and 4 individual agents respectively).</a>
<a href="#7" id="7">Ball-dribbling is a very complex behavior which involves three parallel sub-tasks in a highly dynamic and non-linear environment.</a>
<a href="#8" id="8">The SCARA-RTG has four joints acting simultaneously in a 3-Dimensional space, in which the observed state for the whole system is only the error between the current end-effector position, {a mathematical formula}[x,y,z], and a random target position.</a>
<a href="#9" id="9">Stage3.4 — Determining if the problem is fully decentralizable: one of the goals of this work is evaluating and comparing the response of an RL system under different centralized – decentralized schemes.</a>
<a href="#10" id="10">Thus, splitting the state vector is also proposed in order to have a fully decentralized system, and a very limited state observability for validating the usefulness of coordination of the presented MA DRL algorithms (Lenient and CA).</a>
<a href="#11" id="11">In this case, {a mathematical formula}agentx only state variables {a mathematical formula}[x,x ˙ ] can be observed, as well as {a mathematical formula}agenty only {a mathematical formula}[y,y ˙ ].</a>
<a href="#12" id="12">This corresponds to a very complex scenario because both agents have incomplete observations, and do not even have free or indirect coordination due to different state spaces, decentralized action spaces, and individual reward functions.</a>
<a href="#13" id="13">Moreover, the actions of each agent directly affect the joint environment, and both of the agents' next state observations.</a>
<a href="#14" id="14">Stage3.5 — Completing RL single modelings: one of the main goals of this work is also validating DRL system benefits.</a>
<a href="#15" id="15">Stage3.4 — Determining if the problem is fully decentralizable: since the three state variables, {a mathematical formula}[ Ρ , Γ , Φ ] of the joint vector state are required, this problem is not considered to be fully decentralizable.</a>
<a href="#16" id="16">So, the proposed modeling for learning the 3-Dimensional velocity vector from the joint observed state is detailed in Table 5.</a>
<a href="#17" id="17">Since 17 discrete actions per agent are implemented for the DRL system, if an equivalent CRL system were implemented, that centralized agent would search in an action space of {a mathematical formula}173=4913 possible actions, which would be enormous for most of the RL algorithms.</a>
<a href="#18" id="18">Even though we tried to reduce the number of discrete actions, the performance decreased dramatically.</a>
<a href="#19" id="19">Finally, the only way to achieve asymptotic convergence was using a noiseless model in which observations were taken from the ground truth system.</a>
<a href="#20" id="20">Thus, this CRL implementation is only for academic and comparison purposes.</a>
<a href="#21" id="21">Discrete actions must have been reduced up to five per action dimension, i.e.</a>
<a href="#22" id="22">a {a mathematical formula}53=125 combined action space.</a>
<a href="#23" id="23">The same joint state vector was used and the global reward function is similar to {a mathematical formula}rx in (11), but using {a mathematical formula} Γ th/3 and {a mathematical formula} Φ th/3.</a>
</body>
</html>