<html>
<head>
<meta name="TextLength" content="SENT_NUM:35, WORD_NUM:704">
</head>
<body bgcolor="white">
<a href="#0" id="0">built-in atoms {a mathematical formula}t ≺ u for terms t and u, and</a>
<a href="#1" id="1">In view of the objective of comparing participant systems in a uniform setting, this edition of the ASP Competition did not include a Model&Solve track, but took place in the spirit of the former System track: it was open to any general-purpose solving system, provided it was able to process ASP-Core-2 programs.</a>
<a href="#2" id="2">The general input – output format followed the 2013 edition, so that previous System track submissions should (in principle) be able to participate again.</a>
<a href="#3" id="3">However, the 2013 edition still made exceptions and admitted problem encodings in legacy formats, while the Fifth ASP Competition insisted on ASP-Core-2 compliance.</a>
<a href="#4" id="4">Benchmark domains, encodings, and instances used to assess participant systems were selected by the Organizing Committee; they are detailed in Section 4.</a>
<a href="#5" id="5">As described in Section 3, the benchmarks in the Fifth ASP Competition are categorized into tracks based on the language features utilized by encodings.</a>
<a href="#6" id="6">Table 1 provides an overview that groups benchmark domains in terms of language features in the ASP-Core-2 encodings from 2013.</a>
<a href="#7" id="7">That is, the 2013 encodings for Labyrinth and Stable Marriage belong to the Basic Decision track (#1), and the “ D ” entries in the fourth column indicate that both domains deal with Decision problems.</a>
<a href="#8" id="8">The Advanced Decision track (#2) includes the sixteen 2013 encodings for the domains in rows from Bottle Filling to Weighted-Sequence Problem.</a>
<a href="#9" id="9">Among them, the Reachability domain aims at Query answering, as indicated by the “ Q ” in the fourth column.</a>
<a href="#10" id="10">The next four rows marked with “ O ” provide the domains in the Optimization track (#3).</a>
<a href="#11" id="11">Finally, the last four rows give the domains in the Unrestricted track (#4), where Abstract Dialectical Frameworks is an Optimization problem and Strategic Companies deals with Query answering.</a>
<a href="#12" id="12">In order to furnish an extended benchmark collection for this year, we devised new encoding variants for all domains but Reachability and Strategic Companies, whose 2013 encodings are relatively straightforward positive programs subject to ground queries.</a>
<a href="#13" id="13">The system lp2sat[56], relying on eager translation of logic programs to SAT and using the precosat solver as search back-end, has been the best performing 2013 submission by the Aalto team.</a>
<a href="#14" id="14">Among this year's entries, lp2sat3+glucose and lp2sat3+lingeling are based on the same approach.</a>
<a href="#15" id="15">In order to use common inputs, we compare the previous and this year's systems on Basic Decision encodings only, taking into account that lp2sat processes a legacy format that differs from ASP-Core-2 on advanced constructs such as aggregates.</a>
<a href="#16" id="16">Given that the Basic Decision track on 2013 encodings merely includes two domains, we here also consider the six alternative encodings with basic features only, and Table 13 shows respective scores and cumulative CPU times for solved instances.</a>
<a href="#17" id="17">Regardless of which encoding is used, lp2sat, lp2sat3+glucose, and lp2sat3+lingeling have difficulties with instances of Labyrinth and Knight Tour with Holes, since translation to SAT is particularly intricate for the non-tight programs in these domains.</a>
<a href="#18" id="18">However, lp2sat3+glucose and lp2sat3+lingeling improve on lp2sat by solving from 5 to 8 instances of Labyrinth, depending on which encoding is used, whereas lp2sat times out on all non-tight instances.</a>
<a href="#19" id="19">Another gap is observed on the 2013 encoding for Stable Marriage, while the differences on 2014 encodings are smaller.</a>
<a href="#20" id="20">Nevertheless, lp2sat3+glucose and lp2sat3+lingeling have some advantages in Graph Coloring, but lp2sat solves more instances of Visit-all.</a>
<a href="#21" id="21">In general, the performance differences between lp2sat, on the one hand, and lp2sat3+glucose as well as lp2sat3+lingeling, on the other hand, are owed to re-engineered translation tools and the use of other SAT solvers as search back-ends.</a>
<a href="#22" id="22">Considering the overall picture, the solving approach based on translation to SAT has been clearly advanced since the Fourth ASP Competition in 2013.</a>
<a href="#23" id="23">IJCAR ATP System Competition – CASC 2014 [20], [86]</a>
<a href="#24" id="24">Confluence Competition – CoCo 2014 [21]</a>
<a href="#25" id="25">Hardware Model Checking Competition – HWMCC 2014 [54]</a>
<a href="#26" id="26">QBF Gallery – QBF 2014 [75]</a>
<a href="#27" id="27">SAT Competition – SAT-COMP 2014 [79], [58]</a>
<a href="#28" id="28">Satisfiability Modulo Theories Solver Competition – SMT-COMP 2014 [83], [9]</a>
<a href="#29" id="29">Competition on Software Verification – SV-COMP 2014 [87], [11]</a>
<a href="#30" id="30">Syntax-Guided Synthesis Competition – SyGuS-COMP 2014 [88]</a>
<a href="#31" id="31">Synthesis Competition – SYNTCOMP 2014 [89]</a>
<a href="#32" id="32">Termination Competition – termCOMP 2014 [90]</a>
<a href="#33" id="33">Max-SAT Evaluation – Max-SAT 2014 [67]</a>
<a href="#34" id="34">Mancoosi International Solver Competition – MISC 2012 [69]</a>
</body>
</html>