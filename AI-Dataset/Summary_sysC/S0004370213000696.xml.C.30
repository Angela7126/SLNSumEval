<html>
<head>
<meta name="TextLength" content="SENT_NUM:52, WORD_NUM:972">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}</a>
<a href="#1" id="1">For example, consider the payoff matrix M1 for a scenario in which agents A and B each have three possible actions.</a>
<a href="#2" id="2">If both agents select action 0 (i.e., their joint action is {a mathematical formula}(a0,b0)), then the joint team payoff is {a mathematical formula}m0,0=25.</a>
<a href="#3" id="3">Similarly if their joint action is {a mathematical formula}(a2,b0) their joint payoff is 0.</a>
<a href="#4" id="4">In this case, there is a unique joint action that leads to {a mathematical formula}m ⁎ : {a mathematical formula}m2,2=m ⁎ =40.</a>
<a href="#5" id="5">{a mathematical formula}</a>
<a href="#6" id="6">The second necessary alteration to Algorithm 1 in this case is that it is no longer sufficient to simply arrive at a joint action {a mathematical formula}(ai,bj) such that {a mathematical formula}mi, {a mathematical formula}j=m ⁎ .</a>
<a href="#7" id="7">Rather, the agents must arrive at such an action with a history of Agent A ʼ s actions such that if it keeps playing {a mathematical formula}ai, Agent B will keep selecting {a mathematical formula}bj.</a>
<a href="#8" id="8">{a mathematical formula}</a>
<a href="#9" id="9">{a mathematical formula}</a>
<a href="#10" id="10">That is, we conjectured that it was always the case that {a mathematical formula}L¯(M) ⩽ (min(x,y) − 1) ⁎ mem+1.</a>
<a href="#11" id="11">{a mathematical formula}</a>
<a href="#12" id="12">{a mathematical formula}</a>
<a href="#13" id="13">The learner selects between Arm1 and Arm2, while the teacher can additionally choose {a mathematical formula}Arm ⁎ .</a>
<a href="#14" id="14">Throughout the section we assume that {a mathematical formula} Μ ⁎ > Μ 1> Μ 2.</a>
<a href="#15" id="15">If {a mathematical formula} Μ ⁎ is not the largest, then the teacher ʼ s choice is trivially to always select the arm with the largest expected payoff.</a>
<a href="#16" id="16">Should the teacher select {a mathematical formula}Arm ⁎ , then the learner will select Arm2 (because {a mathematical formula}x¯1<x¯2), yielding an expected total payoff during the round of {a mathematical formula} Μ ⁎ + Μ 2=15.</a>
<a href="#17" id="17">On the other hand, should the teacher select Arm1, there is a greater than 50% chance that the learner will select Arm1 as well.</a>
<a href="#18" id="18">The expected payoff is then {a mathematical formula} Μ 1+ Η Μ 1+(1 − Η ) Μ 2>9+92+52=16.</a>
<a href="#19" id="19">It is never optimal for the teacher to pull{a mathematical formula}Arm2.</a>
<a href="#20" id="20">Third, we show that the teacher ʼ s choice is clear whenever {a mathematical formula}x¯1>x¯2.</a>
<a href="#21" id="21">That is, if the current sample average of Arm1 is greater than that of Arm2 such that the learner will choose Arm1 next, then the teacher should always choose {a mathematical formula}Arm ⁎ : it should not teach.</a>
<a href="#22" id="22">When{a mathematical formula}x¯1>x¯2, it is always optimal for the teacher not to teach (to pull{a mathematical formula}Arm ⁎ ).</a>
<a href="#23" id="23">When starting a new task such that the learner has no experience with any of its arms, the teacher should pull {a mathematical formula}Arm ⁎ : it should not teach.</a>
<a href="#24" id="24">To develop intuition, we begin by considering what the teacher should do when {a mathematical formula}r=1 (one action remaining for each agent).</a>
<a href="#25" id="25">As shown in Section 3.2, the teacher should never teach when {a mathematical formula}x¯1>x¯2.</a>
<a href="#26" id="26">For each reachable combination of values, the algorithm computes the teacher ʼ s optimal action (Arm1 or {a mathematical formula}Arm ⁎ ), denoted {a mathematical formula}Act[ ⋅ ]; and the expected long-term value of taking that action, denoted {a mathematical formula}Val[ ⋅ ]: the expected sum of payoffs for the optimal action and all future actions by both the teacher and the learner.</a>
<a href="#27" id="27">For example, it is only worth considering teaching when {a mathematical formula}x¯1<x¯2.</a>
<a href="#28" id="28">We then consider the case when {a mathematical formula}r=2 in Section 3.4.2 and present some empirical data in Section 3.4.3.</a>
<a href="#29" id="29">In contrast to the discrete case, we do not have an algorithm for exactly computing the optimal action when {a mathematical formula}r>1.</a>
<a href="#30" id="30">If {a mathematical formula}x¯1>x¯2, {a mathematical formula}EVnt= Μ ⁎ + Μ 1</a>
<a href="#31" id="31">Else {a mathematical formula}EVnt= Μ ⁎ + Μ 2.</a>
<a href="#32" id="32">In contrast, when {a mathematical formula}r>1, there is no such closed form method for finding the optimal action.</a>
<a href="#33" id="33">With these values it is barely not worth it for the teacher to teach with {a mathematical formula}r=1.</a>
<a href="#34" id="34">That is, with these values, Inequality (1) is not satisfied, but if {a mathematical formula}x¯1 were 7.01, then it would be satisfied.</a>
<a href="#35" id="35">Thus we know with certainty that the teacher ʼ s optimal action is {a mathematical formula}Arm ⁎ .</a>
<a href="#36" id="36">Thus it selects {a mathematical formula}Arm ⁎ and the learner selects Arm2.</a>
<a href="#37" id="37">Though the teacher ʼ s action has no impact on the relationship between {a mathematical formula}x¯1 and {a mathematical formula}x¯2 for the final round, the learner ʼ s action does.</a>
<a href="#38" id="38">In one set of 2000 samples, the status after the first round was as follows:</a>
<a href="#39" id="39">However with two rounds remaining, the optimal action is {a mathematical formula}Arm ⁎ .</a>
<a href="#40" id="40">Carrying through as in Section 3.2.1, it is clear that the teacher pulling {a mathematical formula}Armc yields a higher expected team value than pulling {a mathematical formula}Arm ⁎ or any other arm.</a>
<a href="#41" id="41">Thus the learner needs to consider pulling at least {a mathematical formula}Arm ⁎ and Arm1 – {a mathematical formula}Armz − 1.</a>
<a href="#42" id="42">The teacher should never take the action that the learner would take next on its own if the teacher were to pull {a mathematical formula}Arm ⁎ .</a>
<a href="#43" id="43">{a mathematical formula}x¯1<x¯2, {a mathematical formula}r=1: closed form solution for optimal teacher action</a>
<a href="#44" id="44">In Section 2.2.2, we examined the complexity of finding the optimal (lowest cost) path through a matrix when Agent B ʼ s {a mathematical formula}mem>1.</a>
<a href="#45" id="45">We let Agent B ʼ s {a mathematical formula}mem=n and we construct Matrix M as follows.</a>
<a href="#46" id="46">{a mathematical formula}</a>
<a href="#47" id="47">The payoff for selecting done only comes at {a mathematical formula}m ⁎ .</a>
<a href="#48" id="48">These payoffs ensure that if Agent A takes the done action before step n, then the cost is already higher than the target of {a mathematical formula}n ⁎ (n4 − 1).</a>
<a href="#49" id="49">That is, finding {a mathematical formula}S ⁎ when {a mathematical formula}mem>1 is NP-hard.</a>
<a href="#50" id="50">It is never optimal for the teacher to pull{a mathematical formula}Arm2.</a>
<a href="#51" id="51">{a mathematical formula}r=1.</a>
</body>
</html>