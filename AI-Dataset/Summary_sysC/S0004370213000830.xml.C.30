<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:390">
</head>
<body bgcolor="white">
<a href="#0" id="0">Next, we present a semi-supervised version of our feature labeling algorithm which assumes that an unlabeled set of instances is present during training.</a>
<a href="#1" id="1">The semi-supervised setting for feature labeling incorporates knowledge from three sources: a small labeled training set, the feature labels provided by the end user and information from the implicit structure of the unlabeled data.</a>
<a href="#2" id="2">We evaluate our semi-supervised algorithm using both oracle feature labels and end-user feature labels from the user study mentioned above.</a>
<a href="#3" id="3">Our feature labeling algorithm is one of the best performing algorithms with oracle feature labels and the best performer with lower quality feature labels from end users.</a>
<a href="#4" id="4">With our results, we can compare the relative gains using the different sources of knowledge available in the training set, the feature labels, and the unlabeled data.</a>
<a href="#5" id="5">Our analysis shows that incorporating unlabeled data during learning sometimes produces worse performance than just using a purely supervised learning approach, both with and without feature labeling.</a>
<a href="#6" id="6">However, adding the information from feature labels consistently improves performance over not including this information, both in the supervised and semi-supervised settings.</a>
<a href="#7" id="7">In order to evaluate the semi-supervised learning algorithms, which assume a pool of unlabeled data instances is available during training, we used the same 6 datasets as those used for supervised learning as well as the same oracle feature labels.</a>
<a href="#8" id="8">Furthermore, we used the same training/validation/test splits, but unlike in supervised learning, we made the unlabeled data instances from the test set available during training.</a>
<a href="#9" id="9">We used the same oracle feature labels from the previous section and present results when 10 oracle feature labels per class were provided to the semi-supervised algorithms.</a>
<a href="#10" id="10">Therefore, for our second experiment, we conducted a user study to harvest feature labels from actual end users on the same 20 Newsgroups classes as used in Section 4.1.</a>
<a href="#11" id="11">We then used the end users Ê¼ data to compare the performance of the same algorithms as in our oracle study, but with smaller validation sets of size 24 (six instances for each class) to simulate a realistic scenario in which end users were able to label only a limited amount of training instances for both a training and a validation set.</a>
<a href="#12" id="12">Having presented results for the supervised feature labeling setting, we now present results for the semi-supervised setting when an unlabeled pool of data was available during training.</a>
</body>
</html>