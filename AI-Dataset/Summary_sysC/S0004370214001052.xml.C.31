<html>
<head>
<meta name="TextLength" content="SENT_NUM:41, WORD_NUM:849">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula} Κ ∈ Nis the number of players;</a>
<a href="#1" id="1">{a mathematical formula} Ρ :S → {0,1, … , Κ }defines the player about to act in each state;</a>
<a href="#2" id="2">{a mathematical formula} ∼ i, for each player{a mathematical formula}i=0,1, … , Κ , is an equivalence relation on S, whose classes are player i's information sets;</a>
<a href="#3" id="3">3.</a>
<a href="#4" id="4">{a mathematical formula} Θ initial:R → Θ is the initial information function, which maps each record to a piece of information.</a>
<a href="#5" id="5">4.</a>
<a href="#6" id="6">{a mathematical formula} Α :M ⁎ ×(R → Θ )×2A → (A → [0,1])is the policy function.</a>
<a href="#7" id="7">This function takes three arguments (the current move history, the current mapping of records to information, and the legal action set for the current state) and returns a probability distribution over the action set.</a>
<a href="#8" id="8">The same function Α is used during selection and simulation phases of the playout.</a>
<a href="#9" id="9">6.</a>
<a href="#10" id="10">{a mathematical formula} Ξ :S×M ⁎ → (R× Ψ ) ⁎ is the capture function.</a>
<a href="#11" id="11">7.</a>
<a href="#12" id="12">{a mathematical formula} Ω : Θ × Ψ ×R Κ → Θ is the backpropagation function.</a>
<a href="#13" id="13">This function takes three arguments (the current information for a record, the capture context specified by the capture function, and the reward vector from the simulation) and returns the new information for the record following a playout.</a>
<a href="#14" id="14">Each step of the playout uses the policy function Α to choose an action a, depending on the current move history {a mathematical formula}[h] ⌣ Ρ (s) for the player about to act from state s, the current information mapping Θ , and the set of available actions {a mathematical formula}A(s) (line 11).</a>
<a href="#15" id="15">The current history h is updated by appending a, and the current state s is updated by applying a.</a>
<a href="#16" id="16">Specification 1 describes the baseline ICARUS definition used by an unenhanced search algorithm, defining the functions used in Algorithm 1.</a>
<a href="#17" id="17">The resulting algorithm is equivalent to UCT [7] in the perfect information case and MO-ISMCTS with the UCB1 selection policy [30] in the imperfect information case.</a>
<a href="#18" id="18">The algorithm uses reward vectors and assumes that each player tries to maximise his own reward in a {a mathematical formula}maxn fashion [39], [40], thus the algorithm can handle games with {a mathematical formula} Κ >2 players as well as single-player and two-player games.</a>
<a href="#19" id="19">a selection node if {a mathematical formula} Θ nbase([h] ⌣ )>0 and {a mathematical formula} Θ nbase([h{an inline-figure}a] ⌣ )>0 for all {a mathematical formula}a ∈ As;</a>
<a href="#20" id="20">an expansion node if {a mathematical formula} Θ nbase([h] ⌣ )>0 but {a mathematical formula} Θ nbase([h{an inline-figure}a] ⌣ )=0 for at least one {a mathematical formula}a ∈ As;</a>
<a href="#21" id="21">a simulation node if {a mathematical formula} Θ nbase([h] ⌣ )=0.</a>
<a href="#22" id="22">The second refinement is features-to-action sampling technique (FAST).</a>
<a href="#23" id="23">This uses the {a mathematical formula}TD( Λ ) temporal difference learning algorithm to learn a value function for actions, both offline before the search begins and online based on the MCTS playouts.</a>
<a href="#24" id="24">In the ICARUS framework, the values learned offline can be encoded in the initial information function {a mathematical formula} Θ initial, and the online learning by embedding {a mathematical formula}TD( Λ ) in the backpropagation function Ω .</a>
<a href="#25" id="25">NAST is defined in Specification 5.</a>
<a href="#26" id="26">Each record is an n-gram, i.e.</a>
<a href="#27" id="27">a sequence of n moves (NAST-1).</a>
<a href="#28" id="28">Note that n is a parameter here; Specification 5 defines a family of enhancements for {a mathematical formula}n=1,2,3, … .</a>
<a href="#29" id="29">The information associated with a record is the total reward and number of visits (NAST-2, NAST-3).</a>
<a href="#30" id="30">The policy uses these to select actions according to UCB1 (NAST-4).</a>
<a href="#31" id="31">Backpropagation updates the records associated with each sequence of n moves in the playout (NAST-6), with the player who played the last move in the sequence as contextual information (NAST-5).</a>
<a href="#32" id="32">The total reward and number of visits are updated in the natural way (NAST-7).</a>
<a href="#33" id="33">Note that NAST with {a mathematical formula}n=1 is equivalent to MAST (Section 4.3.2) with the UCB1 policy.</a>
<a href="#34" id="34">Chaslot et al.</a>
<a href="#35" id="35">[52] introduce progressive bias and progressive unpruning, which use a heuristic value function to bias selection and restrict expansion respectively.</a>
<a href="#36" id="36">In the ICARUS framework this can be achieved by encoding the heuristic in the initial information function {a mathematical formula} Θ initial and modifying the policy function Α appropriately.</a>
<a href="#37" id="37">The second way of combining enhancements is linear combination.</a>
<a href="#38" id="38">For two ICARUSes {a mathematical formula}I1 and {a mathematical formula}I2 as above, and a function {a mathematical formula} Λ : Θ base → [0,1] (the mixing coefficient, which is a function of the information for the baseline ICARUS as defined in Specification 1), the combination {a mathematical formula} Λ I1+(1 − Λ )I2 is defined as in Specification 6 with the exception of the policy function:{a mathematical formula} We can generalise this to define any convex combination of two or more enhancements in the natural way.</a>
<a href="#39" id="39">To define the episode function for EPIC, we set {a mathematical formula}E={D,L1,L2,L3,L4} and{a mathematical formula} The first episode in a game of Hearts encompasses the dealing and card passing stages; subsequent episodes are single tricks.</a>
<a href="#40" id="40">Here EPIC makes use of the fact that similar tricks may appear in many different places in the search tree.</a>
</body>
</html>