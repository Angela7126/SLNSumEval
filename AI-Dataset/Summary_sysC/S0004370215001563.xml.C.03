<html>
<head>
<meta name="TextLength" content="SENT_NUM:29, WORD_NUM:820">
</head>
<body bgcolor="white">
<a href="#0" id="0">Planning domainA planning domain is a tuple {a mathematical formula}D= 〈 P,A, Τ 〉 , where:</a>
<a href="#1" id="1">Agent planning programAn agent planning program is a tuple {a mathematical formula}P= 〈 P,V,v0, Δ 〉 , where:</a>
<a href="#2" id="2">{a mathematical formula}v0 ∈ V is the program initial state of {a mathematical formula}P; and</a>
<a href="#3" id="3">{a mathematical formula} 〈 v0,s0 〉 ∈ R; and</a>
<a href="#4" id="4">A planning program {a mathematical formula}P is said to be realizable in a planning domain {a mathematical formula}D if there exists a realization of {a mathematical formula}P in {a mathematical formula}D from {a mathematical formula}D's initial state.</a>
<a href="#5" id="5">Let us formalize this planning domain {a mathematical formula}D= 〈 P,A, Τ 〉 , as follows:</a>
<a href="#6" id="6">Finally, notice that when {a mathematical formula}Rain is true, the program transition {a mathematical formula} 〈 v0,v2 〉 is not executable, as {a mathematical formula}¬Rain is a guard for that transition, in fact simplifying the realization of the planning program.</a>
<a href="#7" id="7">We now show how to compute a realization of an agent planning program {a mathematical formula}P in a dynamic domain {a mathematical formula}D from an initial state {a mathematical formula}s0, by reduction to synthesis for a 2GS with an LTL weak-fairness formula as winning condition.</a>
<a href="#8" id="8">We assume that the planning domain {a mathematical formula}D starts in the initial state {a mathematical formula}s0.</a>
<a href="#9" id="9">{a mathematical formula}XD=P, containing the propositions of the planning domain {a mathematical formula}D;</a>
<a href="#10" id="10">{a mathematical formula}XV=V, containing the states of the planning program {a mathematical formula}P;</a>
<a href="#11" id="11">That is, {a mathematical formula} Σ v states that {a mathematical formula}P is in state v; and</a>
<a href="#12" id="12">That is, {a mathematical formula}reqv states that (at least) one transition among those available in the state v of the planning program is currently requested.</a>
<a href="#13" id="13">{a mathematical formula}init → ◯ v0, which encodes that the planning program is initially in its initial state.</a>
<a href="#14" id="14">{a mathematical formula} ⋀ v ∈ XV ◯ [v → reqv], which encodes that at least one transition available in the state the planning program moves to must be requested next.</a>
<a href="#15" id="15">{a mathematical formula} ⋀ 〈 v, 〈 Γ , Ψ , Φ 〉 ,v ′ 〉 ∈ Δ [req Γ : Ψ , Φ v,v ′ ∧ last → ◯ v ′ ], capturing that if transition {a mathematical formula}v → Γ : Ψ , Φ v ′ is currently requested and the last action performed has completed the current HT-plan, then the planning program moves to {a mathematical formula}v ′ .</a>
<a href="#16" id="16">A deterministic planning domain[46] is a special case of planning domain {a mathematical formula}D= 〈 P,A, Τ 〉 , where the transition relation Τ is a function {a mathematical formula} Τ :2P×A ↦ 2P.</a>
<a href="#17" id="17">2 shows the pseudo-code of {a mathematical formula}RealizePlanProg, an algorithm for building planning program realizations.</a>
<a href="#18" id="18">The frontier of this visit is the set of pairs {a mathematical formula} 〈 s,v 〉 , of domain and planning program state, such that there is a transition d outgoing from v whose guard holds in s, but for which there is no plan achieving and maintaining the corresponding goal, i.e., {a mathematical formula} Ω (s,d) is undefined.</a>
<a href="#19" id="19">In this case, the planning problem derived to realize such an outgoing transition would be unsolvable, and therefore algorithm {a mathematical formula}RealizePlanProg would backtrack.</a>
<a href="#20" id="20">Let {a mathematical formula} Π k be the planning problem associated to the k-th realization of d with initial state {a mathematical formula}sk ({a mathematical formula}k ∈ {1, … ,n}), and {a mathematical formula} Π k the solution plan computed for {a mathematical formula} Π k so that {a mathematical formula} Ω (sk,d)= Π k.</a>
<a href="#21" id="21">Suppose now that, at the current iteration of loop 5 – 25 of {a mathematical formula}RealizePlanProg, transition d is processed again with an open pair {a mathematical formula} 〈 s ′ ,v 〉 and planning problem {a mathematical formula} Π ′ .</a>
<a href="#22" id="22">Overall, we constructed 1223 planning programs with a randomly generated initial state and {a mathematical formula}| Δ | problem goal sets.</a>
<a href="#23" id="23">For domains Logistics and Storage, 40 planning programs obtained by the programs of benchmark {a mathematical formula}SM50 by adding maintenance goals to the program transitions;</a>
<a href="#24" id="24">12 shows the performance of {a mathematical formula}RealizePlanProg for the described planning programs.</a>
<a href="#25" id="25">the (current) planning program state {a mathematical formula}W[v] ∈ W ∩ V, which always exists and is unique due to definition of {a mathematical formula} Ρ e (recall that {a mathematical formula}XV=V);</a>
<a href="#26" id="26">We will say that W represents domain state {a mathematical formula}W[s], planning program state {a mathematical formula}W[v], transition {a mathematical formula}W[d], and action {a mathematical formula}W[a] (or, alternatively, that these are represented in W).</a>
<a href="#27" id="27">In other words a new request in domain state s and agent planning program state v has just been initiated (in game state {a mathematical formula}Wi+1).</a>
<a href="#28" id="28">Moreover, plan {a mathematical formula} Π ′ maintains the maintenance goal Ψ of Π and {a mathematical formula} Π ′ , because Π maintains Ψ and no action in {a mathematical formula}AP ∪ AT can make it false.</a>
</body>
</html>