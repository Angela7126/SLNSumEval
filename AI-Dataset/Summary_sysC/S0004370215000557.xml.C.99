<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:490">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using model-based reinforcement learning from human reward, this article investigates the problem of learning from human reward through six experiments, focusing on the relationships between reward positivity, which is how generally positive a trainer's reward values are; temporal discounting, the extent to which future reward is discounted in value; episodicity, whether task learning occurs in discrete learning episodes instead of one continuing session; and task performance, the agent's performance on the task the trainer intends to teach.</a>
<a href="#1" id="1">The task objective, subjectively defined here, is to navigate to the trainer; a potential learning objective is to find a behavioral policy that maximizes the expectation of the sum of future human reward.</a>
<a href="#2" id="2">As we point out in Section 7.1.2, the interaction between the example learning objective and reward strategy will typically not lead the agent to perform well with respect to the task objective in certain settings.</a>
<a href="#3" id="3">Second, to account for delays in giving feedback, the causal attribution of each reward is distributed across multiple recent time steps by tamer's credit assignment module [11], further adding variety to the label values of samples for training {a mathematical formula}R ˆ H.</a>
<a href="#4" id="4">Unlike with the mountain-car task, these experiments are on-discounting, involving an agent being trained with the same discount factor by which it calculates return when learning a value function (and thus a policy).</a>
<a href="#5" id="5">In other words, the agent's behavior during training reflects its discount factor, removing the bias inherent in the mountain-car task.</a>
<a href="#6" id="6">Additionally, these results provide initial evidence of the presence of positive circuits: compared with agents optimizing for low Γ values, agents optimizing for high Γ values learn policies that accrue more human reward but do not reach the goal, and a sampling of the learned policies for {a mathematical formula} Γ =0 showed consistently circuitous behavior.</a>
<a href="#7" id="7">At {a mathematical formula} Γ =1, if the RL algorithm learns an infinitely repeatable sequence of actions with positive net reward, then the disastrous policy that repeats that sequence is necessarily within the set of MDP-optimal policies.</a>
<a href="#8" id="8">As mentioned above, we visually checked the behavior of five human models' corresponding algorithms while they exhibited the worst possible performance, and each agent repeatedly visited roughly the same circuit of states until the episode limit was reached.</a>
<a href="#9" id="9">We do not directly investigate the effects of giving positive reward from absorbing state, but the strategy in the following section — making the task appear continuing — is equivalent to setting the absorbing-state reward such that its resulting return is equal to the agent's discounted expectation of return from the distribution of starting states, assuming the value function is accurate for the policy it assesses.</a>
<a href="#10" id="10">From analyzing these results, we believe that adding the failure state affected the ease of training in both positive and negative ways.</a>
<a href="#11" id="11">As an alternate absorbing state to the goal, the failure state generally forces trainers to give more discriminating reward (e.g., the arbitrarily all-negative strategy for the episodic version becomes unsuccessful).</a>
</body>
</html>