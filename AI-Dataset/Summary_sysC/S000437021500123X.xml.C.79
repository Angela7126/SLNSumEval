<html>
<head>
<meta name="TextLength" content="SENT_NUM:23, WORD_NUM:777">
</head>
<body bgcolor="white">
<a href="#0" id="0">By combining the knowledge layers with the models of knowledge effects, we can simultaneously solve several problems in robotics: (i) task planning and execution under uncertainty; (ii) task planning and execution in open worlds; (iii) explaining task failure; (iv) verifying those explanations.</a>
<a href="#1" id="1">These knowledge-modifying (or epistemic) actions also include the ability to make assumptions about the existence of specific object instances — for example, assuming that there is a dining room in a particular house.</a>
<a href="#2" id="2">We pose it as the problem of finding an additional set of assumptions that change the expected outcomes to make them consistent with the actual observations.</a>
<a href="#3" id="3">Typically only the assumptive actions have a non-zero probability of failure, and so in most of our examples {a mathematical formula} ∏ ai ∈ Π Ρ (ai) is precisely the probability of a particular world in which the physical part of the linear plan succeeds.</a>
<a href="#4" id="4">As part of this, it makes a sequence of assumptions to select a possible world in which the subsequent physical actions will succeed.</a>
<a href="#5" id="5">If we modelled sensing in our object search operator in the same way as in the move operator, the robot might end up looking in the wrong place repeatedly, because the knowledge effect asserts that executing the action will result in knowing the object's location unconditionally.</a>
<a href="#6" id="6">At the same time, plans that were valid before this transformation remain so: because a fact {a mathematical formula}(v=x) implies {a mathematical formula}K(v) and {a mathematical formula}A(v,x), the new action is applicable whenever the original action was.</a>
<a href="#7" id="7">The probabilities are filled in at planning time with values obtained from the robot's default knowledge:</a>
<a href="#8" id="8">To this minimal set, we add a limited number of uncertain variables, from the full domain, which (a) are observable by the robot, and (b) if known help to determine whether the minimal relevant assumptions hold.</a>
<a href="#9" id="9">The first condition simply states that when the original plan sequence is executed, given the explanatory assumptions, the resulting state must contain the surprising observation that we want to explain.</a>
<a href="#10" id="10">Both plans start with three assumptive actions, first establishing the category of a room, then assuming the existence of the type of target object in that room, and finally placing a virtual object there, so that there is a concrete instance of “ magazine ” that the planner can reason about.</a>
<a href="#11" id="11">To these we add a set of observation actions{a mathematical formula}oi, which are used to check if the simulated execution results in the same state sequence as that originally experienced by the robot.</a>
<a href="#12" id="12">Thus, the planning problem has a solution only if the planner can find a sequence of assumptions that allow all the original physical actions to be executed in sequence, and which results in expected and actual observations {a mathematical formula}si matching.</a>
<a href="#13" id="13">In the second case, by our allowing instance assumptions based on default knowledge, explanations posit the existence of new places or objects — such as the existence of a new room the object might be in.</a>
<a href="#14" id="14">The diagnostic layer contains two types of action models that achieve this: diagnostic actions and default assumptions.</a>
<a href="#15" id="15">Diagnostic actions are based on the default layer's models of physical actions, augmented with additional possible outcomes and their causes.</a>
<a href="#16" id="16">Each potential view point is then evaluated on the basis of the probability of the target object being within the view cone defined by the view point, taking into account the field of view of the camera.</a>
<a href="#17" id="17">This experiment shows the ability of the system to use a mix of default assumptions and instance assumptions to create explanations for task failure.</a>
<a href="#18" id="18">This shows the ability to plan again with instance assumptions in order to verify an explanation that involves hypothesized new default knowledge.</a>
<a href="#19" id="19">To test the explanation performance of our system, we analyzed the generated explanations for three perception failure runs of the find test condition outlined before and an additional seven runs under a specifically designed explain test condition, where the object was hidden on a shelf in the meeting room.</a>
<a href="#20" id="20">For instance, in run 9 (condition {a mathematical formula}Cc, object in meeting room), where a place-holder was eradicated owing to a navigation problem, the system could not understand how to reach the meeting room, and so correctly concluded that the object must be outside the known and accessible environment.</a>
<a href="#21" id="21">But unlike the current paper, neither of these approaches explicitly reason about the fact that sensing actions can increase the size of the planning domain, nor the benefits of this for goal achievement.</a>
<a href="#22" id="22">[29] give a POMDP formulation of active visual mapping, use direct policy search to find a solution, and use Monte Carlo simulation to generate imaginary observations and action outcomes during optimization.</a>
</body>
</html>