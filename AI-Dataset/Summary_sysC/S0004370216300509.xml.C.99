<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:174">
</head>
<body bgcolor="white">
<a href="#0" id="0">We also develop theoretical analysis that extends the sample complexity result of [29] for dictionary learning using standard sparse coding to the smooth sparse coding case.</a>
<a href="#1" id="1">The smooth sparse coding setting leads to codes that represent a neighborhood rather than an individual sample and that have lower mean square reconstruction error (with respect to a given dictionary), due to lower estimation variance (see for example the standard theory of smoothed empirical process [7]).</a>
<a href="#2" id="2">The main drawback of that approach is that one needs to fit all the data points simultaneously whereas in smooth sparse coding, the coefficient learning step decomposes as n separate problems and this provides a computational advantage.</a>
<a href="#3" id="3">We compare the proposed smooth sparse coding algorithm, standard sparse coding with lasso [17] and marginal regression updates respectively, with a relative reconstruction error {a mathematical formula} ‖ X − D ˆ B ˆ ‖ F/ ‖ X ‖ F convergence criterion.</a>
<a href="#4" id="4">Previous approaches include sparse coding, vector quantization, and k-means on top of the HoG-3d feature set (see [31] for a comprehensive evaluation).</a>
</body>
</html>