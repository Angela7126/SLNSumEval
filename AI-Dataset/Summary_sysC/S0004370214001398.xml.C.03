<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:386">
</head>
<body bgcolor="white">
<a href="#0" id="0">We present a novel framework of anticipatory action selection for human – robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty.</a>
<a href="#1" id="1">Specifically, the robot chooses an action from a repertoire of motor skills based on the prediction of the human partner's intention.</a>
<a href="#2" id="2">However, waiting for a confident prediction causes delayed action selection and reduces the available time for the robot to execute its action [53].</a>
<a href="#3" id="3">The anticipatory action selection decides when it is time to initiate the hitting movement.</a>
<a href="#4" id="4">We propose an original formulation of the action selection problem in robot table tennis as POMDPs, wherein a Monte-Carlo planning algorithm that makes use of the IDDM was presented to address the trade-off between prediction accuracy and reaction delay.</a>
<a href="#5" id="5">For example in the case of human – robot table tennis, the robot's hitting point can usually be precisely predicted 120 ms after the human player hits the ball [53], and, thus, an optimal policy would stop waiting and initiate a hitting movement.</a>
<a href="#6" id="6">The value function {a mathematical formula}Q Π ( Θ ,a) measures the expected future reward when taking action a according to the current belief Θ and following the policy Π thereafter.</a>
<a href="#7" id="7">The optimal policy {a mathematical formula} Π ′ greedily chooses the action that maximizes the corresponding value function {a mathematical formula}Q ˆ Π .</a>
<a href="#8" id="8">As a stopping action terminates the decision process immediately, the value function for the stopping actions {a mathematical formula} ∀ a ∈ A ∖ {0}:Q Π ( Θ ,a)=Q( Θ ,a) holds for any belief state Θ and any policy Π .</a>
<a href="#9" id="9">We evaluated the LSPI and the MCP algorithms for anticipatory action selection in human – robot table tennis setup.</a>
<a href="#10" id="10">Hence, the action set {a mathematical formula}A consisted of one waiting action and three stopping actions, each stopping action corresponding to a type hitting movement.</a>
<a href="#11" id="11">However, returning the ball outside the corresponding hitting region is difficult once the robot has initiated the chosen action [53].</a>
<a href="#12" id="12">Therefore, we enforce that the robot takes a stopping action when the robot's hitting point can be reliably predicted by the vision system, which typically happens within 120 ms after the human player returns the ball [53].</a>
</body>
</html>