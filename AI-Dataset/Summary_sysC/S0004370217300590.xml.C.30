<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:188">
</head>
<body bgcolor="white">
<a href="#0" id="0">This section compares 8 algorithms on learning low-rank kernel: (1) LibSVM [46]: support vector machine, which uses only the small number of labeled samples for training; (2) Nyström: standard Nyström method; (3) CSI: Cholesky with Side Information [2]; (4) Cluster: cluster kernel [47]; (5) Spectral: non-parametric spectral graph kernel [17]; (6) Breg: low-rank kernel learning with Bregman divergence [37]; (7) NCNM [44]: Sparse Gaussian Process for Semi-supervised Learning; and (8) Our proposed method.</a>
<a href="#1" id="1">Most algorithms can learn the {a mathematical formula}n×n low-rank kernel{sup:2} matrix on labeled and unlabeled samples{sup:3} in the form of {a mathematical formula}K=GG ⊤ , which can then be fed into an SVM for classification.</a>
<a href="#2" id="2">The resultant problem will be a linear SVM using G as training/testing samples [34].</a>
<a href="#3" id="3">Method (1) and (2) are baseline methods that do not involve any kernel learning, therefore they are computationally very efficient.</a>
<a href="#4" id="4">2, the computing time of other multiple kernel learning algorithms roughly increases linearly with the number of labeled samples.</a>
<a href="#5" id="5">The computing time of our proposed method do not increase because we fixed the number of landmark points (i.e., the rank of the learned kernel) at 100.</a>
</body>
</html>