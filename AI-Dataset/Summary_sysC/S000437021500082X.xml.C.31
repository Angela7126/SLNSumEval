<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:283">
</head>
<body bgcolor="white">
<a href="#0" id="0">The TiMRLA Value-Addition algorithm described in [31] was executed using the software provided by the authors (available in [57]).</a>
<a href="#1" id="1">The case base used in L3-SARSA( Λ ) was the one that contains N cases selected by random sampling the state set and finding the best action for that state, because this is the case base that produced the best results for the transfer.</a>
<a href="#2" id="2">{sup:6} The parameters used in the experiments were the same over all trials: the learning rate {a mathematical formula} Α =0.2, the exploration/exploitation rate {a mathematical formula} Ε =0.0, {a mathematical formula} Λ =0.95, {a mathematical formula} Γ =1.0.</a>
<a href="#3" id="3">The parameter used to create the heuristics in L3 is {a mathematical formula} Η =1 and the parameter that weights the influence of the heuristic {a mathematical formula} Ξ =1, with a decay of {a mathematical formula}10 − 4 at the end of each episode.</a>
<a href="#4" id="4">To the best of our knowledge, L3 is the only class of TL algorithms for Reinforcement Learning to date that uses the knowledge obtained in one domain as heuristics in another.</a>
<a href="#5" id="5">This characteristic makes L3 robust to negative transfers: if the cases acquired in the source domain are not useful in the target domain, assuming them as heuristics will not speed up the learning procedure but, in the worst case (when every case in the case base is not applicable to the target domain), L3 will be as efficient as the original RL algorithm that it is based.</a>
<a href="#6" id="6">In other words, if the case base contains no useful (or even misleading) information for the target domain, the agent is still able to learn the optimal policy for the domain using the RL component of the algorithm.</a>
</body>
</html>