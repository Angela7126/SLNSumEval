<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:310">
</head>
<body bgcolor="white">
<a href="#0" id="0">To construct an EPM for an algorithm {a mathematical formula}A with configuration space Θ on an instance set Π , we run {a mathematical formula}A on various combinations of configurations {a mathematical formula} Θ i ∈ Θ and instances {a mathematical formula} Π i ∈ Π , and record the resulting performance values {a mathematical formula}yi.</a>
<a href="#1" id="1">We record the k-dimensional parameter configuration {a mathematical formula} Θ i and the m-dimensional feature vector {a mathematical formula}zi of the instance used in the ith run, and combine them to form a {a mathematical formula}p=k+m-dimensional vector of predictor variables{a mathematical formula}xi=[ Θ iT,ziT]T. The training data for our regression models is then simply {a mathematical formula}{(x1,y1), … ,(xn,yn)}.</a>
<a href="#2" id="2">We now examine the most interesting case, where test instances and configurations were both previously unseen.</a>
<a href="#3" id="3">Table 7 provides quantitative results of model performance based on {a mathematical formula}n=10000 training data points, and Fig.</a>
<a href="#4" id="4">8 visualizes performance.</a>
<a href="#5" id="5">Overall, we note that the best models generalized to new configurations and to new instances almost as well as to either alone (compare to Sections 6 and 7, respectively).</a>
<a href="#6" id="6">On the most heterogeneous data set, CPLEX-BIGMIX, we once again witnessed extremely poorly predicted outliers for the ridge regression variants, but in all other cases, the models captured the large spread in runtimes (above 5 orders of magnitude) quite well.</a>
<a href="#7" id="7">As in the experiments in Section 6.3, the tree-based approaches, which are able to model different regions of the input space independently, performed best on the most heterogeneous data sets.</a>
<a href="#8" id="8">Fig.</a>
<a href="#9" id="9">8 also shows some qualitative differences in predictions: for example, ridge regression, neural networks, and projected processes sometimes overpredicted the runtime of the shortest runs, while the tree-based methods did not have this problem.</a>
<a href="#10" id="10">Random forests performed best in all cases, which is consistent with their robust predictions in both the instance and the configuration space observed earlier.</a>
</body>
</html>