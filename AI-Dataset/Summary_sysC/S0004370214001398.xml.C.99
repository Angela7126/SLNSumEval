<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:397">
</head>
<body bgcolor="white">
<a href="#0" id="0">We present a novel framework of anticipatory action selection for human – robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty.</a>
<a href="#1" id="1">Furthermore, the outcome of executing a selected action, e.g., the robot's success of returning the ball to the opponent's court with the chosen hitting movement, is not deterministic as the underlying dynamics of the robot arm are often too complicated to be modeled precisely at high speed.</a>
<a href="#2" id="2">In the policy improvement step, we improve the policy Π by a new policy {a mathematical formula} Π ′ that maximizes the expected reward according to the estimated value function {a mathematical formula}Q ˆ Π .</a>
<a href="#3" id="3">The optimal policy {a mathematical formula} Π ′ greedily chooses the action that maximizes the corresponding value function {a mathematical formula}Q ˆ Π .</a>
<a href="#4" id="4">Therefore, we obtain an improved policy{a mathematical formula} We can obtain the optimal policy by iteratively executing the policy evaluation and improvement steps, as summarized in Algorithm 1.</a>
<a href="#5" id="5">The LSPI algorithm as described above employs a model-free approach to policy learning, using the Intention-Driven Dynamics Model as a black box for updating belief {a mathematical formula} Θ t given a history of observations {a mathematical formula}z1:t. Besides the capability of belief update, the IDDM in fact provides a transition model in state space, estimated from its training data, which has not been exploited by the LSPI algorithm.</a>
<a href="#6" id="6">We, hence, reuse the estimated value function for stopping actions by LSPI, and focus on the value function for the waiting action {a mathematical formula}Q( Θ t,at=waiting), given by the expected future reward{a mathematical formula} with respect to the subsequent belief state {a mathematical formula} Θ t+1.</a>
<a href="#7" id="7">The value function {a mathematical formula}Q( Θ t,at=waiting) measures the expected reward if the agent chooses to wait for more observation.</a>
<a href="#8" id="8">Nevertheless, consider the online planning that finds the optimal action{a mathematical formula} where one only needs to know if the waiting action leads to higher expected reward than all the stopping actions, rather than the actual expected reward of waiting.</a>
<a href="#9" id="9">We can terminate the particle projection routine if the expected reward of waiting is very likely to be higher or lower than that of the optimal stopping action {a mathematical formula}maxa ≠ 0 ⁡ Q( Θ t,a).</a>
</body>
</html>