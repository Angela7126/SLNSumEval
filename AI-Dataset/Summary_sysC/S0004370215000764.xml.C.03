<html>
<head>
<meta name="TextLength" content="SENT_NUM:18, WORD_NUM:495">
</head>
<body bgcolor="white">
<a href="#0" id="0">In such cases, intrinsic motivation can be used to enable the agent to learn a useful model of the environment that is likely to help it learn its eventual tasks more efficiently.</a>
<a href="#1" id="1">This article presents experiments demonstrating that the combination of these two intrinsic rewards enables the algorithm to learn an accurate model of a domain with no external rewards and that the learned model can be used afterward to perform tasks in the domain.</a>
<a href="#2" id="2">In addition, the experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone.</a>
<a href="#3" id="3">In such cases, intrinsic motivation can be used to enable the agent to learn a useful model of its dynamics and the environment that can help it learn its eventual tasks more efficiently.</a>
<a href="#4" id="4">texplore-vanir combines model learning through the use of random forests with two unique intrinsic rewards calculated from this model.</a>
<a href="#5" id="5">The first reward is based on variance in its models' predictions to drive the agent to explore where its model is uncertain.</a>
<a href="#6" id="6">The combination of these two rewards enables the agent to explore in a developing curious way, learn progressively more complex skills, and learn a useful model of the domain very efficiently.</a>
<a href="#7" id="7">Section 4 presents the texplore-vanir algorithm, including its approach to model learning and how its intrinsic rewards are calculated.</a>
<a href="#8" id="8">In addition, it shows that the agent can use the intrinsic rewards in conjunction with external rewards to learn a task faster than if using external rewards alone.</a>
<a href="#9" id="9">The agent can plan a policy to reach intrinsic rewards added into its model to drive exploration to interesting state-actions.</a>
<a href="#10" id="10">When learning with a tabular model, the agent must gain enough experiences in each state-action to learn an accurate model of it.</a>
<a href="#11" id="11">pgrd uses its knowledge of the true reward function to calculate the gradient of intrinsic rewards to agent return.</a>
<a href="#12" id="12">This agent should use intrinsic rewards to 1) efficiently learn a useful model of the domain's transition dynamics; and 2) explore in a developing curious way.</a>
<a href="#13" id="13">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#14" id="14">These intrinsic rewards are combined with the same texplore model learning approach as the other methods.</a>
<a href="#15" id="15">Not only should the agent's intrinsic rewards be useful when learning in task without external rewards, they should also make an agent in a domain with external rewards learn more efficiently.</a>
<a href="#16" id="16">Finally, in a task with external rewards, texplore-vanir can use its intrinsic rewards to speed up learning with respect to an algorithm using only external rewards.</a>
<a href="#17" id="17">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
</body>
</html>