<html>
<head>
<meta name="TextLength" content="SENT_NUM:30, WORD_NUM:838">
</head>
<body bgcolor="white">
<a href="#0" id="0">A move history for player i from state s is a sequence of moves from player i's point of view,{a mathematical formula} 〈 [a1] ⌣ i, … ,[an] ⌣ i 〉 ∈ Mi ⁎ , where{a mathematical formula} 〈 a1, … ,an 〉 is an action history from s. Denote the set of all move histories for player i from s by{a mathematical formula}Hi ⌣ (s), and the set of all move histories for all players by{a mathematical formula}H ⌣ (s).</a>
<a href="#1" id="1">An Information Capture And ReUse Strategy (ICARUS) is an enhancement to MCTS that collects information from visits to one area of the game tree and uses that information to inform the future policy in other areas.</a>
<a href="#2" id="2">Given a game as defined in Section3, an information capture and reuse strategy (ICARUS) is a 7-tuple{a mathematical formula}(R, Θ , Θ initial, Α , Ψ , Ξ , Ω )where</a>
<a href="#3" id="3">{a mathematical formula} Θ initial:R → Θ is the initial information function, which maps each record to a piece of information.</a>
<a href="#4" id="4">Algorithm 1 shows an MCTS algorithm using ICARUS to choose the best action from an information set {a mathematical formula}Iroot.</a>
<a href="#5" id="5">The algorithm begins by initialising the information associated with each record (lines 2 – 4); however, a practical implementation would initialise these values lazily as and when they are needed.</a>
<a href="#6" id="6">Each iteration begins at the root node corresponding to the empty history (line 7), and samples a determinization (state) {a mathematical formula}sroot from the root information set (line 8) which becomes the current state s for this iteration (line 9).</a>
<a href="#7" id="7">Each step of the playout uses the policy function Α to choose an action a, depending on the current move history {a mathematical formula}[h] ⌣ Ρ (s) for the player about to act from state s, the current information mapping Θ , and the set of available actions {a mathematical formula}A(s) (line 11).</a>
<a href="#8" id="8">The current history h is updated by appending a, and the current state s is updated by applying a.</a>
<a href="#9" id="9">The experimental domains in this paper are games of imperfect information, thus Algorithm 1 is designed to handle imperfect information using the approach of Information Set MCTS [30].</a>
<a href="#10" id="10">However it is equally applicable to games of perfect information.</a>
<a href="#11" id="11">In this case the information set {a mathematical formula}Iroot is a singleton {a mathematical formula}{sroot} and line 8 can be omitted.</a>
<a href="#12" id="12">This can be defined by modifying the capture function of Specification 3:{a mathematical formula}</a>
<a href="#13" id="13">This uses the {a mathematical formula}TD( Λ ) temporal difference learning algorithm to learn a value function for actions, both offline before the search begins and online based on the MCTS playouts.</a>
<a href="#14" id="14">In the ICARUS framework, the values learned offline can be encoded in the initial information function {a mathematical formula} Θ initial, and the online learning by embedding {a mathematical formula}TD( Λ ) in the backpropagation function Ω .</a>
<a href="#15" id="15">In [50] we show that NAST works for the three imperfect information games studied in the present paper (Section 7), with {a mathematical formula}n=2 typically giving the strongest performance.</a>
<a href="#16" id="16">Each enhancement maintains its own records and information; the policy functions are combined so that {a mathematical formula}I1 ▹ I2 uses the policy from {a mathematical formula}I1 during selection and expansion, and the policy from {a mathematical formula}I2 during simulation.</a>
<a href="#17" id="17">Selection and expansion nodes are defined in Section 4.2.</a>
<a href="#18" id="18">The second way of combining enhancements is linear combination.</a>
<a href="#19" id="19">For two ICARUSes {a mathematical formula}I1 and {a mathematical formula}I2 as above, and a function {a mathematical formula} Λ : Θ base → [0,1] (the mixing coefficient, which is a function of the information for the baseline ICARUS as defined in Specification 1), the combination {a mathematical formula} Λ I1+(1 − Λ )I2 is defined as in Specification 6 with the exception of the policy function:{a mathematical formula} We can generalise this to define any convex combination of two or more enhancements in the natural way.</a>
<a href="#20" id="20">For the baseline ICARUS (Specification 1) applied to a game of perfect information, we have the following two results:</a>
<a href="#21" id="21">1 have the same position-in-episode.</a>
<a href="#22" id="22">The position-in-episode of a move history is defined similarly to that of a state.</a>
<a href="#23" id="23">Positions-in-episode are always defined relative to the initial state {a mathematical formula}s0 of the game, regardless of the current state.</a>
<a href="#24" id="24">2 illustrates enhancements which exploit a particular type of correlation between states in a game, but any other type of correlation can be encoded as an information capture enhancement within the ICARUS framework.</a>
<a href="#25" id="25">For EPIC's episode function, we set {a mathematical formula}E={M1,M2} and{a mathematical formula} An episode begins when a player moves a piece.</a>
<a href="#26" id="26">Checkers, or Draughts, is a two-player game of perfect information, played on an {a mathematical formula}8×8 board with 12 pieces per player.</a>
<a href="#27" id="27">For a Κ -player game, we play one instance of the ICARUS combination in question against {a mathematical formula} Κ − 1 instances of the baseline player (unenhanced MO-ISMCTS).</a>
<a href="#28" id="28">EPIC provides a way of refining this context in a game-specific way.</a>
<a href="#29" id="29">To determine whether this refinement is helpful, we tested EPIC against NAST with {a mathematical formula}n=2.</a>
</body>
</html>