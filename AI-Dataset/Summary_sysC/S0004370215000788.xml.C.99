<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:944">
</head>
<body bgcolor="white">
<a href="#0" id="0">Although we use sequential auction design to illustrate our method, all of our constructions are general and can be applied to any optimization setting where unknown relations can be represented using regression models that have been learned from data.</a>
<a href="#1" id="1">In Section 2, we formally introduce the problem of optimal ordering for sequential auctions (OOSA), and then we show how to learn regression models from historical auction data in Section 3 using standard machine learning methods.</a>
<a href="#2" id="2">We then sum these up to obtain the overall objective function, i.e., the expected revenue {a mathematical formula}P(S) given a set S ({a mathematical formula}|S|=n) of items:{a mathematical formula} where {a mathematical formula}G(sk,J,L) is a regression function that determines the expected value of {a mathematical formula}sk given that J was auctioned before and L will be auctioned afterwards.</a>
<a href="#3" id="3">We use LASSO regression because more zero coefficients implies we need to compute less feature values in order to evaluate the learned model, which has a positive effect on the optimization performance that we will discuss in Section 4.</a>
<a href="#4" id="4">In addition, since the regression function G uses other values in a (proposed) solution as input ({a mathematical formula}J,L) instead of only external parameters/data, a learned regression model represents unknown relations between the different values in a solution.</a>
<a href="#5" id="5">The model thus answers the question “ What is the value of X given that we do Y? ” , as opposed to “ What is the value of X? ” that is answered by fitting only model parameters.</a>
<a href="#6" id="6">Given regression tree and linear regression models for the expected value per item type, we automatically formulate the problem of finding an optimal ordering as a mixed integer linear program (MIP).</a>
<a href="#7" id="7">We can directly compute the value of the p variables using the linear predictor function:{a mathematical formula} where Feat is the set of all features, {a mathematical formula}fvf,i is feature f's values at index i, and {a mathematical formula}cf,r is the constant coefficient for feature f in the regression function for type r. The only somewhat difficult part is that at every index, the used regression function can change depending on the auctioned item type r. We implemented this choice using indicator functions in CPLEX.</a>
<a href="#8" id="8">To evaluate the performance of the proposed optimization methods, ideally, we should collect real auction data, build the optimization models, run real-world auctions with real bidders using different ordering of items produced by different methods, and then compare the resulting revenues.</a>
<a href="#9" id="9">The optimization methods that we compare include the proposed white-box ILP model which finds a solution based on the abovementioned 6 regression models, the proposed black-box best-first search which evaluates a solution based on the 6 regression models, and in addition, two other simple ordering methods: (i) auctioning the most valuable item first (i.e., mvf), as suggested in [11]{sup:10}; and (ii) a random ordering strategy (i.e., mean5000), as seen in many real-world auctions for the purpose of fairness.</a>
<a href="#10" id="10">It is not feasible for us to compute the best solution given the problem size.</a>
<a href="#11" id="11">Thus, we obtain a lower bound on the optimal solution as follows: given a set of items, we generate 5000 random orderings, and we use the true model (i.e., the simulator) to evaluate them and pick the one returning the highest revenue.</a>
<a href="#12" id="12">In addition, we investigate the scalability of our approach using the following experiments: (1) first, we test the small instances to show how close the orderings found by the learned models are to the actual optimum; (2) second, we generate a randomized population of bidders to test whether our method can handle the cases where the bidders in different auction vary a lot; (3) third, we use a larger set of items and item types to demonstrate how well our approach can be expected to perform in larger auctions with many more items of greater diversity.</a>
<a href="#13" id="13">Moreover, the small difference in {a mathematical formula}R2 scores between trees with depth 5 and depth 8 also has a significant effect on the difference between their model and simulator evaluations (e.g., due to cascading errors).</a>
<a href="#14" id="14">The figure demonstrates that the linear regression based optimization methods return more reliable solutions, i.e., their solutions evaluated on the learned linear models are closer to the solution values returned by the simulator.</a>
<a href="#15" id="15">Given a large set of possible constraints L and some training data, the goal of constraint acquisition is to compose a constraint network (a graphical representation of a CP model, see, e.g., [43]) using the constraints in L such that it classifies all of the training data correctly, i.e., it should specify a language that includes all positive and excludes all negative training examples.</a>
<a href="#16" id="16">Interestingly, in this last work, the added constraints themselves were also learned from data and added as soft constraints to the MIP model.</a>
<a href="#17" id="17">As an auction ordering is essentially a sequence of items, our work is also related to the many machine learning approaches for sequence modeling.</a>
<a href="#18" id="18">Markov decision processes (MDPs) (see, e.g., [48]) may be closest to our auction setting, as they can directly model the expected price per item and come with methods that can be used to optimize the expected total reward (revenue).</a>
<a href="#19" id="19">In this case, a major hurdle will be to find a representation that results in Markovian states, which is needed to apply the dynamic programming methods.</a>
<a href="#20" id="20">Since the problem of deciding whether good auction ordering exists is NP-complete (Theorem 1), and these methods run in polynomial time, this is impossible without an exponentially large state space unless {a mathematical formula}P=NP.</a>
<a href="#21" id="21">Our method relies on solvers and search methods for NP-complete problems, making a polynomial state space possible, and therefore requiring much less data to estimate the model parameters.</a>
</body>
</html>