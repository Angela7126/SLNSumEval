<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:619">
</head>
<body bgcolor="white">
<a href="#0" id="0">On the other hand, the DRL-CA algorithm uses similar ideas to those of Bowling and Veloso [3], and Kaisers and Tuyls [19] for having a variable learning rate, but we are introducing direct cooperation between agents without using joint actions information and not increasing the memory consumption or the state space dimension.</a>
<a href="#1" id="1">Since from a computational point of view, all the individual agents in a DRL system can operate in parallel acting upon their individual and reduced action spaces, the learning speed is typically higher compared to a centralized agent which searches an exponentially larger action space {a mathematical formula}N=N1× ⋯ ×NM, as expressed in (2)[39].</a>
<a href="#2" id="2">On the other hand, Kimura [20] presents a coarse coding technique and an action selection scheme for RL in multi-dimensional and continuous state-action spaces following conventional and sound RL manners; and Pazis and Lagoudakis [38] present an approach for efficiently learning and acting in domains with continuous and/or multidimensional control variables, in which the problem of generalizing among actions is transformed to a problem of generalizing among states in an equivalent MDP, where action selection is trivial.</a>
<a href="#3" id="3">A DRL problem is fully decentralizable if not all the state information is relevant to all the agents, thus, individual state vectors can be defined for each agent.</a>
<a href="#4" id="4">If a system is not fully decentralizable, and it is necessary that all the agents observe the whole state information, the same state vector must be used for all the individual agents, and will be called a joint state vector.</a>
<a href="#5" id="5">However, if a system is fully decentralizable, the next stage is to determine which state variables are relevant to each individual agent.</a>
<a href="#6" id="6">Also, note that state space could be reduced for practical effects, {a mathematical formula}Agentx could eventually work without observing {a mathematical formula}y ˙ speed, as well as {a mathematical formula}Agenty without observing {a mathematical formula}x ˙ speed.</a>
<a href="#7" id="7">As already mentioned, differences are based on determining terminal states separately, resetting conditions, and establishing environment limitations, among other design settings, which can be different among agents and must be well set to coordinate the parallel learning procedure under the joint environmental conditions.</a>
<a href="#8" id="8">Adaptations of the Infinitesimal Gradient Ascent algorithm (IGA) [41] and the Win or Learn Fast (WoLF) principle [3]: not a trivial implementation in the case of more than two agents and non-competitives environments; however, a cooperative and variable learning rate is a promising approach.</a>
<a href="#9" id="9">The implementation of this scheme is presented in Algorithm 1, which depicts an episodic MA-SARSA algorithm for continuous states with Radial Basis Function (RBF) approximation [37], and Ε -greedy exploration [43], where a learning system is modeled with an {a mathematical formula}M − dimensional action space and M single SARSA learners acting in parallel.</a>
<a href="#10" id="10">Unlike the original WoLF-IGA, where gradient ascent is derived from the expected pay-off, or unlike the current utility function from the update rule [3], DRL-CA directly uses the probability of the selected actions, having a common normalized measure of partial quality of the policy performed per agent.</a>
<a href="#11" id="11">Additionally, in order to perform a fair comparison of computing time, we have also carried out a second evaluation, implementing and testing a DRL system with {a mathematical formula}Nvl=5 actions.</a>
<a href="#12" id="12">Stage3.5 — Completing RL single modelings the learning procedure is defined as follows: goal positions are defined in such a way that they are always reachable for the robot; thus, the learning process needs to develop an internal model of the inverse kinematics of the robot which is not directly injected by the designer; through the different trials, a model of the robot inverse kinematics is learned by the system; when a goal position is generated, the robot tries to reach it; each trial can finish as a success episode, i.e.</a>
</body>
</html>