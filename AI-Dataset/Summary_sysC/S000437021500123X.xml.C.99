<html>
<head>
<meta name="TextLength" content="SENT_NUM:29, WORD_NUM:781">
</head>
<body bgcolor="white">
<a href="#0" id="0">Decision-theoretic planning, by contrast, produces a policy maximizing the probability of success{sup:9} from any belief state the robot might reach during plan execution, taking account of the probabilities of every possible action outcome in those states.</a>
<a href="#1" id="1">Decision-theoretic planning thus handles multiple action outcomes (Markov decision processes, MDPs) and unreliable observations (POMDPs), but at an unfeasible computational cost given the size of typical robot domains.</a>
<a href="#2" id="2">We therefore developed a continual switching planner [11], which switches between a deterministic planner [18] and a POMDP solver, as required.</a>
<a href="#3" id="3">First, the deterministic planner creates a linear plan that achieves the goal.</a>
<a href="#4" id="4">As part of this, it makes a sequence of assumptions to select a possible world in which the subsequent physical actions will succeed.</a>
<a href="#5" id="5">Execution will continue (line 21) until the goal is achieved or the plan becomes invalid, at which point replanning occurs.</a>
<a href="#6" id="6">In the rest of this section, we detail the machinery that enables our planning approach to work in open and uncertain worlds, before showing how to use it to explain failures in Section 5.</a>
<a href="#7" id="7">If we modelled sensing in our object search operator in the same way as in the move operator, the robot might end up looking in the wrong place repeatedly, because the knowledge effect asserts that executing the action will result in knowing the object's location unconditionally.</a>
<a href="#8" id="8">To this minimal set, we add a limited number of uncertain variables, from the full domain, which (a) are observable by the robot, and (b) if known help to determine whether the minimal relevant assumptions hold.</a>
<a href="#9" id="9">Surprises in continual planningLet {a mathematical formula}P Π be a continual planning process.</a>
<a href="#10" id="10">Then the surprises{a mathematical formula}S of {a mathematical formula}P Π are defined as a sequence of sets of surprises, one for each planning cycle:{a mathematical formula}{a mathematical formula}</a>
<a href="#11" id="11">This definition of surprise is quite general: any observed, but unexpected fact is a surprise.</a>
<a href="#12" id="12">So Dora might be surprised to find a cornflakes box in the office, but this is not helpful in explaining why the magazine was not found.</a>
<a href="#13" id="13">We thus want to concentrate on the subset of surprises that might have caused the task failure: the task-relevant surprises.</a>
<a href="#14" id="14">Both plans start with three assumptive actions, first establishing the category of a room, then assuming the existence of the type of target object in that room, and finally placing a virtual object there, so that there is a concrete instance of “ magazine ” that the planner can reason about.</a>
<a href="#15" id="15">But the true state of the world, rather than the robot's lack of knowledge, was the direct cause of plan failure.</a>
<a href="#16" id="16">An explanation must thus commit to specific statements about the world (assignments of specific values to variables) that explain the failure.</a>
<a href="#17" id="17">So this is what is enforced in our approach.</a>
<a href="#18" id="18">To these we add a set of observation actions{a mathematical formula}oi, which are used to check if the simulated execution results in the same state sequence as that originally experienced by the robot.</a>
<a href="#19" id="19">Recall that assumptions may occur only before the first physical action.</a>
<a href="#20" id="20">Thus, the planning problem has a solution only if the planner can find a sequence of assumptions that allow all the original physical actions to be executed in sequence, and which results in expected and actual observations {a mathematical formula}si matching.</a>
<a href="#21" id="21">In the second case, by our allowing instance assumptions based on default knowledge, explanations posit the existence of new places or objects — such as the existence of a new room the object might be in.</a>
<a href="#22" id="22">Thus, by using assumptions for explanations, we can also leverage the mix of relational and commonsense knowledge employed for regular task planning.</a>
<a href="#23" id="23">The diagnostic layer contains two types of action models that achieve this: diagnostic actions and default assumptions.</a>
<a href="#24" id="24">Diagnostic actions are based on the default layer's models of physical actions, augmented with additional possible outcomes and their causes.</a>
<a href="#25" id="25">As an example, recall the default move action from Fig.</a>
<a href="#26" id="26">To test the explanation performance of our system, we analyzed the generated explanations for three perception failure runs of the find test condition outlined before and an additional seven runs under a specifically designed explain test condition, where the object was hidden on a shelf in the meeting room.</a>
<a href="#27" id="27">For instance, in run 9 (condition {a mathematical formula}Cc, object in meeting room), where a place-holder was eradicated owing to a navigation problem, the system could not understand how to reach the meeting room, and so correctly concluded that the object must be outside the known and accessible environment.</a>
<a href="#28" id="28">But unlike the current paper, neither of these approaches explicitly reason about the fact that sensing actions can increase the size of the planning domain, nor the benefits of this for goal achievement.</a>
</body>
</html>