<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:334">
</head>
<body bgcolor="white">
<a href="#0" id="0">Verification of cryptographic algorithms by Aaron Tomb These benchmarks are derived from the problem of comparing reference and production implementations of cryptographic algorithms.</a>
<a href="#1" id="1">Since the number of available benchmark problems for SAT solving is huge while computational resources are limited it is necessary to somehow select a subset of the instances to be used for a competition.</a>
<a href="#2" id="2">The instances are not too hard – Instances that none of the participating solvers can solve in the given time limit do not provide any information about the relative performance of the competing solvers.</a>
<a href="#3" id="3">It is possible to solve such problems by solving the incrementally generated instances independently, however this might be very inefficient compared to an incremental SAT solver, which can reuse knowledge acquired while solving the previous instances (for example the learned clauses and variable activity statistics).</a>
<a href="#4" id="4">The benchmark applications were executed in parallel, i.e., eight instances of a solver-application pair were running at the same time on eight different input files.</a>
<a href="#5" id="5">{sup:13} Two different solvers or two different applications were not allowed to run at the same time, therefore each solver-application pair could only interfere with itself (on different input files).</a>
<a href="#6" id="6">By checking the results in the Main Track (Table 2) we can see that already the sequential version of the solver shows a similar performance gap.</a>
<a href="#7" id="7">So CryptoMinisat seems to be specifically tuned to find proofs in contrast to finding counter-examples.</a>
<a href="#8" id="8">As discussed in Subsection 6.5 the evaluation criteria for the ILT are not as simple as for the other two tracks and there are several alternative ways to define them.</a>
<a href="#9" id="9">Since the exact evaluation criteria were not announced ahead the actual selection of the winning solvers was done “ manually ” by the organizers with the goal to be as fair as possible.</a>
<a href="#10" id="10">In order to make CryptoMiniSat adaptive, the top 5 parameter configurations were experimentally determined and then a classifier has been trained with benchmark problems from the previous SAT Competitions to select the best configuration for the problem at hand.</a>
</body>
</html>