<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:426">
</head>
<body bgcolor="white">
<a href="#0" id="0">Another important relationship between SSPs and short-sighted SSPs is through their policies.</a>
<a href="#1" id="1">To formalize this relationship, we first define the concept of t-closed policy w.r.t.</a>
<a href="#2" id="2">s, i.e., policies that can be executed from s independent of the probabilistic outcome of actions for at least t actions without replanning: t-closed policyA policy Π for an SSP {a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 is t-closed w.r.t.</a>
<a href="#3" id="3">Given an SSP{a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 , for{a mathematical formula}t ≥ |S|, every t-closed policy w.r.t.</a>
<a href="#4" id="4">{a mathematical formula}s0for{a mathematical formula}Sis also a closed policy for{a mathematical formula}S.Since Π is t-closed w.r.t.</a>
<a href="#5" id="5">{a mathematical formula}s0 for {a mathematical formula}t ≥ |S|, then for all {a mathematical formula}s ′ ∈ R Π ∩ S Π , {a mathematical formula} Δ (s0,s ′ ) ≥ |S|.</a>
<a href="#6" id="6">By the definition of {a mathematical formula}S Π , we have that all {a mathematical formula}s ′ ∈ S Π is reachable from {a mathematical formula}s0 when following Π .</a>
<a href="#7" id="7">Thus, {a mathematical formula} Δ (s0,s ′ )<|S| because there exists a trajectory from {a mathematical formula}s0 to {a mathematical formula}s ′ that visits each state at most once, i.e., that uses at most {a mathematical formula}|S| − 1 actions.</a>
<a href="#8" id="8">Therefore, {a mathematical formula} ∄ s ′ ∈ S Π such that {a mathematical formula} Δ (s0,s ′ ) ≥ |S| and {a mathematical formula}R Π ∩ S Π = ∅ .</a>
<a href="#9" id="9">Given an SSP{a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 such thatAssumption 1holds and a monotonic lower bound H for{a mathematical formula}V ⁎ , then the loop in line8of SSiPP (Algorithm 1) is equivalent to applying at least one Bellman backup on{a mathematical formula}V ̲ for every state{a mathematical formula}s ′ ∈ S Π Ss,t ⁎ ∖ Gs,t.</a>
<a href="#10" id="10">SSiPP obtains the next state {a mathematical formula}s ′ from the current state s by either executing or sampling one outcome of the optimal policy {a mathematical formula} Π Ss,t ⁎ of the current short-sighted SSP (Algorithm 1 line 11).</a>
<a href="#11" id="11">This procedure is repeated until {a mathematical formula}s ′ is a goal state, either from the original SSP or an artificial goal.</a>
<a href="#12" id="12">The performance difference between SSiPP and Labeled-SSiPP is not significant for small problems: blocks world 1 to 4, triangle tire world problems 1 and 2, and zeno travel problem 1 and 2.</a>
<a href="#13" id="13">For the triangle tire world problems 3 and 4, {a mathematical formula}t=32 is large enough that an Ε -consistent solution is found using a single short-sighted SSP, so the performance of SSiPP and Labeled-SSiPP for {a mathematical formula}t=32 is equivalent to the LRTDP performance.</a>
</body>
</html>