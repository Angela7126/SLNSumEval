<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:629">
</head>
<body bgcolor="white">
<a href="#0" id="0">In order to show that playing a pure strategy does indeed lead to poor defender utility, we conducted an experiment with human subjects by deploying a SUQR based pure strategy on {a mathematical formula}ADS1.</a>
<a href="#1" id="1">{a mathematical formula}</a>
<a href="#2" id="2">{a mathematical formula}</a>
<a href="#3" id="3">{a mathematical formula}</a>
<a href="#4" id="4">(18):{a mathematical formula} where {a mathematical formula}Nr is the total number of rounds and r is the round whose data is under consideration.</a>
<a href="#5" id="5">If a target profile {a mathematical formula} Β u was not exposed to attacker response in round r, the defender will not be able to compute the vulnerability {a mathematical formula}V Β ur.</a>
<a href="#6" id="6">We then generate future round strategies against the boundedly rational adversary characterized by the learned model parameters by solving the following optimization problem:{a mathematical formula}{a mathematical formula}qiR( Ω |x) is the probability that the adversary will attack target i in round R and is calculated based on the following equation:{a mathematical formula}{a mathematical formula} Β ki denotes that target profile {a mathematical formula} Β k is associated with target i.</a>
<a href="#7" id="7">We then updated the propensities based on round 1 attack data for Maximin on {a mathematical formula}ADS1 and computed the corresponding mixed strategy and deployed that as the round 2 strategy.</a>
<a href="#8" id="8">In Section 12.1 we show average defender expected utilities for five models (P-SUQR, P-BSUQR, P-RSUQR, SHARP and Maximin) against actual human subjects, for various rounds of our experiment on two payoff structures ({a mathematical formula}ADS1 and {a mathematical formula}ADS2).</a>
<a href="#9" id="9">8(a) – 8(b) show cumulative defender utility over five rounds on {a mathematical formula}ADS1 and {a mathematical formula}ADS2 respectively.</a>
<a href="#10" id="10">8(a), we can observe that it takes five rounds for P-SUQR to recover from initial round losses and outperform Maximin in terms of cumulative defender utility for {a mathematical formula}ADS1.</a>
<a href="#11" id="11">We deployed an experiment on AMT with the defender strategy computed based on the SUQR model learned from round 1 data of {a mathematical formula}ADS1.</a>
<a href="#12" id="12">This average expected defender utility obtained by deploying a pure strategy based on a learned SUQR model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#13" id="13">We deployed an experiment on AMT with the defender strategy computed based on the RL model learned from round 1 data of Maximin on {a mathematical formula}ADS1 (as explained earlier in Section 11).</a>
<a href="#14" id="14">This average expected defender utility obtained by deploying the defender strategy based on a learned RL model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#15" id="15">Given that this strategy performs worse in one round than the cumulative average expected defender utility of all the other models, it has very little chance of ever recovering and outperforming the other models we have discussed earlier after more rounds.In addition, we show the deployed defender strategy for round 2 on {a mathematical formula}ADS1 in Fig.</a>
<a href="#16" id="16">Therefore, given the poor initial round performance of the RL model on {a mathematical formula}ADS1, we did not conduct any further experiments for future rounds.</a>
<a href="#17" id="17">12(a) – 12(b) and 29(a) – 29(b) show human perceptions of probability in rounds 1 – 4 when the participants were exposed to P-SUQR based strategies on {a mathematical formula}ADS1, {a mathematical formula}ADS2, {a mathematical formula}ADS3 and {a mathematical formula}ADS4 respectively.</a>
<a href="#18" id="18">(17)) from round to round is the mixed strategy x and hence the coverage {a mathematical formula}xi at each target.</a>
<a href="#19" id="19">To further illustrate that the SHARP based strategy does indeed change over rounds, we show SHARP based strategies on {a mathematical formula}ADS2 from rounds 2 – 5 in Figs.</a>
<a href="#20" id="20">19 we show actual defender utilities obtained over 4 rounds for SHARP on {a mathematical formula}ADS3.</a>
<a href="#21" id="21">22, the human perceptions of probability in rounds 1 – 4 when the security experts were exposed to SHARP based strategies on {a mathematical formula}ADS3.</a>
</body>
</html>