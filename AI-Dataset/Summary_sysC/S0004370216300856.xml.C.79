<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:647">
</head>
<body bgcolor="white">
<a href="#0" id="0">The primary contribution of this article is a new model called SHARP (Stochastic Human behavior model with AttRactiveness and Probability weighting) that mitigates these three limitations: (i) Modeling the adversary's adaptive decision making process, SHARP reasons based on success or failure of the adversary's past actions on exposed portions of the attack surface; (ii) Addressing limited exposure to significant portions of the attack surface in initial rounds, SHARP reasons about similarity between exposed and unexposed areas of the attack surface, and also incorporates a discounting parameter to mitigate adversary's lack of exposure to enough of the attack surface; (iii) Addressing shortcomings of existing models in learning the adversaries' weighting of probabilities, we incorporate a two parameter probability weighting function in existing human behavior models.</a>
<a href="#1" id="1">Thus, according to this model, the probability that the adversary will attack target i is given by:{a mathematical formula} where {a mathematical formula}SUia(x) is the Subjective Utility of an adversary for attacking target i when defender employs strategy x and is given by:{a mathematical formula} The vector {a mathematical formula} Ω =( Ω 1, Ω 2, Ω 3) encodes information about the adversary's behavior and each component of Ω indicates the relative importance the adversary gives to each attribute in the decision making process.</a>
<a href="#2" id="2">While researchers often use imputation and sampling techniques to fill missing data due to participant attrition [78], [36], [23], for our repeated measures study of comparing human behavior models this may result in extremely biased estimates of the modeling parameters due to the influence of the retained participants' game plays and therefore may generate biased defender strategies.</a>
<a href="#3" id="3">Given S-shaped probability weighting functions, the learned {a mathematical formula} Ω 1 was negative as it accurately captured the trend that a significantly higher number of people were attacking targets with low to medium coverage probabilities and not attacking high coverage targets.</a>
<a href="#4" id="4">Our approach of not just performing 10-fold cross validation once to select the best model weights, but multiple (10) times and then taking an average of the test set errors of the best learned model weights is also similar to what is traditionally adopted in machine learning literature [25] to improve robustness.</a>
<a href="#5" id="5">Due to the differences in animal densities, if we start from a uniform mixed strategy, it would leave many of the targets of high animal density to be attacked in round 1 (as evidenced from other human subject experiments conducted in the past [66]) and that would result in a defender utility which is much lower than the cumulative utility for any of our models over five rounds (see Section 12 for details).</a>
<a href="#6" id="6">However, it performs worse than SHARP (statistically significant at {a mathematical formula}p=0.01) as SHARP also trusts the data less when we don't have enough information about the adversary's responses to most of the attack surface; in this case the initial rounds.</a>
<a href="#7" id="7">This average expected defender utility obtained by deploying a pure strategy based on a learned SUQR model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#8" id="8">Given that this strategy performs worse in one round than the cumulative average expected defender utility of all the other models, it demonstrates the point that the performance of SUQR without probability weighting is worse than any of the other models that include probability weighting.Notice that the reason this SUQR pure strategy performs so poorly is that it leaves 16 out of 25 targets completely exposed, and among these targets are ones with animal densities 4 and 5.</a>
<a href="#9" id="9">Thus, the poor performance in this initial round of the RL model on {a mathematical formula}ADS1 and its leaving targets of high value exposed to exploitation in the subsequent round illustrates that significant new work would need to be done to adapt the proposed RL framework (based on the model by Erev and Roth [28]) for our Stackelberg Security Game setting.</a>
</body>
</html>