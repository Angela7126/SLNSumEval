<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:755">
</head>
<body bgcolor="white">
<a href="#0" id="0">Standard random backpropagation operates exactly like backpropagation except that the weights used in the backward pass are completely random and fixed.</a>
<a href="#1" id="1">Thus the learning rule becomes:{a mathematical formula} where the randomly backpropagated error satisfies the recurrence relation:{a mathematical formula} and the weights {a mathematical formula}ckih+1 are random and fixed.</a>
<a href="#2" id="2">The boundary condition at the top remains the same:{a mathematical formula} Thus in RBP the weights in the top layer of the architecture are updated by gradient descent, identically to the BP case.</a>
<a href="#3" id="3">Ultimately, for optimal learning, all the information required to reach a critical point of {a mathematical formula}E must appear in the learning rule of the deep weights.</a>
<a href="#4" id="4">Thus, using the learning channel, we are interested in local learning rules of the form:{a mathematical formula} In fact, here we shall focus exclusively on learning rules with the multiplicative form:{a mathematical formula} corresponding to a product of the presynaptic activity with some kind of backpropagated error information, with standard BP and RBP as a special cases.</a>
<a href="#5" id="5">ARBP = adaptive random backpropagation, where the matrices in the learning channel are initialized randomly, and then progressively adapted during learning using the product of the corresponding forward and backward signals, so that {a mathematical formula} Δ crsl= Η Rsl+1Orl, where R denotes the randomly backpropagated error.</a>
<a href="#6" id="6">In this case, the forward channel becomes the learning channel for the backward weights.</a>
<a href="#7" id="7">Updates to a deep layer with RBP or SRBP appear to require updates in the precedent layers in the learning channel.</a>
<a href="#8" id="8">If we fix the weights in layer h, while updating the rest of the layers with SRBP, performance is often worse than if we fix layers {a mathematical formula}l ≤ h.</a>
<a href="#9" id="9">In all these RBPs algorithms, the L-layer at the top with parameters {a mathematical formula}wijL follows the gradient, as it is trained just like BP, since there are no random feedback weights used for learning in the top layer.</a>
<a href="#10" id="10">In other words, BP = RBP = SRBP for the top layer.</a>
<a href="#11" id="11">For instance, consider a linear or non-linear {a mathematical formula}A[N0,N1,1] architecture with coherent weights, and denote by a the weights in the bottom layer, by b the weights in the top layer, and by c the weights in the learning channel.</a>
<a href="#12" id="12">Let us denote by {a mathematical formula}a1 and {a mathematical formula}a2 the weights in the first and second layer, and by {a mathematical formula}c1 the random weight of the learning channel.</a>
<a href="#13" id="13">In this case, we have {a mathematical formula}O(t)=a1a2I(t) and the learning equations are given by:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula} Α =E(IT) and {a mathematical formula} Β =E(I2).</a>
<a href="#14" id="14">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1 and {a mathematical formula}a2 during learning:{a mathematical formula} Note that the dynamic of {a mathematical formula}P=a1a2 is given by:{a mathematical formula} The error is given by:{a mathematical formula} and:{a mathematical formula} the last equality requires {a mathematical formula}ai ≠ 0.</a>
<a href="#15" id="15">In the case of a linear {a mathematical formula}A[1,1,1,1] architecture, for notational simplicity, let us denote by {a mathematical formula}a1,a2 and {a mathematical formula}a3 the forward weights, and by {a mathematical formula}c1 and {a mathematical formula}c2 the random weights of the learning channel (note the index is equal to the target layer).</a>
<a href="#16" id="16">In this case, we have {a mathematical formula}O(t)=a1a2a3I(t)=PI(t).</a>
<a href="#17" id="17">The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}P=a1a2a3.</a>
<a href="#18" id="18">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1,a2 and {a mathematical formula}a3 during learning:{a mathematical formula} The dynamic of {a mathematical formula}P=a1a2a3 is given by:{a mathematical formula}</a>
<a href="#19" id="19">Consider a linear {a mathematical formula}A[1,N,1] architecture (Fig.</a>
<a href="#20" id="20">RBP equations</a>
<a href="#21" id="21">Note that in the case of RBP with backward matrices {a mathematical formula}C1, … ,CL − 1, as opposed to SRBP, one has the system of differential equations:{a mathematical formula} By letting {a mathematical formula}Bi=Ci … CL − 1 one obtains the SRBP equations however the size of the layers may impose constraints on the rank of the matrices {a mathematical formula}Bi.</a>
<a href="#22" id="22">Gradient descent equations</a>
<a href="#23" id="23">Finally, for comparison, in the case of gradient descent, the system is given by:{a mathematical formula} Except for trivial cases, the critical points are again given by Equation (116), and the system always converges to a critical point.</a>
</body>
</html>