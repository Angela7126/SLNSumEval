<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:553">
</head>
<body bgcolor="white">
<a href="#0" id="0">Based on this discriminant function a TET structure learning algorithm is developed that, strictly speaking, will discover a feature {a mathematical formula}(T,f) (T a TET, f a discriminant function defined on T), but from which in a trivial abstraction step we can extract the model-independent TET feature T (Section 5).</a>
<a href="#1" id="1">We give the general definition of TET semantics in two steps: first we define the value space of nested counts associated with a given TET {a mathematical formula}T(V), and then the actual mapping {a mathematical formula}a ↦ V(T(a)).</a>
<a href="#2" id="2">Since no further variables are introduced via edge labels, then all nodes only contain literals in the variables V, and therefore {a mathematical formula}V(T(a1, … ,an)) only depends on the relational sub-structure induced by {a mathematical formula}a1, … ,an in {a mathematical formula}M. By varying the types at the nodes, as well as the tree structure of {a mathematical formula}T(V), the values {a mathematical formula}V(T(a)) can represent a variety of different structural properties.Consider a relational signature that contains a single binary ( “ edge ” ) relation symbol {a mathematical formula}e( ⋅ , ⋅ ).</a>
<a href="#3" id="3">In this case MLNs define a distribution over all models {a mathematical formula}M for a given signature R, and a fixed domain M. This distribution is defined by a knowledge base KB containing first-order logic formulas {a mathematical formula} Φ i with attached numeric weights {a mathematical formula}wi:{a mathematical formula} We refer to [36] for the details of MLN syntax and semantics.</a>
<a href="#4" id="4">Relevant in the current context is the fact that the distribution is defined as a function of count features, which, adapting the notation of the previous example, can be written in the form{a mathematical formula} where now {a mathematical formula} Φ (X) can be any first-order formula with free variables X.</a>
<a href="#5" id="5">Graphs are a special kind of relational models in which there is a single binary (edge) relation {a mathematical formula}e(X,Y), and, in the case of labeled graphs, multiple unary relations {a mathematical formula}li(X) representing the different node labels.</a>
<a href="#6" id="6">3 shows a propositional TET for a, b, c. This TET is labeled with a weight assignment, where the weight at each node corresponds to the empirical frequency {a mathematical formula}n+/(n++n − ) of the positive class among the examples that satisfy all the conditions on the path from the root down to the node.</a>
<a href="#7" id="7">Thus, the interpretation of the discriminant function on propositional TETs also explains the discriminant function on general TETs, with two additional assumptions, or observations: our general independence assumption implies that features {a mathematical formula}T ′ (V,a),T ′ (V,a ′ ) defined by two different substitutions of constants in a sub-TET {a mathematical formula}T ′ (V,W) are assumed as independent, and the {a mathematical formula}d+/d − ratio one obtains is only an approximation of (14), since the weights defined by the initial TET are not exact class frequencies for the ground features, but only shared approximations obtained from aggregating statistics from all groundings (see Section 5).</a>
<a href="#8" id="8">The discriminant function used by the learner (as well as virtually any other conceivable prediction model) is not able to exactly map the feature value obtained from the first branch into the binary threshold function {a mathematical formula}h>7, and therefore additional features represented by the additional branches can still be useful for obtaining a better fit to this threshold.</a>
</body>
</html>