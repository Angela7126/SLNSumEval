<html>
<head>
<meta name="TextLength" content="SENT_NUM:19, WORD_NUM:629">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, performing inference and learning in hybrid domains is a particularly daunting task.</a>
<a href="#1" id="1">The ability to model these kinds of domains is crucial in “ learning to design ” tasks, that is, learning applications where the goal is to learn from examples how to perform automatic de novo design of novel objects.</a>
<a href="#2" id="2">In this paper we present Structured Learning Modulo Theories, a max-margin approach for learning in hybrid domains based on Satisfiability Modulo Theories, which allows to combine Boolean reasoning and optimization over continuous linear arithmetical constraints.</a>
<a href="#3" id="3">The main idea is to leverage a state-of-the-art generalized Satisfiability Modulo Theory solver for implementing the inference and separation oracles of Structured Output SVMs.</a>
<a href="#4" id="4">A number of approaches [3], [4], [5], [6] focused on the feature representation perspective, in order to extend statistical relational learning algorithms to deal with continuous features as inputs.</a>
<a href="#5" id="5">On the other hand, performing inference over joint continuous – discrete relational domains is still a challenge.</a>
<a href="#6" id="6">Most important for the scope of this paper is that there are high-quality OMT solvers which, at least for the {a mathematical formula}LRA theory, can handle problems with thousands of hybrid variables.</a>
<a href="#7" id="7">All these approaches aim at extending statistical relational learning algorithms to deal with continuous features as inputs.</a>
<a href="#8" id="8">On the other hand, our framework aims at allowing learning and inference over hybrid continuous – discrete domains, where continuous and discrete variables are the output of the inference process.</a>
<a href="#9" id="9">Overall, for the scope of this paper, it is important to highlight the fact that OMT solvers are available which, thanks to the underlying SAT and SMT technologies, can handle problems with a large number of hybrid variables (in the order of thousands, at least for the {a mathematical formula}LRA theory).</a>
<a href="#10" id="10">We consider the problem of learning from a training set of n complex objects {a mathematical formula}{(Ii,Oi)}i=1n, where each object {a mathematical formula}(I,O) is represented as a set of Boolean and rational variables:{a mathematical formula} We indicate Boolean variables using predicates such as {a mathematical formula}touching(i,j), and write rational variables as lower-case letters, e.g.</a>
<a href="#11" id="11">In order to encode the set of constraints {a mathematical formula}{ Φ k} that underlie both the learning and the inference problems, it is convenient to first introduce a background knowledge of predicates expressing facts about the relative positioning of blocks.</a>
<a href="#12" id="12">To this end we add a fresh predicate {a mathematical formula}left(i,j), that encodes the fact that “ a generic block of index i touches a second block j from the left ” , defined as follows:{a mathematical formula} Similarly, we add analogous predicates for the other directions: {a mathematical formula}right(i,j), {a mathematical formula}below(i,j), {a mathematical formula}over(i,j) (see Fig.</a>
<a href="#13" id="13">The hard constraints represent the fact that the output O should be a valid block within the bounding box (all the constraints {a mathematical formula} Φ k are implicitly conjoined):{a mathematical formula} Then we require the output block O to “ touch ” the input block I:{a mathematical formula} Note that whenever this rule is satisfied, both conditions (i) and (ii) of the toy example hold, i.e.</a>
<a href="#14" id="14">Finally, we encode the cost function {a mathematical formula}cost=w1dx2+w2dy2, completing the description of the optimization problem.</a>
<a href="#15" id="15">In the following we will see that the definition of the cost function implicitly defines also the set of features, or equivalently the set of soft constraints, of the LMT problem.</a>
<a href="#16" id="16">The loss function determines the dissimilarity between output structures, which in our case contain a mixture of Boolean and rational variables.</a>
<a href="#17" id="17">We observe that by picking a loss function expressible as an OMT({a mathematical formula}LRA) problem, we can readily use the same OMT solver used for inference to also solve the CP separation oracle (Eq.</a>
<a href="#18" id="18">For example, suppose we have an {a mathematical formula}8×8 image of an upper-case “ A ” , as in Fig.</a>
</body>
</html>