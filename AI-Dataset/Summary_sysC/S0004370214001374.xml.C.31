<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:222">
</head>
<body bgcolor="white">
<a href="#0" id="0">Note that in the model-based formulation we use solely the artificial samples to update the policy.</a>
<a href="#1" id="1">Thus, in case of unbiased models, we can avoid the possibly noisy reward samples evaluated on the real system and, hence, eliminate the risk sensitive bias inherent to the REPS algorithm.</a>
<a href="#2" id="2">Additionally, we can increase the number of artificial samples significantly {a mathematical formula}N ≪ M to further improve the accuracy of policy updates.</a>
<a href="#3" id="3">Due to the recent success of using Gaussian Process models to reduce the model bias when learning complex system dynamics [9], we use GP models to learn the forward models of the robot and its environment.</a>
<a href="#4" id="4">Therefore, our method is called Gaussian Process Relative Entropy Policy Search (GPREPS).</a>
<a href="#5" id="5">We use forward models to simulate a trajectory Τ given the context s and the lower-level policy parameters Ω .</a>
<a href="#6" id="6">We learn a forward model that is given by {a mathematical formula}yt+1=f(yt,ut)+ Ε , where {a mathematical formula}y=[xT,bT]T is composed of the state of the robot and the state of the environment b, for instance the position of a ball.</a>
<a href="#7" id="7">The vector Ε denotes zero-mean Gaussian noise.</a>
<a href="#8" id="8">In order to simplify the learning task, we decompose the forward model f into simpler models, which are easier to learn.</a>
<a href="#9" id="9">To do so, we exploit prior structural knowledge of how the robot interacts with the environment.</a>
</body>
</html>