<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:269">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our first algorithm, state abstraction from demonstration (AfD), uses a small set of human demonstrations of the task the agent must learn to determine a state-space abstraction.</a>
<a href="#1" id="1">Then using this abstracted state space allows significant speed-ups in reinforcement learning algorithms with respect to the number of state features in complex domains.</a>
<a href="#2" id="2">We define{a mathematical formula} as a vector whose elements are the mutual information between each feature of the state space and the action taken by the human teacher, according to the samples of H in the state-space subregion {a mathematical formula}E ⊂ S.</a>
<a href="#3" id="3">Our first algorithm, state abstraction from demonstration (AfD) learns a policy for an MDP by building an abstract space {a mathematical formula}S Α and using reinforcement learning to find an optimal policy that can be represented in {a mathematical formula}S Α .</a>
<a href="#4" id="4">AfD obtains {a mathematical formula}S Α by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S Α can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
<a href="#5" id="5">To determine which features are relevant to a particular subtask, we measure the mutual information between each state feature and the action taken by a human teacher in a set of demonstrations.</a>
<a href="#6" id="6">ADA+RL adds another step, policy improvement, in which we use reinforcement learning techniques to find the optimal policy that can be represented in the abstract state space {a mathematical formula}S Α .</a>
</body>
</html>