<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:614">
</head>
<body bgcolor="white">
<a href="#0" id="0">At each time step, the agent can take an action {a mathematical formula}ak ∈ A.</a>
<a href="#1" id="1">For instance, by considering a system with M actuators (an M-dimensional action space) and N discrete actions in each one, a DRL modeling leads to evaluating and storing NM values per state instead of {a mathematical formula}NM as a centralized RL does.</a>
<a href="#2" id="2">Since from a computational point of view, all the individual agents in a DRL system can operate in parallel acting upon their individual and reduced action spaces, the learning speed is typically higher compared to a centralized agent which searches an exponentially larger action space {a mathematical formula}N=N1× ⋯ ×NM, as expressed in (2)[39].</a>
<a href="#3" id="3">A well-known rule is that the RL agent must be able to observe or control every variable involved in the reward function {a mathematical formula}R(S,A).</a>
<a href="#4" id="4">In the centralized modeling for the 3DMC problem, a global reward function is proposed as: {a mathematical formula}r=0 if the common goal is reached, {a mathematical formula}r= − 1otherwise.</a>
<a href="#5" id="5">So, a simplified 3DMC could be modeled as a fully decentralized problem with two individual agents with their own independent state vectors, {a mathematical formula}sx=[x,x ˙ ,y],sy=[x,y,y ˙ ].</a>
<a href="#6" id="6">Once the global DRL modeling has been defined and the tuples state, action, and reward {a mathematical formula}[Sm,Am,Rm] are well identified per every agent {a mathematical formula}m=1, ⋯ ,M, it is necessary to complete each single RL modeling.</a>
<a href="#7" id="7">Algorithm 1 is described for the most general case of a fully-decentralized system with individual rewards, where states and rewards are annotated as {a mathematical formula}sm and {a mathematical formula}rm respectively, but it is also possible to implement a joint state vector or common reward DRL systems.</a>
<a href="#8" id="8">In this case, we have that {a mathematical formula} Σ → 1 and the learning rate tends to Α .</a>
<a href="#9" id="9">Then, in this case, {a mathematical formula} Σ → 1 and the learning rate tends toward a high value.</a>
<a href="#10" id="10">For all the experiments {a mathematical formula} Γ =0.99.</a>
<a href="#11" id="11">Thus, this centralized approach considers all possible action combinations {a mathematical formula}A=AvlAaw and the robot learns {a mathematical formula}[vl,aw] actions from the observed joint state {a mathematical formula}[ Ρ , Γ , Φ ,vw], where {a mathematical formula}[vw=vw(k − 1)+aw].</a>
<a href="#12" id="12">To keep parity with the centralized model, our decentralized modeling considers two individual agents for learning {a mathematical formula}vl and {a mathematical formula}aw in parallel as is shown in Table 3.</a>
<a href="#13" id="13">{a mathematical formula}</a>
<a href="#14" id="14">Stage3.4 — Determining if the problem is fully decentralizable: the joint state vector {a mathematical formula}[ Ρ , Γ , Φ ,vw] is identical to the one proposed for the centralized case.</a>
<a href="#15" id="15">Meanwhile, the continuous state-action FQL algorithm is adequate for learning the continuous linear speed control action of the agent {a mathematical formula}vl.</a>
<a href="#16" id="16">For all the experiments {a mathematical formula} Γ =0.99.</a>
<a href="#17" id="17">All the DRL systems implemented improved the {a mathematical formula}%ofScoredGoals of the centralized one as in the learning evolution traces (Fig.</a>
<a href="#18" id="18">However the DRL five actions final performance was 68.97%, still higher than the 57.14% of its CRL counterpart, although lower than the 75.28% of the DRL with {a mathematical formula}Nvl=7 actions.</a>
<a href="#19" id="19">Since 17 discrete actions per agent are implemented for the DRL system, if an equivalent CRL system were implemented, that centralized agent would search in an action space of {a mathematical formula}173=4913 possible actions, which would be enormous for most of the RL algorithms.</a>
<a href="#20" id="20">The same joint state vector was used and the global reward function is similar to {a mathematical formula}rx in (11), but using {a mathematical formula} Γ th/3 and {a mathematical formula} Φ th/3.</a>
<a href="#21" id="21">For all the experiments {a mathematical formula} Γ =0.99.</a>
<a href="#22" id="22">{a mathematical formula}</a>
<a href="#23" id="23">For all the experiments {a mathematical formula} Γ =1.</a>
</body>
</html>