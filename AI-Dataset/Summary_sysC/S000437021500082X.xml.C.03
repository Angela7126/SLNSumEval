<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:317">
</head>
<body bgcolor="white">
<a href="#0" id="0">The results attest that our transfer learning algorithm outperforms recent heuristically-accelerated reinforcement learning and transfer learning algorithms.</a>
<a href="#1" id="1">The L3 meta-algorithm works in 3 stages: first, it uses an RL algorithm to learn how to perform one task, storing the solution for this problem as a case in a case base; in the second stage, it maps actions of the source domain to actions of the target domain; and, in the last stage, it uses the stored cases as heuristics to speed up the Reinforcement Learning process in the target domain.</a>
<a href="#2" id="2">The application of Transfer Learning within Reinforcement Learning tasks was first proposed in [4], where an algorithm was defined that exploits strong features obtained from RL on one task in order to compose functions in a case base that is used on the solution of a new task.</a>
<a href="#3" id="3">Stage 3: In the final stage, the previously stored case base is used in a CB-HARL algorithm to speed up task learning in the target domain.</a>
<a href="#4" id="4">The case base used in L3-SARSA( Λ ) was the one that contains N cases selected by random sampling the state set and finding the best action for that state, because this is the case base that produced the best results for the transfer.</a>
<a href="#5" id="5">A negative transfer was provided to L3-SARSA( Λ ) from a case base constructed with the N worst cases (as shown in Fig.</a>
<a href="#6" id="6">The results of the negative transfer for both L3-SARSA( Λ ) and TiMRLA Value-Addition algorithm are shown in Fig.</a>
<a href="#7" id="7">This characteristic makes L3 robust to negative transfers: if the cases acquired in the source domain are not useful in the target domain, assuming them as heuristics will not speed up the learning procedure but, in the worst case (when every case in the case base is not applicable to the target domain), L3 will be as efficient as the original RL algorithm that it is based.</a>
</body>
</html>