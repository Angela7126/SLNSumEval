<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:239">
</head>
<body bgcolor="white">
<a href="#0" id="0">A problem closely related to algorithm selection is the algorithm configuration problem: given a parameterized algorithm A, a set of problem instances I and a performance measure m, find a parameter setting of A that optimizes m on I (see [52] for a formal definition).</a>
<a href="#1" id="1">In contrast to ASlib, it is infeasible in AClib to store performance data for all possible parameter configurations, which often number more than 10{sup:50}.</a>
<a href="#2" id="2">Therefore, an experiment on AClib includes new (expensive) runs of the target algorithms with different configurations and hence, experiments on AClib are a lot more costly than experiments on ASlib, where no new algorithm runs are necessary.</a>
<a href="#3" id="3">Some algorithm selectors do not select a single algorithm, but compute a schedule of several algorithms: they apply a to i for a resource budget {a mathematical formula}r âˆˆ R (e.g., CPU time), evaluate the performance metric, evaluate a stopping criterion, and repeat as necessary, taking observations made during the run of a into account.</a>
<a href="#4" id="4">Most of our scenarios were taken from publications that report performance improvements through algorithm selection and consist of algorithms where the virtual best solver (VBS){sup:5} is significantly better than the single best solver.</a>
<a href="#5" id="5">{sup:6} Therefore, these are problems on which it makes sense to seek performance improvements via algorithm selection.</a>
<a href="#6" id="6">To provide further insight into our algorithm selection scenarios, we applied forward selection [61] to the algorithms and features to determine whether smaller subsets still achieve comparable performance.</a>
</body>
</html>