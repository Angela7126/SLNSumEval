<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:748">
</head>
<body bgcolor="white">
<a href="#0" id="0">Finally, we prove several mathematical results for RBP and its variants including: (1) the convergence to optimal fixed points for linear chains of arbitrary length; (2) convergence to fixed points for linear autoencoders with decorrelated data; (3) long-term existence of solutions for linear systems with a single hidden layer, and their convergence in special cases; and (4) convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.</a>
<a href="#1" id="1">Using the chain rule, it is easy to see that the backpropagated error satisfies the recurrence relation:{a mathematical formula} with the boundary condition:{a mathematical formula} Thus in short the errors are propagated backwards in an essentially linear fashion using the transpose of the forward matrices, hence the symmetry of the weights, with a multiplication by the derivative of the corresponding forward activations every time a layer is traversed.</a>
<a href="#2" id="2">The equations that the weights must satisfy at any critical point are simply:{a mathematical formula} Thus in general the optimal weights must depend on both the input and the targets, as well as the other weights in the network.</a>
<a href="#3" id="3">Thus in general, in a solution of the critical equations, the weights {a mathematical formula}wijhmust depend on {a mathematical formula}Ojh − 1, the outputs, the targets, the upper weights, and the upper derivatives.</a>
<a href="#4" id="4">Backpropagation shows that it is sufficient for the weights to depend on {a mathematical formula}Ojh − 1, {a mathematical formula}T − O, the upper weights, and the upper derivatives.</a>
<a href="#5" id="5">Under these assumptions, as shown in [5], the expected value of the activity of each unit in the backward pass is exactly given by the standard BP equations and equal to {a mathematical formula}Bih for unit i in layer h. In other words, standard backpropagation can be viewed as computing the exact average over all backpropagation processes implemented on all the stochastic realizations of the backward network under the three forms of noise described above.</a>
<a href="#6" id="6">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1 and {a mathematical formula}a2 during learning:{a mathematical formula} Note that the dynamic of {a mathematical formula}P=a1a2 is given by:{a mathematical formula} The error is given by:{a mathematical formula} and:{a mathematical formula} the last equality requires {a mathematical formula}ai ≠ 0.</a>
<a href="#7" id="7">Furthermore, except for trivial cases associated with{a mathematical formula}c1=0, starting from any initial conditions the system converges to a fixed point corresponding to a global minimum of the quadratic error function.</a>
<a href="#8" id="8">To find the specific critical point to which it converges to, Equations (30) and (27) must be satisfied simultaneously which leads to the depressed cubic equation:{a mathematical formula} which can be solved using the standard formula for the roots of cubic equations.</a>
<a href="#9" id="9">Except for trivial cases (associated with{a mathematical formula}c1=0or{a mathematical formula}c2=0), starting from any initial conditions the system in Equation(36)converges to a fixed point, corresponding to a global minimum of the quadratic error function.</a>
<a href="#10" id="10">The hypersurface {a mathematical formula}a12a22+c1a2a3+c2a12a3=0 depends on {a mathematical formula}c1,c2 and provides additional critical points for the product P. It can be shown again by linearization that this hypersurface separates stable from unstable fixed points.As in the previous case, small weights and congruent weights can help learning but are not necessary.</a>
<a href="#11" id="11">Thus every {a mathematical formula}ai can be expressed as a polynomial function of {a mathematical formula}a1 of degree {a mathematical formula}2i − 1, containing only even terms:{a mathematical formula} and:{a mathematical formula} By substituting these relationships in the equation for the derivative of {a mathematical formula}a1, we get {a mathematical formula}da1/dt=Q(a1) where Q is a polynomial with an odd degree n given by:{a mathematical formula} Furthermore, from Equation (50) it can be seen that leading coefficient is negative therefore, using Theorem 1, for any set of initial conditions the system must converge to a finite fixed point.</a>
<a href="#12" id="12">In the general case where the weights in the learning channel are non-zero, the critical points are given by the surface {a mathematical formula} Α − Β P=0 and correspond to global optima.</a>
<a href="#13" id="13">In addition to the vertical coupling between {a mathematical formula}ai and {a mathematical formula}bi, there is an horizontal coupling between the {a mathematical formula}ai variables given again by Equation (57) resulting in:{a mathematical formula} Thus, iterating, all the variables {a mathematical formula}ai can be expressed as affine functions of {a mathematical formula}a1 in the form:{a mathematical formula} Thus solving the entire system can be reduced to solving for {a mathematical formula}a1.</a>
</body>
</html>