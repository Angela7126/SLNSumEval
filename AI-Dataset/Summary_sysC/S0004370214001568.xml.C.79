<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:513">
</head>
<body bgcolor="white">
<a href="#0" id="0">As a first step, this paper proposes three challenging but accessible problems that would both change the public perception of artificial intelligence and spur substantive research to advance our science.</a>
<a href="#1" id="1">In response, Section 2 proposes collaboration between a person and a machine, and highlights some crucial differences between human and computer collaborators.</a>
<a href="#2" id="2">Their premise was that “ every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. ” There is no indication in that document that computers were to work together with people; the intent was to develop an autonomous intelligence.</a>
<a href="#3" id="3">The Dartmouth manifesto postulated general goals: to discover “ how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. ” There was no mention of interaction with or impact on people, and no targets beyond the things that people can do.</a>
<a href="#4" id="4">“ Learn ” and “ model ” now clearly dominate, with techniques (e.g., “ plan, ” “ gener, ” “ search, ” “ constraint, ” “ logic, ” and “ network ” ) as important concerns, along with “ game, ” “ social, ” and “ data. ” This is how we, as researchers, now describe our field to one another.</a>
<a href="#5" id="5">The workshop's call solicited papers that address “ the ethical questions implicit in such headlines which go to the centre of the quest to build AI systems with potentially super-human intelligence. ” {sup:5}</a>
<a href="#6" id="6">AI researchers soon recognized that to solve the “ kinds of problems now reserved for humans ” would require a substantive knowledge base and a host of ways to use it [16].</a>
<a href="#7" id="7">A good collaborator monitors its own declarative knowledge ( “ I am unfamiliar with ‘ thickening ‴ ), procedural skill ( “ I've rarely made stock before ” ), and progress on the task ( “ This appears to be taking longer than I expected ” ).</a>
<a href="#8" id="8">Given an ontology of tasks and their components, the computer would assume that any new conversation is a request for help, and seek to elicit from the person some clarification of the task at hand, including the person's goals and relevant constraints.</a>
<a href="#9" id="9">Construction of a complex object requires raw material, a variety of processes, a partial-order planner, and the ability to address such diverse (and likely conflicting) evaluation criteria as safety, economy, and novelty.</a>
<a href="#10" id="10">This CI would be expected to solicit feature values from the person (e.g., height, cost) and to have prior knowledge of fundamental processes for the object's construction (e.g., permits, excavation, foundation).</a>
<a href="#11" id="11">Regardless of the task, a crucial evaluation metric for any collaborative intelligence, in my opinion, must be the extent to which a person believes that her work experience or product has been facilitated or improved by the collaboration.</a>
<a href="#12" id="12">It requires a more nuanced representation of time, a still larger knowledge base, multiple ways to represent the object to the user, and explicit specification of a broad range of evaluation criteria, including government regulations and aesthetic criteria.</a>
</body>
</html>