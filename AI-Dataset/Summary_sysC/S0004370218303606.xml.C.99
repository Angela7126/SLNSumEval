<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:698">
</head>
<body bgcolor="white">
<a href="#0" id="0">A related method [8] formulates the problem as one of finding a distribution over deterministic policies, {a mathematical formula}Pr( Π I), which has the maximum entropy while matching the convex combination of feature expectations from all policies with those from the observed expert's trajectory:{a mathematical formula} Here, Δ is the space of all distributions, {a mathematical formula}Pr( Π I); {a mathematical formula} Φ ˆ k is the expectation over the {a mathematical formula}kth feature from observations of the expert as defined previously; and the visitation frequency {a mathematical formula} Μ Π I is computed as in Eq.</a>
<a href="#1" id="1">Some features critical to the demonstrated behavior may be active entirely in the occluded portion of the state space, which will result in {a mathematical formula} Φ ˆ k=0 for this feature.</a>
<a href="#2" id="2">As a result, we can no longer utilize the gradient of the feature expectation to find the reward weights.</a>
<a href="#3" id="3">Thus, a numerical solving method that does not use the gradient function such as Nelder-Mead's simplex technique [27] is necessary when some states are occluded; these tend to converge slower in the absence of a guiding gradient.</a>
<a href="#4" id="4">In the presence of occlusion, this equation may not be computable as the proportion of interacting states may be unknown to L. Instead, we empirically compute the state-visitation frequencies by projecting in the full environment the policy under consideration for each robot for a large number of time steps while utilizing the equilibrium behavior {a mathematical formula} Σ e when the robots interact.</a>
<a href="#5" id="5">The remaining probability mass, {a mathematical formula}1 − TI(s,a, Ψ (s,a)) is then distributed according to some error model, such as uniformly among all other states or perhaps to the intended outcomes of other actions.</a>
<a href="#6" id="6">This approach requires that Ψ is known to the learning agent.</a>
<a href="#7" id="7">Despite this, not knowing the probability of the expert reaching its intended state is a significant relaxation of the requirement by existing IRL methods that the full stochastic transition function inclusive of all probabilities be known.</a>
<a href="#8" id="8">From this, we obtain the probabilities of transitioning to the intended state given the previous state and action, denoted by {a mathematical formula}q Ψ (s,a), as simply the proportion of times the intended state is observed as the next state in the trajectory:{a mathematical formula} where {a mathematical formula} Δ ( ⋅ , ⋅ ) is the indicator function that is equal to 1 when its two arguments are equal, otherwise 0.</a>
<a href="#9" id="9">Nevertheless, these actions map to feature variables in {a mathematical formula}TI.</a>
<a href="#10" id="10">As some of these features are factors in observed actions, we may obtain (partially) informed transition distributions for the unseen actions under the maximum entropy principle.</a>
<a href="#11" id="11">The mass {a mathematical formula}1 − TI(s,a, Ψ (s,a)) could be distributed according to many possible models: uniformly across all next states excepting the intended state; to a dedicated error state; or among the intended states that would result due to performing actions other than a from s. While one could be chosen based on knowledge of the agent or robot being modeled, a general way is to choose the most likely model given the data on observed unintended transitions.</a>
<a href="#12" id="12">High occlusion presents a difficult challenge for this method: instead of randomly missing some data, much of the trajectory is missing, and furthermore the patrollers do not explore all states and actions resulting in the uniform probability distribution being assigned for most transitions.</a>
<a href="#13" id="13">As a result, runs which used DBN{a mathematical formula}EM could not predict the patrollers accurately and most timed out from never finding a valid path to the goal state (100% under occlusion, 90% with no occlusion).</a>
<a href="#14" id="14">Significant milestones in this rapidly emerging research area include modeling the reward function as a linear combination of features [1], utilizing the principle of maximum entropy to eliminate bias from the learned reward function [37], [8], using a Bayesian framework [32], and employing the structure of the MDP to help learn under noisy feature functions [8].</a>
<a href="#15" id="15">Our technique, mIRL{a mathematical formula}/T ⁎ +Int, is similar to Bard's approach but has several key differences: rather than discrete states we use random variables whose values are unknown during each transition, the presence of occlusion, and our novel extension to learning transition probabilities.</a>
</body>
</html>