<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:446">
</head>
<body bgcolor="white">
<a href="#0" id="0">We now consider a single linear layer with k output units{a mathematical formula} In this case, dropout applied to input units is slightly different from dropout applied to the connections.</a>
<a href="#1" id="1">Dropout applied to the input units leads to the random variables{a mathematical formula} whereas dropout applied to the connections leads to the random variables{a mathematical formula} In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectation and the independence assumption.</a>
<a href="#2" id="2">When dropout is applied to the input units, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula}</a>
<a href="#3" id="3">When dropout is applied to the connections, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula} Note the difference in covariance between the two models.</a>
<a href="#4" id="4">When dropout is applied to the connections, {a mathematical formula}Si and {a mathematical formula}Sl are entirely independent.</a>
<a href="#5" id="5">Thus in summary with any distribution{a mathematical formula}P(N)over all possible sub-networks{a mathematical formula}N, including the case of independent but not identically distributed input unit selector variables{a mathematical formula} Δ iwith probability{a mathematical formula}pi, the NWGM is simply obtained by applying the logistic function to the expectation of the linear input S. In the case of independent but not necessarily identically distributed selector variables{a mathematical formula} Δ i, each with a probability{a mathematical formula}piof being equal to one, the expectation of S can be computed simply by keeping the same overall network but multiplying each weight{a mathematical formula}wiby{a mathematical formula}piso that{a mathematical formula}E(S)= ∑ i=1npiwiIi.</a>
<a href="#6" id="6">For m sufficiently large, by the central limit theorem{sup:2} the means of these quantities are approximately normal with:{a mathematical formula} If these standard deviations are small enough, which is the case for instance when m is large, then Σ can be well approximated by a linear function with slope t over the corresponding small range.</a>
<a href="#7" id="7">In general, the results show how well the {a mathematical formula}NWGM(Oil) and the deterministic values {a mathematical formula}Wil approximate the true expectation {a mathematical formula}E(Oil) in each layer, both at the beginning and the end of learning, and how the deviations can roughly be viewed as small, approximately Gaussian, fluctuations well within the bounds derived in Section 8.</a>
<a href="#8" id="8">This is because it can be viewed as a form of on-line gradient descent with respect to the error function{a mathematical formula} of the true ensemble, where {a mathematical formula}t(I) is the target value for input I and {a mathematical formula}fw is the elementary error function, typically the squared error in regression, or the relative entropy error in classification, which depends on the weights w. In the case of dropout, the probability {a mathematical formula}P(N) of the network {a mathematical formula}N is factorial and associated with the product of the underlying Bernoulli selector variables.</a>
</body>
</html>