<html>
<head>
<meta name="TextLength" content="SENT_NUM:18, WORD_NUM:458">
</head>
<body bgcolor="white">
<a href="#0" id="0">We assume that the domain can be modeled as an MDP {a mathematical formula}D= 〈 S,A,f,r 〉 , where {a mathematical formula}S can be either discrete or continuous, {a mathematical formula}A is finite, and f and r are stochastic and unknown.</a>
<a href="#1" id="1">The MDP D is the one for which an RL method would learn a policy to act in the real domain.</a>
<a href="#2" id="2">Furthermore, we expect the designer to create a model of D which we will denote as {a mathematical formula}Dm= 〈 Sm,A,fm 〉 .</a>
<a href="#3" id="3">This model may or may not include the reward function.</a>
<a href="#4" id="4">In the rest of this article, we will only use models that do not.</a>
<a href="#5" id="5">The model {a mathematical formula}Dm is the one on which a planning algorithm would compute a plan to execute in the real domain.</a>
<a href="#6" id="6">A planning algorithm cannot plan on D since the transition function of D is unknown (and so is the reward function).</a>
<a href="#7" id="7">We also require the designer to specify a mapping {a mathematical formula}o:S → Sm which for every state in {a mathematical formula}S returns a corresponding state in {a mathematical formula}Sm.</a>
<a href="#8" id="8">In order to make sure that what is computed on the model is indeed executable in the environment, the mapping function o has to preserve the applicability of actions, that is:{a mathematical formula} where by {a mathematical formula}A(s) we denote the subset of actions of {a mathematical formula}A applicable in state s (cf.</a>
<a href="#9" id="9">Section 2.1).</a>
<a href="#10" id="10">Through our method we will define a new MDP {a mathematical formula}Dr (where the subscript stands for reduced), computed by planning over {a mathematical formula}Dm, that models the same domain as D but is a reduced version of it.</a>
<a href="#11" id="11">All minimal plans that pass the previous filtering tests are merged into a partial policy.</a>
<a href="#12" id="12">Recall that a policy in an MDP is a function that returns the action to execute for each state (cf.</a>
<a href="#13" id="13">Section 2.1).</a>
<a href="#14" id="14">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#15" id="15">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#16" id="16">The function {a mathematical formula}o:S → Sm is the mapping from the states of the MDP to the states of the model introduced at the beginning of this section.</a>
<a href="#17" id="17">This partial policy is used in the reinforcement learning phase to define an MDP on which the agent learns the optimal policy.</a>
</body>
</html>