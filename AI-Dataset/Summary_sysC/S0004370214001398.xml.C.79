<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:382">
</head>
<body bgcolor="white">
<a href="#0" id="0">We present a novel framework of anticipatory action selection for human – robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty.</a>
<a href="#1" id="1">Experimental results using real data in a simulated environment show the importance of anticipatory action selection, and that POMDPs are suitable to formulate the anticipatory action selection problem by taking into account the uncertainties in prediction.</a>
<a href="#2" id="2">In the table tennis scenario, this Intention-Driven Dynamics Model (IDDM) leads to an online algorithm that, given a time series of observations, continually predicts the human player's intended target, i.e., where the human intends to shoot the ball [53], as shown in Fig.</a>
<a href="#3" id="3">Furthermore, the outcome of executing a selected action, e.g., the robot's success of returning the ball to the opponent's court with the chosen hitting movement, is not deterministic as the underlying dynamics of the robot arm are often too complicated to be modeled precisely at high speed.</a>
<a href="#4" id="4">The states evolve following a Markov transition model, governed by {a mathematical formula}P, where {a mathematical formula}P(s ′ |s,a) represents the probability of going to state {a mathematical formula}s ′ from state s when taking action a.</a>
<a href="#5" id="5">The observations are generated from the states following the observation model Ω , where {a mathematical formula} Ω (z|s) is the probability of observing z in state s. The reward function {a mathematical formula}R(s,a) represents the expected immediate reward obtained for taking action a in state s.</a>
<a href="#6" id="6">We, hence, reuse the estimated value function for stopping actions by LSPI, and focus on the value function for the waiting action {a mathematical formula}Q( Θ t,at=waiting), given by the expected future reward{a mathematical formula} with respect to the subsequent belief state {a mathematical formula} Θ t+1.</a>
<a href="#7" id="7">Nevertheless, consider the online planning that finds the optimal action{a mathematical formula} where one only needs to know if the waiting action leads to higher expected reward than all the stopping actions, rather than the actual expected reward of waiting.</a>
<a href="#8" id="8">We can terminate the particle projection routine if the expected reward of waiting is very likely to be higher or lower than that of the optimal stopping action {a mathematical formula}maxa ≠ 0 ⁡ Q( Θ t,a).</a>
</body>
</html>