<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:470">
</head>
<body bgcolor="white">
<a href="#0" id="0">When dropout is applied to the connections, under similar independence assumptions, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#1" id="1">This formula can be applied recursively across the entire network.</a>
<a href="#2" id="2">Note again that although the expectation {a mathematical formula}E(Sih) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the previous layers ({a mathematical formula}l<h) matter.</a>
<a href="#3" id="3">Therefore it is also the expectation taken over only all the induced subnetworks of node i (corresponding to all the ancestors of node i).</a>
<a href="#4" id="4">Furthermore, using these expectations, all the covariances can also be computed recursively from the input layer to the output layer using a similar analysis to the one given above for the case of dropout applied to the units of a general linear network.</a>
<a href="#5" id="5">In the case of a single output layer of k logistic functions, the network computes k linear sums {a mathematical formula}Si= ∑ j=1nwijIj for {a mathematical formula}i=1, … ,k and then k outputs of the form{a mathematical formula} The dropout procedure produces a subnetwork {a mathematical formula}M=(N1, … ,Nk) where {a mathematical formula}Ni here represents the corresponding sub-network associated with the i-th output unit.</a>
<a href="#6" id="6">Similar calculations can be made for deep linear networks.</a>
<a href="#7" id="7">For instance, the previous calculation can be adapted immediately to the top layer of a linear network with T layers with{a mathematical formula} and{a mathematical formula} which corresponds again to an adaptive quadratic regularization term in {a mathematical formula}wijTl, with a coefficient associated for each input with the corresponding variance of the dropout pre-synaptic neuron {a mathematical formula}Var( Δ jlOjl).</a>
<a href="#8" id="8">For a single sigmoidal unit something quite similar, but not identical holds.</a>
<a href="#9" id="9">With a sigmoidal unit {a mathematical formula}O= Σ (S)=1/(1+ce − Λ S), one typically uses the relative entropy error{a mathematical formula} We can again consider two error functions {a mathematical formula}EENS and {a mathematical formula}ED.</a>
<a href="#10" id="10">Note that while in the linear case {a mathematical formula}EENS is exactly equal to the ensemble error, in the non-linear case we use {a mathematical formula}EENS to denote the error of deterministic network which approximates the ensemble network.</a>
<a href="#11" id="11">Consider unit i in the output layer T receiving a connection from unit j in a layer l (typically {a mathematical formula}l=T − 1) with weight {a mathematical formula}wijTl.</a>
<a href="#12" id="12">The gradient of the error function in the dropout network is given by{a mathematical formula} using the notation of Section 9.5: {a mathematical formula}SijTl=Sil − wijTl Δ jlOjl.</a>
<a href="#13" id="13">Using a first order Taylor expansion to separate out independent terms gives:{a mathematical formula} We can now take the expectation of the gradient{a mathematical formula} Now, using the NWGM approximation {a mathematical formula}E( Σ (SijTl)) ≈ Σ (E(SijTl))= Σ (UijTl)=WijTl ≈ WiT − Σ ′ (UiT)wijTlpjlWjl{a mathematical formula} which has the form{a mathematical formula} where A has the complex expression given by Eq.</a>
</body>
</html>