<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:499">
</head>
<body bgcolor="white">
<a href="#0" id="0">This article presents experiments demonstrating that the combination of these two intrinsic rewards enables the algorithm to learn an accurate model of a domain with no external rewards and that the learned model can be used afterward to perform tasks in the domain.</a>
<a href="#1" id="1">In addition, the experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone.</a>
<a href="#2" id="2">Stout and Barto [13] extend this work to the case where the agent is learning multiple tasks and must balance the intrinsic rewards that promote the learning of each skill.</a>
<a href="#3" id="3">This algorithm requires an external reward, as the intrinsic reward is speeding up the learning of the task defined by the external reward function.</a>
<a href="#4" id="4">For two different algorithms and tasks, they search over a broad set of possible task and agent specific intrinsic rewards and find rewards that make the agent learn faster than if it solely used external rewards.</a>
<a href="#5" id="5">The Policy Gradient Reward Design algorithm (pgrd) learns the best intrinsic rewards on-line for cases where the true reward function is given and the agent is limited in some way [15], [16], [17].</a>
<a href="#6" id="6">pgrd uses its knowledge of the true reward function to calculate the gradient of intrinsic rewards to agent return.</a>
<a href="#7" id="7">Using this gradient, intrinsic rewards are found that enable the best agent performance given its limitations.</a>
<a href="#8" id="8">For example, if the agent has a limited planning depth, then even with the true reward function, it cannot perform well.</a>
<a href="#9" id="9">This generalization enables the model to make predictions about unseen or infrequently visited state-actions, and therefore the agent does not have to visit every state-action.</a>
<a href="#10" id="10">Thus, texplore-vanir approaches the model learning task as a supervised learning problem, with the current state and action as the input, and the next state as the output to be predicted.</a>
<a href="#11" id="11">texplore's model learning algorithm starts by calculating the relative change in the state ({a mathematical formula}srel), then it updates the model for each feature with the new transition and updates the reward model.</a>
<a href="#12" id="12">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#13" id="13">Finally, in a task with external rewards, texplore-vanir can use its intrinsic rewards to speed up learning with respect to an algorithm using only external rewards.</a>
<a href="#14" id="14">It is important to note that the best intrinsic rewards are dependent on the learning algorithm and the domain.</a>
<a href="#15" id="15">In the light world domain, by the time the algorithm has determined error is improving in a region, the agent has already learned a model of that region and no longer needs to explore there.</a>
<a href="#16" id="16">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
</body>
</html>