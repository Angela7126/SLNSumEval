<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:473">
</head>
<body bgcolor="white">
<a href="#0" id="0">We now consider a single linear layer with k output units{a mathematical formula} In this case, dropout applied to input units is slightly different from dropout applied to the connections.</a>
<a href="#1" id="1">Dropout applied to the input units leads to the random variables{a mathematical formula} whereas dropout applied to the connections leads to the random variables{a mathematical formula} In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectation and the independence assumption.</a>
<a href="#2" id="2">When dropout is applied to the input units, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula}</a>
<a href="#3" id="3">When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or the weights, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#4" id="4">This formula can be applied recursively across the entire network, starting from the input layer.</a>
<a href="#5" id="5">When dropout is applied to the connections, under similar independence assumptions, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#6" id="6">This formula can be applied recursively across the entire network.</a>
<a href="#7" id="7">Note again that although the expectation {a mathematical formula}E(Sih) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the previous layers ({a mathematical formula}l<h) matter.</a>
<a href="#8" id="8">Therefore it is also the expectation taken over only all the induced subnetworks of node i (corresponding to all the ancestors of node i).</a>
<a href="#9" id="9">Furthermore, using these expectations, all the covariances can also be computed recursively from the input layer to the output layer using a similar analysis to the one given above for the case of dropout applied to the units of a general linear network.</a>
<a href="#10" id="10">Finally, for the most general case, the same line of reasoning, shows that the dropout ensemble approximation can be used with any continuous, piece-wise twice differentiable, transfer function provided the following properties are satisfied: (1) the curvature of each piece must be small; (2){a mathematical formula} Σ Smust be small relative to the curvature of each piece.</a>
<a href="#11" id="11">The proof is similar to the previous case, interchanging x and {a mathematical formula}1 − x.</a>
<a href="#12" id="12">Throughout the rest of this article, we let {a mathematical formula}Wil= Σ (Uil) denote the deterministic variables of the dropout approximation (or ensemble network) with{a mathematical formula} in the case of dropout applied to the nodes.</a>
<a href="#13" id="13">The main question we wish to consider is whether {a mathematical formula}Wil is a good approximation to {a mathematical formula}E(Oil) for every input, every layer l, and any unit i.</a>
<a href="#14" id="14">For neurons whose activities are close to 0 or 1, and thus in general for neurons towards the end of learning, these two bounds are similar to each other.</a>
<a href="#15" id="15">This is not the case at the beginning of learning when, with very small weights and a standard logistic transfer function, {a mathematical formula}Wil=0.5 and {a mathematical formula}Var(Oil) ≈ 0 (Fig.</a>
</body>
</html>