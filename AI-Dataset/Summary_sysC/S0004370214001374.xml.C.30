<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:221">
</head>
<body bgcolor="white">
<a href="#0" id="0">Second, we want to obtain an accurate estimate of the expected reward {a mathematical formula}Rs Ω for a given context-policy parameter pair to avoid the bias in the sample-based REPS formulation.</a>
<a href="#1" id="1">Due to the recent success of using Gaussian Process models to reduce the model bias when learning complex system dynamics [9], we use GP models to learn the forward models of the robot and its environment.</a>
<a href="#2" id="2">Therefore, our method is called Gaussian Process Relative Entropy Policy Search (GPREPS).</a>
<a href="#3" id="3">For data collection, we observe the context {a mathematical formula}s[i] and sample the parameters {a mathematical formula} Ω [i] using the upper-level policy {a mathematical formula} Π ( Ω |s[i]).</a>
<a href="#4" id="4">Subsequently, we use the lower-level control policy {a mathematical formula} Π (x; Ω [i]) to obtain the trajectory sample {a mathematical formula} Τ [i].</a>
<a href="#5" id="5">Using the learned GP forward model, we need to predict the expected reward{a mathematical formula} for a given parameter vector Ω executed in context s. The expectation over the trajectories is now estimated using the learned forward models.</a>
<a href="#6" id="6">We compared GPREPS to model-free REPS and CrKR [20], a state-of-the-art model-free contextual policy search method.</a>
<a href="#7" id="7">Furthermore, we evaluated GPREPS without decomposing the experiment, and directly predict the reward {a mathematical formula}Rs Ω with a GP model using the policy parameter Ω and the context s as input (GPREPS (direct)).</a>
</body>
</html>