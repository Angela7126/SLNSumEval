<html>
<head>
<meta name="TextLength" content="SENT_NUM:33, WORD_NUM:595">
</head>
<body bgcolor="white">
<a href="#0" id="0">Hierarchical Task Network (HTN) planning is an effective yet knowledge intensive problem-solving technique.</a>
<a href="#1" id="1">It requires humans to encode knowledge in the form of methods and action models.</a>
<a href="#2" id="2">Methods describe how to decompose tasks into subtasks and the preconditions under which those methods are applicable whereas action models describe how actions change the world.</a>
<a href="#3" id="3">Encoding such knowledge is a difficult and time-consuming process, even for domain experts.</a>
<a href="#4" id="4">In this paper, we propose a new learning algorithm, called HTNLearn, to help acquire HTN methods and action models.</a>
<a href="#5" id="5">HTNLearn receives as input a collection of plan traces with partially annotated intermediate state information, and a set of annotated tasks that specify the conditions before and after the tasks' completion.</a>
<a href="#6" id="6">In addition, plan traces are annotated with potentially empty partial decomposition trees that record the processes of decomposing tasks to subtasks.</a>
<a href="#7" id="7">HTNLearn outputs are a collection of methods and action models.</a>
<a href="#8" id="8">HTNLearn first encodes constraints about the methods and action models as a constraint satisfaction problem, and then solves the problem using a weighted MAX-SAT solver.</a>
<a href="#9" id="9">HTNLearn can learn methods and action models simultaneously from partially observed plan traces (i.e., plan traces where the intermediate states are partially observable).</a>
<a href="#10" id="10">We test HTNLearn in several HTN domains.</a>
<a href="#11" id="11">The experimental results show that our algorithm HTNLearn is both effective and efficient.</a>
<a href="#12" id="12">Decomposition constraints and task constraints enhance the learned HTN models using statistical information from the partial decomposition trees</a>
<a href="#13" id="13">These constraints ensure that the learned HTN models are consistent with the given partial decomposition trees.</a>
<a href="#14" id="14">Specifically, the method structures indicated in the partial decomposition trees should be reflected in the learned methods.</a>
<a href="#15" id="15">For example, given the partial decomposition tree in Fig.</a>
<a href="#16" id="16">3, the method structure 〈 (superpose ?x ?y), [(pickup ?x), stack(?x ?y)] 〉 should be learned, where ?x is a variable that is instantiated by “ A ” , and ?y is a variable that is instantiated by “ B ” in Fig.</a>
<a href="#17" id="17">3.</a>
<a href="#18" id="18">We build a set of constraints, called partialness constraints, to ensure these method structures are learned.</a>
<a href="#19" id="19">We would like to test how accuracy changes when each observed intermediate state is not complete.</a>
<a href="#20" id="20">We set the percentage of intermediate states as 1/3, and vary the percentage of propositions in each state from 20% to 100%.</a>
<a href="#21" id="21">We run HTNLearn five times and calculate an average of accuracies.</a>
<a href="#22" id="22">From Fig.</a>
<a href="#23" id="23">7, we can see that for all cases, when percentage of partial structures increases, {a mathematical formula}Accs increases correspondingly.</a>
<a href="#24" id="24">This is reasonable, since the larger the number of partial structures, the more correct structures can be extracted directly from the inputted partial decomposition trees.</a>
<a href="#25" id="25">In addition, {a mathematical formula}Accc also becomes higher when the percentage gets larger.</a>
<a href="#26" id="26">This is because the structure information would provide valuable information for building decomposition constraints DC, which may help learn the HTN conditions.</a>
<a href="#27" id="27">We observed that the accuracy of the learned structures {a mathematical formula}Accs was not less than 0.8 for all the cases when the percentage was not less than 50%.</a>
<a href="#28" id="28">This reveals our learning algorithm HTNLearn could indeed help acquire structure knowledge, which could be provided for people to build HTN models.</a>
<a href="#29" id="29">We further test the human efforts saved by HTNLearn in Section 5.3.3.</a>
<a href="#30" id="30">The number of method structures learned by HTNLearn is not large.</a>
<a href="#31" id="31">To empirically show this fact, we recorded the average numbers (we ran HTNLearn five times to calculate an average) of method structures learned for all domains by setting the percentage of partial structures to be 50%.</a>
<a href="#32" id="32">The results were 13, 15, 18, and 7 method structures learned for the htn-blocks, htn-driverlog, htn-depots, and htn-business domains, respectively.</a>
</body>
</html>