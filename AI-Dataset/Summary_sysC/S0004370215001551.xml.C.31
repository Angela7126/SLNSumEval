<html>
<head>
<meta name="TextLength" content="SENT_NUM:26, WORD_NUM:531">
</head>
<body bgcolor="white">
<a href="#0" id="0">We propose an efficient, online system that tries to transfer old knowledge, but at the same time evaluates new options to see if they work better.</a>
<a href="#1" id="1">The agent gathers experience during its lifetime and enters a new environment equipped with expectations on how different aspects of the world affect the outcomes of the agent's actions.</a>
<a href="#2" id="2">The main idea is to allow an agent to collect a library of world models or representations, called views, that it can consult to focus attention in a new task.</a>
<a href="#3" id="3">In this paper, we concentrate on approximating the dynamics or transition model.</a>
<a href="#4" id="4">The feedback or reward model library can be learned in an analogous fashion.</a>
<a href="#5" id="5">Effective utilization of the library of world models allows the agent to capture the transition dynamics of the new environment quickly; this should lead to a jumpstart in learning and faster convergence to a near optimal policy.</a>
<a href="#6" id="6">A main challenge is in learning to select a proper view for a new task in a new environment, without any predefined mapping strategies.</a>
<a href="#7" id="7">Fig.</a>
<a href="#8" id="8">2c shows an example of this decomposition.</a>
<a href="#9" id="9">The agent uses the feature function f to identify the relevant features, and then uses both state attributes and features to predict the action effects.</a>
<a href="#10" id="10">We also assume that the effect e and current state s determine the next state {a mathematical formula}s ′ , thus {a mathematical formula}P(s ′ |e,s) is either 0 or 1.</a>
<a href="#11" id="11">This defines the semantic meaning of the effect which is assumed to be known by the agent.</a>
<a href="#12" id="12">The remaining task is to learn {a mathematical formula}P(e|s,a)=P(e|x(s),a), where {a mathematical formula}x(s)=(s,f(s)) is a vector containing both the state attributes and the state features.</a>
<a href="#13" id="13">Assuming the effect e is discrete, learning {a mathematical formula}P(e|x(s),a) is a classification problem.</a>
<a href="#14" id="14">In Section 4 we will show how to solve this problem by using multinomial logistic regression methods.</a>
<a href="#15" id="15">Our main task is to turn transition model learning into the learning of conditional distributions {a mathematical formula}P(E|s,f(s),a) using multinomial logistic regression for which attention to relevant features can be efficiently implemented online via mDAGL.</a>
<a href="#16" id="16">Transferring expectations between same dynamics tasks.</a>
<a href="#17" id="17">To ensure that TES is capable of basic model transfer, we first evaluate it on a simple task.</a>
<a href="#18" id="18">We train and test TES on two environments which have the same dynamics and 200 irrelevant binary features that challenge the agent's ability to learn a compact model for transfer.</a>
<a href="#19" id="19">Fig.</a>
<a href="#20" id="20">4a shows how much the other methods lose to TES in terms of accumulated rewards in the test task.</a>
<a href="#21" id="21">loreRL is an implementation of TES equipped with the view learning algorithm that does not transfer knowledge.</a>
<a href="#22" id="22">fRmax is the factored Rmax in which the network structures of transition models are provided by an oracle [36]; its parameter m is set to be 10 in all the experiments.</a>
<a href="#23" id="23">fEpsG is a heuristic in which the optimistic Rmax exploration of fRmax is replaced by an Ε -greedy strategy ({a mathematical formula} Ε =0.1).</a>
<a href="#24" id="24">The results show that these oracle methods still have to spend time to learn the model parameters, so they gain lower accumulated rewards than TES.</a>
<a href="#25" id="25">This also suggests that the transferred view of TES is likely not only compact but also accurate.</a>
</body>
</html>