<html>
<head>
<meta name="TextLength" content="SENT_NUM:25, WORD_NUM:531">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we concentrate on approximating the dynamics or transition model.</a>
<a href="#1" id="1">The feedback or reward model library can be learned in an analogous fashion.</a>
<a href="#2" id="2">Effective utilization of the library of world models allows the agent to capture the transition dynamics of the new environment quickly; this should lead to a jumpstart in learning and faster convergence to a near optimal policy.</a>
<a href="#3" id="3">A main challenge is in learning to select a proper view for a new task in a new environment, without any predefined mapping strategies.</a>
<a href="#4" id="4">A new reinforcement learning algorithm, logistic regression RL (loreRL), that applies mDAGL on a CMDP to learn the transition models by automatically focusing on the relevant features in the environment; and</a>
<a href="#5" id="5">A key idea of this work is that the agent can represent the world dynamics from its sensory state space in different ways.</a>
<a href="#6" id="6">Such different views correspond to the agent's decisions to focus attention on only some features of the state in order to quickly approximate the state transition function.</a>
<a href="#7" id="7">In other words, a view represents an expectation of the agent about the transition dynamics resulting from one (or several) of its actions in the task environment.</a>
<a href="#8" id="8">In a new task, the agent will select appropriate views to solve the task, and to learn new views if the environment is novel.</a>
<a href="#9" id="9">We address three main technical challenges in this framework: First, the transition model {a mathematical formula}T(S,A,S) is task specific, which is probably a reason why there have not been many studies that transfer the transition model.</a>
<a href="#10" id="10">Second, learning or updating a view or a transition model online in a complex and feature-rich environment is computationally expensive.</a>
<a href="#11" id="11">Third, the view scoring method must be simple to be calculated online, based on feedback from the environment.</a>
<a href="#12" id="12">The view library also needs to be efficiently updated.</a>
<a href="#13" id="13">Our main task is to turn transition model learning into the learning of conditional distributions {a mathematical formula}P(E|s,f(s),a) using multinomial logistic regression for which attention to relevant features can be efficiently implemented online via mDAGL.</a>
<a href="#14" id="14">Environment 1 and Environment 3 are deliberately designed so that the robot should learn its views (transition model) based on the blue marks.</a>
<a href="#15" id="15">The transition dynamics in these two environments are very similar.</a>
<a href="#16" id="16">However, the two environments are also different (in terms of irrelevant features): the blue balls, the death places, and the green marks are at different locations.</a>
<a href="#17" id="17">We will explain the features in more detail shortly.</a>
<a href="#18" id="18">Environment 2 is very different from Environment 1 and Environment 3.</a>
<a href="#19" id="19">It is designed so that the robot should learn its views based on the green marks instead of the blue ones.</a>
<a href="#20" id="20">2.</a>
<a href="#21" id="21">LWT: the robot has first experienced Environment 2, and then uses that knowledge (transition model) to learn the action policy and run on Environment 3.</a>
<a href="#22" id="22">The robot does not update its knowledge of the transition model in the new environment.</a>
<a href="#23" id="23">Our work addresses two main challenges in transfer learning in RL: learning the transition (or reward) models, and transferring the relevant knowledge to perform new tasks in possibly different, dynamic environments.</a>
<a href="#24" id="24">In this section, we compare our work with existing efforts in learning transition models and those that focus on transfer learning in RL.</a>
</body>
</html>