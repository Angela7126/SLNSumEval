<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:526">
</head>
<body bgcolor="white">
<a href="#0" id="0">The main challenges of applying RL in robotic domains include the multidimensional state and action spaces that incur high computational costs, the physical constraints that make acting in the real world much more difficult than in simulated worlds, the uncertainty due to partial observability of the physical environments and inherent noise in the sensor measurements, and the difficulty in tailoring the feedback or reward functions to guide intelligent behavior of the robots [7].</a>
<a href="#1" id="1">A task or a problem M for an intelligent agent or robot to solve, e.g., navigate to the master bedroom or fetch a cup from the kitchen table, is typically modeled as a Markov decision process (MDP) defined by a tuple {a mathematical formula}M=(S,A,T,R), where S is a set of states; A is a set of actions; {a mathematical formula}T:S×A×S → [0,1] is a transition function or model, such that {a mathematical formula}T(s,a,s ′ )=P(s ′ |s,a) indicates the probability of transiting to a state {a mathematical formula}s ′ upon taking an action a in state s; {a mathematical formula}R:S×A → R is a reward function or model indicating immediate expected reward after an action a is taken in state s. In RL, the state-action space {a mathematical formula}S×A defines the domain of the task, the transition model T and the reward model R define the objective of the task [21]; the transition model T and (sometimes) the reward model R are unknown.</a>
<a href="#2" id="2">We address three main technical challenges in this framework: First, the transition model {a mathematical formula}T(S,A,S) is task specific, which is probably a reason why there have not been many studies that transfer the transition model.</a>
<a href="#3" id="3">At each time step, a random action a is chosen with a small probability Ε , but otherwise we calculate the optimal policy Π for an MDP with the transition model {a mathematical formula}T(W) is based on the current effect predictors.</a>
<a href="#4" id="4">We aim to demonstrate that the “ single expectation ” model-based RL, loreRL, can a) learn views that generalize and approximate the transition model to achieve fast convergence to near optimal policy, and b) with feature selection, perform well in complex, feature rich environments.</a>
<a href="#5" id="5">However, the important features are manually selected to aggregate information from similar states for action effect prediction, as compared with our focus on learning those features automatically.</a>
<a href="#6" id="6">Most of these methods attempt to transfer structure and experiential knowledge in the forms of low level knowledge such as task instances, action-value pairs, full policies, full task models, prior distributions, or high level knowledge such as relevant action sets, partial policies, rules, relevant feature sets, or proto-value functions.</a>
<a href="#7" id="7">While they are able to show many advantages of their approach with the offline learning fitted Q-iteration algorithm [59] on several experiments, a potential drawback is the need of a large number of samples in all the source tasks, and the strong assumptions on the similarity amongst the different tasks.</a>
<a href="#8" id="8">Instead of keeping multiple models for transfer, Liu and Stone [60] learn a mapping function between actions and features in the source and the target tasks, but need to assume the availability of the structures of the QDBN in the pair of tasks.</a>
</body>
</html>