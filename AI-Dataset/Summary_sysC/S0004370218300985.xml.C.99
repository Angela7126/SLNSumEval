<html>
<head>
<meta name="TextLength" content="SENT_NUM:23, WORD_NUM:751">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using the chain rule, it is easy to see that the backpropagated error satisfies the recurrence relation:{a mathematical formula} with the boundary condition:{a mathematical formula} Thus in short the errors are propagated backwards in an essentially linear fashion using the transpose of the forward matrices, hence the symmetry of the weights, with a multiplication by the derivative of the corresponding forward activations every time a layer is traversed.</a>
<a href="#1" id="1">Under these assumptions, as shown in [5], the expected value of the activity of each unit in the backward pass is exactly given by the standard BP equations and equal to {a mathematical formula}Bih for unit i in layer h. In other words, standard backpropagation can be viewed as computing the exact average over all backpropagation processes implemented on all the stochastic realizations of the backward network under the three forms of noise described above.</a>
<a href="#2" id="2">Thus we can reverse this argument and consider that RBP approximates this average or BP by averaging over the first two kinds of noise, but not the third one where, instead of averaging, a random realization of the weights is selected and then fixed at all epochs.</a>
<a href="#3" id="3">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1 and {a mathematical formula}a2 during learning:{a mathematical formula} Note that the dynamic of {a mathematical formula}P=a1a2 is given by:{a mathematical formula} The error is given by:{a mathematical formula} and:{a mathematical formula} the last equality requires {a mathematical formula}ai ≠ 0.</a>
<a href="#4" id="4">Except for trivial cases (associated with{a mathematical formula}c1=0or{a mathematical formula}c2=0), starting from any initial conditions the system in Equation(36)converges to a fixed point, corresponding to a global minimum of the quadratic error function.</a>
<a href="#5" id="5">By expanding and simplifying Equation (42), it is easy to see that the leading term of Q is negative and given by {a mathematical formula} Β c22/(16c12).</a>
<a href="#6" id="6">Therefore, using Theorem 1, for any initial conditions {a mathematical formula}a1(0), {a mathematical formula}a1(t) converges to a finite fixed point.</a>
<a href="#7" id="7">Since {a mathematical formula}a2 is a quadratic function of {a mathematical formula}a1 it also converges to a finite fixed point, and similarly for {a mathematical formula}a3.</a>
<a href="#8" id="8">Thus in the general case the system always converges to a global minimum of the error function satisfying {a mathematical formula} Α − Β P=0.</a>
<a href="#9" id="9">Thus every {a mathematical formula}ai can be expressed as a polynomial function of {a mathematical formula}a1 of degree {a mathematical formula}2i − 1, containing only even terms:{a mathematical formula} and:{a mathematical formula} By substituting these relationships in the equation for the derivative of {a mathematical formula}a1, we get {a mathematical formula}da1/dt=Q(a1) where Q is a polynomial with an odd degree n given by:{a mathematical formula} Furthermore, from Equation (50) it can be seen that leading coefficient is negative therefore, using Theorem 1, for any set of initial conditions the system must converge to a finite fixed point.</a>
<a href="#10" id="10">In the general case where the weights in the learning channel are non-zero, the critical points are given by the surface {a mathematical formula} Α − Β P=0 and correspond to global optima.</a>
<a href="#11" id="11">These critical points are independent of the weights in the learning channel.</a>
<a href="#12" id="12">Additional critical points for the product {a mathematical formula}P= ∑ iaibi are given by the surface {a mathematical formula} ∑ iai2+bici=0 which depends on the weights in the learning channel.</a>
<a href="#13" id="13">Now let us write:{a mathematical formula} Then the equation {a mathematical formula}B+BT − AAT=K is equivalent to the following system:{a mathematical formula} Since t is small, {a mathematical formula}A ˜ (t) should be sufficiently close to aI.</a>
<a href="#14" id="14">From the second equation of the system above, we have {a mathematical formula}d ˜ =a.</a>
<a href="#15" id="15">If {a mathematical formula}b ˜ =0, then we conclude from the first equation of the same system that {a mathematical formula}a ˜ =a, and hence {a mathematical formula}A ˜ (t)=aI.</a>
<a href="#16" id="16">This implies that {a mathematical formula}(A(t),B(t))=(A,B).</a>
<a href="#17" id="17">So in this case {a mathematical formula}E=0.</a>
<a href="#18" id="18">Things are more complicated when {a mathematical formula}b ˜ ≠ 0.</a>
<a href="#19" id="19">We first assume that {a mathematical formula}a ≠ − 1.</a>
<a href="#20" id="20">In this case, from the third equation of the system above, we have {a mathematical formula}a ˜ − 1d ˜ − 1+d ˜ =0.</a>
<a href="#21" id="21">Since we already have {a mathematical formula}d ˜ =a ≠ 1, for sufficiently small t, {a mathematical formula}a ˜ = − d ˜ − 2= − a − 2, which is distinct from a.</a>
<a href="#22" id="22">Thus in this case {a mathematical formula}b ˜ must be zero.</a>
</body>
</html>