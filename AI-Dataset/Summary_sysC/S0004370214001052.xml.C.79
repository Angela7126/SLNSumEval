<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:828">
</head>
<body bgcolor="white">
<a href="#0" id="0">Additionally we show that enhancements developed for games of perfect information (where the state is fully observable to all players and state transitions are deterministic) can also be effective in games of imperfect information (where the state is partially observable with different observations for different players, and state transitions may be stochastic).</a>
<a href="#1" id="1">Although the actual methods used are different, the idea of a learning system acquiring knowledge over its lifetime as it is confronted by different problems is similar to the idea of a tree search algorithm transferring knowledge from one part of the game tree to another over the “ lifetime ” of a single search.</a>
<a href="#2" id="2">In this paper we present a new enhancement, EPisodic Information Capture and reuse (EPIC), that was designed by considering correlation between states in the card game Dou Di Zhu.</a>
<a href="#3" id="3">Dou Di Zhu has an episodic structure, where a game consists of a sequence of somewhat independent rounds, and EPIC is designed to correlate states in analogous positions within different episodes.</a>
<a href="#4" id="4">This is achieved by building a tree of information sets (sets of states indistinguishable from one player's view point) rather than individual states, and dealing with the increased branching factor by restricting each MCTS iteration to a random determinization (a state sampled at random from the current information set).</a>
<a href="#5" id="5">In this paper we use the MO-ISMCTS version of the algorithm, which deals with games that have partially observable moves by constructing a separate search tree (a “ projection ” of the underlying game tree) to reflect each player's observation of the game.</a>
<a href="#6" id="6">A game can be described as a sequential decision problem, where the players collectively choose a path through {a mathematical formula}(S, Λ ) from {a mathematical formula}s0 to a terminal state.</a>
<a href="#7" id="7">A move history for player i from state s is a sequence of moves from player i's point of view,{a mathematical formula} 〈 [a1] ⌣ i, … ,[an] ⌣ i 〉 ∈ Mi ⁎ , where{a mathematical formula} 〈 a1, … ,an 〉 is an action history from s. Denote the set of all move histories for player i from s by{a mathematical formula}Hi ⌣ (s), and the set of all move histories for all players by{a mathematical formula}H ⌣ (s).</a>
<a href="#8" id="8">If{a mathematical formula}h= 〈 a1, … ,an 〉 is an action history then the corresponding move history from player i's point of view is denoted{a mathematical formula}[h] ⌣ i.</a>
<a href="#9" id="9">Let{a mathematical formula} Ρ = Ρ (f(s, 〈 a1, … ,an − 1 〉 )), so Ρ is the player who played the last action{a mathematical formula}anin the history.</a>
<a href="#10" id="10">Then the move history from player Ρ 's point of view is denoted by omission of the player number, i.e.</a>
<a href="#11" id="11">This function takes three arguments (the current move history, the current mapping of records to information, and the legal action set for the current state) and returns a probability distribution over the action set.</a>
<a href="#12" id="12">This function takes two arguments (the root game state and the current move history) and maps them to a sequence of (record, capture context) pairs which are to be updated following a playout.</a>
<a href="#13" id="13">This function takes three arguments (the current information for a record, the capture context specified by the capture function, and the reward vector from the simulation) and returns the new information for the record following a playout.</a>
<a href="#14" id="14">Each step of the playout uses the policy function Α to choose an action a, depending on the current move history {a mathematical formula}[h] ⌣ Ρ (s) for the player about to act from state s, the current information mapping Θ , and the set of available actions {a mathematical formula}A(s) (line 11).</a>
<a href="#15" id="15">After the playout has reached a terminal state, the capture function is applied to the root determinization {a mathematical formula}sroot and the terminal history h to obtain the sequence of (record, context) pairs to be updated (line 16).</a>
<a href="#16" id="16">The idea is to maintain average reward statistics for each action independently of where it occurs in the game tree, and use these statistics to bias the simulation policy.</a>
<a href="#17" id="17">In [50] we show that NAST works for the three imperfect information games studied in the present paper (Section 7), with {a mathematical formula}n=2 typically giving the strongest performance.</a>
<a href="#18" id="18">All ways of combining ICARUSes make use of information from the baseline definition (Section 4.2) in some way, whether to determine the current stage (selection, expansion or simulation) of the playout or to vary the combination coefficient.</a>
<a href="#19" id="19">This ensures that the baseline tree policy can tailor itself to the context of the current episode if that context matters, whilst the simulation policy that uses episode information but ignores context is still likely to be much stronger than a random policy.</a>
<a href="#20" id="20">If EPIC is combined with the baseline algorithm using sequential combination then during simulation, subset-armed UCB1 selection is applied according to the current position-in-episode (EPIC-4): effectively this means that the simulation policy for the overall search is provided by the tree policy in the episode trees.</a>
</body>
</html>