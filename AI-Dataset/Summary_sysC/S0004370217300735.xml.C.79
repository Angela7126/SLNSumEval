<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:284">
</head>
<body bgcolor="white">
<a href="#0" id="0">In the LDA-based methods, observed variables are token variables (usually denoted as {a mathematical formula}Wd,n), and latent variables are constructs in a hypothetical document generation process, including a list of topics (usually denoted as Β ), a topic distribution vector for each document (usually denoted as {a mathematical formula} Θ d), and a topic assignment for each token in each document (usually denoted as {a mathematical formula}Zd,n).</a>
<a href="#1" id="1">Because the latent variables are introduced layer by layer, and each latent variable is introduced to explain the correlations among a group of variables at the level below, we regard, for the purpose of model interpretation, the edges between two layers as directed and they are directed downwards.</a>
<a href="#2" id="2">Finally, the creation of Table 1 requires the joint distribution of Z21 with each of the words variable in its subtrees (e.g., {a mathematical formula}P(space,Z21)).</a>
<a href="#3" id="3">We refer to the latent variables in the flat model from the first step as level 1 latent variables.</a>
<a href="#4" id="4">The objective of the second step (line 9) is to turn the level 1 latent variables into observed variables through data completion.</a>
<a href="#5" id="5">Such topics can be obtained as follows: First, pick a list of words to characterize a topic using the method described in Section 4; then, form a latent class model using those words as observed variables; and finally, use the model to partition all documents into two clusters.</a>
<a href="#6" id="6">It is clear that the score depends on the choices of M and it generally decreases with M. In our experiments, we set {a mathematical formula}M=4 because some of the topics produced by HLTA have only 4 words and hence the choice of a larger value for M would put other methods at a disadvantage.</a>
</body>
</html>