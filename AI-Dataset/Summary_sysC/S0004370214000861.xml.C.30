<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:247">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our first algorithm, state abstraction from demonstration (AfD) learns a policy for an MDP by building an abstract space {a mathematical formula}S Α and using reinforcement learning to find an optimal policy that can be represented in {a mathematical formula}S Α .</a>
<a href="#1" id="1">AfD obtains {a mathematical formula}S Α by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S Α can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
<a href="#2" id="2">AfD, shown in Algorithm 1, is composed of two steps.</a>
<a href="#3" id="3">First, a feature selection algorithm is applied to human demonstrations to choose the subset of features to use.</a>
<a href="#4" id="4">In the second step, which corresponds to the loop in Algorithm 1, the algorithm learns a policy for the MDP M using a modified version of a Monte Carlo learning algorithm with exploring starts.</a>
<a href="#5" id="5">Instead of learning the Q-values of states in the original state space {a mathematical formula}Q(s,a),s ∈ S, it learns the Q-values of states in the transformed state space {a mathematical formula}Q(s Α ,a), where {a mathematical formula}s Α ∈ S Α .</a>
<a href="#6" id="6">To determine which features are relevant to a particular subtask, we measure the mutual information between each state feature and the action taken by a human teacher in a set of demonstrations.</a>
</body>
</html>