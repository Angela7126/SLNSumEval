<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:338">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our experimental work shows that even for small data sets finding the global optimum produces more accurate results.</a>
<a href="#1" id="1">Formalizing hierarchical clustering as an ILP with constraints has several advantages beyond finding the global optima.</a>
<a href="#2" id="2">Relaxing the dendrogram constraints such as transitivity can produce novel problem variations such as finding hierarchies with overlapping clusterings.</a>
<a href="#3" id="3">It is also possible to add constraints to encode guidance such as must – link, cannot – link, must – link – before etc.</a>
<a href="#4" id="4">In Fig.</a>
<a href="#5" id="5">Fig.</a>
<a href="#6" id="6">Agglomerative hierarchical clustering algorithms take as input a distance function and create a dendrogram, which is a tree structure which can be interpreted as a k-block set partition for each value of k between 1 and n, where n is the number of data points to cluster.</a>
<a href="#7" id="7">Fig.</a>
<a href="#8" id="8">Fig.</a>
<a href="#9" id="9">Guidance constraints such as {a mathematical formula}must – link, {a mathematical formula}cannot – link, and {a mathematical formula}must – link – before constraints can be used to encode domain knowledge or problem constraints into otherwise unsupervised problems such as hierarchical clustering.</a>
<a href="#10" id="10">Previous work showed that hierarchical clustering results can benefit from the addition of these constraints [20], [21], [22], but all previous work relied on ad-hoc schemes to integrate constraint solving into agglomerative clustering algorithms.</a>
<a href="#11" id="11">Fig.</a>
<a href="#12" id="12">Our contributions are formulating hierarchical clustering as an optimization problem, hence we begin our experiments with the question: 1.</a>
<a href="#13" id="13">Does our global optimization formulation of hierarchical clustering provide better results than greedy algorithms?</a>
<a href="#14" id="14">Our results in Table 3, Table 4 show that our method outperforms standard hierarchical clustering for real world language evolution and fMRI data sets for finding the best hierarchy (matching the ground truth).</a>
<a href="#15" id="15">Furthermore, Fig.</a>
<a href="#16" id="16">Fig.</a>
<a href="#17" id="17">6, and they shows our method's ability to outperform many standard agglomerative clustering algorithms for standard hierarchical clustering (no overlapping clusters) on artificial data (Artificial Dataset 1).</a>
<a href="#18" id="18">The results show that our algorithm performs better when there is increasing distance error which is expected since agglomerative algorithms are greedy and erroneous steps cannot be undone.</a>
<a href="#19" id="19">Fig.</a>
<a href="#20" id="20">Can our method find overlapping clusters in datasets?</a>
</body>
</html>