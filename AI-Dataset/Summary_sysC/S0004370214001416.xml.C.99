<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:493">
</head>
<body bgcolor="white">
<a href="#0" id="0">RL provided several advantages over previous work with dynamic programming [9]: a single learning phase leading to the generation of multiple trajectories, better compensation for accumulated error resulting from model approximation, and lack of knowledge of the detailed system dynamics.</a>
<a href="#1" id="1">To keep the system in the proximity of the reference path, we restrict the action space {a mathematical formula}As ⊂ A to only the actions that transition the system in the proximity of the reference path, using proximity constant {a mathematical formula} Δ >0{a mathematical formula} In the case {a mathematical formula}As= ∅ , we use an alternative action subset that transitions the system to the k closest position to the reference trajectory,{a mathematical formula} where {a mathematical formula}argmink denotes k smallest elements.</a>
<a href="#2" id="2">The action selection step, or the policy, becomes the search for action that transitions the system to the highest valued state chosen from the {a mathematical formula}As subset,{a mathematical formula} The policy given in (7) ensures state transitions in the vicinity of the reference path P. If it is not possible to transition the system within given ideal proximity Δ , the algorithm selects k closest positions and selects an action that produces the best minimal residual oscillations characteristics upon transition (see Algorithm 1).</a>
<a href="#3" id="3">When the reference path is a line segment with start in {a mathematical formula}s0 and end in the origin, such as in this setup, the distance d calculation defined in (4), and needed to calculate the action space subsets (5), and (6), can be simplified and depends only the start state {a mathematical formula}s0:{a mathematical formula}</a>
<a href="#4" id="4">Both configurations use the same discount parameter {a mathematical formula} Γ <1 to ensure that the value function is finite and are learning in the sampling box 1 m around the goal state with the load displacements under {a mathematical formula}10 ∘ .</a>
<a href="#5" id="5">The section also compares experimentally the trajectory created using the learned policy to two other methods: a cubic spline trajectory, which is a minimum time {a mathematical formula}C3-class trajectory without any pre-assumptions about the load swing, and, to a dynamic programming trajectory [9], an optimal trajectory for a fixed start position with respect to its MDP setup.</a>
<a href="#6" id="6">The average energy of AVI deterministic ({a mathematical formula}E([ Φ Θ ])=[0.00740.0073]), stochastic AVI ({a mathematical formula}E([ Φ Θ ])=[0.00500.0050]), and DP trajectories ({a mathematical formula}E([ Φ Θ ])=[0.00810.0081) load position signals, we find that AVI deterministic trajectory requires the least energy over the entire trajectory.</a>
<a href="#7" id="7">As another approach to addressing the learning practicality, we first train the agent in fine-grain 2D configuration.</a>
<a href="#8" id="8">The path-following only method creates minimum time path-following trajectories, by choosing actions that transition the system as close as possible to the reference path in the direction of the goal state with no consideration to the load swing.</a>
<a href="#9" id="9">We generate the paths, and create trajectories for three different maximal load displacements ({a mathematical formula}1 ∘ , {a mathematical formula}10 ∘ , and {a mathematical formula}25 ∘ ).</a>
</body>
</html>