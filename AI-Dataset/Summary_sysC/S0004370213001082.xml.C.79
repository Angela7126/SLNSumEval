<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:313">
</head>
<body bgcolor="white">
<a href="#0" id="0">Over the past decade, a considerable body of work has shown how to use supervised machine learning methods to build regression models that provide approximate answers to this question based on given algorithm performance data; we survey this work in Section 2.</a>
<a href="#1" id="1">In the same community, Howe and co-authors [45], [97] used linear regression to predict how both a planner ʼ s runtime and its probability of success depend on various features of the planning problem; they also applied these predictions to decide, on a per-instance basis, which of a finite set of algorithms should be run in order to optimize a performance objective such as expected runtime.</a>
<a href="#2" id="2">More specifically, we show how to extend all models to handle categorical inputs (required for predictions in partially categorical configuration spaces) and describe two new model families well-suited to modelling the performance of highly parameterized algorithms based on potentially large amounts of data: the projected process approximation to Gaussian processes and random forests of regression trees.</a>
<a href="#3" id="3">For some applications such expensive features will be reasonable — in particular, we note that for applications that take features as a one-time input, but build models repeatedly, it can even make sense to use features whose cost exceeds that of solving the instance; examples of such applications include model-based algorithm configuration [55] and complex empirical analyses based on performance predictions [53], [57].</a>
<a href="#4" id="4">For SPEAR-SWV-IBM, 100 training data points sufficed to obtain random forest models that captured the most salient features (e.g., they correctly determined the simplicity of the roughly 20% easiest instances); more training data points gradually improved qualitative predictions, especially in distinguishing good from bad configurations.</a>
<a href="#5" id="5">Likewise, for CPLEX-CORLAT, salient features (e.g., the simplicity of the roughly 25% easiest instances) could be detected based on 100 training data points, and more training data improved qualitative predictions to capture some of the differences between good and bad configurations.</a>
</body>
</html>