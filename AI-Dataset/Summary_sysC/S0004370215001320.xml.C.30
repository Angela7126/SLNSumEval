<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:345">
</head>
<body bgcolor="white">
<a href="#0" id="0">Thus, a meaningful semantic description is obtained in terms of human motions and object properties.</a>
<a href="#1" id="1">In addition, we validated the semantic rules obtained in different conditions, i.e., three different and complex kitchen activities: 1) making a pancake; 2) making a sandwich; and 3) setting the table.</a>
<a href="#2" id="2">In this study, we present a framework that comprises three main modules: 1) observation of human motions and object properties; 2) hierarchical interpretation of human behaviors; and 3) activity imitation by a robot.</a>
<a href="#3" id="3">A description of our framework and the connections between these modules is depicted in Fig.</a>
<a href="#4" id="4">The second module (see Fig.</a>
<a href="#5" id="5">1.2) interprets the visual data obtained from the first module and processes this information to infer the observed human intentions (the goal).</a>
<a href="#6" id="6">This module represents the core of our framework because it is responsible for identifying and extracting the meaning of human motions by generating semantic rules that define and explain these human motions, i.e., it infers human behaviors such as reach, take, pour, and cut.</a>
<a href="#7" id="7">The previous results were obtained using the label information when one human performed different activities with the right hand.</a>
<a href="#8" id="8">The results obtained after testing human activities performed by both hands demonstrated our comprehensive approach.</a>
<a href="#9" id="9">We found that rules obtained using the right hand ({a mathematical formula}Hsandwich) were also valid for inferring the activities performed by humans with the left hand.</a>
<a href="#10" id="10">Therefore, the obtained semantic rules were independent of the hand performing the activity because the meaning was the same.</a>
<a href="#11" id="11">In the next step, we tested the information obtained from module 1 (see Section 7.1), i.e., from the automatic segmentation of human motions and object properties.</a>
<a href="#12" id="12">In the previous subsections, we demonstrated that semantic rules obtained for human activities did not change under different constraints, e.g., time constraints, different scenarios, gender, or the hand used to execute the activity.</a>
<a href="#13" id="13">In this subsection, we present the results obtained when we used the semantic rules to improve our reasoning engine and the knowledge base.</a>
<a href="#14" id="14">Thus, one branch of the tree obtained from Fig.</a>
<a href="#15" id="15">12 (see Section 7.2) was executed as shown in Fig.</a>
</body>
</html>