<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:168">
</head>
<body bgcolor="white">
<a href="#0" id="0">Given an {a mathematical formula}n×n kernel matrix K with rank {a mathematical formula}r<<n, the kernel matrix can be represented as {a mathematical formula}K=GG ⊤ , with G an {a mathematical formula}n×r matrix.</a>
<a href="#1" id="1">In addition, computing the eigenvalue decomposition of the kernel matrix takes {a mathematical formula}O(n2) space and {a mathematical formula}O(n3) time.</a>
<a href="#2" id="2">Let {a mathematical formula}E ∈ Rn×m be the extrapolation kernel matrix between samples {a mathematical formula}X and landmark {a mathematical formula}Z, and let {a mathematical formula}El ∈ Rl×m be the rows of E corresponding to labeled samples.</a>
<a href="#3" id="3">For simplicity, let {a mathematical formula}S0=W † denote the inverse of the dictionary kernel in the standard Nyström method (3).</a>
<a href="#4" id="4">As shown in [34], the final kernel SVM using kernel matrix {a mathematical formula}K ˜ can be equivalently transformed to a linear SVM using virtual samples {a mathematical formula}Q=EU Λ 1/2.</a>
<a href="#5" id="5">However, most existing multiple kernel learning algorithms need to manipulate multiple {a mathematical formula}n×n base kernel matrices, which will take at least {a mathematical formula}O(n2) space and time.</a>
</body>
</html>