<html>
<head>
<meta name="TextLength" content="SENT_NUM:39, WORD_NUM:734">
</head>
<body bgcolor="white">
<a href="#0" id="0">In addition, there is an error function {a mathematical formula}E to be minimized by the learning process.</a>
<a href="#1" id="1">Thus the learning rule becomes:{a mathematical formula} where the randomly backpropagated error satisfies the recurrence relation:{a mathematical formula} and the weights {a mathematical formula}ckih+1 are random and fixed.</a>
<a href="#2" id="2">Let us denote by {a mathematical formula}a1 and {a mathematical formula}a2 the weights in the first and second layer, and by {a mathematical formula}c1 the random weight of the learning channel.</a>
<a href="#3" id="3">In this case, we have {a mathematical formula}O(t)=a1a2I(t) and the learning equations are given by:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula} Α =E(IT) and {a mathematical formula} Β =E(I2).</a>
<a href="#4" id="4">across both the {a mathematical formula}a1 and {a mathematical formula}a2 axis).</a>
<a href="#5" id="5">All these critical points correspond to global minima of the error function {a mathematical formula}E=12E[(T − O)2].</a>
<a href="#6" id="6">If we let {a mathematical formula}w=a2u+a1v we have:{a mathematical formula} Thus if {a mathematical formula} Β (c1a2+a12)>0, w converges to zero and {a mathematical formula}a1,a2 is an attractor.</a>
<a href="#7" id="7">If {a mathematical formula} Β (c1a2+a12)<0, w diverges to + ∞ , and corresponds to unstable critical points as described above.</a>
<a href="#8" id="8">In this case, we have {a mathematical formula}O(t)=a1a2a3I(t)=PI(t).</a>
<a href="#9" id="9">The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}P=a1a2a3.</a>
<a href="#10" id="10">Thus for the rest of this section we can assume {a mathematical formula}c1 ≠ 0 and {a mathematical formula}c2 ≠ 0.The critical points of the system correspond to {a mathematical formula} Α − Β P=0 and do not depend on the weights in the learning channel.</a>
<a href="#11" id="11">Thus in the general case the system always converges to a global minimum of the error function satisfying {a mathematical formula} Α − Β P=0.</a>
<a href="#12" id="12">Again, when all the weights in the learning channel are non-zero, the critical points correspond to the curve {a mathematical formula} Α − Β P=0.</a>
<a href="#13" id="13">Consider a linear {a mathematical formula}A[1,N,1] architecture (Fig.</a>
<a href="#14" id="14">The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula} Α =E(IT) and {a mathematical formula} Β =E(I2).</a>
<a href="#15" id="15">In the general case where the weights in the learning channel are non-zero, the critical points are given by the surface {a mathematical formula} Α − Β P=0 and correspond to global optima.</a>
<a href="#16" id="16">Additional critical points for the product {a mathematical formula}P= ∑ iaibi are given by the surface {a mathematical formula} ∑ iai2+bici=0 which depends on the weights in the learning channel.</a>
<a href="#17" id="17">Its leading term, is the leading term of {a mathematical formula} − c1 Β P.</a>
<a href="#18" id="18">Consider a linear {a mathematical formula}A[N,1,N] architecture (Fig.</a>
<a href="#19" id="19">The on-line learning equations are given by:{a mathematical formula} for {a mathematical formula}i=1, … ,N.</a>
<a href="#20" id="20">The proof is immediate since:{a mathematical formula} where {a mathematical formula}||A||2=a12+ … +aN2.</a>
<a href="#21" id="21">In this case the system can be written as:{a mathematical formula} We define{a mathematical formula} and let {a mathematical formula}A0=A(0).</a>
<a href="#22" id="22">Note that {a mathematical formula} Σ (t) ≥ K.</a>
<a href="#23" id="23">The proof can easily be adapted to the slightly more general case where {a mathematical formula} Σ II is a diagonal matrix.</a>
<a href="#24" id="24">In the case of an autoencoder, {a mathematical formula}T=I and therefore {a mathematical formula} Σ TI= Σ II.</a>
<a href="#25" id="25">As in Lemma 1, we have:{a mathematical formula} Thus we have:{a mathematical formula} It follows that:{a mathematical formula} where {a mathematical formula}C0 is a constant matrix.</a>
<a href="#26" id="26">Thus in fact we have pointwise convergence of {a mathematical formula}C1A2A1.</a>
<a href="#27" id="27">Then the system associated with Equation (88) can be simplified to:{a mathematical formula} where {a mathematical formula}A(t),B(t) are {a mathematical formula}2×2 matrix functions.</a>
<a href="#28" id="28">By Theorem 7, we know that {a mathematical formula}B(t)A(t) is convergent.</a>
<a href="#29" id="29">The weights are {a mathematical formula}a1 and {a mathematical formula}a2 in the forward network, and {a mathematical formula}c1 in the learning channel.</a>
<a href="#30" id="30">Moreover, if we write:{a mathematical formula} then {a mathematical formula}E ≠ 0.</a>
<a href="#31" id="31">As a result, we have:{a mathematical formula} and {a mathematical formula}(A,B)=(aI,a − 1I).</a>
<a href="#32" id="32">From the second equation of the system above, we have {a mathematical formula}d ˜ =a.</a>
<a href="#33" id="33">This implies that {a mathematical formula}(A(t),B(t))=(A,B).</a>
<a href="#34" id="34">So in this case {a mathematical formula}E=0.</a>
<a href="#35" id="35">We first assume that {a mathematical formula}a ≠ − 1.</a>
<a href="#36" id="36">In this case, from the third equation of the system above, we have {a mathematical formula}a ˜ − 1d ˜ − 1+d ˜ =0.</a>
<a href="#37" id="37">Thus in this case {a mathematical formula}b ˜ must be zero.</a>
<a href="#38" id="38">When {a mathematical formula}X=Y=0, it is possible to have {a mathematical formula}E ≠ 0.</a>
</body>
</html>