<html>
<head>
<meta name="TextLength" content="SENT_NUM:19, WORD_NUM:1002">
</head>
<body bgcolor="white">
<a href="#0" id="0">We define the length{a mathematical formula}L(S) of a joint action sequence S to be the number of joint actions prior to the first instance of the infinite sequence of joint actions that yield {a mathematical formula}m ⁎ .</a>
<a href="#1" id="1">The reason is that {a mathematical formula}ai determines Agent B ʼ s next action independently from Agent B ʼ s current action: in all cases, its next action will be {a mathematical formula}bBR(ai).</a>
<a href="#2" id="2">Thus, Agent A ʼ s task is to select its action, {a mathematical formula}aact, that leads to the best possible joint action of the form {a mathematical formula}(aact,bBR(ai)).</a>
<a href="#3" id="3">Now working backwards, in order to reach the optimal joint action {a mathematical formula}(ax − 1,by − 1), Agent A must have selected action {a mathematical formula}ax − 1 in the iteration prior to the first appearance of {a mathematical formula}(ax − 1,by − 1) in the sequence.</a>
<a href="#4" id="4">When that happened, if Agent B had selected anything other than {a mathematical formula}by − 2 ({a mathematical formula}by − 1 is not an option since we are considering the iteration prior to the first appearance of {a mathematical formula}by − 1 in the sequence), then there would have been a payoff of 0, leading to a sequence cost of ⩾ 100.</a>
<a href="#5" id="5">Thus joint action {a mathematical formula}(ax − 1,by − 2) must appear in the optimal sequence.</a>
<a href="#6" id="6">Similarly, considering the first appearance of this joint action, for Agent B to have selected {a mathematical formula}by − 2, Agent A must have selected {a mathematical formula}ax − 2 on the prior iteration.</a>
<a href="#7" id="7">In particular, in line 12, care needs to be taken to compute {a mathematical formula}S ′ correctly, with Agent B ʼ s action being based on the best response to the current history, and the history being the result of taking action {a mathematical formula}ai from the current history.</a>
<a href="#8" id="8">Note that as Ε changes, both the optimal sequence of joint actions and the “ target ” joint actions (the ones that lead to expected value of {a mathematical formula}m ⁎ ) can change.</a>
<a href="#9" id="9">From these results we see that even in {a mathematical formula}3×3 matrices with {a mathematical formula}mem=1, it is not uncommon for Agent A to need to reason about the cost of various sequence lengths: In 44 of 1000 cases, there is at least one joint action from which Agent A is best off not jumping immediately to action {a mathematical formula}a2.</a>
<a href="#10" id="10">To show: there exists a teacher action policy {a mathematical formula} Π ′ starting with {a mathematical formula}Arm ⁎ (or Arm1) that leads to a sequence T with expected value greater than that of S. That is, the initial pull of Arm2 in S does not follow {a mathematical formula} Π ⁎ .The underlying idea is that the sequence T should start with the teacher pulling {a mathematical formula}Arm ⁎ repeatedly, and tracking the values obtained by the learner to see if it can ever discern what the sequence S would have looked like after some number of rounds (it simulates sequence S).</a>
<a href="#11" id="11">For each reachable combination of values, the algorithm computes the teacher ʼ s optimal action (Arm1 or {a mathematical formula}Arm ⁎ ), denoted {a mathematical formula}Act[ ⋅ ]; and the expected long-term value of taking that action, denoted {a mathematical formula}Val[ ⋅ ]: the expected sum of payoffs for the optimal action and all future actions by both the teacher and the learner.</a>
<a href="#12" id="12">For example, when {a mathematical formula}p ⁎ =.5, {a mathematical formula}p1=.4, {a mathematical formula}p2=.16, {a mathematical formula}r=4,m2=2, and {a mathematical formula}n2=5 (the same parameters considered at the beginning of this section), the teacher ʼ s optimal action can differ even for identical values of {a mathematical formula}x¯1.</a>
<a href="#13" id="13">Since there are readily available packages, for example in Java, for computing {a mathematical formula} Φ Μ 1, Σ 1(y), this result can be considered a closed form solution for finding the optimal teacher action and its expected value when {a mathematical formula}r=1.</a>
<a href="#14" id="14">In contrast, when {a mathematical formula}r>1, there is no such closed form method for finding the optimal action.</a>
<a href="#15" id="15">To show: there exists a teacher action policy {a mathematical formula} Π ′ starting with {a mathematical formula}Arm ⁎ (or Arm1) that leads to a sequence T with expected value greater than that of S. That is, the initial pull of Arm2 in S does not follow {a mathematical formula} Π ⁎ .In order to define such a policy {a mathematical formula} Π ′ , we define {a mathematical formula}S1(n) and {a mathematical formula}S2(n) as the number of pulls of Arm1 and Arm2 respectively after n total steps of S. As shorthand, we denote {a mathematical formula}S(n)=(S1(n),S2(n)).Similarly, define the number of pulls of Arm1 and Arm2 after n steps of T (e.g.</a>
<a href="#16" id="16">That is {a mathematical formula}T(n)>S(m) if at least one of the arms has pulled more times after n steps in T than after m steps in S, and neither arm has been pulled fewer times.Finally, we define the concept of the teacher simulating sequence S based on the knowledge of what values would have resulted from each of the actions, starting with the teacher ʼ s pull of Arm2 at step 1.</a>
<a href="#17" id="17">If the teacher ʼ s second action in T is {a mathematical formula}Arm ⁎ and learner ʼ s 2nd action is Arm2, then in the example sequence above, {a mathematical formula}Sim(4)=2.We are now ready to define the teacher ʼ s policy {a mathematical formula} Π ′ for generating T. Let n be the total number of actions taken so far.</a>
<a href="#18" id="18">Thus the simulation of S always lags behind T in terms of number of steps simulated: {a mathematical formula}Sim(n ′ )<n ′ .Note that if it is ever the case that {a mathematical formula}T(n)=S(Sim(n)) and {a mathematical formula}Sim(n) is odd (it is the learner ʼ s turn to act in S), then the teacher will pull {a mathematical formula}Arm ⁎ once more after which the learner will do what it would have done in sequence S after {a mathematical formula}Sim(n) steps.</a>
</body>
</html>