<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:161">
</head>
<body bgcolor="white">
<a href="#0" id="0">In practice, given a large data set {a mathematical formula}X={xi}i=1n, the Nyström method randomly selects m landmark points {a mathematical formula}Z with {a mathematical formula}m ≪ n, and computes the eigenvalue decomposition of W. Then the eigenvectors of W are extrapolated to the whole sample set by (2).</a>
<a href="#1" id="1">Interestingly, the Nyström method is shown to implicitly reconstruct the whole {a mathematical formula}n×n kernel matrix K by{a mathematical formula} Here {a mathematical formula}W † is the pseudo-inverse of W, and {a mathematical formula}E ∈ Rn×m is the kernel matrix defined on the sample set {a mathematical formula}X and landmark points {a mathematical formula}Z.</a>
<a href="#2" id="2">The Nyström method requires {a mathematical formula}O(mn) space and {a mathematical formula}O(m2n) time, which are linear in the sample size considering {a mathematical formula}m ≪ n.</a>
<a href="#3" id="3">It has drawn considerable interest in clustering and manifold learning [27], [18], Gaussian processes [5], and kernel methods [3].</a>
<a href="#4" id="4">We call our method generalized Nyström low-rank decomposition.</a>
<a href="#5" id="5">The method has several desirable properties.</a>
</body>
</html>