<html>
<head>
<meta name="TextLength" content="SENT_NUM:28, WORD_NUM:673">
</head>
<body bgcolor="white">
<a href="#0" id="0">Furthermore, the reward function allows us to obtain the patrollers' policies, which is more useful in the learner's decision making than the realized trajectories because a policy informs the learner about the patroller's action from each state.</a>
<a href="#1" id="1">An assumption common to many IRL methods is that the learner knows the expert's transition function.</a>
<a href="#2" id="2">For the sake of clarity, let us initially focus on the case where there is a single Nash equilibrium of an interaction game.</a>
<a href="#3" id="3">As interactions alter the behavior of the robots, the equilibrium must be considered during IRL otherwise the learned reward and policy will likely be different from the actual policy of the observed mobile robot, as it may falsely attribute the interaction behavior to the MDP's reward function.</a>
<a href="#4" id="4">Let {a mathematical formula} Σ e = {a mathematical formula} 〈 Σ 1, … , Σ N 〉 be an equilibrium profile with {a mathematical formula} Σ I, I = {a mathematical formula}1, … ,N denoting each robot's action that is in equilibrium, respectively.</a>
<a href="#5" id="5">As interactions impact the state-visitation frequencies of the robots, we decompose Eq.</a>
<a href="#6" id="6">1 into a piecewise function for each robot,{a mathematical formula} The two cases in Eq.</a>
<a href="#7" id="7">16 differ by which action is used in the transition function.</a>
<a href="#8" id="8">In particular, the action in an equilibrium is used if {a mathematical formula}s ′ is an interacting state.</a>
<a href="#9" id="9">We may then replace {a mathematical formula} Μ Π I in Eq.</a>
<a href="#10" id="10">Key model assumptions of popular IRL methods are that the expert's stochastic transition function is completely known to the learner as in apprenticeship learning [1] and in Bayesian IRL [31].</a>
<a href="#11" id="11">Let Ψ : {a mathematical formula}S×A → S map an observed robot's transition from state s given action a to a particular next state, {a mathematical formula}s ′ .</a>
<a href="#12" id="12">The function Ψ gives the intended outcome of each action from each state.</a>
<a href="#13" id="13">We may view Ψ as a deterministic transition function.</a>
<a href="#14" id="14">Actions taken by the agent may not always result in the intended outcome leading instead to some other state.</a>
<a href="#15" id="15">We therefore focus on learning the probability of transitioning to the intended next state given a state-action pair for an observed robot I, {a mathematical formula}TI(s,a, Ψ (s,a)), and this probability is estimated from the available trajectory data.</a>
<a href="#16" id="16">The remaining probability mass, {a mathematical formula}1 − TI(s,a, Ψ (s,a)) is then distributed according to some error model, such as uniformly among all other states or perhaps to the intended outcomes of other actions.</a>
<a href="#17" id="17">Unseen actions could have occurred in the occluded portion of the robot's trajectory.</a>
<a href="#18" id="18">Nevertheless, these actions map to feature variables in {a mathematical formula}TI.</a>
<a href="#19" id="19">As some of these features are factors in observed actions, we may obtain (partially) informed transition distributions for the unseen actions under the maximum entropy principle.</a>
<a href="#20" id="20">This observation emphasizes the advantage of considering transition feature variables.</a>
<a href="#21" id="21">We start by evaluating the learned behavior accuracy of mIRL{sup: ⁎ }+Int and mIRL{sup: ⁎ } as a function of the degree of observability.</a>
<a href="#22" id="22">The degree of observability is the proportion of all {a mathematical formula}(x,y) cells in the state space that are visible to L; its complement gives a measure of the occlusion.</a>
<a href="#23" id="23">We evaluate mIRL{a mathematical formula}/T ⁎ +Int for learning the experts' transition functions as previously described in Section 5. mIRL{a mathematical formula}/T ⁎ +Int begins by estimating the transition functions of others, after which it collapses into mIRL{sup: ⁎ }+Int learning reward weights Θ .</a>
<a href="#24" id="24">• Known R{a mathematical formula}/T modifies this method by allowing the transition function to be learned using our approach.</a>
<a href="#25" id="25">Under the assumption that the intended state can be recognized from the observed robot's actions and previous state, our experiments conclusively show that the full transition model (dynamics) can be learned despite high degrees of occlusion.</a>
<a href="#26" id="26">This is beneficial because the actual motion error of robots is often not known, and the learned transition model improves the accuracy of the inversely learned behavior of the expert.</a>
<a href="#27" id="27">Bard [5] used a maximum entropy approach to estimate state probabilities when the probabilities associated with aggregates of states are known.</a>
</body>
</html>