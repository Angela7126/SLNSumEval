<html>
<head>
<meta name="TextLength" content="SENT_NUM:40, WORD_NUM:784">
</head>
<body bgcolor="white">
<a href="#0" id="0">The resulting relational map constitutes the robot's belief state, used as the initial belief state for planning.</a>
<a href="#1" id="1">After obtaining the initial relational map, Dora is given a goal description that a magazine must exist and the robot must know where it is, written:{sup:7}</a>
<a href="#2" id="2">To create a plan to find the magazine, Dora must reason about two entities that do not yet exist in its relational map: the magazine and the room that object is in.</a>
<a href="#3" id="3">This incompleteness in knowledge requires open-world planning — that is, planning that takes into account the fact that new entities may become known to the robot during plan execution.</a>
<a href="#4" id="4">Given the initial relational map, the planner hypothesizes extensions to it.</a>
<a href="#5" id="5">To create these extensions, the planner has assumptive actions it can take, each making one or more assumptions about instance knowledge.</a>
<a href="#6" id="6">The planner selects a sequence of assumptive actions that are likely to be true according to default knowledge, and which enable goal achievement.</a>
<a href="#7" id="7">The planner does not make task-irrelevant assumptions.</a>
<a href="#8" id="8">The remainder of the plan (Fig.</a>
<a href="#9" id="9">3) is to move to the hypothesized meeting room and to plan to conduct a visual search for the object when the robot gets there.</a>
<a href="#10" id="10">All of these actions are assumptions.</a>
<a href="#11" id="11">Assumptions 1 and 2 hypothesize new default relations between objects and places (default assumptions), and assumptions 3 and 4 hypothesize new instance knowledge (instance assumptions).</a>
<a href="#12" id="12">In the case of Q1 and Q2, this means updating the relational map.</a>
<a href="#13" id="13">For Q3 and Q4, it means updating the default knowledge.</a>
<a href="#14" id="14">Thus, Dora can plan and verify explanations that lead to updating of both instance and default knowledge.</a>
<a href="#15" id="15">For example, the search action has the precondition that the robot must be in the room which it plans to search: (= (is-in ?a) ?p) and (= (in-room ?p) ?r).</a>
<a href="#16" id="16">But when planning in partially explored environments, we do not yet know to which room a place node belongs.</a>
<a href="#17" id="17">Suppose, however, that we can make an assumption about a place-node's room, then the knowledge effect (K (in-room ?to)) of the move action should allow us to use the search action in a plan.</a>
<a href="#18" id="18">To achieve this, we specify assumptive actions with parameters for probabilities.</a>
<a href="#19" id="19">The probabilities are filled in at planning time with values obtained from the robot's default knowledge:</a>
<a href="#20" id="20">Since an explanation is a sequence of assumptions, finding one is a new planning problem.</a>
<a href="#21" id="21">We allow the planner to create a new sequence of assumptions that precedes the executed actions of the original sequence of plans.</a>
<a href="#22" id="22">Then we force the planner to replay that sequence of executed actions, checking that the physical actions now cause the effects originally observed by the robot.</a>
<a href="#23" id="23">If a plan is found, its assumptions are an explanation for the task-relevant surprises.</a>
<a href="#24" id="24">The planner is allowed to make all the assumptions {a mathematical formula}Aa of the original plan, but the only physical actions that are allowed are the executed actions of the original plan.</a>
<a href="#25" id="25">To these we add a set of observation actions{a mathematical formula}oi, which are used to check if the simulated execution results in the same state sequence as that originally experienced by the robot.</a>
<a href="#26" id="26">We also add an action variable M to the state.</a>
<a href="#27" id="27">This forces the planner to execute all physical actions in order (see Fig.</a>
<a href="#28" id="28">Recall that assumptions may occur only before the first physical action.</a>
<a href="#29" id="29">Thus, the planning problem has a solution only if the planner can find a sequence of assumptions that allow all the original physical actions to be executed in sequence, and which results in expected and actual observations {a mathematical formula}si matching.</a>
<a href="#30" id="30">If we want to force the planner to make assumptions about certain variables {a mathematical formula}v ∈ V to prevent explanations by omission, we can add the required conditions to the goal.</a>
<a href="#31" id="31">This has two effects: one physical effect, in which the robot ends up at the new location; and one knowledge effect, in which the robot knows the room type of the new location.</a>
<a href="#32" id="32">But even if this action's preconditions are satisfied, the effects are not guaranteed.</a>
<a href="#33" id="33">The default layer contains assumptive actions that hypothesize new instance states.</a>
<a href="#34" id="34">In the diagnostic layer there are corresponding default assumptions that can hypothesize new default knowledge about state.</a>
<a href="#35" id="35">This default state knowledge includes is-a and contains relations between types of entities.</a>
<a href="#36" id="36">A default assumptive action has as its effect an assumption that a new relation exists between two entities already in the default-knowledge graph.</a>
<a href="#37" id="37">The diagnostic actions and default assumptive actions are employed only for planning of explanations.</a>
<a href="#38" id="38">This is because they would add unnecessary complexity to regular task planning.</a>
<a href="#39" id="39">It is also an open question how to assign probabilities to default assumptions, as they deal by definition with new knowledge.</a>
</body>
</html>