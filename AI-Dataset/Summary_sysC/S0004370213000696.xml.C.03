<html>
<head>
<meta name="TextLength" content="SENT_NUM:70, WORD_NUM:985">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}</a>
<a href="#1" id="1">If both agents select action 0 (i.e., their joint action is {a mathematical formula}(a0,b0)), then the joint team payoff is {a mathematical formula}m0,0=25.</a>
<a href="#2" id="2">Similarly if their joint action is {a mathematical formula}(a2,b0) their joint payoff is 0.</a>
<a href="#3" id="3">Assume that {a mathematical formula}b0 is Agent B ʼ s default action or that, for whatever reason, the agents have been playing {a mathematical formula}(a0,b0) in the past.</a>
<a href="#4" id="4">We denote this best response action as {a mathematical formula}BR(a1,a0,a1,a1)=b1.</a>
<a href="#5" id="5">Now consider Agent A ʼ s possible action sequences starting from the joint action {a mathematical formula}(a0,b0) with payoff {a mathematical formula}m0,0=25.</a>
<a href="#6" id="6">We define this value as {a mathematical formula}L¯(M)=maxi,jL(S ⁎ (ai,bj)).</a>
<a href="#7" id="7">Thus {a mathematical formula}L¯(M1)=3.</a>
<a href="#8" id="8">First, in Section 2.2.1 we assume that Agent B has {a mathematical formula}mem=1 and {a mathematical formula} Ε =0 as in Section 2.1.</a>
<a href="#9" id="9">Next in Section 2.2.2 we consider the more difficult case of {a mathematical formula}mem>1.</a>
<a href="#10" id="10">Then, in Section 2.2.3 we allow Agent B ʼ s actions to be random by considering {a mathematical formula} Ε >0.</a>
<a href="#11" id="11">Thus, Agent A ʼ s task is to select its action, {a mathematical formula}aact, that leads to the best possible joint action of the form {a mathematical formula}(aact,bBR(ai)).</a>
<a href="#12" id="12">We denote this value {a mathematical formula}UPPERBOUND(L¯(M)).</a>
<a href="#13" id="13">In both cases, Agent B ʼ s next action in the sequence must be {a mathematical formula}BR(aq).</a>
<a href="#14" id="14">Thus after joint action {a mathematical formula}(aq,br), Agent A could have continued as it actually did after {a mathematical formula}(aq,br ′ ).</a>
<a href="#15" id="15">{a mathematical formula}</a>
<a href="#16" id="16">Thus joint action {a mathematical formula}(ax − 1,by − 2) must appear in the optimal sequence.</a>
<a href="#17" id="17">{a mathematical formula}C(S2)=15+40+40=95.</a>
<a href="#18" id="18">Thus {a mathematical formula}C(S3)=25+2 ⁎ 30+3 ⁎ 7=106.</a>
<a href="#19" id="19">Rather, the agents must arrive at such an action with a history of Agent A ʼ s actions such that if it keeps playing {a mathematical formula}ai, Agent B will keep selecting {a mathematical formula}bj.</a>
<a href="#20" id="20">{a mathematical formula}</a>
<a href="#21" id="21">However from there, Agent B ʼ s best response is {a mathematical formula}b0, not {a mathematical formula}b2.</a>
<a href="#22" id="22">Thus the agents do not remain stably at joint action {a mathematical formula}(a2,b2).</a>
<a href="#23" id="23">In that case, Agent A needs to select action {a mathematical formula}a1mem times before Agent B will switch to {a mathematical formula}b1.</a>
<a href="#24" id="24">{a mathematical formula}</a>
<a href="#25" id="25">{a mathematical formula}</a>
<a href="#26" id="26">Can find optimal action sequence efficiently: {a mathematical formula}O(d3)</a>
<a href="#27" id="27">{a mathematical formula}</a>
<a href="#28" id="28">The learner selects between Arm1 and Arm2, while the teacher can additionally choose {a mathematical formula}Arm ⁎ .</a>
<a href="#29" id="29">Throughout the section we assume that {a mathematical formula} Μ ⁎ > Μ 1> Μ 2.</a>
<a href="#30" id="30">If {a mathematical formula} Μ ⁎ is not the largest, then the teacher ʼ s choice is trivially to always select the arm with the largest expected payoff.</a>
<a href="#31" id="31">Therefore there are situations in which it is better for the teacher to pull Arm1 than {a mathematical formula}Arm ⁎ .</a>
<a href="#32" id="32">Second, we argue that the teacher should only consider pulling {a mathematical formula}Arm ⁎ or Arm1.</a>
<a href="#33" id="33">{a mathematical formula}r=1.</a>
<a href="#34" id="34">This expectation is higher since {a mathematical formula} Μ ⁎ > Μ 1.</a>
<a href="#35" id="35">Consider the sequence S that begins with Arm1 and then follows the optimal policy {a mathematical formula} Π ⁎ thereafter.</a>
<a href="#36" id="36">Since the first two values in S are equivalent to the learner ʼ s first two values in T (it will begin with Arm1 because {a mathematical formula}x¯1>x¯2), the sequences are identical other than the teacher ʼ s first two pulls of {a mathematical formula}Arm ⁎ in T and the last action of each agent in S. Thus the expected value of {a mathematical formula}T − S ⩾ ( Μ ⁎ + Μ ⁎ ) − ( Μ ⁎ + Μ 1)>0.</a>
<a href="#37" id="37">As shown in Section 3.2, the teacher should never teach when {a mathematical formula}x¯1>x¯2.</a>
<a href="#38" id="38">{a mathematical formula}m1+1n1+1>m2n2</a>
<a href="#39" id="39">{a mathematical formula}p ⁎ − p1<p1(p1 − p2)</a>
<a href="#40" id="40">In fact, with {a mathematical formula}r=3 or 4 and all other values above unchanged, the optimal action of the teacher is again not to teach.</a>
<a href="#41" id="41">However when {a mathematical formula}r=2, {a mathematical formula}EVt=.302228<EVnt=.303075: the optimal teacher action is {a mathematical formula}Arm ⁎ .</a>
<a href="#42" id="42">In contrast to the discrete case, we do not have an algorithm for exactly computing the optimal action when {a mathematical formula}r>1.</a>
<a href="#43" id="43">Otherwise, the teacher should pull {a mathematical formula}Arm ⁎ .</a>
<a href="#44" id="44">If {a mathematical formula}x¯1>x¯2, {a mathematical formula}EVnt= Μ ⁎ + Μ 1</a>
<a href="#45" id="45">Else {a mathematical formula}EVnt= Μ ⁎ + Μ 2.</a>
<a href="#46" id="46">For example, consider the case in which {a mathematical formula}r=2.</a>
<a href="#47" id="47">Thus we know with certainty that the teacher ʼ s optimal action is {a mathematical formula}Arm ⁎ .</a>
<a href="#48" id="48">Thus it selects {a mathematical formula}Arm ⁎ and the learner selects Arm2.</a>
<a href="#49" id="49">{a mathematical formula}x¯1>x¯2: 29.5%</a>
<a href="#50" id="50">{a mathematical formula}x¯1>x¯2: 64.0%</a>
<a href="#51" id="51">Thus the optimal action when {a mathematical formula}r=1 is Arm1.</a>
<a href="#52" id="52">However with two rounds remaining, the optimal action is {a mathematical formula}Arm ⁎ .</a>
<a href="#53" id="53">Thus the learner needs to consider pulling at least {a mathematical formula}Arm ⁎ and Arm1 – {a mathematical formula}Armz − 1.</a>
<a href="#54" id="54">The teacher should never take the action that the learner would take next on its own if the teacher were to pull {a mathematical formula}Arm ⁎ .</a>
<a href="#55" id="55">{a mathematical formula}x¯i>x¯j.</a>
<a href="#56" id="56">{a mathematical formula}mj+1nj+1>mini</a>
<a href="#57" id="57">Those are the arms with higher expected value than {a mathematical formula}Arm ⁎ .</a>
<a href="#58" id="58">{a mathematical formula}x¯1>x¯2: do not teach</a>
<a href="#59" id="59">{a mathematical formula}x¯1<x¯2, {a mathematical formula}r=1: closed form solution for optimal teacher action</a>
<a href="#60" id="60">In Section 2.2.2, we examined the complexity of finding the optimal (lowest cost) path through a matrix when Agent B ʼ s {a mathematical formula}mem>1.</a>
<a href="#61" id="61">Otherwise, {a mathematical formula}Gij=0.</a>
<a href="#62" id="62">We let Agent B ʼ s {a mathematical formula}mem=n and we construct Matrix M as follows.</a>
<a href="#63" id="63">Agent A has {a mathematical formula}(n − 1) ⁎ n+2 actions.</a>
<a href="#64" id="64">Agent B has {a mathematical formula}n ⁎ n+n+1 actions.</a>
<a href="#65" id="65">{a mathematical formula}</a>
<a href="#66" id="66">The payoff for selecting done only comes at {a mathematical formula}m ⁎ .</a>
<a href="#67" id="67">after at least {a mathematical formula}n=mem steps.</a>
<a href="#68" id="68">{a mathematical formula}r=1.</a>
<a href="#69" id="69">This expectation is higher since {a mathematical formula} Μ ⁎ > Μ 1.</a>
</body>
</html>