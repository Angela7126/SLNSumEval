<html>
<head>
<meta name="TextLength" content="SENT_NUM:40, WORD_NUM:919">
</head>
<body bgcolor="white">
<a href="#0" id="0">Furthermore, we develop an algorithm to solve it efficiently, called bounded Ε -first, which proceeds in two stages: exploration and exploitation.</a>
<a href="#1" id="1">During exploration, it first uses Ε B of its total budget B to learn estimates of the workers' quality characteristics.</a>
<a href="#2" id="2">Then, during exploitation, it uses the remaining (1 − Ε )B to maximise the total utility based on those estimates.</a>
<a href="#3" id="3">These optimisation problems consider settings where actions (i.e., the pulling of a particular arm) have initially unknown rewards that have to be learnt through noisy observations, and the goal is to maximise the total amount of rewards by sequentially choosing different actions over time.</a>
<a href="#4" id="4">This corresponds exactly to choosing initially unknown workers in an expert crowdsourcing setting.</a>
<a href="#5" id="5">However, as we discuss in Section 2, no existing MAB model considers the specific constraints of the expert crowdsourcing setting.</a>
<a href="#6" id="6">While some work considers MAB problems with a fixed budget, termed budget-limited MABs [33], and proposes a budget-limited Ε -first algorithm for this, their model does not consider task limits per worker.</a>
<a href="#7" id="7">Furthermore, we show that using this algorithm allows us to establish theoretical guarantees for its performance.</a>
<a href="#8" id="8">More specifically, we prove that the performance regret (i.e., the difference between the performance of a particular algorithm and that of the optimal solution) of the bounded Ε -first approach is at most {a mathematical formula}O(B23) with a high probability, where B is the total budget.</a>
<a href="#9" id="9">This sub-linear theoretical bound necessarily implies that our algorithm has the zero-regret property, a key measure of efficiency within the MAB literature.</a>
<a href="#10" id="10">That is, as B increases, the average regret (i.e., the performance regret divided by the total budget) tends to 0.</a>
<a href="#11" id="11">This property guarantees that our algorithm asymptotically converges to the optimal solution with probability 1 as B tends to infinity (for more details, see [36]).</a>
<a href="#12" id="12">As this desirable theoretical property holds only in the limit, we also conduct extensive empirical experiments, in order to ascertain the efficiency of our proposed approach for realistic budgets.</a>
<a href="#13" id="13">In this section, we first derive an upper bound for the bounded Ε -first algorithm, for any given Ε value.</a>
<a href="#14" id="14">We then show that by efficiently tuning the value of Ε , we can refine the upper bound to {a mathematical formula}O(B23) (Section 5.1).</a>
<a href="#15" id="15">In addition, we also investigate the performance of the modified version of the Ε -first, where the uniform exploration phase is replaced with Successive Rejects (SR), a state-of-the-art pure exploration algorithm [3].</a>
<a href="#16" id="16">In particular, we also provide an {a mathematical formula}O(B23) bound for this modified version, however, with larger coefficient constants (Section 5.2).</a>
<a href="#17" id="17">This implies that even with this more sophisticated exploration method, we cannot achieve a better performance, compared to that of uniform exploration.</a>
<a href="#18" id="18">Recall that both {a mathematical formula}Auni and {a mathematical formula}Agreedy together form sequence {a mathematical formula}A Ε -first, which is the policy generated by the bounded Ε -first algorithm.</a>
<a href="#19" id="19">The expected reward for this policy can be expressed as the sum of the expected performance of {a mathematical formula}Auni and {a mathematical formula}Agreedy.</a>
<a href="#20" id="20">That is:{a mathematical formula}</a>
<a href="#21" id="21">Theorem 6</a>
<a href="#22" id="22">Now, by using elementary algebra, we can show that by setting{a mathematical formula} the upper bound given in Theorem 1 is minimised.</a>
<a href="#23" id="23">Thus, we get: Let{a mathematical formula} Ε optdenote the abovementioned value that minimises Eq.</a>
<a href="#24" id="24">(10)and{a mathematical formula}0< Β <1.</a>
<a href="#25" id="25">By setting the exploration budget to be{a mathematical formula}B Ε opt, with at least probability Β , the regret of the bounded Ε -first algorithm is at most{a mathematical formula}</a>
<a href="#26" id="26">Let{a mathematical formula}0< Ε , Β <1.</a>
<a href="#27" id="27">Suppose that{a mathematical formula} Ε B ≥ ∑ j=1Ncj.</a>
<a href="#28" id="28">With at least probability Β , the performance regret of the bounded Ε -first with SR exploration approach is at most{a mathematical formula}In addition, by optimally tuning Ε , we can show that the regret is at most{a mathematical formula}</a>
<a href="#29" id="29">Note that our algorithm approaches the theoretical optimum by up to {a mathematical formula}75% (in the cases of moderate, large and extreme budgets), while it achieves {a mathematical formula}61% of the optimal solution's performance in the scenario with small budgets.</a>
<a href="#30" id="30">This confirms the theoretical regret bounds that show that our solution quality approaches the optimum with a growing budget.</a>
<a href="#31" id="31">Here, the significantly higher performance compared to the benchmarks is due to the ability of our algorithm to take into account the workers' task limits and divide the high budget between several workers.</a>
<a href="#32" id="32">In addition, our algorithm outperforms the others by up to {a mathematical formula}162% (for the case of budget {a mathematical formula}B=$10,000).</a>
<a href="#33" id="33">We can also see that when the budget is sufficiently large, bounded KUBE achieves a higher performance, compared to other benchmarks.</a>
<a href="#34" id="34">However, it can still only achieve less than {a mathematical formula}60% of the bounded Ε -first.</a>
<a href="#35" id="35">In this section, we investigate the performance of all algorithms when we increase the number of candidates available for a crowdsourcing project.</a>
<a href="#36" id="36">Settings with a large number of candidates are likely to create new challenges for the learning approaches (bounded Ε -first, budget-limited Ε -first and trialsourcing), because these rely on exploring all candidates first prior to exploitation.</a>
<a href="#37" id="37">As in Section 6.3.1, our approach, bounded Ε -first, performs significantly better than all other benchmarks when the budget is high.</a>
<a href="#38" id="38">Here, the higher budget also allows it to sustain a high quality of around 80 – 90% of the optimal even when there are a few hundreds of candidates.</a>
<a href="#39" id="39">We now turn to the investigation of whether we can improve the performance of the bounded Ε -first algorithm by replacing the uniform exploration approach with other policies.</a>
</body>
</html>