<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:428">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}S is the finite set of states;</a>
<a href="#1" id="1">{a mathematical formula}P(s ′ |s,a) represents the probability that {a mathematical formula}s ′ ∈ S is reached after applying action {a mathematical formula}a ∈ A in state {a mathematical formula}s ∈ S; and</a>
<a href="#2" id="2">Definition 3</a>
<a href="#3" id="3">Ε -consistentGiven an SSP {a mathematical formula}S, a value function V for {a mathematical formula}S is Ε -consistent if{a mathematical formula} where {a mathematical formula}S ′ =S Π V, i.e., the states reachable from {a mathematical formula}s0 when following a greedy policy {a mathematical formula} Π V.</a>
<a href="#4" id="4">The functions {a mathematical formula}R(s,V) and {a mathematical formula}R(S,V) are known as the Bellman residual w.r.t.</a>
<a href="#5" id="5">V of the state s and the SSP {a mathematical formula}S, respectively.</a>
<a href="#6" id="6">By (2), if V is 0-consistent, then V equals {a mathematical formula}V ⁎ .</a>
<a href="#7" id="7">Definition 8</a>
<a href="#8" id="8">Another important relationship between SSPs and short-sighted SSPs is through their policies.</a>
<a href="#9" id="9">To formalize this relationship, we first define the concept of t-closed policy w.r.t.</a>
<a href="#10" id="10">s, i.e., policies that can be executed from s independent of the probabilistic outcome of actions for at least t actions without replanning: t-closed policyA policy Π for an SSP {a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 is t-closed w.r.t.</a>
<a href="#11" id="11">a state {a mathematical formula}s ∈ S if, for all {a mathematical formula}s ′ ∈ R Π ∩ S Π , {a mathematical formula} Δ (s,s ′ ) ≥ t.</a>
<a href="#12" id="12">Proof</a>
<a href="#13" id="13">Given an SSP{a mathematical formula}S= 〈 S,s0,G,A,P,C 〉 , for{a mathematical formula}t ≥ |S|, every t-closed policy w.r.t.</a>
<a href="#14" id="14">{a mathematical formula}s0for{a mathematical formula}Sis also a closed policy for{a mathematical formula}S.Since Π is t-closed w.r.t.</a>
<a href="#15" id="15">{a mathematical formula}s0 for {a mathematical formula}t ≥ |S|, then for all {a mathematical formula}s ′ ∈ R Π ∩ S Π , {a mathematical formula} Δ (s0,s ′ ) ≥ |S|.</a>
<a href="#16" id="16">By the definition of {a mathematical formula}S Π , we have that all {a mathematical formula}s ′ ∈ S Π is reachable from {a mathematical formula}s0 when following Π .</a>
<a href="#17" id="17">Thus, {a mathematical formula} Δ (s0,s ′ )<|S| because there exists a trajectory from {a mathematical formula}s0 to {a mathematical formula}s ′ that visits each state at most once, i.e., that uses at most {a mathematical formula}|S| − 1 actions.</a>
<a href="#18" id="18">SSiPP obtains the next state {a mathematical formula}s ′ from the current state s by either executing or sampling one outcome of the optimal policy {a mathematical formula} Π Ss,t ⁎ of the current short-sighted SSP (Algorithm 1 line 11).</a>
<a href="#19" id="19">This procedure is repeated until {a mathematical formula}s ′ is a goal state, either from the original SSP or an artificial goal.</a>
</body>
</html>