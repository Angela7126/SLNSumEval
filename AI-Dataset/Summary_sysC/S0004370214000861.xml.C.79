<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:248">
</head>
<body bgcolor="white">
<a href="#0" id="0">AfD obtains {a mathematical formula}S Α by selecting a subset of features from the original state space S with which it can predict the action that a human teacher has taken in a set of demonstrations H. Learning in {a mathematical formula}S Α can be significantly more efficient because a linear reduction in the number of features leads to an exponential reduction in the size of the state space.</a>
<a href="#1" id="1">ADA+RL adds another step, policy improvement, in which we use reinforcement learning techniques to find the optimal policy that can be represented in the abstract state space {a mathematical formula}S Α .</a>
<a href="#2" id="2">Additionally, by updating the model more frequently, the algorithm explores the interesting parts of the state space faster, and as the policy improves, the approximation focuses on accurately representing the part of the state space that is most often visited by the optimal policy.</a>
<a href="#3" id="3">A linear model cannot represent a policy that leads the agent from an arbitrary cell to an arbitrary goal cell, because whether one action is preferred over another does not depend on any of the features of the state space, but on the relation between different features (the coordinates of the agent and the coordinates of the goal).</a>
<a href="#4" id="4">Recent TD methods [58] offer attractive theoretical properties, but so far these methods are limited to prediction, i.e., determining the value function {a mathematical formula}V Π of a given policy Π , as opposed to finding the optimal policy {a mathematical formula} Π ⁎ .</a>
</body>
</html>