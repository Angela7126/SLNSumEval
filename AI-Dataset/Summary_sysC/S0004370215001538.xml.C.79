<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:711">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we provide some insight on these issues, in the form of nine specific questions, by giving a comprehensive account of about thirty computer models, from the 1960s to nowadays, and their relationships, focussing on the range of intelligence test tasks they address, the purpose of the models, how general or specialised these models are, the AI techniques they use in each case, their comparison with human performance, and their evaluation of item difficulty.</a>
<a href="#1" id="1">Extending the parallel from the history of general game playing to computer models solving intelligence tests, the composition of an inventory of problems and the proposal of some criteria to compare the different systems is a crucial step.</a>
<a href="#2" id="2">We hope that our work can be a starting point towards a more systematic and general treatment of computer models solving intelligence tests and towards the development of (more) general intelligent systems passing arbitrary intelligence tests.</a>
<a href="#3" id="3">With regard to the classes of problems and systems considered in our inventory, we will not cover any informal conception about how any of these problems can be solved (e.g., from psychology) if this conception is not accompanied by a computer implementation and the application of the system to a set of intelligence test problems.</a>
<a href="#4" id="4">Taking this perspective, item difficulty can be either explained completely independent of humans by means of algorithmic information theory or based on the assumed complexity of the cognitive processes and representations necessary to solve a test item.</a>
<a href="#5" id="5">2, which resembles some exercises found in intelligence tests but with a theoretical assessment of difficulty (based on the size of the shortest program that generates the problem instance) instead of an experimental one based on how difficult humans find them.</a>
<a href="#6" id="6">On the one hand, algorithms are designed to capture an empirically-known phenomenon of cognition, for example, that humans transfer as many related knowledge elements as possible from one problem to another when reasoning by analogy [85], [86]; on the other hand, models are compared with specific empirical data such as solution times or errors [87].</a>
<a href="#7" id="7">Basically, the criteria for inclusion in this paper are (1) that the tests have been developed as part of human intelligence tests or other tests of mental abilities, or have been introduced in the context of cognitive systems addressing aspects of human intelligence, and (2) that the tests have been attempted by at least one computer model, as we will see in Section 5.</a>
<a href="#8" id="8">He thought that the choice of geometric-analogy problems was suitable because: (i) “ problems of this type require elaborate processing of complex line drawings ” , (ii) “ the form of problems, [ … ] more speculative, [ … ] presents an interesting paradigm of ‘ reasoning by analogy ’ ” , and (iii) “ problems of this type are widely regarded as requiring a considerable degree of intelligence for their solution and in fact are used as a touchstone of intelligence in various intelligence tests ” [10].</a>
<a href="#9" id="9">Considering the results obtained, the authors claim that the visual complexity involved in solving this kind of problems is smaller than the functional difficulty (rules to be applied).</a>
<a href="#10" id="10">However, in their case, these rules were implemented using the well-known cognitive architecture ACT-R [148], a cognitive architecture for simulating and understanding human cognition implemented as a production system, where each symbolic processes is controlled by subsymbolic equations (responsible for most learning processes) which estimate (and then decide) the relative cost and benefit associated with their execution.</a>
<a href="#11" id="11">However, there is no point in making a comparison between these three systems since their aims were completely different: the goal of Sanghi and Dowe [15] was not to excel in any specific task, the aim of Ragni and Klein [121] was to successfully solve large sets of number series from OEIS, and the goal of Siebers and Schmid [128] was to get cognitive plausibility for series actually found in intelligence tests (including failure on those difficult series for which humans fail) instead of excelling on performance.</a>
<a href="#12" id="12">The ultimate goal is to do it the other way round, to find good theories in artificial intelligence (based on computation or information theory) such that the difficulty that we estimate corresponds to the difficulty that is experimentally found by human subjects (e.g., this was the case in [62]).</a>
</body>
</html>