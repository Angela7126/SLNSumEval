<html>
<head>
<meta name="TextLength" content="SENT_NUM:15, WORD_NUM:509">
</head>
<body bgcolor="white">
<a href="#0" id="0">This article presents experiments demonstrating that the combination of these two intrinsic rewards enables the algorithm to learn an accurate model of a domain with no external rewards and that the learned model can be used afterward to perform tasks in the domain.</a>
<a href="#1" id="1">In addition, the experiments show that combining the agent's intrinsic rewards with external task rewards enables the agent to learn faster than using external rewards alone.</a>
<a href="#2" id="2">An alternative to r-iac is to learn a separate predictor of the change in model error and use its predicted values as the intrinsic reward to drive exploration [6].</a>
<a href="#3" id="3">For two different algorithms and tasks, they search over a broad set of possible task and agent specific intrinsic rewards and find rewards that make the agent learn faster than if it solely used external rewards.</a>
<a href="#4" id="4">The Policy Gradient Reward Design algorithm (pgrd) learns the best intrinsic rewards on-line for cases where the true reward function is given and the agent is limited in some way [15], [16], [17].</a>
<a href="#5" id="5">pgrd uses its knowledge of the true reward function to calculate the gradient of intrinsic rewards to agent return.</a>
<a href="#6" id="6">These methods affect the agent's exploration in the domain to speed up learning, but they typically require the user to have specific knowledge about what constitutes improvement in the task.</a>
<a href="#7" id="7">This agent should use intrinsic rewards to 1) efficiently learn a useful model of the domain's transition dynamics; and 2) explore in a developing curious way.</a>
<a href="#8" id="8">texplore's model learning algorithm starts by calculating the relative change in the state ({a mathematical formula}srel), then it updates the model for each feature with the new transition and updates the reward model.</a>
<a href="#9" id="9">Decision trees, committees of trees, random forests, support vector machines, neural networks, nearest neighbor, and tabular models were compared on their ability to predict the transition and reward models across three toy domains after being given a random sample of experiences in the domain.</a>
<a href="#10" id="10">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#11" id="11">For particularly difficult state transitions, the agent's model may make an incorrect prediction in all of its forest trees, and as some of the trees start to learn the transition, the variance increases and the agent moves to explore the true dynamics of how it works.</a>
<a href="#12" id="12">For this experiment, the algorithms are run for 3000 steps with their intrinsic rewards added to the previously used external reward function that rewards moving between rooms.</a>
<a href="#13" id="13">In the light world domain, by the time the algorithm has determined error is improving in a region, the agent has already learned a model of that region and no longer needs to explore there.</a>
<a href="#14" id="14">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
</body>
</html>