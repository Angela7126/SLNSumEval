<html>
<head>
<meta name="TextLength" content="SENT_NUM:28, WORD_NUM:918">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our model is based on similar assumptions, but it differs in three main aspects: i) utterances given to our learner offer a partial or total description of the scene, but they do not refer to things that are not in the scene; ii) we are not limited to word learning (our learning task is more complex: our system learns to generate and understand relevant utterances in a given scene); iii) we use a richer semantic representation.</a>
<a href="#1" id="1">Given a situation and an utterance by the teacher, the learner uses the transitively reduced implication graph {a mathematical formula}Hr in an attempt to determine the meaning of the teacher's utterance as follows.</a>
<a href="#2" id="2">For each successive word of the teacher's utterance, the learner finds the list of all predicate symbols such that there is an edge from the word to the predicate symbol in {a mathematical formula}Hr.</a>
<a href="#3" id="3">Some of the predicate symbols may then be removed from each word's list as follows.</a>
<a href="#4" id="4">The resulting set S of sequences of predicate symbols is then compared with the situation to try to determine the teacher's meaning.</a>
<a href="#5" id="5">In particular, the learner computes a variable-normalized representative of every meaning that is both supported by the situation and is such that its sequence of predicate symbols is in S – these become the learner's guess of the possible meanings of the teacher's utterance in the given situation.</a>
<a href="#6" id="6">These values are used in the learner's production process in such a way that general forms that are not repeatedly matched to teacher utterances become less and less likely to be used by the learner.</a>
<a href="#7" id="7">If the learner finds no possible meanings, or more than one, for the teacher's utterance, the learner does not attempt to update the information in its set of general forms.</a>
<a href="#8" id="8">In attempting to produce an utterance appropriate to the current situation, the learner finds all the meanings generated by its general forms using predicates from the current situation, and tests each meaning to see if it is denoting in the current situation, producing a set of possible denoting meanings for this situation.</a>
<a href="#9" id="9">For each variable atom that the learner has encountered in a unique teacher meaning, there is a decision tree that determines what sequence of words to produce for that atom in the context of the whole meaning.</a>
<a href="#10" id="10">As an example, consider the atom {a mathematical formula}re1(x2).</a>
<a href="#11" id="11">The teacher may find the learner's utterance correct, incorrect but correctable, or incorrect and uncorrectable.</a>
<a href="#12" id="12">In the case that the learner's utterance is found incorrect but correctable, the teacher chooses a possible correction for it.</a>
<a href="#13" id="13">Then the teacher randomly chooses whether or not to use the correction as its utterance; a correction probability parameter governs this choice.</a>
<a href="#14" id="14">If the learner produced no utterance or an uncorrectable one, or if the teacher did not choose to correct the learner's utterance, then the teacher's utterance is chosen uniformly at random from the denoting utterances for the situation.</a>
<a href="#15" id="15">Thus, when the learner's utterance is correct there is some probability that the teacher will simply repeat it.</a>
<a href="#16" id="16">The process used by the teacher to analyze the learner's utterance is as follows.</a>
<a href="#17" id="17">If the learner's utterance is equal to one of the correct denoting utterances for the situation, the teacher classifies the learner's utterance as correct.</a>
<a href="#18" id="18">If the learner's utterance is not correct, the teacher “ translates ” the learner's utterance into a sequence of predicates by using the adult meaning transducer for the language.</a>
<a href="#19" id="19">These interactions show the learner beginning to comprehend the teacher's utterances and acquiring and using both incorrect and correct general forms, producing both incorrect and correct denoting utterances.</a>
<a href="#20" id="20">Russian and Greek require the largest number of interactions, but a natural question arises: is this for computational reasons (the size of the target machines) or linguistic reasons?</a>
<a href="#21" id="21">To avoid this kind of behavior, the basic n-gram model was enhanced to reject randomly generated utterances that are either (1) of a length not observed in the input list of utterances, or (2) contain words {a mathematical formula}w1 and {a mathematical formula}w2 in different positions of the utterance that have occurred in utterances fairly frequently, but never together in the same utterance.</a>
<a href="#22" id="22">This is a very simplified model of the interactions in our system: different tokens represent different possible (atomic) utterances, the teacher can always understand what the learner intended to say, and the learner completely corrects its utterance with one exposure to the correct version.</a>
<a href="#23" id="23">The interaction cycle is as follows: the learner produces an utterance (token), then the teacher produces an utterance (token), and the learner uses the teacher's utterance (token) as a model for correct utterances (tokens) of that kind in the future.</a>
<a href="#24" id="24">We assume that all of the learner's tokens initially have correctness bit 0 and analyze the expected number of interactions until all of the learner's tokens have correctness bit 1.</a>
<a href="#25" id="25">In the case of the non-correcting teacher, the process reduces immediately to the coupon collection process: the choices of the learner are irrelevant, and each learner's token has its correctness bit changed from 0 to 1 the first time the teacher randomly draws its own token with the same number.</a>
<a href="#26" id="26">Thus in this case, the expected number of interactions until the learner's tokens are all corrected is {a mathematical formula}nln ⁡ n+O(n).</a>
<a href="#27" id="27">The learner continues to produce unintelligible utterances, utterances with errors in form and meaning, and correct utterances, while refining its co-occurrence graph (allowing it to understand the teacher's utterances more accurately), acquiring new general forms, and improving its rules for word choice.</a>
</body>
</html>