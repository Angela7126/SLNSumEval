<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:639">
</head>
<body bgcolor="white">
<a href="#0" id="0">This toy example illustrates two key points.</a>
<a href="#1" id="1">First, the problem involves a mixture of numerical variables (coordinates, sizes of block 2) and Boolean variables, along with hard rules that control the feasible space of the optimization procedure (conditions (i) and (ii)), and costs — or soft rules — which control the shape of the optimization landscape.</a>
<a href="#2" id="2">This is the kind of problem that can be solved in terms of Optimization Modulo Linear Arithmetic, OMT({a mathematical formula}LRA).</a>
<a href="#3" id="3">Second, it is possible to estimate the weights {a mathematical formula}w1, {a mathematical formula}w2 from data in order to learn what kind of blocks are to be considered optimal.</a>
<a href="#4" id="4">The goal of our learning procedure is precisely to find a good set of weights from examples.</a>
<a href="#5" id="5">In the following we will describe how such a learning task can be framed within the structured output SVMs framework.</a>
<a href="#6" id="6">We consider the problem of learning from a training set of n complex objects {a mathematical formula}{(Ii,Oi)}i=1n, where each object {a mathematical formula}(I,O) is represented as a set of Boolean and rational variables:{a mathematical formula} We indicate Boolean variables using predicates such as {a mathematical formula}touching(i,j), and write rational variables as lower-case letters, e.g.</a>
<a href="#7" id="7">cost, distance, x, y.</a>
<a href="#8" id="8">Please note that while we write Boolean variables using a First-Order syntax for readability, our method does require the grounding of all Boolean predicates prior to learning and inference.</a>
<a href="#9" id="9">In the present formulation, we assume objects to be composed of two parts: I is the input (or observed) part, while O is the output (or query) part.</a>
<a href="#10" id="10">{sup:9} The learning problem is defined by a set of m constraints{a mathematical formula}{ Φ k}k=1m.</a>
<a href="#11" id="11">Each constraint {a mathematical formula} Φ k is either a Boolean- or rational-valued function of the object {a mathematical formula}(I,O).</a>
<a href="#12" id="12">For each Boolean-valued constraint {a mathematical formula} Φ k, we denote its indicator function as {a mathematical formula}1k(I,O), which evaluates to 1 if the constraint is satisfied and to − 1 otherwise (the choice of − 1 to represent falsity is customary in the max-margin literature).</a>
<a href="#13" id="13">Similarly, we refer to the cost of a rational-valued constraint {a mathematical formula} Φ k as {a mathematical formula}ck(I,O) ∈ Q.</a>
<a href="#14" id="14">The feature space representation of an object {a mathematical formula}(I,O) is given by the feature vector{a mathematical formula} Ψ (I,O), which is a function of the constraints.</a>
<a href="#15" id="15">Each soft constraint {a mathematical formula} Φ k has an associated finite weight {a mathematical formula}wk ∈ Q (to be learned from the data), while hard constraints have no associated weight.</a>
<a href="#16" id="16">We denote the vector of learned weights as {a mathematical formula}w:=(w1,w2, … ,wm), and its Euclidean norm as {a mathematical formula} ‖ w ‖ .</a>
<a href="#17" id="17">Here the input I to the problem is the observed block {a mathematical formula}(x1,y1,dx1,dy1) while the output O is the generated block {a mathematical formula}(x2,y2,dx2,dy2).</a>
<a href="#18" id="18">In order to encode the set of constraints {a mathematical formula}{ Φ k} that underlie both the learning and the inference problems, it is convenient to first introduce a background knowledge of predicates expressing facts about the relative positioning of blocks.</a>
<a href="#19" id="19">To this end we add a fresh predicate {a mathematical formula}left(i,j), that encodes the fact that “ a generic block of index i touches a second block j from the left ” , defined as follows:{a mathematical formula} Similarly, we add analogous predicates for the other directions: {a mathematical formula}right(i,j), {a mathematical formula}below(i,j), {a mathematical formula}over(i,j) (see Fig.</a>
<a href="#20" id="20">2 for the full definitions).</a>
<a href="#21" id="21">The hard constraints represent the fact that the output O should be a valid block within the bounding box (all the constraints {a mathematical formula} Φ k are implicitly conjoined):{a mathematical formula} Then we require the output block O to “ touch ” the input block I:{a mathematical formula} Note that whenever this rule is satisfied, both conditions (i) and (ii) of the toy example hold, i.e.</a>
</body>
</html>