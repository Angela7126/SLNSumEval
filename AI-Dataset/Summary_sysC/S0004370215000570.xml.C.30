<html>
<head>
<meta name="TextLength" content="SENT_NUM:15, WORD_NUM:310">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our POMDP model uses discrete actions and observations.</a>
<a href="#1" id="1">However, instead of forcing the robotic planning problem into a manageable discrete state space as is done e.g.</a>
<a href="#2" id="2">The LIFT action tries to lift an object to expose the objects behind it and allows the agent to gather more information about the occluded objects.</a>
<a href="#3" id="3">A small negative reward representing time cost is associated with the action.</a>
<a href="#4" id="4">Note that the action takes less time than moving the object into the dishwasher.</a>
<a href="#5" id="5">The POMDP planning method described in Section 3.2 is initialized by 10 offline policy improvement rounds.</a>
<a href="#6" id="6">Then, at each time step 4 improvement rounds for the current belief are executed.</a>
<a href="#7" id="7">To evaluate the benefit of planning under uncertainty, the POMDP approach is also compared against heuristic decision making: The heuristic manipulation method assumes that observations are accurate and deterministic.</a>
<a href="#8" id="8">Overall, POMDP planning achieves higher reward than the heuristic manipulation approach.</a>
<a href="#9" id="9">Interestingly, the performance difference between the heuristic approach with and without grasp history is not significant.</a>
<a href="#10" id="10">6 shows performance for the heuristic manipulation method and the POMDP method with a planning horizon of three for different reward choices.</a>
<a href="#11" id="11">In this case, the problem requires no multi-step planning, and the heuristic policy of moving all cups that appear dirty into the dishwasher is sufficient.</a>
<a href="#12" id="12">To test this, and to test whether our observation and state space models are applicable in physical robot arm experiments (we tested the model also in several other robot arm experiments which are discussed below), we performed robotic manipulation in a setup with dirty cups which are not occluded.</a>
<a href="#13" id="13">In contrast to a greedy approach, a POMDP may select actions that gather information, but do not yield immediate reward, when the problem so requires.</a>
<a href="#14" id="14">In the multi-object manipulation experiments, the robot had to decide between lifting objects to gather information or moving objects that appear dirty into the dishwasher.</a>
</body>
</html>