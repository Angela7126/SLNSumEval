<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:448">
</head>
<body bgcolor="white">
<a href="#0" id="0">Given an MDP, the agent aims at identifying the policy {a mathematical formula} Π ⁎ such that {a mathematical formula}v Π ⁎ (s) is maximum for each s. We denote such an optimal value function as {a mathematical formula}v ⁎ (and analogously {a mathematical formula}q ⁎ for the action value function).</a>
<a href="#1" id="1">The model {a mathematical formula}Dm is the one on which a planning algorithm would compute a plan to execute in the real domain.</a>
<a href="#2" id="2">We will then let the agent perform RL over {a mathematical formula}Dr.</a>
<a href="#3" id="3">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#4" id="4">The agent can learn a policy for the reduced MDP {a mathematical formula}Dr, expanding it when necessary if the system transitions into a state where no action is available.</a>
<a href="#5" id="5">In this section, we analyze how the model {a mathematical formula}Dm affects the ability of the agent to learn an optimal policy.</a>
<a href="#6" id="6">It follows that:{a mathematical formula} That is, an action a is available to the agent in a state s if and only if its value in the model for the abstract state {a mathematical formula}o(s) is close to the optimal value by more than the threshold.</a>
<a href="#7" id="7">1, with the agent starting at {a mathematical formula} 〈 10,0 〉 .</a>
<a href="#8" id="8">We compare three agents across all experiments, for each domain D and model {a mathematical formula}Dm: an agent that makes decisions by planning on {a mathematical formula}Dm only, an agent that makes decisions by reinforcement learning on D, and DARLING that uses {a mathematical formula}Dm to compute a partial policy, and limits its exploration to {a mathematical formula}Dr. We refer to the first agent as the P agent, to the second as the RL agent, and to the third as the PRL agent.</a>
<a href="#9" id="9">The P and PRL agents do planning with answer set programming over the same model, and for the PRL agent we always set {a mathematical formula} Μ =1.5 as the parameter for the threshold on plan length.</a>
<a href="#10" id="10">This initial policy returns the action south for all the states between {a mathematical formula} 〈 10,1 〉 and {a mathematical formula} 〈 10,8 〉 in which the optimal action becomes north after episode 200.</a>
<a href="#11" id="11">The average performance of the agent would be {a mathematical formula}G=0.5t+0.5T where t is the time to complete the optimal plan (shown in the figure) and T is the timeout.</a>
</body>
</html>