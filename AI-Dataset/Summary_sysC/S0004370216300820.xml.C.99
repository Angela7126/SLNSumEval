<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:471">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively.</a>
<a href="#1" id="1">Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/.</a>
<a href="#2" id="2">As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.</a>
<a href="#3" id="3">In our recent work [16], we proposed a method that exploits the structural knowledge derived from semantic networks, together with distributional statistics from text corpora, to produce effective representations of individual word senses or concepts.</a>
<a href="#4" id="4">Secondly, each vector represents a concept, irrespective of its language, in a unified semantic space having concepts as its dimensions, permitting direct comparison of different representations across languages and hence enabling cross-lingual applications.</a>
<a href="#5" id="5">In this article, we improve our approach, referred to as Nasari (Novel Approach to a Semantically-Aware Representation of Items) henceforth, and extend their application to a wider range of tasks in lexical semantics.</a>
<a href="#6" id="6">We put forward an approach that allows us to plug in an arbitrary word embedding representation with that of our lexical vector representations, providing three main advantages: (1) benefiting from the word-based knowledge derived as a result of learning from massive corpora for our sense-level representation; (2) reducing the dimensionality of our lexical space to a fixed-size continuous space; and (3) providing a shared semantic space between words and synsets (more details in Section 4), hence enabling a direct comparison of words and synsets.</a>
<a href="#7" id="7">Specifically, given an input text {a mathematical formula}T and a space of word embeddings E, we first calculate the lexical vector of {a mathematical formula}T (i.e., {a mathematical formula}v → lex(T)) as explained in Section 3.2 and then map our lexical vector to the semantic space E as follows:{a mathematical formula} where {a mathematical formula}E(w) is the embedding-based representation of the word w in E, and {a mathematical formula}rank(w,v → lex(T)) is the rank of the dimension corresponding to the word w in the lexical vector {a mathematical formula}v → lex(T), thus giving more importance to the higher weighted dimensions.</a>
<a href="#8" id="8">In contrast, our approach combines expert-based and encyclopedic knowledge from two different types of resource, providing three advantages: (1) more effective measurement of similarity based on rich semantic representations, (2) the possibility of measuring cross-resource semantic similarity, i.e., between Wikipedia pages and WordNet synsets, and (3) the possibility of comparing the semantics of word senses across different languages.</a>
</body>
</html>