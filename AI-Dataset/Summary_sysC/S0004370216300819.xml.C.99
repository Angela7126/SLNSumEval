<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:443">
</head>
<body bgcolor="white">
<a href="#0" id="0">For instance, should the localization module output:{a mathematical formula} this would cause the ASP theory to be unsatisfiable, therefore, this observation according to which the robot would be at two locations at the same time has to be discarded as a localization error.</a>
<a href="#1" id="1">[1] proposed a method close in motivation to ours, in which the agent uses a crude model of a deterministic dynamical system to identify a direction for policy improvement, and then executes trials in the real world along that direction only, updating the model with the data from the environment.</a>
<a href="#2" id="2">Thanks to the subsequent learning phase, the accuracy of this model is less critical than if planning were used with no learning.</a>
<a href="#3" id="3">For this reason, in all our experiments we chose to use a symbolic, discrete, deterministic model of the environment, which does not represent action costs.</a>
<a href="#4" id="4">Therefore, as the metric for the threshold, we used plan length.</a>
<a href="#5" id="5">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#6" id="6">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#7" id="7">We define this reduced MDP {a mathematical formula}Dr= 〈 S,Ar,fr,r 〉 , based on the original {a mathematical formula}D= 〈 S,A,f,r 〉 where the actions available are restricted to those returned by the partial policy:{a mathematical formula} The transition function is defined from the one of D:{a mathematical formula} but it is undefined for actions {a mathematical formula}a ∉ Ar(s).</a>
<a href="#8" id="8">While the partial policy (and therefore the model {a mathematical formula}Dm) determines the possibility to choose a given action in a particular state, the probability distribution over the next state is unmodified for the actions that are available.</a>
<a href="#9" id="9">Thus, an action can make the current state transition into a state that is not what {a mathematical formula}Dm predicts: if the agent takes action a in state s, it can land in a state {a mathematical formula}s ′ such that {a mathematical formula}s ′ ∉ o − 1(fm(o(s),a)).</a>
<a href="#10" id="10">In Ryan's work, resilience to inaccuracies of the model is not taken into account, and the planner is used to generate optimal plans only.</a>
<a href="#11" id="11">Therefore, for an action to be considered, Equation (6) has to be satisfied with {a mathematical formula}t(s)=0 for each state, which also means it is satisfied with equality.</a>
</body>
</html>