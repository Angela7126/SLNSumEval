<html>
<head>
<meta name="TextLength" content="SENT_NUM:22, WORD_NUM:457">
</head>
<body bgcolor="white">
<a href="#0" id="0">Word Sense Disambiguation.</a>
<a href="#1" id="1">We proposed a simple framework for a knowledge-rich unsupervised disambiguation system.</a>
<a href="#2" id="2">Our system obtained state-of-the-art results on multilingual All-Words Word Sense Disambiguation using Wikipedia as sense inventory, evaluated on the SemEval-2013 dataset [95], and on English All-Words Word Sense Disambiguation using WordNet as sense inventory, evaluated on the SemEval-2007 [111] and SemEval-2013 [95] datasets.</a>
<a href="#3" id="3">Additionally, we performed an experiment to measure the reliability of our semantic representations for named entities, obtaining the best results among all unsupervised systems and near state-of-the-art performance on the SemEval-2015 WSD dataset [88].</a>
<a href="#4" id="4">Semantic similarity is the most popular benchmark for the evaluation of different semantic representation techniques.</a>
<a href="#5" id="5">The task here is to measure the semantic closeness of two linguistic items.</a>
<a href="#6" id="6">The similarity of two items can be directly computed by comparing their corresponding vector representations.</a>
<a href="#7" id="7">As we mentioned in Section 3.5, we opted for Weighted Overlap as our vector comparison method for lexical and unified representations, and cosine for the embedded representations.</a>
<a href="#8" id="8">Note that by using our approach we obtain representations for individual BabelNet synsets.</a>
<a href="#9" id="9">Moreover, because BabelNet merges different resources, our representations can be used to calculate the semantic similarity between any two semantic units within and across different resources, for instance between two Wikipedia pages, two WordNet synsets, or a Wikipedia page and a WordNet synset.</a>
<a href="#10" id="10">Throughout this section on the tasks based on semantic similarity, Nasarilexical and Nasariunified represent the systems based on the lexical and unified vectors, respectively.</a>
<a href="#11" id="11">We refer to the combination of both lexical and unified vectors as Nasari.</a>
<a href="#12" id="12">This combination is based on the average similarity scores given by lexical and unified vectors for each sense pair.</a>
<a href="#13" id="13">We also report results of our Nasariembed vector representations which use the pre-trained Word2Vec vectors as input.</a>
<a href="#14" id="14">We performed experiments on monolingual word similarity for English and other languages (presented in Sections 6.1.1 and 6.1.2, respectively) and cross-lingual similarity (presented in Section 6.1.3).</a>
<a href="#15" id="15">Additionally, we evaluate our embedded representations in a cross-level semantic similarity task in Section 6.1.4.</a>
<a href="#16" id="16">Finally, we evaluated our embedded representations on the word to sense semantic similarity task.</a>
<a href="#17" id="17">Recall from Section 4.2 that our embedded vector representations share the same space with word embeddings.</a>
<a href="#18" id="18">Therefore, in order to calculate the similarity between a word and a sense, we only have to compute the cosine similarity between their respective vector representations.</a>
<a href="#19" id="19">In this experiment, we take the BabelNet sense representation of a word sense if it is modeled by Nasari.</a>
<a href="#20" id="20">Otherwise, in order to increase the coverage of our system, we simply take the word embedding of the lemma of the word sense as its representation.</a>
<a href="#21" id="21">Given a set of target words in a text {a mathematical formula}T, we build a lexical vector for the context, as explained in Section 3.2.</a>
</body>
</html>