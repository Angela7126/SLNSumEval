<html>
<head>
<meta name="TextLength" content="SENT_NUM:30, WORD_NUM:683">
</head>
<body bgcolor="white">
<a href="#0" id="0">For the sake of clarity, let us initially focus on the case where there is a single Nash equilibrium of an interaction game.</a>
<a href="#1" id="1">During each iteration of the numerical optimization, a new set of reward weights are generated and an optimum policy found for each patroller.</a>
<a href="#2" id="2">{a mathematical formula} Μ Π Ie is found for each {a mathematical formula} Σ e and the normalized Ω weights are then recomputed.</a>
<a href="#3" id="3">We expect the initial weights to be nearly uniform because learned policies do not correctly model the observed behavior for the most part.</a>
<a href="#4" id="4">Eventually, as the policies become accurate, projecting the true interaction behavior gains importance and the weights between equilibria diverge relatively.</a>
<a href="#5" id="5">We explore the dynamics of the evolution of Ω during optimization in Section 6.2.1.</a>
<a href="#6" id="6">This new method for multiple interacting robots with unknown equilibrium in the presence of occlusion is labeled mIRL{sup: ⁎ }+Int.</a>
<a href="#7" id="7">Let Ψ : {a mathematical formula}S×A → S map an observed robot's transition from state s given action a to a particular next state, {a mathematical formula}s ′ .</a>
<a href="#8" id="8">The function Ψ gives the intended outcome of each action from each state.</a>
<a href="#9" id="9">We may view Ψ as a deterministic transition function.</a>
<a href="#10" id="10">In robotic scenarios, actions often lead to movement.</a>
<a href="#11" id="11">Therefore, the patroller may reveal its intended next state given by {a mathematical formula} Ψ (s,a) during this movement.</a>
<a href="#12" id="12">For instance, a robot moving forward is obviously intending to reach the next state in front of it.</a>
<a href="#13" id="13">Actions taken by the agent may not always result in the intended outcome leading instead to some other state.</a>
<a href="#14" id="14">We therefore focus on learning the probability of transitioning to the intended next state given a state-action pair for an observed robot I, {a mathematical formula}TI(s,a, Ψ (s,a)), and this probability is estimated from the available trajectory data.</a>
<a href="#15" id="15">The remaining probability mass, {a mathematical formula}1 − TI(s,a, Ψ (s,a)) is then distributed according to some error model, such as uniformly among all other states or perhaps to the intended outcomes of other actions.</a>
<a href="#16" id="16">This approach requires that Ψ is known to the learning agent.</a>
<a href="#17" id="17">An added bonus is if the precise error model is also known to the learner although this is challenging in the context of robotics.</a>
<a href="#18" id="18">Despite this, not knowing the probability of the expert reaching its intended state is a significant relaxation of the requirement by existing IRL methods that the full stochastic transition function inclusive of all probabilities be known.</a>
<a href="#19" id="19">Indeed, in domains involving mobile robots where the actions taken correspond to movement, {a mathematical formula} Ψ (s,a) may be determined accurately and we follow this approach in our experimentation.</a>
<a href="#20" id="20">Section 5.6 will explore the error model in more detail.</a>
<a href="#21" id="21">We evaluate mIRL{a mathematical formula}/T ⁎ +Int for learning the experts' transition functions as previously described in Section 5. mIRL{a mathematical formula}/T ⁎ +Int begins by estimating the transition functions of others, after which it collapses into mIRL{sup: ⁎ }+Int learning reward weights Θ .</a>
<a href="#22" id="22">Incorrectly identifying and labeling the state-action pairs of I and J may negatively impact the accuracy of the learned transition function, particularly in the presence of limited data due to occlusion.</a>
<a href="#23" id="23">To help alleviate this, mIRL{a mathematical formula}/T ⁎ +Int uses an iterative approach that utilizes the learned policies of I and J from the previous iteration to help correct observation errors.</a>
<a href="#24" id="24">To accomplish this, the observed trajectory of each robot is updated using the respective learned policies, replacing the perceived action with the one prescribed by the policy for the states in the trajectory.</a>
<a href="#25" id="25">Then, mIRL{a mathematical formula}/T ⁎ +Int restarts with this newly revised trajectory.</a>
<a href="#26" id="26">This procedure iterates until the labels for the observed state-action pairs do not change because the policies fixate.</a>
<a href="#27" id="27">• Known R, similar to KnownPolicy, is an upper bound on the performance of L in our scenario.</a>
<a href="#28" id="28">Here, the reward function is known and the transition probability to the intended state is manually set to various probability values because the true transition probabilities of a TurtleBot are not known.</a>
<a href="#29" id="29">Specifically, set {a mathematical formula}TI(s,a, Ψ (s,a))=x for all states and actions where {a mathematical formula}x ∈ {0.7,0.8,0.9,0.95}.</a>
</body>
</html>