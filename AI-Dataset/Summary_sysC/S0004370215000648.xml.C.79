<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:650">
</head>
<body bgcolor="white">
<a href="#0" id="0">A major drawback of this flexibility is the computational cost, as each single inference step on continuous variables requires to solve a global optimization problem, making the approach infeasible for addressing medium to large scale problems.</a>
<a href="#1" id="1">Overall, for the scope of this paper, it is important to highlight the fact that OMT solvers are available which, thanks to the underlying SAT and SMT technologies, can handle problems with a large number of hybrid variables (in the order of thousands, at least for the {a mathematical formula}LRA theory).</a>
<a href="#2" id="2">To this extent, we notice that the underlying theories and {a mathematical formula}T-solvers provide the meaning and the reasoning capabilities for specific predicates and function symbols (e.g., the {a mathematical formula}LRA-specific symbols “ ≥ ” and “ + ” , or the {a mathematical formula}AR-specific symbols “ read(...) ” , “ write(...) ” ) that would otherwise be very difficult to describe, or to reason over, with logic-based automated reasoning tools — e.g., traditional first-order theorem provers cannot handle arithmetical reasoning efficiently — or with arithmetical ones — e.g., DLP, ILP, MILP, LGDP tools [41], [42], [43] or CLP tools [44], [45], [46] do not handle symbolic theory-reasoning on theories like {a mathematical formula}EUF or {a mathematical formula}AR.</a>
<a href="#3" id="3">First, the problem involves a mixture of numerical variables (coordinates, sizes of block 2) and Boolean variables, along with hard rules that control the feasible space of the optimization procedure (conditions (i) and (ii)), and costs — or soft rules — which control the shape of the optimization landscape.</a>
<a href="#4" id="4">Inference amounts to finding the most compatible output {a mathematical formula}O ⁎ for a given input I, which equates to solving the following optimization problem:{a mathematical formula} Performing inference on structured domains is non-trivial, since the maximization ranges over an exponential (and possibly unbounded) number of candidate outputs.</a>
<a href="#5" id="5">In order to learn the weights from a training set of n examples, one needs to define a non-negative loss function{a mathematical formula} Δ (I,O,O ′ ) that, for any given observation I, quantifies the penalty incurred when predicting {a mathematical formula}O ′ instead of the correct output O.</a>
<a href="#6" id="6">Learning can be expressed as the problem of finding the weights w that minimize the per-instance error {a mathematical formula} Ξ i and the model complexity [23]:{a mathematical formula} Here the constraints require that the compatibility between any input {a mathematical formula}Ii and its corresponding correct output {a mathematical formula}Oi is always higher than that with all wrong outputs {a mathematical formula}O ′ by a margin, with {a mathematical formula} Ξ i playing the role of per-instance violations.</a>
<a href="#7" id="7">The CP algorithm is generic, meaning that it can be adapted to any structured prediction problem as long as it is provided with: (i) a joint feature space representation Ψ of input – output pairs (and consequently a compatibility function f); (ii) an oracle to perform inference, i.e.</a>
<a href="#8" id="8">In order to encode the set of constraints {a mathematical formula}{ Φ k} that underlie both the learning and the inference problems, it is convenient to first introduce a background knowledge of predicates expressing facts about the relative positioning of blocks.</a>
<a href="#9" id="9">The hard constraints represent the fact that the output O should be a valid block within the bounding box (all the constraints {a mathematical formula} Φ k are implicitly conjoined):{a mathematical formula} Then we require the output block O to “ touch ” the input block I:{a mathematical formula} Note that whenever this rule is satisfied, both conditions (i) and (ii) of the toy example hold, i.e.</a>
<a href="#10" id="10">This can be explained by observing that the cutting-plane algorithm does not actually require the separation oracle to be perfect: as long as the approximation is consistent with the hard rules (which is necessarily the case even in sub-optimal solutions), the constraint added to the working set {a mathematical formula}W at each iteration still restricts the quadratic program in a sound manner (see Algorithm 1).</a>
</body>
</html>