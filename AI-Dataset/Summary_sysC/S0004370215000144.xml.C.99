<html>
<head>
<meta name="TextLength" content="SENT_NUM:33, WORD_NUM:1251">
</head>
<body bgcolor="white">
<a href="#0" id="0">Overall, the results of the competition indicate significant progress with respect to previous competitions, but they also reveal that some issues remain open and need further research, such as the coverage of temporal planners when concurrency is required and the performance in the multi-core track.</a>
<a href="#1" id="1">As a novelty, all the data and the software generated for running the competition have been made publicly available allowing researchers to reproduce the competition and to carry out different analysis of the results.</a>
<a href="#2" id="2">Furthermore, the evaluation has to measure the domain independence of planners, quantifying their versatility for different classes of problems across different domains.</a>
<a href="#3" id="3">Therefore, planner performance can be analyzed from different perspectives, such as the number of problems solved, CPU time, plan length, total cost, makespan, or other features such as the diversity or flexibility of the solutions.</a>
<a href="#4" id="4">For this reason, selection of the best planner necessarily depends on the context in which it will be used.</a>
<a href="#5" id="5">For a particular planner with regard to a given planning task, quality is defined as the ratio of the lowest total cost (computed as the sum of the costs of all individual actions included in a solution plan) to the total cost of the best solution found.</a>
<a href="#6" id="6">Thus, each planner p gets a score per planning task i, {a mathematical formula}Sip, expressed as{a mathematical formula} where {a mathematical formula}Cip is the total cost of the best solution found by planner p for instance i, and {a mathematical formula}Ci ⁎ is the lowest total cost found so far by any planner for the same problem, that is, {a mathematical formula}Ci ⁎ =minp ⁡ {Cip}.</a>
<a href="#7" id="7">For example, although there is no difference in theoretical complexity in the general case, optimal planning is harder than satisficing sequential planning in practice [22], [28].</a>
<a href="#8" id="8">Therefore, planners in both tracks were evaluated over the same collection of domains, but problems for the optimal track were carefully selected to be easier to solve.</a>
<a href="#9" id="9">Although the number of objects in a planning problem has been widely used as a weak measure of difficulty, there is currently no effective domain-independent procedure for characterizing the difficulty of problems for a given set of planners, other than attempting to solve them and to look at the results, as in the CSP competition.</a>
<a href="#10" id="10">This approach would require a preliminary IPC to select problems for the official IPC, which, given the large number of participants, was intractable.</a>
<a href="#11" id="11">Therefore, two different methods were adopted for selection of problems, depending on whether data on their difficulty were available (e.g., when using problems from previous IPCs).</a>
<a href="#12" id="12">Problems with an increasing number of objects and goals were generated for solving within a 300-s time limit.</a>
<a href="#13" id="13">As a general rule, the easiest problems of IPC-2011 had similar characteristics to those that were solved in tens of seconds in these limited runs.</a>
<a href="#14" id="14">In our experience, selecting problems for this track seems to be very challenging, mainly because planners scale worse in this track, which reduces the maximum level of problem complexity they can handle.</a>
<a href="#15" id="15">Thus, selection of structurally different problems becomes harder and problems tend in general to be more similar in terms of difficulty than in the other tracks: either they are not solved at all or a large number of planners solve them, as evidenced, for example, by the results for the barman domain.</a>
<a href="#16" id="16">Apart from the quality of the solutions, typical parameters used to compare planner performance in previous IPCs include the number of instances solved (or, alternatively, the overall problem solving success rate, defined as the percentage of problems solved{sup:7}), and the raw speed at which solutions were generated.</a>
<a href="#17" id="17">Although this is not the case when comparing the number of instances solved (since each planning instance is assigned a value per planner: it is either solved or not) or memory usage (even if a planner does not succeed in solving a problem, memory consumption can still be monitored), this situation arises when observing quality and raw speed, since one planner might not solve a particular planning instance whereas the other does.</a>
<a href="#18" id="18">In the third IPC, double hits were used in addition to ordinary analysis when studying plan quality; when observing runtime, the organizers assigned an infinitely bad speed to the planner that did not solve a particular case [37].</a>
<a href="#19" id="19">The organizers of the fifth IPC adopted a different, but similar, methodology: when comparing runtime they assigned twice the time limit to cases for which no solution was generated, observing that this was the minimum value for which the performance gap for a problem solved by one planner and not solved by the other is greater than the performance gap for any problem solved by both.</a>
<a href="#20" id="20">However, when examining plan quality we considered only double hits.</a>
<a href="#21" id="21">The reason is that in preliminary analysis conducted over all pairs of problems (and assigning infinitely bad quality to cases that were unsolved), we observed a significant influence of coverage, which dramatically favored participants that solved more problems.</a>
<a href="#22" id="22">Table 4 lists the number of problems solved and the success rate for each planner in descending order.</a>
<a href="#23" id="23">Since we checked that all correct solution plans had the same quality, the score assigned to each planner and planning task can only take a value of 0 (unsolved or invalid) or 1 (optimally solved).</a>
<a href="#24" id="24">Hence, the final score for each planner equals the number of problems it solved.</a>
<a href="#25" id="25">Therefore, Table 4 also shows the final score for every entry.</a>
<a href="#26" id="26">In this section, we first show how the number of problems solved evolved over time in the range {a mathematical formula}(0,1800]s. Owing to the large number of participants in this track, Fig.</a>
<a href="#27" id="27">The raw speed performance of the planners falls into four different categories: (i) yahsp2-mt dominates all the other entries; (ii) yahsp2 dominates dae_yahsp and popf2 most of the time, except at the end, where dae_yahsp solves one fewer problem than yahsp2 does; (iii) cpt4 and lmtd perform similarly and are dominated throughout the interval by the previously mentioned planners; and (iv) sharaabi and tlp-gp are included only for the sake of completeness, but their curve is hidden because they did not solve any problem.</a>
<a href="#28" id="28">Unfortunately, it is not as straightforward to compare the plan quality for these two planners because the scoring schema used in IPC-2011 may assign a planner a better score simply because the other planners in the same track were worse (Sections 2.3 and 6).</a>
<a href="#29" id="29">Thus, to provide a better assessment of differences in plan quality between lama-2011 and arvandherd, an alternative competition was simulated with all entrants in both the sequential multi-core and satisficing tracks, yielding a total number of 35 entrants.</a>
<a href="#30" id="30">To prove this, observe that the score for planner p when using the best solution found as a reference for a specific planning task i is{a mathematical formula} where {a mathematical formula} Δ ip is the excess committed by planner p over the best solution found (and necessarily equal to zero for at least one planner) when solving instance i.</a>
<a href="#31" id="31">That is, {a mathematical formula}(Cbest+ Δ ip) denotes the cost of the plan generated by planner p for planning instance i.</a>
<a href="#32" id="32">An unwanted effect of not using optimal solutions as a reference baseline is that the final score is not properly scaled over the optimal performance, but instead over the best performance observed.In the preceding example, the scores {a mathematical formula}sa=1.909 and {a mathematical formula}sb=1.8333 are too close to the maximum achievable score of 2, which suggests that both planners perform very well but we can only say that they perform similarly.</a>
</body>
</html>