<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:460">
</head>
<body bgcolor="white">
<a href="#0" id="0">If not differently specified, every action is available in every state.</a>
<a href="#1" id="1">When that is not the case, we denote with {a mathematical formula}A(s) the set of actions available in state {a mathematical formula}s ∈ S.</a>
<a href="#2" id="2">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#3" id="3">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#4" id="4">The function {a mathematical formula}o:S → Sm is the mapping from the states of the MDP to the states of the model introduced at the beginning of this section.</a>
<a href="#5" id="5">This partial policy is used in the reinforcement learning phase to define an MDP on which the agent learns the optimal policy.</a>
<a href="#6" id="6">The partial policy is defined over the state space of the original MDP, to allow the learning algorithm to adapt to the real environment, and not to the model.</a>
<a href="#7" id="7">The agent can learn a policy for the reduced MDP {a mathematical formula}Dr, expanding it when necessary if the system transitions into a state where no action is available.</a>
<a href="#8" id="8">The learning layer is completely independent of the generation of the partial policy, and works on the MDP it induces.</a>
<a href="#9" id="9">Note that the learning algorithm does not learn a policy for {a mathematical formula}Dm, our model.</a>
<a href="#10" id="10">The transitions and the state space are provided by D, the real environment.</a>
<a href="#11" id="11">In this section, we analyze how the model {a mathematical formula}Dm affects the ability of the agent to learn an optimal policy.</a>
<a href="#12" id="12">We carried out three experiments, in order to demonstrate different characteristics of DARLING.</a>
<a href="#13" id="13">We compare three agents across all experiments, for each domain D and model {a mathematical formula}Dm: an agent that makes decisions by planning on {a mathematical formula}Dm only, an agent that makes decisions by reinforcement learning on D, and DARLING that uses {a mathematical formula}Dm to compute a partial policy, and limits its exploration to {a mathematical formula}Dr. We refer to the first agent as the P agent, to the second as the RL agent, and to the third as the PRL agent.</a>
<a href="#14" id="14">We always used {a mathematical formula} Λ =0.9 and {a mathematical formula} Γ =1, since all tasks are episodic.</a>
<a href="#15" id="15">The P and PRL agents do planning with answer set programming over the same model, and for the PRL agent we always set {a mathematical formula} Μ =1.5 as the parameter for the threshold on plan length.</a>
</body>
</html>