<html>
<head>
<meta name="TextLength" content="SENT_NUM:31, WORD_NUM:768">
</head>
<body bgcolor="white">
<a href="#0" id="0">Standard random backpropagation operates exactly like backpropagation except that the weights used in the backward pass are completely random and fixed.</a>
<a href="#1" id="1">Thus the learning rule becomes:{a mathematical formula} where the randomly backpropagated error satisfies the recurrence relation:{a mathematical formula} and the weights {a mathematical formula}ckih+1 are random and fixed.</a>
<a href="#2" id="2">The boundary condition at the top remains the same:{a mathematical formula} Thus in RBP the weights in the top layer of the architecture are updated by gradient descent, identically to the BP case.</a>
<a href="#3" id="3">Derivation of the system</a>
<a href="#4" id="4">The simplest case correspond to a linear {a mathematical formula}A[1,1,1] architecture (Fig.</a>
<a href="#5" id="5">10).</a>
<a href="#6" id="6">Let us denote by {a mathematical formula}a1 and {a mathematical formula}a2 the weights in the first and second layer, and by {a mathematical formula}c1 the random weight of the learning channel.</a>
<a href="#7" id="7">In this case, we have {a mathematical formula}O(t)=a1a2I(t) and the learning equations are given by:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula} Α =E(IT) and {a mathematical formula} Β =E(I2).</a>
<a href="#8" id="8">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1 and {a mathematical formula}a2 during learning:{a mathematical formula} Note that the dynamic of {a mathematical formula}P=a1a2 is given by:{a mathematical formula} The error is given by:{a mathematical formula} and:{a mathematical formula} the last equality requires {a mathematical formula}ai ≠ 0.</a>
<a href="#9" id="9">Derivation of the system</a>
<a href="#10" id="10">In the case of a linear {a mathematical formula}A[1,1,1,1] architecture, for notational simplicity, let us denote by {a mathematical formula}a1,a2 and {a mathematical formula}a3 the forward weights, and by {a mathematical formula}c1 and {a mathematical formula}c2 the random weights of the learning channel (note the index is equal to the target layer).</a>
<a href="#11" id="11">In this case, we have {a mathematical formula}O(t)=a1a2a3I(t)=PI(t).</a>
<a href="#12" id="12">The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}P=a1a2a3.</a>
<a href="#13" id="13">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1,a2 and {a mathematical formula}a3 during learning:{a mathematical formula} The dynamic of {a mathematical formula}P=a1a2a3 is given by:{a mathematical formula}</a>
<a href="#14" id="14">Proof</a>
<a href="#15" id="15">Again, when all the weights in the learning channel are non-zero, the critical points correspond to the curve {a mathematical formula} Α − Β P=0.</a>
<a href="#16" id="16">These critical points are independent of the weights in the learning channel and correspond to global minima of the error function.</a>
<a href="#17" id="17">Additional critical points for the product {a mathematical formula}P=a1 … aL are given by the surface {a mathematical formula} ∑ i=1LPciai ∏ k=1i − 1ak=0.</a>
<a href="#18" id="18">These critical points are dependent on the weights in the learning channel.</a>
<a href="#19" id="19">If the {a mathematical formula}ci are small or congruent with the respective feedforward weights, then {a mathematical formula} ∑ k=1L[ ∏ i ≠ kai][cL − k ∏ j=1j=k − 1aj]>0 and {a mathematical formula}dP/dt has the same sign as {a mathematical formula} Α − Β P.</a>
<a href="#20" id="20">Thus small or congruent weights can help the learning but they are not necessary.To see the convergence, from Equation (45), we have:{a mathematical formula} Note that if one the derivatives {a mathematical formula}dai/dt is zero, then they are all zero and thus there cannot be any limit cycles.</a>
<a href="#21" id="21">Derivation of the system</a>
<a href="#22" id="22">Consider a linear {a mathematical formula}A[1,N,1] architecture (Fig.</a>
<a href="#23" id="23">13).</a>
<a href="#24" id="24">For notational simplicity, we let {a mathematical formula}a1, … ,aN be the weights in the lower layer, {a mathematical formula}b1, … ,bN be the weights in the upper layer, and {a mathematical formula}c1, … ,cN the random weights of the learning channel.</a>
<a href="#25" id="25">In this case, we have {a mathematical formula}O(t)= ∑ iaibiI(t).</a>
<a href="#26" id="26">We let {a mathematical formula}P= ∑ iaibi.</a>
<a href="#27" id="27">The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula} Α =E(IT) and {a mathematical formula} Β =E(I2).</a>
<a href="#28" id="28">With the proper scaling of the learning rate ({a mathematical formula} Η = Δ t) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}ai and {a mathematical formula}bi during learning:{a mathematical formula} The dynamic of {a mathematical formula}P= ∑ iaibi is given by:{a mathematical formula}</a>
<a href="#29" id="29">RBP equations</a>
<a href="#30" id="30">Note that in the case of RBP with backward matrices {a mathematical formula}C1, … ,CL − 1, as opposed to SRBP, one has the system of differential equations:{a mathematical formula} By letting {a mathematical formula}Bi=Ci … CL − 1 one obtains the SRBP equations however the size of the layers may impose constraints on the rank of the matrices {a mathematical formula}Bi.</a>
</body>
</html>