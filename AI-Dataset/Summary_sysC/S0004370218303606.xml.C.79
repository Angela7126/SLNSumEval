<html>
<head>
<meta name="TextLength" content="SENT_NUM:13, WORD_NUM:686">
</head>
<body bgcolor="white">
<a href="#0" id="0">A related method [8] formulates the problem as one of finding a distribution over deterministic policies, {a mathematical formula}Pr( Π I), which has the maximum entropy while matching the convex combination of feature expectations from all policies with those from the observed expert's trajectory:{a mathematical formula} Here, Δ is the space of all distributions, {a mathematical formula}Pr( Π I); {a mathematical formula} Φ ˆ k is the expectation over the {a mathematical formula}kth feature from observations of the expert as defined previously; and the visitation frequency {a mathematical formula} Μ Π I is computed as in Eq.</a>
<a href="#1" id="1">In the presence of occlusion, this equation may not be computable as the proportion of interacting states may be unknown to L. Instead, we empirically compute the state-visitation frequencies by projecting in the full environment the policy under consideration for each robot for a large number of time steps while utilizing the equilibrium behavior {a mathematical formula} Σ e when the robots interact.</a>
<a href="#2" id="2">Toward this objective, we tread a middle path and limit our method to those settings where a mobile robot's stochastic transition function may be viewed as composed of a deterministic core perturbed by transition error probabilities that make it stochastic.</a>
<a href="#3" id="3">Despite this, not knowing the probability of the expert reaching its intended state is a significant relaxation of the requirement by existing IRL methods that the full stochastic transition function inclusive of all probabilities be known.</a>
<a href="#4" id="4">From this, we obtain the probabilities of transitioning to the intended state given the previous state and action, denoted by {a mathematical formula}q Ψ (s,a), as simply the proportion of times the intended state is observed as the next state in the trajectory:{a mathematical formula} where {a mathematical formula} Δ ( ⋅ , ⋅ ) is the indicator function that is equal to 1 when its two arguments are equal, otherwise 0.</a>
<a href="#5" id="5">The mass {a mathematical formula}1 − TI(s,a, Ψ (s,a)) could be distributed according to many possible models: uniformly across all next states excepting the intended state; to a dedicated error state; or among the intended states that would result due to performing actions other than a from s. While one could be chosen based on knowledge of the agent or robot being modeled, a general way is to choose the most likely model given the data on observed unintended transitions.</a>
<a href="#6" id="6">Subsequent to choosing an error model, the full transition function of the other robot, I, is obtained as:{a mathematical formula} where {a mathematical formula} Δ ( ⋅ , ⋅ ) is an indicator function as stated previously.</a>
<a href="#7" id="7">Intuitively, this inverse learning error (ILE) gives the loss of value if L uses the learned policy on the task instead of the expert's: {a mathematical formula}ILE= ‖ V Π I ⁎ − V Π IL ‖ 1.</a>
<a href="#8" id="8">Here, {a mathematical formula}V Π I ⁎ is the optimal value function of I's MDP and {a mathematical formula}V Π IL is the value function due to utilizing the learned policy {a mathematical formula} Π IL in I's MDP.</a>
<a href="#9" id="9">Notice that when the learned reward function results in an identical optimal policy to I's optimal policy, {a mathematical formula} Π I ⁎ = Π IL, ILE will be zero; it increases monotonically as the two policies increasingly diverge in value.</a>
<a href="#10" id="10">High occlusion presents a difficult challenge for this method: instead of randomly missing some data, much of the trajectory is missing, and furthermore the patrollers do not explore all states and actions resulting in the uniform probability distribution being assigned for most transitions.</a>
<a href="#11" id="11">Significant milestones in this rapidly emerging research area include modeling the reward function as a linear combination of features [1], utilizing the principle of maximum entropy to eliminate bias from the learned reward function [37], [8], using a Bayesian framework [32], and employing the structure of the MDP to help learn under noisy feature functions [8].</a>
<a href="#12" id="12">Our technique, mIRL{a mathematical formula}/T ⁎ +Int, is similar to Bard's approach but has several key differences: rather than discrete states we use random variables whose values are unknown during each transition, the presence of occlusion, and our novel extension to learning transition probabilities.</a>
</body>
</html>