<html>
<head>
<meta name="TextLength" content="SENT_NUM:37, WORD_NUM:993">
</head>
<body bgcolor="white">
<a href="#0" id="0">We identify first the needed individual and collaborative cognitive skills: geometric reasoning and situation assessment based on perspective-taking and affordance analysis; acquisition and representation of knowledge models for multiple agents (humans and robots, with their specificities); situated, natural and multi-modal dialogue; human-aware task planning; human – robot joint task achievement.</a>
<a href="#1" id="1">Supported by experimental results, we eventually show how explicit knowledge management, both symbolic and geometric, proves to be instrumental to richer and more natural human – robot interactions by pushing for pervasive, human-level semantics within the robot's deliberative system.</a>
<a href="#2" id="2">Human – Robot Interaction (HRI) represents a challenge for Artificial Intelligence (AI).</a>
<a href="#3" id="3">a belief state that includes a priori common-sense knowledge and mental models of each of the agents involved (the robot and its human partners).</a>
<a href="#4" id="4">provides generic mechanisms for the robot to reason about the mental state of its human partners;</a>
<a href="#5" id="5">reuses the same set of affordances and inferences, together with explicit contextual reasoning on humans and robot abilities, to generate human – robot shared plans.</a>
<a href="#6" id="6">These symbolic statements are stored in the knowledge base Oro and made available to the other cognitive modules.</a>
<a href="#7" id="7">Later, the robot might process a sentence like “ give me another book ” .</a>
<a href="#8" id="8">At run-time, the knowledge available to the robot comes from three sources.</a>
<a href="#9" id="9">As the robot interacts with humans, we must however address the discrepancy between OpenCyc concepts and human terminology.</a>
<a href="#10" id="10">If a human informs the robot that a given object is indeed a bottle, the robot can consequently derive more information on the object.</a>
<a href="#11" id="11">Our knowledge base implements such a mechanism [28]: when the robot detects that a new human has appeared, it initialises a new independent knowledge model (an ontology) for this human agent.</a>
<a href="#12" id="12">The robot updates accordingly the two knowledge models it maintains: the robot model is updated with the fact 〈 BOOK isVisible true 〉 , while the human model is updated with 〈 BOOK isVisible false 〉 .</a>
<a href="#13" id="13">As described in the next section, the environment model of the robot is continuously updated and the derived symbolic knowledge is therefore transient: it lasts only as long as the environment remains in the same state.</a>
<a href="#14" id="14">Building a grounded symbolic model of the physical environment does not suffice in general to fully ground the human – robot interaction, and a model of the current capabilities of the agents interacting with the robot is also required.</a>
<a href="#15" id="15">For example, if the human asks the robot to give her an object, the robot must compute a transfer point where she is able to get the object afterwards.</a>
<a href="#16" id="16">The robot relies on these to monitor the engagement of the human and the overall progress of the human – robot shared plan.</a>
<a href="#17" id="17">For instance, when several instances match a category (the human says “ give me the bottle ” and the robot knows about three bottles), the module may decide to discard some of the candidates based on their visibility for the speaker (implicit communication context taking into account the human position).</a>
<a href="#18" id="18">As another example, when the human says “ this ” , the robot checks if the human is currently pointing at an object.</a>
<a href="#19" id="19">For example, if the human says “ this ” without the robot tracking what the human points at, no 〈 HUMAN1 pointsAt ... 〉 fact is possibly available in the knowledge base, and the robot falls back on the anaphora resolution step alone.</a>
<a href="#20" id="20">This represents the procedural knowledge of the robot as well as its knowledge about the actions that the human partner is able to achieve.</a>
<a href="#21" id="21">This effectively enriches the interaction capabilities of the robot by providing the system with what is in essence a prediction of the human behaviour.</a>
<a href="#22" id="22">This prediction is used by the robot execution controller to monitor the engagement of the human partner during plan achievement.</a>
<a href="#23" id="23">This, in turn, allows the robot to monitor the human engagement and the progress in the shared plan execution.</a>
<a href="#24" id="24">Upon request, it allows the robot to explain to its human partner how a task can be shared [12], [14].</a>
<a href="#25" id="25">HATP includes mechanisms called social rules to promote plans that are considered as suitable for human – robot interaction.</a>
<a href="#26" id="26">As HATP is a generic symbolic task planner and does not enforce any abstraction level for the planning domain, we have designed a planning domain made of top-level tasks whose semantics are close to the one used in the human – robot dialogue: the planner domain effectively contains concepts like Give, table, isOn.</a>
<a href="#27" id="27">It enables the robot as well as the human to communicate their beliefs about the task to be achieved, in order to share mutual knowledge and to synchronise their activities.</a>
<a href="#28" id="28">The first one is focused on knowledge representation and verbal interaction: the human asks for help to find and pack objects (Interactive Grounding I in Table 3).</a>
<a href="#29" id="29">In this scenario, the human and the robot need to cooperatively remove objects from a table.</a>
<a href="#30" id="30">This study focuses on multi-modal, interactive grounding only: the robot observes, builds and maintains knowledge about its human partners perspectives and affordances but does not actually perform any physical action besides verbal interaction and simple head movements.</a>
<a href="#31" id="31">The robot knowledge base is initialised with the Oro commonsense ontology.</a>
<a href="#32" id="32">The robot first needs to ground the word “ box ” .</a>
<a href="#33" id="33">This section synthesises what we see as the main challenges that human – robot interaction brings to Artificial Intelligence.</a>
<a href="#34" id="34">The Dialogs module grounds new concepts introduced by the speaker by asking questions until it can attach the new concept to concepts already present in the knowledge pool (if one asks the robot “ bring me a margarita ” , the robot may initially asks “ what is a margarita? ” .</a>
<a href="#35" id="35">They would need to be accounted for when grounding human – robot interaction.</a>
<a href="#36" id="36">Motion and action execution also requires human models, and the one we use embeds human preferences and physical constraints that need to be accounted for when synthesising robot motion or producing robot plans.</a>
</body>
</html>