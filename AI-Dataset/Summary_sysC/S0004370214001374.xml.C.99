<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:212">
</head>
<body bgcolor="white">
<a href="#0" id="0">A smaller value of Ε results in more conservative policy updates, while a higher Ε leads to faster converging policies.</a>
<a href="#1" id="1">As the context distribution {a mathematical formula} Μ (s) is defined by the learning problem and cannot be chosen by the learning algorithm, the constraints {a mathematical formula} ∫ Ω p(s, Ω )= Μ (s), ∀ s must also be satisfied.</a>
<a href="#2" id="2">The expected reward {a mathematical formula}Rs Ω =E Τ [R( Τ ,s)|s, Ω ] can be estimated by multiple samples from the trajectory distribution {a mathematical formula}p( Τ |s, Ω ), that is,{a mathematical formula} where the trajectories are now generated using the learned forward models in computer simulation and we will assume that the trajectory-dependent reward function {a mathematical formula}R( Τ ,s) is known.</a>
<a href="#3" id="3">Due to the recent success of using Gaussian Process models to reduce the model bias when learning complex system dynamics [9], we use GP models to learn the forward models of the robot and its environment.</a>
<a href="#4" id="4">Therefore, our method is called Gaussian Process Relative Entropy Policy Search (GPREPS).</a>
<a href="#5" id="5">For learning problems where the dynamics of the robot and its environment are stochastic, the variance of the resulting trajectory Τ , and thus, the variance of the reward {a mathematical formula}R( Τ ,s) might be considerably large.</a>
</body>
</html>