<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:401">
</head>
<body bgcolor="white">
<a href="#0" id="0">Next, we present a semi-supervised version of our feature labeling algorithm which assumes that an unlabeled set of instances is present during training.</a>
<a href="#1" id="1">The semi-supervised setting for feature labeling incorporates knowledge from three sources: a small labeled training set, the feature labels provided by the end user and information from the implicit structure of the unlabeled data.</a>
<a href="#2" id="2">We evaluate our semi-supervised algorithm using both oracle feature labels and end-user feature labels from the user study mentioned above.</a>
<a href="#3" id="3">Our feature labeling algorithm is one of the best performing algorithms with oracle feature labels and the best performer with lower quality feature labels from end users.</a>
<a href="#4" id="4">With our results, we can compare the relative gains using the different sources of knowledge available in the training set, the feature labels, and the unlabeled data.</a>
<a href="#5" id="5">Our analysis shows that incorporating unlabeled data during learning sometimes produces worse performance than just using a purely supervised learning approach, both with and without feature labeling.</a>
<a href="#6" id="6">However, adding the information from feature labels consistently improves performance over not including this information, both in the supervised and semi-supervised settings.</a>
<a href="#7" id="7">Fig.</a>
<a href="#8" id="8">8 and Table 4 depict the results of adding 10 oracle feature labels per class to the semi-supervised feature labeling algorithms.</a>
<a href="#9" id="9">LWLR-SS-FL outperformed the other algorithms on the 20 Newsgroups, ModApte, and Industry sectors datasets, with statistically significant improvements over GE and SVM-M3 (Wilcoxon signed-rank test, {a mathematical formula}p<0.05).</a>
<a href="#10" id="10">GE, however, had the best macro-F1 on the WebKb and Movie Review datasets with statistically significant improvements over the second best algorithm (Wilcoxon signed-rank test, {a mathematical formula}p<0.05).</a>
<a href="#11" id="11">For the RCV1 dataset, SVM-M3 had the best macro-F1.</a>
<a href="#12" id="12">For SVM-M3, end-user feature labels degraded the performance of the algorithm below the SVM baseline in this semi-supervised setting, while LWLR was more robust.</a>
<a href="#13" id="13">GE performed the worst out of all the other algorithms with end-user feature labels, indicating that it was very sensitive to the quality of feature labels.</a>
<a href="#14" id="14">The poor performance of GE was due to the lower quality end-user feature labels being the only source of supervision for GE Ê¼ s learning process.</a>
<a href="#15" id="15">Furthermore, in past work, GE performed very well when users were guided to provide feedback on features selected by topic models or by active-learning [10], unlike in our setup where the users had no guidance at all as to which features to label.</a>
<a href="#16" id="16">Both LWLR-SS-FL and MNB/Priors+EM improved upon their respective baseline algorithms with end user feature labels.</a>
</body>
</html>