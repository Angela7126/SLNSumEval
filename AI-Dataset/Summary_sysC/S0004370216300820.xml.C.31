<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:463">
</head>
<body bgcolor="white">
<a href="#0" id="0">4.</a>
<a href="#1" id="1">Word Sense Disambiguation.</a>
<a href="#2" id="2">We proposed a simple framework for a knowledge-rich unsupervised disambiguation system.</a>
<a href="#3" id="3">Our system obtained state-of-the-art results on multilingual All-Words Word Sense Disambiguation using Wikipedia as sense inventory, evaluated on the SemEval-2013 dataset [95], and on English All-Words Word Sense Disambiguation using WordNet as sense inventory, evaluated on the SemEval-2007 [111] and SemEval-2013 [95] datasets.</a>
<a href="#4" id="4">We benchmark our semantic similarity procedure on the word similarity task.</a>
<a href="#5" id="5">Word similarity is a specific task from semantic similarity in which we measure how semantically close two words are.</a>
<a href="#6" id="6">In order to be able to compute the similarity between words we first need to map the two words to their corresponding synsets.</a>
<a href="#7" id="7">However, this mapping is a straightforward process, thanks to the multilingual sense inventory of BabelNet.</a>
<a href="#8" id="8">As frequently done in this task, we measure the similarity between two words w and {a mathematical formula}w ′ as the similarity between their closest senses [115], [13], [106], [17]:{a mathematical formula} where {a mathematical formula}Lw represents the set of synsets which contain w as one of its lexicalizations.</a>
<a href="#9" id="9">As vector comparison VC we use WO (see Section 3.5) to compare lexical and unified representations, and cosine for the embedded representations.</a>
<a href="#10" id="10">Note that, thanks to our unified representation, w and {a mathematical formula}w ′ may belong to different languages.</a>
<a href="#11" id="11">Throughout this section on the tasks based on semantic similarity, Nasarilexical and Nasariunified represent the systems based on the lexical and unified vectors, respectively.</a>
<a href="#12" id="12">We refer to the combination of both lexical and unified vectors as Nasari.</a>
<a href="#13" id="13">This combination is based on the average similarity scores given by lexical and unified vectors for each sense pair.</a>
<a href="#14" id="14">We also report results of our Nasariembed vector representations which use the pre-trained Word2Vec vectors as input.</a>
<a href="#15" id="15">We performed experiments on monolingual word similarity for English and other languages (presented in Sections 6.1.1 and 6.1.2, respectively) and cross-lingual similarity (presented in Section 6.1.3).</a>
<a href="#16" id="16">Additionally, we evaluate our embedded representations in a cross-level semantic similarity task in Section 6.1.4.</a>
<a href="#17" id="17">Given a set of target words in a text {a mathematical formula}T, we build a lexical vector for the context, as explained in Section 3.2.</a>
<a href="#18" id="18">Then, for each target word w in the text {a mathematical formula}T, we retrieve the set of all the possible BabelNet synsets which have this target word as one of its lexicalizations, a set we refer to as {a mathematical formula}Lw.</a>
<a href="#19" id="19">Finally, we simply compute Weighted Overlap (see Section 3.5) between {a mathematical formula}v → lex(T) (the lexical vector of the text {a mathematical formula}T) and the Nasari vectors corresponding to the BabelNet synsets that contain senses of w. In our setting, the top BabelNet synset in terms of WO score ({a mathematical formula}s ˆ ) is selected as the best sense of the given target word:{a mathematical formula}</a>
</body>
</html>