<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:454">
</head>
<body bgcolor="white">
<a href="#0" id="0">We present Domain Approximation for Reinforcement LearnING (DARLING), a method that takes advantage of planning to constrain the behavior of the agent to reasonable choices, and of reinforcement learning to adapt to the environment, and increase the reliability of the decision making process.</a>
<a href="#1" id="1">The behavior of the agent is represented as a function {a mathematical formula} Π :S×A → [0,1] called a (stationary) policy, where {a mathematical formula} Π (s,a)=Pr(At=a|St=s) is the probability of selecting action a in state s at time t. If {a mathematical formula} Π (s,a)={0,1}{a mathematical formula} ∀ s ∈ S and {a mathematical formula}a ∈ A the policy is deterministic, in which case we will denote the action chosen by the policy as {a mathematical formula}a= Π (s).</a>
<a href="#2" id="2">For instance:{a mathematical formula} means that it is not possible to execute action openDoor(D) on any door D at time step I and facing(D) cannot be derived at time step I.</a>
<a href="#3" id="3">This means that facing(D) is a pre-condition for the action openDoor(D).</a>
<a href="#4" id="4">The effect of actions is represented with rules such as:{a mathematical formula} which means that executing the action openDoor(D) at time step I causes the door to be open at time step I+1.</a>
<a href="#5" id="5">[1] proposed a method close in motivation to ours, in which the agent uses a crude model of a deterministic dynamical system to identify a direction for policy improvement, and then executes trials in the real world along that direction only, updating the model with the data from the environment.</a>
<a href="#6" id="6">Differently from the aforementioned work in planning, we use a representation simple enough to compute a partial policy from a single planning instance, and perform reinforcement learning on the execution of that task in the world.</a>
<a href="#7" id="7">Let aaabdca be the plan the algorithm is testing, and let the insertion point of this plan in the ordered list of minimal plans be as follows:{a mathematical formula} The lower bound of aaabdca in this set is aabaca.</a>
<a href="#8" id="8">A partial policy, in this context, is a function {a mathematical formula} Π :S → 2A that maps a state into a set of possible actions.</a>
<a href="#9" id="9">We define this function formally as follows: let {a mathematical formula} Π (L) be the set of minimal plans of cost (in our case length) up to L for a given planning problem, then{a mathematical formula} is the partial policy that returns, for each state in the MDP, the set of actions that belong to at least one minimal plan in the corresponding state of the model.</a>
<a href="#10" id="10">While the partial policy (and therefore the model {a mathematical formula}Dm) determines the possibility to choose a given action in a particular state, the probability distribution over the next state is unmodified for the actions that are available.</a>
</body>
</html>