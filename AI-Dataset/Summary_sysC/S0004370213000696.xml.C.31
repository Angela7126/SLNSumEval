<html>
<head>
<meta name="TextLength" content="SENT_NUM:62, WORD_NUM:983">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}</a>
<a href="#1" id="1">We begin by presenting an efficient algorithm for finding all of the {a mathematical formula}S ⁎ ʼ s for a matrix M when interacting with a deterministic teammate ({a mathematical formula} Ε =0) that always selects the best response to our most recent action ({a mathematical formula}mem=1).</a>
<a href="#2" id="2">Detailed in pseudocode as Algorithm 1, it uses dynamic programming, using the {a mathematical formula}Sn − 1 ⁎ ʼ s to compute the {a mathematical formula}Sn ⁎ ʼ s.</a>
<a href="#3" id="3">{a mathematical formula}</a>
<a href="#4" id="4">Note that in our analysis of this case in which Agent B has {a mathematical formula}mem=1 and {a mathematical formula} Ε =0, all of the arguments hold even if there are multiple cells in the payoff matrix M with value {a mathematical formula}m ⁎ .</a>
<a href="#5" id="5">Furthermore, Algorithm 1 computes the optimal sequence of joint actions from all starting points, not just a particular starting point, all in polynomial time in the dimensionality of the matrix.</a>
<a href="#6" id="6">The second necessary alteration to Algorithm 1 in this case is that it is no longer sufficient to simply arrive at a joint action {a mathematical formula}(ai,bj) such that {a mathematical formula}mi, {a mathematical formula}j=m ⁎ .</a>
<a href="#7" id="7">Rather, the agents must arrive at such an action with a history of Agent A ʼ s actions such that if it keeps playing {a mathematical formula}ai, Agent B will keep selecting {a mathematical formula}bj.</a>
<a href="#8" id="8">We define such a joint action-history to be stable.</a>
<a href="#9" id="9">{a mathematical formula}</a>
<a href="#10" id="10">{a mathematical formula}</a>
<a href="#11" id="11">{a mathematical formula}</a>
<a href="#12" id="12">Can find optimal action sequence efficiently: {a mathematical formula}O(d3)</a>
<a href="#13" id="13">Maximum length of optimal sequence: {a mathematical formula}min(x,y)</a>
<a href="#14" id="14">{a mathematical formula}</a>
<a href="#15" id="15">We use the following notation for the three arms.</a>
<a href="#16" id="16">The learner selects between Arm1 and Arm2, while the teacher can additionally choose {a mathematical formula}Arm ⁎ .</a>
<a href="#17" id="17">While we consider two different forms of distributions for the payoffs, throughout the section we use the following notation:</a>
<a href="#18" id="18">{a mathematical formula} Μ i is the expected payoff of {a mathematical formula}Armi ({a mathematical formula}i ∈ {1,2, ⁎ }).</a>
<a href="#19" id="19">Therefore there are situations in which it is better for the teacher to pull Arm1 than {a mathematical formula}Arm ⁎ .</a>
<a href="#20" id="20">This article is devoted to characterizing exactly what those situations are.</a>
<a href="#21" id="21">Base case</a>
<a href="#22" id="22">{a mathematical formula}r=1.</a>
<a href="#23" id="23">If the teacher starts by pulling Arm2, the best expected value the team can achieve is {a mathematical formula} Μ 2+ Μ 1.</a>
<a href="#24" id="24">Meanwhile, if it starts with {a mathematical formula}Arm ⁎ , the worst the team expects is {a mathematical formula} Μ ⁎ + Μ 2.</a>
<a href="#25" id="25">This expectation is higher since {a mathematical formula} Μ ⁎ > Μ 1.</a>
<a href="#26" id="26">Third, we show that the teacher ʼ s choice is clear whenever {a mathematical formula}x¯1>x¯2.</a>
<a href="#27" id="27">That is, if the current sample average of Arm1 is greater than that of Arm2 such that the learner will choose Arm1 next, then the teacher should always choose {a mathematical formula}Arm ⁎ : it should not teach.</a>
<a href="#28" id="28">2.</a>
<a href="#29" id="29">{a mathematical formula}p ⁎ − p1<p1(p1 − p2)</a>
<a href="#30" id="30">Building on the intuition from Section 3.3.1, this section presents our fully-implemented polynomial memory and time dynamic programming algorithm for determining the teacher ʼ s optimal action with any number of rounds left.</a>
<a href="#31" id="31">It takes as input initial values for {a mathematical formula}m1,n1,m2,n2, and r, which we denote as {a mathematical formula}M1,N1,M2,N2, and R respectively, and it outputs whether the teacher ʼ s expected value is higher if it teaches by pulling Arm1 or if it exploits by pulling {a mathematical formula}Arm ⁎ .</a>
<a href="#32" id="32">Therefore, the teacher should pull Arm1 if and only if{a mathematical formula} (recall that {a mathematical formula}x¯1=m1n1 by definition).</a>
<a href="#33" id="33">Otherwise, the teacher should pull {a mathematical formula}Arm ⁎ .</a>
<a href="#34" id="34">We can then compute the expected value of the optimal action as:</a>
<a href="#35" id="35">If {a mathematical formula}x¯1>x¯2, {a mathematical formula}EVnt= Μ ⁎ + Μ 1</a>
<a href="#36" id="36">Else {a mathematical formula}EVnt= Μ ⁎ + Μ 2.</a>
<a href="#37" id="37">With these values it is barely not worth it for the teacher to teach with {a mathematical formula}r=1.</a>
<a href="#38" id="38">That is, with these values, Inequality (1) is not satisfied, but if {a mathematical formula}x¯1 were 7.01, then it would be satisfied.</a>
<a href="#39" id="39">Thus we know with certainty that the teacher ʼ s optimal action is {a mathematical formula}Arm ⁎ .</a>
<a href="#40" id="40">{a mathematical formula}x¯1>x¯2: 29.5%</a>
<a href="#41" id="41">{a mathematical formula}x¯1>x¯2: 64.0%</a>
<a href="#42" id="42">Carrying through as in Section 3.2.1, it is clear that the teacher pulling {a mathematical formula}Armc yields a higher expected team value than pulling {a mathematical formula}Arm ⁎ or any other arm.</a>
<a href="#43" id="43">Thus the learner needs to consider pulling at least {a mathematical formula}Arm ⁎ and Arm1 – {a mathematical formula}Armz − 1.</a>
<a href="#44" id="44">The proof of Theorem 3.2 from Section 3.2.3 generalizes directly to the following statement.</a>
<a href="#45" id="45">The teacher should never take the action that the learner would take next on its own if the teacher were to pull {a mathematical formula}Arm ⁎ .</a>
<a href="#46" id="46">1.</a>
<a href="#47" id="47">Those are the arms with higher expected value than {a mathematical formula}Arm ⁎ .</a>
<a href="#48" id="48">From among those arms, it should select the {a mathematical formula}Armj with the highest expected value {a mathematical formula}EV=pj+pj2+(1 − pj)pi.</a>
<a href="#49" id="49">{a mathematical formula}x¯1>x¯2: do not teach</a>
<a href="#50" id="50">{a mathematical formula}x¯1<x¯2, {a mathematical formula}r=1: closed form solution for optimal teacher action</a>
<a href="#51" id="51">We let Agent B ʼ s {a mathematical formula}mem=n and we construct Matrix M as follows.</a>
<a href="#52" id="52">{a mathematical formula}</a>
<a href="#53" id="53">(h)</a>
<a href="#54" id="54">The payoff for selecting done only comes at {a mathematical formula}m ⁎ .</a>
<a href="#55" id="55">(n)</a>
<a href="#56" id="56">These payoffs ensure that if Agent A takes the done action before step n, then the cost is already higher than the target of {a mathematical formula}n ⁎ (n4 − 1).</a>
<a href="#57" id="57">Base case</a>
<a href="#58" id="58">{a mathematical formula}r=1.</a>
<a href="#59" id="59">If the teacher starts by pulling Arm2, the best expected value the team can achieve is {a mathematical formula} Μ 2+ Μ 1.</a>
<a href="#60" id="60">Meanwhile, if it starts with {a mathematical formula}Arm ⁎ , the worst the team expects is {a mathematical formula} Μ ⁎ + Μ 2.</a>
<a href="#61" id="61">This expectation is higher since {a mathematical formula} Μ ⁎ > Μ 1.</a>
</body>
</html>