<html>
<head>
<meta name="TextLength" content="SENT_NUM:24, WORD_NUM:461">
</head>
<body bgcolor="white">
<a href="#0" id="0">{a mathematical formula}q=0.2).</a>
<a href="#1" id="1">(9) we have{a mathematical formula} with {a mathematical formula}qi=1 − pi.</a>
<a href="#2" id="2">We now consider a single linear layer with k output units{a mathematical formula} In this case, dropout applied to input units is slightly different from dropout applied to the connections.</a>
<a href="#3" id="3">When dropout is applied to the connections, {a mathematical formula}Si and {a mathematical formula}Sl are entirely independent.</a>
<a href="#4" id="4">When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or the weights, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#5" id="5">When dropout is applied to the connections, under similar independence assumptions, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer.</a>
<a href="#6" id="6">If {a mathematical formula} Μ S=0 we have{a mathematical formula} and the error in the approximation is small and directly proportional to Λ and Σ .</a>
<a href="#7" id="7">As we shall see in Section 11, dropout tends to minimize the variance {a mathematical formula} Σ S and thus the assumption that Σ be small is reasonable.</a>
<a href="#8" id="8">We also let {a mathematical formula}V=Var(O).</a>
<a href="#9" id="9">These approximations become more accurate as {a mathematical formula} Ε i → 0.</a>
<a href="#10" id="10">To further understand the dropout approximation and its behavior in deep networks, we must look at the distribution of the difference {a mathematical formula} Α =E(O) − NWGM(O).</a>
<a href="#11" id="11">with mean {a mathematical formula} Μ O and variance {a mathematical formula} Σ O2.</a>
<a href="#12" id="12">with mean {a mathematical formula} Μ S and variance {a mathematical formula} Σ S2.</a>
<a href="#13" id="13">Very often, {a mathematical formula} Σ ( Μ S) ≈ Μ O.</a>
<a href="#14" id="14">This is particularly true if {a mathematical formula} Μ O=0.5.</a>
<a href="#15" id="15">Furthermore {a mathematical formula} Σ O2=C1 ≈ C2 if {a mathematical formula} Σ O2 is small.</a>
<a href="#16" id="16">We first consider the case where {a mathematical formula}E=NWGM=0.5.</a>
<a href="#17" id="17">This is because it neglects a term in {a mathematical formula} Σ ′ which presumably is close to 0 at the end of learning.</a>
<a href="#18" id="18">When {a mathematical formula}rih<Oih, then the variance is greater in the dropout network.</a>
<a href="#19" id="19">The gradient of the error function in the dropout network is given by{a mathematical formula} using the notation of Section 9.5: {a mathematical formula}SijTl=Sil − wijTl Δ jlOjl.</a>
<a href="#20" id="20">Note that the NWGM approximation requires {a mathematical formula}0 ⩽ Σ ′ (Si) ⩽ 1 for every i, which is always satisfied if {a mathematical formula} Λ ⩽ 4.</a>
<a href="#21" id="21">For instance, if {a mathematical formula} Μ =0 and {a mathematical formula} Λ =c=1= Σ {a mathematical formula}</a>
<a href="#22" id="22">Assume that the weights have mean {a mathematical formula} Μ w and variance {a mathematical formula} Σ w2, the activities have mean {a mathematical formula} Μ O and variance {a mathematical formula} Σ O2, and the weights and the activities are independent of each other.</a>
<a href="#23" id="23">In what follows, we will use {a mathematical formula} Α =0.607.</a>
</body>
</html>