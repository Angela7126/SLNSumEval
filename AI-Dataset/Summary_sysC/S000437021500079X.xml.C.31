<html>
<head>
<meta name="TextLength" content="SENT_NUM:28, WORD_NUM:689">
</head>
<body bgcolor="white">
<a href="#0" id="0">Using these definitions, the automated process of configuring static sequential portfolios can be defined as follows: Given a set of candidate solvers P and a set of training instances I automatically find the portfolio configuration C of solvers {a mathematical formula}p ∈ P such that it maximizes the performance of the resulting portfolio over the given benchmark I.</a>
<a href="#1" id="1">gop consists of three steps.</a>
<a href="#2" id="2">First, every candidate solver is executed with every training problem from the input set to generate the raw data.</a>
<a href="#3" id="3">Second, this raw data is processed to compute the parameters of the MIP model.</a>
<a href="#4" id="4">Third, the MIP task generates the optimal configuration of a static sequential portfolio for the collection of training problems and candidate solvers.</a>
<a href="#5" id="5">This optimal configuration is the best linear combination of the candidate solvers with regard to the objective function (maximize coverage or overall quality or minimize time) of the MIP model.</a>
<a href="#6" id="6">One of the main factors that influences the time required to solve the MIP task defined in the previous section is the size of the training instance set.</a>
<a href="#7" id="7">Thus, in this section we analyze the question of whether there is a subset of the training problems that results in a portfolio with a similar or even equal performance (measured in overall quality or coverage) with regard to the same set of candidate solvers.</a>
<a href="#8" id="8">We refer to this particular problem as the analysis of the utility of the training instances.</a>
<a href="#9" id="9">For example, when optimizing quality, if all training problems are solved by all solvers with the same quality, they do not provide any utility to configure the OSS portfolio since any combination of the candidate solvers will provide the same result.</a>
<a href="#10" id="10">Similarly, the training problems that are not solved by any solver are equally irrelevant since the MIP task will not be able to distinguish between an empty portfolio or a particular combination of the candidate solvers.</a>
<a href="#11" id="11">The first experiment aims to compute the OSS portfolio for the sequential optimization track of the IPC 2011, which allows us to analyze the performance of any solver on that particular benchmark.</a>
<a href="#12" id="12">Since in this experiment we test the performance with the same training instances, the resulting portfolio provides us only with the best achievable performance given the participant planners and instances of the IPC.</a>
<a href="#13" id="13">Therefore, this experiment does not assess the performance of the OSS portfolio on a different set of instances.</a>
<a href="#14" id="14">It analyzes the performance of the awarded planners regarding the upper bound defined by the OSS portfolio.</a>
<a href="#15" id="15">As discussed above, the metric used in the IPC 2011 does not provide any information about the quality of the achieved performance.</a>
<a href="#16" id="16">Hence, we do not know how good the previous results are.</a>
<a href="#17" id="17">We only know the number of solved problems (65) and the size of the test problems set (120).</a>
<a href="#18" id="18">Therefore, we have computed the OSS portfolio (OSS gop-1) for the new domains defined in the IPC 2011 using the same set of candidate planners considered by fdss and gop-1.</a>
<a href="#19" id="19">Also, we have generated the vbs, the sbs and the OSS portfolio (OSS IPC) for these domains using all the participant solvers in the IPC 2011.</a>
<a href="#20" id="20">The performance of the resulting portfolios is shown in Fig.</a>
<a href="#21" id="21">3.</a>
<a href="#22" id="22">As it can be seen, the performance shown by gop-1 and fdss is very high since they are equal to {a mathematical formula}6569 ⋅ 100=94.2% with regard to the performance of the OSS gop-1 portfolio.</a>
<a href="#23" id="23">Table A.7 shows the candidate planners considered to compute the OSS portfolio for the sequential optimization track of the IPC 2011 in Section 6.1.1.</a>
<a href="#24" id="24">The set of candidate planners used in Section 6.1.3 to assess the performance of the portfolios computed with gop in optimal planning is shown in Table A.8.</a>
<a href="#25" id="25">The OSS portfolio for the sequential satisficing track of the IPC 2011 has been configured in Section 6.2.1 with the set of candidate planners shown in Table A.9.</a>
<a href="#26" id="26">The assessment of gop to configure sequential portfolios in satisficing planning has been described in Section 6.2.3.</a>
<a href="#27" id="27">We have compared the performance of the portfolios automatically derived with gop against fdss using the sets of candidate planners shown in Table A.10, Table A.11.</a>
</body>
</html>