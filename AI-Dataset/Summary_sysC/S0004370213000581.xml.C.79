<html>
<head>
<meta name="TextLength" content="SENT_NUM:8, WORD_NUM:442">
</head>
<body bgcolor="white">
<a href="#0" id="0">As far as we know, all the existent non-vectorial techniques work through the definition of a distance function {a mathematical formula}D(X,Y) that provides a way of comparing any two non-vectorial entities X and Y (where these entities are bags in our problem).</a>
<a href="#1" id="1">{sup:2} In contrast, in the ES paradigm, the extraction of information from the whole bag is performed explicitly through the definition of a mapping function that defines how the relevant information is represented into a single vector {a mathematical formula}v → .</a>
<a href="#2" id="2">However, in order to perform this mapping, the bag X is compared with other bags Y from the training set through the definition of a bag-level distance function {a mathematical formula}D(X,Y).</a>
<a href="#3" id="3">For example, if we learn an instance-level model of class 1 in order to obtain {a mathematical formula}f(x → ), then we cannot infer the classification {a mathematical formula}F(X) based only on the individual scores {a mathematical formula}f(x → ), as we find that both positive and negative bags contain instances of class 1.</a>
<a href="#4" id="4">The parameter R is optimized by maximizing the number of positive bags in the training set that contain at least one instance in R and, at the same time, the number of negative bags that do not contain any instance in R. Based on this, the bag-level classifier can be expressed by using the max rule:{a mathematical formula} i.e., X is considered positive if at least one of the instances {a mathematical formula}x → ∈ X is positive.</a>
<a href="#5" id="5">For this purpose, the algorithm estimates the parameters Θ of the SVM function {a mathematical formula}f(x → ; Θ ) by minimizing a standard SVM objective function (see [20] for details) subject to the following constraints:{a mathematical formula}{a mathematical formula} The first set of constraints (*) forces the SVM function to provide a negative value when applied to negative instances (allowing a certain degree of misclassification through the slack variable {a mathematical formula} Ξ − ).</a>
<a href="#6" id="6">This condition depends on the size of the bag X from where {a mathematical formula} Μ (X) is extracted: if X only contains one instance, we have the standard condition {a mathematical formula}f( Μ (X); Θ ) ⩾ 1 − Ξ +, i.e., we require the SVM to provide a positive value (allowing again some misclassification).</a>
<a href="#7" id="7">However, the methods from the last section tend to discard a big part of the information, by either only modelling the characteristics of certain instances (as in MI-SVM [17], where only one instance per positive bag is considered in the learning stage, see also our technical report [19]), or by considering only the average vector of a positive bag (as in SMIL [20]).</a>
</body>
</html>