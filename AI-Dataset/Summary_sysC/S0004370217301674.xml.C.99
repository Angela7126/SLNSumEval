<html>
<head>
<meta name="TextLength" content="SENT_NUM:18, WORD_NUM:600">
</head>
<body bgcolor="white">
<a href="#0" id="0">Since from a computational point of view, all the individual agents in a DRL system can operate in parallel acting upon their individual and reduced action spaces, the learning speed is typically higher compared to a centralized agent which searches an exponentially larger action space {a mathematical formula}N=N1× ⋯ ×NM, as expressed in (2)[39].</a>
<a href="#1" id="1">Also, note that state space could be reduced for practical effects, {a mathematical formula}Agentx could eventually work without observing {a mathematical formula}y ˙ speed, as well as {a mathematical formula}Agenty without observing {a mathematical formula}x ˙ speed.</a>
<a href="#2" id="2">So, a simplified 3DMC could be modeled as a fully decentralized problem with two individual agents with their own independent state vectors, {a mathematical formula}sx=[x,x ˙ ,y],sy=[x,y,y ˙ ].</a>
<a href="#3" id="3">In our DRL-CA approach, we replace such term by a cooperative adaptive factor Σ defined as{a mathematical formula}</a>
<a href="#4" id="4">The main principle of DRL-CA is supported on this cooperative factor that adapts a global learning rate on-line, which is based on a simple estimation of the partial quality of the joint policy performed.</a>
<a href="#5" id="5">So, Σ is computed from the probability of selected action ({a mathematical formula}Pa ⁎ ), according to the “ weakest ” among the M agents.</a>
<a href="#6" id="6">This is an interesting point to analyze and discuss in the following sections, taking the previous results of the 3DMC problem into account, and the fact that this particular problem also uses two agents with full observability of the joint state space.</a>
<a href="#7" id="7">As was mentioned in Section 5.2, the number of discretized actions for the linear velocity was optimized, obtaining {a mathematical formula}Nvl=5 for the CRL scheme, and {a mathematical formula}Nvl=7 for the DRL-Ind.</a>
<a href="#8" id="8">So, total discrete actions are: {a mathematical formula}NT=NvlNaw=15 for the CRL scheme, and {a mathematical formula}NT=7+3 for the DRL-Ind.</a>
<a href="#9" id="9">Stage3.3 — Defining the reward functions: the proposed dribbling modeling has three well-defined individual goals, ball-pushing, alignment, and ball-turning.</a>
<a href="#10" id="10">Thus, individual rewards are proposed for each agent as:{a mathematical formula} where {a mathematical formula}[ Ρ th, Γ th, Φ th] are desired thresholds at which the ball is considered to be controlled, while {a mathematical formula}vx.max ′ reinforces walking forward at maximum speed.</a>
<a href="#11" id="11">Fault-state constraints are set as: {a mathematical formula}[ Ρ th, Γ th, Φ th]=[250mm,15°,15°], and {a mathematical formula}vx.max ′ =0.9vx.max.</a>
<a href="#12" id="12">This is a good example for depicting how and why to define individual rewards; for instance, since only {a mathematical formula}Agentx involves {a mathematical formula}vx for the ball-pushing sub-task, {a mathematical formula}Agenty, and {a mathematical formula}Agent Θ reward functions do not include this variable.</a>
<a href="#13" id="13">Stage3.4 — Determining if the problem is fully decentralizable: since the three state variables, {a mathematical formula}[ Ρ , Γ , Φ ] of the joint vector state are required, this problem is not considered to be fully decentralizable.</a>
<a href="#14" id="14">So, the proposed modeling for learning the 3-Dimensional velocity vector from the joint observed state is detailed in Table 5.</a>
<a href="#15" id="15">Stage3.5 — Completing RL single modelings the learning procedure is defined as follows: goal positions are defined in such a way that they are always reachable for the robot; thus, the learning process needs to develop an internal model of the inverse kinematics of the robot which is not directly injected by the designer; through the different trials, a model of the robot inverse kinematics is learned by the system; when a goal position is generated, the robot tries to reach it; each trial can finish as a success episode, i.e.</a>
<a href="#16" id="16">However, Lenient and CA schemes were able to resolve that issue.</a>
<a href="#17" id="17">As was mentioned in Section 5.1, this 3DMC problem with ObsL presents different state spaces, decentralized action spaces, and individual reward functions.</a>
</body>
</html>