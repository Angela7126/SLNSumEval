<html>
<head>
<meta name="TextLength" content="SENT_NUM:43, WORD_NUM:998">
</head>
<body bgcolor="white">
<a href="#0" id="0">a belief state that includes a priori common-sense knowledge and mental models of each of the agents involved (the robot and its human partners).</a>
<a href="#1" id="1">reuses the same set of affordances and inferences, together with explicit contextual reasoning on humans and robot abilities, to generate human – robot shared plans.</a>
<a href="#2" id="2">The remaining of the article details this robotic architecture.</a>
<a href="#3" id="3">We organise this discussion in five sections.</a>
<a href="#4" id="4">The next section introduces the architecture as a whole, as well as the knowledge model that we have developed for our robots.</a>
<a href="#5" id="5">Section 3 discusses each of the cognitive components of the architecture.</a>
<a href="#6" id="6">Section 4 presents two studies that illustrate in a practical way what can be currently achieved with our robots.</a>
<a href="#7" id="7">The Sections 5 and 6 finally summarise our main contributions and restate the key challenges that human – robot interaction brings to Artificial Intelligence.</a>
<a href="#8" id="8">For instance, a book laying on a furniture might be picked up by Spark and represented in symbolic terms as 〈 BOOK1 type Book, BOOK1 isOn TABLE 〉 .</a>
<a href="#9" id="9">These symbolic statements are stored in the knowledge base Oro and made available to the other cognitive modules.</a>
<a href="#10" id="10">Later, the robot might process a sentence like “ give me another book ” .</a>
<a href="#11" id="11">The Dialogs module would then query the knowledge base: find(?obj type Book, ?obj differentFrom BOOK1), and write back assertions like 〈 HUMAN desires GIVE_ACTION45, GIVE_ACTION45 actsOn BOOK2 〉 to Oro.</a>
<a href="#12" id="12">This would in turn trigger the execution controller Shary to prepare to act.</a>
<a href="#13" id="13">It would first call the HATP planner.</a>
<a href="#14" id="14">The planner uses the knowledge base to initialise the planning domain (e.g.</a>
<a href="#15" id="15">find(BOOK2 isAt ?location)), and returns a full symbolic plan to the execution controller.</a>
<a href="#16" id="16">Finally, the controller would execute the plan and monitor its achievement, both for itself and for the human.</a>
<a href="#17" id="17">We present complete examples of similar interactions in Section 4.</a>
<a href="#18" id="18">Building a grounded symbolic model of the physical environment does not suffice in general to fully ground the human – robot interaction, and a model of the current capabilities of the agents interacting with the robot is also required.</a>
<a href="#19" id="19">Monitoring human activity is needed by the execution controllers to track the engagement of the human and the progress of their actions.</a>
<a href="#20" id="20">It is also needed to synchronise seamlessly its own actions with the human actions.</a>
<a href="#21" id="21">Full human action and activity recognition is a task that requires knowledge and reasoning both on high-level facts like goals, intentions and plans, as well as bottom-up data from human and object motions.</a>
<a href="#22" id="22">Spark implements a set of simple temporal and geometric heuristics on human hand trajectories and possible objects placements to recognise simple elementary actions.</a>
<a href="#23" id="23">Those primitive actions are assessed by monitoring situations like “ an empty hand is close to an object on a table ” (precursor for a pick), or “ a hand holding an object is over a container ” (precursor for a put).</a>
<a href="#24" id="24">Spark recognises a set of such primitives.</a>
<a href="#25" id="25">When combined with the other geometric computations and a predictive plan of the human actions (see Section 3.4), the execution controller can track the fulfilment of the pre- and post-conditions of the predicted human actions.</a>
<a href="#26" id="26">The robot relies on these to monitor the engagement of the human and the overall progress of the human – robot shared plan.</a>
<a href="#27" id="27">The HATP planning domain defines a set of methods describing how to incrementally decompose a task and to allocate subtasks and actions to the robot and/or the human depending on the context.</a>
<a href="#28" id="28">This represents the procedural knowledge of the robot as well as its knowledge about the actions that the human partner is able to achieve.</a>
<a href="#29" id="29">It is stored outside of the central knowledge base, using a specific formalism (see the related discussion at the end of this section).</a>
<a href="#30" id="30">As HATP is a generic symbolic task planner and does not enforce any abstraction level for the planning domain, we have designed a planning domain made of top-level tasks whose semantics are close to the one used in the human – robot dialogue: the planner domain effectively contains concepts like Give, table, isOn.</a>
<a href="#31" id="31">This leads to an effective mapping between the knowledge extracted from the situation assessment or the dialogue, and the planner.</a>
<a href="#32" id="32">Finally, when an action requires the motion of both the human and the robot, Mhp can plan for both of them in order for the robot to take the lead by automatically computing where the interaction might preferably take place [3], [4].</a>
<a href="#33" id="33">This can effectively smoothen the interaction by off-loading a part of the cognitive load of the interaction from the human to the robot.</a>
<a href="#34" id="34">As such, this is the geometric counterpart of the HATP symbolic task planner: it is able to make use of a set of social rules to adapt geometric plans to social interactions.</a>
<a href="#35" id="35">We present here elements of two of them in order to illustrate in a practical way the different aspects of the architecture.</a>
<a href="#36" id="36">The first one is focused on knowledge representation and verbal interaction: the human asks for help to find and pack objects (Interactive Grounding I in Table 3).</a>
<a href="#37" id="37">The second one (Cleaning the table) involves the Shary execution controller and the HATP symbolic task planner.</a>
<a href="#38" id="38">In this scenario, the human and the robot need to cooperatively remove objects from a table.</a>
<a href="#39" id="39">Robot behaviours and motions are fully planned and then executed.</a>
<a href="#40" id="40">The previous sections provide a perspective on a complete deliberative architecture for social robots, including its implementation, supported by experimental deployments.</a>
<a href="#41" id="41">This section synthesises what we see as the main challenges that human – robot interaction brings to Artificial Intelligence.</a>
<a href="#42" id="42">We first discuss how embodied cognition is an essential challenge in human – robot interaction; we rephrase then the requirements of joint actions in terms of five questions; we discuss the importance of building and maintaining a multi-level model of the human; and we finally reflect on the importance of explicit knowledge management in robotic architectures that deal with human-level semantics and state in that respect the current limits of our logic framework.</a>
</body>
</html>