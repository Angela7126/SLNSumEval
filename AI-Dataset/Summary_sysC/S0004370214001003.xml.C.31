<html>
<head>
<meta name="TextLength" content="SENT_NUM:35, WORD_NUM:884">
</head>
<body bgcolor="white">
<a href="#0" id="0">We now describe our method for learning general appliance models which will generalise to previously unseen appliances of the same type.</a>
<a href="#1" id="1">The aim is to learn distributions over the model parameters for each appliance, such that both the mean and variance around each appliance parameter is derived from data.</a>
<a href="#2" id="2">This process is effective as it allows tight distributions to be learned over appliance parameters which are similar for different appliance instances, and broad distributions to be learned when parameters vary greatly between different instances.</a>
<a href="#3" id="3">In general, the most important factor is to ensure that the learned states align between different appliance instances, which we demonstrate through the Bayesian framework described in Section 2.1.</a>
<a href="#4" id="4">We adopt a hierarchical approach to model multiple appliances of the same type, as shown by Fig.</a>
<a href="#5" id="5">2.</a>
<a href="#6" id="6">In this model, we represent an appliance type (e.g.</a>
<a href="#7" id="7">refrigerator) as a distribution from which appliance instances (e.g.</a>
<a href="#8" id="8">Bosch Logixx KSV36AW41G refrigerator) are drawn.</a>
<a href="#9" id="9">As such, the appliance type represents any behaviour which is common to all instances of that type, while an appliance instance also represents behaviour which is specific to that single instance and its usage.</a>
<a href="#10" id="10">Furthermore, we observe sequences of power data from each appliance instance.</a>
<a href="#11" id="11">Therefore, the aim is to infer the parameters of an appliance type, Θ , from sequences of power data, {a mathematical formula}x(n)={x1, … ,xT}, generated by individual appliance instances described by parameters {a mathematical formula} Θ ={ Θ (1), … , Θ (N)}, where n is one of N appliance instance indices.</a>
<a href="#12" id="12">We use this Bayesian approach to parameter estimation in HMMs to individually learn the parameters, {a mathematical formula} Θ (n), of each appliance instance, n, from sequences of their power data, {a mathematical formula}x(n).</a>
<a href="#13" id="13">Since there is no analytical solution to parameter estimation in HMMs, we performed inference using variational message passing [39], full details of which are included in Appendix A. Variational message passing was used since it provides an efficient and deterministic method of Bayesian parameter estimation for which convergence is guaranteed [27].</a>
<a href="#14" id="14">We now describe a method by which the parameters learned in Section 2.1 can be combined to form a model that represents the whole appliance type, and therefore generalises to previously unseen instances of that appliance type.</a>
<a href="#15" id="15">Our method consists of fitting distributions to samples drawn from the posterior distributions over appliance instance parameters.</a>
<a href="#16" id="16">As a result, this method averages over our uncertainty around the appliance instance parameters.</a>
<a href="#17" id="17">We introduce the notation:{a mathematical formula} to represent the parameters of the general model of an appliance type as defined in the following paragraphs.</a>
<a href="#18" id="18">In the case of the refrigerator, Θ represents a distribution over all possible refrigerator instances.</a>
<a href="#19" id="19">Crucially, this general model allows the probability to be calculated that an appliance instance belongs to the refrigerator appliance type.</a>
<a href="#20" id="20">As identified in the introduction, some appliance behaviour is unique to a particular household and therefore cannot be captured by the general model of the appliance.</a>
<a href="#21" id="21">Such behaviour can be due to the unique characteristics of the appliance instances present in a household (e.g.</a>
<a href="#22" id="22">a freezer with a defrost cycle), and also due to their pattern of usage by the household's occupants (e.g.</a>
<a href="#23" id="23">a microwave often used on low power).</a>
<a href="#24" id="24">Therefore, in this section we propose a method for learning such behaviour that is unique to a single household, which uses both general appliance models and household aggregate data.</a>
<a href="#25" id="25">More formally, this tuning process directly corresponds to learning the parameters {a mathematical formula} Θ (n) for an appliance instance n in a household, given the appliance type's general model Θ and the household's aggregate data x.</a>
<a href="#26" id="26">Our approach differs from the training approach used by [20], in which appliances are detected using a factorial HMM but are also required to be manually labelled.</a>
<a href="#27" id="27">Similarly, Kolter and Jaakkola [22] proposed a training approach in which individual appliance motifs are extracted from aggregated data, but again each motif was also required to be manual labelled with an appliance name.</a>
<a href="#28" id="28">Once the signatures of a single appliance instance have been extracted from aggregate data, the aim is to tune the general model to include the behaviour of the appliance instance which is unique to the previously unseen household.</a>
<a href="#29" id="29">Given that both the general model for this appliance type, Θ , and signatures sampled from the specific appliance instance are available, {a mathematical formula}x¯i:j, Bayesian integration [15] provides a natural approach to infer the posterior distribution over such appliance instance parameters with the state sequence marginalised out:{a mathematical formula} Therefore, we update the general model again using variational message passing as described in Appendix A.</a>
<a href="#30" id="30">Fig.</a>
<a href="#31" id="31">9 illustrates the outcome of the tuning process using the microwave's on state as an example.</a>
<a href="#32" id="32">It can be seen that the prior distribution, as learned during the generalisation method described in Section 2, shows a broad distribution over the mean power of all microwaves.</a>
<a href="#33" id="33">In contrast, the posterior distribution, as tuned using the method described in this section, shows a more precise distribution over the mean of this specific microwave instance.</a>
<a href="#34" id="34">However, it should be noted that the mean power of the on state, {a mathematical formula} Μ k, is just one of the set of appliance model parameters, Θ , and therefore it is not expected that appliances will be uniquely distinguishable using only this parameter.</a>
</body>
</html>