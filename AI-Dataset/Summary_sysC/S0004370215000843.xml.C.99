<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:808">
</head>
<body bgcolor="white">
<a href="#0" id="0">We then introduce representational structures and a common vocabulary for representing knowledge about robot actions, events, objects, environments, and the robot's hardware as well as inference procedures that operate on this common representation.</a>
<a href="#1" id="1">The content of a robot KR system has to be continuously updated with new perceptual or actuation data.</a>
<a href="#2" id="2">As this information is already present in the robot's control system, we consider the KR system as a kind of ‘ parasite ’ on top of the existing data structures whose task is to make sense from them and to lift them to a conceptual level.</a>
<a href="#3" id="3">As a consequence, the content of the knowledge base is grounded in these data structures that have carefully been developed by a robot programmer in such a way that they are grounded in the outer world, for instance using specialized perception routines.</a>
<a href="#4" id="4">However, the level of detail and the many types of information required for successfully accomplishing robot tasks, such as exact times, 3D geometric information, kinematic structures and appearance models, are difficult to encode in purely symbolic form.</a>
<a href="#5" id="5">A proper representation of the robot's complex dynamic surroundings would require very expressive formalisms, while the fact that much information is obtained from partial observations with noisy sensors will inevitably lead to wrong and inconsistent information that is difficult to resolve in a purely logical knowledge base.</a>
<a href="#6" id="6">Our knowledge base only has a rather shallow symbolic representation of general concepts, while most information about the robot, its environment and perceptions is merely a “ virtual knowledge base ” computed at runtime from the data structures of the robot's control program.</a>
<a href="#7" id="7">These reasoners need access to the robot's task logs, to proprioceptive information about the gripper movements, to the environment model and to general knowledge about types of objects.</a>
<a href="#8" id="8">We therefore decided to use a common underlying representation for all these kinds of information and to have an ensemble of expert reasoners operate on this shared knowledge.</a>
<a href="#9" id="9">There is a wide range of aspects that contribute to the performance of a robot knowledge processing system, including the number of facts it contains, the coverage of relevant knowledge areas, the range of inference methods that are supported, the run time for common queries, the scalability regarding the amount of information stored, the usefulness of the computed results for robot tasks, etc.</a>
<a href="#10" id="10">They do not have to be of logical nature, but can employ any kind of computation as long as they read their input data from the knowledge base and produce facts formulated in the common representation language.</a>
<a href="#11" id="11">Since there is no structural difference between static objects in an environment map, movable objects detected by the robot's perception system, and parts of the robot, any kind of object information in the system can easily be compared.</a>
<a href="#12" id="12">On the logical level, it requires the ability to qualify a relation (in this case the atLocation property that links an object instance to a pose) with the time when it was valid, turning it from a binary relation between the object and the pose to a ternary one that also includes the time.</a>
<a href="#13" id="13">In our case, however, the reified objects are more than just a changing value because they provide a memory of past states, they describe the source of this relation by their type, and allow to reason about multiple “ possible worlds ” , for example the perceived, desired and simulated world.</a>
<a href="#14" id="14">In particular fine manipulation or tool usage require detailed geometric information about the objects involved that is difficult to represent in a symbolic knowledge base.</a>
<a href="#15" id="15">We therefore extend the object classes with links to detailed three-dimensional CAD models, which provide a very detailed geometric description and are often available from free databases on the Web.</a>
<a href="#16" id="16">for visualization or grasp planning, robots often need to interact with specific object parts that have functional meaning for the action at hand: For picking up items, they should use the handle; for pouring something from or into a container, they need information about its opening direction and volume; for pouring something onto a surface, they have to select a suitable horizontal surface.</a>
<a href="#17" id="17">These inferences require a tight integration of geometric and semantic information, which we again implement as a “ virtual knowledge base ” , i.e.</a>
<a href="#18" id="18">These action models have originally been developed as “ action recipes ” as part of the RoboEarth language [45] for the exchange of task descriptions between robots and has been extended to other use cases since.</a>
<a href="#19" id="19">The description of a concrete task usually derives specific sub-classes from the general action classes in the KnowRob ontology and annotates them with task-specific information about objects, tools, locations or timing.</a>
<a href="#20" id="20">To quantify the performance in a common task, we have simulated the creation of a large number of object perceptions as described in Section 4.3, which are among the most complex structures in KnowRob.</a>
</body>
</html>