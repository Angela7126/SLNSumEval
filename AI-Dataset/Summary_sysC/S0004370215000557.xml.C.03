<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:525">
</head>
<body bgcolor="white">
<a href="#0" id="0">As described in Section 1, this article examines the effect of various objectives for learning from human reward on task performance.</a>
<a href="#1" id="1">For the experiments described in this article, a Markovian model of human reward, {a mathematical formula}R ˆ H, is learned from human reward instances.</a>
<a href="#2" id="2">At the other extreme, an agent with a {a mathematical formula} Γ =1 objective values near-term reward equally as reward infinitely far in the future.</a>
<a href="#3" id="3">In our experiments, a predictive model of human reward, {a mathematical formula}R ˆ H, is learned and provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D} (Fig.</a>
<a href="#4" id="4">The tamer framework is a fully myopic (i.e., {a mathematical formula} Γ =0), high-level algorithm for learning from human reward.</a>
<a href="#5" id="5">At the extreme of this trend, the tamer framework discounts by {a mathematical formula} Γ =0, learning a model of human reward that is (because of this discounting) also an action-value function.</a>
<a href="#6" id="6">Thus, setting {a mathematical formula} Γ =0 effectively reduces reinforcement learning of a value function to supervised learning of a reward function.</a>
<a href="#7" id="7">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#8" id="8">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#9" id="9">During training, human reward signals form labels for learning samples that have state-action pairs as features; a regression algorithm continuously updates {a mathematical formula}R ˆ H with new training samples.</a>
<a href="#10" id="10">We call these experiments off-discounting{sup:3} because the human reward data was gathered under {a mathematical formula} Γ =0 discounting, which usually differs from the discounting used during evaluation, when the agent learns a value function from the learned reward model.</a>
<a href="#11" id="11">The positive circuits problem is most clearly explained for the case when {a mathematical formula} Γ =1, but circuits with net-positive reward can also be problematic at high Γ values that are less than 1.</a>
<a href="#12" id="12">Thus, at {a mathematical formula} Γ =1{a mathematical formula}R ˆ H is being used as if it were interchangeable with a conventional MDP reward function.</a>
<a href="#13" id="13">Mean task performance and mean total reward per episode for each tested discount factor across all 19 {a mathematical formula}R ˆ H models are displayed in Fig.</a>
<a href="#14" id="14">In the episodic-task analysis described here in Section 7.2.2 — as in the off-discounting analysis in the previous Section 7.2.1 — the human reward model {a mathematical formula}R ˆ H is learned by tamer and provides predictions that are interpreted as reward by an RL algorithm.</a>
<a href="#15" id="15">To illustrate, we note that an example reward function for {a mathematical formula} Γ <1 that provides only goal-related information is to give +1 reward at the goal and 0 otherwise.</a>
<a href="#16" id="16">as mentioned in Section 7.2.1, the {a mathematical formula}R ˆ Hs likely reflect trainer adaptation to the {a mathematical formula} Γ =0 discount factor under which these human reward models were trained;</a>
</body>
</html>