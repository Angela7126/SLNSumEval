<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:407">
</head>
<body bgcolor="white">
<a href="#0" id="0">10a shows that it required a configuration budget of at least 10{sup:4} seconds to find improving Clasp-3.0.4-p8 parameters for the 3cnf benchmark (where the default version of Clasp-3.0.4-p8 produced 18 timeouts).</a>
<a href="#1" id="1">While the configuration of DCCASat+march-rw's single parameter had long converged by 10{sup:4} seconds, the configuration of Clasp-3.0.4-p8's 75 parameters continued to improve performance until the end of the configuration budget, and, in particular for the 3cnf benchmark, performance would have likely continued to improve further if the budget had been larger.</a>
<a href="#2" id="2">We thus conclude that the solver's flexibility should be chosen in relation to the available budget for configuration: solvers with few parameters can often be improved more quickly than highly flexible solving frameworks, but, given enough computational resources and powerful configurators, the latter ones can typically offer a greater performance potential.</a>
<a href="#3" id="3">More specifically, it uses previously observed 〈 configuration, performance 〉 pairs {a mathematical formula} 〈 Θ ,f( Θ ) 〉 to learn a random forest of regression trees (see, e.g., [22]) that express a function {a mathematical formula}f ˆ : Θ → R predicting the performance of arbitrary parameter configurations (including those not yet evaluated) and then uses this function to guide its search.</a>
<a href="#4" id="4">When instance characteristics {a mathematical formula}x Π ∈ F are available for each problem instance Π , SMAC uses observed 〈 configuration, instance characteristic, performance 〉 triplets {a mathematical formula} 〈 Θ ,x Π ,f( Θ , Π ) 〉 to learn a function {a mathematical formula}g ˆ : Θ ×F → R that predicts the performance of arbitrary parameter configurations on instances with arbitrary characteristics.</a>
<a href="#5" id="5">These so-called empirical performance models[49] are then marginalized over the instance characteristics of all training benchmark instances in order to derive the function {a mathematical formula}f ˆ that predicts average performance for each parameter configuration: {a mathematical formula}f ˆ ( Θ )=E Π ∼ Π train[g ˆ ( Θ , Π )].</a>
<a href="#6" id="6">After an initialization phase, SMAC iterates the following three steps: (1) use the performance measurements observed so far to fit a marginal random forest model {a mathematical formula}f ˆ ; (2) use {a mathematical formula}f ˆ to select a promising configuration {a mathematical formula} Θ ∈ Θ to evaluate next, trading off exploration in new parts of the configuration space and exploitation in parts of the space known to perform well; and (3) run Θ on one or more benchmark instances and compare its performance to the best configuration observed so far.</a>
</body>
</html>