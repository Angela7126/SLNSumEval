<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:468">
</head>
<body bgcolor="white">
<a href="#0" id="0">In open agent systems, however, the outcome of an action may well depend on what the other agents in the situation choose to do.</a>
<a href="#1" id="1">Thus an individual's choice does not necessarily determine the state that will be reached.</a>
<a href="#2" id="2">The values may be ascribed to transitions on the basis of the source and target states, or in virtue of an action in the joint action, where that action has intrinsic value.</a>
<a href="#3" id="3">Given a representation of the problem situation as an AATS+V, the discovery of arguments, counter arguments and objections can be implemented in several ways, including that used in [44].</a>
<a href="#4" id="4">While problem formulation and the identification of the current state can be resolved using normal theoretical reasoning techniques, and the option selection stage can be carried out using value-based reasoning based on VAFs as described in [16], how the joint action should be determined in the epistemic stage is less obvious and is the topic of this paper.</a>
<a href="#5" id="5">The real advance here over previous work such as [19] is that there is no longer any need to make assumptions about what the other believes and prefers: the agent can now come to a decision using its own relative preferences between values, its own beliefs and the degree of risk it is prepared to take, whilst requiring no additional machinery: it uses only the AATS+V [19] based on its own beliefs, causal model and values [45].</a>
<a href="#6" id="6">Even where the action performed does affect values, as in [47], so that the intrinsic value of an action can be taken into account, what matters for the agent concerned is its own individual action, and so all transitions between the same pair of states containing that action will promote and demote the same values, as far as that agent is concerned.</a>
<a href="#7" id="7">Thus, for our current purposes, we will consider all joint actions with the same action by the agent concerned leading to the same state to be equivalent, so that consideration can be limited to the different outcomes possible for a given action of the agent engaged in the reasoning, irrespective of how many joint actions reach each outcome.</a>
<a href="#8" id="8">Now, unlike previous work such as [19], there is no longer any need for the reasoning agent to make assumptions about the others' beliefs, domain conceptualisation and preferences that this other agent would use to choose a particular action: the reasoning agent will then be able to decide using its own relative preferences between values, its own beliefs and, where necessary, the particular degree of risk it is subjectively prepared to accept.</a>
<a href="#9" id="9">However, the utilities of these payoffs are subjective with respect to the individual goals and aspirations of the agent concerned, and so can be individually set and made subject to change, possibly as a result of persuasive argument, or of empirical evidence.</a>
</body>
</html>