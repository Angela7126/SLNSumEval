<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:380">
</head>
<body bgcolor="white">
<a href="#0" id="0">Alors exploits the {a mathematical formula}(n,m) collaborative filtering matrix {a mathematical formula}M reporting the domain-dependent performance {a mathematical formula}Mi,j of the j-th algorithm for the i-th problem instance for a fraction of the {a mathematical formula}(i,j) pairs.</a>
<a href="#1" id="1">The cold start (CS) functionality achieves algorithm selection for a new problem instance, for which the latent representation cannot be determined from the CF matrix by construction.</a>
<a href="#2" id="2">Alors exploits the known problem instances to learn the latent representation from the initial representation as follows.</a>
<a href="#3" id="3">Each problem instance defines a training example {a mathematical formula}(xi,Ui).</a>
<a href="#4" id="4">From the training set{a mathematical formula} a mapping Φ ({a mathematical formula} Φ :Rd ↦ Rk) is learned using random forests.</a>
<a href="#5" id="5">{sup:4} For each new problem instance with initial representation x, the associated latent representation {a mathematical formula}Ux is approximated as {a mathematical formula} Φ (x).</a>
<a href="#6" id="6">The cold-start problem is then brought back to the matrix completion one, by estimating the performance of the j-th algorithm (or the rank thereof) on the new problem instance as {a mathematical formula} 〈 Φ (x),Vj 〉 , and algorithm selection then follows (Algorithm 1).</a>
<a href="#7" id="7">This section presents the experimental setting used for the comparative empirical validation of Alors.</a>
<a href="#8" id="8">The first indicator, noted cf, measures the performance of algorithm selection on known problem instances based on an excerpt of matrix {a mathematical formula}M. After the above discussion, the cf performance reflects the quality of the set of problem instances, called benchmark in the following.</a>
<a href="#9" id="9">The second performance indicator, noted cs, measures the quality of algorithm selection for new problem instances.</a>
<a href="#10" id="10">The sensitivity of both performances with respect to the incompleteness rate p of the {a mathematical formula}M matrix and the number k of latent factors is studied on two real-world benchmarks.</a>
<a href="#11" id="11">An artificial problem is also defined to investigate and compare Alors and Matchbox.</a>
<a href="#12" id="12">iii)</a>
<a href="#13" id="13">using the latent factors to fill-in matrix {a mathematical formula}Mp and determining the selected algorithm for each i-th problem instance;</a>
<a href="#14" id="14">On the OpenML dataset, the performance indicator (the regret or excess prediction loss compared to the oracle algorithm, section 5.1) shows a high diversity of the problem instances (Fig.</a>
<a href="#15" id="15">12).</a>
<a href="#16" id="16">The rank of the single best baseline is circa 60 – with the caveat that the statistical significance of the differences between the algorithm performances is not available.</a>
</body>
</html>