<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:504">
</head>
<body bgcolor="white">
<a href="#0" id="0">This article presents an intrinsically motivated model-based RL algorithm, called texplore with Variance-And-Novelty-Intrinsic-Rewards (texplore-vanir), that uses intrinsic motivation both for improved sample efficiency and to give the agent a curiosity drive.</a>
<a href="#1" id="1">The agent is based on a model-based RL framework and is motivated to learn models of domains without external rewards as efficiently as possible.</a>
<a href="#2" id="2">texplore-vanir combines model learning through the use of random forests with two unique intrinsic rewards calculated from this model.</a>
<a href="#3" id="3">The first reward is based on variance in its models' predictions to drive the agent to explore where its model is uncertain.</a>
<a href="#4" id="4">The second reward drives the agent to novel states which are the most different from what its models have been trained on.</a>
<a href="#5" id="5">The combination of these two rewards enables the agent to explore in a developing curious way, learn progressively more complex skills, and learn a useful model of the domain very efficiently.</a>
<a href="#6" id="6">When learning with a tabular model, the agent must gain enough experiences in each state-action to learn an accurate model of it.</a>
<a href="#7" id="7">Thus it makes sense to use intrinsic motivation to drive the agent to acquire these experiences, as done by r-max[4].</a>
<a href="#8" id="8">With a model learning approach that generalizes as texplore-vanir's does, the best intrinsic rewards are different again.</a>
<a href="#9" id="9">Our goal is to develop an intrinsically motivated curious agent using RL.</a>
<a href="#10" id="10">This agent should use intrinsic rewards to 1) efficiently learn a useful model of the domain's transition dynamics; and 2) explore in a developing curious way.</a>
<a href="#11" id="11">We therefore evaluate texplore-vanir in four ways on a complex domain with no external rewards.</a>
<a href="#12" id="12">First, we measure the accuracy of the agent's learned model in predicting the domain's transition dynamics.</a>
<a href="#13" id="13">Second, we test whether the learned model can be used to perform tasks in the domain when given a reward function.</a>
<a href="#14" id="14">Third, we examine the agent's exploration to see if it is exploring in a developing, curious way.</a>
<a href="#15" id="15">Finally, we demonstrate that texplore-vanir can combine its intrinsic rewards with external rewards to learn faster than if it was given only external rewards.</a>
<a href="#16" id="16">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#17" id="17">Second, we test whether the learned model can be used to perform tasks in the domain when given a reward function.</a>
<a href="#18" id="18">These results demonstrate that the intrinsic rewards and model learning approach texplore-vanir uses are sufficient for the agent to explore in a developing curious way and to efficiently learn a transition model that is useful for performing tasks in the domain.</a>
<a href="#19" id="19">These results show that texplore-vanir's intrinsic rewards drive the agent to explore more of the state space and discover more affordances in the task than if the agent explored randomly.</a>
<a href="#20" id="20">Exploring with texplore-vanir enables the robot to learn a more accurate model of more of the state space and perform better on possible tasks given to the robot after exploring the domain.</a>
</body>
</html>