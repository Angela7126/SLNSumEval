<html>
<head>
<meta name="TextLength" content="SENT_NUM:16, WORD_NUM:717">
</head>
<body bgcolor="white">
<a href="#0" id="0">consists of all ground instances of atom a that belong to every answer set of P. Taking the prerequisites of instantiation procedures like “ intelligent grounding ” [36] into account, non-ground ASP-Core-2 programs have to comply with additional requirements [15], such as finiteness of answer sets and safety.</a>
<a href="#1" id="1">In a nutshell, these conditions require the availability of (positive) occurrences of variables within the bodies of rules as well as aggregate or choice elements in order to restrict the relevant substitutions and enable grounders to compute a (finite) ground program, which is typically much smaller yet equivalent to {a mathematical formula}grnd(P).</a>
<a href="#2" id="2">In view of the objective of comparing participant systems in a uniform setting, this edition of the ASP Competition did not include a Model&Solve track, but took place in the spirit of the former System track: it was open to any general-purpose solving system, provided it was able to process ASP-Core-2 programs.</a>
<a href="#3" id="3">The second column of Table 1 summarizes the usage of particular language features, among the ones introduced in Section 2, for the encodings from the Fourth ASP Competition in 2013.</a>
<a href="#4" id="4">{sup:5} While merely normal rules and comparison operators, considered as basic features, are used for Stable Marriage, the Basic Decision encoding for Labyrinth induces non-tight ground instantiations with positive recursion among atoms [40], [35].</a>
<a href="#5" id="5">Provided that preference values lie in the range from 1 to n, as sufficient to distinguish n men as well as women, a ground instantiation of the subprogram in (11), (12), (13), (14) is of space complexity {a mathematical formula}O(n2) and thus much more compact than (10).</a>
<a href="#6" id="6">Regarding the revised Advanced Decision encodings, significant reductions in grounding size, by about one order of magnitude in comparison to their 2013 counterparts (as indicated in Table 15), were achieved for the domains Incremental Scheduling, Nomystery, Partner Units, and Weighted-Sequence Problem.</a>
<a href="#7" id="7">Assuming that instances of {a mathematical formula}share(J1,J2) provide jobs {a mathematical formula}J1 and {a mathematical formula}J2 to be executed on a common device, the 2013 encoding includes a subprogram as follows for picking starting times and denying overlaps:{a mathematical formula}{a mathematical formula} Similar to (10), the integrity constraint in (16) lists all forbidden starting times {a mathematical formula}T2 for job {a mathematical formula}J2 relative to a starting time T and the length L of job {a mathematical formula}J1 explicitly.</a>
<a href="#8" id="8">In order to use common inputs, we compare the previous and this year's systems on Basic Decision encodings only, taking into account that lp2sat processes a legacy format that differs from ASP-Core-2 on advanced constructs such as aggregates.</a>
<a href="#9" id="9">Given that the Basic Decision track on 2013 encodings merely includes two domains, we here also consider the six alternative encodings with basic features only, and Table 13 shows respective scores and cumulative CPU times for solved instances.</a>
<a href="#10" id="10">To this end, Table 15 provides grounding parameters, obtained by running gringo-4 under the same time and memory limits as participant systems, along with the scores of systems in the SP category (except for wasp-wpm1-only-weak, which did not participate in the majority of domains) on previous and novel encoding variants, given in the upper or lower row, respectively, per domain.</a>
<a href="#11" id="11">As described in Section 4, one objective of revising previous encodings was to reduce grounding bottlenecks that affected all participant systems and hampered a meaningful comparison.</a>
<a href="#12" id="12">We performed a number of ex-post analyses on the results of the 2011 and 2013 editions, experimenting with different scoring schemes.</a>
<a href="#13" id="13">At first, we found that time quota did not make much difference and accordingly use {a mathematical formula}Ssolve(P) only (simply denoted {a mathematical formula}S(P) in Section 3.5), which as before linearly reflects the number of solved instances of a Decision or Query problem P. Furthermore, we pondered that absolute objective values are not adequate for scoring solutions for Optimization problems, given that they depend heavily on domains and are susceptible to perturbations; rather than that, relative rankings can draw a more reliable picture.</a>
<a href="#14" id="14">Hence, for an Optimization problem P, we determine {a mathematical formula}Ssolve(P) by accumulating ranks by solution quality for instances of P. Interestingly, if employed in the latest editions, the winners would have been the same.</a>
<a href="#15" id="15">The way tracks are defined depends on the competition at hand, but three main criteria can be identified: language features of inputs, the kind of problem domains, and reasoning tasks to be accomplished.</a>
</body>
</html>