<html>
<head>
<meta name="TextLength" content="SENT_NUM:31, WORD_NUM:696">
</head>
<body bgcolor="white">
<a href="#0" id="0">The criteria that we will use are shown in Table 3.</a>
<a href="#1" id="1">The ‘ years ’ criterion gives us a chronological perspective that can show trends about the interest in computer models solving intelligence test problems.</a>
<a href="#2" id="2">The ‘ range ’ specifies the kinds of tests addressed by the models.</a>
<a href="#3" id="3">This criterion is fundamental to understand whether the computer models are specialised or general.</a>
<a href="#4" id="4">The ‘ intention ’ criterion specifies the purpose of the study: to better understand human cognition, to understand general principles of intelligence, to propose a metric for AI evaluation (i.e., psychometric AI), or to make a philosophical/epistemological question (human cognition, AI principles, AI evaluation, philosophical).</a>
<a href="#5" id="5">This provides key information to truly understand the model achievements.</a>
<a href="#6" id="6">The ‘ techniques ’ criterion shows which techniques are used by each model, which is relevant to see the role and progress of AI techniques or other ad-hoc techniques.</a>
<a href="#7" id="7">The ‘ representation ’ criterion indicates whether the data was transformed to other representations or the original representation was used.</a>
<a href="#8" id="8">‘ Performance ’ specifies what the kind of comparison with humans was made while ‘ Difficulty ’ specifies the kind of difficulty assessment derived from the model.</a>
<a href="#9" id="9">For these two last criteria, we specify whether it is at the test level or item by item (values can be no, global, or itemwise).</a>
<a href="#10" id="10">Apart from possibly being useful as benchmarks, these models are usually proposed as tools to improve our understanding of intelligence.</a>
<a href="#11" id="11">In particular, the third question (Q3) is: Are computer models for solving intelligence test problems useful for understanding human cognition?</a>
<a href="#12" id="12">We want to know whether these tasks are good for cognitive modelling and to ascertain what constructs and cognitive mechanisms are needed to solve these problems.</a>
<a href="#13" id="13">It is also crucial to see why and how humans solve (some of) them.</a>
<a href="#14" id="14">In order to answer this question, we will require several criteria, mostly ‘ intention ’ , ‘ range ’ , and ‘ performance ’ .</a>
<a href="#15" id="15">Basically, this question is about the implications for cognitive science.</a>
<a href="#16" id="16">We need to be clear that focussing on the overall results of a computer model and comparing them with the results of humans (column ‘ performance ’ on Table 4) is not very informative about how challenging the problem is (question Q2).</a>
<a href="#17" id="17">First, humans are general-purpose systems and it is not fair to compare them with some systems that are only able to solve one problem — even if the problem comes from an intelligence test.</a>
<a href="#18" id="18">Second, many of these intelligence test problems have been developed for humans, and hence it can be unfair to evaluate AI systems limitations with anthropocentric measures.</a>
<a href="#19" id="19">Third, some of the works perform an interesting analysis in terms of difficulty (column ‘ difficulty ’ on Table 4).</a>
<a href="#20" id="20">The purpose is to determine what instances are more difficult, but this is not very related to how challenging the problem is.</a>
<a href="#21" id="21">In fact, focussing on the most difficult problems may even make the system more specialised to the intelligence test task at hand.</a>
<a href="#22" id="22">Some of the previous works have studied whether difficulty is related to the size of the working memory, the size of the pattern, or the number of elements that need to be combined or retrieved from background knowledge [11], [14], [131], [132].</a>
<a href="#23" id="23">These notions of difficulty are much more general and can work independently of the problem and the representation.</a>
<a href="#24" id="24">Also, if we compare Table 1, Table 2 we see a mismatchbetween deduction and induction.</a>
<a href="#25" id="25">This may suggest that inductive reasoning is still the area where more progress is required.</a>
<a href="#26" id="26">In fact, for most approaches the system does not learn to solve the problems but it is programmed to solve the problems.</a>
<a href="#27" id="27">In other words, the task is hard-coded into the program and it can be easier to become ‘ superhuman ’ in many specific tasks, as happens with chess, draughts, some kinds of planning, and many other tasks.</a>
<a href="#28" id="28">But humans are not programmed to do intelligence tests.</a>
<a href="#29" id="29">Only for a few cases (such as Spaun, but with many limitations and just some kinds of tasks), the system is trained to solve intelligence test tasks.</a>
<a href="#30" id="30">We think that this is one of the lessons learnt in AI.</a>
</body>
</html>