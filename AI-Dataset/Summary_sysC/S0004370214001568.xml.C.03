<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:489">
</head>
<body bgcolor="white">
<a href="#0" id="0">For example, an NBC review of one toy car noted that “ The AI can also get easily confused ” [13].</a>
<a href="#1" id="1">AI researchers soon recognized that to solve the “ kinds of problems now reserved for humans ” would require a substantive knowledge base and a host of ways to use it [16].</a>
<a href="#2" id="2">A CI is intended not to substitute for a human employee, but to engage in a task with one.</a>
<a href="#3" id="3">To collaborate effectively with a person, a CI must be able to model the human view of the world.</a>
<a href="#4" id="4">A CI must engage in dialogue with its human partner.</a>
<a href="#5" id="5">As an ongoing example, consider a kitchen CI that is asked, “ How do you make soup? ” A CI with extensive knowledge must filter its possible responses, because its help will only be valued if it is neither too simplistic (e.g., “ Just open a can and heat the contents. ” ) nor too detailed (e.g., “ … the history of soup is probably as old as the history of cooking … ” {sup:6}).</a>
<a href="#6" id="6">A CI should also be able to explain its answers to its human collaborator.</a>
<a href="#7" id="7">A CI that elicits its user's goals can ask how they should be prioritized, so that it can model both the user's knowledge and the user's expectations.</a>
<a href="#8" id="8">Nonetheless, the construction of a CI must contend with the fundamental differences in the way that any machine and any person function.</a>
<a href="#9" id="9">As envisioned here, a CI is not a general intelligence.</a>
<a href="#10" id="10">Each CI would target problem areas in which it could assist people, and provide representations and procedures to support particular human activities.</a>
<a href="#11" id="11">A CI should also tailor itself to the needs of the person it serves.</a>
<a href="#12" id="12">Periodically, a CI should consolidate and consider its own knowledge store.</a>
<a href="#13" id="13">Within the communication it receives, a CI will detect only agreement (e.g., “ Sure ” ), disagreement (e.g., “ nope ” ), questions (e.g., by intonation), and context-dependent instruction (e.g., “ have lunch ” will trigger a scheduling algorithm).</a>
<a href="#14" id="14">Consider, for example, the differences between the proactive CI in Fig.</a>
<a href="#15" id="15">If a user meets someone to plan a project one day and meets the same person to play tennis the next, a CI that asks for clarification (e.g., “ isn't Saturday an unusual meeting time? ” ) has the opportunity to learn a new feature value and new associations for it.</a>
<a href="#16" id="16">CI has the potential to make productive and welcome contributions in many human endeavors.</a>
<a href="#17" id="17">This CI would work at multiple levels of granularity.</a>
<a href="#18" id="18">As decisions are made, the CI could ask for further refinements (e.g., from “ in 2016 ” to “ in summer, 2016 ” to “ about a week after most European universities ’ spring terms end ” ).</a>
<a href="#19" id="19">This CI would be expected to solicit feature values from the person (e.g., height, cost) and to have prior knowledge of fundamental processes for the object's construction (e.g., permits, excavation, foundation).</a>
</body>
</html>