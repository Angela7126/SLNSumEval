<html>
<head>
<meta name="TextLength" content="SENT_NUM:20, WORD_NUM:521">
</head>
<body bgcolor="white">
<a href="#0" id="0">Effective utilization of the library of world models allows the agent to capture the transition dynamics of the new environment quickly; this should lead to a jumpstart in learning and faster convergence to a near optimal policy.</a>
<a href="#1" id="1">A new reinforcement learning algorithm, logistic regression RL (loreRL), that applies mDAGL on a CMDP to learn the transition models by automatically focusing on the relevant features in the environment; and</a>
<a href="#2" id="2">When the agent does not have any initial information about the transition dynamics of the environment in a new task, it selects “ expectations ” or views based on history that tells how well each view in the library has worked in previous tasks.</a>
<a href="#3" id="3">We address three main technical challenges in this framework: First, the transition model {a mathematical formula}T(S,A,S) is task specific, which is probably a reason why there have not been many studies that transfer the transition model.</a>
<a href="#4" id="4">Second, learning or updating a view or a transition model online in a complex and feature-rich environment is computationally expensive.</a>
<a href="#5" id="5">This way, we can apply an efficient feature selection method on a large number of state features to capture the transition dynamics, while maintaining a compact state space.</a>
<a href="#6" id="6">E is an action effect variable such that the transition function can be factored as{a mathematical formula}</a>
<a href="#7" id="7">The agent uses the feature function f to identify the relevant features, and then uses both state attributes and features to predict the action effects.</a>
<a href="#8" id="8">The remaining task is to learn {a mathematical formula}P(e|s,a)=P(e|x(s),a), where {a mathematical formula}x(s)=(s,f(s)) is a vector containing both the state attributes and the state features.</a>
<a href="#9" id="9">This prediction is based on the features of the state and the parameters {a mathematical formula} Θ ( Τ ) of the view that may be adjusted based on the actual effects observed in the task environment.</a>
<a href="#10" id="10">We denote the subset of views that specify the effects for action a by {a mathematical formula}Ta ⊂ T.</a>
<a href="#11" id="11">Our main task is to turn transition model learning into the learning of conditional distributions {a mathematical formula}P(E|s,f(s),a) using multinomial logistic regression for which attention to relevant features can be efficiently implemented online via mDAGL.</a>
<a href="#12" id="12">We also randomly select a starting state {a mathematical formula}s0.</a>
<a href="#13" id="13">At each time step, a random action a is chosen with a small probability Ε , but otherwise we calculate the optimal policy Π for an MDP with the transition model {a mathematical formula}T(W) is based on the current effect predictors.</a>
<a href="#14" id="14">In case the true transition model is representable by a sparse {a mathematical formula}W ⁎ , we would most probably converge to a near optimal policy.</a>
<a href="#15" id="15">The parameters for view learning algorithm are that {a mathematical formula} Λ =0.05, {a mathematical formula} Α =1.5.</a>
<a href="#16" id="16">loreRL is an implementation of TES equipped with the view learning algorithm that does not transfer knowledge.</a>
<a href="#17" id="17">The robot has to learn to select the relevant features based on feedback while interacting with the environment.</a>
<a href="#18" id="18">The robot does not update its knowledge of the transition model in the new environment.</a>
<a href="#19" id="19">This setting would allow us to see if TES can learn good views to transfer between similar Environment 1 and Environment 3.</a>
</body>
</html>