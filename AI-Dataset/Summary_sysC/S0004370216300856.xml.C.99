<html>
<head>
<meta name="TextLength" content="SENT_NUM:15, WORD_NUM:625">
</head>
<body bgcolor="white">
<a href="#0" id="0">Furthermore, our analysis reveals that the issue is not just the amount of data, but insufficient exposure of attack surface[45], [55] in the initial rounds which prevents the defender from collecting sufficient information about adversary responses to various strategies and learn a reliable model.</a>
<a href="#1" id="1">Here we define an attack surface as the n-dimensional feature space used to model the attacker's behavior (detailed definition in Section 8).</a>
<a href="#2" id="2">[62] experimented with three features: defender's coverage probability, adversary's reward and penalty at each target.</a>
<a href="#3" id="3">Thus, according to this model, the probability that the adversary will attack target i is given by:{a mathematical formula} where {a mathematical formula}SUia(x) is the Subjective Utility of an adversary for attacking target i when defender employs strategy x and is given by:{a mathematical formula} The vector {a mathematical formula} Ω =( Ω 1, Ω 2, Ω 3) encodes information about the adversary's behavior and each component of Ω indicates the relative importance the adversary gives to each attribute in the decision making process.</a>
<a href="#4" id="4">While researchers often use imputation and sampling techniques to fill missing data due to participant attrition [78], [36], [23], for our repeated measures study of comparing human behavior models this may result in extremely biased estimates of the modeling parameters due to the influence of the retained participants' game plays and therefore may generate biased defender strategies.</a>
<a href="#5" id="5">Using this data collected in a particular round, the defender computes her utility of playing each pure strategy k (Line 3).</a>
<a href="#6" id="6">More specifically, this utility is computed for each pure strategy as the reward that would result to the defender given the observed adversary response if the defender were playing only this pure strategy.</a>
<a href="#7" id="7">The reinforcement of playing each pure strategy is then computed as the difference between the corresponding utility and the minimum possible utility over all the pure strategies (Line 4).</a>
<a href="#8" id="8">The defender then updates her propensities of playing each pure strategy by adding the reinforcements to the propensities computed in the earlier round (Line 5).</a>
<a href="#9" id="9">Due to the differences in animal densities, if we start from a uniform mixed strategy, it would leave many of the targets of high animal density to be attacked in round 1 (as evidenced from other human subject experiments conducted in the past [66]) and that would result in a defender utility which is much lower than the cumulative utility for any of our models over five rounds (see Section 12 for details).</a>
<a href="#10" id="10">Therefore, for our experiment with the RL approach, we assumed that the defender starts with the robust Maximin strategy in round 1.</a>
<a href="#11" id="11">Given that this strategy performs worse in one round than the cumulative average expected defender utility of all the other models, it demonstrates the point that the performance of SUQR without probability weighting is worse than any of the other models that include probability weighting.Notice that the reason this SUQR pure strategy performs so poorly is that it leaves 16 out of 25 targets completely exposed, and among these targets are ones with animal densities 4 and 5.</a>
<a href="#12" id="12">This leaves cells with high rewards but very few attacks in the past rounds less protected in the subsequent round and therefore completely exposed to a lot of attacks.</a>
<a href="#13" id="13">RL based strategies for other reward structures similarly leave other targets of high value (but very attacks) with little protection and therefore open to exploitation by the attacker in the subsequent round.</a>
<a href="#14" id="14">Thus, the poor performance in this initial round of the RL model on {a mathematical formula}ADS1 and its leaving targets of high value exposed to exploitation in the subsequent round illustrates that significant new work would need to be done to adapt the proposed RL framework (based on the model by Erev and Roth [28]) for our Stackelberg Security Game setting.</a>
</body>
</html>