<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:521">
</head>
<body bgcolor="white">
<a href="#0" id="0">We then repeat one of the episodic-task experiments in a continuing version of the same task, in what we call the continuing-task experiment.</a>
<a href="#1" id="1">We make the following four observations.</a>
<a href="#2" id="2">First, we find for discount factors {a mathematical formula} Γ <1 that task performance of MDP-optimal policies during training is generally high and independent of discounting, a pattern that is quite different from that seen in the episodic setting.</a>
<a href="#3" id="3">Second, several strong correlations observed in the episodic-task grid-world experiment disappear, including the relationships between Γ and both reward positivity and task performance.</a>
<a href="#4" id="4">Third, in this investigation, the {a mathematical formula} Γ =0.99 condition with the vi-tamer algorithm is the first known instance of successful non-myopic learning from human-generated reward (i.e., with a high Γ with relatively long time steps).</a>
<a href="#5" id="5">Fourth, in two additional tests using the training data from this continuing-task experiment, we find evidence for the theoretically based conjecture that low discount rates (high Γ s) facilitate the communication of higher-level task information — e.g., the location of the goal rather than the exact sequence of steps to reach it.</a>
<a href="#6" id="6">Such higher-level information enables learning that is more robust to environmental changes, better guides action in unexperienced states, and has the potential to lead the agent to learn policies that surpass those known to the trainer.</a>
<a href="#7" id="7">In all experiments a model of human reward, {a mathematical formula}R ˆ H, is learned through the tamer framework [13], and the output of this model provides reward for the agent within an MDP specified as {a mathematical formula}{S,A,T,R ˆ H, Γ ,D}.</a>
<a href="#8" id="8">Fig.</a>
<a href="#9" id="9">2 illustrates this scenario.</a>
<a href="#10" id="10">During training, human reward signals form labels for learning samples that have state-action pairs as features; a regression algorithm continuously updates {a mathematical formula}R ˆ H with new training samples.</a>
<a href="#11" id="11">Additionally, all {a mathematical formula}R ˆ H models and all value functions are initialized to 0.</a>
<a href="#12" id="12">These analyzes in mountain car use 19 fixed {a mathematical formula}R ˆ Hs learned from the training logs created from a past experiment using tamer[13], taken from the third run of 19 trainers of the mountain car task.</a>
<a href="#13" id="13">In the episodic-task analysis described here in Section 7.2.2 — as in the off-discounting analysis in the previous Section 7.2.1 — the human reward model {a mathematical formula}R ˆ H is learned by tamer and provides predictions that are interpreted as reward by an RL algorithm.</a>
<a href="#14" id="14">But unlike the previous analysis, {a mathematical formula}R ˆ H is learned while performing reinforcement learning, and the RL algorithm — not tamer — selects actions while learning {a mathematical formula}R ˆ H.</a>
<a href="#15" id="15">Thus this experiment is on-discounting, and the human trainer will be adapting to the same algorithm, with the same Γ , that is being tested.</a>
<a href="#16" id="16">The agent, task, and experiment conform to the baseline specified in Section 6.2.</a>
<a href="#17" id="17">We observe that the results for the VI-cont condition are similar to that of the equivalent {a mathematical formula} Γ =0.99 condition in the continuing-task experiment (shown in Fig.</a>
<a href="#18" id="18">10, Fig.</a>
<a href="#19" id="19">14).</a>
<a href="#20" id="20">The performance is insignificantly higher in this experiment by Fisher's exact test of whether the agent reached the goal 10 times ({a mathematical formula}p=0.1085).</a>
</body>
</html>