<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:178">
</head>
<body bgcolor="white">
<a href="#0" id="0">For a fair comparison, one needs to look at the performance of smooth sparse coding (or standard sparse coding) when it uses lasso updates and marginal regression updates.</a>
<a href="#1" id="1">We consider the following smooth version of the sparse coding problem:{a mathematical formula}{a mathematical formula}{a mathematical formula} where {a mathematical formula} ∑ j=1nw(xj,xi)=1 for all i.</a>
<a href="#2" id="2">Alternatively, {a mathematical formula} Ρ ( ⋅ , ⋅ ) may be expressed by domain experts, learned from data before the sparse coding training, or learned jointly with the dictionary and codes during the sparse coding training.</a>
<a href="#3" id="3">We leverage the analysis for dictionary learning in the standard sparse coding setting [29] and extend it to the smooth sparse coding setting.</a>
<a href="#4" id="4">We compare the proposed smooth sparse coding algorithm, standard sparse coding with lasso [17] and marginal regression updates respectively, with a relative reconstruction error {a mathematical formula} ‖ X − D ˆ B ˆ ‖ F/ ‖ X ‖ F convergence criterion.</a>
<a href="#5" id="5">From Table 1, we see that smooth sparse coding with marginal regression takes significantly less time to achieve a fixed reconstruction error.</a>
</body>
</html>