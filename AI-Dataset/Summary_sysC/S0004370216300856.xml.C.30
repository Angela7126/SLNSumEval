<html>
<head>
<meta name="TextLength" content="SENT_NUM:21, WORD_NUM:616">
</head>
<body bgcolor="white">
<a href="#0" id="0">In our game, starting from an equal initial propensity for each pure strategy (as proposed in [28]) would result in a mixed strategy with equal coverage probability at each target.</a>
<a href="#1" id="1">Due to the differences in animal densities, if we start from a uniform mixed strategy, it would leave many of the targets of high animal density to be attacked in round 1 (as evidenced from other human subject experiments conducted in the past [66]) and that would result in a defender utility which is much lower than the cumulative utility for any of our models over five rounds (see Section 12 for details).</a>
<a href="#2" id="2">Therefore, for our experiment with the RL approach, we assumed that the defender starts with the robust Maximin strategy in round 1.</a>
<a href="#3" id="3">The use of Maximin as the initial strategy also ensures that we allow the RL model to start from the same starting point as all other models, and that it does not have an initial advantage or disadvantage compared to other models.</a>
<a href="#4" id="4">Thus, we used Maximin to compute the initial propensities for pure strategies in this setup; we used Comb Sampling on the Maximin mixed strategy [76] to compute the probability of playing each pure strategy and considered those to be the initial propensities for each pure strategy.</a>
<a href="#5" id="5">This section shows the results of our human subjects experiments on AMT, while the next section shows results against security experts, based on the experimental setting discussed in Sections 4, 5 and 6.</a>
<a href="#6" id="6">In Section 12.1 we show average defender expected utilities for five models (P-SUQR, P-BSUQR, P-RSUQR, SHARP and Maximin) against actual human subjects, for various rounds of our experiment on two payoff structures ({a mathematical formula}ADS1 and {a mathematical formula}ADS2).</a>
<a href="#7" id="7">In Section 12.2, we show results of learning the shape of the probability weighting function for the adversaries on {a mathematical formula}ADS1 and {a mathematical formula}ADS2 for three different scenarios: (i) when Gonzalez and Wu's probability weighting function Eqn.</a>
<a href="#8" id="8">Comparison with SUQR ({a mathematical formula}w1>0): As mentioned earlier in Section 7.1, we conduct additional human subjects experiments on {a mathematical formula}ADS1 to show that the performance of SUQR without probability weighting is worse than any of the other models.</a>
<a href="#9" id="9">We deployed an experiment on AMT with the defender strategy computed based on the SUQR model learned from round 1 data of {a mathematical formula}ADS1.</a>
<a href="#10" id="10">The resulting SUQR weight vector had a positive weight on coverage probability and thus resulted in a defender pure strategy.</a>
<a href="#11" id="11">The game was deployed with this strategy on AMT.</a>
<a href="#12" id="12">60 people played the game, and out of them 48 participants passed the validation test.</a>
<a href="#13" id="13">For our experimental results, we considered the data from only the participants who passed the validation test.</a>
<a href="#14" id="14">The average expected defender utility obtained was − 4.75.</a>
<a href="#15" id="15">This average expected defender utility obtained by deploying a pure strategy based on a learned SUQR model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#16" id="16">Comparison with RL based approach: We conducted human subjects experiments on {a mathematical formula}ADS1 with the RL based approach (Algorithm 1) to compare its performance against our behavioral models.</a>
<a href="#17" id="17">We deployed an experiment on AMT with the defender strategy computed based on the RL model learned from round 1 data of Maximin on {a mathematical formula}ADS1 (as explained earlier in Section 11).</a>
<a href="#18" id="18">The average expected defender utility obtained was − 4.139.</a>
<a href="#19" id="19">This average expected defender utility obtained by deploying the defender strategy based on a learned RL model is significantly less than that of all the other models on {a mathematical formula}ADS1 in Round 2 (Fig.</a>
<a href="#20" id="20">19 we show actual defender utilities obtained over 4 rounds for SHARP on {a mathematical formula}ADS3.</a>
</body>
</html>