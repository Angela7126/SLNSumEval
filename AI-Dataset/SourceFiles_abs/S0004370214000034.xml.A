<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Concept drift detection via competence models.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Learning under concept drift poses an additional challenge to existing learning algorithms. Instead of considering all the past training data, or making a stationary distribution assumption[1], [2], [3], an effective learner should be able to track these changes and quickly adapt to them [4]. Otherwise, as concept drifts, the induced pattern may not be relevant to the new data [5], [6], which may result in an increasing number of errors [7].
     </paragraph>
     <paragraph>
      The issue of concept drift refers to the change of distribution underlying the data [4], [8]. More formally, the problem can be framed as follows. If we denote the feature vector as x and the class label as y, then the data stream will be an infinite sequence of {a mathematical formula}(x,y). If the concept drifts, it means the distribution of {a mathematical formula}p(x,y) is changing between the current data chunk and the yet-to-come data. If we decompose {a mathematical formula}p(x,y) into the following two parts as {a mathematical formula}p(x,y)=p(x)×p(y|x), we could say there are two sources of concept drift: one is {a mathematical formula}p(x), which evolves with time t, and can also be written as {a mathematical formula}p(x|t), and the other is {a mathematical formula}p(y|x), the conditional probability of feature x[2].
     </paragraph>
     <paragraph>
      Concept drift can be categorized into two basic types: virtual concept drift (or drift in data distribution), and real concept drift (or drift in decision concepts) [8]. Other kinds of concept drift have also been defined and discussed; for example, based on the extent of drift, Stanley [9] mentioned three kinds of drift: sudden drift, moderate drift and slow drift. Based on class distribution, Forman [10] classified concept drift into three categories: shifting class distribution – shifting the distribution among categories but remaining stable within a given class; shifting sub-class distribution – shifting distribution sub-classes within a category, but remaining stable within a given sub-class; and fickle concept drift – individual cases may take on different ground truth labels at different times. Zhang et al. [2] defined and analyzed two kinds of concept drift in their studies: loose concept drifting, in which the genuine concepts remain relatively stable, whereas the vision of the drift is mainly caused by the biased observation of instances; and rigorous concept drifting, in which the genuine concepts undergo continuous change, although such changes can worsen in the face of biased observation. Tsymbal et al. [11] discussed a special scenario of concept drift called local concept drift, by which they mean that the change of concept or data distribution occurs only in some regions of the instance space.
     </paragraph>
     <paragraph>
      Based on the literature, many learning algorithms have been used as base models to handle concept drift. These include rule-based learning [4], [8], [12], decision trees and their incremental versions [5], [13], [14], info-fuzzy networks [15], clustering [16], support vector machines [17], and case-based reasoning (CBR) [11], [18], [19], [20]. Among them, the CBR method has three reported advantages for handling the concept drift problem [21]. First, CBR performs well with disjointed concepts. Second, CBR, as a lazy learner, is easy to update. Third, CBR allows easy sharing of knowledge for particular types of problems, making it easier to maintain multiple distributed case-bases. Therefore, this study focuses on concept drift detection for CBR.
     </paragraph>
     <paragraph>
      According to a literature review [22], the first attempt to handle concept drift with the case-based technique was IB3 [20], which discards noisy and outdated cases by monitoring each case's accuracy and retrieval frequency. IB3 has been criticized for being suitable only for gradual concept drift, and for its costly adaptation process [4]. The Locally Weighted Forgetting (LWF) algorithm [23], which reduces the weights of the k-nearest neighbors of a new case and discards a case if its weight falls below a threshold θ, was believed to be one of the best adaptive learning algorithms of its time. Klinkenberg [17] later showed in his experiments that instance weighting techniques tend to overfit the data and perform more poorly than analogous instance selection techniques. Elwell and Polikar [24] presented an ensemble learning algorithm for non-stationary environments (Learn++.NSE) that assumes data are incrementally acquired in batches. For each incoming dataset {a mathematical formula}Dt, their algorithm trains an independent base classifier {a mathematical formula}ht which is forced to emphasize misclassified data by adjusted data weighting. Then, a weight is assigned to each base classifier based on its performance on the latest dataset. The final classification result is determined by weighted majority voting of all base classifiers. Recent research and development in case-base maintenance (CBM) provides a number of methods for updating all knowledge containers [25] of a CBR system. Among them, the competence-based CBM methods [19], [26], [27], [28], [29], [30] and case-base mining technologies [31] have been empirically shown to be capable of preserving the competency of a CBR system while removing noisy and redundant cases. Few studies, however, discussed when to trigger maintenance operations, which is also an important consideration, according to Wilson and Leake's CBM framework [32]. In addition, current methods are incapable of distinguishing between noisy cases and cases representing a new concept. Knowing whether concept drift happens could help to recognize obsolete cases that conflict with current concepts and distinguish noise cases from novel cases. Moreover, developing a detection method that is able to explain where and how concept drifts could facilitate further decision capabilities and be suitable for handling local concept drift problems [11].
     </paragraph>
     <paragraph>
      Motivated by these issues, we propose a new method of concept drift detection for CBR systems, which compares the case distributions of existing cases with newly available cases. The proposed method requires no prior knowledge about the case distribution, but estimates the probability distribution and detects change via a competence model. Besides determining whether there is a concept drift, our method also quantifies and describes the detected change in terms of the competence model. To the best of our knowledge, no literature has reported any research that uses a competence model for concept drift detection purposes.
     </paragraph>
     <paragraph>
      Compared with other famous non-parametric methods, our detection method demonstrates the following advantages: 1) it can be easily adopted in multi-dimensional data while maintaining similar results in one-dimensional data; 2) it is more stable and achieves better results as shown in experiments, especially for small samples, because data can share distribution contributions among related competence areas, rather than splitting strictly by cutting edges, which makes it more tolerable to sample bias; 3) it is able to describe the detected changes by highlighting some competence areas, which is testified by a real world application.
     </paragraph>
     <paragraph>
      The novelty and main contribution of this paper lie in the endeavor to discover the difference between the inner nature of the competence group model and our proposed competence closure model. The detailed definitions and theorems afford other researchers an inside view of case-base competence, which has never been discussed in the literature. The theoretical study provided in this paper also reveals the essential differences between the two competence models, and identifies three important aspects of the competence closure model which the competence group model does not possess. Compared with our previous work on competence-based concept drift detection [33], which aims to investigate the impact of concept drift on case-base competence and assert to the possibility of detecting change via competence models, this paper additionally conducts a tremendous number of experiments to thoroughly evaluate our proposed concept drift detection approach.
     </paragraph>
     <paragraph>
      This paper is organized as follows. Section 2 discusses related works. Section 3 proposes the new competence model and discusses the relationship between our model and current competence models. Section 4 presents the competence-based change detection method. Section 5 determines the critical region and provides a statistical guarantee for the proposed detection method. Section 6 outlines the results of the experimental evaluation. Section 7 concludes this study, with a discussion of future work.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      This section formally presents the problem of concept drift detection (Section 2.1), and analyzes the pros and cons of established literature with regard to concept drift detection (Section 2.2).
     </paragraph>
     <section label="2.1">
      <section-title>
       Problem description: Concept drift detection
      </section-title>
      <paragraph>
       Concept drift detection can be formulated as follows. Suppose there is a CBR system listening to a data stream where each new observation is represented by {a mathematical formula}ci=(xi,yi), where {a mathematical formula}xi=(xi1,xi2,…,xin)∈X is the feature vector, {a mathematical formula}yi∈Y is the target label. As it is unrealistic to store the full history of the stream, we base our concept drift detection algorithm on a two-sliding-window paradigm. Both windows contain a number of successive data points. We assume data points within each window are independent random samples taken from two unknown, multi-dimensional, non-parametric distributions F and {a mathematical formula}F′, respectively. We then define the null hypothesis {a mathematical formula}H0, which asserts that F and {a mathematical formula}F′ are identical. The goal is to design a proper statistical test that is able to not only refuse {a mathematical formula}H0, if it is not true, but also highlight some local regions of the problem space where {a mathematical formula}H0 does not hold and quantify the difference between F and {a mathematical formula}F′. When {a mathematical formula}H0 is true, the probability of making an error (where the test says that F and {a mathematical formula}F′ are different when in fact they are not) should be, at most, α, where α is a user-supplied parameter.
      </paragraph>
      <paragraph>
       A real world scenario for the application of our method would be spam filtering. As is well-known, one of the challenges in the spam filtering domain is to handle concept drift problems. In a case-based spam filtering system where emails are continuously classified, the problem arises of how we can benefit from the available feedback (new cases) and improve the accuracy of the system. Treating the newest emails as an independent training set, e.g., emails received during the last month, we can detect whether there is a concept drift between our existing case-base that is assumed to follow an unknown distribution {a mathematical formula}F′, and the most recent emails, which are assumed to follow an unknown distribution {a mathematical formula}F′. The correct maintenance can accordingly be carried out when a drift has been reported.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Change detection methods
      </section-title>
      <paragraph>
       The most popular trigger technique for learner adaptivity is change detection, which is often implicitly related to a sudden drift [34]. This is usually conducted by a statistical test that monitors the raw data distribution [35], [36], [37], [38], the outputs (error) of learners [39], [40], [41], [42], or the parameters of the learners [43].
      </paragraph>
      <section label="2.2.1">
       <section-title>
        Detecting concept drift by data distribution
       </section-title>
       <paragraph>
        When comparing two samples and determining whether these samples are drawn from the same distribution, the Wilcoxon test [44] and the Kolmogorov–Smirnov test [45], [46] are the most famous non-parametric methods. We do not assume that the data follows any particular parametric distribution (Section 2.1), since in real world applications, the data that one typically encounters may not arise from any standard distribution, which makes non-parametric tests more practical. However, the Wilcoxon test and the Kolmogorov–Smirnov test are initially designed for data with only one dimension and cannot be easily extended to multi-dimensional data, which limits their scalability [36]. In this sense, we intentionally omit methods that are designed for one-dimensional data [37], [38].
       </paragraph>
       <paragraph>
        Kifer, Ben-David and Gehrke [35] proposed a modification of the Kolmogorov–Smirnov test that, in principle, compares the cumulative distribution functions of two samples with all possible orderings and takes the largest resulting test statistics. They employed a notation of {an inline-figure}-distance as their test statistic, which in fact is a relaxation of the total variation distance. Their method reported several advantages, including being able to control the rate of false alarm (false positive) and missed detection (false negative), and to describe and quantify the detected change. Some technical challenges remain, however, which need to be overcome before putting their work into practice, such as how to determine an interesting class of sets {an inline-figure} in higher dimensions.
       </paragraph>
       <paragraph>
        Dasu et al. [36] suggested an information-theoretic approach for change detection in data streams, which resorts to the Kullback–Leibler divergence to measure the difference between two given distributions. They further estimated whether their measurement is statistically significant through the Percentile Bootstrap method [47]. By partitioning the problem space using a kdq-tree, their method also exhibited the capability of identifying the regions of greatest difference. However, the kdq-tree does not guarantee that a partition will coincide with the real interesting concepts. This means that the detected regions may not be easily explained and understood.
       </paragraph>
      </section>
      <section label="2.2.2">
       <section-title>
        Detecting concept drift by learner outputs
       </section-title>
       <paragraph>
        Gama et al. [39] presented a Drift Detection Method (DDM) that traces and controls the online error-rate of the learning algorithm. Treating the error of a set of examples as a random variable from Bernoulli trails, the probability for the number of errors in a sample of n examples can be generalized as Binomial distribution. A significant increase in the error of the algorithm suggests that the class distribution is changing. Their method declares a new concept if the error reaches the warning level, and a new model is learnt when the drift level is exceeded. Although being independent of the learning algorithm, their method is more suitable for rebuilding models rather than updating an existing model, since it assesses a learner through its overall error rate. In addition, their method is criticized for having difficulties when the change is slowly gradual [40].
       </paragraph>
       <paragraph>
        Baena-García et al. [40] proposed the Early Drift Detection Method (EDDM) to improve the detection in the presence of gradual concept drift. Their EDDM, which is different to DDM, considers the distance between two consecutive erroneous classifications instead of the error rate. They assume that a significant decrease in the distance suggests that the concept is changing. With the calculated average distance between two errors ({a mathematical formula}pi′) and its standard deviation ({a mathematical formula}si′), they defined two thresholds α for a warning level and β for a drift level. When {a mathematical formula}β⩽(pi′+2×si′)(pmax′+2×smax′)&lt;α, where {a mathematical formula}pmax′ and {a mathematical formula}smax′ are stored value when {a mathematical formula}pi′+2×si′ reaches its maximum, the examples will be stored in case of a possible change of context; when {a mathematical formula}(pi′+2×si′)(pmax′+2×smax′)&lt;β, a new model is learnt using the examples stored since the warning level was triggered. The values for {a mathematical formula}pmax′ and {a mathematical formula}smax′ are reset. ‘EDDM performs well for gradual changes; however, it is not good at detecting drift in noisy examples’ [48].
       </paragraph>
       <paragraph>
        Yasumura, Kitani and Uehara [41] developed a sensitive detection method for concept drift that measures the number of instances classified differently by two successive ensemble classifiers and determines whether the difference is significant. To suppress the influence caused by noise, they weighted the instances by taking the inverse weights generated by the AdaBoost algorithm [49], but their method requires building a new classifier each time a new chunk arrives. This is apparently not suitable for all non-ensemble based algorithms to handle concept drift.
       </paragraph>
       <paragraph>
        Li et al. [42] also suggested tracking the error rate of classification in a chunked data stream. Considering the observed error rate of the latest data chunk {a mathematical formula}e¯f as a historical classification result, they fitted the estimated error rate of current chunk {a mathematical formula}e¯s into Hoeffding's inequality to provide a statistical guarantee for the detection. Nevertheless, they still experienced the same problem as occurred in Gama's work [39].
       </paragraph>
      </section>
      <section label="2.2.3">
       <section-title>
        Detecting concept drift by parameters
       </section-title>
       <paragraph>
        To the best of our knowledge, the only attempt to model concept drift as a change of parameters was made by Su, Shen and Xu [43]. In their framework, a dynamic probabilistic model is framed as {a mathematical formula}p(Ck|xt)=f(wt)+v, where {a mathematical formula}wt is an optimal parameter vector inferred continuously by the extended Kalman filter [50]; and v is a random variable that represents the uncertainty in the posterior distribution {a mathematical formula}p(Ck|xt). Assuming the expectation of parameter {a mathematical formula}wt−1 will be the same as {a mathematical formula}wt when there is no concept drift, they model the concept drift as a change of parameter vector {a mathematical formula}wt=wt−1+s, where s is the uncertainty caused by concept drift. For simplicity, they assume that s and v follow zero mean normal distributions with isotropic covariance, {a mathematical formula}s∼N(0,aI), {a mathematical formula}v∼N(0,r), where I is the identity matrix and a is a single value which controls the variance of the parameter vector w; r is the noise variance. In the implementation of their model, the degree of concept drift aI and the noise variance r need to be estimated from data.
       </paragraph>
       <paragraph>
        Their framework for modelling concept drift is creative and can be easily applied to many learning models, such as Support Vector Machines (SVM), Regression or Artificial Neural Networks (ANN), however, it is not suitable for a knowledge-based learner like CBR. In addition, an optimal parameter vector may not be organized in a user-understandable way, which prohibits concept drift interpretation.
       </paragraph>
       <paragraph>
        In this section, we have summarized existing concept drift detection methods into three categories. As shown in later sections, our proposed method will belong to the first category. There is also research on learning methods in outlier and anomaly detection [51], [52]. The difference here is that outlier detection is concerned with finding exceptional observations, whereas concept drift detection deals with identifying a shift in the underlying data distribution [53].
       </paragraph>
      </section>
     </section>
    </section>
    <section label="3">
     <section-title>
      A new competence model
     </section-title>
     <paragraph>
      This study aims to provide an innovative solution for concept drift detection, which compares the data distribution through competence measurement instead of the feature space.
     </paragraph>
     <paragraph>
      Competence is a measurement of how well a CBR system fulfills its goals. As CBR is a problem-solving methodology, competence is usually taken to be the proportion of problems at hand that can be solved successfully [54]. Because the competence measures the problem-solving capabilities of a CBR system, the probability distribution change of its cases should also reflect upon its competence. This inspired our research to detect concept drift through a competence model. The key idea is to measure the distribution change of cases with regard to their competencies instead of their real distributions.
     </paragraph>
     <paragraph label="Definition 1">
      Smyth and McKenna [26], [28], [55] proposed a series of models to measure the competence of a CBR system. The definitions are shown as follows. (See [26].) For a case base {a mathematical formula}CB={c1,c2,…,cn}, given a case {a mathematical formula}c∈CB, {a mathematical formula}CoverageSet(c)={c′∈CB:Solves(c,c′)}, where {a mathematical formula}Solves(c,c′) means that c can be retrieved and adapted to solve {a mathematical formula}c′.
     </paragraph>
     <paragraph label="Definition 2">
      (See [26].) {a mathematical formula}ReachibilitySet(c)={c′∈CB:Solves(c′,c)}.
     </paragraph>
     <paragraph label="Definition 3">
      (See [55].) {a mathematical formula}RelatedSet(c)=CoverageSet(c)∪ReachibilitySet(c).
     </paragraph>
     <paragraph label="Definition 4">
      (See [55].) For {a mathematical formula}c1,c2∈CB, {a mathematical formula}SharedCoverage(c1,c2) iff{a mathematical formula}
     </paragraph>
     <paragraph label="Definition 5">
      (See [55].) Let {a mathematical formula}G={c1,c2,…,cm}⊆CB, we say that G has the property {a mathematical formula}CompetenceGroup(G) iff for any {a mathematical formula}ci∈G, there exists {a mathematical formula}cj∈G−{ci} so that {a mathematical formula}SharedCoverage(ci,cj) holds; and for any {a mathematical formula}ck∈CB−G, there does not exist {a mathematical formula}cl∈G, so that {a mathematical formula}SharedCoverage(ck,cl) holds.
     </paragraph>
     <paragraph>
      In their competence model, coverage of a case is the set of problems that this case can solve; conversely, reachability is the set of all cases that can solve this case. A cluster of cases, called a competence group, is formed using their reachability and coverage sets. Based on these definitions, we put forward the following propositions:
     </paragraph>
     <paragraph label="Proposition 1">
      For any{a mathematical formula}G⊆CBif G has property{a mathematical formula}CompetenceGroup(G)then{a mathematical formula}|G|&gt;1.
     </paragraph>
     <paragraph label="Proof">
      Obvious.  □
     </paragraph>
     <paragraph label="Proposition 2">
      This makes it difficult for the competence group model to measure and detect noise cases that are distinguished from their close neighbors; thus noise cases tend to solve and be solved only by themselves. Given{a mathematical formula}G1,G2⊆CB, if{a mathematical formula}G1and{a mathematical formula}G2are with property{a mathematical formula}CompetenceGroup(G1)and{a mathematical formula}CompetenceGroup(G2)respectively, then{a mathematical formula}G1∪G2has property{a mathematical formula}CompetenceGroup(G1∪G2).
     </paragraph>
     <paragraph label="Proof">
      For any {a mathematical formula}ci∈G1∪G2, {a mathematical formula}ci∈G1 or {a mathematical formula}ci∈G2. Without loss of generality, we suppose {a mathematical formula}ci∈G1. Since {a mathematical formula}G1 has property {a mathematical formula}CompetenceGroup(G1), there must exist {a mathematical formula}ck∈G1⊆G1∪G2 and {a mathematical formula}ck≠ci, so that {a mathematical formula}SharedCoverage(ck,ci) holds. And for any {a mathematical formula}cj∈CB−{G1∪G2}⊆CB−G1, there does not exist {a mathematical formula}cm∈G1, so that {a mathematical formula}SharedCoverage(cj,cm) holds. If there exists {a mathematical formula}cm∈G2 and {a mathematical formula}cj∈CB−{G1∪G2}⊆CB−G2, we have a contradiction where {a mathematical formula}G2 has property {a mathematical formula}CompetenceGroup(G2). Therefore {a mathematical formula}G1∪G2 has property {a mathematical formula}CompetenceGroup(G1∪G2).  □
     </paragraph>
     <paragraph label="Proposition 3">
      Given{a mathematical formula}G1,G2⊆CB,{a mathematical formula}G1∩G2≠ϕ, if{a mathematical formula}G1and{a mathematical formula}G2are with property{a mathematical formula}CompetenceGroup(G1)and{a mathematical formula}CompetenceGroup(G2)respectively, then{a mathematical formula}G1∩G2has property{a mathematical formula}CompetenceGroup(G1∩G2).
     </paragraph>
     <paragraph label="Proof">
      For any {a mathematical formula}ci∈G1∩G2, {a mathematical formula}ci∈G1 and {a mathematical formula}ci∈G2. Since {a mathematical formula}G1 has property {a mathematical formula}CompetenceGroup(G1), then there must exist {a mathematical formula}ck∈G1 and {a mathematical formula}ck≠ci, so that {a mathematical formula}SharedCoverage(ck,ci) holds. If {a mathematical formula}ck∉G2, as {a mathematical formula}ci∈G2, we have a contradiction where {a mathematical formula}G2 has property {a mathematical formula}CompetenceGroup(G2). For any {a mathematical formula}cj∈CB−G1∩G2, {a mathematical formula}cj∈CB−G1 or {a mathematical formula}cj∈CB−G2. Without loss of generality, we suppose {a mathematical formula}cj∈CB−G1. Since {a mathematical formula}G1 has property {a mathematical formula}CompetenceGroup(G1), then does not exist {a mathematical formula}cm∈G1∩G2⊆G1, so that {a mathematical formula}SharedCoverage(cj,cm) holds. Therefore {a mathematical formula}G1∩G2 has property {a mathematical formula}CompetenceGroup(G1∩G2).  □
     </paragraph>
     <paragraph>
      Through Proposition 2 and Proposition 3, we prove that the competence group model cannot fulfill the claim that “competence groups individually make an independent contribution to global competence” [56]. To be precise, the competence group model provides a dichotomous partition of the case-base, G and {a mathematical formula}CB−G, so that cases of each partition together make a collectively independent contribution to overall case-base competence. The competence group model is useful for analyzing a competence independent sub-set of the case-base, but it is still inadequate and deficient. First, the competence group model does not guarantee a complete splitting of the case-base into several competence groups (inadequacy); in other words, {a mathematical formula}CB−G is not necessarily a competence group, e.g., it contains a disjointed case that is not related to any case. Second, although each competence group may be further split to find more independent partitions, which may finally lead to several smaller independent competence groups, the competence group model does not guarantee that any two competence groups are mutually independent (deficiency). As a result, the competence group model is inappropriate for analyzing the competence of the case-base, which may be composed of several disjointed partitions.
     </paragraph>
     <paragraph label="Definition 6">
      We therefore propose two new competence models – Competence Closure and Related Closure[33], because current competence models are not sufficient for concept drift detection purposes, although, with existing competence models, one can transfer the infinite case domain into a finite domain of related sets, which solves one difficulty of measuring the statistical distance between two case samples. We will demonstrate how our models are superior to existing models by the comparisons shown later in this section. We intentionally omit other proposed competence models such as the LiabilitySet[19] and the Complexity model [29], [30], since they do not provide a measurement of what is solved in the problem space, but only how well or how surely the problems are solved. (See [33].) For {a mathematical formula}G={c1,c2,…,cm}⊆CB, {a mathematical formula}G≠ϕ, we say that G has the property {a mathematical formula}CompetenceClosure(G) if, and only if, for any {a mathematical formula}ci,cj∈G, {a mathematical formula}ci≠cj, there exist {a mathematical formula}{ci1,ci2,…,cik}⊆G so that {a mathematical formula}SharedCoverage(cij,cij+1)(j=0,…,k) holds, where {a mathematical formula}ci=ci0, {a mathematical formula}cj=cik+1, and for any {a mathematical formula}ck∈CB−G, there does not exist {a mathematical formula}cl∈G, so that {a mathematical formula}SharedCoverage(ck,cl) holds.
     </paragraph>
     <paragraph>
      In other words a competence closure is a group of cases that can connect to one another through a series of SharedCoverage relations, while a case not in this competence closure cannot have a SharedCoverage relation with cases in this competence closure.
     </paragraph>
     <paragraph label="Proposition 4">
      For{a mathematical formula}G∈CB,{a mathematical formula}|G|&gt;1, if G has property{a mathematical formula}CompetenceClosure(G)then G has property{a mathematical formula}CompetenceGroup(G).
     </paragraph>
     <paragraph label="Proof">
      Obvious.  □
     </paragraph>
     <paragraph label="Remark">
      In Proposition 4, the condition of {a mathematical formula}|G|&gt;1 is essential. When {a mathematical formula}|G|=1, G is a single case set {a mathematical formula}G={c1}, which has property {a mathematical formula}CompetenceClosure(G) if there does not exist {a mathematical formula}ck∈CB−G, so that {a mathematical formula}SharedCoverage(c1,ck) holds. However, G cannot have property {a mathematical formula}CompetenceGroup(G), since we cannot find an element other than {a mathematical formula}c1 in G.
     </paragraph>
     <paragraph label="Theorem 1">
      Proposition 4 shows that the competence closure model is fully compatible with the competence group model because any competence closure with at least two cases also has the CompetenceGroup property, and is also a competence group. A single case point can never be modeled by the competence group model. Proposition 4 ensures that any method that adopts the competence group model can also be fitted with the competence closure model. For{a mathematical formula}G1,G2,…,Gn⊆CB,{a mathematical formula}|Gi|&gt;1, if{a mathematical formula}Gihas property{a mathematical formula}CompetenceClosure(Gi)then{a mathematical formula}⋃i=1nGihas property{a mathematical formula}CompetenceGroup(⋃i=1nGi).
     </paragraph>
     <paragraph label="Proof">
      As for Proposition 2 and Proposition 4.  □
     </paragraph>
     <paragraph label="Theorem 2">
      For{a mathematical formula}G1,G2⊆CB,{a mathematical formula}G1≠G2,{a mathematical formula}ifG1and{a mathematical formula}G2have properties{a mathematical formula}CompetenceClosure(G1)and{a mathematical formula}CompetenceClosure(G2)then{a mathematical formula}G1∩G2=ϕ.
     </paragraph>
     <paragraph label="Proof">
      As {a mathematical formula}G1≠G2, without loss of generality, we suppose there exists {a mathematical formula}c1∈G1 and {a mathematical formula}c1∉G2. If {a mathematical formula}G1∩G2≠ϕ, then let {a mathematical formula}c0∈G1∩G2⊂G1. Since {a mathematical formula}G1 has property {a mathematical formula}CompetenceClosure(G1), then there exists {a mathematical formula}{ci1,ci2,…,cik}⊆G1, so that {a mathematical formula}SharedCoverage(cij,cij+1)(j=0,…,k) holds, where {a mathematical formula}c0=ci0, {a mathematical formula}c1=cik+1. As {a mathematical formula}c1∈G1 and {a mathematical formula}c1∉G2 and {a mathematical formula}c0∈G2, there must exist {a mathematical formula}cim, {a mathematical formula}cim+1(0⩽m⩽k), {a mathematical formula}cim∈G2, {a mathematical formula}cim+1∉G2, {a mathematical formula}cim+1∈G1, so that {a mathematical formula}SharedCoverage(cim,cim+1) holds. However, this conflicts with the condition of {a mathematical formula}G2, which has the property {a mathematical formula}CompetenceClosure(G2).  □
     </paragraph>
     <paragraph>
      Compared with the competence group model, our competence closure model does not permit sharing competence coverage between competence closures and truly makes an independent contribution to global competence. We claim that this is an important characteristic because partitioning the space independently assists in the estimation of empirical probability, e.g., {a mathematical formula}P(A)+P(B)=P(A∪B), when A and B are mutually exclusive.
     </paragraph>
     <paragraph label="Corollary 1">
      For{a mathematical formula}G1⊆CB, if{a mathematical formula}G1has property{a mathematical formula}CompetenceClosure(G1)then there does not exist{a mathematical formula}G2⊆CB,{a mathematical formula}G1⊂G2so that{a mathematical formula}G2has property{a mathematical formula}CompetenceClosure(G2).
     </paragraph>
     <paragraph label="Proof">
      As for Theorem 2.  □
     </paragraph>
     <paragraph>
      Corollary 1 proves the later claim that ‘a competence closure is a maximal set of cases, which are competence related’.
     </paragraph>
     <paragraph label="Proposition 5">
      For{a mathematical formula}G⊆CB,{a mathematical formula}G1⊆G, G has property{a mathematical formula}CompetenceGroup(G)and{a mathematical formula}G1has property{a mathematical formula}CompetenceClosure(G1). Let{a mathematical formula}G2=G−G1, if{a mathematical formula}G2≠ϕ, then{a mathematical formula}G2has property{a mathematical formula}CompetenceGroup(G2).
     </paragraph>
     <paragraph label="Proof">
      For any {a mathematical formula}ci∈G2=G−G1, as G has property {a mathematical formula}CompetenceGroup(G), there exists {a mathematical formula}cj∈G, so that {a mathematical formula}SharedCoverage(ci,cj) holds. As {a mathematical formula}G=G1∪G2 and {a mathematical formula}G1∩G2=ϕ, we have {a mathematical formula}cj∈G1 or {a mathematical formula}cj∈G2. If {a mathematical formula}cj∈G1, this conflicts with {a mathematical formula}G1, which has property {a mathematical formula}CompetenceClosure(G1). Therefore, {a mathematical formula}cj∈G2, that is for any {a mathematical formula}ci∈G2, there exists {a mathematical formula}cj∈G2, so that {a mathematical formula}SharedCoverage(ci,cj) holds.For any {a mathematical formula}cj∈CB−G2=(CB−G)∪G1, {a mathematical formula}cj∈CB−G or {a mathematical formula}cj∈G1. If {a mathematical formula}cj∈CB−G, since G has property {a mathematical formula}CompetenceGroup(G), there does not exist {a mathematical formula}cm∈G2⊂G, so that {a mathematical formula}SharedCoverage(cj,cm) holds. Or, if {a mathematical formula}cj∈G1, there exists {a mathematical formula}cm∈G2=G−G1, so that {a mathematical formula}SharedCoverage(cj,cm) holds. This will conflict with {a mathematical formula}G1, which has property {a mathematical formula}CompetenceClosure(G1).  □
     </paragraph>
     <paragraph label="Theorem 3">
      For{a mathematical formula}G⊆CB, if G has property{a mathematical formula}CompetenceGroup(G)and G does not have property{a mathematical formula}CompetenceClosure(G), then there exists{a mathematical formula}⋃i=1nGi=G, so that{a mathematical formula}Gihas property{a mathematical formula}CompetenceClosure(Gi).
     </paragraph>
     <paragraph label="Proof">
      For any {a mathematical formula}ci∈G, since G has property {a mathematical formula}CompetenceGroup(G), but does not have property {a mathematical formula}CompetenceClosure(G), then there must exist {a mathematical formula}cj∈G, so that we cannot find {a mathematical formula}{ci1,ci2,…,cik}⊆G so that {a mathematical formula}SharedCoverage(cij,cij+1)(j=0,…,k) holds, where {a mathematical formula}ci=ci0, {a mathematical formula}cj=cik+1. We continuously remove {a mathematical formula}cj from G until we cannot find a {a mathematical formula}cj, to construct a {a mathematical formula}G1⊂G. Since G has property {a mathematical formula}CompetenceGroup(G), there at least exist {a mathematical formula}ci,ck∈G1, so that {a mathematical formula}SharedCoverage(ci,ck) holds, and we have {a mathematical formula}G1≠ϕ. Then, for any {a mathematical formula}cm∉G1, if there exists {a mathematical formula}cl∈G1, so that {a mathematical formula}SharedCoverage(cm,cl) holds, then for any {a mathematical formula}ci∈G1 we can find {a mathematical formula}{ci1,ci2,…,cik}⊆G1 so that {a mathematical formula}SharedCoverage(cij,cij+1)(j=0,…,k) holds where {a mathematical formula}cm=ci0, {a mathematical formula}cl=ci1,ci=cik+1. Therefore, {a mathematical formula}cm∈G1, which conflicts with {a mathematical formula}cm∉G1. So, for any {a mathematical formula}cm∉G1, there does not exist {a mathematical formula}cl∈G1, so that {a mathematical formula}SharedCoverage(cm,cl) holds. We then conclude {a mathematical formula}G1⊂G, and {a mathematical formula}G1 has property {a mathematical formula}CompetenceClosure(G1). Let {a mathematical formula}G′=G−G1, according to Proposition 5, {a mathematical formula}G′ has property {a mathematical formula}CompetenceGroup(G′) if {a mathematical formula}G′≠ϕ. We can construct {a mathematical formula}Gm+1 continuously from {a mathematical formula}G′=G−⋃i=1mGi, {a mathematical formula}m=1,2,…,n−1, until {a mathematical formula}Gm+1=G−⋃i=1mGi, and {a mathematical formula}Gm+1 has property {a mathematical formula}CompetenceClosure(Gm+1).  □
     </paragraph>
     <paragraph>
      As the entire case-base can be viewed as a competence group, Theorem 3 certifies that any case-base can be split into a set of competence closures.
     </paragraph>
     <paragraph>
      To sum up, the competence closure model outweighs the competence group model because of its features, as follows:
     </paragraph>
     <paragraph>
      First, the competence closure model is able to uniquely model the entire case-base. Considering the whole case-base as a competence group, according to Theorem 3, the case-base can be modeled into a set of competence closures. A suspicious case, which differs from its neighbors and solves nothing and can only be solved by itself, cannot be modeled by the competence group model. This is important because suspicious cases are more likely to behave like noisy or novel cases [57].
     </paragraph>
     <paragraph>
      Second, the competence closure model provides a smaller granularity than the competence group model (Theorem 1 and Theorem 3). This is essential for competence-guided case discovery [56], in which the competence holes are believed to exist between neighbor competence groups.
     </paragraph>
     <paragraph>
      Third, competence closure is the maximum set concerning a series of related problems (Corollary 1) and two competence closures are said to be independent in terms of the related set (Theorem 2). This facilitates the narrowing of the analysis within any interesting sub-problem space represented by one or more competence closures.
     </paragraph>
     <paragraph>
      We also show the differences between our competence closure model and the existing competence group model in Fig. 1. It can be seen that the competence closure is defined as the maximal set of cases linked through their related sets, where a competence group is the union of competence closures or any competence closure with more than one case.
     </paragraph>
     <paragraph>
      In Smyth and McKenna's [26], [28], [55] competence model, the competence contributed by a certain case is modeled by its coverage set (Definition 1). Meanwhile, the existence of a case could also be a support for those cases that solve it: in other words, its reachability set (Definition 2). The related set provides a measurement of related competence for each single case. Intuitively, the related set highlights a set of interesting target problems related to a case. However, since the related set overlaps, the related set of a certain case may not be the only measurement of problem space that relates to this case. In this sense, we propose another competence model – related closure (Definition 7) to represent the problem space with relation to a certain case or a group of cases, in terms of related sets.
     </paragraph>
     <paragraph label="Definition 7">
      (See [33].) For {a mathematical formula}c∈CB, denote the {a mathematical formula}RelatedSet(c) with regard to CB as {a mathematical formula}RCB(c), and we define the Related Closure of c with regard to CB as{a mathematical formula} For a group of cases {a mathematical formula}S⊆CB, we define the Related Closure of S with regard to CB as{a mathematical formula}
     </paragraph>
     <paragraph label="Example 1">
      Let {a mathematical formula}CB={c1,c2,c3,c4}, {a mathematical formula}RCB(c1)={c1,c2}, {a mathematical formula}RCB(c2)={c1,c2,c3,c4}, {a mathematical formula}RCB(c3)=RCB(c4)={c2,c3,c4}. The related closure of {a mathematical formula}c3 is the set of all related sets, with regard to CB, which contain the case {a mathematical formula}c3. That is {a mathematical formula}ℜCB(c3)={{c1,c2,c3,c4},{c2,c3,c4}}.
     </paragraph>
     <paragraph>
      We have now proposed a new competence model. We have not only discussed in more detail the properties of Smyth and McKenna's [26], [28], [55] competence model, but we have also shown, theoretically, how our competence model is more suitable for change detection purposes. In the following section, we introduce a metric that measures the distance between case chunks, and based on this metric, we develop our change detection method.
     </paragraph>
    </section>
    <section label="4">
     <section-title>
      Competence-base empirical distance
     </section-title>
     <paragraph>
      When mining concept drifting data, a common assumption is that the up-to-date data chunk and the yet-to-come data chunk share identical, or considerably close distributions [1]. This means that the newly available cases represent the concept that we may be interested in, in the future. In CBR, considering cases in the existing case base and the newly available cases as two samples drawn from two probability distributions, we are able to identify whether there is a concept drift by detecting a possible distribution change between the existing case base and the newly available case chunk. This section proposes a weighted approach to measure the difference between case chunks that weight the related sets differently, according to the distribution of cases. The competence-based empirical distance between two case chunks is defined through those weights.
     </paragraph>
     <paragraph>
      Given a case base CB, and two case sample sets {a mathematical formula}S1,S2⊆CB, we obtain two related closures, {a mathematical formula}ℜCB(S1) and {a mathematical formula}ℜCB(S2). Intuitively, we measure the difference between {a mathematical formula}ℜCB(S1) and {a mathematical formula}ℜCB(S2) as the distance between {a mathematical formula}S1 and {a mathematical formula}S2. However, it will only represent the distance between the competencies covered by these two samples. The relative distribution discrepancy within the competence is missing. This introduces a problem when we compare two case samples that solve similar problems, but with dramatically different distributions. To address this problem, we assign a weight for each element in {a mathematical formula}ℜCB(S1) and {a mathematical formula}ℜCB(S2) to represent the relative density of the cases distributed over their related closures.
     </paragraph>
     <paragraph label="Definition 8">
      Let {a mathematical formula}ℜCB(S)={r1CB(S),r2CB(S),…,rnCB(S)}, {a mathematical formula}ℜiCB(S)={riCB(S)}, we define the density of {a mathematical formula}riCB(S) with regard to S as{a mathematical formula}
     </paragraph>
     <paragraph label="Example 2">
      Let {a mathematical formula}S={c1,c4} be a case sample set taken from the case base in Example 1. The related closure of S is the set of all related sets that contain at least one element in S. We have{a mathematical formula} thus {a mathematical formula}ℜCB(S)={{c1,c2},{c2,c3,c4},{c1,c2,c3,c4}}. Let {a mathematical formula}ℜ1CB(S)={c1,c2}, the weight of {a mathematical formula}{c1,c2} with regard to S is{a mathematical formula}
     </paragraph>
     <paragraph label="Theorem 4">
      The sum of the densities of all elements in{a mathematical formula}ℜCB(S)equals 1.{a mathematical formula}
     </paragraph>
     <paragraph label="Proof">
      Substitute Eq. (4) into Eq. (5), we have the left side as:{a mathematical formula}As {a mathematical formula}ℜiCB(S)∩ℜjCB(S)=ϕ(i≠j), {a mathematical formula}ℜCB(S)=⋃i=1|ℜCB(S)|ℜiCB(S) and according to the definition of related closure (Definition 4), {a mathematical formula}ℜCB(cj)⊆ℜCB(S), therefore we have:{a mathematical formula}Substitute Eq. (7) into Eq. (6); the left side equals the right side.  □
     </paragraph>
     <paragraph>
      From a practical point of view, this means that all cases in sample S are equally important with regard to the contribution to the density of elements in {a mathematical formula}ℜCB(S), no matter what their related sets are.
     </paragraph>
     <paragraph label="Definition 9">
      The density weights each related set in a related closure by the degree to which the sample cases are distributed. We then define the competence-based empirical weight of a case sample with regard to a certain interesting sub-problem space (Definition 9). Given a case base CB, and a case sample set {a mathematical formula}S⊆CB, denote the power set of {a mathematical formula}ℜCB(CB) as {a mathematical formula}℘(ℜCB(CB)). Considering {a mathematical formula}℘(ℜCB(CB)) as the measurable space {an inline-figure}, for {a mathematical formula}A∈{an inline-figure}, we define the competence-based empirical weight of S with regard to A over CB as{a mathematical formula}
     </paragraph>
     <paragraph>
      For any case sample set S, the competence-based empirical weight provides a reference to the degree of case distribution on a competence area represented by A – a sub-set of {a mathematical formula}ℜCB(CB). The higher the weight is, the larger is the proportion of cases in S that support the selected competence area.
     </paragraph>
     <paragraph label="Definition 10">
      For two case sample sets {a mathematical formula}S1,S2⊆CB, we define the competence-based empirical distance between {a mathematical formula}S1 and {a mathematical formula}S2 as{a mathematical formula}
     </paragraph>
     <paragraph label="Proposition 6">
      Given a case base of finite size CB and a case sample set{a mathematical formula}S⊆CB,{a mathematical formula}ℜCB(S)is a finite set.
     </paragraph>
     <paragraph label="Proof">
      Since each case c in CB corresponds to a related set – {a mathematical formula}RCB(c) (whereas several cases may correspond to the same related set) the size of {a mathematical formula}ℜCB(CB) must be less, or equal to, the size of the case base. Again, according to the definition of related closure (Definition 7), for any case sample set {a mathematical formula}S⊆CB, {a mathematical formula}ℜCB(S)⊆ℜCB(CB), we have {a mathematical formula}|ℜCB(S)|⩽|ℜCB(CB)|⩽|CB|. As the size of the case base is finite, {a mathematical formula}ℜCB(S) must be a finite set.  □
     </paragraph>
     <paragraph label="Remark">
      This ensures that a solution {a mathematical formula}A∈{an inline-figure} exists in the defined competence-based empirical distance, since {an inline-figure} is also a finite a set.
     </paragraph>
     <paragraph label="Theorem 5">
      Given a case base of finite size CB, the competence-based empirical distance in the case sample{a mathematical formula}S⊆CB, is a normalized quasi-distance function{a mathematical formula}dCB:S×S→[0,1], which satisfies the following conditions for all{a mathematical formula}S1,S2,S3⊆CB.
     </paragraph>
     <list>
      <list-item label="1.">
       {a mathematical formula}0⩽dCB(S1,S2)⩽1
      </list-item>
      <list-item label="2.">
       {a mathematical formula}dCB(S1,S2)=0if{a mathematical formula}S1=S2
      </list-item>
      <list-item label="3.">
       {a mathematical formula}dCB(S1,S2)=dCB(S2,S1)
      </list-item>
      <list-item label="4.">
       {a mathematical formula}dCB(S1,S3)⩽dCB(S1,S2)+dCB(S2,S3).
      </list-item>
     </list>
     <paragraph label="Proof">
      Condition 1, 2 and 3 are obvious. For condition 4, assume there exists {a mathematical formula}A1∈{an inline-figure} where, for any {a mathematical formula}Ai∈{an inline-figure}, we have{a mathematical formula}We say {a mathematical formula}dCB(S1,S3)=2×|S1CB(A1)−S3CB(A1)| (the existence of {a mathematical formula}A1 can be proven through Proposition 6). Again, we have {a mathematical formula}A2,A3∈{an inline-figure}, that for any {a mathematical formula}Ai∈{an inline-figure}{a mathematical formula}{a mathematical formula}Clearly we have {a mathematical formula}dCB(S1,S2)⩾2×|S1CB(A1)−S2CB(A1)| and {a mathematical formula}dCB(S2,S3)⩾2×|S2CB(A1)−S3CB(A1)|, so we have{a mathematical formula} □
     </paragraph>
     <paragraph>
      Note that {a mathematical formula}S1=S2 is only a sufficient condition for {a mathematical formula}dCB(S1,S2)=0, since the competence-based empirical distance compares the distance between two case sets through their competencies, rather than their real distribution. Any pair of case sets that exhibit identical distribution with regard to the competence will result in a distance of zero. This can be easily proven by constructing sample sets with paired cases, which are different but of the same related set.
     </paragraph>
     <paragraph>
      We say that there is a concept drift when the competence-based empirical distance between the current case base and the newly available case chunk is greater than ε. Similar to Kifer, Ben-David and Gehrke's work [35], the set A depicts a local competence area, in which the largest distribution discrepancy lies between two samples, which helps to explain the detected change. The determination of ε and the statistical guarantee of the detection method will be discussed in Section 5.
     </paragraph>
    </section>
    <section label="5">
     <section-title>
      Statistical guarantee
     </section-title>
     <paragraph>
      The choice of distance function used to determine change is only one aspect of the concept drift detection method; another is to provide statistical significance of the detected change. Given an observation of two case samples, we achieve this by answering the question “How likely is it that the observation could have been obtained under the null hypothesis {a mathematical formula}H0 (in our case, that no concept drift occurs)?”. The smaller this value (the so-called “p-value”), the stronger the evidence against {a mathematical formula}H0.
     </paragraph>
     <paragraph>
      This work resorts to the two-sample non-parametric permutation test method [47] to provide a statistical guarantee for the detected change. The permutation test is easy to implement and free of mathematical assumptions, and is commonly used when the theoretical distribution of the test statistic is complicated or unknown, which suits our situation.
     </paragraph>
     <section label="5.1">
      <section-title>
       Permutation test
      </section-title>
      <paragraph>
       Recall the problem description in Section 2.1, where we have two case sets CB and {a mathematical formula}CB′, representing two unknown distributions {a mathematical formula}FCB and {a mathematical formula}FCB′, respectively. We would like to perform a hypothesis test and determine whether {a mathematical formula}FCB and {a mathematical formula}FCB′ are identical. In Section 4, we defined a notation of distance as test statistic, and for simplicity we denote the test statistic as {a mathematical formula}θˆ.
      </paragraph>
      <paragraph>
       Once an observation {a mathematical formula}θˆ is made (in our case, the calculation of the distance between the current case base and the incoming new case samples), the achieved significance level (ASL), or the p-value of the test, is defined as the probability of observing at least as extreme as the observed {a mathematical formula}θˆ, assuming that the null hypothesis is true,{a mathematical formula} where the quantity {a mathematical formula}θˆ is fixed at its observed value; the random variable {a mathematical formula}θˆ⁎ has the null hypothesis distribution, the distribution of {a mathematical formula}θˆ if {a mathematical formula}H0 is true [47].
      </paragraph>
      <paragraph>
       The permutation test is a clever way of approximating an ASL for the null hypothesis {a mathematical formula}FCB=FCB′, which works as follows: Given a case base of n cases and an incoming new case sample of m cases, under the null hypothesis, any observed case could have come equally well from either of the case sets. We therefore combine all the {a mathematical formula}m+n cases, then take a sample of size n without replacement to represent the case base; the remaining m cases constitute the incoming case sample. We compute the test statistic for each permutation and repeat the process a large number of times (N). Finally, we estimate the ASL of the permutation test through the Monte Carlo approach [58].{a mathematical formula}
      </paragraph>
      <paragraph>
       Once we fix a desired significance level α, we compare α with the permutation ASL. We say that there is a concept drift when {a mathematical formula}ASLperm&lt;α. For example, when choosing α to be 0.05, we reject the null hypothesis at a 5% level, corresponding respectively to a 5% chance of rejecting the null hypothesis when it is true (false positive). In fact, any permutation test that relies on sampling rather than full enumeration will yield an actual significance level larger than α, due to the Monte Carlo error [59], [60].
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Permutation sample size
      </section-title>
      <paragraph>
       In real world applications, obtaining an exact ASL of a permutation test via full enumeration quickly becomes unfeasible as the permutation sample space increases. This raises the question of how many permutation replications (N) are required.
      </paragraph>
      <paragraph>
       As illustrated by Efron and Tibshirani [47] and Opdyke [60], {a mathematical formula}N×Aˆ follows a binomial distribution of {a mathematical formula}Bi(N,A), where {a mathematical formula}Aˆ=ASˆLperm and {a mathematical formula}A=ASLperm. Using the normal approximation to {a mathematical formula}Bi(N,A), the 95% confidence interval of {a mathematical formula}Aˆ is approximated by {a mathematical formula}A±(1.96×σ), where {a mathematical formula}σ=[A(1−A)/N]12 is the standard deviation of {a mathematical formula}Aˆ. If we do not want the Monte Carlo error to affect our estimation by more than 30% ({a mathematical formula}σ/A⩽0.3), that gives {a mathematical formula}N⩾100 when {a mathematical formula}A=0.1. The precision can be improved with larger N (Fig. 2).
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Experimental evaluation
     </section-title>
     <paragraph>
      Our evaluation of the proposed competence-based concept drift detection method consists of three sections, through eight experiments; all source code can be downloaded from http://decide.it.uts.edu.au/Philip/index.php:
     </paragraph>
     <list>
      <list-item label="1.">
       We evaluate the competence-based empirical distance and compare it to the test statistics of the two-sample Kolmogorov–Smirnov test (Experiment 1, Experiment 2, Experiment 3).
      </list-item>
      <list-item label="2.">
       We compare our change detection method to Dasu et al. [36]. We choose synthetic datasets for the first two parts, since we need to know the change in the generated distribution in advance. In addition, with simulated data, we are able to control the change more easily and to see how our detection method performs against different types of changes (Experiment 4, Experiment 5, Experiment 6, Experiment 7).
      </list-item>
      <list-item label="3.">
       We perform a case-base editing method based on the results of our detection and compare our results on two real datasets against those from the original author [18], as our detection method aims not only to identify a change, but also to describe it (Experiment 8).
      </list-item>
     </list>
     <section label="6.1">
      <section-title>
       Evaluating the competence-based empirical distance
      </section-title>
      <paragraph>
       In Section 4, we proposed a competence-based empirical distance to measure the difference between one distribution and another. To establish how it varies according to the change between generated distributions, we ran three experiments with generated artificial datasets following 1D normal distributions and compared the results with the test statistics of the two-sample Kolmogorov–Smirnov test. Two cases are considered to be able to mutually solve each other, if their Euclidean distance is smaller than a threshold {a mathematical formula}dε. All results are calculated as the mean of 100 independent tests.
      </paragraph>
      <paragraph label="Experiment 1">
       Varying the mean μIn this experiment, we compared distances between data samples, both of size 100, drawn from 11 normal distributions of fixed standard deviation {a mathematical formula}σ=0.2, but with a moving mean value {a mathematical formula}μ=0.2+0.06×(i−1) for the ith distribution, {a mathematical formula}i=1,2,…,11. When {a mathematical formula}t=2×i−1 we compared two samples, both drawn from the ith distribution; when {a mathematical formula}t=2×i we compared two samples drawn from the ith and {a mathematical formula}(i+1)st distribution. Fig. 3 shows how the competence-based empirical distance changes as the distributions vary. Since the extent of the difference depends only on the mean values, we find a similar height on all peaks for each series (Fig. 3). We also tried fixing one data sample while moving the other, in order to show how the distance increases as the difference between the means increases (Fig. 4). It is worth noting that the test statistics of the two sample K-S test seems to be more sensitive to the change, as the margin between the peaks and valleys is larger (Fig. 3), and it also increases faster as the difference accumulates (Fig. 4). This is because our competence-based empirical distance eliminates any change that happened within a problem space where cases are considered to be similar in CBR. In return, our detection method is more reliable for small samples, as shown in Experiment 2.
      </paragraph>
      <paragraph label="Experiment 2">
       Varying the sample sizeIn this experiment, we compared distances between two data samples drawn from {a mathematical formula}N(0.2,0.2) and {a mathematical formula}N(0.44,0.2), respectively. We increased the sample size from 100 to 1000. As shown in Fig. 5, the test statistics of the two sample K-S test shrinks as the sample size increases, while our competence-based empirical distances remain relatively steady. Also, it can be seen that, when the sample size is larger than 800, the distance remains the same for all series.
      </paragraph>
      <paragraph label="Experiment 3">
       Varying σIn this experiment, we fixed the mean at {a mathematical formula}μ=0.5, but varied the standard deviation {a mathematical formula}σ=0.1+0.02×(i−1) for the ith distribution, {a mathematical formula}i=1,2,…,11. Again, when {a mathematical formula}t=2×i−1 we compared two samples drawn from the ith distribution; when {a mathematical formula}t=2×i we compared two samples drawn from the ith and {a mathematical formula}(i+1)st distribution. The sample size was set at 1000. As shown in Fig. 6, the competence-based empirical distances have a larger margin between peaks and valleys, which means that our method is more sensitive to smaller changes than the K-S test, but the amount of change shrinks as σ increases compared with Experiment 1. Intuitively, this is because the distribution becomes less concentrated, so the relative distance is smaller.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Evaluating the competence-based change detection method
      </section-title>
      <paragraph>
       In the above experiments, we have demonstrated how the competence-based empirical distance fluctuates as the underlying distribution changes. In order to determine whether a given measurement is statistically sufficiently significant to qualify as a concept drift, we plug in the permutation test to estimate the ASL, as described in Section 5. Given a desired significance level α, we say that there is a concept drift when {a mathematical formula}ASLperm&lt;α. In the following experiment, we aim to compare our change detection results with Dasu et al. [36]. There are two main reasons for us to choose their method for comparison. First, we both resort to a similar approach for concept drift detection: that is, adopting a distance metric and then performing a hypothesis test. Second, compared with other error-rate based detection methods, our methods do not depend on a classifier and, therefore, have a broader application scope.
      </paragraph>
      <paragraph>
       To make a fair comparison with Dasu et al. [36], we set up the same experimental environments, which includes the following three features: strategy, data source and parameter. For strategy, we also adopt the fix-sliding windows model: that is, keeping a fixed reference window if no concept drift is reported, or else moving both windows. Also, we say that a detection is late when it is reported after moving two or more windows, since the actual window contains the change. For the data source, we implement the same artificial datasets used in Dasu et al. [36]. The detailed information of each artificial data source is described in the experiment setup section. We keep all parameters the same for the permutation test when comparing, which includes the desired significance level {a mathematical formula}α=1% and the permutation size of {a mathematical formula}N=500. We do not evaluate changed parameters because their effects have already been shown and proved in Section 5. For all experiments in this section, the parameter {a mathematical formula}dε, which is used to construct the competence model, is chosen empirically to have a similar partition size to the kdq-tree as in Dasu et al. [36]. A more practical solution for constructing competence models in real world applications is described and used in Section 6.3.
      </paragraph>
      <paragraph label="Experiment 4">
       Normal distributionsThis experiment implements two artificial datasets used in Dasu et al. [36]: the {a mathematical formula}M(Δ) stream – varying the mean {a mathematical formula}μ1 and {a mathematical formula}μ2 independently in {a mathematical formula}[0.2,0.8] with a step size chosen randomly in {a mathematical formula}[−Δ,−Δ/2]∪[Δ/2,Δ] and the {a mathematical formula}C(Δ) stream – varying ρ, which starts at 0 and then randomly walks within {a mathematical formula}[−1,1], with step size chosen randomly in {a mathematical formula}[−Δ,−Δ/2]∪[Δ/2,Δ]. Each stream consists of 5,000,000 two-dimensional normal distributed points, which are further divided into groups of 50,000, giving 99 changes in total. To construct the competence model, {a mathematical formula}dε is set to 0.05.
      </paragraph>
      <paragraph>
       From the results shown in Table 1, we can see that our method out-performs Dasu et al. [36]. Both methods achieve similar results for the M(0.05) and C(0.2) stream, however ours performs significantly better for the other streams, which means our method is more sensitive to smaller changes, compared to Dasu et al. [36]. As we expected, the overall detection accuracy increases when the window size increases. Our method results in a dramatic decrease in the number of late detections as the window size increases. This is probably because our proposed competence-based empirical distance allows different, but related cases to share their distribution contributions with regard to our competence model, rather than being cut exclusively by regions, thus improving its robustness to smaller samples. The number of false detections is almost the same as Dasu et al. [36]. In fact, the false detection rate largely depends on the desired significance level α due to the nature of the permutation test. Our detection method also depends on the appropriateness of the competence model. In real world scenarios, one possible way of constructing the competence model is to use the leave-one-out strategy (shown in Section 6.3), or resort to other available information such as reasoning history or experts.
      </paragraph>
      <paragraph label="Experiment 5">
       Poisson distributionsIn this experiment, we compare the detection results on 2D (discrete) Poisson distributions, with data streams generated according to {a mathematical formula}(X,Y)∼Poisson(500(1−ρ),500(1−ρ),500ρ), where ρ starts at 0.5 and then performs a random walk between 0 and 1 with step size {a mathematical formula}Δ=0.2,0.1, in the same manner as in Experiment 4. We generated the bivariate Poisson using Trivariate Reduction [61]. To construct the competence model, {a mathematical formula}dε is set to 10.
      </paragraph>
      <paragraph label="Experiment 6">
       Higher dimensions{a mathematical formula} To test the scalability and performance of our scheme in high dimensions, we also take the C(0.2) stream and extend it to d-dimensional streams by adding dimensions in which the data distributions (also Gaussian with {a mathematical formula}σ=0.2) do not change. We keep the same standard deviation of all added dimensions as the C streams in order to maintain the marginal distribution of all dimensions, so that the overall distance between cases contributed by each dimension is equally important. In fact, since our competence model depends on the distance between cases, large marginal stable distributions will dominate calculation of the distance between cases and cause failure of our detection method. However, this issue can be easily solved by adopting a weighted distance calculation. To construct the competence model, {a mathematical formula}dε is set to 0.15 for 4-dimensional, 0.3 for 6-dimensional and 0.5 for 10-dimensional; recall that {a mathematical formula}dε is chosen to have a similar partition size as the kdq-tree.
      </paragraph>
      <paragraph>
       The experimental results for different dimensions are listed in Table 3. As we expected, with more stationary dimensions added, it becomes harder to detect the real changes. However, our detection method preserves much of its power as the number of dimensions increase. This again is another proof that our detection method is more sensitive to smaller changes. Also, we have less late detection because of the robustness of the smaller sample of our method.
      </paragraph>
      <paragraph label="Experiment 7">
       EfficiencyThe nature of our detection method consists of three parts: space partition, calculating test statistics and determining the critical region. We, and Dasu et al. [36] are able to compute the test statistics directly, and we both adopt the permutation test to determine critical region. We only compare the cost of the first part, which is maintenance of the competence model in our case vs. updating the kdq-tree in Dasu's case. We use the standard approach for updating the competence model for case addition [62]. These running times were obtained from our unoptimized C# code on a laptop PC with a 2.3 GHz Intel i5 processor and 4 GB memory.
      </paragraph>
      <paragraph>
       The efficiency of the competence-based concept drift detection algorithm is little affected by the number of dimensions, but directly related to the window size. While the window size increases, the time grows linearly. In fact, the maintenance cost was proven to be estimated by {a mathematical formula}O(n), because each time a new case is added to the current case-base, it must be compared to every case in this case-base to determine which cases can solve it and which cases it can solve [62]. To the contrary, the updating cost of the kdq-tree is little affected by the window size, but exhibits a linear relationship with the dimension. This is because the updating cost of the kdq-tree has been proven to be {a mathematical formula}O(dlog1δ)[36].
      </paragraph>
      <paragraph>
       {a mathematical formula}
      </paragraph>
     </section>
     <section label="6.3">
      <section-title>
       Evaluating the description of the competence-based change detection method
      </section-title>
      <paragraph>
       In order to show how our method will help in real world scenarios, we apply our detection method to two real world concept drift datasets that are available from http://decide.it.uts.edu.au/Philip/index.php. Each dataset consists of more than 10,000 emails collected over a period of approximately one year by an individual. A training set of 1000 cases, including the last 500 spam emails and 500 legitimate emails, is set up for each dataset. The remaining data is used to test our algorithm over time. We perform real-time change detection for every incoming new email, and trigger maintenance operations in the competence areas that are identified to be undergoing changes. For more detailed information of the concept drift datasets, please refer to Section 5 of Delany et al. [18].
      </paragraph>
      <paragraph label="Experiment 8">
       We compare our results with the original author of this dataset [18] and a recent work on concept drift [24]. In order to compare fairly with Delany et al. [18], we choose the same classifier, that is the k-nearest neighbor with {a mathematical formula}k=3 using unanimous voting. Again, we also weight all features equally and the similarity between two cases is measured by the proportion of matched features. The related set is constructed using leave-one-out classification. That is, any case {a mathematical formula}ci is considered to be solved by the actually retrieved cases that successfully solve {a mathematical formula}ci bounded by the closest case {a mathematical formula}cj that fails to solve to {a mathematical formula}ci. For Learn++.NSE [24], we build a new case-base every month using the same criteria described above as the base classifier, and choose the same parameter values that are suggested in their paper, i.e., {a mathematical formula}a=0.5, {a mathematical formula}b=10. We are aware that it may not be fair to choose the same parameter values for a different dataset; however, a method for determining these parameters was not provided and these values are reported to work well on all the scenarios attempted.
      </paragraph>
      <paragraph>
       Our spam filtering algorithm starts with the provided training set. After each classification, we perform the competence-based change detection algorithm. A maintenance operation is triggered when a concept drift is detected; therefore, the system can adapt quickly to the new concept and tune for future detections. Our tuning mechanism is a hybrid of PECS [23] and BBNR [19], which is specifically adopted to enhance PECS. On one hand, when there is no concept drift, BBNR prevents a noisy case from inclusion; on the other hand, when there is indeed concept drift, from all the areas that are reported to be changing, which in fact are measured by the set A (Definition 10), we identify and remove cases that conflict with the new case. The idea behind this tuning strategy is that, when there is no concept drift in the data stream, old cases can help to identify noise and improve accuracy; however, when there is indeed concept drift, new instances are more representative of the novel concept, while old cases that conflict may not help.
      </paragraph>
      <paragraph>
       As the CBE method described in Delany et al. [18] considers both competence enhancement and competence preservation, we also plugin a redundancy removal operation, which removes cases uniformly in the competence space and ensures all removed cases can still be correctly solved. We keep 1500 cases for dataset 1 and 2500 cases for dataset 2. This limit is determined according to Delany et al. [18], which reports that ‘the resulting size of the case-base after all the data has been applied (i.e. after 10 months for dataset 1 and 12 months for dataset 2) is 1512 and 2518, respectively’. We do not incorporate any redundancy removal operation for Learn++.NSE, because: 1) we intend to retain their original methods; 2) each base classifier in Learn++.NSE is relatively small, which meets the speed and storage requirements.
      </paragraph>
      <paragraph>
       Since error rate is not a good metric for skewed datasets, the most common performance metrics [63] for spam filtering are used here to evaluate performance, where the Legitimate Recall (LR) is defined as {a mathematical formula}LR=TNTN+FP=TNN, and the Legitimate Precision (LP) is defined as {a mathematical formula}LP=TNTN+FN, where FP means legitimate emails that are incorrectly classified as spam; FN means spam emails that are incorrectly classified as legitimate.
      </paragraph>
      <paragraph>
       The classification results of each month for both datasets are shown, respectively, in Fig. 7 and Fig. 8.
      </paragraph>
      <paragraph>
       These results clearly demonstrate that our strategy effectively improved the results on both LR and LP for both datasets. Our approach mainly improves LP. This is because we are using k-NN with unanimous voting. As a result, mistakenly retaining one or two noisy spam emails in the same competence area will not affect the classification result for a legitimate email; however, mistakenly retaining a legitimate email will dramatically affect the classification result for a spam email. Compared with CBE, the improvement on LP is statistically significant for both datasets, by paired t-test, at 94% confidence level. This shows that our method can accurately catch the concept drift and perform well on targeted reactions. We also observed that Learn++.NSE was the worst on most months, which was principally for the following two reasons: 1. The ensemble approach adopted in Learn++.NSE performed a postponed updating schema, i.e., a new base classifier was trained monthly. As a result, the system could not take advantage of the up-to-date feedback and make a timely adaptation. It would appear that an online learning schema is more suitable for the spam filtering domain where the feature space is large and sparse, and concept drift changes rapidly. 2. The weight of each base classifier is calculated by a log operator which may generate an unlimited figure. As a result, a single base classifier can dominate the final prediction result. In our experiments at time t, the base classifier {a mathematical formula}ht trained with the latest dataset {a mathematical formula}Dt always achieved the best performance on {a mathematical formula}Dt. Note that the weight of each base classifier is assigned based on its behavior on {a mathematical formula}Dt; however, when concept drifts, the incoming dataset {a mathematical formula}Dt+1 may follow a different distribution of {a mathematical formula}Dt. Unfortunately, Learn++. NSE does not incorporate any drift detection mechanism to cope with this situation.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusions and further studies
     </section-title>
     <paragraph>
      The competence group model is inadequate and deficient. We have consequently proposed and defined a competence closure model. This study conducts a theoretical study on the competence group model, and finds that it does not guarantee a complete splitting of the case-base into several competence groups (inadequacy), nor does it guarantee that any two competence groups are mutually independent (deficiency). This research defines a competence closure model to overcome these problems and reveals the essential differences between these two models. It also identifies three important aspects of the competence closure model which the competence group model does not possess: 1) the competence closure model is able to uniquely model the entire case-base; 2) the competence closure model provides a smaller granularity than the competence group model; 3) competence closure is the maximum set concerning a series of related problems and two competence closures are said to be independent in terms of the related set.
     </paragraph>
     <paragraph>
      Concept drift can also reflect on competence measurement. We have presented a method to detect concept drift via competence models which requires no prior knowledge of distribution but measures it through a competence model. The competence-based change detection method can be applied to CBR systems, where new cases are available sequentially over time. Empirical experiments report three advantages of our proposed competence-based change detection method: 1) a high achieved detection rate, 2) robustness on small sample size, and 3) ability to quantify and describe the changes it detects, which makes it highly suitable for handling local concept drift problems.
     </paragraph>
     <paragraph>
      We have found that the competence-based empirical weight provides a rough estimation of the competence distribution of the cases. Our next attempt will aim to provide a more reliable competence distribution through fuzzy probability theory. Another improvement may be achieved by drawing a sample that contains no duplicates in the permutation test, as proven by Opdyke [60]. Finally, this paper is part of our work in handling concept drift problems in CBR. Successive case base editing methods and metric learning methods that take advantage of change detection are needed to improve the final learning performance.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>