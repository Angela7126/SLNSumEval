<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Relational reinforcement learning with guided demonstrations.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Learning tasks with robots is a very interesting topic, where impressive results may be obtained without having to design specific algorithms for each task. However, learning high-level tasks can be very time consuming because considerable experience is required to learn in real-world domains where stochastic actions with multiple effects can be performed. In general, learning from scratch in such domains requires that hundreds of actions are executed even for simple tasks. Robots require several seconds, or even minutes, to execute high-level actions, which means that the total time required to learn a task can be excessively large. However, learning with the help of a human teacher can reduce this learning time greatly, although human time is usually considered to be more valuable than that of a robot, and thus the robot should only request help from a teacher a limited number of times.
     </paragraph>
     <paragraph>
      This is the underlying concept of the learning approach in the European project IntellAct [1], the goal of which is to exploit the semantics of manipulations in terms of objects, actions, and their consequences to reproduce human actions using robots. This project provides a framework where a robot should learn manipulation tasks with no prior knowledge, as well as performing high-level reasoning to adapt to changes in the domain. The robot starts without any knowledge of the available actions and it has to request demonstrations from a teacher whenever new actions are required to complete a task. These actions are learned at a low level using programming by teleoperation [2], as well as at a high level with a decision maker. Moreover, a 3D object recognition algorithm [3] and a particle filter tracker [4] are used to obtain a state for reasoning and to generate semantic event chains [5], which encode the demonstrated action sequences. Using this information, the decision maker can learn about the domain constantly while completing the assigned tasks [6]. In this study, we propose a new reinforcement learning algorithm for the decision maker called Relational Exploration with Demonstrations (REX-D), which can learn to perform manipulation tasks based on only a small number of action executions with the help of a teacher guided by rule analysis.
     </paragraph>
     <paragraph>
      In some robotic systems, such as those that involve complex manipulation sequences, the decision maker has little input data and long periods of time to process it, because the actions take a long time to execute. Therefore, a good approach for learning such high-level tasks is model-based reinforcement learning (RL) [7]. This approach allows a model to be obtained that represents the actions that the robot can execute. The model is generated from the experiences obtained when the robot executes the actions.
     </paragraph>
     <paragraph>
      To learn tasks as rapidly as possible, we need a highly compact representation of the model, and thus we use relational models. These models generalize over different objects of the same type, thereby reducing the learning complexity of domains to the number of different types of objects in them. Several approaches apply RL to obtain good results in specific robotic tasks using relational domains [8], [9], as well as with general relational models [10], [11]. A fundamental problem in RL is balancing exploration and exploitation. To reduce the number of experiences required, Lang et al. [12] proposed the REX algorithm, which used relational count functions to apply relational generalization to the exploration–exploitation dilemma. In our approach, we learn general relational models using the relational generalization of REX, but we also include teacher demonstrations and rule analysis to reduce the number of experiences required even further, as well as facilitating generalization over different tasks.
     </paragraph>
     <paragraph>
      In general, there is some uncertainty about the effects of an action executed by a robot. This uncertainty is important in some tasks, thus the RL algorithm should be able to handle stochastic effects. In the KWIK framework [13], a method was proposed for learning the probabilities associated with a given set of action effects using linear regression [14], as well as an extension for learning the action effects themselves [15]. However, a large number of samples are needed because the problem of learning action effects is NP. In our proposed method, we use the learner proposed by Pasula et al. [16], which employs a greedy algorithm to obtain rule sets that optimize a score function. Although this does not obtain the optimal solution, it generates good rule sets based on only a few experiences. Furthermore, it generates rules with deictic references and noisy effects, which make models more compact and tractable.
     </paragraph>
     <paragraph>
      Learning from Demonstration (LfD) is a supervised learning paradigm, where a teacher transfers knowledge of tasks or skills to a robotic system by performing a demonstration of the task or skill [17]. Thus, we propose the combination of RL with LfD to obtain a system that learns new tasks without needing to know the actions in advance. Demonstrations are also very useful for improving the learning process, as well as for adapting to different tasks when new unknown actions need to be introduced.
     </paragraph>
     <paragraph>
      The problem of integrating demonstrations with RL-like algorithms has been addressed previously. Meriçli et al. [18] used a teacher to improve the robot policies, where corrective demonstrations were issued whenever the robot did not perform as well as the teacher expected. Walsh et al. [19] expanded the apprenticeship protocol of Abbel and Ng [20] by proposing a system where the teacher reviews the actions selected, and a demonstration of the optimal action is performed whenever they are not optimal. TAMER [21] is a framework where the teacher provides reinforcement rewards that evaluate the performance of the robot, and the system exploits its current model by choosing the actions that are expected to be the most highly reinforced. In these approaches, the teacher has to intervene to improve the robot behavior whenever it is not sufficiently satisfactory. By contrast, our algorithm actively requests demonstrations from the teacher whenever help is needed, thereby releasing the teacher from having to monitor the system continuously.
     </paragraph>
     <paragraph>
      Active demonstration requests have been included in algorithms with confidence thresholds [22], which request demonstrations for a specific part of the state space whenever the system is not sure about the expected behavior. A confidence-based method was also described in [23], which was combined with supplementary corrective demonstrations in error cases. Agostini et al. [24] request demonstrations from the teacher when the planner cannot find a solution with its current set of rules. Our approach combines active demonstration requests with autonomous exploration. Because the teacher's time is considered to be very valuable, demonstration requests should be limited and replaced with exploration whenever possible.
     </paragraph>
     <paragraph>
      When a demonstration is requested from the teacher, he does not know which parts of the model are already known. In many tasks, several actions may be selected at a given time to complete the task. However, if no guidance is provided to the teacher, he may demonstrate an action that is already known by the system. In the model, the actions are represented as a set of action rules, which can be applied over a state space. Therefore, we propose the use of a rule analysis approach to provide some guidance to the teacher so he can demonstrate the unknown parts needed by the decision maker. To explain failures when planning such models, Göbelbecker et al. [25] designed a method for finding “excuses”, which are changes made to the state that make the planner find a solution. Based on these excuses, we analyze the rules to provide guidance to the teacher. Moreover, because we use a greedy learning algorithm, excuses are also useful for finding alternative models that explain unexpected states.
     </paragraph>
     <paragraph>
      In summary, we propose a RL algorithm that can request demonstrations from a teacher whenever it requires new unknown actions to complete a task, which significantly reduces the number of experiences required to learn. This approach is very useful for addressing two fundamental problems in robotics: generalizing to different tasks and the large amount of time required for learning. Adding new actions allows the robot to apply previous knowledge to different tasks that may require new actions, while optimal demonstrations provide more information to the learner than random exploration. Moreover, we also employ model analysis to guide the teacher, thereby making his demonstrations as helpful as possible and improving the model when using a greedy learner.
     </paragraph>
     <paragraph>
      The remainder of this paper is organized as follows. First, we review the background related to our study, before presenting our REX-D algorithm. Next, the model analysis and its advantages are explained, followed by an experimental evaluation of our system. Finally, we give our conclusions and some suggestions for future research.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background on relational RL
     </section-title>
     <paragraph>
      In this section, we present the background related to our RL algorithm. First, we describe Markov Decision Processes (MDPs), which we use to formulate the problem. Next, we explain the relational representation used for a compact state space. Finally, we explain the learner and how to apply relational generalization to RL.
     </paragraph>
     <section label="2.1">
      <section-title>
       MDP
      </section-title>
      <paragraph>
       MDPs are used to formulate fully observable problems with uncertainty. A finite MDP is a five-tuple {a mathematical formula}〈S,A,T,R,α〉 where:
      </paragraph>
      <list>
       <list-item label="•">
        S is a set of discrete states.
       </list-item>
       <list-item label="•">
        A is a set of actions that an agent can perform.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T:S×A×S→[0,1] is the transition function that describes the probability of obtaining a successor state by executing an action from a given state.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R:S×A→R is the reward function.
       </list-item>
       <list-item label="•">
        {a mathematical formula}α∈[0,1) is a discount factor that measures the degradation of future rewards.
       </list-item>
      </list>
      <paragraph>
       The goal is to find a policy {a mathematical formula}π:S→A which selects the best action for each state to maximize the future rewards. The sum of rewards is {a mathematical formula}Vπ(s)=E[∑tαtR(st)|s0=s,π] which is the function to maximize.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Relational representation
      </section-title>
      <paragraph>
       Relational domains represent the state structure and objects explicitly. These domains are described using a vocabulary of predicates P and actions A, and a set of objects {a mathematical formula}Cπ. Predicates and actions take objects as arguments to define their grounded counterparts.
      </paragraph>
      <paragraph>
       The state is defined with a set of positive grounded predicates. An example of a predicate is on(X,Y), and some of its groundings could be on(box1, box2) and on(box3, box1).
      </paragraph>
      <paragraph>
       In a relational domain, actions are represented as grounded functions. The arguments of an action are the objects with which the action interacts. An example of an action is pickup(X), while a grounded action would be pickup(box1).
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       Model
      </section-title>
      <paragraph>
       The system represents the model as a set of rules Γ, which define the preconditions and effects of the actions. Because we want to address stochastic environments, we use Noisy Indeterministic Deictic (NID) rules [16]. A NID rule r is defined as{a mathematical formula} where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}ar is the action that the rule represents.
       </list-item>
       <list-item label="•">
        {a mathematical formula}ϕr(χ) are the preconditions. When the action {a mathematical formula}ar is executed, only the rule r with preconditions present in the state will be applicable.
       </list-item>
       <list-item label="•">
        {a mathematical formula}Ωr,i are the effects, which have an associated probability of {a mathematical formula}pr,i∈[0,1]. The sum of probabilities is always {a mathematical formula}∑ipr,i=1. Effects define the set of predicates that are changed in the state when the rule is applied.
       </list-item>
       <list-item label="•">
        {a mathematical formula}Ωr,0 is called the noise effect, which represents all the other rare and complex effects that are not represented by any {a mathematical formula}Ωr,i and that are not covered explicitly for compactness and generalization reasons.
       </list-item>
       <list-item label="•">
        χ is the set of variables for the rule. To obtain a grounded action, each one of these variables is set as an object, thereby yielding a set of grounded predicates in the preconditions and the effects of the rule. There are two types of variables: the parameters of the action and the deictic references.
       </list-item>
       <list-item label="•">
        An action has a set of parameters {a mathematical formula}χa, which represent the objects with which the action interacts.
       </list-item>
       <list-item label="•">
        Actions also have deictic references {a mathematical formula}DR=χ∖χa, which identify objects related to the action that is being performed. These variables are referenced in the preconditions and their possible groundings depend on the context of the action.
       </list-item>
      </list>
      <paragraph>
       A NID rule represents only one action. However, each action may be represented by several rules, where each has different preconditions.
      </paragraph>
     </section>
     <section label="2.4">
      <section-title>
       Learner
      </section-title>
      <paragraph>
       A learner is used to obtain the rules that represent the model. The learner uses experiences obtained from executing previous actions to infer the rules.
      </paragraph>
      <paragraph>
       Experiences comprises a set of triples {a mathematical formula}E=(s,a,s′), which represent the new state s obtained after executing the action a in state s. We say that a rule covers an experience when the rule preconditions are satisfied by s, the rule represents the action executed {a mathematical formula}ar=a, and the effects of the rule are the changes between the initial and the final state for the experience {a mathematical formula}Ωr,i=s′∖s.
      </paragraph>
      <paragraph>
       Pasula et al.'s learning algorithm [16] is used to learn the NID rules. The problem of learning stochastic rule sets is NP-hard [16], so a greedy heuristic search is used to obtain the rule sets. The heuristic generates rule candidates and a score function is employed to select the best options. Pasula et al.'s score function for optimizing the trade-off between the accuracy of the rules and their complexity is:{a mathematical formula} where {a mathematical formula}rs,a is the rule that covers the experience when a is performed in s, {a mathematical formula}Pˆ is the likelihood of the experience, {a mathematical formula}PEN(r) is a complexity penalty, and α is a scaling parameter.
      </paragraph>
      <paragraph>
       Complete observability is also assumed throughout this study. REX-D can address partially observable domains if the learner and the planner used are able to learn them. Other approaches have been proposed that learn complex models in partially observable domains [26]. However, we use Pasula et al.'s learner because its generalization capabilities and its ability to tackle uncertain action effects are better suited than partial observability learners to the tasks considered in the present study.
      </paragraph>
     </section>
     <section label="2.5">
      <section-title>
       RL in relational domains
      </section-title>
      <paragraph>
       In a RL problem, the transition distribution T is unknown a priori, and thus it has to be learned. In particular, we use a model-based RL, where a model is estimated based on experiences using the learner described earlier and the policies that the robot executes are obtained from this model. The agent can employ two strategies to solve the task, as follows.
      </paragraph>
      <list>
       <list-item label="•">
        Exploration: Executing actions to explore the model and to estimate T, thereby allowing better policies to be obtained in the future.
       </list-item>
       <list-item label="•">
        Exploitation: Executing actions to obtain high rewards.
       </list-item>
      </list>
      <paragraph>
       In RL, a very important problem is the exploration–exploitation dilemma, which involves finding a balance of sufficient exploration to obtain a good model without consuming too much time addressing the low-value parts of the state.
      </paragraph>
      <paragraph>
       Lang et al. [12] proposed a solution to the exploration–exploitation dilemma in relational worlds, which employs the relational representation to reduce the number of samples before treating a state as known. Their proposed method uses the following context-based density formula for state-action pairs:{a mathematical formula} where {a mathematical formula}cE(r) counts the number of experiences that cover the rule with any grounding and {a mathematical formula}I(⋅) is a function, which is 1 if the argument is evaluated as true and 0 otherwise.
      </paragraph>
      <paragraph>
       Using Eq. (2), the algorithm (REX) is specified to obtain near-optimal solutions in relational domains. Relational generalization is applied to well-known RL algorithms to obtain considerable improvements in relational domains. It has two variants: one based on R-MAX [27] and another based on {a mathematical formula}E3[28].
      </paragraph>
     </section>
     <section label="2.6">
      <section-title>
       Teacher demonstrations
      </section-title>
      <paragraph>
       When a demonstration is requested, the teacher demonstrates how to perform an action, its name, and its arguments. In the present study, it is assumed that the teacher knows the optimal policy {a mathematical formula}π⁎, but the system can also work with suboptimal policies if they also achieve the goal successfully. Inferior demonstrations may lead to worse policies being learned, but the system will request further demonstrations and explore until it has learned how to meet its goal. In addition, even if the teacher knows the optimal (or near optimal) policy, implementing a policy in a robot is a very tedious task, which may take a much longer time to model and to test properly compared with learning it using the proposed algorithm. Moreover, RL algorithms also facilitate adaptation to unexpected changes and different tasks.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Robot platform
     </section-title>
     <paragraph>
      As an initial step, all of the modules related to high-level reasoning were integrated into a virtual reality (VR) system with a simulated robot [29], [30], which was very useful for testing the whole system before moving it onto the real robot platform because the same interfaces were used. The VR system provided the decision maker with symbolic states, the actions executed by the robot, and the information supplied to generate semantic event chains [5], which were used to recognize the actions executed by the teacher.
     </paragraph>
     <paragraph>
      The VR setup comprised a multi-screen stereoscopic rear projection installation for showing the scenario and a wireless data glove with a tracking system for interacting with it. This system could keep track of hand movements, which were used to interact with objects in the scenario in the same manner as if they were in the real world. This VR system also simulated robot executions, including noise and action failures. The decision maker could request actions from this simulated robot in the same manner as it would request them from a real robot. Fig. 1 shows the teacher demonstrating an action in the VR system.
     </paragraph>
     <paragraph>
      The same high-level reasoning skills were successfully integrated in a real robot platform [6]. The robot could learn new actions from demonstrations, recognize the actions executed by the teacher, and provide REX-D with the skills to generate symbolic states that represented the scenario.
     </paragraph>
     <paragraph>
      To demonstrate an action, the teacher provided the action name and its arguments, before moving the robot arm to perform the action. When a new unknown action was demonstrated, programming by teleoperation [2] was used to learn the demonstrated action at a low level, thereby allowing it to be reproduced. The action was associated with the symbolic name given to the action. Transferring the knowledge to a high-level cognitive system could also be achieved with a symbolic representation that captured the interaction between objects and actions [31], [32]. However, in the present study, we assumed that the teacher supplied the name of the action that he executed for the robot.
     </paragraph>
     <paragraph>
      The predicates that represented the state used for high-level reasoning were obtained using the robot cameras. A 3D object recognition module [3] determined the different objects in the scene and their positions in the very first frame, which were then tracked as they moved [4], before transforming them into semantic event chains [5] to generate a symbolic state for planning and learning. Semantic event chains also facilitate the recognition of the actions executed by the teacher because they encode the changes in the relationships between objects at decisive time points during a manipulation procedure. Although the underlying perception algorithms work with continuous perceptions, REX-D only uses the information available before and after an action is executed because its cognitive processing is based on the state changes introduced by actions.
     </paragraph>
     <paragraph>
      Although REX-D was finally integrated and tested in the real robot, the development process and experiments were mostly performed in the VR setup because it allowed us to conduct large numbers of replicates to obtain meaningful statistics, thereby facilitating the evaluation of the algorithm. The main limitations of using a real robot platform compared with a VR system are as follows,
     </paragraph>
     <list>
      <list-item label="•">
       In a state, the objects need to be recognized perfectly, and thus they should be easily distinguishable.
      </list-item>
      <list-item label="•">
       Objects need to be tracked continuously while the robot or the teacher interacts with the scenario. This process requires at least two cameras, which point in different angles to avoid occlusions, and the objects should not move faster than the tracker's frame rate.
      </list-item>
      <list-item label="•">
       The robot has to learn actions in a manner that allows them to be generalized to different object positions. In this study, these skills were limited to simple pick-and-place actions, which were sufficient to solve the tasks assigned.
      </list-item>
     </list>
     <section label="3.1">
      <section-title>
       Cranfield benchmark
      </section-title>
      <paragraph>
       The Cranfield benchmark, which is a standardized assembly task in robotics, was the scenario used for the real-world experiments. This scenario involves assembling an industrial item, the parts of which are shown in Fig. 2. There are precedence constraints in the assembly that restrict the order in which the parts have to be assembled. Square pegs have to be placed before the separator, the shaft before the pendulum, and all other pieces before the front faceplate.
      </paragraph>
      <paragraph>
       Moreover, some variants of this scenario are used in more difficult and interesting problems, as follows.
      </paragraph>
      <list>
       <list-item label="•">
        Pegs are difficult to place when they are in a horizontal position. Therefore, actions are required to place them in a vertical position.
       </list-item>
       <list-item label="•">
        The standard initial state is shown in Fig. 2. To add more complexity to the domain, other initial states can be used, e.g., an initial state where the separator is placed before the pegs. In this case, the robot first has to remove the separator in order to place the square pegs.
       </list-item>
       <list-item label="•">
        Changing some parts for new ones, such as replacing the front faceplate with another that is not compatible with a pendulum. This is an interesting problem for learning when part of the domain is already known.
       </list-item>
      </list>
     </section>
    </section>
    <section label="4">
     <section-title>
      Incremental reinforcement learning with demonstrations
     </section-title>
     <paragraph>
      Our environment contains a robot, which can perform different actions, a perception system to obtain the state, and a human teacher who can demonstrate actions to the robot when requested. The robot starts with no previous knowledge of the transition function T or the set of available actions A. When new goals are assigned to the robot, it tries to solve the tasks and it requests demonstrations if they were needed.
     </paragraph>
     <paragraph>
      Although the number of teacher interactions should be low, it is considered beneficial to request a demonstration if it eliminates numerous automatic robot executions, which may take a very long time. The teacher is considered to employ an optimal policy {a mathematical formula}π⁎ so he always demonstrates the best action for the current state.
     </paragraph>
     <section label="4.1">
      <section-title>
       Reinforcement learning with demonstrations
      </section-title>
      <paragraph>
       To incorporate demonstrations into RL, we propose the use of a variant of the {a mathematical formula}E3 version of the REX algorithm. The {a mathematical formula}E3 algorithm is based on the concept of known states: a state is considered to be known when all the state-action pairs for that state have been visited at least the confidence threshold ζ number of times. Depending on the experiences of the algorithm, it proceeds as follows.
      </paragraph>
      <list>
       <list-item label="•">
        Whenever the robot enters an unknown state, it performs the action that requires the fewest times to explore.
       </list-item>
       <list-item label="•">
        If it enters a known state, and a valid plan through known states is found, then that plan is executed (exploitation).
       </list-item>
       <list-item label="•">
        If it enters a known state but no plan is found, it plans using a MDP where unknown states are assumed to have a very high value (planned exploration).
       </list-item>
      </list>
      <paragraph>
       The REX algorithm generalizes this to a relational domain, but it maintains the same concepts. It uses Eq. (2) as a count function to decide whether a state is already known.
      </paragraph>
      <paragraph>
       We propose the algorithm REX-D (Algorithm 1), where we modify REX by adding the option to request demonstrations from a teacher. REX-D explores the list of available actions until they are considered to be known in the same manner as REX, but after it enters a known state where no solution is found, the planned exploration step is substituted with a demonstration request, where the teacher executes the best action for the current state. When the teacher demonstrates an action, its treatment depends on the current knowledge of the robot, as follows.
      </paragraph>
      <list>
       <list-item label="•">
        If the demonstration is the first execution of the action, the robot learns how to execute that action and adds it to the set of actions that the learner must learn.
       </list-item>
       <list-item label="•">
        If the action has already been executed, its rules are simply refined by the learner.
       </list-item>
      </list>
      <paragraph>
       The ability to make a few requests of a teacher inside the RL loop has two main advantages: faster learning and the ability to add actions as they are required.
      </paragraph>
      <section label="4.1.1">
       <section-title>
        Faster learning
       </section-title>
       <paragraph>
        Executing actions takes a long time for robots, so it is recommended to request help from the teacher to save large periods of learning time.
       </paragraph>
       <paragraph>
        Exploration is performed until a known state is reached. However, once a known state is entered, the decision maker has to decide where to explore. Exploring the whole state space requires a large number of samples; thus, a demonstration may be requested to guide the system through the optimal path.
       </paragraph>
       <paragraph>
        The problem when learning a model only by exploration (in the case where a list of actions is available) is finding examples where the actions are successful. As noted by Walsh [15], when an action fails because its preconditions are not satisfied (which we will call negative examples), these experiences are highly uninformative because the learner cannot determine the reason for the failure. However, positive examples where an action changes predicates, provide very useful information because a superset of the literals that belong to the precondition is identified.
       </paragraph>
       <paragraph label="Proposition 1">
        In the worst case, learning the preconditions of a rule r needs{a mathematical formula}Ω(2|Pr|)examples, where{a mathematical formula}|Pr|is the maximum number of grounded predicates that may appear in the rule preconditions, and{a mathematical formula}|Pr|=∑p∈P|χr||χp|where{a mathematical formula}χrare the variables of the rule r and{a mathematical formula}χpare the parameters of the predicate p.
       </paragraph>
       <paragraph label="Proof">
        If the rule preconditions have a grounded predicate set {a mathematical formula}Pr which is either positive or negative, then only one combination that uses all the predicates satisfies the preconditions and {a mathematical formula}2|Pr| action executions are needed to test all possible combinations. In the worst case, the last combination tested is the only valid combination, thus {a mathematical formula}2|Pr| examples are needed to find it. □
       </paragraph>
       <section>
        <section-title>
         Example
        </section-title>
        <paragraph>
         Consider that we have to open a box with the action open(X,Y), but it requires that the objects X and Y are connected in a particular way (using the predicates connected(X,Y) and on(X)). In this case, {a mathematical formula}|Pr|=22+21=6, and the possible preconditions are:{a mathematical formula} In total, there are: {a mathematical formula}2|Pr|=26=64 combinations. In the worst case, where the correct combination is the last one tested, the {a mathematical formula}open(X,Y) action has to be executed in 64 different states until a suitable one is found.
        </paragraph>
        <paragraph label="Proposition 2">
         With only one positive example of a rule{a mathematical formula}er=(s,ar,s′), the action rule preconditions can be learned with{a mathematical formula}Ω(|s|)examples.
        </paragraph>
        <paragraph label="Proof">
         Because the preconditions comprise a conjunction of predicates, we know that only the predicates in the state s where the action was executed may be present in the rule preconditions. Thus, setting each one of these predicates to its opposite to test whether it is actually one of the preconditions requires {a mathematical formula}|s| examples. □
        </paragraph>
        <paragraph label="Proposition 3">
         Adding a new positive examples of a rule{a mathematical formula}Er={e1,…,en}reduces the complexity of learning its preconditions to{a mathematical formula}Ω(|PEr|)examples, where{a mathematical formula}PEr={s1∩…∩sn}.
        </paragraph>
        <paragraph>
         In real-world scenarios, learning rules requires far fewer experiences than the worst cases described above. In general, most predicates are irrelevant to the execution of an action, which increases the probability of obtaining a positive example. Furthermore, robots operate in small subsets of the overall state space and their actions are designed to be executed only in the parts of the state space that they can reach. Although actions can be learned with fewer experiences, the previously described propositions provide a general view of the complexity of the learning process and the advantages of using demonstrations.
        </paragraph>
       </section>
      </section>
      <section label="4.1.2">
       <section-title>
        Online action additions
       </section-title>
       <paragraph>
        Initially, the learning system does not know which actions are available because the teacher has to demonstrate them only when they are needed. Therefore, exploration alone is not a valid strategy because a previously unknown action may be required to complete the task. When the current state is known (using only the list of actions that have already been learned) and no plan is found, a demonstration is requested by REX-D. The teacher will execute the best action for that state, which may be a new and previously unlearned action.
       </paragraph>
      </section>
     </section>
     <section label="4.2">
      <section-title>
       Generalizing to different tasks
      </section-title>
      <paragraph>
       A model is generated to represent the actions required to complete a task. If these actions are also used in another task, previous experiences are useful for this new task. Moreover, relational representations allow generalization over different objects of the same type, and thus new tasks that use the same types of objects but different layouts will also reuse previous models.
      </paragraph>
      <paragraph>
       In addition to transferring the model to other tasks, the ability to request demonstrations from the teacher gives the system much more flexibility. Thus, actions do not have to be defined in advance, but the system will request demonstrations from the teacher when they are needed if unknown actions are required during the execution.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Rule analysis
     </section-title>
     <paragraph>
      The system generates rule sets that represent the model, and thus they contain the known information about the task. The weaknesses of the model can be assessed by analyzing these rules, as well as the relationships between different states. This information is useful for providing some guidance to the teacher, for improving the model, and for trying to complete partial tasks via subgoals.
     </paragraph>
     <section label="5.1">
      <section-title>
       Explaining planning failures
      </section-title>
      <paragraph>
       Whenever the system fails to plan, information can be extracted about the failure using excuses [25], [33]. Excuses are changes to the initial state that would make the task solvable, and thus they indicate the important predicates that cause the planner to fail.
      </paragraph>
      <paragraph label="Definition 1">
       ExcuseGiven an unsolvable planning task, using a set of objects {a mathematical formula}Cπ and an initial state {a mathematical formula}s0, an excuse is a pair {a mathematical formula}φ=〈Cφ,sφ〉, which makes the task solvable, where {a mathematical formula}Cφ is a new set of objects and {a mathematical formula}sφ is a new initial state.
      </paragraph>
      <paragraph>
       Excuses can be classified as acceptable, good, or perfect, as follows.
      </paragraph>
      <list>
       <list-item label="•">
        Acceptable excuses change the minimum number of predicates in the initial state. An excuse φ is acceptable iff {a mathematical formula}∀φ′,Cφ⊆Cφ′ and {a mathematical formula}s0△sφ⊆s0△sφ′ (where △ denotes the symmetric set difference).
       </list-item>
       <list-item label="•">
        Good excuses are acceptable excuses with changes that cannot be explained by another acceptable excuse.
       </list-item>
       <list-item label="•">
        A perfect excuse is a good excuse that obtained the minimal cost using a cost function.
       </list-item>
      </list>
      <paragraph>
       Applying goal regression over all acceptable excuses would be highly suboptimal, so a set of good excuse candidates will be generated [25], which are then checked with the planner.
      </paragraph>
      <paragraph>
       Candidates for good excuses can be obtained under certain assumptions using a causal graph and a domain transition graph [34], which are generated with the rule set Γ that defines the domain. A causal graph {a mathematical formula}CGΓ is a directed graph that represents the dependencies of predicates between each other. A domain transition graph {a mathematical formula}Gp of a predicate p is a labeled directed graph that represents the possible ways that the groundings of the predicate can change and the conditions required for those changes.
      </paragraph>
      <paragraph label="Definition 2">
       A causal graph {a mathematical formula}CGΓ is a directed graph that represents the dependencies of predicates between each other. An arc {a mathematical formula}(u,v) exists when {a mathematical formula}u∈ϕr and {a mathematical formula}v∈Ωr for a rule {a mathematical formula}r∈Γ, or when both are in the effects of the rule {a mathematical formula}u,v∈Ωr.
      </paragraph>
      <paragraph label="Definition 3">
       A domain transition graph {a mathematical formula}Gp of a predicate p is a labeled directed graph that represents the possible ways that the groundings of the predicate can change and the conditions required for those changes. An arc {a mathematical formula}(u,v) exists when there is a rule {a mathematical formula}r∈Γ such that u and v are groundings of p, {a mathematical formula}u∈ϕr and {a mathematical formula}v∈Ωr. The label comprises the predicates {a mathematical formula}ϕr∖{u}.
      </paragraph>
      <paragraph>
       To restrict the number of candidates, we only consider those that are relevant to achieving the goal. Using the causal graph and the domain transition graph, the candidates can be obtained with a fix point iteration [25] by adding the predicates that contribute to the goal and those that are potentially required to reach other predicates added previously to the candidate set. From the set of excuse candidates, we select those that are not reachable by {a mathematical formula}Gp from any predicate in the current state, or those that are involved in a cyclic dependency in {a mathematical formula}CGΓ.
      </paragraph>
      <paragraph>
       Finally, the planner is used to test which of the excuse candidates should be added to obtain the best results. The best are selected as the excuses that explain the failure.
      </paragraph>
      <paragraph>
       Note that rules in stochastic domains may have several effects with different probabilities as well as a noisy effect. To generate the excuses in these conditions, noisy and low probability effects are ignored when generating the causal graph and domain transition graph.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Providing guidance to the teacher
      </section-title>
      <paragraph>
       The REX-D algorithm requests demonstrations from the teacher whenever it does not know how to complete the task. However, several actions may be required to complete the task, only one of which might be unknown to the system. Thus, if no guidance is provided, the teacher may demonstrate actions that the system already knows before demonstrating the action that is actually required. Our objective is to avoid these unnecessary demonstrations, thereby minimizing the number of interactions with the teacher.
      </paragraph>
      <paragraph>
       The excuses presented in Section 5.1 are used to find problems in the model to guide the teacher so he demonstrates the action that the system needs. There are several possible reasons why the system may fail to plan, i.e., the wrong preconditions have been added, missing action effects, or dead ends.
      </paragraph>
      <paragraph label="Proposition 4">
       Let Γ be the complete rule set where there is an incorrect rule{a mathematical formula}rincorrect∈Γ, which has{a mathematical formula}pwas a precondition but its effects are not dependent on it. Let φ be an excuse that adds{a mathematical formula}pwto the state and if φ does not prevent any other transition from other rules, φ is an acceptable excuse.
      </paragraph>
      <paragraph label="Proposition 5">
       Let{a mathematical formula}Γ′be an incomplete rule set, which has one rule missing,{a mathematical formula}rmissing∉Γ′. If the effects{a mathematical formula}Ωrmissingare required to achieve the goal and no other rule can obtain the predicates, i.e.,{a mathematical formula}∃p∈Ωrmissing|∀r′∈Γ′,p∉Ωr′, then an excuse φ that includes the necessary changes to{a mathematical formula}Ωrmissingis an acceptable excuse.
      </paragraph>
      <paragraph>
       We will suppose that excuses found point at the problems explained in the previous propositions. However it should be noted that excuses that create invalid alternative paths to the goal may also exist in some domains. The teacher will be warned about possible wrong preconditions or unknown needed effects:
      </paragraph>
      <list>
       <list-item label="•">
        Warning about incorrect preconditions. When the changes in the excuse φ affect the preconditions of one or more rules {a mathematical formula}(∃r∈Γ|φ∈ϕr), the teacher is warned that these rules require the excuse φ. If one of the preconditions is incorrect, the teacher can simply execute the action to prove to the system that it is not actually required.
       </list-item>
       <list-item label="•">
        Warning about missing effects. The teacher is warned that the system does not know how to obtain the changed predicates in φ.
       </list-item>
      </list>
      <section label="5.2.1">
       <section-title>
        Guidance examples
       </section-title>
       <paragraph>
        Fig. 3 shows an example of the Cranfield benchmark scenario (as explained in Section 3.1), where the robot has to mount the different parts to complete an assembly. In the current state, two possible actions have the same reward: placing a red peg, or placing the pendulum (gray). If the system already knows how to place a peg, the teacher will be warned that the planner does not know how to obtain a “pendulumPlaced” predicate.
       </paragraph>
       <paragraph>
        It is recommended that the predicates used in the domain have descriptive names to make excuses guidance useful for non-technical teachers. Teaching the system can be relatively easy if the domain semantics are described appropriately in the predicates, as demonstrated by the following examples of robot messages.
       </paragraph>
       <list>
        <list-item label="•">
         I want to “pickUp(block)”, but this requires “onTable(block)” [possible incorrect precondition]. If the teacher knows that the “onTable” predicate is not required, he only has to execute the pickUp action to demonstrate this.
        </list-item>
        <list-item label="•">
         I don't know how to remove a “flatTire()” [missing effect]. The action “changeTire” that removes “flatTire” has to be demonstrated.
        </list-item>
        <list-item label="•">
         I don't know how to remove a “broken(table)” [missing effect]. If the table is needed to complete the task and it cannot be repaired, this is actually a dead end. Thus, the teacher has to signal a dead end to REX-D.
        </list-item>
       </list>
      </section>
     </section>
     <section label="5.3">
      <section-title>
       Rule alternatives
      </section-title>
      <paragraph>
       The ζ parameter (for considering known states and actions, as introduced in Section 4.1) has to be optimized for each different domain to obtain good results. However, we use a small threshold to learn rapidly and excuses are used to generate alternative models that compensate for this overconfidence. The system uses the Pasula learner (as explained in Section 2.4), which is a greedy heuristic learner for obtaining rule sets that explain previous experiences. Because the learner obtains rule sets that optimize Eq. (1), if these experiences are not sufficiently complete, many different rule sets could explain the experiences with similar scores.
      </paragraph>
      <paragraph>
       Let E be a set of experiences and Γ a rule set that explains those experiences. If E does not cover the whole state space, different rule sets {a mathematical formula}Γ′ may explain the experiences as well as Γ ({a mathematical formula}score(Γ)≃score(Γ′)). If no plan is found but the system obtains an excuse φ, it is assumed that Γ might not be the correct rule set. The changes in the excuses {a mathematical formula}pφ∈s△sφ are used to find alternative models. The system will analyze rules with excuse changes in their preconditions {a mathematical formula}pφ∈ϕr and check for alternative equivalent rules that may obtain a plan for the current state. Note that the complexity grows exponentially when finding rule alternatives with excuses that change more than one predicate.
      </paragraph>
      <paragraph>
       If no plan is found, the algorithm GenerateRuleAlternatives (Algorithm 2) tries to find an alternative rule set that explains past experiences and obtains a plan from the current state.
      </paragraph>
      <list>
       <list-item label="•">
        First, for each excuse that explains the planner's failure, it finds the rules that are required to reach the goal and that can only be executed when the excuse has been applied. The GetRulesRequiringExcuse routine (Algorithm 3) finds these rules and it outputs the set {a mathematical formula}Γφ of rules that allow a valid plan to be obtained by removing the predicates changed by the excuse from their preconditions.
       </list-item>
       <list-item label="•">
        {a mathematical formula}Γφ is used to create the set of candidates that can be valid rule alternatives. Each candidate is generated by making a copy of the current rules {a mathematical formula}Γnew and modifying one of the rules {a mathematical formula}rφ∈Γnew, which is also in {a mathematical formula}Γφ. The preconditions {a mathematical formula}rφ are changed by replacing a predicate that is changed by the excuse φ with a predicate that is present in the current state {a mathematical formula}p∈s. The aim is to check whether the predicates changed by the excuse are actually in the preconditions, or if any other predicates in the current state should be used instead.
       </list-item>
       <list-item label="•">
        After the rule set candidates are obtained, the score is calculated (using Eq. (1)) for each. If the score is at least equal to the current rule set score, then the candidate can explain past experiences as well as the current rule set, and thus it is a good candidate. Note that although many candidates are usually generated, the score can be calculated very rapidly and most of the candidates are pruned during this step.
       </list-item>
       <list-item label="•">
        Finally, the planner uses good candidates to plan from the current state and a candidate is selected as the output rule set if it yields a plan.
       </list-item>
      </list>
     </section>
     <section label="5.4">
      <section-title>
       Subgoals
      </section-title>
      <paragraph>
       Help is requested from a teacher if the planner cannot obtain a sequence of actions to complete the task. However, the teacher may be asked to perform an action that has to be executed after executing other actions that the system already knows. In some scenarios, it is useful to get as close as possible to the goal to minimize the number of actions that the teacher has to execute. Thus, subgoals are used to approach the goal. Note that the goal has to be defined as a set of target predicates for this feature to work.
      </paragraph>
      <paragraph label="Definition 4">
       SubgoalGiven a goal {a mathematical formula}G={p1,…,pn}, a subgoal is a set of predicates that belongs to the goal, or the requirements of goal predicates {a mathematical formula}G′={p1,…,pn|pi∈G∨pi∈requirements(G)}. The requirements of a predicate are obtained by following the precondition-effect arcs in the causal graph defined by the rules, {a mathematical formula}requirements(p)={p′∨requirements(p′)|∃ arc(p′,p)∈CGΓ}.
      </paragraph>
      <paragraph>
       With a valid excuse φ, we can generate a subgoal {a mathematical formula}Gφ to complete the task up to the point where φ is needed, as explained in the routine SubgoalGeneration (Algorithm 4). This algorithm starts by initializing the subgoal with the goal predicates and finding the subgoal predicates that have a dependency on the predicates changed by the excuse in the causal graph. These subgoal predicates are considered to be unreachable without the changes defined by the excuse, and thus they must be removed from the subgoal. Instead of only removing those predicates, they are substituted by their dependencies in the causal graph. This process is repeated until all of the subgoal predicates no longer depend on the excuse changes.
      </paragraph>
      <paragraph>
       Fig. 4 shows an example of the Cranfield benchmark domain, where the robot has to mount several parts to complete the assembly. If the robot does not know how to place the front faceplate, the teacher would have to assemble all the other parts before demonstrating how to assemble the faceplate. To reduce the number of demonstrations required, the decision maker can create a subgoal where all of the known parts (the peg and the pendulum) should be mounted before requesting help.
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Experimental results
     </section-title>
     <paragraph>
      In this section, we present the results obtained using the REX-D approach. We compared the following configurations to assess the performance of the different algorithms explained in the present study.
     </paragraph>
     <list>
      <list-item label="•">
       REX: The {a mathematical formula}E3 version of the REX algorithm from Lang et al. The results were obtained from [12].
      </list-item>
      <list-item label="•">
       REX-D basic: The REX-D algorithm (Section 4.1).
      </list-item>
      <list-item label="•">
       REX-D rules: The REX-D algorithm with rule alternatives (Section 5.3). Only excuses that changed one predicate were considered.
      </list-item>
      <list-item label="•">
       REX-D rules+sub: The REX-D algorithm with rule alternatives and subgoals (Section 5.4).
      </list-item>
     </list>
     <paragraph>
      Different scenarios were used to perform these comparisons. First, two problems from the international planning competition [35] were tested in order to make comparisons with other relational RL algorithms: Exploding Blocksworld and Triangle Tireworld. These are interesting problems with dead ends, which make extensive use of relational actions.
     </paragraph>
     <paragraph>
      The robot setup for the REX-D algorithm was devised using the Cranfield benchmark (Section 3.1). To demonstrate the performance of REX-D in this scenario, extensive experiments were conducted using this setup in a VR system. The following features of our approach were assessed.
     </paragraph>
     <list>
      <list-item label="•">
       The capacity to cope with stochastic domains compared with deterministic domains.
      </list-item>
      <list-item label="•">
       The generalization capabilities over similar tasks.
      </list-item>
      <list-item label="•">
       The improvements obtained using rule alternatives in terms of reducing the number of demonstration requests needed (see Section 5.3).
      </list-item>
      <list-item label="•">
       The improvements obtained by using subgoals in terms of reducing the number of demonstration requests (see Section 5.4).
      </list-item>
     </list>
     <paragraph>
      To evaluate the relational algorithms we set the threshold ζ to consider known states and actions. Similar to other RL algorithms [12], [36], we set ζ heuristically because it cannot be derived from theory when using a heuristic learner. To obtain results that are comparable to those produced by REX, we used a value employed in previous studies ({a mathematical formula}ζ=2), which obtains good results in many scenarios. In addition, the decision maker was limited to executing 100 actions per episode. After these actions were executed, the episode was marked as failed and the experiment continued with the following episode. All of the REX-D configurations used Gourmand [37] as the planner.
     </paragraph>
     <section label="6.1">
      <section-title>
       Comparing REX and REX-D
      </section-title>
      <paragraph>
       In this section, we compare the REX and REX-D algorithms. Although the results are not directly comparable due to the addition of demonstrations, these experiments illustrate the improvements obtained by using a teacher that demonstrates a few actions. The domains used for these experiments were taken from the international planning competition in 2008 [35].
      </paragraph>
      <section label="6.1.1">
       <section-title>
        Experiment 1: Exploding Blocksworld (IPPC)
       </section-title>
       <paragraph>
        This domain is an extension of the well-known Blocksworld domain that includes dead ends. The robot employs “pick up” and “put on” actions to position a set of blocks in a given layout. A block that is placed on the table, or another block, has a probability of exploding and destroying the object beneath. After a block is destroyed, it cannot be picked up and no other blocks can be placed on top of it. Destroying the table also implies that the system cannot place blocks on it anymore. To solve these problems, the planner has to take care to avoid destroying the blocks that are important for reaching the solution. The representation of this domain with NID rules comprised four actions represented by six rules, and eight different types of predicates.
       </paragraph>
       <paragraph>
        The results obtained are shown in Fig. 5. The performance of the REX-D algorithm was clearly better because demonstrations allowed it to quickly learn the correct usage of the different actions. By contrast, REX had to execute an average of 120 extra exploration actions to learn how to apply the actions correctly. Given that each action may take several seconds to complete, the amount of time saved can be very important.
       </paragraph>
       <paragraph>
        The main problem for algorithms without demonstrations is finding positive examples. As explained in Proposition 1, we can calculate the number of experiences that would be required to obtain a positive example with the pick-up(X,Y) action in the worst case. For simplicity, we assume that this action does not have deictic references. As {a mathematical formula}Pr=16, {a mathematical formula}Ω(216) different experiences would be required in the worst case. Nevertheless, having one positive example, such as pick-up(box5, box3), allows its preconditions to be learned with just 16 different experiences, i.e., one to test each of the 16 predicates in the state that may be related to the action, as explained in Proposition 2. Although the actual number of experiences required to learn this domain is much lower (the rule only has four preconditions and the initial state also limits the number of incorrect experiences that can be performed), these theoretical numbers reflect the reduction in complexity obtained through demonstrations.
       </paragraph>
       <paragraph>
        Using rule alternatives and subgoals reduced the number of demonstrations required. However, the alternative paths that they generated sometimes led the robot to a dead end, whereas demonstrations took the optimal path.
       </paragraph>
       <paragraph>
        The number of teacher interactions varied between five and seven, which was sufficiently low considering the improvements in the overall number of actions required and the improvements in the success ratio. It should also be noted that the teacher was no longer required after the fifth episode.
       </paragraph>
      </section>
      <section label="6.1.2">
       <section-title>
        Experiment 2: Triangle Tireworld (IPPC)
       </section-title>
       <paragraph>
        In this domain, a car has to move to its destination, but it has a probability of getting a flat tire while it moves. The car starts with no spare tires but it can pick them up in some locations. The actions available in this domain are: a “Move” action to go to an adjacent position, a “Change Tire” action to replace a flat tire with a spare tire, and a “Load Tire” action to load a spare tire into the car if there are any in the current location. The main difficulty in the Triangle Tireworld domain is the dead end when the agent gets a flat tire and no spare tires are available. Safe and long paths exist with spare tires, but the shortest paths do not have any spare tires.
       </paragraph>
       <paragraph>
        The results obtained in this domain are shown in Fig. 6. The difficulty of this domain was the dead ends that the agent encountered when it had a flat tire and no spare tires were available. Both algorithms failed to learn how to change a tire if no flat tires were obtained in a location with a spare tire during the initial episodes. After they considered the actions as known, they always took the shortest dangerous path because they did not know that tires could be changed. The main problem for REX is that it moved through random paths initially and it could become confident about its model before learning how to fix a tire. REX-D exhibited better behavior because the initial demonstrations already moved the agent through roads with spare tires and a flat tire could be replaced if the car had a spare tire in its location. REX-D also required more actions per episode because it selected longer and safer paths.
       </paragraph>
       <paragraph>
        Because it was easy to learn the preconditions in this domain and the difficulty was avoiding dead ends, rule alternatives and subgoals did not improve the results.
       </paragraph>
      </section>
     </section>
     <section label="6.2">
      <section-title>
       Cranfield benchmark scenario
      </section-title>
      <paragraph>
       The REX-D algorithm was devised in the framework of the EU Project IntellAct [1]. In this project, the robot has to learn how to complete the Cranfield benchmark assembly with the help of a teacher. The robot starts with no previous knowledge and it learns different actions based on teacher demonstrations and experience. The decision maker that uses the REX-D algorithm is in charge of high-level reasoning, where it decides when to request teacher demonstrations and which actions to execute. Although REX-D has been integrated successfully into a robot platform [6] as described in Section 3, all of the results presented in this section were executed by a simulated robot in the VR setup because it permitted large numbers of replicates to be generated to allow a meaningful statistical analysis of the performance of our algorithms.
      </paragraph>
      <paragraph>
       The state that defines the scene sent to the decision maker included the following information.
      </paragraph>
      <list>
       <list-item label="•">
        The parts of the assembly laid horizontally on the table.
       </list-item>
       <list-item label="•">
        The parts that could be grasped.
       </list-item>
       <list-item label="•">
        The holes that were free.
       </list-item>
       <list-item label="•">
        The parts that were already placed correctly.
       </list-item>
      </list>
      <paragraph>
       In this section, we present several experiments based on the Cranfield benchmark scenario. These experiments were performed in the VR system since a very large number of teacher requests were required to obtain statistically significant results, which would have been tedious for a human teacher. Therefore, an automated teacher that planned with the correct rule set was used to perform the experiments described in this section. This automated teacher also considered the guidance provided by the system when generating plans. Because it obtained optimal policies, the results were the same as those that would have been obtained with a human teacher.
      </paragraph>
      <section label="6.2.1">
       <section-title>
        Experiment 3: Standard Cranfield benchmark
       </section-title>
       <paragraph>
        We tested how well REX-D performs in stochastic domains compared with deterministic domains. These experiments were performed using the standard Cranfield benchmark, where we changed the success probability of the actions to 60% for the stochastic case. There were no dead ends and the only change was the success ratio of the actions; thus, the differences in the results simply indicated the changes obtained with noise when learning.
       </paragraph>
       <paragraph>
        Fig. 7 shows the results obtained. The REX-D algorithm produced good results in both the deterministic and stochastic domains. By contrast, REX had to explore intensively, especially in the stochastic case. Using REX-D with rule alternatives significantly reduced the number of demonstrations requested. In the deterministic case, only the initial demonstrations were required, while only a few requests were made in the probabilistic case.
       </paragraph>
      </section>
      <section label="6.2.2">
       <section-title>
        Experiment 4: Generalizing tasks
       </section-title>
       <paragraph>
        In this experiment, the decision maker had to complete different tasks. The changes in the task required new actions to be learned to solve problems that had not been presented previously. Three different variants of the Cranfield benchmark were used.
       </paragraph>
       <list>
        <list-item label="•">
         Episodes 1–4: The standard Cranfield benchmark was used.
        </list-item>
        <list-item label="•">
         Episodes 5–8: The task started with a peg in a horizontal position. The action to place pegs failed if they were not in a vertical position, and thus the peg had to be repositioned with a new action before placing it.
        </list-item>
        <list-item label="•">
         Episodes 9–12: Initially, the separator was placed without any pegs. To complete the scenario successfully, the robot needed to place the pegs but they could only be placed when the separator was not in place, so it had to be removed first, which required that a new action was demonstrated.
        </list-item>
       </list>
       <paragraph>
        Fig. 8 shows the results obtained with the REX-D algorithm. Each time the task was changed, after executing a few actions, the system realized that it needed a new action and requested a demonstration from the teacher. As more episodes were added, although some changes were made to the task, the results became closer to the optimum. By contrast, when REX-D restarted after a problem change, numerous demonstrations and exploration actions had to be executed to complete the task because everything had to be learned again.
       </paragraph>
      </section>
      <section label="6.2.3">
       <section-title>
        Experiment 5: Goal changes
       </section-title>
       <paragraph>
        This experiment demonstrated the advantages of using subgoals. The decision maker started learning from scratch with the standard Cranfield benchmark assembly and a change was made in the scenario after four episodes, where the goal changed such that a new special front faceplate with no requirement for a pendulum had to be placed.
       </paragraph>
       <paragraph>
        The REX-D algorithm without subgoals requested a demonstration from the teacher before placing any parts because it did not know how to place the new front faceplate, which was required to reach the goal. This new part required all the pegs and the separator to be placed in advance, so the teacher had to position them before teaching the new action. However, the use of subgoals allowed the robot to complete the known parts of the assembly, i.e., the pegs and the separator, before issuing a help request to the teacher to learn only the missing action.
       </paragraph>
       <paragraph>
        Fig. 9 shows the improvement obtained after adding subgoals. The REX-D algorithm with subgoals only requested one demonstration from the teacher, whereas the standard REX-D required that all the actions were executed.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusions
     </section-title>
     <paragraph>
      Learning with real robots is very time consuming if actions take several seconds to complete. Therefore, learning algorithms should require as few executions as possible. We addressed this problem using relational RL with demonstrations. We combined the generalization capabilities of learning in relational domains, where the same type of objects have to be learned only once, with teacher demonstrations, thereby significantly reducing the number of exploration actions required, which is the longest part of the learning process.
     </paragraph>
     <paragraph>
      In the proposed REX-D algorithm, we added demonstration requests to the relational RL algorithm REX. In addition to the faster learning, demonstrations allow the algorithm to learn actions as they are required by extending generalization to different tasks, and new actions can be added if necessary.
     </paragraph>
     <paragraph>
      Another aspect of the proposed method is the use of action rules analysis to help the teacher, which minimizes the number of demonstrations required in a task. The improvements include providing guidance to the teacher so only the unknown actions need to be demonstrated, checking for valid action rule alternatives before requesting new demonstrations, and using subgoals in tasks where demonstrating new actions may require the execution of previously known actions.
     </paragraph>
     <paragraph>
      We performed several experiments to analyze the performance of the REX-D algorithm. Standard domains from the international planning competition were used to compare our approach with other relational RL methods, which demonstrated that significant improvements were obtained. Further experiments were also performed using the Cranfield benchmark encoded in a VR system, which provided the user with a similar experience to a real robot.
     </paragraph>
     <paragraph>
      In conclusion, we proposed a relational RL algorithm with demonstration requests that improves the learning time while aiming to minimize the number of teacher interactions.
     </paragraph>
     <paragraph>
      In future research, it would be useful to adapt REX-D to partially observable domains, where the planner and the learner would have to consider uncertainty. Approaches are available that learn partially observable domains instead of deterministic domains [26], but a new learning algorithm would have to be developed that covers both partially observable and stochastic actions. On the planning side, any partially observable MDP solver can be used.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>