<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Learning Bayesian network parameters under equivalence constraints.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      In machine learning tasks, the examples of a dataset are generally assumed to be independent and identically distributed (i.i.d.). There are numerous situations, however, where this assumption does not hold, and there may be additional information available that ties together the examples of a dataset. We can then, in turn, exploit this background knowledge to learn more accurate models.
     </paragraph>
     <paragraph>
      Consider, as a motivating example, the following scenarios that arise in medical diagnosis, where we would like to learn a model that could be used to diagnose diseases from symptoms. Typically, we would have data consisting of patient records, which we assume to be independent. However, we may obtain further information that ties some of these records together. For example, we may learn that two patients are identical twins, and hence may both be subject to increased risk of certain genetic diseases, i.e., they share the same genetic variants that may cause certain genetic disorders. We may also, for example, learn that two patients were both exposed to a third patient, who was diagnosed with a contagious disease. When learning a model from data, we would like to be able to take advantage of this type of additional information, when it is available.
     </paragraph>
     <paragraph>
      We can view this type of additional information more generally as equivalence constraints that bear on an incomplete dataset, where we may not know the particular value of a variable, but whatever that value is, we know that it must be the same across different examples in our dataset. In this paper, we introduce a simple but principled way to deal with such additional information. In particular, we introduce and formalize the problem of learning under equivalence constraints. We first introduce the notion of a constrained dataset, which implies a corresponding constrained log likelihood. We then define the problem of learning the parameters of a Bayesian network from a constrained dataset, by maximizing the constrained log likelihood.
     </paragraph>
     <paragraph>
      There are a variety of applications, across a variety of different domains, that can be viewed as learning from a constrained dataset. For example, in the information extraction task of named-entity recognition, we seek to label the elements of a text by the type of entity that they refer to (e.g., in an abstract for a talk, we would want to identify those elements that refer to the speaker). Hence, if we see a name that appears multiple times in the same text, we may presume that they all refer to an entity of the same type [1] (an equivalence constraint). As another example, in the task of (vision-based) activity recognition [2], our goal is to annotate each frame of a video by the activity that a human subject is involved in. In this case, a video could be partially annotated by a human labeler, specifying that different frames of a video that depict the same activity (again, an equivalence constraint).
     </paragraph>
     <paragraph>
      Indeed, the notion of an equivalence constraint, for the purposes of learning, has appeared before in a variety of different domains (either implicitly or explicitly), where a variety of domain-specific approaches have been developed for disparate and specialized tasks. One notable domain, is that of semi-supervised clustering [3]. Here, the notion of a must-link constraint was proposed for k-means clustering, to constrain those examples that are known to belong to the same cluster{sup:1}; see, e.g., [4], [5]. For example, when clustering different movies, a user may find that the clusters they learned assigned two different movies to two different clusters, when they should have been assigned to the same cluster (say, based on their personal preferences). In the topic modeling domain, a significantly different approach was proposed to accommodate must-link constraints (based on Dirichlet forest priors), to assert that different words should appear in the same topic (with high probability) [6].
     </paragraph>
     <paragraph>
      In this paper, we show how the different tasks described above can be viewed uniformly as learning a Bayesian network from a dataset that is subject to equivalence constraints. We further propose a simple but principled way of learning a Bayesian network from such a dataset, which is competitive with, and sometimes outperforming, more specialized approaches that were developed in their own domains. Given the simplicity and generality of our approach, we further relieve the need to (a) derive new and tailored solutions for applications in new domains, or otherwise (b) adapt or generalize existing solutions from another domain (both non-trivial tasks).
     </paragraph>
     <paragraph>
      Our paper is organized as follows. In Section 2, we review the task of learning Bayesian networks from incomplete datasets. In Section 3, we introduce the notion of a constrained dataset, and in Section 4 we introduce the corresponding notion of a constrained log likelihood. In Section 5, we consider the problem of evaluating the constrained log likelihood, and in Section 6, we discuss an iterative algorithm for optimizing it. In Section 7, we evaluate our approach for learning Bayesian networks from constrained datasets, further comparing it with more specialized approaches from two different domains: semi-supervised clustering and topic modeling. Finally, we review related work in Section 8, and conclude in Section 9.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Technical preliminaries
     </section-title>
     <paragraph>
      We use upper case letters (X) to denote variables and lower case letters (x) to denote their values. Sets of variables are denoted by bold-face upper case letters (X), and their instantiations by bold-face lower case letters (x). Generally, we will use X to denote a variable in a Bayesian network and U to denote its parents. A network parameter will further have the general form {a mathematical formula}θx|u, representing the probability {a mathematical formula}Pr(X=x|U=u). We will further use θ to denote the set of all network parameters.
     </paragraph>
     <paragraph>
      Given a network structure G, our goal is to learn the parameters of the corresponding Bayesian network, from an incomplete dataset. We use {a mathematical formula}D to denote a dataset, and {a mathematical formula}di to denote an example. Typically, one seeks parameter estimates θ that maximize the log likelihood, defined as:{a mathematical formula} where {a mathematical formula}Prθ is the distribution induced by network structure G and parameters θ. In the case of complete data, the maximum likelihood (ML) parameters are unique and easily obtainable. In the case of incomplete data, obtaining the ML parameter estimates is more difficult, and iterative algorithms, such as Expectation–Maximization (EM) [7], [8], are typically employed.
     </paragraph>
     <paragraph>
      In this paper, we are interested in estimating the parameters of a Bayesian network from a similar perspective, but subject to certain equivalence constraints, which we introduce in the next section. Our approach is largely motivated by the use of meta-networks, which are more commonly used for Bayesian parameter estimation [9], [10]. In a meta-network, the parameters θ that we want to learn are represented explicitly as nodes in the network. Moreover, the dataset {a mathematical formula}D is represented by replicating the original Bayesian network, now called a base network, as many times as there are examples {a mathematical formula}di in the data. Each example {a mathematical formula}di of the dataset {a mathematical formula}D is then asserted as evidence in its corresponding base network. Such a meta-network explicitly encodes an i.i.d. assumption on the dataset {a mathematical formula}D, where data examples are conditionally independent given the parameters θ (which follows from d-separation).
     </paragraph>
     <paragraph label="Example 1">
      Consider a Bayesian network {a mathematical formula}A→B with Boolean variables A and B, and the following incomplete dataset {a mathematical formula}D:{a mathematical formula} Here we have four examples, each row representing a different example {a mathematical formula}di. The symbol ? denotes a missing value. In this example, variable B is fully observed (its value is never missing), whereas variable A is fully unobserved (its value is always missing). The corresponding meta-network for this dataset is depicted in Fig. 1, along with the corresponding plate representation, which is also commonly used [10]. In the meta-network, each example {a mathematical formula}di has a corresponding base network {a mathematical formula}Ai→Bi where instance {a mathematical formula}di is asserted as evidence (observed nodes are shaded). Moreover, the network parameters {a mathematical formula}θA and {a mathematical formula}θB|A are represented explicitly as random variables. Here, the probability of variable A depends on the parameters {a mathematical formula}θA, and the probability of variable B depends on its parent B and the parameters {a mathematical formula}θB|A.
     </paragraph>
     <paragraph>
      In Bayesian parameter estimation, one typically estimates the network parameters by considering the posterior distribution obtained from conditioning the meta-network on the given dataset {a mathematical formula}D. For our purposes, we want to condition instead on the parameter variables, asserting a given parameterization θ (as in maximum likelihood estimation). In this case, one induces a meta-distribution {a mathematical formula}P(.|θ) over the variables of the base networks. If our dataset {a mathematical formula}D is specified over variables X, then let {a mathematical formula}Xi denote the variables of the base-network corresponding to example i in the meta-network. Moreover, let {a mathematical formula}X1:N=∪i=1NXi denote the set of all base-network variables in the meta-network, and let {a mathematical formula}x1:N denote a corresponding instantiation. Our meta-distribution is then:{a mathematical formula} where, again, {a mathematical formula}Prθ(X)=P(Xi|θ) is the distribution induced by a network with structure G and parameters θ. The likelihood of a set of parameters θ is then:{a mathematical formula} where {a mathematical formula}x1:N∼D denotes compatibility between a complete instantiation {a mathematical formula}x1:N and the (incomplete) dataset {a mathematical formula}D, i.e., each {a mathematical formula}x1:N is a valid completion of dataset {a mathematical formula}D. The corresponding log likelihood, of the meta-network, is thus equivalent to the log likelihood of Equation (1):{a mathematical formula} Again, when estimating the parameters of a Bayesian network from data, we typically seek those parameters that maximize the log likelihood. In this paper, we take advantage of this meta-network perspective on this parameter estimation task, as it facilitates the learning of Bayesian networks from constrained datasets, which we discuss next.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Constrained datasets
     </section-title>
     <paragraph>
      As a motivating example, consider the following problem that we may encounter in the domain of medical diagnosis.
     </paragraph>
     <paragraph label="Example 2">
      Consider an incomplete dataset {a mathematical formula}D composed of four medical records:{a mathematical formula} where each row represents a medical record {a mathematical formula}di over four binary features:
     </paragraph>
     <list>
      <list-item label="•">
       “has genetic variant” (V), which can be true or false,
      </list-item>
      <list-item label="•">
       “has diabetes” (D), which can be true or false,
      </list-item>
      <list-item label="•">
       “has increased hunger” (H), which can be true or false,
      </list-item>
      <list-item label="•">
       “has increased thirst” (T), which can be true or false.
      </list-item>
     </list>
     <paragraph>
      Our goal is to take advantage of the type of background information available in Example 2 (in this case, that two records correspond to identical twins), in order to learn more accurate models. In particular, we view this type of background information as an equivalence constraint, which constrains the values of a variable to be the same across different examples in a dataset, although not necessarily to a specific value.
     </paragraph>
     <paragraph>
      More formally, consider the following definition of an equivalence constraint.
     </paragraph>
     <paragraph label="Definition 1">
      Equivalence constraintAn equivalence constraint on a variable X, in a dataset {a mathematical formula}D={d1,…,dN}, is an index set {a mathematical formula}CX⊆{1,…,N}, which constrains the corresponding instances of X to have the same value, i.e., we have that {a mathematical formula}Xi≡Xj in examples {a mathematical formula}di and {a mathematical formula}dj, for all pairs of indices {a mathematical formula}i,j∈CX.
     </paragraph>
     <paragraph>
      We further define a trivial equivalence constraint {a mathematical formula}CX to be one that contains a single index i, i.e., variable {a mathematical formula}Xi must be equivalent to itself, which is vacuous. We denote a set of equivalence constraints on a variable X by {a mathematical formula}CX. Typically, we consider sets {a mathematical formula}CX of equivalence constraints that partition the examples {a mathematical formula}{1,…,N}. Such a partition may also include trivial constraints {a mathematical formula}CX over a single index. We further assume, without loss of generality, that the equivalence constraints {a mathematical formula}CX of a set {a mathematical formula}CX are pairwise disjoint (otherwise, we could merge them into a single equivalence constraint).
     </paragraph>
     <paragraph label="Example 3">
      Consider again the medical dataset of Example 2. In this example, we had an equivalence constraint {a mathematical formula}CV={2,4} that asserts that the state of a genetic variant in Example 2, Example 4 must be the same (either both true, and the variant is present, or both false, and the variant is absent). We can also partition the examples into a set of equivalence constraints {a mathematical formula}CG={{2,4},{1},{3}}, which includes two equivalence constraints which are trivial: {1} and {3}. This dataset also respects the equivalence constraint {a mathematical formula}CT={1,2,3}, as all three examples observe the same value true on the variable T.
     </paragraph>
     <paragraph>
      In general, we may constrain multiple variables X in a dataset. We thus introduce the notion of a constrained dataset.
     </paragraph>
     <paragraph label="Definition 2">
      Constrained datasetA constrained dataset over variables {a mathematical formula}X, is composed of two components: (1) a traditional dataset {a mathematical formula}D={d1,…,dN} over variables X, where each example {a mathematical formula}di is a partial instantiation of the variables; and (2) a set of equivalence constraints {a mathematical formula}C={CX|X∈X} over dataset {a mathematical formula}D, where each {a mathematical formula}CX∈C is a set of equivalence constraints on variable {a mathematical formula}X∈X.
     </paragraph>
     <paragraph>
      Finally, we will in general assume that the values of variables involved in equivalence constraints have hidden values in the data. Constraints on fixed values are not useful here, when they are already equivalent (otherwise, it is not meaningful to assert an equivalence constraint on two variables that are fixed to two different values).
     </paragraph>
    </section>
    <section label="4">
     <section-title>
      Learning with constrained datasets
     </section-title>
     <paragraph>
      Now that we have introduced the notion of a constrained dataset, we can consider the problem of learning from one. Given a traditional dataset {a mathematical formula}D, we would typically want to seek parameter estimates θ that maximize the likelihood of Equation (2). When we subject the dataset {a mathematical formula}D to equivalence constraints {a mathematical formula}C, we propose instead to maximize the likelihood, but conditional on the equivalence constraints {a mathematical formula}C. That is, we seek parameters θ that maximize the constrained likelihood:{a mathematical formula} Next, we show how to represent and evaluate this conditional distribution.
     </paragraph>
     <section label="4.1">
      <section-title>
       Encoding equivalence constraints
      </section-title>
      <paragraph>
       We can encode each individual equivalence constraint {a mathematical formula}CX∈CX for each set {a mathematical formula}CX∈C, locally in the meta-network. In particular, we introduce an observed variable {a mathematical formula}CX, fixed to the value {a mathematical formula}cx, where {a mathematical formula}CX has parents {a mathematical formula}XC={Xi|i∈CX}. The CPT of {a mathematical formula}CX is then:{a mathematical formula}
      </paragraph>
      <paragraph label="Example 4">
       Consider again the simple Bayesian network {a mathematical formula}A→B, and the traditional dataset {a mathematical formula}D, of Example 1. Suppose that we obtain a constrained dataset from {a mathematical formula}D by asserting the constraint that variable A is equivalent in Example 1, Example 2, and variable A is equivalent in Example 3, Example 4, i.e. {a mathematical formula}A1≡A2 and {a mathematical formula}A3≡A4. That is, we have the set of equivalence constraints {a mathematical formula}C={CA} where {a mathematical formula}CA={C1,C2}={{1,2},{3,4}}. In the corresponding meta-network, depicted in Fig. 2, we introduce additional variables {a mathematical formula}C1 and {a mathematical formula}C2 for each equivalence constraint. Variable {a mathematical formula}C1 has parents {a mathematical formula}A1 and {a mathematical formula}A2, and variable {a mathematical formula}C2 has parents {a mathematical formula}A3 and {a mathematical formula}A4. By conditioning on the instantiation {a mathematical formula}C=(C1=c1,C2=c2), we enforce the above equivalence constraints in the meta-distribution {a mathematical formula}P(X1:4|C,θ).
      </paragraph>
      <paragraph>
       Note that a given equivalence constraint {a mathematical formula}CX is independent of all other equivalence constraints (and further, the network parameters θ), given the variables {a mathematical formula}XC, which follows from d-separation.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       The constrained log likelihood
      </section-title>
      <paragraph label="Theorem 4.1">
       As a meta-network induces a log likelihood, a constrained meta-network induces a constrained log likelihood (CLL):{a mathematical formula} To learn the parameters of a Bayesian network, subject to equivalence constraints, we seek to obtain those estimates θ maximizing Equation (3). Consider first the following theorem, that decomposes the constrained log likelihood, into two components. Given a Bayesian network with structure G, parameters θ, and a constrained dataset{a mathematical formula}(D,C), the constrained log likelihood is:{a mathematical formula}where{a mathematical formula}PMI(D,C|θ)=log⁡P(D,C|θ)P(D|θ)P(C|θ)is the pointwise mutual information between the dataset{a mathematical formula}Dand the equivalence constraints{a mathematical formula}C.
      </paragraph>
      <paragraph label="Proof">
       Starting from Equation (3), we have:{a mathematical formula} as desired.  □
      </paragraph>
      <paragraph>
       Maximizing the constrained log likelihood is thus balancing between maximizing the traditional log likelihood and the pointwise mutual information [11] between the data and the constraints (i.e., maximizing the likelihood that the data and constraints appear together, as opposed to appearing independently). Moreover, when there are no equivalence constraints (i.e., {a mathematical formula}C=∅), the constrained log likelihood reduces to the traditional log likelihood {a mathematical formula}LL(θ|D), i.e., the pointwise mutual information {a mathematical formula}PMI(D,C|θ) is equal to zero.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Computing the constrained log likelihood
      </section-title>
      <paragraph>
       To evaluate the traditional log likelihood {a mathematical formula}LL(θ|D)=P(D|θ), as in Equations (1) and (2), it suffices to compute the factors {a mathematical formula}Prθ(di) (in the meta-network, the examples {a mathematical formula}di are independent given the parameters θ). Hence, to compute the log likelihood, we only require inference in the base network (and not in the meta-network), which we assume is tractable, using a jointree algorithm (for example). In general, computing the constrained log likelihood is intractable, as it does not factorize like the traditional log likelihood. In particular, the terms {a mathematical formula}P(D,C|θ) and {a mathematical formula}P(C|θ) do not necessarily factorize, as the equivalence constraints create dependencies across different examples {a mathematical formula}di in the meta-network.
      </paragraph>
      <paragraph>
       In the following section, we consider exact and approximate inference in the constrained meta-network. In order to evaluate the constrained log likelihood, and further to optimize it, we will need to compute (or approximate) the relevant quantities, which are in general intractable to compute. Primarily, we will be concerned with approximate inference, although we will consider some special cases where exact inference in the constrained meta-network is feasible (which are applicable to certain tasks in semi-supervised clustering and topic modeling).
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Inference in the constrained meta-network
     </section-title>
     <paragraph>
      For inference in the constrained meta-network, several alternatives are available. This choice further impacts the subsequent algorithm that we propose for estimating the parameters of a Bayesian network from a constrained dataset.
     </paragraph>
     <paragraph>
      As we just discussed, exact inference is in general intractable in the constrained meta-network, so we must appeal to approximate inference algorithms. Popular approaches include stochastic sampling, (loopy) belief propagation and variational inference. Further, all are commonly used in lieu of exact inference, in EM and in other algorithms, for the purposes of parameter learning [12], [13], [14], [15]. Gibbs sampling and importance sampling, however, are known to be inefficient in the presence of deterministic constraints (such as the equivalence constraints used in our constrained meta-network), requiring exponentially many samples, or slow convergence to the stationary distribution [16], [17], [18].{sup:2} Variational approximations and variational EM offer a number of attractive properties, such as lower bounds on the log likelihood [22]. On the other hand, mean-field approximations also suffer from other problems, such as many local optima, and may lead to coarser approximations, compared to (for example) belief propagation [23], although belief propagation does not provide any bounds, and is not guaranteed to converge [24], [25].
     </paragraph>
     <paragraph>
      For inference in the constrained meta-network, we shall in fact appeal to a class of belief propagation approximations, which are based on the Relax-Compensate-Recover (RCR) framework for approximate inference in probabilistic graphical models; for an overview, see [26]. This choice is particularly suitable for our purposes as RCR is expressly based on relaxing equivalence constraints in a probabilistic graphical model, in order to obtain a tractable approximation (and it is precisely the equivalence constraints that we introduce in a constrained meta-network, that makes inference intractable).
     </paragraph>
     <paragraph>
      More specifically, RCR is an approach to approximate inference that is based on performing exact inference in a model that has been simplified enough to make inference tractable. Here, we apply the Relax and Compensation steps of RCR, without Recovery, which yields an approximation that corresponds to an iterative joingraph propagation (IJGP) approximation [27], [28]. In the extreme case, where a fully-disconnected approximation is assumed, RCR corresponds to the influential iterative belief propagation algorithm (and also the Bethe free energy approximation) [29], [30].
     </paragraph>
     <paragraph>
      For inference in the constrained meta-network, we shall only relax the equivalence constraints {a mathematical formula}C, while performing exact inference in each base-network (which, again, we assume is tractable using, for example, a jointree algorithm). This corresponds to an iterative joingraph propagation algorithm, with a corresponding free energy approximation of the likelihood [27], [28]. Later, we shall also consider some interesting cases where this RCR approximation will be exact.
     </paragraph>
     <section label="5.1">
      <section-title>
       An approximation: relax and compensate
      </section-title>
      <paragraph>
       By asserting the equivalence constraints {a mathematical formula}C in our meta-network, we introduce complexity to its topology, which can make evaluating the constrained likelihood an intractable inference problem. Consider the following example.
      </paragraph>
      <paragraph label="Example 5">
       Consider a Bayesian network structure {a mathematical formula}G→A←S, and the following dataset:{a mathematical formula}Fig. 3 depicts two meta-networks for this example, one without equivalence constraints, and one with equivalence constraints. In the meta-network without equivalence constraints (top), one can verify by inspection that each example {a mathematical formula}di is independent of the other examples (by d-separation), when the values of the parameters θ are clamped. Hence, to compute the log likelihood, it suffices to compute the probability of each {a mathematical formula}di, for a given parameterization θ, independently. However, when we assert equivalence constraints in a meta-network (bottom), this independence (and d-separation) no longer holds. In our example, the base networks of Example 1, Example 3 are tied due to the constraint {a mathematical formula}CS1. Similarly, the base networks of Example 2, Example 4 are tied due to the constraint {a mathematical formula}CG1. In general, as we introduce more constraints, we increase the connectivity among the base networks, which correspondingly makes inference (and evaluating the constrained log likelihood) more challenging. In the extreme case, inference would be exponential in the number of unobserved values in the dataset (which, at worst, would entail summing over all completions of the dataset).
      </paragraph>
      <paragraph>
       Computing the constrained log likelihood is challenging because of the equivalence constraints {a mathematical formula}C that we assert in our meta-network, which may make the topology of the meta-network too complex for exact inference algorithms. Hence, we shall temporarily relax these equivalence constraints, which will make the constrained log likelihood as easy to compute as the traditional log likelihood again. However, just relaxing these constraints may result in a coarse approximation. Hence, we compensate for these relaxations, which shall restore a weaker notion of equivalence, but in a way that does not increase the complexity of performing inference in the meta-network. The RCR framework specifies a particular way to perform this “compensation” step, which we analyze in more depth here, in the context of the CLL. In particular, the “weaker notion of equivalence” is based on a special case where we assert an equivalence constraint on a set of independent variables. We consider this special case, in the following example.
      </paragraph>
      <paragraph label="Example 6">
       Let {a mathematical formula}X1,…,Xk be a set of k variables in a Bayesian network, over the same domain of values. In particular, let {a mathematical formula}xi,s and {a mathematical formula}xi,t denote the s-th and the t-th state of variable {a mathematical formula}Xi. We shall refer to these states by {a mathematical formula}x.,s and {a mathematical formula}x.,t, when the index of the variable is not relevant. Suppose now that variables {a mathematical formula}X1,…,Xk are marginally independent, i.e., {a mathematical formula}Pr(X1,…,Xk)=∏iPr(Xi). To characterize this distribution, it suffices for us to consider the odds:{a mathematical formula} for each variable i, and for each pair of states s and t.{sup:3} Suppose that we assert an equivalence constraint {a mathematical formula}CX over the variables {a mathematical formula}X1,…,Xk. The resulting marginals, and hence the odds, for each variable {a mathematical formula}Xi must also be equivalent. We shall refer to these consensus odds by {a mathematical formula}O(x.,s,x.,t|CX). These consensus odds can be characterized by the original odds {a mathematical formula}O(xi,s,xi,t), prior to conditioning on the equivalence constraint{sup:4}:{a mathematical formula} In other words, when we assert an equivalence constraint on a set of independent variables, the resulting consensus odds are found by simply accumulating the odds of the variables being constrained.
      </paragraph>
      <paragraph>
       Consider an equivalence constraint {a mathematical formula}CX over the variables {a mathematical formula}X1,…,Xk; see Fig. 4 (left). By relaxing, or deleting, each edge {a mathematical formula}Xi→CX in the meta-network (as in RCR), we ignore the dependencies that exist between different examples {a mathematical formula}di, due to the equivalence constraint {a mathematical formula}CX. We can compensate for this relaxation by asserting soft evidence on each variable {a mathematical formula}Xi, which can be used to restore a weaker notion of equivalence; see Fig. 4 (right). In lieu of the equivalence constraint, we use the soft evidence to enforce that the variables {a mathematical formula}X1,…,Xk have at least equivalent marginals. In particular, we will enforce that these marginals correspond to the ones that they would have, as if we asserted an equivalence constraint on independent variables (as in our example above). If the variables {a mathematical formula}X1,…,Xk are indeed independent in the meta-network, then these marginals would be correct.
      </paragraph>
      <paragraph>
       In the special case where there is a single equivalence constraint {a mathematical formula}CX in the meta-network, then the variables {a mathematical formula}X1,…,Xk would indeed be independent (after relaxing the equivalence constraint). Hence, the compensation would yield exact marginals for variables {a mathematical formula}X1,…,Xk.{sup:5} However, in general, these examples may interact through other equivalence constraints on example i. Hence, when we compensate for the relaxation of one equivalence constraint, we may violate the “weaker notion of equivalence” that we restored for a previous equivalence constraint (i.e., the equivalent marginals). In general, however, we can iterate over each equivalence constraint that we relax, and compensate for them until convergence. At convergence, each equivalence constraint that we relaxed will indeed satisfy their respective “weaker notion of equivalence.” This iterative procedure basically corresponds to the iterative algorithm proposed for RCR, for probabilistic graphical models in general [26].
      </paragraph>
      <paragraph>
       Consider again Fig. 4 (right), where we have asserted soft evidence on each variable {a mathematical formula}Xi, where soft evidence, more specifically, is an observation that increases or decreases the belief in an event, but not to the point of certainty [24], [31]. To assert soft evidence on a variable {a mathematical formula}Xi, which was subject to a constraint {a mathematical formula}CX that we relaxed, we need to specify a vector over the values of variables X. This vector, which we denote by {a mathematical formula}λCX(Xi), specifies the strength of our soft evidence. We can implement soft evidence by the method of virtual evidence [24], [31], which introduces a variable {a mathematical formula}Vi as a child of the variable {a mathematical formula}Xi, which is clamped to the value {a mathematical formula}vi, and whose CPT is set according to {a mathematical formula}Pr(vi|xi)=λCX(xi). We further note that a given example i may be involved in multiple equivalence constraints. Let {a mathematical formula}Vi denote the virtual evidence variables introduced to example i by relaxing its equivalence constraints, and let {a mathematical formula}vi denote the corresponding instantiation.
      </paragraph>
      <paragraph>
       We thus want to enforce that each variable {a mathematical formula}Xi, that was constrained by an equivalence constraint {a mathematical formula}CX, to have the consensus odds:{a mathematical formula} Here, {a mathematical formula}Prθ,λ denotes the distribution of the base network parameterized by the CPT parameters θ, but also by the soft evidence parameters λ that we introduced after relaxation. Similarly, {a mathematical formula}Oθ,λ denotes the corresponding odds.
      </paragraph>
      <paragraph>
       Consider again Fig. 4 (right). The odds {a mathematical formula}Oθ,λ(xi,s,xi,t|di,vi) correspond to the odds of {a mathematical formula}Xi, given example {a mathematical formula}di and all soft observations {a mathematical formula}vi introduced for example i. In contrast, the odds {a mathematical formula}Oθ,λ(xi,s,xi,t|di,vi∖Vi) correspond to the same odds of {a mathematical formula}Xi, except that we retract the soft observations {a mathematical formula}Vi for the constraint {a mathematical formula}CX. Hence, to obtain the desired consensus odds, we need to set the soft evidence vector {a mathematical formula}λCx(Xi) to obtain the corresponding odds change (i.e., Bayes factor):{a mathematical formula} (the required strengths of soft evidence satisfy {a mathematical formula}F(xi,s,xi,t)=λCX(xi,s)λCX(xi,t); see, e.g., [31]). As we described before, updating the soft evidence for one equivalence constraint may disturb the consensus odds that were obtained for a previous equivalence constraint. Hence, one typically performs these updates in an iterative fashion, until all updates converge.
      </paragraph>
      <paragraph>
       Once we have relaxed all equivalence constraints, and compensated for them, the resulting meta-network induces a distribution that approximates the original one. In particular, it corresponds to an iterative joingraph propagation (IJGP) approximation [29], [27], [28]. The resulting meta-distribution now factorizes according to its examples, and thus inference in the meta-network requires only inference in the respective base networks, which we assume is tractable using, e.g., a jointree algorithm. In some (restricted) cases, however, the resulting computations may still be exact, allowing us to evaluate the CLL exactly (as well as certain marginals).
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       On computing the CLL exactly
      </section-title>
      <paragraph>
       In some relevant cases, the RCR approximation of the meta-network that we just described, can still be used to compute the CLL exactly. Consider again the CLL:{a mathematical formula} We can approximate the terms {a mathematical formula}P(D,C|θ) and {a mathematical formula}P(C|θ) in the RCR approximation of the meta-network, which corresponds to (corrected) partition functions in the RCR framework [30]. However, these approximations are known to be exact in some known cases [30]. The following proposition characterizes a class of constrained datasets where the above approximation of the CLL will also be exact.
      </paragraph>
      <paragraph label="Proposition 5.1">
       Say we are given a Bayesian network with structure G, parameters θ, and a constrained dataset{a mathematical formula}(D,C), where each example{a mathematical formula}diof the dataset{a mathematical formula}Dis constrained by at most one equivalence constraint in{a mathematical formula}C. For such a constrained dataset, the RCR approximation of the constrained log likelihood is exact. Moreover, the RCR approximations for marginals over families{a mathematical formula}XUare also exact.
      </paragraph>
      <paragraph>
       This special case is interesting because it captures a variety of learning tasks in different domains, which can be reduced to the problem of learning the parameters of a Bayesian network from a constrained dataset.
      </paragraph>
      <paragraph label="Example 7">
       In semi-supervised clustering, we can learn naive Bayes models and Gaussian mixture models with “must-link” constraints, where it suffices to assert at most one constraint on each example. In particular, for each maximal set of examples that are known to belong to the same cluster, we assert a single equivalence constraint. In this case, we can seek to optimize the constrained log likelihood, which we can now evaluate exactly.
      </paragraph>
      <paragraph>
       We remark that Proposition 5.1 follows from the RCR framework. In particular, if we relax (delete) an edge {a mathematical formula}Y→X that splits a network into two independent sub-networks, then we can compensate for the relaxation exactly, and recover the true probability of evidence [30]. This is a generalization of the correctness of the Bethe free energy approximation in polytree Bayesian networks [32], since deleting any edge {a mathematical formula}Y→X in a polytree splits the network into two.
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Optimizing the CLL
     </section-title>
     <paragraph>
      Our task is now to learn the parameters θ of a Bayesian network, from a dataset {a mathematical formula}D that is subject to the constraints {a mathematical formula}C. We propose to seek those parameter estimates θ that maximize the constrained log likelihood, as given in Equation (3){a mathematical formula} However, due to the constraints {a mathematical formula}C, it may be intractable to even evaluate the CLL, for a given candidate set of estimates θ. Hence, we will propose to optimize instead an approximation of the CLL found by relaxing the equivalence constraints {a mathematical formula}C, and then compensating for the relaxations, as dictated by the RCR framework (which we further described in the previous section). This approach to parameter estimation is akin to approaches that use a tractable approximation of the log likelihood, such as one obtained by loopy belief propagation; see, e.g., [14], [15].
     </paragraph>
     <paragraph>
      First, when we relax all equivalence constraints {a mathematical formula}C, we compensate by asserting soft evidence {a mathematical formula}vi on each example i. Moreover, the meta-distribution factorizes according to the examples i. This leads to the following approximation of the constrained log likelihood, that also factorizes according to the examples i:{a mathematical formula} Here, {a mathematical formula}Prθ,λ denotes the distribution of the base network, which is now determined by two sets of parameters: (1) the parameter estimates θ of the Bayesian network that we seek to learn, and (2) the parameters λ of the soft observations {a mathematical formula}vi that are used to compensate for the relaxed equivalence constraints {a mathematical formula}C. Note that we use two sets of compensations {a mathematical formula}λ1 and {a mathematical formula}λ2, to approximate {a mathematical formula}P(D,C|θ) (with the dataset observed in the meta-network) and {a mathematical formula}P(C|θ) (with the dataset unobserved in the meta-network).
     </paragraph>
     <paragraph>
      We are now interested in optimizing our approximation {a mathematical formula}CLL′ of the constrained log likelihood, which is done with respect to the network parameters θ, but also with respect to the soft evidence parameters {a mathematical formula}λ1,λ2. We propose a simple fixed-point iterative algorithm, for doing so, which is summarized in Algorithm 1.
     </paragraph>
     <paragraph>
      Our fixed-point algorithm alternates between updating the soft evidence parameters {a mathematical formula}λ1,λ2, and updating the parameter estimates θ. We first fix the parameter estimates θ and then update the soft evidence parameters {a mathematical formula}λ1,λ2. This corresponds to performing relax-and-compensate in the meta-network, as described in the previous section. We next fix the soft evidence parameters {a mathematical formula}λ1,λ2, and update the parameter estimates θ, which we shall discuss further next. These two steps are repeated, until all parameters converge to a fixed-point.
     </paragraph>
     <paragraph label="Proposition 6.1">
      Fixed-points ofAlgorithm 1are stationary points of the approximation to the constrained log likelihood of Equation(4).
     </paragraph>
     <paragraph>
      Note that optimal parameter estimates, with respect to the (approximate) CLL, must be stationary points of the (approximate) CLL, but not necessarily vice versa.
     </paragraph>
     <paragraph>
      Consider the first partial derivative of the CLL, with respect to a parameter {a mathematical formula}θx|u, for the instantiation {a mathematical formula}xu of family {a mathematical formula}XU (keeping the soft evidence parameters {a mathematical formula}λ1,λ2 fixed):{a mathematical formula} We are interested in the stationary points of the CLL, but subject to sum-to-one constraints on the parameters {a mathematical formula}θx|u. Hence, we construct the corresponding Lagrangian, and set the gradient to zero. We can then obtain the following stationary conditions which we can use in an iterative fixed-point algorithm:{a mathematical formula} We remark further that the above stationary condition for the constrained log likelihood reduces to a stationary condition for the traditional log likelihood, when no equivalence constraints are used:{a mathematical formula} The above stationary conditions further correspond to the EM algorithm for traditional datasets, when used as an update equation in an iterative fixed-point algorithm.
     </paragraph>
     <paragraph>
      In general, however, Algorithm 1 seeks the stationary points of an approximation of the CLL (Equation (4)), which is based on an RCR approximation. RCR is in turn a generalization of loopy belief propagation [26], and hence inherits some of its drawbacks (i.e., it is not guarantee to converge in general, and does not provide bounds). Our learning algorithm, is thus more akin to approaches to parameter estimation that use loopy belief propagation in lieu of exact inference, for example, to approximate the expectations in EM, when such computations are otherwise intractable [14], [15]. Other alternatives, in terms of inference and learning paradigms, could similarly be employed to optimize the CLL. However, as we shall see in the next section, our proposed algorithm performed well, for the purposes of empirically evaluating the CLL, as an approach to learning from constrained datasets.
     </paragraph>
    </section>
    <section label="7">
     <section-title>
      Experiments
     </section-title>
     <paragraph>
      In our first set of experiments, we study the CLL as an objective function for learning Bayesian networks, showing how it can learn more accurate models as more side information, in the form of equivalence constraints, is provided. Subsequently, we consider two different learning tasks in two different domains, that we can reduce to the problem of learning a Bayesian network from a constrained dataset: (1) semi-supervised clustering with naive Bayes models, and (2) topic modeling with domain-specific knowledge.
     </paragraph>
     <section label="7.1">
      <section-title>
       Synthetic data
      </section-title>
      <paragraph>
       We consider first the constrained log likelihood, as an objective function for learning Bayesian networks. In particular, we evaluate our ability to learn more accurate models, as more side information is given. We consider two classical networks, alarm and win95pts, with 37 variables and 69 variables, respectively. We simulated complete datasets from each network, then obtained an incomplete dataset for each by hiding values at random, with some probability. We further simulated equivalence constraints, for each incomplete dataset, by randomly constraining pairs of missing values (which were known to have the same value in the original complete dataset). Our baseline, in this set of experiments, is the standard EM algorithm for traditional (unconstrained) datasets. EM does not incorporate side information in the form of equivalence constraints, and our main goal first is to illustrate the benefits of doing so.
      </paragraph>
      <paragraph>
       Since we are simulating datasets, we measure the quality of an algorithm's estimates using the KL-divergence between (1) the distribution {a mathematical formula}Prθ⋆ induced by the parameters {a mathematical formula}θ⋆ that generated the dataset originally, and (2) the distribution {a mathematical formula}Prθ induced by the parameters θ estimated by the corresponding learning algorithm{sup:6}:{a mathematical formula} where we iterate over all families {a mathematical formula}XU of our Bayesian network, and all parent instantiations u, i.e., we iterate over all CPT columns of the network. Note that the KL-divergence is non-negative, and equal to zero iff the two distributions are equivalent. We also assume pseudo-counts of one, i.e., Laplace smoothing, for both the EM and CLL optimization algorithms. Both algorithms were also initialized with the same random seeds. Further, each algorithm was given 5 initial seeds, and the one that obtained their best respective likelihood was chosen for evaluation.
      </paragraph>
      <paragraph>
       Fig. 5 highlights our first set of experiments where we observe the increase in quality of our parameter estimates (on the y-axis), as we assert equivalence constraints on more variables (on the x-axis). Here, each plot point represents an average of 30 simulated datasets of size 2{sup:9}, where values were hidden at random with probability 0.4. On the x-axis, from left-to-right, we progressively selected a larger number of variables to subject to constraints. For each constrained variable, we randomly sampled a proportion of the hidden values under constraints; the proportions that we constrained were also varied, using separate curves.
      </paragraph>
      <paragraph>
       At one extreme, when no variables are constrained, our CLL optimization algorithm performs the same as EM, as expected. As we move right on the plot, where we select more variables to constrain, we see that our CLL optimization algorithm does, generally speaking, produce more accurate parameter estimates, as compared to EM (which does not exploit equivalence constraints). Based on the individual curves, from top-to-bottom, where we increase the proportion of missing values that are subject to constraints, we see the quality of the estimates generally improve further. We also observe that, as we approach an overly large number of constraints, the quality of our estimates appear to degrade. This is also expected, as the optimization algorithm that we proposed assumes an approximation of the CLL, based on relaxing (and then compensating for) all equivalence constraints, which we expect to be coarse when many equivalence constraints must be relaxed.
      </paragraph>
      <paragraph>
       Fig. 6 highlights our second set of experiments, where we measure now the relative improvement of our CLL optimization algorithm compared to EM, on the y-axis. Here, each point is an average over 120 simulated datasets of a fixed dataset size where values were hidden with probabilities 0.2, 0.4, and 0.6. Further, 20 variables were constrained and half the missing values were covered by equivalence constraints. As we move right-to-left on the x axis, where we decrease the size of the dataset, we find that our CLL optimization algorithm is increasingly effective compared to EM, as less data is available, at least up to a point (at dataset size {a mathematical formula}N=128, which is a relatively small amount of data, the effectiveness starts to diminish again). This highlights how important additional side-information becomes as less data is available. We also observe the increased effectiveness as more values are missing in the data (going from the bottom curve to the top one).
      </paragraph>
     </section>
     <section label="7.2">
      <section-title>
       Application: semi-supervised clustering
      </section-title>
      <paragraph>
       In our first illustrative application, we are interested in clustering tasks where we have must-link constraints that are used to constrain examples that are known to belong in the same cluster; see, e.g., [4]. We consider semi-supervised clustering here with naive Bayes models in particular, where the constrained variable is the class variable. Unlike our previous set of experiments, we are able in this case to evaluate the constrained log likelihood exactly. We compare our method with the EM algorithm proposed by [33] (originally proposed for Gaussian mixture models, but adapted here for naive Bayes models), which we refer to by em-sbhw. Note that em-sbhw was specifically proposed for the task of semi-supervised clustering, under the presence of must-link constraints. We also compare with the traditional EM algorithm, as a baseline, which again does not take advantage of any equivalence constraints.
      </paragraph>
      <paragraph>
       We use datasets from the UCI Machine Learning repository, and in some cases, used Weka to discretize data with continuous values, and to fill-in missing values by mean/mode, as is commonly done in such evaluations. We start with complete datasets where the true clusterings are known, and then generate equivalence constraints based on these known labels. We then hide these labels to yield an incomplete dataset. We follow the approach of [33], where we first partition the dataset into K partitions ({a mathematical formula}K=5) and then randomly select a fixed number m of examples from each. In each of these m selected examples, we asserted an equivalence constraint across those examples that had the same labels. Note that this yields equivalence constraints of varying sizes. Effectively, {a mathematical formula}K⋅m controls the amount of side-information available. When {a mathematical formula}K⋅m equals the number of examples N, then every example in the dataset is subject to an equivalence constraint. Given a constrained dataset, we evaluate the results of each algorithm based on their performance in clustering, using the F-measure, which is computed using {a mathematical formula}f=2PRP+R, where P and R are the precision and recall, respectively.
      </paragraph>
      <paragraph>
       Here, we steadily increase the amount of side-information available, from no side-information to perfect side-information (in the latter case, all examples that were assigned to the same cluster in the original complete dataset, were constrained to be in the same cluster in the incomplete dataset that we evaluated). In Fig. 7, we observe the increase in F-measure as we increase the number of equivalence constraints made available. With no equivalence constraints given, all algorithms evaluated were equivalent to vanilla EM. As we increase the number of constraints, we see that both cem and em-sbhw exhibit smoothly increasing performance, and in some cases obtaining perfect clusterings when all known equivalences were provided. In datasets lymph and tumor, we see that our CLL optimization algorithm is superior. In dataset credit, we see that em-sbhw is mildly favorable. In most of the datasets, however, we see that both algorithms perform similarly. Hence, this suggests that our general approach, based on optimizing the CLL, is comparable (and sometimes outperforming) a domain-specific algorithm developed for the relatively specialized task of semi-supervised clustering.
      </paragraph>
     </section>
     <section label="7.3">
      <section-title>
       Application: topic modeling
      </section-title>
      <paragraph>
       We consider another application in topic modeling [35], where we want to assert some domain knowledge, in the form of equivalence constraints, in order to learn improved topics. In particular, one could use equivalence constraints to interactively refine the topics learned by a topic model. For example, a practitioner may inspect the topics assigned to the individual words of a document, and may observe that some words are associated with topics that are not reasonable, based on their background knowledge. In particular, they may find words that are associated with different topics, that they judge should be in the same topic. In this case, the practitioner can assert an equivalence constraint between the topics of these words, to encourage the topic model to associate the same topic with them. In this way, a practitioner can “debug”, or otherwise have some refined mechanism to control, the topics that are learned by a topic model.
      </paragraph>
      <paragraph>
       We next present some experiments, illustrating another example of how equivalence constraints can be used to refine the topics of a topic model. In particular, we propose to introduce equivalence constraints, in order to encourage the formation of new topics, whose words are otherwise scattered across disparate topics. The dataset that we consider here consists of 539 abstracts from the Journal of Artificial Intelligence Research (JAIR). The corpus covers a broad range of different topics related to AI, for example, agent-based systems, heuristic search and logical reasoning (as analyzed by the annual reports, produced by the editors of JAIR).
      </paragraph>
      <paragraph>
       We first learn from this corpus a standard LDA model, over 10 topics (where the number of topics here is based roughly on the number of topics manually identified in the annual reports of JAIR).{sup:7} A Bayesian network representation of the LDA model is provided in Fig. 8. The top words of each topic are illustrated in Table 1. For example, topic 7 can clearly be interpreted as a planning topic.
      </paragraph>
      <paragraph>
       Suppose now that a practitioner decides that they want other topics, based on background knowledge, to emerge from the model. Take for example, a possible “complexity” topic (which is not purely an AI topic, per se, but a topic frequently discussed in AI papers). Words, such as “complexity”, or “NP”, or “polynomial”, when appearing together in the same abstract, may plausibly come from the same topic, about “complexity.” Hence, we propose to impose an equivalence constraint among the topics of these words, if they appear in the same abstract. Such an equivalence constraint would encourage the topic model to include these set of words in the same topic, and perhaps encourage the formation of a new topic, around these words.
      </paragraph>
      <paragraph>
       Consider Table 2, where we have learned an LDA model while optimizing the CLL, based on the above constraints. We see that in topic 0, a new topic has formed, which contain the above words “complexity”, “NP”, and “polynomial” (in boxes). Moreover, we see that the other words in the topic are also strongly reminiscent of a “complexity” topic. In addition to the above constraints, we also constrained the topics of the words “Bayesian” and “inference” and “network”, when they appeared together in the same abstract. We see also in topic 6, another topic has appeared around these words (in boxes), reminiscent of a “Bayesian network modeling and inference” topic.
      </paragraph>
      <paragraph>
       We also compared with a more specialized method, which incorporates analogous (must-link) constraints via a Dirichlet forest prior (LDA-DF) [6], learned using the same constraints described above. First, we consider another visualization of the topics, as in [6], in Table 3. Here, we want to visualize whether our constrained words appear in the same topic, as desired, or whether they are dispersed across different topics. More specifically, we sorted the words of each learned topic by probability, and then selected the 50 most probable words for each topic. We then observed whether or not our selected keywords appeared as one of the most probable words, for each topic (if so, we denote it by a dot in Table 3).
      </paragraph>
      <paragraph>
       In vanilla LDA, we found that our selected keywords were indeed dispersed across multiple topics. For example, the word “complexity” appears strongly in multiple topics (topics 5 and 6). By asserting the background knowledge in the form of constraints (CLL), we see that we are indeed able to learn a more specialized “complexity” topic, whose words are strongly associated to it (and none other). We observe similarly for LDA-DF. In Table 4, we visualize the topics learned by LDA-DF, more directly. We can see, in topic 0, a topic where all of our “complexity” keywords appear strongly. However, in contrast to topic 0 learned in Table 2 for LDA-CLL, we see that this topic is much less reminiscent of “complexity” (it appears to be a topic much broader than “complexity”), and did not encourage as focused a topic, as suggested by the constrained keywords, to the extent that LDA-CLL did.
      </paragraph>
     </section>
    </section>
    <section label="8">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      As we discussed in our experiments, there are learning tasks in a variety of domains, where specialized methods were developed, that can be formulated as learning a Bayesian network under a constrained dataset. For example, the work of [33] proposed to learn Gaussian mixture models (GMMs) under equivalence constraints to improve clustering. Moreover, [6] considered the use of equivalence constraints in topic models, albeit less directly, to constrain words that should have similar importance in different topics. Here, a Dirichlet forest prior was used to accommodate such constraints, with a corresponding Gibbs sampling method for learning the parameters of the topic model.
     </paragraph>
     <paragraph>
      The works of [33], [36] are more closely related to our proposal. Rather than connect two meta-network variables {a mathematical formula}Xi and {a mathematical formula}Xj to a common child, which constrained them to take the same value, these proposals effectively assumed a direct edge {a mathematical formula}Xi→Xj, where the CPT of {a mathematical formula}Xj was set so that variable {a mathematical formula}Xj assumes the value of variable {a mathematical formula}Xi, i.e., {a mathematical formula}Pr(xj|zi)=1 iff {a mathematical formula}xi=xj. This induces a meta-network with a different type of structure and likelihood, where equivalent variables were merged into a single node. However, the type of equivalence constraint implied by “merging” equivalent variables {a mathematical formula}Xi→Xj, does not generalize obviously when variable X is not a root node.
     </paragraph>
    </section>
    <section label="9">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      In this paper, we proposed a general framework for learning Bayesian network parameters under equivalence constraints. These constraints assert that the values of unobserved variables in certain examples of a dataset must be the same, even if that particular value is not known. We proposed a notion of a constrained dataset, and a corresponding constrained log likelihood. We proposed a fixed-point iterative algorithm for optimizing the constrained log likelihood, and showed empirically that it can be effective at learning more accurate models given more background knowledge in the form of equivalence constraints. We further highlighted how our framework naturally models tasks in a variety of domains, where often domain-specific and (sometimes) less principled approaches have been previously proposed.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>