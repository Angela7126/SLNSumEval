<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    The MADLA planner: Multi-agent planning by combination of distributed and local heuristic search.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      If planning is to be used in large-scale personal, corporate or military applications, multiple independent entities will need to cooperate during the plan synthesis process. As witnessed in many other applications, such independent entities may have serious concerns in protecting the privacy of their input data. Privacy-preserving multi-agent planning allows the definition of factors of the global planning problem private to the respective entities (i.e., agents). The recently most prevalent model for Multi-Agent Planning (MAP) goes back to the roots of research in planning itself—to STRIPS. The STRIPS model [1] formalized planning by a propositional description of the world and the actions, together forming a transition system representing the planning problem. An extension of STRIPS towards MAP denoted as MA-STRIPS[2] generalizes the model by allowing more than one finite set of actions, each characterizing capabilities of one agent. Since not all agents must necessarily be capable of influencing the whole environment, some parts of the information about it can be private to a single agent, alongside the common information public to all agents.
     </paragraph>
     <paragraph>
      In contrast to previous models, MA-STRIPS targets cooperative domain-independent multi-agent planning for discrete and deterministic environments. In this setting, each agent is planning for itself, communicating with other agents in order to ensure agreement on common coordination points. Although execution is not explicitly considered (we are talking about off-line planning), it is assumed that the agents will execute the plans in parallel, interacting at the mentioned coordination points. The privacy in MA-STRIPS can be also understood as a special case of partial observability, where the agents should not know, observe and/or use private knowledge of other agents. Although the privacy in MA-STRIPS is induced (i.e., implied by the partitioning of the problem), the principles described in this paper are similarly applicable when the privacy is explicitly defined as a part of the input.
     </paragraph>
     <paragraph>
      The real-world motivation for MA-STRIPS spans over a wide variety of problems [3], as in classical planning [4]. The examples can be a consortium of cooperating logistic companies with common transportation tasks, but private know-how about local transport possibilities; a team of spatially separated gas station inspectors with a common goal to analyze quality of gasoline in the whole country, but with private knowledge about the quality of particular gas stations; or a heterogeneous fleet of satellites and rovers surveying a distant planet, for which keeping local information private is the only feasible way not to overload the communication network. In these motivational cases, MA-STRIPS partitioning of actions would be defined over trucks, inspectors, rovers and satellites. The know-how of mentioned corporations, knowledge of inspectors and local information of robots would define private knowledge in MA-STRIPS and prevent sharing all of the information freely among all the agents.
     </paragraph>
     <paragraph>
      As in classical planning, forward-chaining state-space search guided by an automatically derived heuristic is a well established technique for multi-agent planning. A multi-agent variant of the well-known A* search algorithm was proposed as Multi-Agent Distributed A* (MAD-A*) in [5] and the multi-agent variant of general best-first search as Multi-Agent Best-First Search (MA-BFS) in [3], [6].
     </paragraph>
     <paragraph>
      The MAD-A* planner uses classical heuristics restricted to projected problems, that is each particular agent is using the heuristics only on the part of the MAP problem it has access to. Such projection can substantially underestimate the cost as it does not consider private actions of any other agent. A seeming remedy is to distribute the process of heuristic estimation among all agents such that each agent preserves its privacy by not communicating anything else than its partial heuristic estimations.
     </paragraph>
     <paragraph>
      One of the most prominent classical planning heuristics still in use is the Fast-Forward (FF) [7] heuristic. A distributed estimator for the FF heuristic, proven to return the same estimations as centralized FF, is MA-FF [8] (based on building a distributed form of a Relaxed Planning Graph [9]). MA-FF is, however, practically inefficient, more precisely, the results indicate that there is a trade-off between the quality of the heuristic estimation and the efficiency of its computation. This is similar to classical planning, where the computation of more informed heuristic is typically more time consuming. In multi-agent planning, the communication overhead of the distributed heuristic adds to the complexity of more informed heuristic and must be considered. Two distributed variants of MA-FF focusing on practical improvement are lazily computed lazyFF [8], [6] and rdFF using recursive distributed computation [6]. Although both heuristics improve efficiency of the underlying search in contrast to MA-FF, the projected variant of FF still performs better in a non-negligible number of planning problems, because of its computational ease and lack of communication requirements [6]. A next step forward we take in this article is to combine the projected and distributed variants of FF.
     </paragraph>
     <paragraph>
      The principle of combining multiple heuristic estimators is well known in classical planning as multi-heuristic search [10], [11], yet, it was never analyzed in the context of MAP. Specifically, the way to combine projected and distributed heuristic estimators in the multi-agent distributed state-space search resulting in an efficient planning approach, is an open problem. This is because the methods used in classical planning are clearly not suitable for this class of heuristics. As the main contribution of this article, we address this issue by designing a new search scheme, proving its soundness and completeness and evaluating it experimentally.
     </paragraph>
     <paragraph>
      In practice, most of the MA-STRIPS-based multi-agent planners operate similarly to the classical planners. They are off-line, in the sense that the planner is presented with an input describing the planning problem, runs for some given time and either returns a solution, or reports that no solution has been found. The common input description language in classical planning is PDDL [12], but there is no standard in multi-agent planning (the recently introduced MA-PDDL [13] attempts to become one).
     </paragraph>
     <paragraph>
      Similarly to many other multi-agent planners, the MADLA planner starts with a classical PDDL input and a description of which PDDL objects are actually agents. After grounding the PDDL input to an internal representation of the global planning problem, it is partitioned to the agents and private and public parts are determined automatically, based on the properties of the problem and the MA-STRIPS definition.
     </paragraph>
     <paragraph>
      In particular, each action is assigned to the agent which (its respective PDDL object) first appears in the action parameters. Each action is assigned to exactly one agent (the first agent in the parameters), but the action may refer to other agents (their respective PDDL objects) as objects of the action. Imagine a construction crane and a truck, each operated by an agent, the process of loading a concrete slab by the crane into the truck is an action done solely by the crane, although the truck is involved in the action only as a target (object), therefore the action is owned by the crane or more precisely the agent representing the crane. There are no actions without an assigned agent and no joint actions requiring co-operation of two or more agents and no actions without an agent. This places some requirements on the domain designer and also the domain design influences which objects are agents and which actions belong to which agent. Based on this factorization, the public facts are determined. The public facts are those shared among at least two agents (according to the MA-STRIPS definition) or appearing in the common goal. All other facts are private. Subsequently, actions which operate with public facts are public.
     </paragraph>
     <paragraph>
      The planner is run so that each agent uses its own thread and memory space, communicating with each other only through message passing via a TCP-IP connection (possibly on a network). An alternative way to provide an input to a multi-agent planner is such each agent receive its own separate (possibly PDDL) input, with factorization and privacy defined explicitly (possibly not adhering to the MA-STRIPS definition). This option is currently not supported by the MADLA Planner, but the techniques and algorithms presented in this paper are well suited for such approach (and the used formalism even suggests such use).
     </paragraph>
     <paragraph>
      Throughout the paper, we use a small logistics example depicted in Fig. 1. In this example, there are two agents, a truck {a mathematical formula}t and a plane {a mathematical formula}a (naturally, the agents could be the drivers, the logistic companies, etc. here we adhere to the benchmark domains commonly used in multi-agent planning literature). They are transporting a package {a mathematical formula}p from city {a mathematical formula}A to city {a mathematical formula}C, while the truck can move between {a mathematical formula}A and {a mathematical formula}B and the plane between {a mathematical formula}B and {a mathematical formula}C.
     </paragraph>
     <paragraph>
      The paper is structured as follows. First, we define a formalism based on MA-STRIPS and define privacy in MAP. Next, we describe the novel planning system, MADLA Planner, with detailed focus on the new variant of the distributed FF heuristic and the novel distributed multi-heuristic search scheme. We prove the soundness and completeness of the search algorithm and provide analysis of privacy of both the search and the heuristics. In the last section, we evaluate the planner by analyzing the performance of the particular building blocks, comparing it with a centralized solution and evaluating it against other comparable state-of-the-art multi-agent planners.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Multi-agent planning
     </section-title>
     <paragraph label="Definition 1">
      The global Ma-STRIPS definition becomes confusing when describing more complex distributed algorithms, therefore we use a more suitable definition. To describe a multi-agent planning problem, where each agent is planning for itself, we explicitly define a set of classical STRIPS problems, one for each agent. Otherwise, we keep the MA-STRIPS treatment of privacy mentioned above. We also explicitly define that if a fact is a goal fact, it is public, resulting in one common public goal state definition. This is a simplification{sup:1} assumed by most MA-STRIPS planners, but not explicitly stated in the MA-STRIPS definition. The formal definition is as follows. Let {a mathematical formula}A={αi}i=1n be a set of n agents and P be a set of propositions describing the world, where {a mathematical formula}s⊆P is a state of the world. Then {a mathematical formula}M={Πi}i=1n is a multi-agent planning problem where for each agent {a mathematical formula}αi∈A, an {a mathematical formula}αi-agent planning problem is a quadruple {a mathematical formula}Πi=〈Pi⊆P,Ai,sI∩Pi,G〉, where:
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}Pi is a finite set of propositions describing facts about the world relevant to agent {a mathematical formula}αi.
      </list-item>
      <list-item label="•">
       {a mathematical formula}Ai is a finite set of actions agent {a mathematical formula}αi can perform.
      </list-item>
      <list-item label="•">
       {a mathematical formula}sI⊆P is the initial state of the world, each agent observes only its part {a mathematical formula}sI∩Pi,
      </list-item>
      <list-item label="•">
       {a mathematical formula}G⊆Ppub is the common goal condition defining the goal (final) states of the problem; a state s is a goal state iff {a mathematical formula}G⊆s.
      </list-item>
     </list>
     <paragraph>
      For the example from Section 1, the truck problem {a mathematical formula}Πt consists of the following (public facts and actions are bold):
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}Pt={truck-at-A,truck-at-B,package-at-A,package-at-B,package-in-t,package-at-C}
      </list-item>
      <list-item label="•">
       {a mathematical formula}At={move-t-A-B,move-t-B-A,load-t-A,load-t-B,unload-t-A,unload-t-B}{a mathematical formula}move-t-A-B=〈{truck-at-A},{truck-at-B},{truck-at-A},move-t-A-B,t〉{a mathematical formula}move-t-B-A=〈{truck-at-B},{truck-at-A},{truck-at-B},move-t-B-A,t〉{a mathematical formula}load-t-A=〈{truck-at-A,package-at-A},{package-in-t},{package-at-A},load-t-A,t〉{a mathematical formula}load-t-B=〈{truck-at-B,package-at-B},{package-in-t},{package-at-B},load-t-B,t〉{a mathematical formula}unload-t-A=〈{truck-at-A,package-in-t},{package-at-A},{package-in-t},unload-t-A,t〉{a mathematical formula}unload-t-B=〈{truck-at-B,package-in-t},{package-at-B},{package-in-t},unload-t-B,t〉
      </list-item>
      <list-item label="•">
       {a mathematical formula}sI∩Pt={truck-at-A,package-at-A}
      </list-item>
      <list-item label="•">
       {a mathematical formula}G={package-at-C}
      </list-item>
     </list>
     <paragraph>
      and the plane problem {a mathematical formula}Πa:
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}Pa={plane-at-B,plane-at-C,package-at-B,package-in-a,package-at-C}
      </list-item>
      <list-item label="•">
       {a mathematical formula}Aa={move-a-B-C,move-a-C-B,load-a-C,load-a-B,unload-a-C,unload-a-B}{a mathematical formula}move-a-B-C=〈{plane-at-B},{plane-at-C},{plane-at-B},move-a-B-C,a〉{a mathematical formula}move-a-C-B=〈{plane-at-C},{plane-at-B},{plane-at-C},move-a-C-B,a〉{a mathematical formula}load-a-C=〈{plane-at-C,package-at-C},{package-in-a},{package-at-C},load-a-C,a〉{a mathematical formula}load-a-B=〈{plane-at-B,package-at-B},{package-in-a},{package-at-B},load-a-B,a〉{a mathematical formula}unload-a-C=〈{plane-at-C,package-in-a},{package-at-C},{package-in-a},unload-a-C,a〉{a mathematical formula}unload-a-B=〈{plane-at-B,package-in-a},{package-at-B},{package-in-a},unload-a-B,a〉
      </list-item>
      <list-item label="•">
       {a mathematical formula}sI∩Pa={plane-at-B}
      </list-item>
      <list-item label="•">
       {a mathematical formula}G={package-at-C}
      </list-item>
     </list>
     <paragraph>
      The locations of the truck and plane are private to the respective agents as well as the location of the package unless it is in the cities {a mathematical formula}B or {a mathematical formula}C. The location of the package in {a mathematical formula}B is public because it is shared by the two agents, whereas the package in location {a mathematical formula}C is public because {a mathematical formula}package-at-C is a public goal. Because of that, also the {a mathematical formula}unload-t-B, {a mathematical formula}unload-a-C and the respective load actions are public.
     </paragraph>
     <paragraph>
      The agents are not restricted only to their {a mathematical formula}Πi problems, as each agent is aware of other agents' public actions in the form of projections. An i-projection of an action {a mathematical formula}a∈Aj is defined as{a mathematical formula}
     </paragraph>
     <paragraph>
      The i-projected action {a mathematical formula}a▷i where {a mathematical formula}a∈Aj is restricted only to the public facts {a mathematical formula}Ppub, as no other facts than {a mathematical formula}Ppub are shared among agents {a mathematical formula}αi and {a mathematical formula}αj, thus we can omit the agent index i and write simply{a mathematical formula} Note also that the label {a mathematical formula}lbl(a) and the agent {a mathematical formula}αj owning the original action a is retained in the projection.
     </paragraph>
     <paragraph>
      An i-projected problem is defined as{a mathematical formula}
     </paragraph>
     <paragraph>
      An i-projected state {a mathematical formula}s▷i is a global state {a mathematical formula}s⊆P restricted to {a mathematical formula}Pi, formally {a mathematical formula}s▷i=s∩Pi. Analogously a publicly projected state {a mathematical formula}s▷pub is a global state {a mathematical formula}s⊆P restricted to {a mathematical formula}Ppub, formally {a mathematical formula}s▷pub=s∩Ppub.
     </paragraph>
     <paragraph>
      Although the agent {a mathematical formula}αi cannot plan using {a mathematical formula}Πi▷, as the resulting plan might not be executable in the multi-agent problem {a mathematical formula}M, it can be used for heuristic computation (the projected problem is in fact an abstraction of {a mathematical formula}M).
     </paragraph>
     <paragraph>
      In the example, the problem {a mathematical formula}Πt▷ projected to agent {a mathematical formula}t differs from {a mathematical formula}Πt in that it contains the additional projected actions, that is
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}load-a-C▷=〈{package-at-C},∅,{package-at-C},load-a-C,a〉
      </list-item>
      <list-item label="•">
       {a mathematical formula}load-a-B▷=〈{package-at-B},∅,{package-at-B},load-a-B,a〉
      </list-item>
      <list-item label="•">
       {a mathematical formula}unload-a-C▷=〈∅,{package-at-C},∅,unload-a-C,a〉
      </list-item>
      <list-item label="•">
       {a mathematical formula}unload-a-B▷=〈∅,{package-at-B},∅,unload-a-B,a〉
      </list-item>
     </list>
     <paragraph>
      Notice the empty preconditions of the actions {a mathematical formula}unload-a-B▷ and {a mathematical formula}unload-a-C▷ as all the facts {a mathematical formula}plane-at-B, {a mathematical formula}plane-at-C and {a mathematical formula}package-in-a are private to the agent {a mathematical formula}a. The projected problem for agent {a mathematical formula}a, that is {a mathematical formula}Πa▷ is defined analogously.
     </paragraph>
     <paragraph>
      Let us now formally define the notion of a multi-agent plan. Let {a mathematical formula}π=(a1,...,am) be a sequence of actions from A (the actions may be owned by different agents). The sequence π is applicable in state {a mathematical formula}s0 if there are states {a mathematical formula}s1,...,sm s.t. for {a mathematical formula}1≤k≤m, action {a mathematical formula}ak is applicable in {a mathematical formula}sk−1 and {a mathematical formula}sk=sk−1[ak]. The sequence of actions π will be referred to as a {a mathematical formula}s0–{a mathematical formula}sm-plan and {a mathematical formula}s0[π] denotes the resulting state {a mathematical formula}sm. For a state s, an s-plan is a sequence of actions π from A applicable in s, which results in a goal state, that is {a mathematical formula}G⊆s[π]. An {a mathematical formula}sI-plan is a multi-agent plan solving the multi-agent problem {a mathematical formula}M. Clearly, the multi-agent plan may contain public and private actions of different agents. An {a mathematical formula}s▷i-plan solving {a mathematical formula}Πi▷ is defined analogously, but using only the actions from {a mathematical formula}Ai▷. Any multi-agent s-plan can be expressed in a distributed multi-agent plan form {a mathematical formula}{πi}i=1n, where {a mathematical formula}πi retains all actions in {a mathematical formula}π∩Ai and all actions from {a mathematical formula}A\Ai are replaced with an empty (no-op) action {a mathematical formula}ϵ=〈∅,∅,∅,−,−〉. In other words, {a mathematical formula}ak in {a mathematical formula}πi is {a mathematical formula}ak from π if {a mathematical formula}ak∈Ai. Otherwise, {a mathematical formula}ak in {a mathematical formula}πi is ϵ, that is a no-op action with no preconditions and effects.
     </paragraph>
     <paragraph>
      A multi-agent plan solving the example problem consists of the following 6 actions:{a mathematical formula} and can be reformulated as a distributed multi-agent plan{a mathematical formula}
     </paragraph>
     <paragraph>
      Notice, that an optimal solution to {a mathematical formula}Πt▷ is a plan consisting of a single action {a mathematical formula}〈unload-a-C▷〉. The projected problem {a mathematical formula}Πa▷ is analogous and can be solved using four actions, in particular {a mathematical formula}〈unload-t-B,load-a-B,move-a-B-C,unload-a-C〉.
     </paragraph>
     <paragraph label="Definition 2">
      In the rest of the article, we will use the term plan for multi-agent plans. We will also say that a state s is a dead end (or dead-end state), if no s-plan exists. We assume the quality criterion for a plan π to be its length {a mathematical formula}|π|, that is a plan π is optimal iff it contains the minimal number of actions among all plans solving {a mathematical formula}M. Nevertheless, all techniques presented in this article are easily modified for a quality criterion based on the (non-negative) cost of actions. A (distributed) algorithm is a multi-agent planner iff it accepts a multi-agent planning problem {a mathematical formula}M={Πi}i=1n as an input and produces a set of sequences {a mathematical formula}{πi}i=1n of actions from {a mathematical formula}A∪{ϵ} s.t. each {a mathematical formula}πi contains only actions from {a mathematical formula}Ai∪{ϵ}. Such a planner is soundiff every set of sequences {a mathematical formula}{πi}i=1n produced is a distributed multi-agent plan for {a mathematical formula}M,completeiff the multi-agent planner produces a distributed multi-agent plan for any multi-agent problem {a mathematical formula}M for which a plan exists.
     </paragraph>
     <paragraph>
      Notice that according to Definition 2, any classical planner accepting the defined input (e.g., by converting it to the global STRIPS problem) is a multi-agent planner. In the next section we will specify some more interesting properties a multi-agent planner should satisfy to set it apart from the classical planners, namely the property of being privacy-preserving.
     </paragraph>
     <paragraph label="Definition 3">
      A heuristic function is a function estimating the length of the shortest s-plan for a given state s. A heuristic estimator is the actual algorithm computing the heuristic function. We formalize two variants: A function {a mathematical formula}hM:2P→Z0+ is a global heuristic function for a multi-agent planning problem {a mathematical formula}M. If the function {a mathematical formula}hM is computed by a distributed heuristic estimator, we say it is a distributed (global) heuristic.
     </paragraph>
     <paragraph label="Definition 4">
      For an agent {a mathematical formula}αi, a function {a mathematical formula}hi▷:2Pi→Z0+ is an i-projected heuristic function.
     </paragraph>
     <paragraph>
      The term local heuristic denotes the i-projected heuristic function for an unspecified agent. For brevity, we refer to a heuristic function simply as a heuristic.
     </paragraph>
     <section label="2.1">
      <section-title>
       Privacy in multi-agent planning
      </section-title>
      <paragraph>
       As already emphasized, privacy is an important aspect of multi-agent planning. Privacy has been thoroughly studied in the context of distributed constraint satisfaction problems [14], [15], but sparsely in the context of (multi-agent) planning. A significant contribution is the concept of privacy-preserving multi-agent planning (PP-MAP) described in [3].
      </paragraph>
      <paragraph>
       The idea of privacy-preserving planning is based mainly on the research in secure multi-party computation [16], which is a subfield of cryptography. In secure multi-party computation, multiple agents are jointly computing a function, each agent having a portion of the function input as private data. The goal is to compute the function without revealing the private input data (except for what can already be reasonably deduced from the resulting value of the function).
      </paragraph>
      <paragraph>
       In secure multi-party computation, various assumptions are placed on the adversary model (i.e., the other agents trying to violate the privacy), the network model and the security guarantees. Here, we assume the agents to be honest, but curious. This means that the agents will not purposefully deviate from the defined protocol, but may try to deduce some additional information during the computation. This is consistent with the idea of cooperative planning—the agents are cooperating to achieve a common goal and not trying to exploit each other. Concerning the network model, as with other algorithms in this paper, we assume asynchronous communication without information loss and with in-order delivery.
      </paragraph>
      <paragraph>
       To summarize the assumptions:
      </paragraph>
      <list>
       <list-item label="a)">
        Agents are honest, but curious.
       </list-item>
       <list-item label="b)">
        Agents may use polynomial time and memory in order to deduce additional information from the algorithm.
       </list-item>
       <list-item label="c)">
        The communication channels are asynchronous and safe, that is, no information is lost.
       </list-item>
      </list>
      <paragraph>
       The most comprehensive definition of privacy for MA-STRIPS planning up to date has been presented in [3], together with discussion of privacy of the distributed multi-agent heuristic search we are building upon in this work. The privacy in MA-STRIPS based multi-agent planning based on [3] can be formally defined as follows. For each agent {a mathematical formula}αi∈A, the private part of its problem {a mathematical formula}Πi is
      </paragraph>
      <list>
       <list-item label="i)">
        the set of private facts {a mathematical formula}Pipriv and its size,
       </list-item>
       <list-item label="ii)">
        the set of private actions {a mathematical formula}Aipriv, i.e., the number of actions, the facts in {a mathematical formula}pre(a), {a mathematical formula}add(a) and {a mathematical formula}del(a),
       </list-item>
       <list-item label="iii)">
        the private parts of the public actions in {a mathematical formula}Aipub, i.e., the private facts in {a mathematical formula}pre(a), {a mathematical formula}add(a) and {a mathematical formula}del(a).
       </list-item>
       <list-item label="iv)">
        In addition, private actions in a multi-agent plan π must be kept private to their respective agents.
       </list-item>
      </list>
      <paragraph label="Definition 5">
       The public parts of actions in {a mathematical formula}Aipub can be shared in the form of projections. The algorithms may fall in two categories. A (weak) privacy-preserving algorithm is a distributed algorithm that does not directly communicate any private part of the agent problems.
      </paragraph>
      <paragraph label="Definition 6">
       A strong privacy-preserving algorithm is a distributed algorithm wherein no agent {a mathematical formula}αi can deduce existence of a private fact or an isomorphic (that is differing only in renaming) model of a private action or private part of a public action belonging to some other agent {a mathematical formula}αj, except for what can already be deduced from the projected problem {a mathematical formula}Πj▷ and the resulting distributed multi-agent plan {a mathematical formula}{πi}i=1n.
      </paragraph>
      <paragraph>
       This definition of privacy is a reasonable baseline, even though covering only extreme cases. It is straightforward to design a weak-privacy preserving algorithm, but the guarantees are weak (it may be possible to deduce a complete isomorphic or equivalent problem formulation). On the other hand, designing a strong-privacy preserving algorithm is extremely hard. It is often not possible to make the algorithm strong privacy-preserving without incurring significant (or even forbidding) increase in the computation complexity. An example of such algorithm is the treatment of the shortest path problem in [17], where it is solved using a cryptographic technique polynomial in the size of the input graph. Since in planning the implicit transition graph is already exponential in the size of the input problem, such approach is clearly intractable.
      </paragraph>
      <paragraph label="Definition 7">
       We are interested in algorithms which fall somewhere in-between the weak and strong privacy definition. A privacy-preserving multi-agent planner is a multi-agent planner (Definition 2) which is privacy-preserving (Definition 5).
      </paragraph>
      <paragraph>
       The MAD-A* was shown in [3] to be a privacy-preserving multi-agent planner, but is not strong privacy-preserving. It does not communicate the private parts, but some information can be deduced. More in-depth discussion is provided later in the description of the MADLA Search.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      The MADLA planner
     </section-title>
     <paragraph>
      The MADLA (Multi-Agent Distributed and Local Asynchronous) Planner proposed in this article is a sound and complete MA-STRIPS privacy-preserving multi-agent planner (Definition 7). In order to find a solution (multi-agent plan) to a multi-agent planning problem {a mathematical formula}M the MADLA Planner employs a distributed search through the state space induced by the problem. If a solution exists, it returns a distributed multi-agent plan {a mathematical formula}{πi}i=1n solving {a mathematical formula}M. The novel aspect of the search is that it is guided by a combination of two heuristics: {a mathematical formula}hLi and {a mathematical formula}hDi. The first heuristic {a mathematical formula}hLi is a local projected heuristic (Definition 4), evaluating the single agent's view (projection) of the problem, that is, {a mathematical formula}Πi▷. The other one is a distributed heuristic {a mathematical formula}hDi (Definition 3) evaluating the multi-agent problem {a mathematical formula}M in a distributed manner. In order to increase the efficiency of the planning process, MADLA Planner exploits properties often present in the distributed and projected heuristics.
     </paragraph>
     <paragraph>
      A distributed heuristic estimator can require computation of parts of the heuristic estimate by other agents. The heuristic estimator may require all (or some) agents to compute the heuristic synchronously and wait for the replies of other agents. On the contrary, if the heuristic estimator does not wait for the responses from other agents (i.e., is non-blocking), it allows the agent to run asynchronously and perform some other computations in the meantime. We say that such distributed heuristic estimator is asynchronous.
     </paragraph>
     <paragraph>
      A projected heuristic evaluates states based on the projected problem {a mathematical formula}Πi▷. As we have seen in the example in Section 1, a projected problem often misses crucial information (such as the preconditions of {a mathematical formula}unload-a-C▷) and the estimate is lower than the estimate for the global problem {a mathematical formula}M (the optimal plan for {a mathematical formula}Πt▷ has the length of 1 although 6 is the length of the optimal solution for {a mathematical formula}M). Often, a distributed heuristic dominates its projected counterpart, which means that for each state s, {a mathematical formula}hDi(s)≥hLi(s) generally holds.
     </paragraph>
     <paragraph>
      Although a distributed heuristic does not necessarily dominate a projected heuristic (or not in all states), dominance is a desirable property, because otherwise time is wasted on distributed computation without any benefit. In sub-optimal planning, dominance of heuristics does not automatically mean better informativeness as the heuristics are not admissible. Thus we are rather looking for better informed heuristics. Informally, a heuristic function {a mathematical formula}h1 is better informed than heuristic {a mathematical formula}h2 for a search algorithm A if that search algorithm guided by {a mathematical formula}h1 expands fewer states than the same algorithm guided by {a mathematical formula}h2. A distributed heuristic is typically better informed than a projected heuristic as it takes other agents private part of the problem into account.
     </paragraph>
     <paragraph>
      It is also obvious that a distributed heuristic will typically take longer time to compute than the projected one as a) it is computed on a larger problem and b) it involves communication, which is more time-consuming than local computation.
     </paragraph>
     <paragraph>
      With the exception of the asynchronous estimator of the distributed heuristic, the properties described above are not crucial for the usability of heuristics in the proposed search scheme, but are important to increase the efficiency of the planning process. In the next sections, we present the MADLA Search scheme, the heuristics used in the MADLA Planner and show the soundness and completeness of the search. We also assess how much privacy is preserved in all described algorithms.
     </paragraph>
     <section label="3.1">
      <section-title>
       Search
      </section-title>
      <paragraph>
       In order to find a plan, the MADLA Planner searches through a state-space (where states are {a mathematical formula}s⊆P) induced by the input multi-agent planning problem, driven by a pair of a projected (local) heuristic and a distributed (global) heuristic. From the literature [6], [18], we know that the performance of local and distributed heuristics differ over various multi-agent planning problems. The novel search scheme we are proposing combines the local and distributed heuristics in a manner inspired by the classical multi-heuristic search [10], but modified to be suitable for similar (or as in our case the same) heuristics which differ only in that one is computed on the projected problem and the other one on the global problem using a distributed algorithm.
      </paragraph>
      <paragraph>
       Since MADLA is a distributed multi-agent planner, the search is a distributed computation spread over the agents. We build on the idea of distributed state-space search [5], [6] and extend it to a distributed multi-heuristic search. We base the algorithm on the Greedy Best-First Search (GBFS) principle (the states are ordered solely according to the heuristic value) and also use the technique of deferred heuristic evaluation. Deferred heuristic evaluation means that unlike in A*, the heuristic estimate of a state is computed when it is extracted from the open list and its children are inserted into the open list according to the heuristic value of their parent state. This is useful especially for a time-consuming heuristic which is typically the case when it is computed distributedly.
      </paragraph>
      <paragraph>
       In this section, we first briefly introduce the techniques we are building on—the distributed heuristic search and the classical multi-heuristic search and later on describe the novel MADLA Search algorithm.
      </paragraph>
      <section label="3.1.1">
       <section-title>
        Multi-agent single-heuristic search
       </section-title>
       <paragraph>
        A multi-agent variant of heuristic search was proposed in [5] as a multi-agent optimal search (MAD-A*) and in [6], [3] as Multi-Agent Greedy Best-First Search (MA-BFS). The search in MADLA adheres to the latter.
       </paragraph>
       <paragraph>
        The main principle of multi-agent heuristic search is that all agents explore their portions of the search space asynchronously and in parallel, each agent using only its actions from {a mathematical formula}Ai. In order to manage the coordination, states expanded using a public action are sent to all other agents. Thus the other agents are informed about the new reached state and can expand it. A simplified principle is illustrated in Fig. 2. The figure is simplified in that the states are expanded in synchronous steps, whereas in reality both agents proceed asynchronously.
       </paragraph>
       <paragraph>
        In multi-agent single-heuristic search, each agent {a mathematical formula}αi has its own separate open list {a mathematical formula}Oi, closed list {a mathematical formula}Ci and a heuristic function {a mathematical formula}hi. The search begins with {a mathematical formula}Oi={sI▷i} and {a mathematical formula}Ci=∅ for each agent. In parallel, each agent {a mathematical formula}αi extracts a state {a mathematical formula}smin▷i=arg⁡mins▷i∈Oi⁡hi(s▷i) from {a mathematical formula}Oi, adds {a mathematical formula}smin▷i into {a mathematical formula}Ci, computes a new heuristic value {a mathematical formula}hi(smin▷i) and expands {a mathematical formula}smin▷i. All {a mathematical formula}s′▷i∈S, where S is the set of new expanded states{a mathematical formula} are added into the open list {a mathematical formula}Oi (and communicated to other agents if the expanding action was public, as described later). The search terminates if any agent {a mathematical formula}αi finds {a mathematical formula}s▷i s.t. {a mathematical formula}G⊆s▷i, or if all open lists are empty and no communication is waiting to be processed. Recall that the goal G is public and thus it is the same for all agents.
       </paragraph>
       <paragraph>
        As each agent uses only its own actions for the state expansions, it is necessary to communicate reached states to other agents in order to ensure completeness. In MA-BFS, the states are communicated via message broadcasts, as exemplified later. If a state {a mathematical formula}s▷i is expanded by an agent {a mathematical formula}αi using a public action {a mathematical formula}a∈Aipub, the state is sent to all other agents {a mathematical formula}αj≠i and added to their open lists {a mathematical formula}Oj. In order to hide the private facts {a mathematical formula}Pis=Pipriv∩s of a sent state s, the agent {a mathematical formula}αi can obfuscate the facts in {a mathematical formula}Pis (as proposed in [19]) or replace the facts in set {a mathematical formula}Pis with a private unique identifier (or a hash value) {a mathematical formula}δi(s) known only to {a mathematical formula}αi (this private part of the state cannot be modified by other agents). If a modified state amended by such identifier returns by one of the later broadcasts back to the agent {a mathematical formula}αi, it can use the identifier {a mathematical formula}δi(s) and restore the private facts {a mathematical formula}Pis back as if they were always part of the state. The initial state is treated as a special case. As already said, each agent {a mathematical formula}αi starts only with its projection of the initial state, that is {a mathematical formula}sI▷i. As all agents start in fact from the same initial state {a mathematical formula}sI (although not completely observable by any of them), the unique identifier can use some specific value (the same for all agents), such as {a mathematical formula}δi(sI)=0 for all i. When an agent {a mathematical formula}αj receives a state from {a mathematical formula}αi with its private part {a mathematical formula}Pjs replaced by the identifier {a mathematical formula}δj(sI)=0, agent {a mathematical formula}αj knows that it must restore the values in {a mathematical formula}Pjs from the initial state, that is, {a mathematical formula}sI▷j.
       </paragraph>
       <paragraph>
        When a goal state {a mathematical formula}sg s.t. {a mathematical formula}G⊆sg is found, the solution plan needs to be also reconstructed in a distributed way. This can be done by modifying the parent of a state to contain either the previous state (and the respective action) as in classical search, or a reference to the agent from which the state was received. The backward reconstruction of the plan then proceeds as usual, except for when instead of a state, the parent is an agent {a mathematical formula}αj, in which case a message is sent to {a mathematical formula}αj to continue the plan extraction process.
       </paragraph>
       <paragraph>
        We illustrate the process on the running example in Fig. 3. Each agent starts with its projection of the initial state {a mathematical formula}sI=s0={truck-at-A,plane-at-B,package-at-A}. The truck starts expanding {a mathematical formula}s0▷t={truck-at-A,package-at-A},δa(s0) using the {a mathematical formula}load-t-A and {a mathematical formula}move-t-A-B actions sequentially. Further on in the process, when the truck expands the state {a mathematical formula}s2▷t={truck-at-B,package-at-B},δa(s0) using the action {a mathematical formula}unload-t-B, the resulting state {a mathematical formula}s3={truck-at-B, {a mathematical formula}plane-at-B,package-at-B}, seen by the truck as {a mathematical formula}s3▷t={truck-at-B,package-at-B},δa(s0), is sent to the plane as {a mathematical formula}{package-at-B},δt(s3),δa(s0). Here {a mathematical formula}δt(s3) encodes the private part of the truck in the state {a mathematical formula}s3. When received by plane, it reconstructs the state {a mathematical formula}s3 as {a mathematical formula}s3▷a={plane-at-B,package-at-B},δt(s3) by using the private part of the state with the identifier {a mathematical formula}δa(s0)=0 which is {a mathematical formula}s0.
       </paragraph>
      </section>
      <section label="3.1.2">
       <section-title>
        Classical multi-heuristic search
       </section-title>
       <paragraph>
        In classical planning, multi-heuristic search was pioneered by the Fast Downward planning system [10] as a way to combine different heuristic estimators without the need to combine the heuristic values, and was also one of the main mechanisms behind the success of the LAMA planner [20].
       </paragraph>
       <paragraph>
        The principle of multi-heuristic search is simple. Instead of a single open list O, multi-heuristic search uses a set of open lists {a mathematical formula}{O1,...,Om}, one for each used heuristic {a mathematical formula}h1,...,hm. In each search step, a state is extracted from one open list according to a open list selection function. Then the state is evaluated by each heuristic and its successors placed in the respective open list (if using the deferred evaluation scheme). This means that if a state s is evaluated by a heuristic {a mathematical formula}hk, its successors are placed in {a mathematical formula}Ok.
       </paragraph>
       <paragraph>
        The choice of an appropriate open list selection function for classical planning was thoroughly examined in [11] with a conclusion that the simple alternation mechanism, where the open lists are chosen in turns, appears to be the best one (this mechanism was used in both FD and LAMA planners).
       </paragraph>
       <paragraph>
        The idea of multi-heuristic search has not yet been used in multi-agent planning. A straightforward utilization of the multi-heuristic search in multi-agent planning is to extend the MA-BFS scheme with multiple heuristics (and thus multiple open lists) for each agent. If we aim to use such a search scheme not with significantly different heuristics but with a projected and distributed variant of the same heuristic, we encounter several limitations of this simple approach. Mainly if the states evaluated by the projected heuristic, which is arguably less informed and thus gives worse estimates, are placed in the open list of the distributed heuristic, they would skip better states evaluated by the more informative distributed heuristic. Nevertheless, we propose this simple approach as a baseline and present a significantly better one in the following section.
       </paragraph>
      </section>
      <section label="3.1.3">
       <section-title>
        MADLA search: multi-agent distributed and local asynchronous search
       </section-title>
       <paragraph>
        The main contribution of this paper is the MADLA Search, which is a variant of the multi-agent multi-heuristic search mentioned in Section 3.1.2. As in MA-BFS (Section 3.1.1), the search is performed in parallel by all agents, each searching using its own set of actions and communicating states expanded by public actions, in order to reach a common public goal. The plan is then extracted in a distributed manner, so that each agent knows only its respective part of the plan.
       </paragraph>
       <paragraph>
        The main distinctive feature of the MADLA search is its use of two open lists per agent, where the first one is associated with a local projected heuristic and the second one with a distributed global heuristic. The open list selection function is tailored to handle this special case. The main high-level principles of the search are the following:
       </paragraph>
       <list>
        <list-item label="a)">
         Evaluate a state only using a single heuristic.
        </list-item>
        <list-item label="b)">
         Prefer the distributed heuristic, and only if the distributed heuristic is waiting for replies from other agents (i.e. is busy), use the projected heuristic instead.
        </list-item>
       </list>
       <paragraph>
        An overview of the principle is shown in Fig. 4 and it is elaborated on in the next paragraphs. In the main search loop, after processing the communication (i.e. receiving and sending queued messages), a state is extracted from an open list. Which open list is used for the extraction is determined based on whether the distributed estimator of {a mathematical formula}hDi is busy and the open lists are empty or not (as shown in Fig. 4). As we use deferred heuristic evaluation, the extracted state is evaluated by a heuristic (after it was checked for being in the closed list or being a solution), again depending on the state of the distributed heuristic estimator. If the extracted state was created using a public action, it is sent to all other agents. Then, the state is expanded using all applicable actions of the agent and its successors are added to the respective open list(s) as shown in Fig. 4 with the heuristic estimate of the parent state (again because of the deferred heuristic evaluation used).
       </paragraph>
       <paragraph>
        The implementation of the distribution in the proposed search follows the principles of MA-BFS, i.e., broadcasts are used to inform other agents about states reached by public actions. Additionally, information as to which open list the state should be added to is included (whether it is the local or the distributed one). The overall principles of the MADLA Search will now be presented and the algorithm itself will be described in detail later on.
       </paragraph>
       <paragraph>
        The MADLA Search uses two heuristics for each agent {a mathematical formula}αi, a local projected {a mathematical formula}hLi and a distributed {a mathematical formula}hDi. The search uses an open list for each of the heuristics {a mathematical formula}OLi,ODi for each agent {a mathematical formula}αi. The open list selection function prioritizes expansion of states in the open list {a mathematical formula}ODi respective to the distributed heuristic {a mathematical formula}hDi, if the heuristic estimator of {a mathematical formula}hDi is not in the process of computing an heuristic estimate (i.e. waiting for some replies from other agents).
       </paragraph>
       <paragraph>
        Unlike the classical multi-heuristic search (Section 3.1.2), in MADLA Search, the extracted states are not evaluated using both heuristics. The heuristic used depends on the state of the distributed heuristic {a mathematical formula}hDi estimator, represented in the algorithms as a boolean variable {a mathematical formula}busyDi. The variable is set by the heuristic estimator to {a mathematical formula}true if it is currently evaluating a state and {a mathematical formula}false if not. If {a mathematical formula}busyDi=false, the state is evaluated by {a mathematical formula}hDi. If {a mathematical formula}busyDi=true, the state is evaluated by {a mathematical formula}hLi, that is the distributed heuristic is preferred if possible. This approach is most reasonable if {a mathematical formula}hDi dominates {a mathematical formula}hLi for most states, which is typically the case for a projected and distributed variant of the same heuristic such as FF.
       </paragraph>
       <paragraph>
        As the MADLA Search is running in a single process (except for the communication) for each agent, the local heuristic search is performed only when the distributed heuristic search is waiting for the distributed heuristic estimation to finish. This principle makes sense only if finishing an estimation of {a mathematical formula}hDi takes longer than that of {a mathematical formula}hLi and if computation of the {a mathematical formula}hDi estimator does not block the search process (incl. {a mathematical formula}hLi estimations). These two requirements often hold for distributed and projected versions of one heuristic and hold for the two variants of the FF heuristic we use as well (described in detail in Section 3.2.2).
       </paragraph>
       <paragraph>
        Using two separate open lists has the benefit of using two heuristics independently, but if some information between the two searches could be shared,{sup:2} most importantly the heuristically best state found so far, it could boost the efficiency of the planner. The direction {a mathematical formula}ODi→OLi is straightforward as most of the time, {a mathematical formula}hDi dominates {a mathematical formula}hLi. Thus, we can add all states evaluated by {a mathematical formula}hDi also to {a mathematical formula}OLi without ever skipping a better state evaluated by {a mathematical formula}hLi with a worse state evaluated by {a mathematical formula}hDi.
       </paragraph>
       <paragraph>
        If a state s is taken from the local open list, its successor is inserted only in the local open list. If the state s is taken from the distributed open list, its successor is inserted into both local and distributed open lists. If state {a mathematical formula}s′ was obtained by application of a private action, it is inserted in the respective open lists of agent {a mathematical formula}αi. If the action is public, it is also inserted in open lists of all agents {a mathematical formula}αj≠i via a message sent to them by {a mathematical formula}αi. The messages include additional information whether the state should be added to {a mathematical formula}ODj (if it was evaluated by {a mathematical formula}hDi) or to {a mathematical formula}OLj (if it was evaluated by {a mathematical formula}hLi). In the latter case, the heuristic estimate is recomputed using {a mathematical formula}hLj, because the local heuristic estimate from the agent {a mathematical formula}αi may significantly differ from that of agent {a mathematical formula}αj.
       </paragraph>
       <paragraph>
        The other direction {a mathematical formula}OLi→ODi is trickier. If we added a state s evaluated by {a mathematical formula}hLi to {a mathematical formula}ODi the search would skip many states which are actually closer to the goal only because the local, less informative heuristic, gives a lower estimate. The way at least some information can be shared in this direction is that whenever the open list {a mathematical formula}ODi is empty and the heuristic estimator {a mathematical formula}hDi is not computing any heuristic, the best state s is extracted from the local open list {a mathematical formula}OLi and evaluated by the distributed heuristic {a mathematical formula}hDi and its successors are added to both open lists. This way, the states placed in {a mathematical formula}ODi are evaluated only by {a mathematical formula}hDi, but sometimes, the best state from {a mathematical formula}OLi is taken to be evaluated by {a mathematical formula}hDi.
       </paragraph>
       <paragraph>
        The search is terminated when a goal state is reached by one of the agents, followed by a distributed plan extraction, or if there is no solution. Detection of solution nonexistence in the multi-agent setting is more complicated than in classical planning. Even if both open lists are empty, the agent cannot be sure that some other agent is not going to find a solution or broadcast some new state in the future. Therefore, the agents need to check that all open lists are empty and also that there are no pending messages to be delivered (that is no state is “in the air”). By that we close the high-level description of the MADLA Search and the MADLA Planner and continue with a detailed formal description.
       </paragraph>
      </section>
      <section label="3.1.4">
       <section-title>
        Details of the MADLA search
       </section-title>
       <paragraph label="Definition 8">
        To talk about the actual algorithm formally, we first need to distinguish the state {a mathematical formula}s⊆P and its representation in the search algorithm. Note, that due to privacy concerns (as explained later), each agent must have its own set of search nodes with its own representation of the actual state. When a state is to be sent to another agent, only its public projection is sent together with a tuple of ids representing the private parts of each agent. Thus, the search node of agent {a mathematical formula}αi is represented as follows. A search node representing a state {a mathematical formula}s∈P is a tuple {a mathematical formula}u=〈s▷i,p,apar,h,g,αi,〈δ1,...,δn〉〉 where {a mathematical formula}s▷i⊆Pi is the i-projected state, p is the parent of the search node u. Action {a mathematical formula}apar∈Ai∪{ϵ} is the action used to create s, h is the heuristic value, g the distance of s from {a mathematical formula}sI, {a mathematical formula}αi is the agent u belongs to and {a mathematical formula}〈δ1,...,δn〉 is the n-tuple of private unique identifiers representing the private parts {a mathematical formula}s∩Pjpriv of s for all agents {a mathematical formula}αj∈A including {a mathematical formula}αi.The parent p is determined as follows. If the search node u is created from a predecessor search node {a mathematical formula}u′ by the application of an action a, then {a mathematical formula}p=u′ and {a mathematical formula}apar=a. In the case the search node is created from a state received from another agent {a mathematical formula}αj≠i, then {a mathematical formula}p=αj and {a mathematical formula}apar=ϵ.Each search node is agent-dependent, that is, it belongs to the agent {a mathematical formula}αi, which is part of the search node definition, and thus indexing a node u as {a mathematical formula}ui would be superfluous and is omitted.We extend the action application to search nodes so that for a search node {a mathematical formula}uk=〈sk▷i,p,ak,hk,gk,αi,〈δ1,...,δi,...,δn〉〉 and an action {a mathematical formula}ak+1∈Ai we define {a mathematical formula}uk[ak+1]=uk+1=〈sk▷i[ak+1],uk,ak+1,hk,gk+1=gk+1,αi,〈δ1,...,δi′,...,δn〉〉, where p can either be a search node or an agent. As we use deferred heuristic evaluation, the heuristic estimate {a mathematical formula}hk is in fact the heuristic estimate of the state represented by the parent node. Thus, the heuristic estimate is not changed by the action application, but is updated later after the new search node is extracted from the open list and evaluated.We define {a mathematical formula}Ui as the set of all possible search nodes of agent {a mathematical formula}αi.A public search node is a tuple {a mathematical formula}u▷pub=〈s▷pub,p,apar,h,g,〈δ1,...,δn〉〉 where {a mathematical formula}s▷pub⊆Ppub is the public projection of state s, i.e., {a mathematical formula}s▷pub=s∩Ppub.
       </paragraph>
       <paragraph>
        For use in the algorithm (and consequent proofs) we define {a mathematical formula}u.state=s▷i, {a mathematical formula}u.action=apar, {a mathematical formula}u.parent=p (that is {a mathematical formula}u.parent=u′ or {a mathematical formula}u.parent=αj), {a mathematical formula}u.h=h, {a mathematical formula}u.g=g, {a mathematical formula}u.agent=αi, {a mathematical formula}u.uids=〈δ1,...,δn〉 and {a mathematical formula}u.uidi=δi. A search node is agent-dependent, it is always created by an agent and never sent to another agent. When sending a state represented by a search node u, it is first converted to a public search node {a mathematical formula}u▷pub and then only the public projection {a mathematical formula}s▷pub of the state is sent together with the private unique identifiers {a mathematical formula}〈δ1,...,δn〉.
       </paragraph>
       <paragraph label="Definition 9">
        Each agent {a mathematical formula}αi starts with an initial search node {a mathematical formula}uI created from the initial state {a mathematical formula}sI as the following {a mathematical formula}uI=〈sI▷i,null,null,h0,0,αi,〈0,...,0〉〉. By assigning 0 to each {a mathematical formula}δj we represent that the private part of other agents represent the initial state (of course any value can be used as long as it is agreed upon by all agents). The agent data structure is defined as{a mathematical formula}
       </paragraph>
       <paragraph>
        The data structures related to each agent {a mathematical formula}αi are grouped in an object-like description in Definition 9. In each algorithm or procedure, the agent object is given as the first parameter (similarly to “this” in object-oriented languages) and the data structures are accessed directly. The structures {a mathematical formula}ODi, {a mathematical formula}OLi and {a mathematical formula}Ci denote the open lists and the closed list of agent {a mathematical formula}αi and {a mathematical formula}busyDi is a boolean variable denoting whether the distributed heuristic estimator of agent {a mathematical formula}αi is busy or not, similarly {a mathematical formula}searchi denotes whether the search loop should continue or not. Moreover, {a mathematical formula}hDi and {a mathematical formula}hLi denote the distributed and local heuristic evaluators of agent {a mathematical formula}αi. Finally, the definition includes the state reconstruction function {a mathematical formula}μi, and a set {a mathematical formula}plansi of all currently reconstructed plans in the form of a tuple {a mathematical formula}〈αj,πji〉 consisting of an agent (the plan reconstruction originator) and the (partially) reconstructed plan {a mathematical formula}πji, initially empty, prospectively a part of the multi-agent plan {a mathematical formula}{πi}i=1n. The set {a mathematical formula}plansi is initially empty.
       </paragraph>
       <paragraph>
        Now, let us have a detailed look at the pseudo-code and some implementation details. First, we start with Algorithm 1 which outlines the main loop of the search together with initialization. The algorithm (and all subsequent algorithms) is seen from the perspective of agent {a mathematical formula}αi, that is each agent runs a copy of Algorithm 1 in parallel with all other agents. All data structures are local to agent {a mathematical formula}αi and all search nodes contain i-projected states.
       </paragraph>
       <paragraph>
        The initialization starts with the open lists containing the initial search node {a mathematical formula}uI=〈sI▷i,null,null,h0,0,αi,〈0,...,0〉〉 consisting of the i-projected initial state {a mathematical formula}sI▷i, no parent, no action and the agent {a mathematical formula}αi. The closed list {a mathematical formula}Ci is initialized as empty and the distance of the initial search node is set to {a mathematical formula}uI.g←0. Although the open lists are ordered only by the heuristic value, we need the g value for the later plan reconstruction to know the length of the plan being reconstructed upfront in order to be able to insert the appropriate number of no-op actions in the place of actions of other agents. The heuristic value of the initial node is set to some initial value {a mathematical formula}uI.h=h0 which is only a mock value for later state selection.
       </paragraph>
       <paragraph>
        The main loop terminates when a solution is found, or both open lists are empty (for all agents) and there are no pending messages, which means that there is no solution. Such synchronization can be straightforwardly achieved using the textbook distributed snapshot algorithm [21], but we leave it out of the pseudo-code for simplicity.
       </paragraph>
       <paragraph>
        The distributed snapshot algorithm is a general technique to determine the state of a distributed system and roughly works as follows. The agent performing the snapshot saves its own local state and sends a snapshot request message with a snapshot token to all other agents. When any agent receives this particular snapshot token for the first time, it sends the initiator agent its own saved state (e.g., all open lists are empty) and attaches the snapshot token to all subsequent messages. When an agent that has already received the snapshot token receives a message that does not have the snapshot token, the agent needs to inform the snapshot initiator about the message (e.g., if it was a state message, its open lists are no longer empty). This message was sent before the snapshot started and needs to be included in the snapshot. This means that all messages need to have the possibility to include the snapshot token, which we have also not included in the formal description in order to keep it simpler.
       </paragraph>
       <paragraph>
        The main loop follows the classical heuristic search with deferred evaluation scheme with some modifications. From line 8 to line 15, an open list is determined based on the state of the distributed heuristic and the node u with minimal heuristic value {a mathematical formula}u.h is extracted from the selected open list (if both open lists are empty, the loop continues with checking the messages). Node u is processed as in classical search (added to open list, checked for goal), but if u contains a goal state, distributed plan reconstruction is initiated (see Algorithm 4). If u does not contain a goal state, u is expanded (see Algorithm 2). Since we use the deferred heuristic evaluation, the heuristic is evaluated before the actual expansion.
       </paragraph>
       <paragraph>
        The expansion procedure is detailed in Algorithm 2. The node u is evaluated using either the local projected heuristic {a mathematical formula}hLi, which proceeds as usual, or using the distributed global heuristic {a mathematical formula}hDi. The distributed heuristic evaluation is represented as a function call for simplicity. In reality, the procedure evaluating the heuristic function is asynchronous. This means that a callback is passed to the procedure and the expand({a mathematical formula}αi,u,d) procedure exits. Only when the heuristic evaluation is finished, the rest of the procedure continues. This means the following steps are performed either directly after the local heuristic evaluation, or in the callback of the distributed heuristic evaluation.
       </paragraph>
       <paragraph>
        If node u was expanded by a public action, a message {a mathematical formula}MSTATE is sent to all other agents, containing the public projection {a mathematical formula}s▷pub of state s together with the private unique identifiers representing the private parts of all agents, its {a mathematical formula}u.g, its heuristic value {a mathematical formula}u.h and a parameter d determining whether it was evaluated using the local or distributed heuristic in order to determine to which open list it should be added.
       </paragraph>
       <paragraph>
        The states sent to other agents are stored by a mapping function {a mathematical formula}μi:2Ppub×N→Ui, which assigns to a public projection of a state {a mathematical formula}s▷pub and a private unique identifier {a mathematical formula}δi the respective search node. The function {a mathematical formula}μi is used both to be able to reconstruct the path later and to reconstruct the private part of a received state. The storing does not cause significant memory overheads as the states are stored in the closed list anyway.
       </paragraph>
       <paragraph>
        Next, the node u is expanded using all applicable actions from {a mathematical formula}Ai, the new nodes {a mathematical formula}u′ are created according to the Definition 8 (so that {a mathematical formula}u′.state=s▷i[a], {a mathematical formula}u′.parent←u, {a mathematical formula}u′.action←a, {a mathematical formula}u.h←u′.h, {a mathematical formula}u.g←u′.g+1, {a mathematical formula}u′.agent←αi, {a mathematical formula}u′.uidj←u.uidj for all {a mathematical formula}j≠i and {a mathematical formula}u′.uidi is assigned a new private unique identifier) and the new search nodes are added to the open list(s) based on which heuristic was used for evaluation.
       </paragraph>
       <paragraph>
        In addition to the main loop in Algorithm 1, there is an inner loop on lines 17–21, which is performed when the distributed open list is empty or busy with a heuristic evaluation (this happens when the distributed heuristic is being asynchronously evaluated). This inner loop is again the classical heuristic search loop with deferred evaluation, this time taking into account only the local open list and local heuristic. This inner local search loop also needs to process the communication, that is to send and to receive messages (otherwise the distributed heuristic computation could not be finished).
       </paragraph>
       <paragraph>
        The part of the communication taking care of message receiving is shown in Algorithm 3 (message sending is trivial). There are four types of messages sent among the agents for the distributed search (more messages are sent for the distributed heuristic computation). A state message {a mathematical formula}MSTATE=〈s▷pub,〈δ1,...,δn〉,h,g,d〉 contains a public projection {a mathematical formula}s▷pub of a state s, the private unique identifiers, the state's heuristic value h, g and the parameter d. When received, the i-projection {a mathematical formula}s▷i is reconstructed from {a mathematical formula}s▷pub and the private unique identifier {a mathematical formula}δi using the function {a mathematical formula}μi, which encodes the node and respective i-projected state from which the private part should be copied. Thus the previously anonymized private part of agent i is restored, and a new search node u is created, its parent is set to the sending agent {a mathematical formula}αj. The heuristic is re-computed to reflect the local problem of the receiving agent, although not recomputing the heuristic does not have significant effect in most domains. As the heuristics are not admissible, taking the maximum as in MAD-A* is not reasonable. Finally, the node u is added to the open list(s) selected based on the received parameter d.
       </paragraph>
       <paragraph>
        The “plan found” message {a mathematical formula}MPLANFOUND (with no parameters, except for the implicit sender {a mathematical formula}αj) is sent by an agent {a mathematical formula}αj when it reaches a goal state to inform other agents that it has found a plan and it is starting the plan reconstruction process. When received by the agent {a mathematical formula}αi, it knows that it can exit the search loop (the communication still has to be processed). A new tuple {a mathematical formula}〈αj,πji〉 is added to {a mathematical formula}plansi as the reconstruction of plan initiated by {a mathematical formula}αj will take place. Note that before receiving the {a mathematical formula}MPLANFOUND message, the agent {a mathematical formula}αi could have already started the plan reconstruction itself, thus multiple plans can be reconstructed in parallel.
       </paragraph>
       <paragraph>
        The plan reconstruction message {a mathematical formula}MRECONSTRUCT=〈s▷pub,δi,t,αk〉 contains the state s (represented as {a mathematical formula}s▷pub and the private unique identifier {a mathematical formula}δi of the receiving agent), from which the reconstruction should continue, a discrete time-point t, initially set to {a mathematical formula}u.g of the last search node u (that is, the search node which satisfied the goal condition) and the agent {a mathematical formula}αk which started the plan reconstruction. The identifier of the agent {a mathematical formula}αk is used to determine which partial plan {a mathematical formula}〈αk,πki〉∈plansi the reconstruction message belongs to. To reconstruct the plan, first the respective search node needs to be retrieved, which is the search node returned by {a mathematical formula}μi(s▷pub,δi) based on the public part {a mathematical formula}s▷pub and private part {a mathematical formula}δi of the state {a mathematical formula}s▷i, which was sent to {a mathematical formula}αj on line 9 in Algorithm 2. Next, the plan reconstruction procedure is invoked (see Algorithm 4).
       </paragraph>
       <paragraph>
        Finally, the termination message {a mathematical formula}MTERMINATE=〈αk〉 announces that the reconstruction of the plan initiated by the agent {a mathematical formula}αk has been finished by the sender agent {a mathematical formula}αj. Only when a message {a mathematical formula}MTERMINATE=〈αk〉 is received for all {a mathematical formula}αk such that {a mathematical formula}MPLANFOUND was received from {a mathematical formula}αk before and thus there is {a mathematical formula}〈αk,πki〉∈plansi, the plan reconstruction of all plans is finished and the shortest plan can be selected (ties are broken based on the ordering of the agent indexes). Of course, it may happen that the agent {a mathematical formula}αi was not involved in the plan reconstruction at all and thus its plan {a mathematical formula}πii consists of l no-op actions ϵ, where l is the length of the shortest plan.
       </paragraph>
       <paragraph>
        Note that messages for the distributed heuristic {a mathematical formula}hDi are also handled in Algorithm 3 and are forwarded to the heuristic estimator.
       </paragraph>
       <paragraph>
        The distributed plan reconstruction procedure is shown in Algorithm 4. Recall that the plan is stored in a list {a mathematical formula}〈αk,πki〉 of actions for each agent, together forming a multi-agent plan {a mathematical formula}{πki}i=1n for each such k. The reconstruction process is started by an agent {a mathematical formula}αk in a search node u s.t. {a mathematical formula}G⊆u.state (see Algorithm 1 line 25) and with {a mathematical formula}t=u.g which is the distance from {a mathematical formula}uI to u. Every time an action is added to the position t of {a mathematical formula}πki (line 3), t is decreased by 1, which represents backward reconstruction of the plan. All positions between the last added action and t are padded with the no-op action ϵ (line 4).
       </paragraph>
       <paragraph>
        If the initial node is reached (line 6), the terminate message is sent to all other agents. Otherwise, the parent {a mathematical formula}u.parent of node u is retrieved, which is either the predecessor node, or an agent {a mathematical formula}αj≠i. If {a mathematical formula}u.parent=αj≠i a reconstruct message is sent to {a mathematical formula}αj prompting it to continue with the reconstruction from a state s represented by the search node u.
       </paragraph>
       <paragraph>
        As multiple agents may start plan reconstruction independently, any agent that found a solution reports to all other agents by sending a {a mathematical formula}MPLANFOUND message. This way, each agent {a mathematical formula}αi receiving the message terminates the search and adds {a mathematical formula}〈αj,πji〉 to the {a mathematical formula}plansi set (see Algorithm 3 line 15). This means that a plan reconstruction was started by agent {a mathematical formula}αj. When the plan reconstruction started by the agent {a mathematical formula}αk is finished by an agent {a mathematical formula}αj by reaching the initial search node {a mathematical formula}uI (see Algorithm 4 line 6) the {a mathematical formula}MTERMINATE=〈αk〉 message is sent to all agents. When {a mathematical formula}MTERMINATE=〈αk〉 sent by {a mathematical formula}αj is received by {a mathematical formula}αi, the agent {a mathematical formula}αi knows that the reconstruction of {a mathematical formula}〈αk,πki〉 has been finished. When received for all such {a mathematical formula}〈αk,πki〉∈plansi, the shortest plan from {a mathematical formula}plansi is chosen (see Algorithm 3 line 22) and reported as the solution for agent {a mathematical formula}αi (see Algorithm 3 line 23). To ensure that each agent {a mathematical formula}αi chooses the corresponding plan {a mathematical formula}πki together forming the distributed multi-agent plan {a mathematical formula}{πki}i=1n, the ties are broken based on the ordering of the indexes k of the agents which started the reconstruction of the respective plan.
       </paragraph>
       <paragraph>
        Description of details of the distributed plan reconstruction concludes the proposed MADLA Search.
       </paragraph>
      </section>
     </section>
     <section label="3.2">
      <section-title>
       Heuristics
      </section-title>
      <paragraph>
       The key principle behind the MADLA Planner is a favorable combination of both local (Definition 4) and distributed (Definition 3) heuristics. A heuristic pair complying with the search used in the MADLA Planner combines a light single-agent and a heavy distributed multi-agent heuristic which in this article are both forms of the classical Fast-Forward (FF) heuristic [7].
      </paragraph>
      <paragraph>
       The local heuristic is the classical FF applied to each agent's projected problem {a mathematical formula}Πi▷, whereas the distributed variant is a novel privacy-preserving modification of previously published distributed FF [8], [6]. Here, we describe both variants and show what properties important for the MADLA Search are satisfied and how privacy is treated.
      </paragraph>
      <section label="3.2.1">
       <section-title>
        Local heuristic
       </section-title>
       <paragraph>
        One of the most successful and most studied heuristics for satisficing planning is the FF heuristic. The FF heuristic belongs to the delete relaxation heuristic family. The idea behind delete relaxation heuristics is to simplify the problem by ignoring negative effects of actions. In the STRIPS formalism, this means that an action {a mathematical formula}a=〈pre(a),add(a),del(a)〉 is transformed to a relaxed form {a mathematical formula}a+=〈pre(a),add(a),∅〉. A set of relaxed actions {a mathematical formula}A+={a+|a∈A) is used in the definition of a classical relaxed planning problem {a mathematical formula}Π+=〈P,A+,sI,G〉 respective to the original planning problem {a mathematical formula}Π=〈P,A,sI,G〉. By relaxation, the whole problem becomes additive, meaning that whenever a fact is added it is never deleted again and as both preconditions and goal are positive in STRIPS, any action that becomes applicable remains applicable in all subsequent relaxed states. The solution of a relaxed problem {a mathematical formula}Π+ is a relaxed plan {a mathematical formula}π+. Notice that thanks to the additivity of the relaxed problem, the relaxed plan can be represented as an unordered set of relaxed actions. From such set, the relaxed plan can be reconstructed by iteratively applying all actions as soon as they are applicable. This way, all facts achievable by any permutation of the actions in {a mathematical formula}π+ are achieved. This also means that in an optimal relaxed plan, no action is used more than once.
       </paragraph>
       <paragraph>
        An optimal relaxed heuristic {a mathematical formula}h+ is defined as the length of an optimal relaxed plan {a mathematical formula}π+. In contrast to STRIPS planning, which is PSPACE-complete, finding an optimal relaxed plan {a mathematical formula}π+ is NP-Complete [22], which is still impractical as a heuristic. In order to lower the complexity even more, approximations of {a mathematical formula}h+ are used in classical planning. The most commonly used approximation in satisficing planning is to use the length of a sub-optimal relaxed plan (RP) instead of an optimal one. Finding a sub-optimal RP can have as low as polynomial complexity and therefore can be fast enough in practice.
       </paragraph>
       <paragraph>
        The main idea behind the FF heuristic is to find a sub-optimal relaxed plan by analyzing which facts are successively reachable by applied relaxed actions (reachability analysis). From this analysis the relaxed plan is determined in a backward fashion. The principle is based on a notion of supporter action a of fact p which is an action a s.t. {a mathematical formula}p∈add(a). Let {a mathematical formula}Π+=〈P,A+,sI,G〉 be a relaxed planning problem, then the principle of relaxed plan extraction is the following:
       </paragraph>
       <list>
        <list-item label="1.">
         Initialize a set of unsupported facts U to contain all goal facts and a set of supported facts S to contain all initial state facts: {a mathematical formula}U←G,S←sI.
        </list-item>
        <list-item label="2.">
         Move an unsupported fact p from U to a set of supported facts S and determine its supporter a.
        </list-item>
        <list-item label="3.">
         Mark all preconditions of a as unsupported if not supported already: {a mathematical formula}U←U∪(pre(a)∖S).
        </list-item>
        <list-item label="4.">
         Loop 1–3 until all facts in U are supported: until {a mathematical formula}U∖S=∅.
        </list-item>
       </list>
       <paragraph>
        There are many ways of implementing this high-level scheme (which differ mainly in the way the supporters are chosen) and many methods to perform the reachability analysis. In our work, we have used one of the most prevalent, based on an Exploration Queue algorithm as implemented for example in the Fast-Downward planning system [10]. The details of the reachability analysis algorithm are not relevant for this work as any standard technique can be used, therefore we will remain focused on the relaxed plan extraction part.
       </paragraph>
       <paragraph>
        In the MADLA Planner, the FF heuristic is used as the local heuristic {a mathematical formula}hLi. As such it is computed by each agent {a mathematical formula}αi on its respective i-projected problem {a mathematical formula}Πi▷. The i-projected (Definition 4) FF heuristic is computed on a relaxed i-projected problem, formally{a mathematical formula}
       </paragraph>
       <paragraph>
        Although projected heuristics have been used previously in [3], privacy preservation of the projected heuristics was not explicitly discussed. Indeed, it is trivial to see that when using only a projected heuristic the privacy is not compromised in any way. Each agent computes the heuristic estimate separately, using only the information present in the projected problem. How the heuristic is used depends on the search scheme, but even if the heuristic estimate is shared among agents, other agents have no means to decode and exploit the heuristic value (it can possibly be computed using an algorithm completely different to their own).
       </paragraph>
      </section>
      <section label="3.2.2">
       <section-title>
        Distributed heuristic
       </section-title>
       <paragraph>
        In addition to a local heuristic, the MADLA Planner uses a distributed multi-agent heuristic (Definition 3). In contrast to the computation of the local heuristic by a single agent, a coordinated computation by multiple agents is necessary for evaluation of the distributed heuristic estimates. Computation of such a heuristic incurs a communication overhead, but the overhead is typically outweighed by heuristic estimates significantly better than the estimates of a projected heuristic.
       </paragraph>
       <paragraph>
        Although the benefits of using a global heuristic estimate computed in a distributed way are clear, computing such heuristic efficiently whilst preserving privacy is a major challenge. So far, the distribution has been tackled by a couple of approaches. In [23], the authors present a plan-space multi-agent planner FMAP, which is guided by a heuristic similar to the high-level principle of the FF heuristic described in Section 3.2.1, but computed on the non-relaxed problem using (distributed) Domain Transition Graphs (DTGs) [10] for the reachability analysis. The benefit of DTGs is that they are not state-dependent, allowing to cache the results and thus lower the communication load.
       </paragraph>
       <paragraph>
        The FF heuristic was first distributed in [8], where the aim was to compute provably the same heuristic estimates as the centralized FF would give on the global problem. The approach was based on synchronized reachability analysis computed by all agents, which was shown to be rather impractical. To improve the computation speed, a lazy alternative was used, where the relaxed plan extraction process was distributed instead of the reachability analysis, which was kept local and computed only when necessary (lazily).
       </paragraph>
       <paragraph>
        In [6], a general scheme for computing distributed relaxation heuristics was proposed. The FF heuristic is again present in two variants, one based on the distribution of the reachability analysis, and the other one based on the distribution of the relaxed plan extraction.
       </paragraph>
       <paragraph>
        In the following section, we present an improved distributed FF heuristic algorithm as a part of the MADLA Planner. The novel distributed FF heuristic is based on the idea of distributing the relaxed plan extraction process, while performing the reachability analysis lazily, that is only when requested. The novel aspect of the algorithm is the use of a set-additive principle [24] to overcome over-counting and also an adaptation towards more privacy.
       </paragraph>
      </section>
      <section label="3.2.3">
       <section-title>
        Privacy-preserving set-additive FF
       </section-title>
       <paragraph>
        The two leading ideas of the novel distribution of the FF heuristic (ppsaFF) are the use of the lazy approach and the idea of the set-additive heuristic [24]. The lazy principle states that the distributed heuristic first starts as a projected FF, computing the reachability analysis and relaxed plan as in Section 3.2.1. Such relaxed plan {a mathematical formula}πi+ computed on an i-projected relaxed problem {a mathematical formula}Πi▷+ may contain projected actions, which may have private preconditions. The private preconditions are not satisfied in {a mathematical formula}πi+ as {a mathematical formula}αi is not aware of them. Let {a mathematical formula}a▷+ be such a projected action and let {a mathematical formula}a+∈Aj+ for some {a mathematical formula}j≠i. In that case, {a mathematical formula}αi requests {a mathematical formula}αj to provide a relaxed plan that satisfies the private part of precondition of {a mathematical formula}a+ (the public part is already satisfied in the projected RP). Here the set-additive principle comes into play. In the original lazy FF variant, agent {a mathematical formula}αj would report just the cost of achieving private preconditions of a, which leads to significant over-counting. In the new variant, the agent {a mathematical formula}αj provides the actual relaxed plan {a mathematical formula}πa+, which satisfies the private precondition of {a mathematical formula}a+, that is {a mathematical formula}pre(a+)∩Pjpriv, which can be merged with the original relaxed plan {a mathematical formula}πi+←πi+∪πa+ as both relaxed plans are represented as sets of actions. This request-reply protocol is performed for all projected actions in {a mathematical formula}πi+, even those newly received from {a mathematical formula}αj and even for public actions of {a mathematical formula}αi itself received in {a mathematical formula}πa+ (in which case the request can be handled by an internal call).
       </paragraph>
       <paragraph>
        At this moment we have possibly violated privacy by sharing a relaxed plan {a mathematical formula}πa+ which may contain private actions of agent {a mathematical formula}αj, even though we share only a unique identifier of that private action and no preconditions or effects. In order to treat the privacy correctly, the algorithm has to be further modified.
       </paragraph>
       <paragraph>
        Instead of sending back the relaxed plan {a mathematical formula}πa+, agent {a mathematical formula}αj builds a local relaxed reply plan {a mathematical formula}πj,iRE+ for the requesting agent {a mathematical formula}αi (we always write the index of the agent owning the variable/data structure first and the other agent it relates to second), which is updated for every requested action {a mathematical formula}a+ as {a mathematical formula}πj,iRE+=πj,iRE+∪πa+. To maintain privacy, {a mathematical formula}αj keeps the private part of the plan locally, that is {a mathematical formula}πj,ipriv+=πj,iRE+∩Ajpriv+ and sends only its public part {a mathematical formula}πj,ipub+=πj,iRE+∩Ajpub+ together with the length of the private part {a mathematical formula}lj,i=|πj,ipriv+|. The relaxed plan {a mathematical formula}πj,iRE+ is maintained by {a mathematical formula}αj throughout the whole computation of the heuristic estimate for the agent {a mathematical formula}αi and a single state s, so that each action of {a mathematical formula}αj is counted at most once. This is made easier by the fact that each agent is computing the distributed heuristic for at most one state, thus the agent has to keep track of at most {a mathematical formula}|A|−1 relaxed plans of other agents. Meanwhile, agent {a mathematical formula}αi builds a single relaxed plan {a mathematical formula}πi+ containing actions from {a mathematical formula}Ai▷+ and a value {a mathematical formula}li,jpriv for each agent {a mathematical formula}αj≠i representing the length of the private part of the relaxed plan of agent {a mathematical formula}αj. After all projected actions in {a mathematical formula}πi+ are processed (that is all replies received), the resulting heuristic value is computed as:{a mathematical formula}
       </paragraph>
       <paragraph>
        For clarity, the algorithm is transcribed into pseudo-code in Algorithm 5. The algorithm is split into three procedures, each has the agent which is performing the procedure (i.e., is written from its perspective) as its first parameter. The procedures work as follows.
       </paragraph>
       <paragraph>
        The main procedure computeDistributedFF({a mathematical formula}αi,s▷i,〈δ1,...,δn〉,Πi▷+) is called by the search to evaluate a state {a mathematical formula}s▷i by agent {a mathematical formula}αi (shown as a call to {a mathematical formula}hDi at line 5 in Algorithm 2). After the initialization steps, the relaxed plan {a mathematical formula}πi+ is computed such that {a mathematical formula}πi+ achieves the goal G in the projected relaxed problem (Equation (1)), using actions from {a mathematical formula}Ai▷+. Then, while there is some projected action {a mathematical formula}a▷+ in {a mathematical formula}πi+ which has not been processed yet (i.e., is not in {a mathematical formula}ADONE), process it by sending a request {a mathematical formula}MREQUEST=〈αi,s▷pub,δj,a▷+〉 to the agent {a mathematical formula}αj, the owner of {a mathematical formula}a+ (the owner of an action is known by definition). The loop also does not terminate while there are some actions in {a mathematical formula}AWAITING, which means a reply has not been received for them (a request is not sent if there is no unprocessed action in {a mathematical formula}πi+, this condition has been omitted for simplicity). When all projected actions and replies are processed, the heuristic value (Equation (2)) is returned. Note that the actual implementation differs from the pseudocode in that the loop is implemented as an asynchronous event-based message processing (i.e., waiting for replies from other agents is non-blocking).
       </paragraph>
       <paragraph>
        When the agent {a mathematical formula}αj receives a request from the agent {a mathematical formula}αi to evaluate private preconditions of some action {a mathematical formula}a+∈Ajpub+, the procedure processRequest({a mathematical formula}αj, {a mathematical formula}MREQUEST=〈αi,s▷pub,δj,a▷+〉) is called. Agent {a mathematical formula}αj first reconstructs the state {a mathematical formula}s▷j and then computes a relaxed plan {a mathematical formula}πa+, which solves the j-projected relaxed problem of {a mathematical formula}αj starting in {a mathematical formula}s▷j with goal being the private preconditions of {a mathematical formula}a+, formally {a mathematical formula}pre(a+)∩Pjpriv. The computed relaxed plan (RP) is then used to update the reply RP {a mathematical formula}πj,iRE+, whose private length {a mathematical formula}lj,i←|πj,iRE+∩Ajpriv+| is sent back to {a mathematical formula}αi together with the public part of {a mathematical formula}πj,iRE+. The public part of {a mathematical formula}πj,iRE+ consists of public actions of {a mathematical formula}αj and all projected actions of {a mathematical formula}αk≠j including projected actions of {a mathematical formula}αi, which are in {a mathematical formula}πa+. Note that the reply RP {a mathematical formula}πj,iRE+ is kept for each agent {a mathematical formula}αi over all requests regarding one particular state {a mathematical formula}s▷j. When the agent {a mathematical formula}αj receives a request for another state {a mathematical formula}s′ from the agent {a mathematical formula}αi, the reply RP {a mathematical formula}πj,iRE+ is initialized to {a mathematical formula}πj,iRE+=∅ first. This works thanks to the fact that each agent {a mathematical formula}αk computes the heuristic estimate for at most one state at any moment.
       </paragraph>
       <paragraph>
        When a reply message {a mathematical formula}MREPLY=〈πj,ipub+,lj,i,a▷+〉 is received from the agent {a mathematical formula}αj for the request {a mathematical formula}MREQUEST=〈αi,s▷pub,δj,a▷+〉 by agent {a mathematical formula}αi, the procedure processReply({a mathematical formula}αi,MREPLY=〈πj,ipub+,lj,i,a▷+〉) is called. The relaxed plan {a mathematical formula}πi+ is updated with the received public part and the estimate of the private part for agent {a mathematical formula}αj is replaced with the new received value. Action {a mathematical formula}a▷+ is removed from {a mathematical formula}AWAITING as its processing has been finished.
       </paragraph>
       <paragraph>
        Note that the search and heuristic estimation is all running in a single thread and all messages are managed through a message queue and the calls on the line 6 and line 21 in Algorithm 1. This means that the procedure computeDistributedFF() is called once (line 5 of Algorithm 2) and the loop on line 7 is in fact managed through callbacks and thus the call to the heuristic computation is asynchronous. Meanwhile, the processRequest() and processReply() procedures are called in response to the messages received on line 25 of Algorithm 3, sequentially, one at a time.
       </paragraph>
       <paragraph>
        We illustrate the process on the running example. Let us start with a situation, where the truck is computing the heuristic estimate for the initial state. First, the projected relaxed plan {a mathematical formula}πt+ is computed{a mathematical formula} In the {a mathematical formula}t-projected problem, the unload action of plane has no preconditions and it fulfills the goal. Next, the truck sends a request to the plane, which computes a relaxed plan from the initial state to the private precondition of {a mathematical formula}unload-a-C, which is {a mathematical formula}{package-in-a,plane-at-C}. The plane comes up with the following relaxed plan{a mathematical formula} and sends back a reply containing only the public actions and the number of private actions which is 1. The truck accordingly updates its relaxed plan to{a mathematical formula} and proceeds by sending requests for the newly added actions. The request for {a mathematical formula}load-a-B does not have to be sent as it was received from the plane (this optimization is ignored in the algorithm for simplicity). The request for {a mathematical formula}unload-t-B has to be sent, but as the receiver is the truck itself, it can be forwarded via a direct call. Also the private actions of the truck are directly included in the relaxed plan {a mathematical formula}πt+ by the local computation. The resulting relaxed plan is{a mathematical formula} with the additional number of private actions of the plane being 1, thus the complete heuristic estimate is {a mathematical formula}hppsaFF(sI)=5+1=6.
       </paragraph>
       <paragraph>
        This wraps up the description of the novel distributed variant of the FF heuristic. We did not pay attention to the actual process of finding the relaxed plan, as the distribution is general so that any approach can be used and the reachability analysis is kept local (computed on the projected relaxed problem).
       </paragraph>
      </section>
      <section label="3.2.4">
       <section-title>
        Termination of the privacy-preserving set-additive FF
       </section-title>
       <paragraph label="Theorem 10">
        Now we formally show that the Privacy-Preserving Set-Additive FF heuristic always terminates. Assuming liveness of the communication, the heuristic ppsaFF shown inAlgorithm 5always terminates.
       </paragraph>
       <paragraph label="Proof">
        Let s be the state the ppsaFF heuristic is computed for by agent {a mathematical formula}αi. If the goal is not reachable from s in {a mathematical formula}Πi▷+ then the computation of computeRelaxedPlan(s, G, {a mathematical formula}Ai▷+) will fail and ∞ is returned. Otherwise, {a mathematical formula}πi+ contains a finite number of (relaxed) actions. For each action {a mathematical formula}a▷+∈πi+∖ADONE such that {a mathematical formula}a+∈Aj+∧j≠i a request is sent to the action owner {a mathematical formula}αj. The computation of the reply, that is computeRelaxedPlan({a mathematical formula}s,pre(a+)∩Pjpriv,Aj▷+) always finishes, with either a finite non-empty or an empty plan {a mathematical formula}πa+. When the reply is received, the action a is added to {a mathematical formula}ADONE and the public actions in {a mathematical formula}πa+ are added to {a mathematical formula}πi+. In this step, a finite (but possibly zero) number of actions is added to {a mathematical formula}πi+ and the number of actions in {a mathematical formula}ADONE increases by 1 as a is added. As the number of actions in A is finite (and so is the number of public actions), and in each iteration, the number of actions in {a mathematical formula}ADONE increases, eventually, the set of actions {a mathematical formula}a▷+∈πi+∖ADONE such that {a mathematical formula}a+∈Aj+∧j≠i becomes empty and the computation terminates. □
       </paragraph>
      </section>
      <section label="3.2.5">
       <section-title>
        Suitability of the projected and privacy-preserving set-additive FF for the MADLA search
       </section-title>
       <paragraph>
        The MADLA Search places a number of assumptions on the properties of the pair of heuristics it operates with. Here, we examine, how these assumptions hold for projected FF and Privacy-Preserving Set-Additive FF. Let us recall and expand upon the assumptions, where i is a required property and ii and iii are desirable properties:
       </paragraph>
       <list>
        <list-item label="i)">
         The distributed heuristic {a mathematical formula}hDi is non-blocking. This is a required property, meaning that the distributed heuristic allows the agent computing it to run other computations meanwhile (all in a single computational process). The motivation behind this is the expectation that a distributed heuristic will communicate with other agents and while waiting for replies, the search will continue using the local heuristic.
        </list-item>
        <list-item label="ii)">
         The distributed heuristic {a mathematical formula}hDi is better informed than the local heuristic {a mathematical formula}hLi, i.e., a single-heuristic search using {a mathematical formula}hDi expands fewer states than the same search using {a mathematical formula}hLi. This assumption is not strict, but using a less informed distributed heuristic gives no advantage. Although it cannot be guaranteed in general, it can be expected that for most states the dominance holds.
        </list-item>
        <list-item label="iii)">
         The local heuristic {a mathematical formula}hLi is easier to compute than the distributed heuristic {a mathematical formula}hDi. Again, this assumption is not strict. Let us assume unit-time atomic computational steps used by both heuristic estimators (i.e., procedures computing the heuristic functions), in that sense, {a mathematical formula}hLi is assumed to take less steps than {a mathematical formula}hDi for a given state s. As the assumption is not strict, it suffices to hold for a significant number of states. The reasoning behind this assumption is that in conjunction with assumption ii it does not make sense to use a less informed local heuristic which takes longer time to compute.
        </list-item>
       </list>
       <paragraph>
        Let us have a look how the proposed projected FF and distributed FF algorithms adhere to assumptions i–iii. As addressed in the description of Algorithm 5, the loop on lines 7–10 is actually implemented as an asynchronous event-based message processing, which means that after all messages are sent, another computation can proceed until some reply message is received. This can be utilized by the MADLA Search as shown in Algorithm 1, where on line 15 the processNode({a mathematical formula}αi,u,d) is called with the parameter {a mathematical formula}d=true and thus Algorithm 2 proceeds with the asynchronous call to the distributed heuristic on line 5 and the search can continue with the local search loop on lines 17–21 in Algorithm 1. Notice that the communication is processed also inside the local search loop, thus if a reply message is received, the local search loop is exited as the {a mathematical formula}busyD flag is set to {a mathematical formula}true. This behavior assures that the ppsaFF heuristic adheres to Assumption i.
       </paragraph>
       <paragraph>
        To assess the assumptions ii and iii we first observe that the initial phase of the distributed heuristic computation as shown in Algorithm 5 is to compute the projected relaxed plan (line 3), which is exactly how the projected FF is computed. After that, some additional steps are performed, in order to improve the quality of the relaxed plan estimation. This means that the number of computational steps performed by the distributed FF is always at least as high as the number of steps performed by the projected FF or higher, thus confirming the assumption iii.
       </paragraph>
       <paragraph>
        For similar reasons as above, it can be expected that the informativeness of the distributed heuristic would be higher as information is only added, nevertheless, the informativeness of a heuristic is best assessed by an experimental evaluation. Here we refer to the Section 4 where the number of expanded states is compared.
       </paragraph>
      </section>
     </section>
     <section label="3.3">
      <section-title>
       Soundness and completeness
      </section-title>
      <paragraph>
       Here we present the proofs of soundness and completeness of the MADLA Search algorithm. The proofs are based on proofs for classical single-agent single-heuristic search.
      </paragraph>
      <paragraph>
       Before delving into the proof, we state the assumptions on the used heuristics. First, the heuristics always terminate. Second, the heuristics are safe, i.e., if {a mathematical formula}h(s)=∞ then {a mathematical formula}h⁎(s)=∞ where {a mathematical formula}h⁎ is the perfect heuristic. In other words, if the heuristic reports a dead-end, it truly is a dead-end (note that the other direction of the implication does not have to hold, a heuristic (e.g., FF) can report a finite heuristic value for a state which actually is a dead-end).
      </paragraph>
      <paragraph>
       Throughout the section, we use the notion of a search node from Definition 8. Note that the search node is agent-specific, that is a search node {a mathematical formula}u=〈s▷i,p,ap,h,g,αi,〈δ1,...,δn〉〉 is known to and manipulated by agent {a mathematical formula}αi only. Subsequently, a search node represents a global state s as an i-projection {a mathematical formula}s▷i and a tuple {a mathematical formula}Δ=〈δ1,...,δn〉 encoding the private parts of all agents in {a mathematical formula}A. We use the dot notation {a mathematical formula}u.state=s▷i, {a mathematical formula}u.action=a, {a mathematical formula}u.parent=p (that is {a mathematical formula}u.parent=u′ or {a mathematical formula}u.parent=αj), {a mathematical formula}u.h=h, {a mathematical formula}u.g=g, {a mathematical formula}u.agent=αi, {a mathematical formula}u.uids=Δ=〈δ1,...,δn〉 and {a mathematical formula}u.uidi=δi.
      </paragraph>
      <paragraph label="Definition 11">
       We define a global equality relation for search nodes as follows. Let {a mathematical formula}u,u′ be search nodes such that {a mathematical formula}u.agent=αi and {a mathematical formula}u′.agent=αj and let {a mathematical formula}s,s′ be global states reconstructed from {a mathematical formula}u.state and {a mathematical formula}u′.state respectively using all private parts defined in {a mathematical formula}u.uids and {a mathematical formula}u′.uids respectively. Then the search nodes u and {a mathematical formula}u′ are globally equal iff the states s and {a mathematical formula}s′ are equal, formally {a mathematical formula}u=Gu′⇔s=s′.
      </paragraph>
      <paragraph>
       In other words, two search nodes are globally equal if the global states represented by them are equal.
      </paragraph>
      <section label="3.3.1">
       <section-title>
        Soundness
       </section-title>
       <paragraph label="Definition 12">
        First, we define the sequence of search nodes corresponding to a particular run of the algorithm. A path with a resulting search node{a mathematical formula}ul is a sequence of search nodes {a mathematical formula}path(ul)=(u0,...,ul), possibly of different agents (different k s.t. {a mathematical formula}1≤k&lt;l can exist for which {a mathematical formula}uk.agent≠uk+1.agent). The search nodes do not repeat, although the respective states may repeat.A path is a valid path iff {a mathematical formula}u0=uI is the initial node for some agent {a mathematical formula}αI and for each k s.t. {a mathematical formula}1≤k≤l: {a mathematical formula}uk−1=uk.parent and {a mathematical formula}ak=uk.action is applicable in {a mathematical formula}uk−1.state, or {a mathematical formula}uk.agent=αi and {a mathematical formula}uk.parent=αj s.t. {a mathematical formula}i≠j, in which case {a mathematical formula}uk−1.agent=αj and {a mathematical formula}uk=Guk−1.
       </paragraph>
       <paragraph label="Definition 13">
        Informally, a path represents one particular trace of the exploration of the search space. In the case a state was reached through expansions of multiple agents, agent {a mathematical formula}αj sends a state {a mathematical formula}sk−1 to agent {a mathematical formula}αi which creates a new search node {a mathematical formula}uk from it. This results in a sequence of search nodes {a mathematical formula}(u0,...,uk−1,uk,...,ul) where the search nodes {a mathematical formula}uk−1,uk represent the same state {a mathematical formula}sk−1, but {a mathematical formula}uk−1 was created by {a mathematical formula}αj (using a public action) and {a mathematical formula}uk was created subsequently (after receiving the message) by {a mathematical formula}αi, setting {a mathematical formula}uk.parent to {a mathematical formula}αj (see Algorithm 3, line 7). Next, we define the corresponding sequence of actions. For a path {a mathematical formula}path(ul)=(u0,u1,...,ul), we say {a mathematical formula}path(ul)-plan is a sequence of actions {a mathematical formula}(a1,...,al) s.t. {a mathematical formula}ak=uk.action and {a mathematical formula}ak∈A∪{ϵ} for all {a mathematical formula}1≤k≤l. For all {a mathematical formula}ak it holds that either {a mathematical formula}ak∈Ai where {a mathematical formula}αi=uk.agent or {a mathematical formula}ak=ϵ if {a mathematical formula}uk was received form another agent (that is {a mathematical formula}uk−1.agent≠uk.agent). The {a mathematical formula}path(ul)-plan is a valid{a mathematical formula}path(ul)-plan iff {a mathematical formula}path(ul) is a valid path.
       </paragraph>
       <paragraph>
        A trivial consequence of the above definitions and Definition 1 is that a valid {a mathematical formula}path(ul)-plan {a mathematical formula}(a1,...,al) is a multi-agent plan solving {a mathematical formula}M iff {a mathematical formula}ul.state is a goal state, that is {a mathematical formula}G⊆ul.state (remember that the goal is public, i.e. {a mathematical formula}G⊆Ppub).
       </paragraph>
       <paragraph label="Lemma 14">
        To prove the soundness, the following lemma will be shown first: (Invariant) At any given step of the MADLA Search, for any search node{a mathematical formula}uL∈OLiand any search node{a mathematical formula}uD∈ODifor any agent{a mathematical formula}αi,{a mathematical formula}path(uL)and{a mathematical formula}path(uD)are valid paths.
       </paragraph>
       <paragraph label="Proof">
        In the initial step for every agent, {a mathematical formula}OLi=ODi={uI}, where {a mathematical formula}path(uI)=(uI), which is a valid path. There are two possibilities where new nodes are added to any of the open lists. In expand({a mathematical formula}u,d), regardless of the d parameter, for each action applicable in {a mathematical formula}u.state, a new search node {a mathematical formula}u′=〈s′▷i,u,a,h′,g+1,αi,Δ〉 is created such that {a mathematical formula}s′▷i=u.state[a]. If we assume that {a mathematical formula}path(u)=(uI,...,u) is a valid path, then after expansion, {a mathematical formula}path(u′)=(uI,...,u,u′) is also valid for each new {a mathematical formula}u′.A node may be received from another agent in processComm(). Assume that for some node {a mathematical formula}uk=〈sk▷j,uk−1,ak,hk,gk,αj,Δk〉, {a mathematical formula}path(uk) is a valid path and {a mathematical formula}uk is a first node in {a mathematical formula}path(uk) expanded using a public action {a mathematical formula}ak∈Ajpub. According to Algorithm 2, line 9, {a mathematical formula}uk is sent to {a mathematical formula}αi as a message {a mathematical formula}MSTATE=〈s▷pub,Δk,h,g,d〉. The message is received by {a mathematical formula}αi and a new search node {a mathematical formula}uk+1←〈sk▷i,αj,ϵ,hk,gk,αi,Δk〉 is created (based on {a mathematical formula}Δk and the state reconstruction function {a mathematical formula}μi) and added to either open list based on the parameter d. According to Definition 12, {a mathematical formula}path(uk+1) is a valid path. There is no other possible way a node could be added to any of the open lists. □
       </paragraph>
       <paragraph label="Theorem 15">
        When the MADLA Search terminates and returns a solution, it is a distributed multi-agent plan solving{a mathematical formula}M.
       </paragraph>
       <paragraph label="Proof">
        The algorithm terminates on line 23 of Algorithm 3. In each step, either an action {a mathematical formula}ak=uk.action is added to the solution {a mathematical formula}πmi for the solution initiated by agent {a mathematical formula}αm and continues the recursion on {a mathematical formula}uk−1=uk.parent (if {a mathematical formula}uk.action≠ϵ), or a {a mathematical formula}MRECONSTRUCT=〈uk▷pub.state,uk.uidj,t,αm〉 message is sent to {a mathematical formula}αj=uk.agent. When received, the reconstructPlan() procedure is called on a node {a mathematical formula}uk′ s.t. {a mathematical formula}uk′=Guk and {a mathematical formula}uk′.action is public ({a mathematical formula}uk′ is obtained from {a mathematical formula}μi). Thanks to the condition on line 6 of Algorithm 4, upon termination, the last action added to the returned solution {a mathematical formula}πmi is the action that was applied on the initial node {a mathematical formula}uI. This ensures that the recursion proceeds along the path {a mathematical formula}path(ut), where {a mathematical formula}ut is the node on which reconstructPlan() was first called.Apart from the recursive call, the procedure reconstructPlan() is called only from line 30 of Algorithm 1, where {a mathematical formula}ut was extracted from {a mathematical formula}OLi or {a mathematical formula}ODi. From Lemma 14 it follows that {a mathematical formula}path(ut) is a valid path. Because on line 30 of Algorithm 1, {a mathematical formula}G⊆ut.state always holds, the {a mathematical formula}path(ut)-plan corresponding to {a mathematical formula}path(ut) is a multi-agent plan π solving {a mathematical formula}M. As each agent {a mathematical formula}αi adds to {a mathematical formula}πmi only actions from {a mathematical formula}Ai (and ϵ actions as padding, see Algorithm 4 line 4), the resulting set {a mathematical formula}{πmi}i=0n is a distributed multi-agent plan solving {a mathematical formula}M for each m as the reconstructions are independent if started by different agents and each agent can start the reconstruction of at most one plan. The final plan is chosen consistently and uniquely as it is the shortest plan, ties broken based on unique agent indices (ordering). □
       </paragraph>
      </section>
      <section label="3.3.2">
       <section-title>
        Completeness
       </section-title>
       <paragraph>
        To show completeness, we first consider a modification of the algorithm such that any reachable state is expanded eventually. The modified algorithm is named {a mathematical formula}MADLA+ Search in which the condition on lines 25–30 in Algorithm 1 are ignored and the algorithm terminates only when both {a mathematical formula}OLi and {a mathematical formula}ODi are empty for all agents {a mathematical formula}αi and no messages are pending, which can be detected using the distributed snapshot algorithm [21].
       </paragraph>
       <paragraph label="Lemma 16">
        In the{a mathematical formula}MADLA+Search, each state s is added to{a mathematical formula}ODiand to{a mathematical formula}OLiof any agent{a mathematical formula}αiat most finite times, each time represented by a different search node.
       </paragraph>
       <paragraph label="Lemma 17">
        Because the number of possible search nodes (with respect to the state {a mathematical formula}s▷i and the set of private unique identifiers Δ) is finite as each search node represents one state of the finite sets of states ({a mathematical formula}2|P| since {a mathematical formula}s⊆P) and the number of actions (of each agent) is also finite, each expansion produces a finite number of search nodes consequently added to {a mathematical formula}ODi, {a mathematical formula}OLi or both (line 17 and 19 of Algorithm 2 respectively). If a search node u is extracted from {a mathematical formula}OLi or {a mathematical formula}ODi, it is added to the closed list {a mathematical formula}Ci and no search node {a mathematical formula}u′ s.t. {a mathematical formula}u=Gu′ is ever added to any of the open lists of agent {a mathematical formula}αi again.Another possibility of adding a search node to an open list is when it is received from another agent. A state s is sent by the agent {a mathematical formula}αi, only if a search node {a mathematical formula}u′ (representing a different state {a mathematical formula}s′) was extracted from either {a mathematical formula}OLi or {a mathematical formula}ODi and a public action {a mathematical formula}a∈Aipub was applied. Since there is a finite number of public actions and a finite number of agents and each action is applicable only in finite number of states (which are then placed into {a mathematical formula}Ci and never expanded again), state s can be sent and received only a finite number of times. □In the{a mathematical formula}MADLA+Search, each search node in{a mathematical formula}OLiand in{a mathematical formula}ODiof all agents{a mathematical formula}αiis eventually extracted.
       </paragraph>
       <paragraph label="Theorem 18">
        In each step of the outer search cycle (lines 6–21 of Algorithm 1), a node is extracted from {a mathematical formula}ODi, if {a mathematical formula}hDi is not busy. Since {a mathematical formula}hDi always terminates, any finite number of nodes can be extracted in finite time. From Lemma 16 follows that only a finite number of nodes may be added to {a mathematical formula}ODi, therefore {a mathematical formula}ODi eventually becomes empty. When {a mathematical formula}ODi is empty, in each step of the inner search cycle a node is extracted from {a mathematical formula}OLi. Following the same reasoning as before, {a mathematical formula}OLi eventually becomes empty as well. □The MADLA{sup:+}Search terminates.
       </paragraph>
       <paragraph label="Proof">
        Follows directly from Lemma 16 and Lemma 17. □ Recall that for states {a mathematical formula}s0 and {a mathematical formula}sm a {a mathematical formula}s0–{a mathematical formula}sm-plan is a sequence of actions {a mathematical formula}π=(a1,...,am) from A (the actions may be from different agents and may repeat) if there are states {a mathematical formula}s1,...,sm s.t. for all k in {a mathematical formula}1≤k≤m, action {a mathematical formula}ak is applicable in {a mathematical formula}sk−1 and {a mathematical formula}sk=sk−1[ak]. For short, {a mathematical formula}s0[π] denotes the resulting state {a mathematical formula}sm. We extend this notion to the search nodes the same way action application was extended (i.e., the actions are applied on the respective nodes).
       </paragraph>
       <paragraph label="Definition 19">
        We define the notion of reachability for the multi-agent planning problem {a mathematical formula}M as follows. A state {a mathematical formula}s⊆P is reachable in {a mathematical formula}M iff a {a mathematical formula}sI–s-plan {a mathematical formula}π=(a1,...,am) exists such that {a mathematical formula}sI is the initial state. We say that s is reachable by agent {a mathematical formula}αi iff {a mathematical formula}am∈Ai.
       </paragraph>
       <paragraph label="Lemma 20">
        Now we show equality of reachability and the existence of a valid path. A state s is reachable in{a mathematical formula}Mby agent{a mathematical formula}αiiff a valid path{a mathematical formula}path(u)=(uI,...,u)exists such that{a mathematical formula}uI.state=sI▷jfor some agent{a mathematical formula}αj,{a mathematical formula}u.state=s▷i,{a mathematical formula}u.uidsrepresent private parts of s and{a mathematical formula}u.agent=αi.
       </paragraph>
       <paragraph label="Definition 21">
        If a valid path exists, the corresponding {a mathematical formula}path(u)-plan proves the reachability. If a state s is reachable in {a mathematical formula}M by {a mathematical formula}αi, there exists the sequence {a mathematical formula}π=(a0,...,al) of actions from Definition 19 (and {a mathematical formula}al∈Ai). Let {a mathematical formula}a0∈Aj. Since {a mathematical formula}a0 is applicable in {a mathematical formula}sI▷j=uI.state, it will be applied by the agent {a mathematical formula}αj and the resulting search node {a mathematical formula}u1 will be added to its open lists. Additionally, if {a mathematical formula}a0∈Ajpub, {a mathematical formula}s1▷pub=u1.state together with {a mathematical formula}u1.uids will be sent to other agents. According to the Definition 19, {a mathematical formula}a1 is applicable in {a mathematical formula}s1▷j. If {a mathematical formula}a1∈Aj the process is repeated. If {a mathematical formula}a1∈Ak≠j, {a mathematical formula}a1 is applicable in the received state {a mathematical formula}s1▷k and thus will be applied by agent {a mathematical formula}αk. By induction we conclude that {a mathematical formula}path(u)=(uI,...,u) is a valid path. □ Now, we provide an alternative definition of reachability in {a mathematical formula}M, with the focus shifted on the agents. We use the bracketed index (k) to annotate the k-th agent in the sequence which is not the same as the agent {a mathematical formula}αk∈A. A state s is reachable in {a mathematical formula}M by a sequence of agents {a mathematical formula}ϖ=(α(1),...,α(m)) of length {a mathematical formula}m+1 (agents in ϖ can repeat and an agent can perform zero actions or a no-op action ϵ) if a sequence of {a mathematical formula}u1(i)–{a mathematical formula}uki(i)-plans {a mathematical formula}(π¯1,...,π¯m) exists such that each {a mathematical formula}π¯i contains only actions from {a mathematical formula}A(i), {a mathematical formula}π¯1 is applicable in {a mathematical formula}uI and for each i s.t. {a mathematical formula}1≤i≤m, {a mathematical formula}uki−1(i−1)=Gu1(i).
       </paragraph>
       <paragraph>
        Informally, in the sequence of agents, each agent performs a sequence of actions, such that the final resulting state is s.
       </paragraph>
       <paragraph label="Lemma 22">
        The following lemma uses either the assumption that the used heuristics are safe, or requires the states evaluated as dead ends to be placed in the open-list nonetheless, with the heuristic value of ∞. This is necessary to make sure that a reachable state is never unreached only because of the heuristic evaluation. If a state s is reachable in{a mathematical formula}Mby the sequence of agents{a mathematical formula}ϖ=(α(1),...,α(m))of length m, it is placed into the closed list{a mathematical formula}C(m)after a finite number of steps.
       </paragraph>
       <paragraph label="Proof">
        We will show the proof by induction in the number of agents m.If a state s is reachable by a sequence containing single agent {a mathematical formula}ϖ=(αi), then assume that no search node u such that {a mathematical formula}s=u.state is ever placed in {a mathematical formula}Ci. Since s is reachable, based on Lemma 20 there exists a search node u s.t. {a mathematical formula}u.state=s and {a mathematical formula}path(u)=(uI,...,u). Let {a mathematical formula}um be the first search node in {a mathematical formula}path(u), s.t. {a mathematical formula}um is not added to {a mathematical formula}Ci. Note that there exists an action {a mathematical formula}a∈Ai, such that {a mathematical formula}um=um−1[a]. Since at some point {a mathematical formula}um−1∈Ci, {a mathematical formula}um−1 must have been taken from {a mathematical formula}OLi or {a mathematical formula}ODi. At that point, {a mathematical formula}um−1 was also expanded and because a is applicable in {a mathematical formula}um−1.state it must have been applied. The resulting node {a mathematical formula}um=um−1[a] was added to either {a mathematical formula}OLi or {a mathematical formula}ODi and because of Lemma 17, {a mathematical formula}um was eventually extracted and added to {a mathematical formula}Ci. This contradicts the assumption that {a mathematical formula}um is not added to {a mathematical formula}Ci.Let us now assume that for all k s.t. {a mathematical formula}k≤m if a node is reachable by a sequence of agents {a mathematical formula}ϖ=(α(1),...,α(k)) of length k, it is added to {a mathematical formula}C(k) after finite many steps. We will show that the same holds if a node is reachable by a sequence of agents {a mathematical formula}ϖ′=(α(1),...,α(m),α(m+1)) of length {a mathematical formula}m+1. We will show the induction step by a contradiction. For the contradiction let us assume that s is a state reachable by sequence of agents {a mathematical formula}ϖ′ of length {a mathematical formula}m+1, but no search node u such that {a mathematical formula}s=u.state is ever added to {a mathematical formula}C(m+1). Let {a mathematical formula}path(u)=(u1,...,u) and let {a mathematical formula}ul be the first node that is never added to {a mathematical formula}C(m+1). One of the following holds:
       </paragraph>
       <list>
        <list-item label="i)">
         {a mathematical formula}ul is reachable by agent {a mathematical formula}αm+1. If so, the same reasoning used in the first part of the proof can be used to obtain a contradiction.
        </list-item>
        <list-item label="ii)">
         {a mathematical formula}ul is reachable by some {a mathematical formula}k′&lt;m+1 agents {a mathematical formula}ϖ″=(α(1),...,α(k′)). In that case, we have a contradiction with the assumption of the induction. □
        </list-item>
       </list>
       <paragraph label="Theorem 23">
        Now we can conclude the proof by the following theorem. The MADLA Search is complete.
       </paragraph>
       <paragraph label="Proof">
        In the MADLA{sup:+} Search, for every state s reachable in {a mathematical formula}M by an agent {a mathematical formula}αi such that {a mathematical formula}s⊆G, a search node u such that {a mathematical formula}s=u.state is eventually placed into {a mathematical formula}Ci (Lemma 22). The first such search node is, after adding it to the closed list {a mathematical formula}Ci, given to the reconstructPlan() procedure which reports a valid multi-agent plan (as MADLA Search is sound, Theorem 15). □
       </paragraph>
      </section>
     </section>
     <section label="3.4">
      <section-title>
       Privacy of the algorithms
      </section-title>
      <paragraph>
       In this section, we analyze the privacy of the MADLA Search, the distributed heuristic from Section 3.2.2 and the combination of both according to Section 2.1.
      </paragraph>
      <section label="3.4.1">
       <section-title>
        Privacy of the MADLA Search
       </section-title>
       <paragraph>
        To discuss the privacy of the MADLA Search, it is important to notice that in terms of privacy, MADLA Search is no different from MAD-A* or MA-BFS. This is due to the fact that MADLA Search uses exactly the same mechanism for sharing states expanded by a public action (see Section 3.1.1). This leads us to the analysis of what information is compromised by the search.
       </paragraph>
       <paragraph>
        According to [3], MA-BFS and thus also the MADLA Search is at least weak privacy-preserving, as no private information is explicitly sent to other agents. Similarly, both MA-BFS and thus also the MADLA Search are not strong privacy-preserving as some additional information can be deduced from the search. Here we analyze what this information is.
       </paragraph>
       <paragraph>
        Recall the definition of a valid path {a mathematical formula}path(u)=(u0,...,u) (Definition 12), a sequence of search nodes leading from the initial search node {a mathematical formula}u0=uI to the search node u, such that each consecutive search node (except for {a mathematical formula}uI) is a result of application of an action on the previous one (or a result of receiving a message in which case the represented state is the same as the state represented by the previous node). If {a mathematical formula}αj=uk.agent≠uk+1.agent=αi holds for two nodes, then {a mathematical formula}uk−1 was expanded by the agent {a mathematical formula}αj using a public action, resulting in a search node {a mathematical formula}uk which was via its state sent to {a mathematical formula}αi, resulting in {a mathematical formula}uk+1. In order to reconstruct the part of {a mathematical formula}uk+1 private to {a mathematical formula}αi, the agent is able to determine the last search node {a mathematical formula}ul in {a mathematical formula}path(u) s.t. {a mathematical formula}l&lt;k−1 and {a mathematical formula}ul.agent=αi (in an extreme, this may be {a mathematical formula}uI which is known to all agents). If {a mathematical formula}ul≠uI, the search node was expanded from {a mathematical formula}ul−1 using a public action and sent to all agents, including {a mathematical formula}αj. We say that {a mathematical formula}ul is an i-parent of {a mathematical formula}uk+1. Informally, an i-parent of a search node of agent {a mathematical formula}αi is the last predecessor of the search node which is also of agent {a mathematical formula}αi. All states in between are of other agents. We also say that {a mathematical formula}uk+1 is an i-successor of {a mathematical formula}ul and {a mathematical formula}i-succ(ul) is a set of all i-successors of {a mathematical formula}ul.
       </paragraph>
       <paragraph>
        An important observation an agent may make is that for two search nodes {a mathematical formula}u,u′ s.t. the i-projections of their respective states are equal it holds that {a mathematical formula}i-succ(u)≠i-succ(u′). This indicates that for each search node {a mathematical formula}udif∈i-succ(u)∖i-succ(u′), there is some action a in the u–{a mathematical formula}udif-plan which is not applicable in the sequence starting with u (analogously for {a mathematical formula}i-succ(u′)∖i-succ(u)), which can be only due to an unknown private proposition(s) with a different value in u and {a mathematical formula}u′. The agent can now deduce that u and {a mathematical formula}u′ are in fact two different nodes. Although a may be private, the final effect of {a mathematical formula}u,u′ being different is demonstrated by the public action {a mathematical formula}a′=udif′.action, where {a mathematical formula}udif′ is the search node preceding {a mathematical formula}udif. Notice that based on the difference of public parts of u and {a mathematical formula}udif, agent {a mathematical formula}αi can determine the transition in the public projection induced by the action {a mathematical formula}a′. We refer to such actions as state-discerning actions.
       </paragraph>
       <paragraph>
        Let us demonstrate the behavior of state-discerning actions on the example from Section 1. Imagine that the truck has performed the public action {a mathematical formula}unload-t-B on two nodes, resulting in two states which are the same in {a mathematical formula}t-projection, namely {a mathematical formula}s▷t=s′▷t={truck-at-B,package-at-B}. Since the states were results of a public action, they are both sent to the plane. Throughout the search, the truck receives only the t-successor of {a mathematical formula}s▷t, but no t-successor of state {a mathematical formula}s′▷t. This indicates that {a mathematical formula}s▷t and {a mathematical formula}s′▷t were not the same and there is some private fact which discerns them (it was {a mathematical formula}plane-at-B which holds in s, but not in {a mathematical formula}s′). Moreover, as the t-successor of {a mathematical formula}s▷t is {a mathematical formula}{truck-at-B}, that is {a mathematical formula}package-at-B is no longer present, the state-discerning (public) action must have {a mathematical formula}package-at-B in its delete effects. In this problem, {a mathematical formula}load-a-B is the only such action.
       </paragraph>
       <paragraph>
        Clearly, the existence of state-discerning actions enables agents to determine some non-trivial information about the other agent's private parts of the problem, such as existence of a private variable. Combination of multiple state-discerning actions can reveal even the existence of multiple private variables (e.g., if a is applicable in s and {a mathematical formula}s′, but not in {a mathematical formula}s″ and {a mathematical formula}a′ is applicable in s but not in {a mathematical formula}s′, it means that there are at least two private variables). Thus we can conclude that MAD-A* and thus also MADLA Search is not strong privacy-preserving.
       </paragraph>
      </section>
      <section label="3.4.2">
       <section-title>
        Privacy of the distributed heuristic
       </section-title>
       <paragraph>
        The distributed heuristic hides the private information by communicating only public actions and heuristic estimates. This satisfies the requirements for a weak privacy-preserving algorithm. But is there some information that can be deduced from the heuristic computation?
       </paragraph>
       <paragraph>
        It turns out that there is a very similar principle of revealed information as in the search. In the heuristic computation, agent {a mathematical formula}αi directly asks agent {a mathematical formula}αj about applicability of actions. More precisely, if agent {a mathematical formula}αi is computing a heuristic for state s, it computes a projected relaxed plan {a mathematical formula}πi+. If such projected relaxed plan contains a projection of some action {a mathematical formula}a+∈Ajpub, {a mathematical formula}αi requests {a mathematical formula}αj to provide the heuristic estimate of reaching the private preconditions of {a mathematical formula}a+ from s. Agent {a mathematical formula}αj sends a reply containing public actions used to reach the private preconditions and the number of private actions used to reach the private preconditions.
       </paragraph>
       <paragraph>
        Again, agent {a mathematical formula}αi can observe that although from its projection the two states s and {a mathematical formula}s′ are the same, and thus the respective projected relaxed plans {a mathematical formula}πi+ and {a mathematical formula}πi′+ are the same, the replies from {a mathematical formula}αj differ in either the public actions or the number of private actions sent in the reply. This simply means that the states {a mathematical formula}s,s′ are different in the problem of {a mathematical formula}αj. Here, a similar situation to the state-discerning actions in search occurs in the heuristic computation, thus again revealing some non-trivial private information. Again, we conclude that the privacy-preserving set-additive FF heuristic is not strong privacy-preserving.
       </paragraph>
       <paragraph>
        In the example problem and in a similar situation as in the previous section, the replies to a request for {a mathematical formula}unload-a-C would yield 1 as the number of private actions in one case (that is the plane first needs to use {a mathematical formula}move-a-C-B) and 0 in the other case (plane is already at {a mathematical formula}B as {a mathematical formula}plane-at-B holds).
       </paragraph>
       <paragraph>
        Obviously, the combination of weak privacy-preserving search and heuristic is also weak privacy-preserving and as both the search and heuristic are not strong privacy-preserving, also their combination is not strong privacy-preserving.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="4">
     <section-title>
      Implementation and evaluation
     </section-title>
     <paragraph>
      After the formal verification of the proposed MADLA Search, we analyze its practical properties in an experimental evaluation. First, we describe the implementation in more technical detail. Second, we discuss the related work. Third, we compare the building blocks of the planner, namely projected FF and distributed ppsaFF heuristics in a multi-agent single-heuristic search, the pair of the heuristics in a multi-agent multi-heuristic search and finally in the MADLA Search. Fourth, we discuss the properties of the planner on particular planning domains and on various metrics in detail. Finally, we compare the planner with the state-of-the-art distributed multi-agent planners on a multi-agent benchmark set.
     </paragraph>
     <section label="4.1">
      <section-title>
       Implementation
      </section-title>
      <paragraph>
       The MADLA Planner proceeds in a number of stages (similarly to many classical planners), shown in Fig. 5. The input of the planner is a description of the domain and problem in classical PDDL, accompanied with a file in a novel format Agent Domain Description Language (ADDL). The ADDL file lists which objects in the PDDL represent agents.
      </paragraph>
      <paragraph>
       The first stage is the translation of the PDDL input into SAS+ format, which is used internally by the planner. The translation is performed by the standard tool in the Fast-Downward Planning System [10] and thus is centralized (we leave the distribution of the translation process for future work). Next, the bootstrapping part of the MADLA Planner takes the SAS+ and ADDL inputs and partitions the problem for the specified agents.
      </paragraph>
      <paragraph>
       The partitioning process starts with partitioning of (grounded) actions. A grounded action a is assigned to an agent {a mathematical formula}αi if {a mathematical formula}αi (its respective PDDL object) is the first parameter of a mentioned in the ADDL input file representing an agent. Next step is partitioning of the variables and values and determining which of them are public (i.e. those shared among multiple agents).
      </paragraph>
      <paragraph>
       After the partitioning step, each agent is initialized and run in a separate process. From now on, the planning is distributed, each agent is running on its own thread and communicating with other agents via message passing over the TCP/IP protocol. Detecting the nonexistence of a solution in the distributed setting is nontrivial (using the distributed snapshot algorithm) and introduces communication overheads and thus we currently resort to a time limit instead, which is not a problem in practice, as the planner is typically run within a time limit anyway. When a solution is found, we use a variant of the plan reconstruction process slightly more efficient than the described one. The main difference is that if a plan of length l is being reconstructed, the reconstruction process of any plan with length {a mathematical formula}l′ such that {a mathematical formula}l′&gt;l is terminated as soon as it is detected (e.g. when the reconstruction message is received).
      </paragraph>
      <paragraph>
       Also, in the implementation, we use separate closed lists {a mathematical formula}CDi and {a mathematical formula}CLi for the states evaluated using the distributed and the local heuristic respectively, but we are using only a single closed list {a mathematical formula}Ci for the simplicity of the presentation. The use of the separate closed lists improves the coverage of MADLA Search by 2.9% and the use of local heuristic re-computation when state is received by 3.2% (see the next sections). All the theoretical results hold also when using two closed lists as eventually, all states will be in both closed lists.
      </paragraph>
      <paragraph>
       The last implementation detail we present here is related to the non-blocking property of the distributed heuristic (see Section 3.2.2 and Section 3.2.3). The heuristic computation is not ideally non-blocking, as parts of the heuristic are computed locally in the agent's thread and thus do not allow other computation to run at the same time. Such situation is namely when the heuristic is initiated and computes local relaxed plan. Another such situation occurs when the initiator agent needs to satisfy private preconditions of its own public action. The experimental evaluation shows, that in some domains, this violation of the non-blocking property degrades the efficiency of the MADLA Planner.
      </paragraph>
      <paragraph>
       The MADLA Planner is written in Java, its code, the data sets and processing scripts and used benchmarks are available at http://github.com/stolba/MADLAPlanner.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Related work
      </section-title>
      <paragraph>
       The planners for MA-STRIPS and related multi-agent models range in the literature from centralized{sup:3} single-core planners [25], [19], parallel multi-core ones [5], [26], to fully distributed implementations with communication within one physical computer or over the network [5], [6], [23]. In Fig. 6, we summarize the related planners relevant to the MADLA Planner (i.e., compatible with the MA-STRIPS model) with visualization of which planners were compared and how.
      </paragraph>
      <paragraph>
       The first MA-STRIPS planner called Planning First [27] was based on the principles proposed in the MA-STRIPS paper [2]. Planning First coordinates local plans resulting from agents' forward-search planners by a distributed solver of Constraint Satisfaction Problems (DisCSP). Planning First was the first representative of a plan-based (top-down) coordination multi-agent planner, whereas the MADLA Planner is a multi-agent planner based on state-based (bottom-up) coordination. Planning First outperformed a Java implementation of the FF planner JavaFF [28] in the coverage metrics.
      </paragraph>
      <paragraph>
       In the plan-based coordination planners, the process towards the resulting global plan works with complete or partial local plans. The plans are usually locally sound and finding a valid combination of such local plans is the coordinating part of the planner. Provided that the coordination of the local planning processes is based on states, not plans, we talk about state-based coordination of the planning process. This is not the only distinction of multi-agent planners possible. Moreover, one can come up with a planner which uses a mixture of the described coordination techniques. For the context of this article it helps us, however, to explain how our contribution fits in the set of related planners, where are similarities and how the techniques proposed here can help other multi-agent planners. The most notable benefit of state-based coordination planners is the possibility to use state-based heuristic functions not only for local planning, but also for the global (coordination) planning as it uses states both locally and globally. This is also the key motivation for the MADLA Planner being designed as a state-based coordination planner. The key benefit of the plan-based coordination planners is the ability to represent the local (partial) plans compactly.
      </paragraph>
      <paragraph>
       Besides Planning First, other representatives of plan-based coordination planners are Distributed Planning by Graph Merging (DPGM) [9], [29], μ-SAT-PLAN [30] and Best Response Planning (BRP) [31], each using a different method to find the coordinated set of local plans and representation of the plans. Similar to Planning First, DPGM uses a Constraint Satisfaction Problem solver for coordination of the local plans, which are however not planned by a forward-search planner, but represented and extracted from distributed planning graphs. μ-SATPLAN limited to at most two agents represents the plans and their coordination as a SAT problem. BRP planner uses the best-response principle, i.e., it iteratively amends a plan by compatible local plans (not containing mutually exclusive actions) of other agents eventually getting a global multi-agent plan.
      </paragraph>
      <paragraph>
       The plan-based coordination planner Distoplan [32] pioneered an idea of planning by intersection of Finite State Machines (FSM) representing the local plans of the agents. This idea was extended and practically refined in a satisficing planner based on nondeterministic FSMs representing local agents' plans and their merging—Planning State Machine Merging (PSMM) in [26].
      </paragraph>
      <paragraph>
       Highly efficient incomplete planners from the plan-based coordination family, able to outperform the state-of-the-art centralized planners such as LAMA [20], FF [7], or LPG [33], are Multi-Agent Planning by Plan Reuse (MAPR) [19] and its variation called Plan Merging by Reuse (PMR) [34], which additionally proposes a distribution scheme. MAPR and PMR are based on the best-response principle similarly to BRP, sequentially using a plan repairing algorithm (particularly LPG-adapt [35]) to converge to a globally sound plan.
      </paragraph>
      <paragraph>
       The multi-agent state-space search was first used in a planner using distributed variant of the A* algorithm called MAD-A* [5] with admissible projected landmark heuristic LM-cut [36] and Merge&amp;Shrink [37]. MADLA Search described in the previous sections is a Multi-Agent Greedy Best-First Search (MA-BFS) variant of MAD-A* with inadmissible heuristics. The MA-BFS with projected FF heuristic evaluated in Section 4.3.2 represents the same search algorithm and projected heuristic as used in the MAFS [3], therefore we do not evaluate it explicitly again. Similarly rdFF [6] and lazyFF with MAFS [8] presented in our previous work use the same principle of state-space search distribution. Although the state-space search in MAD-A* is distributed, it does not treat the local private states differently than the public states with exception of communication of the public states to other agents providing the state-based coordination. Therefore planners related to MAD-A* search fit the state-based coordination group. A recent distributed multi-agent state-space search planner inspired by MAD-A* called Greedy Privacy Preserving Planner (GPPP) [18] uses global landmark extraction and is based on an iterative deepening backtrack search both in the relaxed private subproblems and in the non-relaxed public subproblem. The landmarks are used for the computation of the heuristic function as well as the public search.
      </paragraph>
      <paragraph>
       Another distributed search using A* was proposed in [38] as A# planner. A# does not explicitly communicate public states, instead, it transforms the coordination problem into multiple goal variants. When solving the planning problem by a state-space search, the planner implicitly solves the coordination problem by searching for particular goals describing an appropriate solution of the coordination sub-problem.
      </paragraph>
      <paragraph>
       The Agent Decomposition-based Planner (ADP) [25] uses Relaxed Planning Graphs for repeated extraction of subgoals of a multi-agent planning problem and the FF heuristic to steer the search toward the extracted subgoals. The extracted subgoals help to escape plateaus of the FF heuristic towards states at the overlapping boundaries of the agent's problem factors.
      </paragraph>
      <paragraph>
       Multi-agent planners using partial-order plans were firstly studied in a multi-agent temporal planner TFPOP [39], which required an extended model in contrast to MA-STRIPS because of an additional requirement on temporal constraints. A series of partial-order planners based on a model compatible with MA-STRIPS and fitting the state-based coordination class begun with incomplete MAP-POP [40] and two versions of complete planners FMAP [41], [23]. FMAP does not use state-space search similarly to the planners in the MAD-A* family, but as different local and global searches. First, it uses local forward searches run by the agents to successively complete partial ordered plans. Second, the coordination subproblem is driven by a synchronized Best First Search (all agents search in a synchronized manner the same tree for efficiency reasons) over a space of the partial ordered plans. The synchronized Best First Search utilizes a distributed state heuristic based on Domain Transition Graphs (DTGs) approximating relaxed plans [10], the length of which is used as an evaluation of the partial ordered plans. The state heuristic is computed for frontier states of agent's local plans similarly to e.g., in OPTIC a centralized partial-ordered planner [42].
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Evaluation
      </section-title>
      <paragraph>
       In the classical planning literature, the typical method for comparing planners or heuristics is to test them on a set of benchmark domains, each having a number of problem instances. The domains (or problems) vary in many properties, such as size, combinatorial hardness, structural traits and others. The same methodology was adopted in multi-agent planning. A notable difference is in the set of benchmark domains, which is much less “standardized” in multi-agent planning than in classical planning, as there is no track at International Planning Competition (IPC) [43] for multi-agent planning yet. Therefore the set of benchmarks we are using in this article is a union of domains used for comparison of the four planners we are comparing MADLA to in the last section. All these domains and problems originate in single-agent domains and problems from the IPC series. {an inline-figure} Not many properties specific only to the multi-agent planners (as most of the multi-agent planners are variations on distribution of principles used in classical planning) were studied so far, therefore no special multi-agent domains and problems are usually used. The most notable property studied already by [2] is coupling of the agents in the problems which correlates (to some extent) with the ratio of private and public actions in the planning problems. In the detailed analysis section, we will discuss influence of this and other more subtle traits of the problems on the efficiency of the used search and heuristics.
      </paragraph>
      <paragraph>
       As the core comparison metrics, we use coverage, that is the number of solved problems under 20 minutes with 8 GB total memory limit (the memory is shared among all agents, assigned on an as needed basis until exhausted). In the detailed analysis, we use the number of expanded states (and their ratios for the used heuristics), computation time of each particular heuristic and the number of public actions requiring a (private) action supporter for a private precondition fact.
      </paragraph>
      <paragraph>
       The MADLA Planner is a distributed multi-agent system, where the agents communicate over a TCP/IP connection, even if running on a single physical machine. The 20 minute time limit used for coverage timeout represents a make-span of the distributed process (that is, the time between the earliest time any agent starts computing and the latest time any agent finished) and thus we used wall-clock time, as it is not viable to deduce make-span including communication based on CPU time of the individual processes. For the same reasons, we have used wall-clock time also for all other time-related measurements. We are aware, that wall-clock time can be influenced by external sources, such as other processes running on the system. We attempted to mitigate such effects by running the experiments on dedicated machines. Moreover, the non-determinism of the planning process is inherent and impossible to be synchronized under the assumption of unknown ordering in message delivery from two different agents to one recipient. Considering the non-determinism and the unlikely, but possible interference of other system processes, every measurement was repeated 10 times{sup:4} and the results were averaged. Each machine was equipped with 8 hyper-threading{sup:5} i7 cores (i.e., 16 threads) at 2.6 GHz. Each agent was running on two threads. One thread is receiving messages and filling in content data structures in appropriate collections (e.g., states into the open lists {a mathematical formula}OLi,ODi) and the other thread is searching and evaluating the heuristics.
      </paragraph>
      <section label="4.3.1">
       <section-title>
        Selected domains
       </section-title>
       <paragraph>
        The set of benchmarks we are using in this article is a union of domains used in the literature on the state-of-the-art MA-STRIPS based planners [3], [5], [6], [23], [19], [18]. The domains are based on the classical IPC domains converted to multi-agent domains by choosing some objects to be treated as agents. This choice is arbitrary and we adhere to the conventions used in the cited papers. All agents are chosen so that each action is assigned to exactly one agent (this is required by the MA-STRIPS formalism), sometimes necessitating minor changes in the domain descriptions. Privacy is determined by the MA-STRIPS privacy definition—a fact is public if used by actions of multiple agents, an action is public if it uses some public fact. Here, we describe the domains and their complexities in more detail: blocksworldThis domain is the same as the classical blocksworld domain except for having multiple hands as agents, the holding and free facts being private. Each agent can solve the problem on its own, which makes it hard for MAP as for the projected heuristic it seems that the solution by other agents is cheaper (ignoring the private preconditions). All actions in the domain are public but have some private precondition, which means there are some dependencies among the actions not known to all agents. As an example, agents do not know that other agents must use pick-up before put-down, it seems to them that other agents can simply move blocks by the put-down action without picking them up first, that is ignoring the necessary preconditions of the pick-up action such as that the block is free (on top).depotIn depot, the trucks, depots, and distributors are agents. Most of the actions are public, nearly a quarter of them have private preconditions. Only the drive-truck actions are completely private. Truck locations and truck loads (crates) are always private. The position of a crate is specified by a fact on. A crate can be either on a hoist at a depot or on a truck. When it is on a truck, the fact is private.driverlogIn driverlog, the drivers are agents (the problem is modified so that each action has the driver as a parameter). Their locations, walk action and the fact that a driver is driving a truck are private, everything else is public. Most of the problems can be solved by a single agent, but unlike the blocksworld domain, this does not cause agents to significantly underestimate costs of other agents (the ignored costs are basically only those of walking actions, which are not that significant).elevators08In the elevators domain, the elevators are agents. The locations of passengers are public only if shared among multiple elevators (i.e., changing floors). Public actions are only those board and leave actions involving a floor accessible to multiple elevators. Most of the problems can be solved by a subset of agents (typically the slow elevators). All public actions have private preconditions on the state of the lift, its capacity, etc. which makes the agents significantly underestimate the costs of other agents.logistics00The logistics domain contains two types of agents: trucks and planes, transporting packages between cities. The goal specifies only the locations of packages. The transporting task often requires cooperation of several agents (that is so in the classical benchmark domains as well). Locations of packages accessible to only one truck are private to that truck. The loading and the unloading actions at these locations are private as well. The multi-agent logistics domain, similarly to the classical variant, is suitable for the relaxation heuristics.openstacksThe openstacks problems contain two agents: a manager and a manufacturer, which are added atop of the classical IPC domain. The goal of the problems is to produce and ship several orders. The manufacturer has a number of orders. The orders are started and shipped by the manager agents. Each order is for a combination of different products, and the manufacturer can only make one product at a time. The stacks are temporary storage spaces for open orders. The information about made products is private. The rest of the information is public and the two agents have to coordinate finishing the orders. Shipment is public as it is in the goal.roversThe domain models Mars exploration rovers, each represented by one agent. The goal is to collect samples and communicate acquired data. Rovers problems can be well decomposed as each agent has its own private set of targets and reachable locations (even if a location is shared, the rovers do not interfere), but the communication channel is public, shared and imposes coordination constraints. If a sample can be analyzed only by one rover, the location of this sample is the agent's private fact. Rovers is another domain suitable for relaxation heuristics, in contrast to logistics, the number of required interactions is lower, however the private plans are longer.satellitesThe problems of the domain contain agents representing satellites independently taking images in space by various instruments, which have to be powered from a limited on-board power source. The state of the instruments is private to the particular satellites. Pointing directions of each satellite are private unless they appear in the goal. The domain is almost completely decomposed to agents as each satellite is practically independent, sharing only the global goal.woodworking08In the woodworking domain, each tool is an agent. All facts and actions in this domain are public, except for the fact stating that a high-speed saw is empty or loaded. Subsequently, loading and unloading the high-speed saw are the only public actions with private preconditions.zenotravelThe zenotravel problems contain agents representing planes with limited fuel. The goal is to transport passengers between cities and park some of the planes at designated airports. Only the planes are represented by agents. Positions of planes are private and positions of passengers are public. Fly and zoom (fast fly) actions are private. The fuel levels and the positions of passengers in cities reachable by only one plane are also private.
       </paragraph>
      </section>
      <section label="4.3.2">
       <section-title>
        Comparison of the building blocks
       </section-title>
       <paragraph>
        The building blocks of the MADLA Planner are the projected and distributed FF heuristics and the scheme that combines these in a search. A baseline approach (as we presented in Section 3.1.2) is to adapt a classical multi-heuristic (MH) search for multi-agent planning without the non-blocking MADLA principle with a simple alternation mechanism of the open lists respective to the projected and distributed FF heuristics. However, this approach is not viable as the heuristics are not “orthogonal”. The proposed MADLA Search utilizes the requirement for a non-blocking distributed heuristic estimator (particularly implemented in the form of the Privacy-Preserving Set-Additive FF heuristic) by running projected FF in the spare time, therefore in contrast to the MH search utilizing the waiting times for computation of the projected heuristic. Table 1 summarizes the coverage of the projected FF (projFF) and Privacy-Preserving Set-Additive FF (ppsaFF) heuristics in separate multi-agent single-heuristic searches, in the classical multi-heuristic search (Section 3.1.1) and in the MADLA Search (Section 3.1.3) with both heuristics.
       </paragraph>
       <paragraph>
        The results clearly indicate (as expected) that the baseline multi-heuristic approach is not suitable for the pair of projected and distributed FF heuristics. The summed up coverage results are similar for the single-heuristic searches following the results in our previous work [6]. The price for better estimates of the distributed variant of FF than projected FF is its slower computation. Driverlog, woodworking08, and zenotravel are domains where projected FF is performing substantially better than the distributed variant. In such cases, where the FF heuristic is not appropriate or the overhead of its distributed computation overweights the fact the heuristic is more informative, the search using only the projected FF performs better.
       </paragraph>
       <paragraph>
        The openstacks domain is the only case where even the classical multi-heuristic search outperforms both single-heuristic searches. The efficiency of the FF heuristic in the openstacks problems differs even between the projected and distributed heuristics, therefore their combination outperforms each one separately (see Section 4.3.3 for more details) even with the alternating scheme. Elevators08 and woodworking08 are domains where one of the single-heuristic searches outperformed the MADLA Search (with reasonable difference). In the former case, as the implementation of the MADLA scheme is not completely non-blocking (see Section 4.1 for details), the computation of the distributed heuristic can be slowed down by the computation of the local heuristic. In the latter case, as the FF heuristic itself is not guiding the search well, a large number of states needs to be evaluated. The distributed heuristic does not achieve better guidance, only slows down the whole search process and thus the projected heuristic performs slightly better on its own.
       </paragraph>
       <paragraph>
        Fig. 7 shows the coverage of MA-BFS using a single heuristic compared with the MADLA Search using both. The results were obtained by first averaging the runtime over all 10 runs per problem, which results in slightly different final coverage than in Table 1 as each problem is counted as 1 even if not solved in some runs. The plot shows very interesting properties. First, it shows that the projected heuristic (projFF) solves more problems in the same time than the distributed heuristic (ppsaFF), this holds for all time limits. Even more interesting result is that the MADLA Search solves less problems than pure projFF in very short time limits (approx. less than 10 s), because the MADLA Search is slowed down by the distributed heuristic. For longer time limits, the MADLA Search solves more problems than projFF as it is able to harness the benefits of the distributed heuristic.
       </paragraph>
       <paragraph>
        Table 2 lists results of the two heuristics run separately in MA-BFS and their MADLA combination evaluated using additional metrics of expanded states, plan length, bytes communicated among the agents and total planning time, all in form of the IPC score. For a particular problem, the IPC score is computed as a ratio of the optimal value (or the best value of all configurations if the optimum is not known) {a mathematical formula}V⁎ and the particular value V, formally {a mathematical formula}V⁎V. This means that the best configuration for a given problem gets 1, worse configurations get a value &lt;1. The number in the table is the sum of the IPC scores over all problems in all domains for each configuration. The table shows that the distributed heuristic ppsaFF expands the least states (highest score) indicating the heuristic is most informed in comparison to projFF and MADLA which both compensate for the informativeness by the speed of computation of projFF, as the total planning time shows. MADLA slightly improves over projFF in the metric of plan length as the distributed more informed heuristic can shorten some parts of the plans. Notice, that on its own, the distributed heuristic leads to longer plans, as it overestimates the true cost more often. This illustrates one of the benefits of the heuristic combination in the MADLA Search. The results of communicated bytes demonstrate the fact that the projected heuristic does not communicate at all (only the search messages are counted). On the contrary, the distributed heuristic is communication intensive and MADLA is balancing both.
       </paragraph>
       <paragraph>
        The proposed MADLA Search scheme improves the overall coverage over both single-heuristic searches and doubles the coverage of the classical multi-heuristic scheme with the same heuristics. It also provides the best quality plans.
       </paragraph>
      </section>
      <section label="4.3.3">
       <section-title>
        Detailed analysis
       </section-title>
       <paragraph>
        In this section, we analyze the performance of presented building blocks and MADLA Search{sup:6} in detail. A view on comparison of the multi-agent single-heuristic searches with projected FF and distributed Privacy-Preserving Set-Additive FF is presented in Fig. 8. The top graph shows the heuristic values for the initial state of all problems for which the value was computed. It is clear that for most of the domains, as the complexity of the problem grows, also the difference between the distributed and projected heuristic grows (note this does not say anything about the heuristic quality). Bottom is the number of expanded states.
       </paragraph>
       <paragraph>
        Together the two plots show some interesting properties. First, the elevators08 domain is an example of a domain where the distributed heuristic gives much larger heuristic estimates, which also seems to be significantly more informed, as suggested by the number of expanded states. As the heuristic difference grows, also the difference of the number of expanded states grows in favor of the distributed heuristic. Similar behavior, only not as prominent, can be observed in the blocksworld domain. The depot domain paints a completely different picture, where the distributed heuristic also gives significantly larger estimates, but as shown in the plot of the expanded states, the heuristic guidance degrades and for larger problems, the projected heuristic is better informed for the search. The driverlog domain also fits into this category, where the larger distributed heuristic estimates do not necessarily lead the search better. On the other hand, in the woodworking08 domain, we can observe that, although the heuristic estimates are pretty much the same for both heuristics, the number of states expanded by the projected heuristic grows in comparison with the distributed FF, which suggests that even slight differences in the heuristic may have a significant impact on the heuristic quality and its ability to lead the search.
       </paragraph>
       <paragraph>
        Table 3 shows a comparison of various metrics measured using the multi-agent single-heuristic search with projected FF, distributed Privacy-Preserving Set-Additive FF and MADLA Search using both. The first two columns are ratios of coverage and the number of expanded states of the single-heuristic search with projected FF and distributed FF respectively. The next two columns show the percentage of states in MADLA Search expanded using projected FF and the ratio of states expanded using projected FF and distributed FF. The next three columns show the time per state (in milliseconds) the MADLA Search spends on computing the projected and distributed heuristics and the distributed/projected ratio. The last two columns show the average percentage of public and state discerning (SD) actions (see Section 3.4.1) in the domain. A SD action is public and has some private preconditions, which are hidden for agents other than the owner of the SD actions and may cause dependences between SD actions. Ignoring such dependencies in the projected heuristic may significantly influence the quality of estimates.
       </paragraph>
       <paragraph>
        Now, we analyze the results shown in the Table 3 for each of the domains in detail. blocksworldAll actions in the domain are SD actions. This results in better heuristic guidance of the distributed heuristic, best-first search expanding over 6× more states with the projected heuristic than with the distributed one (column exp in Table 3). In the MADLA Search, only 6% of states are expanded using distributed heuristic (column MADLA exp), which is, nevertheless, enough to slightly reduce the coverage of MADLA compared to MA-BFS with only the distributed heuristic.depotThe distributed heuristic takes approx. 8× longer to evaluate on average (column MADLA{a mathematical formula}th right in Table 3), but its better heuristic guidance (11×more expanded states using the projected heuristic) results in almost equal coverage (Table 1). In MADLA, due to the time demanding distributed heuristic computation, about 96% of states are expanded using the projected heuristic (column MADLA exp left). But the small number of states expanded using the distributed heuristic improves the coverage of MADLA in comparison to the single-heuristic search using either of the heuristics.driverlogThe distributed heuristic seems to lead the search slightly better (approx. 2× less expanded states, column exp in Table 3) and takes approx. 1.3× more time per state. Nonetheless, the coverage of MA-BFS using the distributed heuristic is worse than using the projected one, which is likely caused by the higher chance of finding the goal with more expanded states (although less informed), especially in the harder problems (column MADLA{a mathematical formula}th right, column cvg and Table 1). In MADLA, this is improved by approx 80% of states expanded using the projection (column MADLA exp left), which is enough to reach the coverage score of the projected heuristic on its own.elevators08The single-heuristic search with a projected heuristic expands over 50× more states than with the distributed one (column exp in Table 3) and the distributed heuristic takes on average only approx. 1.5× longer to compute (column MADLA{a mathematical formula}th right). This results in a difference in problem coverage of over 10 problems in favor of the distributed heuristic (Table 1). In MADLA, even though only 6% of states are expanded using the distributed heuristic (column MADLA exp left), it is enough to reduce the performance of the single-heuristic search with the distributed heuristic.openstacksThe openstacks domain is the only case, where even the classical multi-heuristic search outperforms both single-heuristic searches. The projected heuristic is in several instances not informed enough, as the two agents need to strongly coordinate the orders. On the other hand, the distributed estimation is in the other instances computationally hard and not leading the search well (see Fig. 8-bottom). There are no state discerning actions. The favorable combination of distributed and projected heuristic (over 30% of states evaluated with the distributed one) improves significantly the coverage of MADLA.woodworking08In the single-heuristic search, the distributed heuristic expands approx. 7× fewer states and takes over 7× longer to evaluate per state (columns exp and MADLA{a mathematical formula}th right in Table 3). The MA-BFS using projected heuristic solves nearly twice as many problems as MA-BFS using the distributed heuristic. This is probably caused by the dominant effect of the number of expanded states, where a higher number of expanded (albeit worse guidance) leads to a solution more often. The MADLA Search, however, is able to take advantage of projected heuristic by expanding only 10% of states using the distributed one. Combined, MADLA performs nearly as well as MA-BFS using only the projected heuristic.zenotravelThe zenotravel domain is one of a few domains where the projected heuristic actually offers better guidance resulting also in better coverage of MA-BFS using only the projected heuristic. Again, MADLA is able to take advantage of that and match the coverage. Based on the measured values and also on the understanding of the domain, the logistics00 domain is similar to the elevators08 domain, although much easier to solve. The rovers and satellites domains are very loosely coupled domains with only a small portion of public actions and are also easy to solve.
       </paragraph>
       <paragraph>
        In summary, the distributed heuristic is useful in domains with a high number of public actions with private preconditions, which is a sign of necessary interaction among the agents, not visible to the projected heuristic, or with some crucial information being private (as in woodworking). On the contrary, the projected heuristic performs better on domains where the agents are interchangeable (driverlog, zenotravel). In most cases, the MADLA Search is able to let the better heuristic dominate the search and thus on most domains, MADLA closely matches the better one of the single heuristic approaches. Notice that on some domains, MADLA even improves the coverage over each heuristic used separately (namely depot and openstacks). It is also important to note that in woodworking, both projected and distributed FF heuristics ignore a significant number of dead-ends (as described in Section 3.2.3), thus slowing the search and solving fewer problems.
       </paragraph>
      </section>
      <section label="4.3.4">
       <section-title>
        Comparison with a centralized planner
       </section-title>
       <paragraph>
        To report on the effects of the multi-agent partitioning (sometimes referred to as factorization for its resemblance with factored planning), we run the benchmark problems on a centralized Greedy Best-First Search with the FF heuristic. In Table 4 the results are compared with the MADLA Search with the projected and distributed FF heuristics. The centralized planner is a configuration of the MADLA Planner using the same codebase but no agent factorization in order to have a fair comparison of the techniques. The original FF planner or Fast Downward (FD) would most probably perform better as they are more optimized and use additional techniques such as preferred operators. In order to account for the MADLA Planner running on 8 cores, the centralized planner was allowed an 8× longer time limit.
       </paragraph>
       <paragraph>
        The results show substantial differences in the depot, driverlog, elevator08, and openstacks domains. The multi-agent search doubles the efficiency in depot and 1.5× in elevators08, as the agent subproblems are loosely coupled (the same holds for logistics00, rovers, satellites, and zenotravel). Loose coupling results in beneficial decomposition, making the agents' problems significantly smaller but with not much overhead in coordination. The strong efficiency improvement in openstacks is caused by better informed combination of heuristics and beneficial partitioning of the problem. In driverlog, the most plausible explanation of the coverage drop is the relaxation principle of the FF heuristic provides better estimates without the partitioning. In general, the results follow the results of similar experiments performed with MAD-A* [3].
       </paragraph>
      </section>
      <section label="4.3.5">
       <section-title>
        Comparison with the state of the art
       </section-title>
       <paragraph>
        The MADLA Planner, as all its predecessors MAFS with rdFF and lazyFF and MA-BFS, fits in the MAD-A* family—a distributed forward-chaining heuristic search with no distinction of privately or publicly expanded states during the local search (with exception of informing the other agents), therefore MADLA follows the state-based coordination paradigm. In contrast to MAD-A*, MADLA adopts a distributively computed heuristic, similarly to FMAP and GPPP do. The use of projected actions is another similarity of MAD-A* and MADLA. Both planners do not use projections of other agents during the search, however they use such actions within the heuristic evaluation. In the case of MAD-A*, the LM-cut and Merge&amp;Shrink use projected public actions of other agents, which holds for both the projected and distributed heuristics of MADLA too, however in the latter case all projected public actions are additionally evaluated by requesting the owning agent for additional costs caused by private supporting actions. The projected heuristic estimation is nothing more than classical computation of the heuristic on a projected planning problem of one agent. The distributed computation of the heuristic is principally the same as computation of the distributed DTG heuristic in FMAP and landmark heuristic in GPPP planners.
       </paragraph>
       <paragraph>
        In contrast to ADP and GPPP, MADLA does not extract sub-goals, thus does not use them neither as a heuristic (ADP) nor in search for the public subproblem (GPPP). Coordination of the planning process in MADLA is driven by communicating public states, therefore MADLA does not need to adjust (MAPR, PMR), validate (PSMM) or amend (FMAP) partial plans, or partially ordered plans. As in MAD-A*, MADLA represents prefixes of the plans for all reached states in the form of references to parent search nodes. MADLA's distributed heuristic computation can be seen as a process using plan-based coordination on the relaxed planning problem. It uses projected actions of other agents, merges partial results of other agents which are requested if the agent needs them and it uses distributed extraction of the plan in the Privacy-preserving Set-Additive FF (ppsaFF) heuristic.
       </paragraph>
       <paragraph>
        Distributed computation of the ppsaFF heuristic results in an approximation of a relaxed plan to the goal from the current state. The principle MADLA is using is similar to PSMM and Planning First planners, but in a backtracking fashion: the PSMM process can be viewed as one central initiator agent, which generates public plans (in a compact form of the Finite State Machines), asks the other agents if they can fill in the required supporting private actions (gaps) caused by the projection of their actions. In the non-relaxed case, this process requires backtracking, whereas in the relaxed case, the process is monotonous and can be easily done in the backward manner from the goals. In the Planning First case the DisCSP represents the coordination problem which is simultaneously solved by the agents, however the used unary Internal Planning Constraint [2] represents planning of the local private plans of the agents. Therefore, similarly to PSMM and the distributed FF, the planner fills the gaps locally by the respective agents. As a matter of fact, MADLA's distributed process to evaluate the FF heuristic could be used as another plan-based coordination planning approach. In the non-monotonic case, however, it would have to search, i.e., adopt distributed backtracking. A naive implementation would need synchronized stacks representing the distributed state of the recursion and would require substantial communication among the agents.
       </paragraph>
       <paragraph>
        Using the state-based heuristic is troublesome in the coordination-centric approaches as such planners usually do not have a complete state the heuristic could evaluate. Therefore, straightforward adoption of the MADLA distributed heuristic is not possible, however the heuristic can be used to improve efficiency of the public planning sub-problem (e.g., PSMM proposes to use distributed relaxed heuristic in initial state) or can be used if the partial plan can generate a frontier state as the local search in FMAP. Moreover, hybrid approaches such as FMAP and GPPP, which work with full states as subgoals, could use MADLA heuristics to evaluate the subgoal states if needed, which is a similar principle as proposed by GPPP, but using the landmark heuristic.
       </paragraph>
       <paragraph>
        In Table 5, we show a comparison of problems solved by MADLA and four complete and distributed multi-agent planners. The results show that MADLA loses considerably in woodworking08 and openstacks against all planners supporting action costs. These domains contain substantial number of dead-ends of which the FF heuristic (especially in the projected form) is oblivious.
       </paragraph>
       <paragraph>
        We argue that MADLA is not fairly comparable to (a) planners which do not consider multi-agent privacy (ADP), (b) planners incompatible with MA-STRIPS (μ-SATPLAN, BRP, Distoplan, A#, TFPOP), or (c) optimal planners (MAD-A*). Additionally, we present comparison only with the most efficient planners using a particular paradigm. We do not present detailed comparisons with ADP planner, however on the benchmark set present, ADP outperforms MADLA by more than 28% solved problems. By definition ADP cannot preserve privacy in the same sense as MADLA in general, as it does not obey the definition of the agents by which MA-STRIPS defines the privacy. Moreover MADLA has to use one partitioning of the planning problem defined in the input PDDL and ADDL, but ADP targets classical planning benchmarks and is free to partition the problem as it sees fit.
       </paragraph>
       <paragraph>
        Although the result table does not contain the PMR planner, MADLA outperforms it on the presented benchmark set as well, just because PMR is an incomplete planner as stated in [19], [34]. PMR solves only problems where each goal fact is solvable by a single agent. Thus it does not solve problems of depot, logistics00, openstacks, and woodworking08 domains. Even if PMR solved all problems of all other domains, MADLA would outperform it by 38%.
       </paragraph>
       <paragraph>
        Against MAFS running rdFF [6], our recent multi-agent heuristic search using different distribution schemes and implementations of FF, MADLA shows more than 2× improvement over all domains with an exception of driverlog and woodworking08, where the improvement is about 20%. Similarly, MADLA outperforms GPPP nearly 2× over many domains and PSMM by 36%. Finally, MADLA solves 16 more problems of the benchmark set in contrast to the top performing multi-agent planner FMAP, which correspond to nearly 11% improvement. In comparison to FMAP, MADLA is considerably better on four domains (blocksworld, depot, logistics, satellites) and significantly worse only on two (elevators08, woodworking08).
       </paragraph>
       <paragraph>
        The MADLA Planner also took part in the recent Competition of Distributed and Multi-Agent Planners (CoDMAP). See the on-line results{sup:7} or [44] for more details. Note, that although the MADLA Planner itself was not significantly modified for the competition, one of the best performing planners, MAPlan [45] is built on the same principles. In particular, it uses distributed heuristic search with the privacy-preserving set-additive FF heuristic (Section 3.2.3), but without the MADLA Search (Section 3.1.3).
       </paragraph>
      </section>
     </section>
    </section>
    <section label="5">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      As for classical planning, heuristic state-space search is a viable technique for multi-agent planning. However, in contrast to the classical heuristic search, the multi-agent setup raises its own challenges. The dilemma of highly informed but slow versus less informed but fast heuristic estimators is manifested in the dichotomy of projected heuristic restricted to the agent's local view of the problem versus distributed heuristic estimating the global heuristic value at the cost of significant communicational or computational overhead. The MADLA Planner combines both fast local projection of the FF heuristic with a global distributed FF heuristic in an attempt to combine their benefits and mitigate their negative effects.
     </paragraph>
     <paragraph>
      There are two contributions of this paper. The main contribution is a novel distributed multi-heuristic multi-agent search scheme and its practical implementation. A minor contribution is an improved approach to computing privacy-preserving distributed FF heuristic, while reducing the negative effect of overcounting.
     </paragraph>
     <paragraph>
      The novel technique used in the MADLA Planner to combine the heuristic estimators is based on the classical multi-heuristic search, but it does not evaluate all states by both heuristics. Instead, the local heuristic is used only while the distributed heuristic is busy (that is, waiting for replies from other agents). The projected heuristic is used to fully utilize the computational resources of the agent, even for less-informed, but faster search of the state space of the individual agent. When the estimation of the more-informed heuristic is finished, the evaluated state is used in both searches. This principle was theoretically analyzed and practically evaluated in the form of a MA-STRIPS based planner, which on many domains outperforms all current multi-agent planners in the coverage metric over a common benchmark set.
     </paragraph>
     <paragraph>
      Some research directions are left for future work. First, whether the principle of MADLA Search can be used for optimal multi-agent planning with similarly promising results. Second, as the results show, the planner does not perform well on domains with dead-ends, which relaxation heuristics are oblivious to. We can ask what efficiency boost could be achieved by the combination not only of one heuristic as projected and distributed estimators, but also with other heuristics possibly orthogonal to the first heuristic pair; landmarks are an obvious choice. Moreover, the open list selection scheme is by no means perfect and could be possibly improved, e.g., by occasionally evaluating the best state from {a mathematical formula}OLi by {a mathematical formula}hLi even if {a mathematical formula}ODi is not empty. Also, the use of preferred operators might significantly improve the overall performance.
     </paragraph>
     <paragraph>
      Generally, utilizing the principle of combining fast and less accurate and slow but more informed heuristics in an asynchronous manner may be also an interesting research direction in classical planning (especially on multicore machines) and in search in general.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>