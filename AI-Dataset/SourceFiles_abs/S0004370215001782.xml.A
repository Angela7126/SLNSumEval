<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Topic-based term translation models for statistical machine translation.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      A term is a linguistic expression that is used as the designation of a defined concept in a language (ISO 1087). The following sentences provide several term examples (in Italic).
     </paragraph>
     <list>
      <list-item>
       Cambodia and Vietnam jointly hold commodity exhibition.
      </list-item>
      <list-item>
       Indonesia reiterated its opposition to foreign military presence.
      </list-item>
      <list-item>
       Native Mandarin speakers teach you Chinese as foreign language.
      </list-item>
     </list>
     <paragraph>
      As shown in these examples, terms are compound words that are composed of nouns, adjectives and prepositions in special linguistic patterns.
     </paragraph>
     <paragraph>
      As terms convey concepts of a text, appropriately translating terms is crucial when the text is translated from its original language to another language. The translations of terms are often affected by the domain in which terms are used and the context that surrounds terms [1]. In this article, we study domain-specific and context-sensitive term translation in the context of statistical machine translation (SMT).
     </paragraph>
     <paragraph>
      In order to achieve this goal, we focus on three issues of term translation: 1) ambiguity, 2) consistency and 3) unithood. First, term translation ambiguity is related to multiple translations of the same term in different domains. A source term may have different translations when it occurs in different domains. Second, term translation consistency is about consistent translations of terms that occur in the same document. Usually, it is undesirable to translate the same term in different ways as it occurs in different parts of a document. Finally, term unithood{sup:1} concerns whether a multi-word term is still a unit after translation. Normally, a multi-word source term is translated as a whole unit into a contiguous target string.
     </paragraph>
     <paragraph>
      Table 1 demonstrates the three issues of term translation with two Chinese-to-English translation examples. The first translation example (Eg. 1) visualizes two issues of term translation: ambiguity and unithood. In regard to the term translation ambiguity, the underlined source term “chang2gui1 sai4” can be translated into either “conventional tournament” or “regular season”. The latter translation “regular season” is more widely used in the specific domain of NBA basketball games. Therefore given the domain of Eg. 1, “regular season” is a more appropriate translation for “chang2gui1 sai4” than “conventional tournament” that is chosen by the machine-generated baseline translation. As for the term unithood, the underwaved source term “ji4hou4 sai4” should be translated as a unit into target string “after season games”. Unfortunately, the baseline translation violates the unithood constraint of this source term and translates it into an inconsecutive phrase that is interrupted by word “shocks”.
     </paragraph>
     <paragraph>
      The second translation example (Eg. 2) is related to term translation consistency. In this example, we display two sentences in the same text. The underlined source term “wai4guo2 jun1dui4 jin4zhu4” is not translated consistently in the baseline translations. It is translated as “foreign military presence” in the first sentence while “foreign troops stationed” in the second sentence (an undesirable translation).
     </paragraph>
     <paragraph>
      In order to address these three issues of term translation, we propose a topic-based framework to model term translation for SMT. We capitalize on document-level topic information to disambiguate term translations in different documents and to maintain consistent translations for terms that occur in the same document. In particular, we propose the following three models.
     </paragraph>
     <list>
      <list-item label="•">
       Term Translation Disambiguation Model: In this model, we condition the translations of source terms in different documents on the topic distributions of corresponding documents. In doing so, we enable the decoder to favor translation hypotheses with topic-specific term translations.
      </list-item>
      <list-item label="•">
       Term Translation Consistency Model: We introduce a topic-dependent translation consistency metric for each source term to measure how consistently it is translated across documents in training data. With this metric, we encourage the same terms with a high strength of translation consistency that occur in different parts of a document to be translated in a consistent fashion.
      </list-item>
      <list-item label="•">
       Term Unithood Model: We explore rich contextual information in the term unithood model to calculate how likely a source term should remain contiguous after translation. We use this unithood model to reward translation hypotheses where multi-word terms are translated as a whole unit.
      </list-item>
     </list>
     <paragraph>
      A bilingual term bank is required to build these three models. We construct this term bank from our bilingual training data via automatic term extraction methods. We use a hierarchical phrase-based SMT system [3] to validate the effectiveness of the three term translation models. Large-scale experiment results show that they are all able to achieve substantial improvements of up to 0.88 BLEU points over the baseline. When simultaneously integrating the three models into SMT, we can gain a further improvement. The combination of the three models outperforms the baseline by up to 1.27 BLEU points.
     </paragraph>
     <paragraph>
      The three term translation models have been first presented in our previous paper [4]. In this article, we make significant extensions to our previous work. First, for the purpose of completeness, we provide a background introduction of SMT and topic modeling, more details about bilingual term extraction, especially how we pair monolingual terms into bilingual terms based on word alignments, as well as more details about how we calculate the proposed term translation consistency model. Second, we conduct new experiments to study the impact of the size of extracted bilingual term bank on the three models and the impact of topic information on the term translation consistency and unithood model. We also carry out experiments to compare different bilingual term extraction methods and different approaches to consistent term translation. Finally, we provide in-depth analyses on extracted bilingual terms and translation outputs to demonstrate why and how the proposed term translation models improve translation quality.
     </paragraph>
     <paragraph>
      The remainder of this article is organized as follows. Section 2 begins with a brief introduction on statistical machine translation and topic modeling as background knowledge. Section 3 describes the process of bilingual term extraction. Section 4 elaborates the proposed three models for term translation with details of training. Section 5 introduces how we integrate the three models into SMT. Section 6 conducts experiments to validate the effectiveness of the proposed models. Section 7 presents in-depth analyses of extracted bilingual terms and translation results. Section 8 gives a brief overview of related work and highlights the differences of our work from them. Finally, we conclude and provide directions for future work in Section 9.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background
     </section-title>
     <paragraph>
      Before we present our term translation models, we provide a brief introduction of statistical machine translation and topic modeling in this section. This will help build relevant background knowledge.
     </paragraph>
     <section label="2.1">
      <section-title>
       Statistical machine translation
      </section-title>
      <paragraph>
       SMT is one of the most popular machine translation paradigms, which relies on statistical models to capture translation equivalents between the source and target language. Most SMT systems adopt a log-linear model [5] to find the best translation {a mathematical formula}eˆ among all possible translations for a given source sentence f, which can be formulated as follows:{a mathematical formula} where {a mathematical formula}hm(f,e) are feature functions defined on the source sentence f and its corresponding translation e, {a mathematical formula}λm are weights of feature functions.
      </paragraph>
      <paragraph>
       In the SMT literature, feature functions {a mathematical formula}hm(f,e) are also referred to as sub-models of the log-linear model, or just models for simplicity. Normally, the log-linear model of SMT includes a language model that measures the fluency of a generated target translation, a translation model that estimates the probabilities of translation equivalents, a reordering model that captures the word order differences between the source and target language, as well as other models that incorporate knowledge useful for machine translation.
      </paragraph>
      <paragraph>
       These sub-models are trained separately and independently. Trained sub-models are then combined into the log-linear model of SMT with associated weights. The weights {a mathematical formula}λs are tuned via algorithms such as the Minimum Error Rate Training (MERT) [5] or Margin Infused Relaxed Algorithm (MIRA) [6]. We choose the best weights by optimizing the log-linear model towards some translation quality metrics such as bleu[7], instead of maximizing the mutual information of the log-linear model. This is because feature weights learned by maximizing the mutual information are not necessarily optimal with respect to translation quality [5].
      </paragraph>
      <paragraph>
       A full introduction of SMT is far beyond the scope of this article. We refer readers to the textbook “Statistical Machine Translation” [8] or book “Linguistically Motivated Statistical Machine Translation” [9] for more details on SMT.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Topic modeling
      </section-title>
      <paragraph>
       Generally, topic models are statistical models that automatically learn hidden topics in a collection of documents (i.e., corpus) in an unsupervised fashion. They do not require any annotations of documents. They analyze words of a corpus and infer latent distributional patterns in a text. Most topic models are extensions of Latent Dirichlet allocation (LDA) model [10], which is a generalization of the probabilistic latent semantic analysis (pLSA) [11], an early topic model.
      </paragraph>
      <paragraph>
       In LDA, a document D is assumed to be a mixture of topics, or a distribution {a mathematical formula}p(z|D) over a set of topics z while a topic z is defined as a distribution {a mathematical formula}p(w|z) over a fixed vocabulary w. The former distribution is referred to as per-document topic distribution and the latter per-topic word distribution. Both distributions are assumed to have a Dirichlet prior. Given a corpus, the generative process of LDA is as follows: 1) draw a per-document topic distribution {a mathematical formula}θi for the ith document from a Dirichlet distribution {a mathematical formula}Dir(α); 2) for each word {a mathematical formula}wi,j in the ith document {a mathematical formula}Di, 2.1) draw a topic assignment {a mathematical formula}zi,j∼Multinomial(θi); and 2.2) draw a word {a mathematical formula}wi,j∼φzi,j where {a mathematical formula}φzi,j is a per-topic word distribution drawn from a Dirichlet distribution {a mathematical formula}Dir(β).
      </paragraph>
      <paragraph>
       Latent variables and distributions of the LDA model, e.g., the set of topics, topic assignments of words, per-document topic distribution and per-topic word distribution, can be learned via Bayesian inference algorithms, such as variational inference [10] and collapsed Gibbs sampling [12].
      </paragraph>
      <paragraph>
       Although topic models are first described and applied in the context of natural language processing, they are also adapted to find hidden patterns in other data, e.g., images. And various extensions to the LDA model have been proposed, such as polylingual topic models [13]. We refer readers to a general introduction of topic models [14] by David Blei for more details about topic modeling.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Bilingual term extraction
     </section-title>
     <paragraph>
      Bilingual term extraction is to extract terms from two languages with the purpose of creating or expanding a bilingual term bank, which in turn can be used to improve other tasks such as information retrieval and machine translation. In this article, we want to automatically build a bilingual term bank so that we can model term translation to improve translation quality of SMT. Particularly, our interest is to extract multi-word terms.
     </paragraph>
     <paragraph>
      There are two strategies to conduct bilingual term extraction from parallel corpora. One of them is to extract term candidates separately for each language according to monolingual term measures, such as the C-value/NC-value [15], [16], or other common co-occurrence measures such as the Likelihood Ratio, Dice coefficient and Pointwise Mutual Information [17], [18]. The extracted monolingual terms are then paired together [19], [20], [21]. The other strategy is to align words and word sequences that are translation equivalents in parallel corpora and then classify them into terms and non-terms [22], [23].
     </paragraph>
     <paragraph>
      We adopt the first strategy to build our bilingual terminology.{sup:2} We first extract monolingual term candidates from the source and target language, and then pair them according to word alignments. The following two subsections will introduce how we extract monolingual term candidates and pair them into bilingual terms respectively.
     </paragraph>
     <section label="3.1">
      <section-title>
       Extracting monolingual terms
      </section-title>
      <paragraph>
       We extract monolingual terms using two methods: one with the C-value/NC-value measure and the other with the Log-Likelihood Ratio (LLR) measure. For the C-value/NC-value measure based term extraction, we implement it in the same way as described by Frantzi et al. [15]. This method combines linguistic and statistical properties of terms and correspondingly runs in two steps: linguistic and statistical step. In the linguistic step, it recognizes all linguistic units according to the three linguistic patterns (mainly noun phrases) listed as follows:{a mathematical formula} which are written as regular expressions. “NounPrep” is a combination of a noun and preposition, e.g., “language of” in the term “language of instruction”. The three linguistic patterns are used to capture linguistic structures of terms.
      </paragraph>
      <paragraph>
       In the statistical step, the C-value/NC-value method measures statistical properties of multi-word term candidates that pass the linguistic filters{sup:3} in the first step. Statistical properties of terms include their occurrence frequencies in a corpus, frequencies that nested terms (terms appearing in other longer terms) overlap with longer term candidates. The C-value is used to extract multi-word nested terms while the NC-value is used to incorporate context information into the C-value for general term extraction.
      </paragraph>
      <paragraph>
       Specifically, the C-value measures the degree to which a term candidate is related to domain-specific context based on the frequency of the candidate and the frequency that the candidate is a substring of other longer candidate terms (i.e., a nested term). Given a candidate term a, the C-value of a can be formulated as follows:{a mathematical formula} where {a mathematical formula}fa is the frequency of candidate a with {a mathematical formula}|a| words, {a mathematical formula}Ta is a set of candidate terms that contain a as a sub-part, {a mathematical formula}|Ta| is the number of items in {a mathematical formula}Ta.
      </paragraph>
      <paragraph>
       Once the C-value is calculated, it is used to further compute the NC-value that combines the C-value and a score based on context words, which is called N-value. The N-value is formulated as follows:{a mathematical formula} where b is a term context word, {a mathematical formula}Ca is the set of distinct term context words, {a mathematical formula}f(b) is the frequency of b as a context word for a, {a mathematical formula}w(b) is the weight of b, defined as the number of terms appearing with b (t(b)) over the total number of terms T. A term context word is an adjective, noun or verb that either precedes or follows a candidate term in a 5-word window. The NC-value is therefore computed as follows:{a mathematical formula}
      </paragraph>
      <paragraph>
       For the LLR metric based term extraction, we implement it following Daille [17], who estimates the propensity of two words to appear together as a multi-word expression. The LLR of word a and b is defined as follows:{a mathematical formula} where {a mathematical formula}fa, {a mathematical formula}fb, {a mathematical formula}fab are the frequency of the occurrence of a, b and the co-occurrence of a and b respectively, {a mathematical formula}L(x,y,z) is a function defined as {a mathematical formula}zx(1−z)y−x. We also adopt LLR-based hierarchical reducing algorithm proposed by Ren et al. [21] so that we can extract terms with arbitrary lengths.
      </paragraph>
      <paragraph>
       The C-value/NC-value extraction method obtain terms strictly satisfying the linguistic rules defined in the equation (2). In contrast, the LLR method extracts terms without using any linguistic constraints. Table 2 shows the number of terms extracted from the target side of our training data by these two methods. As shown in the table, LLR extracted more terms than C-value/NC-value does as it does not need to satisfy linguistic constraints. Terms extracted by both LLR and C-value/NC-value account for around 25% of terms extracted by either of the two methods.
      </paragraph>
      <paragraph>
       We also give examples of terms that are extracted only by C-value/NC-value, only by LLR or by both in Table 3. From the table, we can observe that the C-value/NC-value method is not able to find some term patterns that do not satisfy the linguistic rules in the equation (2). For example, both “privately financed infrastructure projects” and “transnational organized crime” cannot be extracted by C-value/NC-value. However, these terms can be detected by LLR as words in them often appear together.
      </paragraph>
      <paragraph>
       As shown in Table 2, Table 3, C-value/NC-value and LLR are complementary to each other. We therefore combine the two sets of term candidates that are separately extracted by these two methods to construct our monolingual term bank.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Pairing monolingual terms
      </section-title>
      <paragraph>
       After we extract two sets of monolingual terms from the source and target side with the methods described above, we pair monolingual terms into bilingual terms based on word alignments. In particular, for each extracted source term {a mathematical formula}tf, we find all target phrases {a mathematical formula}Atf that are aligned to {a mathematical formula}tf. And for each of these target phrases {a mathematical formula}te∈Atf,
      </paragraph>
      <list>
       <list-item label="•">
        If {a mathematical formula}te is also a term on the target side, we store the source and target term as a bilingual term pair {a mathematical formula}〈tf,te〉.
       </list-item>
       <list-item label="•">
        If not, we use the following heuristic rules.
       </list-item>
      </list>
      <paragraph>
       These heuristic rules are used to reduce the impact of word alignment errors on bilingual term extraction. Some examples are provided in Table 4 to show how we use these rules. We empirically set the threshold {a mathematical formula}τ=0.2 according to our preliminary experiment results.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Models
     </section-title>
     <paragraph>
      In this section, we elaborate the three models proposed for term translation. They are the (A) term translation disambiguation model, (B) term translation consistency model and (C) term unithood model respectively.
     </paragraph>
     <section label="4.1">
      <section-title>
       Model A: term translation disambiguation
      </section-title>
      <paragraph>
       The most straightforward way to disambiguate term translations in different domains is to calculate the conditional translation probability of a term given domain information. We use the topic distribution of a document obtained by a topic model to represent the domain information of the document.
      </paragraph>
      <paragraph>
       There are a great variety of different topic models that can infer topic distributions of documents. As we mentioned in Section 2.2, most of them use Latent Dirichlet Allocation (LDA) [10] as their foundation. Without loss of generality, we exploit the LDA topic model for inferring topic distributions of documents.
      </paragraph>
      <paragraph>
       For each term pair in the bilingual term bank created from training data as described in the last section, we calculate the source-to-target term translation probabilities conditioned on the topic distribution of the source document where the source term occurs. We maintain a K-dimension (K is the number of topics) vector for each term pair. The k-th component {a mathematical formula}p(te|tf,z=k) measures the conditional translation probability from source term {a mathematical formula}tf to target term {a mathematical formula}te given the topic k.
      </paragraph>
      <paragraph>
       The probability {a mathematical formula}p(te|tf,z=k) is computed via maximum likelihood estimation using counts from training data. For each bilingual term pair {a mathematical formula}〈tf,te〉, the source part of which occurs in a document D with a topic distribution {a mathematical formula}p(z|D) estimated via the LDA model, we collect an instance {a mathematical formula}(tf,te,p(z|D),c), where c is the fraction count of the instance as described by Chiang [25]. In this way, we can obtain a set of instances {a mathematical formula}I={(tf,te,p(z|D),c)} with different per-document topic distributions for each bilingual term pair. Using these instances, we calculate the probability {a mathematical formula}p(te|tf,z=k) as follows:{a mathematical formula}
      </paragraph>
      <paragraph>
       Table 5 displays examples of bilingual terms with their topic-conditioned translation probabilities. For each bilingual term, we only show the topic t where the bilingual term has the highest topic-conditioned translation probability {a mathematical formula}p(te|tf,z=t), ignoring translation probabilities under other topics. We can clearly see that the same source term “fang2yu4 xi4tong3” is translated differently under different topics. For example, in the health domain, the term is tightly related to immune mechanisms. But in the military/politics domain, “defense system” is a widely-used translation for this term. This is also true for translations of the source term “zhan4lue4 si1xiang3” that occurs in different domains with slight meaning shifts.
      </paragraph>
      <paragraph>
       We associate each extracted bilingual term pair in the bilingual term bank with its corresponding topic-conditioned translation probabilities estimated in the equation (7). When translating sentences of document {a mathematical formula}D′, we first get the topic distribution of {a mathematical formula}D′ via LDA. Given a sentence which contains T terms {a mathematical formula}{tfi}1T in {a mathematical formula}D′, our term translation disambiguation model TermDis can be denoted as{a mathematical formula} where the conditional source-to-target term translation probability {a mathematical formula}Pd(te|tfi,D′) given the document {a mathematical formula}D′ is formulated as follows:{a mathematical formula} Whenever a source term {a mathematical formula}tfi is translated into {a mathematical formula}te, we check whether the pair of {a mathematical formula}tfi and its translation {a mathematical formula}te can be found in our bilingual term bank. If can be found, we calculate the conditional translation probability from {a mathematical formula}tfi to {a mathematical formula}te given the document {a mathematical formula}D′ according to the equation (9).
      </paragraph>
      <paragraph>
       Through the term translation disambiguation model, we can enable the decoder to favor translation hypotheses that contain target term translations appropriate for the domain represented by the topic distribution of the corresponding document.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Model B: term translation consistency
      </section-title>
      <paragraph>
       The term translation disambiguation model enables the decoder to select appropriate translations for terms that are in accord with their domains. Yet another translation issue related to the domain-specific term translation is to what extent a term should be translated consistently given the domain where it occurs. A straightforward way to address this issue is to simply count how many times a term translation is reused in recently translated sentences. Those term translations frequently reused will be encouraged. However, such term translations selected by the method are not necessarily correct or appropriate translations under current topics. We therefore propose a topic-based term consistency model. Instead of simply counting the number of times of a term translation being used in previously translated sentences, we enable the decoder to detect terms that are highly consistently translated under given topics in training data with the proposed model, and encourage the decoder to translate these terms.
      </paragraph>
      <paragraph>
       The essential component of our term translation consistency model is the translation consistency strength of a source term estimated under the per-document topic distribution. We describe how to calculate it before introducing the whole model.
      </paragraph>
      <paragraph>
       For the bilingual term bank created from training data, we first group each source term and all its corresponding target terms into a 2-tuple {a mathematical formula}G〈tf,Set(te)〉, where {a mathematical formula}tf is the source term and {a mathematical formula}Set(te) is the set of {a mathematical formula}tf's corresponding target terms. We maintain a K-dimension vector for each 2-tuple {a mathematical formula}G〈tf,Set(te)〉. The k-th component measures the translation consistency strength {a mathematical formula}cons(tf,k) of the source term {a mathematical formula}tf given the topic k.
      </paragraph>
      <paragraph>
       We calculate {a mathematical formula}cons(tf,k) for each {a mathematical formula}G〈tf,Set(te)〉 with counts from training data as follows:{a mathematical formula} where M is the number of documents in which the source term {a mathematical formula}tf occurs, {a mathematical formula}Nm is the number of unique corresponding term translations of {a mathematical formula}tf in the mth document {a mathematical formula}Dm, {a mathematical formula}qmn is the frequency of the nth translation of {a mathematical formula}tf(∈Set(te)) in document {a mathematical formula}Dm, {a mathematical formula}p(k|Dm) is the conditional probability of document {a mathematical formula}Dm over topic k, and {a mathematical formula}Qk is the normalization factor. All translations of {a mathematical formula}tf are from {a mathematical formula}Set(te).
      </paragraph>
      <paragraph>
       The term consistency strength {a mathematical formula}cons(tf,k) measures how consistently a term is translated in a domain. It is computed according to two factors: 1) the number of translation variations of a term {a mathematical formula}tf in a domain and 2) topic-related frequencies of these variations {a mathematical formula}qmn×p(k|Dm). If a term is always translated into only one unique translation in a domain, the consistency strength will be the highest. If there are multiple translation variations for a term in a domain and most translations are concentrated on one translation variation, the consistency strength will remain high. If there are multiple variations and the majority of translations do not concentrate, the consistency strength will be low.
      </paragraph>
      <paragraph>
       Table 6 shows an example of a source term “fang2yu4 xi4tong3” with its various translations in different documents. We calculate the translation consistency values of the term in each document under the topic 30 (shown in the last column of Table 6). Summing up all these values in all documents, we can obtain the term translation consistency strength of “fang2yu4 xi4tong3” under the topic 30 (as shown in the last row of Table 6).
      </paragraph>
      <paragraph>
       We adapt Itagaki et al. [26]'s translation consistency index for terms to our topic-based translation consistency measure in the equation (10). The significant difference between our term translation consistency measure and their consistency index is that we take the topic distributions of documents into account when we calculate the term translation consistency strength. Table 7 shows the translation consistency strength values of term “fang2yu4 xi4tong3” under different topics. We display the first 5 topics with the highest values of {a mathematical formula}cons(tf,k) and the last 5 topics with lowest values of {a mathematical formula}cons(tf,k) in the table. From Table 7, we can observe that
      </paragraph>
      <list>
       <list-item label="•">
        In the topics that are most closely related to the term “fang2yu4 xi4tong3” (e.g., military and war), the term is translated flexibly. This may be because the term has several different target translations, all of which are acceptable in these domains.
       </list-item>
       <list-item label="•">
        In a general domain like news, “fang2yu4 xi4tong3” is consistently translated into a target term that is widely accepted by most people. Yet another reason for the high consistency strength value in the news domain is because newswire services normally use translation memories or handbooks to encourage consistency.{sup:5}
       </list-item>
       <list-item label="•">
        In the topics that are not quite related to the term “fang2yu4 xi4tong3”, such as religion, the term is not consistently translated either.
       </list-item>
      </list>
      <paragraph>
       These suggest that the consistency of term translation is topic-sensitive. The same terms may be translated in a different consistency pattern in different topics. Therefore we calculate the translation consistency strength of a source term {a mathematical formula}tf based on topic distributions.
      </paragraph>
      <paragraph>
       We reorganize our bilingual term bank into a list of 2-tuples {a mathematical formula}G〈tf,Set(te)〉s, each of which is associated with a K-dimension vector storing the topic-conditioned translation consistency strength values calculated in the equation (10). When translating sentences of document D, we first get the topic distribution of D. Given a sentence which contains T terms {a mathematical formula}{tfi}1T in D, our term translation consistency model TermCons can be denoted as follows:{a mathematical formula} where the strength of term translation consistency for {a mathematical formula}tfi given the document D is formulated as follows:{a mathematical formula}
      </paragraph>
      <paragraph>
       We translate a document in a sentence-by-sentence manner. During decoding of a sentence, whenever a hypothesis translates a source term {a mathematical formula}tfi into {a mathematical formula}te, we check whether the translation {a mathematical formula}te can be found in {a mathematical formula}Set(te) of {a mathematical formula}tfi from the reorganized bilingual term bank. If it can be found, we calculate the strength of term translation consistency for {a mathematical formula}tfi given the document D according to the equation (12). If the topic-dependent consistency strength is very high, this indicates that: 1) the number of translation variations of the term is small or 2) the majority of translations concentrate under a topic distribution similar to that of D in training data. Our model will encourage the decoder to translate this term consistently using translations from the bilingual term bank. If the strength is low, this suggests that the term has many scattered translation variations under the current topic distribution. Our model will either choose to translate a larger term with a high consistency strength, which subsumes the term as a nested term, or just let other models decide how to translate this term. All these topic-sensitive term translation consistency patterns (e.g., patterns shown in Table 7) are learned by the term translation consistency model from training data and used during decoding.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Model C: term unithood
      </section-title>
      <paragraph>
       The term translation disambiguation model and consistency model concern the term translation accuracy with domain information. We further propose a term unithood model to guarantee the integrality of term translation. Xiong et al. [27] propose a syntax-driven bracketing model for phrase-based translation, which predicts whether a target translation of a phrase is contiguous with rich syntactic constraints. It is also desirable for multi-word terms to be contiguous units after translation. We therefore adapt Xiong et al. [27]'s bracketing approach to term translation and build a classifier to measure the probability that a source term should be translated into a contiguous unit.
      </paragraph>
      <paragraph>
       For all source parts of the extracted bilingual terms, we find their target counterparts in the word-aligned training data. If the corresponding target counterpart remains contiguous, we take the source term as a true unithood instance, otherwise a false unithood instance. With these instances, we train a maximum entropy (MaxEnt) binary classifier to predict the unithood ({a mathematical formula}u∈{true,false}) probability of a given source term {a mathematical formula}tf within particular contexts {a mathematical formula}c(tf). The binary classifier is formulated as follows:{a mathematical formula} where {a mathematical formula}hj∈{0,1} is a binary feature function and {a mathematical formula}θj is the weight of {a mathematical formula}hj.
      </paragraph>
      <paragraph>
       The feature {a mathematical formula}hj takes the following binary form:{a mathematical formula} where {a mathematical formula}μ∈{true,false}, ψ represents a contextual element for the source term {a mathematical formula}tf and ν denotes the value of the element. We use the following contextual elements of a source term to define our features: 1) the word sequence of the source term, 2) the first word of the source term, 3) the last word of the source term, 4) the preceding word of the first word of the source term, 5) the succeeding word of the last word of the source term, and 6) the number of words in the source term. Taking the third contextual element as an example, we can define a feature as follows:{a mathematical formula}
      </paragraph>
      <paragraph>
       Given a source sentence which contains T terms {a mathematical formula}{tfi}1T, our term unithood model TermUnit can be denoted as follows:{a mathematical formula} Whenever a hypothesis translates a source term {a mathematical formula}tfi into a contiguous unit on the target side, we calculate the unithood probability of {a mathematical formula}tfi according to the equation (13). For those source terms that are not translated into a whole unit, we do not calculate this probability.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Integration of the three models into SMT
     </section-title>
     <paragraph>
      Our Models can be integrated into any SMT formalisms that use the log-linear model for feature combination as described in Section 2.1. This is because we only need to integrate the three models as new features into the log-linear model. The integration itself will neither change values of other features nor methods to calculate these values.
     </paragraph>
     <paragraph>
      Without loss of generality, we choose hierarchical phrase-based SMT [3], one of state-of-the-art SMT formalisms, to show how we integrate the three models into SMT and to validate their effectiveness on term translation. The integration can be easily adapted to other SMT formalisms. Before we describe the integration, we give a short introduction of hierarchical phrase-based SMT, especially translation rules and the log-linear model of it.
     </paragraph>
     <paragraph>
      The rules used in hierarchial phrase-based SMT are synchronous context-free grammar rules, which can be represented as follows:{a mathematical formula} where X is a nonterminal, α and β are strings that contain terminals (words) and nonterminals on the source and target side respectively, ∼ denotes the one-to-one alignment between nonterminals in α and nonterminals in β. These rules can be automatically extracted from word-aligned bilingual training data. If rules only contain terminals, we refer to them as phrase rules since they are the same as bilingual phrase pairs used in phrase-based SMT [24]. Our extracted bilingual term pairs described in Section 3 are exactly phrase rules. In addition to rules that are extracted from bilingual training data, the grammar of hierarchical phrase-based SMT also includes two special glue rules that concatenate nonterminal {a mathematical formula}Xs in a monotonic fashion.
     </paragraph>
     <paragraph>
      The log-linear model of hierarchical phrase-based SMT can be formulated as follows:{a mathematical formula} where {a mathematical formula}D is a derivation, {a mathematical formula}w(D) is the score of D, {a mathematical formula}t(r) is the translation probability of rule r, {a mathematical formula}Plm(e) is the language model, {a mathematical formula}|e| is the number of words in the target translation e and I is the number of rules in {a mathematical formula}D. The derivation {a mathematical formula}D is defined as a set of triples {a mathematical formula}(r,i,j), each of which denotes an application of a rule that spans words from i to j on the source side.
     </paragraph>
     <paragraph>
      Each of the tree term translation models is treated as a new feature when we integrate them into hierarchical phrase-based SMT. With these three features, the log-linear model shown in the equation (17) is reformulated as follows:{a mathematical formula} where TermDis, TermCons, TermUnit are the term translation disambiguation, consistency and unithood model calculated according the equation (8), (11), (15) respectively, {a mathematical formula}λtd,λtc,λtu are their weights that are tuned with other feature weights via the Minimum Error Rate Training [5]. The integrated three models contribute to term translation selection in proportion to their weights. They collectively enable the decoder to select translation hypotheses that translate terms appropriately and consistently as a whole unit.
     </paragraph>
     <paragraph>
      The three models only apply to translation rules that contain a source term. On the one hand, as the values of the three models calculated on these rules are either probabilities (the term translation disambiguation and unithood model) or between 0 and 1 (the term translation consistency model), the integrated values of the three models will be minus (after taking the logarithm). Generally, the more rules are applied, the lower the overall score {a mathematical formula}w(D) calculated in the equation (18) will be. On the other hand, the best translation is generated from the best derivation with the highest score according to the equation (18). This indicates that the integration of the three models has a bias towards translation hypotheses generated by fewer rules against those generated by more translation rules. We don't want this. Therefore we add a counting feature to balance this as follows:{a mathematical formula} where T is the number of source terms translated by the derivation {a mathematical formula}D. This counting feature will reward translation hypotheses that are generated with more translation rules.
     </paragraph>
     <paragraph>
      We can also integrate the three models into SMT one-by-one with the term counting feature. For example, we can only integrate the term translation disambiguation model into SMT as follows:{a mathematical formula}
     </paragraph>
    </section>
    <section label="6">
     <section-title>
      Experiments
     </section-title>
     <paragraph>
      In this section, we evaluated the effectiveness of the proposed term translation models and their combination and variants. We also conducted experiments to investigate the impact of various factors, such as the number of topics and the size of extracted bilingual term bank, on our models. Please refer to Appendix A for details of our experiment setup.
     </paragraph>
     <section label="6.1">
      <section-title>
       Overall performance
      </section-title>
      <paragraph>
       We incorporated the proposed three models simultaneously into the decoder so as to investigate whether they can collectively obtain significant improvements over the baseline system, a state-of-the-art hierarchical phrase-based system that is built following Hiero [25]. In addition to the baseline, we also compared against an open source system Moses-Chart that is a re-implementation of hierarchical phrase-based SMT system in the Moses framework [28] and a method (“CountFeat”) proposed by Ren et al. [21] who use a binary feature to indicate whether a bilingual phrase contains a term. The reason that we compared against the “CountFeat” method is because it is one of the few approaches that consider term translations in the context of SMT. The binary feature in this method is 1 if a target language phrase contains a term otherwise 0.
      </paragraph>
      <paragraph>
       We set the number of topics {a mathematical formula}k=150 and used the combination of bilingual terms extracted by the LLR method and C-value/NC-value method with all bilingual training data. For more details on the impact of these parameters, please refer to Section 6.2.
      </paragraph>
      <paragraph>
       Results are reported in Table 8. Our re-implemented hierarchical phrase-based system (baseline) is more competitive than the Moses re-implementation. We also find that the combination of the three models (Combined-Model) achieves higher BLEU scores than the baseline, Moses-Chart and the CountFeat method. The final gain of Combined-Model over the baseline is 1.27 BLEU points and 0.92 points on MT06 and MT08 respectively. It is also significantly better than the CountFeat method by 0.77 BLEU points on the MT08 test set.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Impacts of various factors
      </section-title>
      <paragraph>
       We carried out a series of experiments to further investigate the impacts of various factors on our models. Particularly, we would like to answer the following three questions by this group of experiments.
      </paragraph>
      <list>
       <list-item label="•">
        What's the impact of the topic number k on the term translation disambiguation and consistency models that explore document-level topic information?
       </list-item>
       <list-item label="•">
        What's the impact of the size of extracted bilingual term bank on the term translation disambiguation and consistency model?
       </list-item>
       <list-item label="•">
        Which term extraction method, LLR, C-value/NC-value or their combination, is the best method for our term translation models?
       </list-item>
      </list>
      <section label="6.2.1">
       Topic number k
       <paragraph>
        As we integrate document-level topic information into the term translation disambiguation and consistence model, we want to study the impact of the number of topics on these two models. We therefore carried out experiments on the development test set with the number of topics k varying from 50 to 200. Table 9 shows the results. From the table, we clearly find that the performance of both models in terms of BLEU goes up when we increase k from 50 to 150 and down when k continuously rises to 200. This trend can be observed again on the final test set. This suggests that the two models obtain the best performance when {a mathematical formula}k=150.
       </paragraph>
       <paragraph>
        In order to investigate why this happens, let us suppose that we use the most probable topic of a document as the topic assignment for the document.{sup:6} We find that terms occur in 50 different topics if we set the topic number k to 50, 86 topics if {a mathematical formula}k=100, 110 topics if {a mathematical formula}k=150 and 116 topics if {a mathematical formula}k=200 in our training data. The data sparseness problem is becoming serious when {a mathematical formula}k=200 as terms do not occur in many topics (about 42%). However, the data sparseness problem is under control when {a mathematical formula}k=150 since only 26.7% topics do not have terms. This is further alleviated as we use per-document topic distributions instead of the most probable topics on all documents. The value of {a mathematical formula}k=150 is the best tradeoff between the benefit from using topic information and the cost of data sparseness caused by a larger topic number. Therefore we set the number of topics k to 150 for all experiments hereafter.
       </paragraph>
      </section>
      <section label="6.2.2">
       <section-title>
        Bilingual term bank size
       </section-title>
       <paragraph>
        We built several bilingual term banks with different numbers of bilingual term pairs by varying the size of bilingual data, from which we extracted bilingual term pairs according to the method described in Section 3. Specifically, we built 5 different bilingual term banks {a mathematical formula}{TBi}15, where {a mathematical formula}TBi⊂TBi+1. We then evaluated the term translation disambiguation model with these 5 bilingual term banks on the test sets MT06 and MT08. The BLEU scores are plotted in Fig. 1. From the figure, we can obviously see that the BLEU score of the term translation model gradually rises from 32.65 to 33.22 on MT06 and 24.30 to 24.72 on MT08 when we feed more bilingual terms to the model. On average, we can obtain an improvement of 0.1 BLEU points for each increase of 0.2 million bilingual terms on the development test set MT06. The curves do not level off even if we extract all bilingual terms from our bilingual training data, this indicates that the upward trend does not stop and that we can potentially achieve further improvement if we have more bilingual training data.
       </paragraph>
      </section>
      <section label="6.2.3">
       <section-title>
        LLR vs. C-value/NC-value in bilingual term extraction
       </section-title>
       <paragraph>
        We further carried out experiments to empirically compare different term extraction methods (i.e., LLR vs. C-value/NC-value) on the proposed three term translation models. We report the experiment results in Table 10. From the table, we can observe that
       </paragraph>
       <list>
        <list-item label="•">
         First, for all three models, the LLR method is marginally better than the C-value/NC-value term extraction method. This may be because that the LLR method extracted more bilingual term pairs than the C-value/NC-value method (1.01M vs. 0.90M).
        </list-item>
        <list-item label="•">
         Second, if we combine the bilingual terms extracted by the C-value/NC-value and LLR method together, we achieve the best performance for all three models. The combination of the C-value/NC-value and LLR outperforms the C-value/NC-value by up to 0.2 BLEU points.
        </list-item>
       </list>
      </section>
     </section>
     <section label="6.3">
      <section-title>
       Effect of the proposed three models
      </section-title>
      <paragraph>
       In this section, we thoroughly validated the effectiveness of the proposed term translation disambiguation model, consistency model and unithood model respectively. In particular,
      </paragraph>
      <list>
       <list-item label="•">
        We evaluated the term translation disambiguation, consistency and unithood models against the baseline systems.
       </list-item>
       <list-item label="•">
        We derived new variations of the term translation consistency and unithood models with or without topic information to study the impact of topic information on these two models.
       </list-item>
       <list-item label="•">
        We compared our term translation consistency model against an alternative consistency method that rewards hypotheses with repeated terms from previously translated sentences.
       </list-item>
      </list>
      <paragraph>
       We used the same parameter setting as described in Section 6.1 in all experiments of this section.
      </paragraph>
      <section label="6.3.1">
       <section-title>
        Term translation disambiguation model
       </section-title>
       <paragraph>
        We carried out experiments to investigate the effect of the term translation disambiguation model (Dis-Model) and report the results in Table 11. According to the table, our Dis-Model gains higher performance in terms of BLEU than both the baseline system and the “CountFeat” method. The “CountFeat” method rewards translation hypotheses that contain bilingual terms. It does not explore any domain information. In contrast, our Dis-Model incorporates document-level topic information to conduct translation disambiguation. Particularly, the term translation disambiguation model outperforms the “CountFeat” method by up to 0.45 BLEU points. It is also significantly better than the baseline by 0.81 and 0.58 BLEU points on MT06 and MT08, respectively. The final gain over the baseline is 0.69 BLEU points on average.
       </paragraph>
      </section>
      <section label="6.3.2">
       <section-title>
        Term translation consistency model
       </section-title>
       <paragraph>
        We conducted experiments to study whether the term translation consistency model (Cons-Model) is able to improve the performance in terms of BLEU. Results are shown in Table 12. Cons-Model gains significant improvements of 0.79 and 0.72 BLEU points over the baseline system on MT06 and MT08, respectively. It also outperforms the “CountFeat” method by 0.5 BLEU points on average.
       </paragraph>
       <paragraph>
        We also compared our term translation consistency model against another two models.
       </paragraph>
       <list>
        <list-item label="•">
         “Cons-Count”: This model rewards a translation hypothesis whenever a target term in the hypothesis has already occurred in recently translated sentences. We maintain a counter for each sentence and store all target terms in previously translated sentences in the same document. If a previously used target term repeats itself in the current translation hypothesis, we accumulate the counter.
        </list-item>
        <list-item label="•">
         “Cons-No-Topic”: This model is a simplified version of our term translation consistency model, which does not use any topic information. In this model, the term translation consistency strength {a mathematical formula}cons(tf) is calculated as follows:{a mathematical formula} where {a mathematical formula}M,Nm,qmn are the same as defined in the equation (10).
        </list-item>
       </list>
       <paragraph>
        Although these two models are better than the baseline and the “CountFeat” method as shown in Table 12, they are worse than our term translation consistency model. Our model outperforms the “Cons-Count” model by up to 0.33 BLEU points. This suggests that the strategy of capturing information of how terms are consistently translated in the training data is better than the strategy of counting the times of a term being consistently translated in a test set on the fly. Our model is also better than the “Cons-No-Topic” model by 0.25 BLEU points on average. This indicates that topic information is useful in modeling term translation consistency.
       </paragraph>
      </section>
      <section label="6.3.3">
       <section-title>
        Term unithood model
       </section-title>
       <paragraph>
        We compared the term unithood model against the baseline. Additionally, we also compared the term unithood model against its two variations: “Unit-All” and “Unit-Topic”. The Unit-All model predicts the unithood property for any source phrases of length up to 6 words (not limited to terms). In order to train this model, we extracted all source phrases that are translated as a whole unit as true unithood instances, otherwise false unithood instances. The second variation Unit-Topic model incorporates document topics as features into the MaxEnt classifier to predict whether a source term is translated as a whole unit into the target language. We set the topic number {a mathematical formula}k=150.
       </paragraph>
       <paragraph>
        Experiment results are shown in Table 13. From the table, we can observe that
       </paragraph>
       <list>
        <list-item label="•">
         The term unithood model achieves an improvement of 0.83 BLEU points over the baseline on average. It also outperforms the “CountFeat” method. This suggests that our term unithood model is useful for term translation.
        </list-item>
        <list-item label="•">
         The “Unit-All” model also outperforms the baseline. However it is worse than the term unithood model. This might suggest that terms are more likely to be translated as a whole unit than other source phrases (ordinary phrases). The errors in predicting the unithood property of ordinary phrases may be more serious than those for terms. These errors, in turn, jeopardise translation quality.
        </list-item>
        <list-item label="•">
         The “Unit-Topic” model is marginally worse than the term unithood model. This seems to suggest that document-level topic information is not helpful for predicting the unithood property of terms.
        </list-item>
       </list>
      </section>
     </section>
    </section>
    <section label="7">
     <section-title>
      Analysis
     </section-title>
     <paragraph>
      In this section, we will provide more details of our three term translation models from two distinct perspectives: extracted bilingual terms and translations generated by the baseline and the system enhanced with our models. In the first perspective, we will study the distributions of bilingual terms in sentences and documents and evaluate the quality of extracted bilingual terms. In the second perspective, we will take a deeper look at the differences that our models make on target translations. The analysis from these two angles will help us gain some insights into why we need to propose the three models and how the proposed models improve term translation in SMT.
     </paragraph>
     <section label="7.1">
      <section-title>
       Analysis on extracted bilingual terms
      </section-title>
      <paragraph>
       We provide some statistics of bilingual terms extracted from the training data in this section. First, we show the total number of bilingual terms extracted from the training data and the number of sentences that contain bilingual terms in the training data in Table 14. We can see that the majority of sentences contain bilingual terms ({a mathematical formula}2.72M/4.28M≈65.07%). On average, a source term has about 1.70 different translations ({a mathematical formula}1.81M/1.08M≈1.68). These statistics indicate that terms are frequently used in real-world data and that a source term can be translated into different target terms.
      </paragraph>
      <paragraph>
       Second, we also present some bilingual term examples extracted from the training data in Table 15. Accordingly, we show the total number of documents in which the corresponding source term occurs and the number of documents in which the corresponding source term is translated into different target terms. The source term “fang2yu4 xi4tong3” has 23 different translations in total. They are distributed in 470 documents in the training data. In 414 documents, “fang2yu4 xi4tong3” has only one single translation. However, in the other 56 documents it has different translations. This indicates that “fang2yu4 xi4tong3” is not consistently translated in these 56 documents. Different from this, the source term “zhan4lve4 dao3dan4 fang2yu4 xi4tong3” only has one translation. And it is translated consistently in all 7 documents where it occurs.
      </paragraph>
      <paragraph>
       In fact, according to our statistics, there are about 5.19% source terms whose translations are not consistent even in the same document. This percentage sharply increases to 23.49% in the development and test set. The examples and statistics shown here suggest 1) that source terms have domain-specific translations and 2) that terms are not necessarily translated in a consistent manner even in the same document. These are exactly the reasons why we propose the term translation disambiguation and consistency model based on domain information represented by topic distributions.
      </paragraph>
      <paragraph>
       Third, we also give the statistics of the unithood information of extracted bilingual terms in Table 16. We can see that the percentage of source terms that are always translated into contiguous target strings is 63.87% among all extracted source terms. It indicates that 36.13% source terms are not translated into target strings as a whole unit. This is the reason why we propose a unithood model to predict whether a source term remains contiguous or not after translation.
      </paragraph>
      <paragraph>
       Finally, we conducted a human evaluation on the quality of extracted bilingual terms. We randomly selected 1000 bilingual terms from our bilingual term bank. We showed the selected bilingual terms to a Chinese native speaker who is also fluent in English. We asked her to judge whether the source part of a given term pair is a term, whether the target part of a given term pair is a term and whether two parts are both terms and translations of each other. Based on her judgment, we calculated the percentage that items are judged as real terms among all given items. Table 17 shows the manual evaluation results. 64% of extracted bilingual terms are real bilingual terms according to the human evaluation. Table 18 display some examples of extracted terms, where both/one/neither sides of examples are manually judged as real terms.
      </paragraph>
      <paragraph>
       In the majority of cases where only one side (source or target) is judged as a real term, boundary words (leftmost/rightmost) on the false term side is incorrectly aligned to the real term on the other side. If we remove these incorrectly aligned boundary words, both sides are real terms. For example, word “zhu4yi4” in “zhu4yi4 guo2ji4 an1quan2 ||| international security” in Table 18 is wrongly aligned to “international security”. If it is removed, the remaining phrase will be a term. This is especially true for the source side, which explains why the term accuracy of the target side is much higher than that of the source side as shown in Table 17 (72.3% vs. 64.6%). We can enhance our monolingual term pairing procedure described in Section 3.2 by removing these incorrectly aligned boundary words. We believe that a higher percentage of real terms on both sides will further improve our models. We leave this to our future work.
      </paragraph>
     </section>
     <section label="7.2">
      <section-title>
       Analysis on generated target translations
      </section-title>
      <paragraph>
       In this section, we investigate to what extent the proposed models affect the translations on the test sets from a high level. We also provide translation examples to visualize how our models improve translation quality.
      </paragraph>
      <paragraph>
       In Table 19, we show the percentages of final translations where the combined model and the three single models are activated on the development test set MT06 and the final test set MT08. We can see that final translations of sentences affected by any of the proposed models account for a high proportion ({a mathematical formula}45%∼55%) on both MT06 and MT08. This indicates that the three proposed models indeed significantly affect the translations of the two sets. For the combined model, if any of the three models are activated on a given final translation hypothesis, we consider the combined model is activated on this translation. Based on this counting strategy, we find that final translations affected by the combined model account for 62.32% and 56.48% on MT06 and MT08 respectively. Table 19 also presents the improvements of BLEU scores obtained by our models on sentences where our models are activated. We can achieve an improvement of up to 1.80 BLEU points over the baseline on these sentences.
      </paragraph>
      <paragraph>
       In order to investigate how source terms are consistently translated in the two test sets, we calculate the term translation consistency index, similar to Itagaki et al. [26]. Table 20 report the consistency index values of the baseline and our term translation consistency model on the two sets MT06 and MT08. We can clearly observe that the consistency index values of Cons-Model are higher than those of the baseline system. This strongly suggest that terms are more consistently translated if we integrate the proposed term translation consistency model into the decoder.
      </paragraph>
      <paragraph>
       Table 21 displays 4 translation examples which visualize how our models affect translation hypotheses. In the first example, the baseline system incorrectly translates the source phrase “zhu3liu2 min2yi4” into “mainstream opinion” while the Dis-model produces the correct translation “mainstream public opinion” with topic information. Table 22 shows the translation probabilities of different target translations for the source term “zhu3liu2 min2yi4” calculated by our term translation disambiguation model given the topic {a mathematical formula}z=32. Obviously our Dis-Model favors the target translation “mainstream public opinion” against the translation “mainstream opinion” selected by the baseline system (0.56 vs. 0.24).
      </paragraph>
      <paragraph>
       In the second example, the source term “huan2jing4 zhi2fa3 ren2yuan2” has only one translation “environmental law enforcement personnel” in our training data. Therefore the consistency strength of the term is the highest 1. We encourage the decoder to translate this term with the translation from our bilingual term bank. In contrast, the nested term “zhi2fa3 ren2yuan2” has more than 25 different translations in our training data. It has a very low consistency strength 0.0236804 under the current topic. If we choose to translate this nested term, our translations for the whole source term “huan2jing4 zhi2fa3 ren2yuan2” will be not consistent across sentences.
      </paragraph>
      <paragraph>
       In the third translation example, the source term “qu1ru3 li4shi3” is translated by the baseline system into two discontinuous strings “history” and “humiliation”, separated by the translation of “ge1rang4”. However, our term unithood model successfully translates this source term as a whole unit into “humiliating history”, which is the same as the reference translation.
      </paragraph>
      <paragraph>
       The three translation examples discussed above show how our term translation disambiguation, consistency and unithood model improve translation quality. In the final example, our model recognizes two phrases as terms. The first phrase “jie2guo3 jie1xiao3” is actually not a term. Due to this noisy term in our bilingual term bank, our model wrongly translates this phrase into “results announced”. The other phrase “ban1jiang3 yi2shi4” is a correct term and our model correctly translate it. This suggests that our models, to some extent, are sensitive to noises of extracted terms.
      </paragraph>
     </section>
    </section>
    <section label="8">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      In this section, we introduce related work and highlight the differences between our work and previous studies. The exploration of statistical term translation in SMT is quite limited. To the best of our knowledge, our work is the first attempt to systematical investigation of term translation in the context of statistical machine translation.
     </paragraph>
     <paragraph>
      Bilingual Terminology for Machine Translation: Itagaki and Aikawa [29] employ bilingual term bank as a dictionary for machine-aided translation. Ren et al. [21] propose a binary feature (0/1) to indicate whether a bilingual phrase contains a bilingual term pair. Arcan et al. [30] extract and integrate bilingual terminology into SMT in a CAT environment. Weller et al. [31] mine bilingual terminology from comparable corpora to enhance SMT in domain adaptation. These studies either focus on a specific single issue of term translation or use extracted bilingual terminology as an additional resource to enhance machine translation. They do not systematically investigate term translation in the context of SMT as we do. Furthermore, document-level information is not used to assist term translation in their work.
     </paragraph>
     <paragraph>
      Finding Term Translations: A number of approaches have been proposed to find term translations or extract bilingual terminology from parallel/comparable corpora [32], [33], [23], [34], [35]. To name a few, Fung and Mckeown [32] propose a method to extract terminology translations from non-parallel corpora. Lefever et al. [23] introduce a language-independent method for bilingual terminology extraction from a word-aligned parallel corpus. We also extract bilingual terminology from our parallel training data via a strategy that pairs source and target term candidates based on word alignments. We will explore more different methods for bilingual terminology extraction in our future work.
     </paragraph>
     <paragraph>
      Translation Consistency and Term Unithood: A variety of methods have been proposed to encourage translation consistency, ranging from cache-based models [36], [37] and post-editing methods [38] to soft constraints as additional features of the log-liner model [39]. Our consistency model is most related to the method by Itagaki et al. [26] who propose a statistical method to evaluate translation consistency for terms. Partially inspired by them, we introduce a topic-based term translation consistency metric. The differences between our term translation consistency model and their consistency index are twofold. First, we introduce per-document topic distributions into our model to calculate topic-dependent term translation consistency strengths for terms as shown in the equation (10). Second, we integrate the proposed term translation consistency model into an actual SMT system, which has not been done by Itagaki et al. [26].
     </paragraph>
     <paragraph>
      Term unithood is first described by Kageura and Umino [2]. Lefever et al. [23] use the mutual expectation measure to estimate term unithood. Our term unithood model is different from theirs in that we use a classifier to predict the probability of term unithood after translation. Our model is related to Xiong et al.[27]'s syntax-driven bracketing model for phrase-based translation, which predicts whether a phrase is translated as a whole unit with rich syntactic constraints. The difference is that we construct the model with automatically created bilingual terms and do not depend on any syntactic knowledge.
     </paragraph>
     <paragraph>
      Topic Modeling for SMT: As we approach term translation disambiguation and consistency via topic modeling, our models are related to previous work that explores topic models for machine translation [40], [41], [42], [43], [44]. Among them, our topic-based term translation disambiguation model is most related to the work of Xiao et al. [42], who propose a topic similarity and sensitivity model for translation rule selection in hierarchical phrase-based SMT. Although we also use topic information to help disambiguate term translation, our term translation disambiguation model is significantly different from Xiao et al. [42]'s topic similarity model in that they estimate the rule-topic distributions {a mathematical formula}p(z|r) while we estimate term translation probabilities {a mathematical formula}p(tf|te,z) conditioned on topics. Furthermore, we focus on the three translation issues (disambiguation, consistency and unithood) of terms that are special phrases with linguistic and statistical properties.
     </paragraph>
     <paragraph>
      Document-Level Machine Translation: Finally our work is also related to previous work [45], [46], [47], [48], [49], [50] on document-level machine translation in that we use document-level information for term translation. The significant difference between our work and these studies is that term translation has not been investigated in these document-level machine translation models.
     </paragraph>
    </section>
    <section label="9">
     <section-title>
      Conclusions and future work
     </section-title>
     <paragraph>
      We have studied the three issues of term translation in the context of SMT and proposed three different term translation models to address these issues. The term translation disambiguation model enables the decoder to favor the most suitable domain-specific translations with document-level information for source terms. The term translation consistency model encourages the decoder to translate source terms with a high topic-dependent translation consistency strength into consistent target terms. Finally, the term unithood model rewards hypotheses that translate terms into continuous target strings as a whole unit.
     </paragraph>
     <paragraph>
      We integrate the three models into a hierarchical phrase-based SMT system{sup:7} and evaluate their effectiveness on NIST Chinese–English translation with large-scale training data. Experiment results show that
     </paragraph>
     <list>
      <list-item label="•">
       The term translation disambiguation model is able to obtain a substantial improvement of 0.58 BLEU points over the baseline on the test set.
      </list-item>
      <list-item label="•">
       The term translation consistency model outperforms the baseline by 0.72 BLEU points. We also observe 1) that topic information improves the model as term translation consistency is topic-sensitive and 2) that modeling how terms are translated consistently in training data is better than counting the number of times that they are translated consistently in a test set.
      </list-item>
      <list-item label="•">
       The term unithood model is also better than the baseline by 0.79 BLEU points on the test set. Additionally, we find that document-level topic information cannot improve the model.
      </list-item>
      <list-item label="•">
       The combination of the three models can obtain further improvement, which is 0.92 BLEU points over the baseline on the final test set.
      </list-item>
     </list>
     <paragraph>
      Our experiments also disclose 1) that the more bilingual terms we extract, the better translation quality will be, 2) and that the LLR method is marginally better than the C-value/NC-value method in the bilingual term extraction and the combination of the two methods achieves the best performance. Our in-depth analyses further validate that the proposed three term translation models are indeed able to improve term translation.
     </paragraph>
     <paragraph>
      As shown in our analysis with translation examples, noises of extracted bilingual terms will guide our models to wrong translations. Therefore, we want to improve the procedure of bilingual term extraction so that we can further improve the performance of our method in the future. Additionally, we also plan to extend our models for the purpose of multilingual text analysis as well as multilingual terminology and ontology construction for specific domains as terms are able to convey concepts of a text or a domain.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Experiment setup
     </section-title>
     <paragraph>
      Our training data consist of 4.28M sentence pairs extracted from LDC{sup:8} data with document boundaries explicitly provided. The bilingual training data contain 67,752 documents, 124.8M Chinese words and 140.3M English words. We used the ICTCLAS segmenter [51] for Chinese word segmentation. We chose NIST MT05 as the development set for MERT tuning, NIST MT06 as the development test set, and NIST MT08 as the final test set. The numbers of documents/sentences in the NIST MT05, MT06 and MT08 are 100/1082, 79/1664 and 109/1357 respectively. There are 4 different human-generated reference translations for each source sentence in these dev/test sets.
     </paragraph>
     <paragraph>
      The word alignments were obtained by running GIZA++{sup:9}[52] on the corpora in both directions and using the “grow-diag-final-and” balance strategy [24]. We adopted SRI Language Modeling Toolkit [53] to train a 4-gram language model with modified Kneser–Ney smoothing on the Xinhua portion of the English Gigaword corpus (306 million words).
     </paragraph>
     <paragraph>
      We used the Stanford natural language processing toolkit{sup:10} to perform part-of-speech tagging. The tagger detects nouns, adjectives and prepositions for the linguistic filter in the C-value/NC-value based monolingual term extraction (see Section 3). Empirically, we set the maximum length of a term to 6 words.{sup:11} For both the C-value/NC-value and LLR-based extraction methods, we set the context window size to 5 words, which is a widely-used setting in previous work. Additionally, we set the C-value/NC-value score threshold to 0 and LLR score threshold to 10 based on our preliminary experiments on the training corpora.
     </paragraph>
     <paragraph>
      For the topic model, we used the open source LDA topic modeling tool GibbsLDA++{sup:12} with the default setting for training and inference. We performed 100 iterations of the L-BFGS algorithm implemented in the maximum entropy classifier toolkit{sup:13} with both Gaussian prior and event cutoff set to 1 to train the term unithood prediction model (Section 4.3).
     </paragraph>
     <paragraph>
      We used the case-insensitive 4-gram NIST BLEU{sup:14} as our evaluation metric, which measures modified precisions of n-grams against multiple reference translations. As terms occur frequently in text (more than 65% sentences in our corpus contains terms according to our statistics), changes in term translations can be captured by BLEU. In order to alleviate the impact of the instability of MERT, we ran it three times for all our experiments and presented the average BLEU scores on the three runs following the suggestion by Clark et al. [54].
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>