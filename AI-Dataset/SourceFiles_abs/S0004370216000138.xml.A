<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Learning an efficient constructive sampler for graphs.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Graph data structures allow us to model complex entities in a natural and expressive way. In the machine learning literature, several types of discriminative systems that can deal with graphs in input are known (e.g. recursive neural networks, graph kernels, graphical models, etc.), however, there are few generative approaches that can sample structures belonging to a desired distribution or class. We call the task of generating samples from an empirical probability distribution of graphs a “constructive learning problem” (CLP) when the generative machinery is adaptive and data driven. We argue that systems for CLPs are of great interest in order to address a vast class of interesting problems that require some notion of “creativity” to be solved. More precisely we are interested in learning problems that have a very large combinatorial solution space. As cursory examples let us mention problems such as de novo small molecules synthesis or oligonucleotides polymers synthesis in life sciences; automatic algorithm generation in the computer science domain; character or level generation in game domains. Even tasks that are traditionally associated to the artistic domain, such as music or poetry/prose generation, could in principle be represented as CLPs given appropriate high level representations.
     </paragraph>
     <paragraph>
      We cast the problem of generating elements in a domain of interest as the equivalent problem of sampling from a corresponding underlying probability distribution defined over that domain. Classic approaches to sampling include “direct enumeration” of all possibilities and evaluations of their probabilities. For combinatorial problems this approach is however intractable as the solution space grows exponentially with the problem size. Another approach is “rejection sampling” where samples are generated without enforcing constraints but are then rejected if violations in the probability distribution occur. However this technique cannot be used when random sampling from the underlying product space rarely yields valid samples. Another approach is “Markov Chain Monte Carlo” (MCMC) methods, where the problem of sampling is reduced to the easier task of simulation, provided that one can build an ergodic Markov chain with the desired distribution as its equilibrium distribution.
     </paragraph>
     <paragraph>
      The extension of these approaches to structured domains (i.e. where instances are strings, trees, graphs or hypergraphs) is currently underdeveloped, and while specialized applications exist, e.g. sampling phylogenetic trees [50], sampling dependency graphs for structural learning in graphical models [30], or sampling large Web like networks [21], data driven approaches that can deal with general types of CLPs, are, to the author's knowledge, still in their infancy. General purpose generative techniques for structured domains are in fact more difficult than their non-structured counterparts; complex encodings are inevitably associated to complex feasibility constraints which in turn implies that only a vanishingly small fraction of all possible structures is allowed. Naïve approaches that apply local search under simple neighborhood definitions fail to capture those complex feasibility constraints and tend to produce objects that are almost always syntactically and/or semantically incorrect. Imagine for example the high failure rate of an approach that builds molecular graphs by connecting at random atoms regardless of their valence, or of an approach that constructs sentences by inserting or replacing words at random from a dictionary. Both rejection sampling approaches and Markov chain simulations would almost always reject those low probability instances, provided that the probability mass of any proposed instance could be reliably predicted in the first place.
     </paragraph>
     <paragraph>
      To obtain a general purpose structure generator, we need a data driven procedure that can learn and adaptively model complex constraints. Here we propose to cast the problem as two associated inference problems: 1) a grammatical inference problem and 2) a high dimensional probability inference problem. These two procedures will then be used to inform a MCMC sampling scheme. The main contribution of this paper lies in the formulation of a novel type of grammar for graphs that admits efficient inference and that can be used efficiently in a generative mode.
     </paragraph>
     <paragraph>
      We proceed as follows. We start off by formally introducing the problem and describing the key sampling technique (the Metropolis Hastings algorithm) in Section 2. Then, we introduce a novel graph grammar with the associated induction strategy in Section 3. In Section 4 we address the problem of estimating probabilities for structured data and describe a graph kernel based approach to this end. We then connect all the various parts together and in Section 5 we describe the procedure for learning how to generate the samples of interest. Before concluding, in Section 6 we present an extensive experimental evaluation and we report encouraging experimental results in a de-novo molecular synthesis problem, where the constructive task is to sample novel molecular graphs with the desired biological activity.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Generating structures from empirical distributions
     </section-title>
     <paragraph>
      In this section we give a formalization of the constructive learning problem (CLP), which can be presented as the task of generating structures from an empirical probability distribution using an adaptive, data driven procedure. When we use the term “structure” we mean equivalently an object that can be efficiently encoded as a graph or a hyper graph (see Section 3.1.2).
     </paragraph>
     <section label="2.1">
      <section-title>
       Theoretical model
      </section-title>
      <paragraph>
       We now introduce the necessary notions to characterize the CLP as an optimization problem over a structured domain. Let {a mathematical formula}G be the domain of graphs equipped with a probability notion, such that we can model a concept of interest C using an associated probability distribution P.
      </paragraph>
      <paragraph label="Definition 2.1">
       GeneratorLet {a mathematical formula}G⊆G be a set of graphs. Let {a mathematical formula}Rθ⊆G×G be a binary relation over the structured domain {a mathematical formula}G parametrized by θ. A “generator” {a mathematical formula}Mθ is defined as:{a mathematical formula} that is {a mathematical formula}Mθ is a procedure that given a set of graphs G in input, returns the set of all the graphs for which the relation {a mathematical formula}Rθ holds true with at least one element of G.
      </paragraph>
      <paragraph>
       Given P over {a mathematical formula}G, the relation {a mathematical formula}Rθ defines a probability distribution {a mathematical formula}Pθ:{a mathematical formula} that is, the probability to generate an instance G is the sum of the probabilities of those graphs in G that together with G belong to {a mathematical formula}Rθ.
      </paragraph>
      <paragraph>
       In a CLP for structured domains we are interested in finding the generator's parameters {a mathematical formula}θ⁎ that minimize the difference between the concept C and the concept corresponding to the set of generated graphs, that is, between the two probability distributions P and {a mathematical formula}Pθ.
      </paragraph>
      <paragraph label="Definition 2.2">
       Constructive learning problemLet L be a non-negative, convex function between probability distributions over {a mathematical formula}G that evaluates to zero if the two probability distributions are identical. L can be defined in terms of the Kullback–Leibler divergence as:{a mathematical formula} where {a mathematical formula}DKL is the Kullback–Leibler divergence:{a mathematical formula}The constructive learning problem is a solution to the following optimization:{a mathematical formula}
      </paragraph>
      <paragraph>
       In practical scenarios however, P and {a mathematical formula}Pθ are both unknown and need to be estimated from a finite set of examples. Let {a mathematical formula}G0⊆G be a finite sample drawn according to P and let {a mathematical formula}Gθ be the corresponding sample generated by {a mathematical formula}Mθ, {a mathematical formula}Gθ=Mθ(G0). Let {a mathematical formula}fw be a parametrized procedure to estimate the probability distribution of an input sample. Let {a mathematical formula}fG0 ({a mathematical formula}fGθ) be the estimator fit to the sample {a mathematical formula}G0 ({a mathematical formula}Gθ) respectively. Let {a mathematical formula}G=G0∪Gθ. Considering {a mathematical formula}fG ({a mathematical formula}fGθ) instead of P ({a mathematical formula}Pθ) we write:{a mathematical formula}
      </paragraph>
      <paragraph>
       In order to avoid the trivial solution {a mathematical formula}Mθ(G0)=G0, i.e. a generator that returns an identical copy of the input sample, we add a penalty term to encourage sample diversity. To this end we trade-off the discrepancy of the generated probability distribution with the diversity of the generated graph set via a user defined parameter λ.
      </paragraph>
      <paragraph label="Definition 2.3">
       Constructive learning problem for finite samplesLet {a mathematical formula}S(G,G′) be a similarity measure between two sets of graphs defined in terms of an underlying graph kernel {a mathematical formula}K(⋅,⋅) between two graphs:{a mathematical formula}{a mathematical formula}The constructive learning problem for finite samples is the solution to the following optimization problem:{a mathematical formula}
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Solution to the CLP
      </section-title>
      <paragraph>
       The main difficulty in solving the constructive learning problem rests on 1) the generator {a mathematical formula}Mθ, which can be viewed as a procedure to obtain a set of instances, and 2) its optimization, which is a procedure to adapt the probability distribution of the output instances to a desired shape. These operations are reminiscent of Markov chain Monte Carlo (MCMC) methods, a class of procedures for obtaining a sequence of samples from a probability distribution for which direct sampling is difficult. Here we propose to model {a mathematical formula}Mθ using a type of MCMC known as the Metropolis–Hastings algorithm (MH). The two main components of MH are (see Section 2.3) the proposal and the acceptance probability distribution. We model the proposal probability distribution using a “graph grammar” (see Section 3.1.3) and the acceptance probability distribution using the probability estimator {a mathematical formula}fG0. The probability estimators are implemented using calibrated one-class SVMs [40] (see Section 4.3) equipped with an efficient graph kernel [8] (see Section 4.1) to process graphs.
      </paragraph>
      <paragraph>
       The proposed system for CLP is therefore composed of: 1) a MH sampler with a graph grammar and 2) probability estimators {a mathematical formula}fG0(G),fGθ(G).
      </paragraph>
      <paragraph>
       The parameterization of {a mathematical formula}Mθ include therefore parameters for the grammar, parameters for the MH algorithm, parameters for the one-class SVM estimator and parameters for the graph kernel. The optimization of the compound system is difficult; many components in fact have an algorithmic nature that cannot be reduced to a functional description which prevents us from using derivative based optimization methods. For this reasons we proceed in stages: first the parameters for the grammar, for the MH algorithm and for the graph kernel are fixed, then the grammar is induced and the probability estimators are optimized using classical SVM optimization schemes; then these parameters are changed uniformly at random within user defined parameters ranges and the procedure is iterated. After a predefined number of iterations the procedure terminates and the configuration that minimizes the objective function in Equation (9) is returned in the spirit of randomized search [3]. While more refined global derivative free optimization methods exist, randomized search offers interesting properties of efficiency and scalability: in certain conditions, i.e. when certain parameters have only a limited influence on the objective function, [3] showed that this approach can be surprisingly efficient; in addition, given that all runs are independent, we have a case of perfect parallelizability that can be effectively exploited on modern multi-core machines.
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       The Metropolis Hastings algorithm
      </section-title>
      <paragraph>
       We propose to address the constructive learning problem using a variant of the Metropolis Hastings (MH) algorithm [33]. MH is a Markov chain Monte Carlo (MCMC) method that can be used to generate a set of instances to approximately recover a hard to sample probability distribution (e.g. in high-dimensional cases). It does so by building a Markov process whose stationary distribution {a mathematical formula}π(x) asymptotically matches the desired distribution {a mathematical formula}P(x). A Markov process is defined by the probability {a mathematical formula}P(x→x′) of transitioning between any two states x to {a mathematical formula}x′. Two conditions are needed to guarantee the existence and unicity of the stationary distribution, namely detailed balance and ergodicity. The detailed balance condition requires that transitions {a mathematical formula}x→x′ should be reversible, that is, the probability of being in state x and transit to the state {a mathematical formula}x′ must be equal to the probability of being in state {a mathematical formula}x′ and transit to the state x, {a mathematical formula}π(x)P(x→x′)=π(x′)P(x′→x). To additionally guarantee that the limit distribution {a mathematical formula}π(x) is uniquely defined, the Markov process needs also to be ergodic, which implies that 1) every state must be aperiodic (i.e. there shouldn't be fixed intervals when the system returns to the same state); and 2) the process must be positive recurrent (i.e. the system has a nonzero probability to reach any state). The MH algorithm offers a way to build processes that automatically meet these requirements (for details see [34]). To do so, MH decomposes the transition probability in two factors, 1) the proposal and 2) the acceptance-rejection probability distributions: {a mathematical formula}P(x→x′)=g(x→x′)A(x→x′). The proposal distribution {a mathematical formula}g(x→x′) is the conditional probability of proposing a state {a mathematical formula}x′ given x. The acceptance distribution {a mathematical formula}A(x→x′) is the conditional probability to accept the proposed state {a mathematical formula}x′. In MH the proposal distribution {a mathematical formula}g(x→x′) can be chosen freely (provided ergodicity is ensured) and adjusted to the specific problem as long as the acceptance distribution is defined as {a mathematical formula}A(x→x′)=min⁡(1,P(x′)P(x)g(x′→x)g(x→x′)). The MH algorithm starts from a random x and then iterates the proposal and the acceptance steps; a new state {a mathematical formula}x′ is generated according to {a mathematical formula}g(x→x′); {a mathematical formula}x′ is then evaluated by {a mathematical formula}A(x→x′); if {a mathematical formula}P(x′)P(x)g(x′→x)g(x→x′)&gt;1 then {a mathematical formula}x′ is accepted with certainty, else it is accepted with the corresponding probability.
      </paragraph>
      <paragraph>
       Note that in combinatorial problems and in structured domains, there often exist strong and deterministic constraints between variables, which severely restrict the set of possible values that a variable can assume given the value of the other variables. It can therefore occur that no single variable update is possible without violating some constraints, a fact that renders the underlying Markov chain non-ergodic. Lin and Iii [26] address these problems introducing the notion of bridging to connect different regions of the sample space that can hardly “communicate with each other”. In the current work we do not directly address this issue since its effects are made less severe due to two specific characteristics of our setup. Firstly, we adopt a frequent restart policy, i.e. we generate samples starting from a large set of “seed” instances, sampling therefore the space. Second, the transitions in the proposals are data driven (see the following sections for details) and allow not only exponentially large jumps in the sample space,{sup:1} but also single vertex changes. This connects distant regions of the sample space while at the same time allowing atomic transitions.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      A grammar for structures
     </section-title>
     <paragraph>
      The key element for an efficient MH design consists in shaping the proposal distribution {a mathematical formula}g(x→x′) in such a way that the generated elements {a mathematical formula}x′ will not be rejected too often. One way to improve the efficiency of the approach (i.e. the ratio between the rejected and the generated instances) is to resort to an adaptive strategy whereby the proposal distribution is inferred from the available data. In practice we need to define a procedure that can be employed to construct instances that do not often violate the implicit constraints present in the data. To this end we will resort to the notion of grammatical inference. In a grammatical inference task, one is looking for a syntactic device of some predefined class (i.e. a grammar) that can generate a language containing a desired set of patterns. While the case of strings (i.e. finite sequences of discrete symbols) has been extensively studied, to date, the extension of grammatical inference techniques to graphs has not produced widely accepted techniques or readily available systems. The reason is mainly due to the fact that the key step of parsing is an NP-hard task for many classes of graph grammars [38] and hence the resulting parser implementations are complex and do not scale well.
     </paragraph>
     <section label="3.1">
      <section-title>
       Graph grammar induction literature
      </section-title>
      <paragraph>
       The large majority of graph grammar inference papers deal with non-probabilistic grammars of the context-free type. The context-free type, although less expressive than the context-sensitive type, exhibits several interesting properties that have been extensively investigated by the community. Among the main properties is the possibility to define the notion of derivation tree, which can be used to define the syntactic structure of an object. A related property is that of being confluent[9], which implies that two rewriting steps in distinct parts of the graph can be applied in any order and yield the same result. Being confluent and associative w.r.t. the substitution operator are the two necessary and sufficient conditions for any rewriting system to be context-free. Being confluent guarantees the uniqueness of each derivation; in other words, there is only one way to produce a graph, which depends on the set of the rules used but that does not depend on the order in which these are applied. Although this type of grammar naturally supports the extension to the probabilistic case, there is not much work that deal with the problem of how to induce the rules and their associated probabilities from a finite set of examples. The problem of deterministic context-free graph grammar induction is marginally more explored. As previously mentioned, the reason for this state of affairs is mainly due to the fact that the notion of parsing is an NP-hard task for many classes of graph grammars [38] and hence the resulting parser implementations are quite complex and do not scale in real world applications.
      </paragraph>
      <paragraph>
       Among the systems for context-free graph grammar induction, Jeltsch and Kreowski [17] introduce an induction algorithm for hyperedge replacement grammar based on the notion of iterative generalizations of the trivial initial grammar. The work of Kukluk et al. [20] is based on the MDL principle and aims at compressing frequent subgraphs; however, they lack an embedding mechanism, without which one cannot use the grammar to generate instances. In Blockeel and Nijssen [4] the authors induce a node-label controlled grammar to compress a single graph via its frequent subgraphs. More recently Fürst et al. [11] showed a system for layered graph grammars induction; however their approach suffers from an exponential worst case run time and it is not robust to outliers since it tries to find the smallest grammar that covers all graphs from the positive sample and none from the negative sample.
      </paragraph>
      <paragraph>
       All these systems deal with the induction of context-free graph grammars, however we argue that, in order to adequately model relational data, we need some notion of context-sensitiveness. In our view, it is not possible to describe with enough precision (i.e. without greatly over-generalizing) how to assemble a graph using only rules that ignore the constraints between the various parts of each instance. To the author's knowledge, context-sensitive grammars for graphs are not developed in literature as in general they pose greater challenges than context-free grammars.
      </paragraph>
      <paragraph>
       For these reasons we forgo the properties of context-free grammars in favor of greater expressivity provided that the solution is computationally efficient. Our proposal diverges substantially from the graph grammar inference literature, it 1) employs a context sensitive notion, 2) is efficient in the induction and production phase, and 3) is robust to outliers.
      </paragraph>
      <section label="3.1.1">
       <section-title>
        Principles for the proposed grammar
       </section-title>
       <paragraph>
        We start from the concept of distributional semantics[12], [13], which is the theory that quantifies and categorizes semantic similarities between (linguistic) items based on their “distributional” properties in large samples of (language) data. In words, the tenet of distributional semantics can be summed up as: items with similar distributions have similar meanings. In computational linguistics, the notion of distributional similarity has been an active research area for several decades [16], [39], [25], [48], [46], [22]. In Clark and Eyraud [7] the authors show that, exploiting the notion of distributional semantic, it is not necessary to learn a constituent structure in order to define a context-free grammar for string languages. Rather, it is enough to learn the syntactic congruence; formally, two words u and v are syntactically congruent w.r.t. a language L, written {a mathematical formula}u≡Lv, if and only if {a mathematical formula}∀l,r∈Σ⁎{a mathematical formula}lur∈L⇔lvr∈L. They show how one can efficiently learn (in Gold's identification in the limit sense) a language L, simply by looking at pairs of strings u and v and checking whether they occur in the same contexts; if pairs of strings of the form lur and lvr both occur in the language L, than one can assume the existence of a non-terminal symbol that generates both strings u and v. This is known as the “substitutability principle”. In other words, the non-terminal symbols are implicitly defined by the context {a mathematical formula}l−r.
       </paragraph>
       <paragraph>
        Here we propose two extensions: 1) we use a local notion of context and 2) we upgrade the substitutability principle from strings to graphs. The first point consists in reducing the context so that the grammar based on this notion of local substitutability is parametrized by the degree of locality, i.e. the size of the context. For the second point we need to develop techniques to match contexts in the form of graphs, i.e. solving the isomorphism problems between graphs in an efficient even if approximate way.
       </paragraph>
      </section>
      <section label="3.1.2">
       <section-title>
        Notation and definitions
       </section-title>
       <paragraph>
        In order to fix ideas, we give a short list of definitions and notation on graphs and basic operations on them.
       </paragraph>
       <paragraph>
        A graph {a mathematical formula}G=(V,E) consists of two sets: the vertex set V and the edge set {a mathematical formula}E⊆V×V. The notation {a mathematical formula}V(G) and {a mathematical formula}E(G) is used when G is not the only graph considered. The elements of V are called vertices and the elements of E are called edges. Each edge has a set of two elements in V associated with it, that are called its endpoints, which we denote by concatenating the vertices variables, e.g. uv represents the edge between the vertices u and v. A labeled graph is a graph whose vertices and/or edges are labeled using symbols from a finite alphabet by the function ℓ. Two labeled graphs {a mathematical formula}G1=(V1,E1) and {a mathematical formula}G2=(V2,E2) are isomorphic, which we denote by {a mathematical formula}G1≅G2, if there is a bijection {a mathematical formula}ϕ:V1→V2, such that for any two vertices {a mathematical formula}u,v∈V1, there is an edge uv if and only if there is an edge {a mathematical formula}ϕ(u)ϕ(v) in {a mathematical formula}G2 and if the label information is preserved, i.e. {a mathematical formula}ℓ(ϕ(v))=ℓ(v). An isomorphism invariant or graph invariant is a graph property that is identical for two isomorphic graphs (e.g. the number of vertices and/or edges). A certificate for isomorphism is an isomorphism invariant that is identical for two graphs if and only if they are isomorphic.
       </paragraph>
       <paragraph>
        A key notion in the proposed approach is that of “rooted neighborhood graph”. A graph is rooted in v, denoted {a mathematical formula}Gv, when we distinguish one of its vertices v, called root. In a graph G, the induced-subgraph on a set of vertices {a mathematical formula}W={w1,…,wk} is a graph that has W as its vertex set and whose edge set contains exactly every edge of G that has endpoints in W. For a given graph {a mathematical formula}G=(V,E), and an integer {a mathematical formula}r≥0, let the neighborhood graph {a mathematical formula}Nrv(G) denote the subgraph of G rooted in v and induced by the set of vertices {a mathematical formula}Vrv≐{x∈V:d(x,v)≤r}, where {a mathematical formula}d(x,v) is the shortest-path distance between x and v. A neighborhood graph is therefore a topological ball with center v and radius r.
       </paragraph>
       <paragraph>
        In the following we will be working with hypergraphs. In a hypergraph, the notion of edge (a binary relation between vertices) is generalized to that of hyperedge (relation of higher arity for a set of vertices).
       </paragraph>
       <paragraph>
        Formally, a graph is bipartite if its vertex set can be partitioned into two subsets X and Y so that every edge has one end in X and the other in Y. We denote a bipartite graph G with partition {a mathematical formula}(X,Y) by {a mathematical formula}G([X,Y],E). A hypergraph is an ordered pair {a mathematical formula}(V,F) where V is a set of elements and {a mathematical formula}F is a family of subsets of V. Note that when {a mathematical formula}F consists of pairs of elements of V, then {a mathematical formula}(V,F) is a simple graph. The elements of V are called vertices of the hypergraph and the elements of {a mathematical formula}F the hyperedges. A hypergraph can be represented as an incident graph: a bipartite graph {a mathematical formula}G([V,F],E) where {a mathematical formula}v∈V and {a mathematical formula}F∈F are adjacent if {a mathematical formula}v∈F. In the reminder we exploit the fact that all the notions that we have defined in our approach (e.g. paths, distances, neighborhood graphs, etc.) can be naturally extended to the incident graph representation of the hypergraph. Without any additional burden we can now operate with a more flexible encoding machinery that can be used immediately to encode relational representations of the domain of interest.
       </paragraph>
      </section>
      <section label="3.1.3">
       <section-title>
        Internal cores and interfaces
       </section-title>
       <paragraph>
        We define two key notions: internal core graphs and interface graphs. An internal core graph (or core{sup:2} for simplicity), denoted {a mathematical formula}CRv(G), is a neighborhood graph of G of radius R rooted in v. An interface graph, denoted {a mathematical formula}IR,Tv(G) is the difference graph of two neighborhood graphs of G with the same root in v and radius {a mathematical formula}R−1 and {a mathematical formula}R+T. The difference graph is the graph induced by the set difference of the two vertex sets. Intuitively an interface corresponds to the “external shell” of a core with thickness T. This shell represents the context available for the definition of a production rule, as will be detailed shortly. Given a set of graphs {a mathematical formula}G, a graph {a mathematical formula}CRv(G) is said to be a corresponding core of an interface {a mathematical formula}IR,Tv(G) when the graph induced by the union graph of {a mathematical formula}CRv(G) and {a mathematical formula}IR,Tv(G) is a subgraph of at least one {a mathematical formula}G∈G. The union graph is the graph induced by the union of the vertex sets. Intuitively this means that a neighborhood graph of radius {a mathematical formula}R+T can be decomposed in an inner neighborhood graph of radius R and an overlapping shell of thickness T (see Fig. 1). Cores with identical corresponding interfaces (up to isomorphism) are said to be congruent in {a mathematical formula}G.
       </paragraph>
       <paragraph>
        Although we can in principle assign any non-negative integer value to the parameters R and T, in this work we choose {a mathematical formula}R∈{0∪2n+1|n∈N}, i.e. odd integers and zero for the radius, and {a mathematical formula}T∈{2n|n∈N}, i.e. even integers for the thickness. This choice guarantees that the intersection between interfaces and corresponding cores is always a subset of hyperedges, or, in case of graphs, of edges (see Fig. 1).
       </paragraph>
      </section>
     </section>
     <section label="3.2">
      <section-title>
       Locally substitutable graph grammar
      </section-title>
      <paragraph>
       A graph grammar [38] is a finite set of productions; a production is a triple {a mathematical formula}S=(M,D,E) where M is the “mother” graph, D is the “daughter” graph and E is an embedding mechanism. Given a “host” graph H, a production is applied as follows: 1) identify one occurrence of M as an isomorphic subgraph of H; 2) remove M from H and obtain {a mathematical formula}H−; 3) replace M by an isomorphic copy of D; and finally 4) use the embedding mechanism E to attach D to {a mathematical formula}H−.
      </paragraph>
      <paragraph>
       In our proposal the general concepts of mother and daughter graphs are specialized as follows: given a production S, the mother graph M and the daughter graph D are union graphs of an interface graph and a correspondent internal core graph with the additional constraint that the interface graphs in the daughter and mother graph must be identical (up to isomorphism); finally, the embedding mechanism E is given by the isomorphic bijection between the interface graphs. In words, we produce new elements by swapping congruent inner cores (see Fig. 2). In the graph grammar literature this type of attachment is called gluing and the resulting grammar is said to belong to the algebraic approach [38]. The embedding mechanism is generally the critical element in any graph grammar. The reason is that in practice it is hard to achieve at the same time expressiveness, determinism and efficiency; simple and efficient approaches are generally not expressive, while expressive mechanisms tend to be computationally complex and simple systems cannot adequately control the combinatorial non-determinism. In the proposed approach the isomorphic mapping of interface graphs allows us to trade off non-determinism with expressivity as increasing the size of the (labeled) interface graph decreases the chance of multiple isomorphic mappings. Finally we note that as the radius and thickness decrease we approach single vertex edit operations thus improving the Markov process ergodicity.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Locally substitutable graph grammar inference
      </section-title>
      <paragraph>
       One of the advantages of the proposed grammar is the efficiency of the induction phase. The key factor to ensure efficiency is the specialization of the graph types used in production rules. In our case we use only neighborhood graphs, which can be efficiently enumerated using multiple breadth-first visits.
      </paragraph>
      <paragraph>
       In Algorithm 1 we report the pseudo-code for the grammar inference, i.e. the operation of deriving the rule set from a finite collection of examples. The induction procedure is parametrized by two values: the desired maximal radius {a mathematical formula}Rˆ and maximal thickness {a mathematical formula}Tˆ. The procedure starts from each graph in the data set (line 3), considers all neighborhood graphs with roots in all vertices (line 4); extracts all possible inner core and interface graphs (the two loops at lines 5 and 8), computes the pseudo-identifier code for each core and interface graph (lines 6 and 9) and finally stores them in a hash map (line 11) that associates cores to the corresponding interface.
      </paragraph>
      <paragraph>
       Complexity: The inference procedure complexity is determined by two key procedures: 1) the core/interface extraction (lines 6 and 9 in Algorithm 1) and 2) the pseudo-identifier computation (line 10 in Algorithm 1). Enumerating cores and interfaces is achieved running a breadth-first visit on all vertices of all the N graphs in the data set for all possible radius × thickness pairs, i.e. {a mathematical formula}O(RˆTˆN|V||E|) in the worst case scenario. Note that in case of sparse graphs, e.g. for molecular graphs, (i.e. {a mathematical formula}|E|≈k|V| with small constant k) and of small values for {a mathematical formula}Rˆ and {a mathematical formula}Tˆ, the algorithm runs with a complexity that is essentially linear in the cumulative data set vertex size. The pseudo-identifier computation complexity is analyzed in more detail in the following paragraph.
      </paragraph>
     </section>
     <section label="3.4">
      <section-title>
       Matching interface graphs with pseudo-identifiers
      </section-title>
      <paragraph>
       The pairs of corresponding cores-interfaces need to be stored using a data structure that can be efficiently queried to retrieve all congruent cores given the desired interface. This can be achieved solving the graph isomorphism problem (GIP) between interface graphs in an efficient way. Unfortunately GIP is a problem for which it is not known whether polynomial algorithms exist. Algorithms for general graphs that are in the worst case exponential but that are fast in practice do exist [31], [49]. For special graph classes, such as bounded degree graphs [29], there exist polynomial time algorithms. In our case we want to minimize the runtime complexity as we expect the number of productions to be very large in all non-trivial cases. For this we employ an approximate solution similar in spirit to Sorlin and Solnon [42]. The key idea here is to devise a function to compute an integer pseudo-identifier for each graph such that isomorphic graphs are guaranteed to be associated to the same identifier. The procedure works in two steps: 1) a graph invariant encoding {a mathematical formula}Lg(G) is constructed; and 2) a hash function {a mathematical formula}H(Lg(G))→N is used over the encoding to obtain an integer. The isomorphism test is then replaced by the identity test between integers. Note however that we cannot hope to exhibit a generic and efficient certificate for isomorphism in this way. In particular we cannot exclude that there will be collisions between two non-isomorphic graphs, either because the same encoding is assigned to non-isomorphic graphs or because of the non-perfect hashing, which can assign the same encoding to different graph invariant encodings.
      </paragraph>
      <paragraph>
       The procedure to build the graph invariant encoding {a mathematical formula}Lg(G) uses two label functions: one for the vertices and one for the edges, denoted {a mathematical formula}Lv and {a mathematical formula}Le respectively; {a mathematical formula}Lv(v) assigns to vertex v a lexicographically sorted sequence of pairs each composed of a topological distance and a vertex label, i.e. {a mathematical formula}Lv(v) returns a sorted list of pairs {a mathematical formula}〈D(v,u),ℓ(u)〉 for all {a mathematical formula}u∈G. The new edge label is produced by composing the new vertex labels with the original edge label, that is {a mathematical formula}Le(uv) assigns to edge uv the triplet {a mathematical formula}〈Lv(u),Lv(v),ℓ(uv)〉. Finally {a mathematical formula}Lg(G) assigns to the graph G the lexicographically sorted list of {a mathematical formula}Le(uv) for all {a mathematical formula}uv∈E(G). In words: we relabel each vertex with a sequence that encodes the vertex distance from all other (labeled) vertices; the graph encoding is obtained as the sorted edge list, where each edge is annotated with the new labels of its endpoints. We finally resort to a Merkle–Damgård type of hash [10] that accepts input sequences of variable length.
      </paragraph>
      <paragraph>
       Complexity: The computation of the pseudo-identifier is achieved running a depth breadth-first visit on all vertices to obtain the pairwise vertex distances. Subsequent operations involve sorting the codes for lists of size {a mathematical formula}|V(n)| and {a mathematical formula}|E(n)| with n indicating the neighborhood graph. In the worst case scenario this yields a complexity of {a mathematical formula}O(N|V||E|log⁡|E|) where N is the number of graphs in the dataset and {a mathematical formula}|V| ({a mathematical formula}|E|) is the average vertex (edge) set size respectively. Note that in practice pseudo-codes are computed for neighborhood graphs with small radius {a mathematical formula}R&lt;5 and hence the code computation is mostly obtained in (small) constant time.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Probability estimate for structured data
     </section-title>
     <paragraph>
      The acceptance distribution {a mathematical formula}A(x→x′) requires the computation of {a mathematical formula}P(x), a probability distribution defined over structured data that needs to be estimated from data. More precisely, we are often interested in modeling the joint probability {a mathematical formula}P(x,C) where C is the “concept”, “class” or “target” that we are modeling.
     </paragraph>
     <paragraph>
      In order to manipulate structured data one common strategy is to develop suitably kernelized algorithms that can effectively exploit the “kernel trick” [32], [1], [5]. The idea is to implicitly (or explicitly as we shall see) map the input instances in a vector space, generally of high dimension (possibly infinite). From this moment one can operate in a vector field with standard algebraic tools rather than on the original representation. In this way it becomes possible to manipulate complex data structures such as sequences, trees or graphs.
     </paragraph>
     <paragraph>
      Unfortunately modeling the joint probability of the mapped data requires probability density estimation in high dimensions, a difficult task that is particularly affected by the curse of dimensionality[2]. There exist two main approaches for density estimation: parametric and the non-parametric. Parametric methods are generally mixture models, with a fixed number of components each belonging to a parametric function family. Their main drawback is that unless the modeling is correct the estimate can be quite poor. Since in our case we expect to work in a general setting, we cannot assume exact prior knowledge on the type of density functions involved and we cannot therefore rely on this approach.
     </paragraph>
     <paragraph>
      Non-parametric density estimation methods, such as the kernel estimator [36], or local likelihood [28], work well for low-dimensional problems, but are not applicable in high-dimensional settings due to 1) intractable computational costs and 2) slow rates of convergence of the estimators. The first problem is due to the necessity of bandwidth selection on each dimension using expensive cross validation approaches. An efficient approach is the sparse nonparametric density estimation rodeo by Liu et al. [27], which has a provably fast convergence but only under some restrictive modeling assumptions, such as the presence of a fixed small number of relevant features and works best up to dimensionality of the order of 10{sup:3}. Unfortunately when working with graphs or hypergraphs we typically need to work in mapped spaces with dimensionality of the order of 10{sup:5} or greater to achieve a sufficient discriminative power.
     </paragraph>
     <paragraph>
      We address the problem of efficiently estimating densities in high-dimensional cases solving a related proximal problem, that of identifying simple high dimensional regions with high support. This is the approach taken by Schölkopf et al. [40] with their one class problem formulation. There the authors find a “simple and small” region of the feature space such that the probability of a test point to lie outside the region is bounded by some a priori specified small value. In this formulation one finds high density regions employing the same techniques developed for inducing standard Support Vector Machines for binary classification problems. In Tax and Duin [45] an equivalent formulation is presented where one computes the smallest sphere in feature space enclosing the image of the input data.
     </paragraph>
     <section label="4.1">
      <section-title>
       An efficient graph kernel
      </section-title>
      <paragraph>
       To work efficiently on graphs we make use of the graph kernel developed in Costa and De Grave [8], called Neighborhood Subgraph Pairwise Distance Kernel (NSPDK), whose bias perfectly matches the elements in our proposed graph grammar. Here for completeness we report the definitions used in Costa and De Grave [8]. The idea in NSPDK is to decompose a graph into small neighborhood subgraphs of increasing radii {a mathematical formula}r&lt;rmax. Then, all pairs of such subgraphs whose roots are at a distance not greater than {a mathematical formula}d&lt;dmax are considered as individual features (see Fig. 3). The similarity notion is finally given as the fraction of features in common between two graphs.
      </paragraph>
      <paragraph>
       More formally, NSPDK is an instance of a decomposition kernel, where the “parts” in which the graph is decomposed are pairs of small near subgraphs (for more details on decomposition kernels see Haussler [15]). The relation is defined in terms of neighborhood subgraphs (see Section 3.1.2) as:{a mathematical formula} that is, a relation {a mathematical formula}Rr,d that identifies pairs of neighborhoods of radius r whose roots are exactly at distance d. Finally:{a mathematical formula} where {a mathematical formula}Rr,d−1(G) indicates the multi-set of all pairs of neighborhoods of radius r with roots at distance d that exist in G, and where 1 denotes the indicator function and ≅ the isomorphism between graphs.
      </paragraph>
      <paragraph>
       To increase efficiency and generalization power, we consider the zero-extension of K obtained by imposing an upper bound on the radius and the distance parameter:{a mathematical formula} We employ a normalized version of {a mathematical formula}κr,d, that is:{a mathematical formula} this ensures that the relations induced by all values of radius and distance are equally weighted regardless of the size of the induced part sets, and has the effect of mitigating issues associated to the “diagonal dominance” effect [18]. The diagonal dominance effect is the tendency of generating kernel matrices that are close to the identity matrix; in these cases all instances appear orthogonal to each other in the feature space, and the generalization capacity of any downstream learning algorithm is negatively impacted.
      </paragraph>
      <paragraph>
       In addition to the radius-distance-pair normalization, we perform a global normalization step:{a mathematical formula} in this way we avoid {a mathematical formula}Krmax,dmax(G,G′)≥Krmax,dmax(G,G) when {a mathematical formula}G⊂G′, that is, having a kernel that scores the similarity between two graphs G and {a mathematical formula}G′ higher than the similarity of G with itself when G is a subgraph of {a mathematical formula}G′ since this is in most of the cases not desirable.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Explicit feature encoding
      </section-title>
      <paragraph>
       The NSPDK relies on the efficient matching of neighborhood subgraphs. This is achieved using a strategy similar to that used for the graph pseudo-identifier technique described in 3.4. Here we convert the features to pseudo-identifiers that can then be directly used to build explicit sparse vector representations. In this way we transform the graph kernel into an explicit non-parametric feature constructor. With non-parametric we mean that the number of features is not predefined and it is a function of the dataset size; crucially, as the dataset grows in size, so does the number of features and hence the related discriminative capacity of the representation.
      </paragraph>
      <paragraph>
       In order to encode the specific NSPDK features, that is, pairs of rooted neighborhood graphs whose roots are at a given distance, we build triplets of the form {a mathematical formula}&lt;d,H(Lg(A)),H(Lg(B))&gt;, where d is the distance between roots of the neighborhood graphs A and B (the canonical order between A and B is determined by the numerical order of the corresponding hash pseudo-identifiers). Each explicit feature identifier is then computed as the hash of the corresponding triplet.
      </paragraph>
      <paragraph>
       Note that it is easy to control the size of the resulting feature space considering only the b least significant bits of the identifier, in a similar fashion as in the b-bit min-wise hashing [24]. Typical values for b that achieve good discriminative power, in the case of molecular graphs for example, range in the interval 14–20, yielding explicit feature spaces with dimensionality ranging in the interval 10{sup:4}–10{sup:6}.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       One class SVM for NSPDK
      </section-title>
      <paragraph>
       The one class problem formulation given in Schölkopf et al. [40] considers the following setting: we are given a data set of instances belonging to one class {a mathematical formula}x1,x2,…,xn in some set {a mathematical formula}X; with Φ a feature map {a mathematical formula}X→Rm such that the dot product in the image of Φ can be determined by evaluating some easy to compute kernel function[47]{a mathematical formula}k(xi,xj)=(Φ(xi),Φ(xj)). The objective is to build a function that returns +1 in a simple and small region of the mapped space that, at the same time, contains most of the data points, and that returns −1 everywhere else. The key insight in Schölkopf et al. [40] is that this can be obtained by identifying the hyperplane that separates most of the data from the origin with maximum margin.
      </paragraph>
      <paragraph>
       To separate the data set from the origin, they solve the quadratic program:{a mathematical formula} where ν controls the number of outliers, i.e. the fraction of instances that can be left out of the characteristic region, and ρ is the offset that controls the size of the region. The solution of the dual problem is then:{a mathematical formula} where the patterns {a mathematical formula}xi with non-zero Lagrange multipliers {a mathematical formula}αi are called support vectors.
      </paragraph>
      <paragraph>
       In our setting, however, we have access to the explicit feature map Φ and we can operate directly on the vector representation {a mathematical formula}Φ(x)=z∈Rm. Moreover since we consider normalized kernels, the region we seek becomes a hyperspherical cap (i.e. the portion of a hypersphere cut off by a hyperplane). This allows us to re-formulate the one class SVM problem in a straightforward way; we build the set {a mathematical formula}{(zi,yi)}i=1n with {a mathematical formula}yi∈{−1,1} by considering the original one class set {a mathematical formula}Z+={(z1,1),(z2,1),…,(zn,1)} together with the set of virtual negative instances {a mathematical formula}Z−={(−z1,−1),(−z2,−1),…,(−zn,−1)}, and we consider the resulting standard SVM binary classification problem:{a mathematical formula}
      </paragraph>
      <paragraph>
       This linear formulation in the primal allows us to use fast stochastic gradient descent optimization techniques [6] to efficiently solve the SVM problem in the resulting large scale setting (i.e. large n and m). Here the idea is to look for a parameter assignment for the linear model {a mathematical formula}f(z)=wTz that minimizes the empirical risk {a mathematical formula}En(f)=1n∑i=1nℓ(f(zi),yi) for a loss function {a mathematical formula}ℓ(yˆ,y) that equates the regularized SVM loss: {a mathematical formula}ℓ(f(zi),yi)=max{0,1−yiwTzi}+λwTw. The stochastic gradient descent iterative technique finds the optimal assignment {a mathematical formula}wˆ using the following updates: {a mathematical formula}wt+1=wt−γ∇θℓ(f(zi),yi) that simplifies the ordinary gradient descent algorithm and estimates the gradient direction on a per instance basis. Taking the derivatives gives us:{a mathematical formula} Decreasing the gain γ with a rate of {a mathematical formula}≈1t one can ensure a fast convergence [6] (i.e. in all practical cases 5–10 iterations are enough).
      </paragraph>
      <paragraph>
       The solution of the one class problem defines a region of the feature space where the probability distribution that has generated the data has high support. We can therefore use the induced model to find whether a test point {a mathematical formula}x′ has a probability higher than a given threshold, but not to find its actual probability.
      </paragraph>
      <paragraph>
       To improve the probability estimate approximation we adopt the “calibration” approach introduced by Platt et al. [37]. A well calibrated (binary) classifier should classify the instances in such a way that approximately only {a mathematical formula}x% of those that receive a score of {a mathematical formula}x/100 actually belong to the positive class. We solve the one-class problem for a user defined threshold ν to specify the fraction of instances to consider as outliers. We then calibrate the linear estimator considering the ν fraction of instances as representatives of the negative class. In this way we obtain an improved probability estimator while at the same time preserving an efficient computation.
      </paragraph>
      <paragraph>
       To further improve the probability estimate approximation one can consider the recent work in non-linear kernelized quantile estimation techniques [35]. In the current paper however, we have preferred computational efficiency over estimate accuracy, since the sampling strategy requires the fast induction of grammars and probability estimators from potentially large datasets.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      An efficient generator for (hyper)graphs
     </section-title>
     <paragraph>
      In Algorithm 2 we report the pseudo-code for the overall generative procedure. Given a set of hypergraphs we extract a Local Substitutable Graph Grammar via Algorithm 1 (line 4) and we learn the associated approximate probability function via the one class SVM procedure detailed in Section 4.3 (line 6). The generative procedure takes in input a set of seed graphs and a number of iterations. Starting from each graph in the seed set (line 8), we apply the Metropolis–Hastings algorithm where the proposal distribution {a mathematical formula}g(x→x′) is defined using the substitutability principle, i.e. by swapping congruent cores (line 20) sampling uniformly at random from all possible root nodes, all legal values for the parameters radius and thickness. Note that the graph grammar implicitly defines a proposal distribution with probabilities distributed uniformly on the graphs that can be generated applying all the possible rules.
     </paragraph>
     <paragraph>
      The acceptance distribution {a mathematical formula}A(x→x′) is defined on the basis of the one class SVM model (line 21). Accepted graphs are then emitted according to the MH policy, which includes notions such as the number of initial sample to “burnin”, and the step to use to guarantee that the samples are independent from each other.
     </paragraph>
     <paragraph>
      The Metropolis–Hastings procedure considers a sample graph G and the new proposal {a mathematical formula}G′ obtained by the proposal distribution {a mathematical formula}g(G→G′). Note that the difficult part here is building {a mathematical formula}G′ in the first place rather than computing the conditional probability. Next, we evaluate whether to accept {a mathematical formula}G′ and we do this using the acceptance distribution {a mathematical formula}A(G→G′):{a mathematical formula} Since our proposal distribution is in first approximation symmetric we have{a mathematical formula} and we can therefore simplify the acceptance rule as:{a mathematical formula} that in our proposal is reduced to assessing whether the proposed {a mathematical formula}G′ has a greater distance than G from the discriminative hyperplane induced by the linear one class SVM.
     </paragraph>
     <paragraph>
      To convince oneself that the substitutable graph grammar yields a proposal distribution that is in first approximation consider how the inner core replacement procedure works: starting from a graph G we select an inner core graph C and an interface graph I, we then identify all core/interface pairs where the interface matches I up to isomorphism and uniformly at random choose one core {a mathematical formula}C′ for the replacement to yield graph {a mathematical formula}G′. If we now start from graph {a mathematical formula}G′ and select the inner core graph {a mathematical formula}C′ and interface I we can see that we can choose C with the same uniform probability (the core/interface pairs set is the same) yielding G.
     </paragraph>
     <paragraph>
      Note that the exact proposal distribution computation should take into account the fact that different cores and interfaces are available after the substitution could alter the probability of the reverse transition. As an illustrative example consider the substitution of a small core with a large core; in this case a greater number of nodes exist after the substitution and hence the exact inverse transition {a mathematical formula}g(G′→G) is less probable since there exist more roots to sample from for the core replacement. This effect can be ignored in first approximation when the cores are comparable in size (which is the case in our empirical investigations).
     </paragraph>
     <paragraph>
      Complexity: The main contributions to the complexity of Algorithm 2 are given by 1) the induction of the grammar (line 4), 2) the fitting of the one class SVM estimator (line 5) and 3) the substitution step were an inner core/interface is extracted (line 15) and replaced (line 20). The complexity for 1) has been analyzed in Sections 3.3 and 3.4. The complexity of a linear one class SVM implemented via a stochastic gradient descent technique is linear in the size of the dataset and the size of the nonzero elements of the explicit feature representation, which for NSPDK is linear with the size of the graph vertex set. The complexity of the substitution step depends on the complexity of the construction of the pseudoidentifier codes and on the search of a congruent core in the set of productions M. In detail, we assume that the M is implemented using hash tables over the pseudoidentifier codes, offering therefore an access in {a mathematical formula}O(1). The substitution step (line 20) requires the computation of the isomorphism mapping between interface graphs. While this step is in principle computationally expensive, we resort to the technique introduced in Section 3.4 and use {a mathematical formula}H(Lv) as a vertex pseudoidentifier. When multiple matches are possible we randomly pick one.{sup:3} This allows us to compute the isomorphism mapping between interface graphs with linear complexity w.r.t. the interface graph vertex set size.
     </paragraph>
     <paragraph>
      Finally the overall complexity depends on the rejection ratio, i.e. the fraction of times that line 21 is evaluated to False. In order to guarantee the termination of the algorithm we fix a user defined maximum number of attempts after which the loop in line 10 will be interrupted.
     </paragraph>
     <section label="5.1">
      <section-title>
       Metropolis–Hastings sampling vs. graph grammars
      </section-title>
      <paragraph>
       Note that the induction of a graph grammar alone is not likely to be enough to solve a CLP by itself. There are two main reasons for this. The size of the grammar, i.e. the number of production rules, is large in practice (although linear w.r.t. the initial data set). This implies that the number of produced novel graphs grows exponentially with a very large multiplicative factor.{sup:4} In order to control the combinatorial explosion we need to employ some ranking notion to score the alternatives and select only the most promising ones at each step. In practice one can hope to induce only context-free or locally context-sensitive grammars, since the amount of needed training material grows rapidly with the complexity of the grammar, which in turn grows with the size of the context considered. For this reason global or just long range constraints cannot be modeled by the grammar and the resulting language would therefore inevitably contain non-viable instances. Note that the regularized statistical model can only score instances once these are available and cannot be used alone in a generative mode (i.e. it cannot be used to actually construct the graphs).
      </paragraph>
      <paragraph>
       In our approach we employ two distinct mechanisms to deal separately with local and global constraints: we use a locally context-sensitive graph grammar for the local constraints and a regularized statistical model for the global or long range constraints. The two approaches complement each other: the grammar is a flexible non-parametric approach that can model complex dependencies between vertices that are within a short path distance from each other; the statistical model, instead, can employ the bias derived from the particular functional family (linear) and the type of regularization used (a penalty over the {a mathematical formula}L2 norm of the parameters) to generalize long range dependencies to cases that are similar but not identical to those observed in the training set. We believe that in many cases of practical relevance, natural dependencies are likely to exhibit this type of behavior, i.e. with local constraints that are more complex than long range ones.
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Empirical results
     </section-title>
     <section label="6.1">
      <section-title>
       The validation problem
      </section-title>
      <paragraph>
       A crucial issue in constructive learning problems is the assessment of the quality of the constructed structures. While in classification problems it is easy to devise metrics to compare the predicted class to the available true class, in the case of constructed instances resorting to an oracle can be difficult or at times impossible.{sup:5} The reason is that while in classification problems we can pretend to ignore the target on a subset of instances, in a constructive problem we really have no access to the true class information since these instances do not exist ahead of their instantiation by the generator. One strategy would be to count the samples that are identical to a held-out subset. However, in most cases of interest, the size of the sample space makes the chance of obtaining identical matches vanishing small. Intuitively, one would be satisfied when obtaining samples that are sufficiently similar to bona-fide cases. We therefore propose to cast the problem as a discriminative tasks between the generated and the original instances. The key idea is that when the generated sample is representative of the underlying true distribution then this becomes indistinguishable from the original empirical sample. We therefore train a discriminative estimator to distinguish the two sets and measure its failure rate. The validation of the samples quality should however also take into account the problem of redundancy, i.e. it should not give a high score to sets made of (nearly) identical instances. To this end we propose to additionally analyze the learning curve of a predictor that is trained using the generated samples together with the original data. We then evaluate the predictive performance on a held-out test set and measure if there is a reduction in the classification error rate. A significant improvement indicates then that the elements in the sample are effectively different from the original material.
      </paragraph>
      <paragraph>
       In the remainder we make use of the area under the curve of the receiver operating characteristic (AUC ROC) [43], which is the area under the curve obtained by expressing the true positive rate (i.e. the proportion of positives that are correctly identified) with respect to the false positive rate (i.e. the proportion of negatives that are correctly identified) at various threshold settings.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Carcinogenicity assay: the NCI60 dataset
      </section-title>
      <paragraph>
       In the following experimental setup we want to address two fundamental questions: Q1) are the samples coming from the same distribution as the original data? and Q2) how redundant are the samples w.r.t. the original instances? We gain insights into these problems by studying the behavior of NSPDK SVM predictors when trained in specific settings. For each predictor we analyze the corresponding learning curve, i.e. the curve obtained by plotting the estimated predictive performance for increasing sizes of the training data.
      </paragraph>
      <paragraph>
       The parameters of the experiment are: 3 repetitions for each experiment, capped dataset size to 400 molecular graphs, of which 70% is used for training and 30% for testing, inner core radius set to 0 (i.e. replacement of a single atom), interface thickness set to 2 and 4, 5 substitution steps and two sample per seed.{sup:6}
      </paragraph>
     </section>
     <section label="6.3">
      <section-title>
       Generator optimization
      </section-title>
      <paragraph>
       The parameter ranges that have been used are: for the graph grammar{sup:7}: inner core radius {a mathematical formula}∈{1,3,5}, interface thickness {a mathematical formula}∈{2,4,6,8}; for the graph kernel: radius {a mathematical formula}∈{0,…,3}, distance {a mathematical formula}∈{0,…,5}; for the MH: burnin {a mathematical formula}∈{0,…,5}, number of iterations {a mathematical formula}∈{1,…,10}, step {a mathematical formula}∈{2,…,5}; for the one-class SVM: {a mathematical formula}ν∈{0.1,…,0.4} as a proportion of the dataset size.
      </paragraph>
      <paragraph>
       Note that the trade-off parameter λ in Equation (9), which controls the degree of “diversity” of the generated sample, is modeled via the combination of the max radius and the minimal thickness values. To understand the relationship between λ and the radius and thickness, consider that when only small inner cores with large interfaces are allowed, the generator is very “conservative” in the substitutions that are proposed and only small variations are explored (this corresponds to high values for λ). When only large inner cores with small interfaces are allowed, the generator is very “creative” and large substitutions are proposed (this corresponds to small values for λ).
      </paragraph>
     </section>
     <section label="6.4">
      <section-title>
       Case study: de novo molecular synthesis
      </section-title>
      <paragraph>
       In order to study the properties of a structural constructive system, we need a domain where structures arise naturally. One such case is the chemical domain, where small molecules can be encoded as undirected graphs: atoms are represented as labeled vertices and the strong covalent or aromatic bonds are represented as labeled edges. Typical values for molecular graphs are 5–30 vertices (and edges) with an alphabet size of 1–10 distinct vertex symbols. The CLP here is called “de novo synthesis” of chemical compounds. In the chemoinformatics literature this type of problem is well known and well studied, with techniques ranging from exhaustive product enumeration and deterministic combinatorial approaches, to stochastic sampling by evolutionary algorithms and particle swarms, simulated annealing, and Markov chains (see Hartenfeller and Schneider [14] for a review). Our approach can be viewed as belonging to the stochastic sampling type. These methods have essentially two ways to proceed, related to the type of “building blocks” used: 1) the atom-based or 2) the fragment-based approach; i.e. the stochastic moves allowed are the insertion/deletion/replacement of a single atom (vertex) or of an entire fragment (subgraph).
      </paragraph>
      <paragraph>
       The atom-based approach has the drawback of using very small moves in the chemical configuration space; the search space is not constrained and, as a consequence, an exponential number of implausible molecular graphs need to be considered and then discarded. Another drawback is that the preference scoring function is likely to exhibit plateaus which do not offer reliable clues on the direction for improving molecular configurations.
      </paragraph>
      <paragraph>
       The fragment-based approach allows the use of fragments derived from chemical knowledge (for example residues that can easily be added by known chemical reactions). However, due to the current limited knowledge of the chemical space, these methods are generally over-constrained and do not facilitate the exploration/construction of substantially novel and interesting compounds.
      </paragraph>
      <paragraph>
       Our approach lies somewhat in between these two cases: it can perform very large steps in the search space (like fragment based methods); however, in our case, the rules are induced directly from a dataset and do not rely on expert knowledge elicitation. Our approach is non-parametric, as the number of rules grows with the size of the dataset used to derive them. This allows us to reach a substantially larger portion of the chemical space, not unlike the atom based approach.
      </paragraph>
     </section>
     <section label="6.5">
      <section-title>
       Datasets
      </section-title>
      <paragraph>
       We consider toxicity and cancer inhibition assays of medium size (thousands of molecules).
      </paragraph>
      <paragraph>
       NCI-60:
      </paragraph>
      <paragraph>
       Several tens of thousands of chemical compounds have been tested by different laboratories for evidence of the ability to inhibit the growth of human tumor cell lines.{sup:8} The experimental results have been collected and made available by the Developmental Therapeutics Program (DTP) at the U.S. National Cancer Institute (NCI). In Swamidass et al. [44] a selection procedure was employed to balance the various datasets and it has now become a popular benchmark for QSAR algorithm research, often referred to as either NCI-60 or just NCI. The data set contains growth inhibition measurements on 60 cell lines. Each cell line has inhibition data on about 3500 compounds. Note however that across the different datasets there is a considerable overlap so that the number of unique compounds is only 3910 in total.
      </paragraph>
      <paragraph>
       Bursi:
      </paragraph>
      <paragraph>
       Mutagenicity or toxicity is one of the main properties that a potentially marketable compound needs to avoid. Some toxic properties can be associated to specific chemical substructures that are called toxicophores. While a number of toxicophores have already been identified in the literature, knowledge about what makes a compound toxic is still far from being complete. For this reason, automated approaches that can work starting from a set of experimentally verified toxic compounds without any prior domain knowledge are of interest.
      </paragraph>
      <paragraph>
       In Kazius et al. [19] a dataset of 4337 molecular structures is constructed. The dataset is available with corresponding Ames target data,{sup:9} i.e. each molecule has been tested in a short-term in vitro assay designed to detect genetic damage caused by chemicals and has become the standard test to determine mutagenicity. The class distribution is balanced with 2401 mutagens and 1936 nonmutagens (further details are available at: http://ftp.ics.uci.edu/pub/baldig/learning/Bursi/source/).
      </paragraph>
      <section label="6.5.1">
       <section-title>
        Discriminability analysis
       </section-title>
       <paragraph>
        We consider the following experimental scenario: for each classification problem in NCI60 we annotate as positive class the active compounds and as negative class the samples produced by the proposed approach. We report the AUC ROC estimate at various sizes for the training set. Note that a ROC value of 0.5 is indicative of a random classification performance while a lower value is indicative of a “paradox problem”. A paradox problem happens when a predictor considers instances that are labeled differently in the training and in the test set, as similar. We report one representative experimental run (for the set of all results refer to the IPython Notebook) in Fig. 4. The results indicate that for small training set sizes we are in the paradox state; as the training set size increases the ROC value increases but tends to saturate around the value of 0.6. Such a low ROC value suggests that the classifier cannot reliably discriminate the original data from the sample instances.
       </paragraph>
       <paragraph>
        Note that this experiment does not rule out the possibility that the classifier is simply too weak to discriminate molecular graphs of any kind. To refute this hypothesis we evaluate the classifier discriminative power when distinguishing active from inactive molecules in various bioassays. The results reported (see Section 6.5.2) support a positive answer to Q1: they suggest that a linear classifier based on the NSPDK has indeed a good discriminative power and that the samples are indistinguishable in a meaningful sense.
       </paragraph>
      </section>
      <section label="6.5.2">
       <section-title>
        Learning curve analysis
       </section-title>
       <paragraph>
        In the following experiment we consider the following three training scenarios, using: 1) original data, 2) sampled data, and 3) original + sampled data. If we compare the curve obtained from the original data to the one obtained from the sampled data, we can answer positively to question Q1 when the two estimators behave in a comparable way on a held out set. Finally, we can answer Q2 by comparing the curve from original data to the one obtained from the original in addition to the sampled data: if the latter curve is dominating the former we can conclude that the samples are not redundant and can provide useful information to the estimator. To quantify the answer to Q2 we introduce a non-redundancy score defined as the log of ratio of the area under the learning curves; positive (negative) values indicate that the learning curve obtained using the additional sampled instances is dominating (is dominated by) the original curve respectively. In Table 2 we report the empirical evaluation of the non-redundancy score over 47 NCI datasets. In Fig. 7 we show a few learning curves at various score values.
       </paragraph>
       <paragraph>
        Note that, as reported in Costa and De Grave [8], discriminative classifiers built on top of the NSPDK achieve state-of-the-art results, therefore any improvement in these classifiers is an interesting result in itself. The empirical results show that in 10/47 cases we obtain positive scores and that (in most cases) adding the samples to the training set has a significant positive impact when the data set is small.
       </paragraph>
      </section>
      <section label="6.5.3">
       <section-title>
        Likelihood analysis
       </section-title>
       <paragraph>
        To gain further insights we compute the average likelihood of instances in a held-out set using models trained in different scenarios. As a reference we consider a one-class NSPDK SVM model fit and calibrated over sub sets of the original data of increasing size. We then compare it to a model fit and calibrated over the samples generated starting from those sets (i.e. both the grammar and the scoring function are induced only from the given sets). Here we consider the instances generated by 4 substitution iterations in addition to the original seeding instance. In Fig. 5 we report two representative cases that illustrate the general behavior: the likelihood improvement is evident in small datasets, the difference however tends to vanish as the estimators are fit over a larger datasets. We conclude that the additional samples allow the classifier to increase its confidence when evaluating positive test instances, but that the effect tends to saturate, most likely due to the limits in accuracy that the classifier can achieve.
       </paragraph>
      </section>
     </section>
     <section label="6.6">
      <section-title>
       Toxicity assay: the Bursi dataset
      </section-title>
      <paragraph>
       In Fig. 6 we report the learning curve analysis for the Bursi dataset. In this case we see that the toxicity prediction is consistently enhanced by the presence of the samples obtained by the proposed procedure.
      </paragraph>
      <section label="6.6.1">
       <section-title>
        Grammar analysis
       </section-title>
       <paragraph>
        We analyze the efficiency of the sampling procedure. On a Xeon 5160, 3.0 GHz, the wall-clock runtime for model induction is linear with coefficient 0.7 sec per graph, while the runtime for production phase is linear with coefficient 20 graphs per second.
       </paragraph>
       <paragraph>
        The production set size grows linearly, with coefficient number of {a mathematical formula}interfaces≈2.5×number of instances, number of {a mathematical formula}cores≈4.5×number of instances, number of interface-cores {a mathematical formula}pairs≈9×number of instances, confirming the insight that the locally substitutable graph grammar is non-parametric and grows its complexity with the amount of available data. In Table 1 we report a break down of the grammar size for various values of radius and thickness as the number of instances for its induction is increased.
       </paragraph>
       <paragraph>
        Considering the number of cores that are associated to each interface and the dual notion, the number of interfaces that are associated to each core, we observe (data not reported) a linear dependency in log–log scale, a property that indicates the presence of a significant structure in the grammar definition.
       </paragraph>
      </section>
      <section label="6.6.2">
       <section-title>
        Reconstruction analysis
       </section-title>
       <paragraph>
        We evaluated a simple but stringent notion of performance quality: we computed the fraction of instances in the held out test set that are identically reconstructed starting from the seed set. When {a mathematical formula}R=5,T=8, the sampling procedure generates 26 graphs that have an identical counterpart in the original test set of 700 molecules, achieving therefore a remarkable 4% reconstruction rate. A simple approximate calculation shows that a purely random process{sup:10} would generate a duplicate with probability {a mathematical formula}≈2.8⋅10−56.
       </paragraph>
      </section>
      <section label="6.6.3">
       <section-title>
        Domain specific semantic analysis
       </section-title>
       <paragraph>
        As an additional sanity check we manually inspect the produced molecular graphs and analyze them w.r.t. the chemical knowledge available on toxicity. In Kazius et al. [19] the authors identify structural moieties as toxicophores defined as substructures that can potentially cause mutagenicity, i.e. reactive substructures or substructures that are prone to either metabolic activation or intercalation. Finally they compile a list of eight different substructures (see Fig. 8) that can detect 75% of all mutagens in the dataset. When manually inspecting the samples we observe that: 1) the constructed structures are plausible molecular graphs, i.e. valence constraints are respected, and 2) each molecule includes one or more of the toxicophores identified in Kazius et al. [19]. These observations do not contradict the expectation that, given an appropriate training set, the sampling procedure is capable to capture implicit constraints that will yield well formed instances. Moreover, in this case, the procedure seems to have been able to identify the “characteristic parts” for the desired target without requiring external expert domain knowledge.
       </paragraph>
      </section>
     </section>
     <section label="6.7">
      <section-title>
       Comparison with a domain specific approach
      </section-title>
      <paragraph>
       In Lewell et al. [23] the authors introduced a method called Retrosynthetic Combinatorial Analysis Procedure (RECAP) that breaks molecular graphs into fragments to be used as chemical building blocks. The way RECAP performs the fragmentation is based on the identification of 11 functional groups. A functional group is a specific portion of the molecule that is responsible for its characteristic chemical behavior. Crucially, identical functional groups will undergo the same chemical reaction independently of the remainder of the molecules they are located in. All bonds connecting these groups are then broken to extract the fragments. The reason for choosing this type of fragments lies in the fact that the broken bonds can be physically restored using known chemical reactions. This property is highly regarded by domain experts since while probing for new virtual compounds synthesizability is in practice an implicit constraint. While there exist commercial implementations of RECAP (e.g. Chemaxon), we use an open source implementation{sup:11} available under the Galaxy framework.{sup:12}
      </paragraph>
      <paragraph>
       We selected a random sample of 200 molecules from the Bursi dataset (100 form the positive data set and 100 from the negative one). Out of those only 32% (40% respectively) satisfy the RECAP requirements (i.e. exhibit known functional groups) and can therefore generate fragments. This is one example of the “limitations” that a system based on domain rules has. In total the RECAP procedure generates 76 fragments for the positives and 118 for the negatives. In comparison, the locally substitutable graph grammar identifies 47 cores and 41 interfaces for a total of 170 substitution rules for radius and thickness equal to two. When the radius and the thickness parameters are allowed to range from 2 to 6 then we obtain a total of 512 cores, 225 interfaces for a total of 832 substitution rules.
      </paragraph>
      <paragraph>
       We trained a NSPDK SVM predictor on the molecules generated by the recombination of the fragments according to the RECAP procedure. We also trained NSPDK SVM predictor on the samples generated by the proposed approach. We tested the predictors on the remaining balanced dataset of 4058 molecules obtaining for the RECAP method an F-measure{sup:13} of 0.68 and a ROC AUC of 0.75 while for the proposed constructive approach we obtained an F-measure of 0.70 and a ROC AUC of 0.77. The results are therefore comparable even if the constructive approach does not make use of any domain specific knowledge.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusions and discussion
     </section-title>
     <paragraph>
      In this paper we have presented a novel way to sample graphs from an empirical probability distribution. The approach merges the expressiveness of a novel context-sensitive graph grammar with the computational efficiency and robustness to noisy inputs of one-class support vector machines. Applications to molecular synthesis are encouraging. Interestingly, as shown in the experimental section, the generated samples can be used to “virtually” augment the size of the available data to enhance the performance of traditional machine learning classification algorithms. A related notion was presented in Schölkopf et al. [41] where samples are generated on the basis of some desired invariance (e.g. rotations for images). The fact that in our experimental case, the predictive performance of a discriminative learner trained on the augmented set increases, indicates that the generator is capable to construct instances that are characteristic of the class being modeled. Differently from Schölkopf et al. [41], here we are not injecting domain specific knowledge in the problem (encoded in the chosen type of class preserving invariance), but rather, we believe that the sampling augmentation works as an effective variance reduction technique using a sort of “general purpose” bias encoded in the locally substitutable graph grammar, namely that similar instances can be produced by replacing parts occurring in the same context.
     </paragraph>
     <paragraph>
      In future work we will investigate non-linear kernelized quantile estimation techniques (as done in Muandet and Schölkopf [35]) to better characterize the empirical probability distribution of graphs.
     </paragraph>
     <paragraph>
      A further element of the proposed approach that we will explore in more details is the “convergence issue”, i.e. the fact that the sampling procedure identifies instances belonging to a distribution that differs from the desired one if the Markov Chain has not properly converged. The issue is a research subject in the ordinary unstructured case and definitely needs further investigation in the structured case.
     </paragraph>
     <paragraph>
      Finally we will explore applications of the proposed approach to a variety of design tasks in different application domains. Interestingly the sampling process can be interpreted as a way to iteratively improve pre-existing designs and can be used as a computer aided creativity tool.
     </paragraph>
     <paragraph>
      A software implementation of the graph sampler is available at: https://github.com/fabriziocosta/GraphLearn.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>