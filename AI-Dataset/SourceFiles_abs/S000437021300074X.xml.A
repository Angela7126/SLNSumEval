<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Type Extension Trees for feature construction and learning in relational domains.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Probabilistic logical (or relational) models provide models for properties and relationships of entities in domains with a relational structure, such as graphs, networks, or, generally, any kind of structure found in a relational database. The prevalence of this type of structured data, and the challenges posed by it for traditional machine learning methods based on simple attribute-value data models has led to an increasing interest over the past 10 years in probabilistic logical models, and associated statistical-relational learning techniques [10], [6].
     </paragraph>
     <paragraph>
      When modeling entities embedded in a relational domain a key question is what features of the entities are relevant to model and predict properties of interest. Apart from using attributes of the given entities themselves, one has in relational learning the ability to construct new features by considering the relational neighborhood of an entity. Taking into consideration related entities and their attributes, one obtains a basically unlimited supply of potential features.
     </paragraph>
     <paragraph>
      A word on terminology here may be in order: by an attribute we mean a formal representation in a dataset of a property of individual entities by a data column. The color property of a flower, for example, could be formalized by attributes such as {a mathematical formula}color∈{red,green,blue,orange,…}, or three distinct attributes {a mathematical formula}RGB_red,RGB_green,RGB_blue∈{0,…,255}. The value space of an attribute will typically be a simple data type like Boolean, enumeration, or numeric. In classic attribute-value data, feature is often a synonym for attribute. By contrast, we use feature to denote formalized properties in a much broader sense. First, a feature may only be implicit in the data as a function of explicit data. For example, brightness as a function of {a mathematical formula}RGB_red,RGB_green, and RGB_blue is a feature for an attribute-value dataset containing the attributes {a mathematical formula}RGB_red,RGB_green,RGB_blue; the “number of friends older than 22” is a feature (of a person entity) in a relational social-network dataset. Second, for relational data, a feature can also represent a property relating multiple entities. Thus, “titles having more than 3 words in common” would be a feature for a pair of paper entities in a bibliographic database. Third, unlike other common frameworks in statistical learning (like for example kernel methods), in this paper we are not interested in simple numerical features but will focus on features whose values are complex combinatorial data structures representing what we loosely will call counts-of-counts. In all cases, however, we require that a feature has a well-defined formal specification and value space. The language available for the formal specifications defines the feature space.
     </paragraph>
     <paragraph>
      Relational learning frameworks differ widely to what extent they are linked to a clearly defined feature space, and to what extent feature selection or feature construction is integrated into the model learning process. On the one hand, there are techniques that only require the availability of features of a simple data type. The features construction is not part of the learning framework, and usually requires an application-dependent data pre-processing [42]. Propositionalization approaches also maintain a strict separation between feature construction and learning, but specific frameworks and representation languages for feature specification are a crucial ingredient [22].
     </paragraph>
     <paragraph>
      On the other extreme there are approaches in which feature construction and model learning are tightly integrated, and, in fact, the learned model essentially consists of a list of features represented in a formal specification language. To this category belong most frameworks that are based on predicate logic as the feature representation language [3], [20], [21]. In between, there are approaches where feature construction is an integral part of the learning process, but the exact feature space accessible to the learner is less clearly delimited [1], [18].
     </paragraph>
     <paragraph>
      A key component in the design of relational features is given by the tools that are available for constructing features from properties of entities that are related to the entity of interest by chains of one or several relations. Since the number of entities that are reached by such “slotchains” [8] varies from instance to instance, this feature construction usually involves a form of combination or aggregation of the properties of multiple related entities.
     </paragraph>
     <paragraph>
      In non-probabilistic inductive logic programming approaches, such an aggregation is usually based purely on existential quantification, i.e., a feature only determines whether or not a related entity with certain attributes exists. So, for example, for an “author” entity in a bibliographic database one could define a Boolean feature saying whether there exists a paper citing a paper of this author. A number of frameworks that are more closely linked to relational databases [31], [13] construct features based on aggregation operators. Here, it would be possible, for example, to construct a feature that represents the average count of citations that papers of an author have received, or a feature that represents the average price of items purchased by a customer. Recently approaches that define probability distributions over entire structures based on pure count features have become quite popular [36], [43]. Here the probability of a relational structure (over a given domain of entities) is determined by the count of entity tuples that satisfy some relational constraints, typically expressed as a logical clause.
     </paragraph>
     <paragraph>
      All these approaches are based on features that only represent relatively simple summary statistics about quantitative properties of an entityʼs relational neighborhood. However, for many prediction tasks, a more detailed picture of combinatorial count-of-count features may be relevant. Consider the tiny bibliographic dataset shown in Fig. 1, for instance. It represents 5 different authors, 10 different papers by these authors, and citation links between the papers. Simple summary features for an author a could be the number of aʼs papers, or his/her total or average citation count. However, a currently important attribute for an author is the h-index [14]. To predict the h-index (or another attribute closely linked to the h-index – like receiving a professional award, or a large research grant) one may need to consider the more detailed feature of the count of papers with given counts of citations. In Fig. 1, the values for the 5 authors of this count-of-count feature are shown on the right (an expression {a mathematical formula}k:l meaning that there are l papers with k citations).
     </paragraph>
     <paragraph>
      As another example for count of count features consider the Internet Movie Database (IMDB), a quite popular object of investigation in relational machine learning. Here one may be interested in predicting some attribute of a movie, e.g., whether it will be a box-office success (e.g. [38], [31]). For this prediction one may consider the cast of the movie, for example in terms of its size, the count of actors in the cast who have previously received an award nomination, the total number of award nominations shared by the actors, etc. Again, a more detailed count-of-count feature can be more informative than only flat counts: it will make a difference, perhaps, whether there is a single actor in the cast with many award nominations (perhaps a single box office draw actor, but maybe beyond the peak of his/her career?), or whether there are many actors with one or two nominations each (perhaps a young all-star cast?).
     </paragraph>
     <paragraph>
      In information retrieval, relevance measures for a document d given a query q are often based on counting terms appearing both in d and q. These counts will usually be weighted by a term weight such as inverse document frequency, which is given by the number of documents in the collection containing the term. Thus, the relevance measure is computed from a count-of-count feature. Similarly to Fig. 1, for example, a query-document pair {a mathematical formula}(d,q) could have a feature value {a mathematical formula}[3:1,10:2,7:1] expressing the fact that d and q have 1 term in common that appears in a total of 3 documents, 2 terms in common that each appear in 10 documents, and 1 term in common that appears in 7 documents.
     </paragraph>
     <paragraph>
      Finally, consider the relational domain consisting of Web-pages and the links-to relation. An important attribute of a web-page is its pagerank [5], and we may want to estimate the pagerank of a web-page based on information of its local relational neighborhood. Unlike the h-index in the bibliographic example, which is precisely determined by a relational neighborhood of radius 2 defined by the chain {a mathematical formula}authorOf(A,P),cites(P′,P), the pagerank{sup:1} is fully determined only by the structure of the whole relational domain. However a useful approximation might be obtained already from local information. Clearly relevant for the pagerank of page P is the number of its incoming links. Also important is the pagerank of the pages {a mathematical formula}P′ linking to P, and hence the number of their incoming links. Furthermore, it is important to know for the pages {a mathematical formula}P′ linking to P the number of outgoing links of {a mathematical formula}P′ (pointing to pages other than P), because this determines how much of the pagerank of {a mathematical formula}P′ is “inherited” by P. Again, the full relevant information is only given by a comprehensive count-of-count feature.
     </paragraph>
     <paragraph>
      The purpose of this paper is to develop a framework for the representation of rich combinatorial count-of-count features in the context of relational learning. The methodological contribution of the paper has three main components:
     </paragraph>
     <list>
      <list-item label="(C1)">
       The definition of Type-Extension Trees (TETs) [7] as a formal representation language for count-of-count features.
      </list-item>
      <list-item label="(C2)">
       A method for learning Type-Extension Trees in conjunction with a simple predictive model based on TET-features. This gives us a method for relational feature discovery, as well as a baseline supervised learning framework. In [7] TET learning employed the classic notion of information gain, while in this paper we take advantage of the recently introduced relational information gain[26].
      </list-item>
      <list-item label="(C3)">
       The definition of a novel metric on TET feature values, which enables distance-based learning techniques that make use of count-of-count features in a substantial manner.
      </list-item>
     </list>
     <paragraph>
      To illustrate the relationship and significance of these three components, consider an analogy with learning from standard numerical attribute-value data, where each instance is fully characterized by a tuple of numeric attributes, and a class variable. Fig. 2 on the left shows a small dataset with numeric attributes {a mathematical formula}A1,A2,N1,…,N4 and a binary class label with values p(ositive) and n(egative). In this dataset the class is only correlated with the attributes {a mathematical formula}A1 and {a mathematical formula}A2, whereas {a mathematical formula}N1,…,N4 are random noise. The plot in the left part of the figure represents the values of {a mathematical formula}A1,A2 and C. The relevant feature subset for predicting the class label then is {a mathematical formula}{A1,A2}, out of the space of all possible feature subsets (Fig. 2A). The set {a mathematical formula}{A1,A2} may also be called a sufficient or model-independent feature for predicting C. No concrete type of machine learning model will use all the information represented by this feature. A decision tree model, for instance, will only use finitely many Boolean features defined by lower- or upper-bounds on {a mathematical formula}A1,A2-values. A linear classifier will only use a linear function of {a mathematical formula}A1,A2. We may call the spaces of such features reduced or model-specific feature spaces (Fig. 2B). A proper distinction between the sufficient and reduced feature spaces is important when we interpret the result of learning a specific model also in terms of feature discovery: the decision tree learned from our example dataset (Fig. 2C left) uses four Boolean features {a mathematical formula}A1&gt;0.79,…,A2&gt;0.75, and, strictly speaking, has “discovered” exactly these four features. However, one will typically want to generalize, take {a mathematical formula}{A1,A2} as the discovered feature in the sufficient feature space, and assume that suitable reductions of {a mathematical formula}{A1,A2} in other model-specific feature spaces will also lead to good performance of other types of models. Fitting a logistic regression model to our data (Fig. 2C right) directly leads only to the construction of the linear function {a mathematical formula}12.8−22.1A1⋯+3.4N4 as a predictive feature. Again, one can abstract from this reduced feature, and try to identify the “discovered” model-independent feature. Here this abstraction is not as clear-cut as in the decision tree case. Considering the attributes whose coefficients in the linear function have the highest absolute values for inclusion in the feature subset, one here might take any of the subsets {a mathematical formula}{A1}, {a mathematical formula}{A1,A2}, {a mathematical formula}{A1,A2,N3,N4} as the discovered model-independent feature.
     </paragraph>
     <paragraph>
      Our objective (C1) aims at defining for relational data a rich, model-independent feature space that corresponds to the space of attribute subsets (Fig. 2A), and that includes complex count-of-count features. This definition is developed in Section 2 via syntax and semantics of Type Extension Trees (see Section 2.1). The construction of a sufficient feature space for relational data not only faces the challenge of the basically unlimited supply of possible features, but also the challenge of the diversity of relational learning tasks: attribute prediction for individual entities, link prediction, and classification of whole relational structures (e.g. molecules) require the specification not only of features for single entities, but also for pairs of entities, tuples of entities, and global features of a whole relational dataset. TET features provide a uniform and coherent framework for all these cases.
     </paragraph>
     <paragraph>
      Component (C2) corresponds to the feature discovery process via the learning of a lightweight model as illustrated in Fig. 2. This is achieved by first defining directly on TET-feature values a discriminant function that turns a TET-feature into a predictive model (Section 3). Based on this discriminant function a TET structure learning algorithm is developed that, strictly speaking, will discover a feature {a mathematical formula}(T,f) (T a TET, f a discriminant function defined on T), but from which in a trivial abstraction step we can extract the model-independent TET feature T (Section 5).
     </paragraph>
     <paragraph>
      Given a TET – either learned or manually constructed based on expert knowledge – we finally in component (C3) define a predictive model based on TET features that makes use of the TETʼs count-of-count values in a more substantial and sophisticated manner than the discriminant function. For this we define a metric on TET values that then enables nearest neighbor classification (Section 4).
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Feature representation with Type Extension Trees
     </section-title>
     <section label="2.1">
      <section-title>
       TET syntax and semantics
      </section-title>
      <paragraph>
       In this section, we review the basic syntax and semantics definitions of Type Extension Trees [17]. To simplify the definitions, we will assume that all attributes and relations are Boolean, which means that a multi-valued attribute like ʼcolorʼ that would give rise to atomic propositions such as color(hat, red) is assumed to be encoded using Boolean attributes like color_red(hat). Relational data can then be viewed as a model in the sense of the following definition.
      </paragraph>
      <paragraph label="Definition 2.1">
       Let R be a relational signature, i.e. a set of relation symbols of different arities. A (finite) model for R, {a mathematical formula}M=(M,I)  consists of a finite domain M, and an interpretation function {a mathematical formula}I:r(a)→{t,f} defined for all ground atoms{a mathematical formula}r(a) constructible from relations {a mathematical formula}r∈R and arguments {a mathematical formula}a∈Marity(r).
      </paragraph>
      <paragraph>
       Throughout the paper we use f and t as shorthands for false and true. Furthermore, we denote objects (entities) from a domain with lowercase letters, and logical variables with uppercase letters. Only first-order formulas are acceptable, i.e. variables always stand for objects. Bold symbols always denote tuples of the corresponding non-bold symbols, e.g., in the foregoing definition: {a mathematical formula}a=(a1,…,aarity(r)). In logic programming terminology, {a mathematical formula}M is a Herbrand interpretation for the signature consisting of R and constant symbols for the elements of M. For convenience we may assume that the domain M is partitioned into objects of different types, that arguments of relations are typed, and that I is only defined for ground atoms with arguments of appropriate types. For the sake of simplicity we do not introduce any special notation for specifying types. Rather, in our examples below we will typically use generic capital letters (X, Y, Z, U, V, W) to indicate variables which may range over all domain objects, and specific letters (like A for author or P for paper) to implicitly mean that the corresponding variables are restricted to a subset of objects. With a slight abuse of notation, if {a mathematical formula}τ(a) is a complex ground sentence, we denote by {a mathematical formula}I(τ(a)) its truth value under interpretation I.
      </paragraph>
      <paragraph label="Definition 2.2">
       An R-literal is a (negated) atom {a mathematical formula}r(V) ({a mathematical formula}r∈R∪{=}, V a tuple of variable symbols). We also allow the special literal {a mathematical formula}⊤(V), which always evaluates to t. An R-type is a conjunction of R-literals.A type extension tree (TET) over R is a tree whose nodes are labeled with R-types, and whose edges are labeled with (possibly empty) sets of variables.
      </paragraph>
      <paragraph>
       In the following we will usually omit the reference to the underlying signature R, and talk about literals and types, rather than R-literals and R-types.
      </paragraph>
      <paragraph>
       Note that according to Definition 2.2 a literal cannot contain any constant symbols as arguments. Since R is assumed to only contain relation and no constant symbols, this is consistent with the usual definition of an R-literal. The term “type” for a conjunction of literals is motivated by two distinct (yet compatible), existing uses of “type”: on the one hand, the type of an entity is commonly understood as a property expressed by a single unary predicate, e.g., {a mathematical formula}movie(V) or {a mathematical formula}person(V). Our definition generalizes this to types of tuples of objects, and to types which are expressed via a conjunction of literals. On the other hand, “type” is used in mathematical model theory for consistent sets of formulas using free variables {a mathematical formula}V1,…,Vn that describe properties of n-tuples of domain elements [15]. Our definition is a special case of type in this sense by limiting it to a single, quantifier-free conjunction of literals.
      </paragraph>
      <paragraph label="Example 2.3">
       The following is a TET for a signature containing relations author, authorOf and cites:{a mathematical formula} According to the semantics given below, this TET represents a feature that is sufficient for computing the h-index of an author, for example.
      </paragraph>
      <paragraph label="Example 2.4">
       A TET that will be sufficient for representing relevance features based on inverse-document frequency weights is{a mathematical formula}
      </paragraph>
      <paragraph>
       Labeled edges in a TET are related to quantifiers in predicate logic: like a quantifier, a labeled edge binds all occurrences of the variables associated with the edge in the subtree rooted at this edge. The free variables of a TET are all variables not bound by an edge label. We call a TET propositional if all edge labels are empty. The TET in (1) has the single free variable A, the one in (2) the two free variables D and Q. In both cases, the root node essentially serves to introduce the free variables, and, in case of (1), to explicitly establish that the variable ranges over entities of type author. If such type constraints on variables are assumed to be implicit in the variable names, then the root will usually be a vacuous {a mathematical formula}⊤() atom, as in (2).
      </paragraph>
      <paragraph>
       We write {a mathematical formula}T(V) to denote a TET whose free variables are among the variables V (but does not necessarily contain all of them). We write{a mathematical formula} to denote a TET with a root labeled with {a mathematical formula}τ(V), and m sub-trees {a mathematical formula}T1(V,Wi,) reached by edges labeled with variables {a mathematical formula}Wi (possibly empty).
      </paragraph>
      <paragraph>
       A TET {a mathematical formula}T(V) with free variables {a mathematical formula}V=V1,…,Vk will define a feature for k-tuples of domain entities: for any model {a mathematical formula}M, and any {a mathematical formula}a∈Mk the TET defines a feature value{a mathematical formula}V(T(a)). Fig. 1 on the right shows (in a somewhat simplified form) the values {a mathematical formula}V(T(a1)),…,V(T(a5)) for the TET {a mathematical formula}T(A) in (1). We give the general definition of TET semantics in two steps: first we define the value space of nested counts associated with a given TET {a mathematical formula}T(V), and then the actual mapping {a mathematical formula}a↦V(T(a)).
      </paragraph>
      <paragraph label="Definition 2.5">
       For any set A we denote with {a mathematical formula}multisets(A) the set of all multisets over A. We denote with {a mathematical formula}{a1:k1,…,an:kn} a multiset that contains {a mathematical formula}ki copies of {a mathematical formula}ai. The value space{a mathematical formula}V(T) of a TET T is inductively defined as follows:Base: If {a mathematical formula}T=[τ] consists of a single node, then {a mathematical formula}V(T)={t,f}.Induction: If {a mathematical formula}T=[τ,(W1,T1),…,(Wm,Tm)], then{a mathematical formula}
      </paragraph>
      <paragraph>
       We note that according to this definition the structure of {a mathematical formula}V(T) only depends on the tree structure of T, but not on the labeling of the edges of T, or the types at the nodes of T.
      </paragraph>
      <paragraph label="Example 2.6">
       (1) is a graphical representation of a TET that following (3) can be written as{a mathematical formula} The recursive definition of {a mathematical formula}V(T) is grounded in {a mathematical formula}V([cites(P2,P1)])={t,f}. In other words, the single node TET {a mathematical formula}[cites(P2,P1)] represents a Boolean feature for pairs of papers. The inductive construction proceeds with the definition of {a mathematical formula}V(T′(A,P1)), where{a mathematical formula} represents a feature of an author-paper pair {a mathematical formula}A,P1. This value space is constructed according to the inductive case of Definition 2.5 as the union of {a mathematical formula}{f} and pairs of the form {a mathematical formula}(t,A), where A is a multiset of t, f values. Thus, examples are: f (according to Definition 2.7 below, this is the feature value of an author-paper pair {a mathematical formula}(a,p), where a is not the author of p), {a mathematical formula}(t,{f:9,t:1}) (this will be the feature value, e.g., of the pairs {a mathematical formula}(a1,p1) and {a mathematical formula}(a2,p4) in Fig. 1), or {a mathematical formula}(t,{f:8,t:2}) (the feature value of {a mathematical formula}(a1,p2) in Fig. 1).Finally, values of the full TET (4) are either f, or {a mathematical formula}(t,A), where A is a multiset of values from {a mathematical formula}V(T′(A,P1)). Examples are{a mathematical formula}Here, for better readability, the outer multisets are written as column vectors, rather than in comma-separated linear form (used for the inner multisets). These five values are just the feature values of the five authors {a mathematical formula}a1,…,a5 in Fig. 1. We will usually use γ to denote TET values. We note that the t component of a value of the form {a mathematical formula}(t,A), A a multiset, is redundant: since every occurrence of a multiset is prefixed by such a t one could just write A instead of {a mathematical formula}(t,A). However, adding the explicit t as an embellishment to the value lets us maintain a clearer match between the structure of a TET T and its values {a mathematical formula}γ∈V(T): otherwise, for example, the value {a mathematical formula}(t,{f:1}) of a two-level TET would become just {a mathematical formula}{f:1} and easily confused with a value f.
      </paragraph>
      <paragraph>
       In the preceding example we have already introduced the values of the TET feature (1) for the entities {a mathematical formula}a1,…,a5 in Fig. 1. These values refine the informal count-of-counts shown in Fig. 1 by representing in a more principled way the recursive nature of count-of-counts, and by also including f counts (in this case, the number of papers not written by a given author, and the number of papers not citing a given paper). In the following we give the general definition of the feature value {a mathematical formula}V(T(a))∈V(T(V)) for a specific tuple a.
      </paragraph>
      <paragraph label="Definition 2.7">
       Let {a mathematical formula}M=(M,I) be a model, {a mathematical formula}T(V1,…,Vk) a TET, and {a mathematical formula}a∈Mk. The value {a mathematical formula}V(T(a))∈V(T) is defined as follows:Base: If {a mathematical formula}T(V)=[τ(V)] consists of a single node, then {a mathematical formula}V(T(a)):=I(τ(a)).Induction: If {a mathematical formula}T(V)=[τ(V),(W1,T1(V,W1)),…,(Wm,Tm(V,Wm))]:
      </paragraph>
      <list>
       <list-item label="(a)">
        If {a mathematical formula}I(τ(a))=f then {a mathematical formula}V(T(a)):=f.
       </list-item>
       <list-item label="(b)">
        If {a mathematical formula}I(τ(a))=t then{a mathematical formula} with {a mathematical formula}μ(a,Wi,Ti)∈multisets(V(Ti)) given by{a mathematical formula} where γ ranges over all values in {a mathematical formula}V(Ti), and {a mathematical formula}ki is the number of variables in {a mathematical formula}Wi.
       </list-item>
      </list>
      <paragraph>
       We remark that the original definitions given in [17] treated the cases {a mathematical formula}I(τ(a))=f and {a mathematical formula}I(τ(a))=t symmetrically, which is why values according to Definition 2.5 were called “pruned” in [7] (because here the recursive evaluation of counts is cut off once a node evaluating to f is encountered on a TET branch).
      </paragraph>
      <paragraph>
       We next consider in some detail the special case of unlabeled edges in a TET, and, in particular, purely propositional TETs. Consider a TET with a single unlabeled branch attached to the root: {a mathematical formula}T=[τ(V),(∅,T1(V))]. The multiset (7) then contains the single value {a mathematical formula}γ=V(T1(a)). Generally, an unlabeled edge in a TET induces in the recursive structure of {a mathematical formula}V(T) a single value of the sub-TET {a mathematical formula}T′ reached by this edge, whereas an edge labeled with one or several variables induces a multiset of such values. The following example serves to illustrate the nature of propositional TETs. At the same time the example shows how different tree structures of a TET can be used to represent different logical properties.
      </paragraph>
      <paragraph label="Example 2.8">
       Consider a propositional TET {a mathematical formula}T(V) with {a mathematical formula}V=v1,…,vn. Since no further variables are introduced via edge labels, then all nodes only contain literals in the variables V, and therefore {a mathematical formula}V(T(a1,…,an)) only depends on the relational sub-structure induced by {a mathematical formula}a1,…,an in {a mathematical formula}M. By varying the types at the nodes, as well as the tree structure of {a mathematical formula}T(V), the values {a mathematical formula}V(T(a)) can represent a variety of different structural properties.Consider a relational signature that contains a single binary (“edge”) relation symbol {a mathematical formula}e(⋅,⋅). The upper part of Table 1 shows 4 different propositional TETs with the two free variables {a mathematical formula}v1,v2. In all cases, the TET value {a mathematical formula}T(a1,a2) is determined by the relational sub-structure induced by {a mathematical formula}a1, {a mathematical formula}a2.The leftmost column in Table 1 lists the four different possible sub-structures, and the remaining columns the values returned by TETs (a)–(d) for these sub-structures.TET (a) represents a feature that only tests whether {a mathematical formula}a1, {a mathematical formula}a2 define the sub-structure {a mathematical formula}•a1→•a2. TET (b) is doing a two-stage test for the two possible edge relations. If the first test fails, i.e. there is no edge {a mathematical formula}e(a1,a2) then the value is false, regardless of the presence or absence of the converse edge.Both (c) and (d) use a vacuous root-type {a mathematical formula}⊤(v1,v2) to connect sub-TETs that are evaluated separately. The values in columns (c) and (d) in the table list the values of the different sub-TETs according to the top-to-bottom order on the branches defined by the graphical representation above. Both TETs discriminate between all four {a mathematical formula}a1,a2-structures in the sense that each structure has a unique value. In this sense, (d) could be seen as a redundant version of (c). However, as we will see in Section 4, (c) and (d) exhibit quite distinct behavior with respect to the metric we will define on TET values.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       TET definable features
      </section-title>
      <paragraph>
       In this section we illustrate that TETs provide a representation language for a rich class of fundamental features that are usable in a variety of learning frameworks, and that, thus, TET-features can play the role of the sufficient feature space in analogy to Fig. 2A.
      </paragraph>
      <paragraph label="Example 2.9">
       While in this previous work nested aggregation was considered, all these frameworks require an immediate aggregation at each step along a chain of relations. Thus, considering bibliographic data for example, one could define an author feature that represents whether the author has at least seven publications with at least seven citations each. However, a complex count-of-count feature that would be sufficient for computing the h-index is outside the scope of these feature construction methods.Most of the approaches here mentioned also consider (or, in some cases, focus on) numeric predicates, and aggregation of numeric values using functions like mean and max. For the purely categorical (especially Boolean) data that we consider in this paper, the only common aggregation function is count (with “exists” as the special case “count ⩾1”). In spite of our current restriction to Boolean data, the same basic TET architecture could also be extended to numeric data, and thereby also be used to represent the underlying sufficient combinatorial and numerical information needed to compute more model-specific aggregated features.
      </paragraph>
      <paragraph label="Example 2.10">
       In this example we focus on Markov-Logic Networks (MLNs) [36] in the generative setting. In this case MLNs define a distribution over all models {a mathematical formula}M for a given signature R, and a fixed domain M. This distribution is defined by a knowledge base KB containing first-order logic formulas {a mathematical formula}ϕi with attached numeric weights {a mathematical formula}wi:{a mathematical formula} We refer to [36] for the details of MLN syntax and semantics. Relevant in the current context is the fact that the distribution is defined as a function of count features, which, adapting the notation of the previous example, can be written in the form{a mathematical formula} where now {a mathematical formula}ϕ(X) can be any first-order formula with free variables X. While, thus, very similar in appearance to the aggregate features of the preceding example, there are some essential differences: first, MLNs depend on the actual integer-valued count feature, not only on derived Boolean features of the form {a mathematical formula}count{…}⩾k. Second, (10) takes the count of all substitutions of tuples of domain elements for all the free variables X in {a mathematical formula}ϕ(X). As a result, whereas {a mathematical formula}count{Y|triangle(Y)∧in(Y,X)}⩾3 was a feature of the object X, now {a mathematical formula}count{X|ϕ(X)} is a feature of entire models {a mathematical formula}M.Concrete MLN implementations will often allow only a restricted class of formulas {a mathematical formula}ϕ(X) in the model specification (e.g. quantifier free formulas). However, the use of arbitrary first-order formulas poses no problem at the level of semantic definitions, and so we here consider models with no restrictions on the {a mathematical formula}ϕ(X). For any such count feature one can construct a TET {a mathematical formula}T() such that the integer-valued feature (10) is obtained as a reduction of the values {a mathematical formula}V(T()). We construct {a mathematical formula}T() in the form {a mathematical formula}⊤→XTϕ(X), where now {a mathematical formula}Tϕ(X) is a TET with free variables X, such that the truth value of {a mathematical formula}ϕ(a) can be read off the TET value {a mathematical formula}V(Tϕ(a)).The construction of {a mathematical formula}Tϕ is by induction on the structure of ϕ. In the atomic case {a mathematical formula}ϕ(X)≡r(X){a mathematical formula}Tϕ contains the single node {a mathematical formula}r(X). For a conjunction {a mathematical formula}ϕ(X)=ϕ1(X1)∧ϕ2(X2) one lets {a mathematical formula}Tϕ(X)=[⊤,(∅,Tϕ1(X1)),(∅,Tϕ2(X2))]. At this point in the construction TET values {a mathematical formula}V(Tϕ(a)) already encode more information than the mere Boolean value {a mathematical formula}ϕ(a), since it also contains the individual truth values for the two conjuncts {a mathematical formula}ϕ1, {a mathematical formula}ϕ2 (cf. (c) in Table 1). For the negation case {a mathematical formula}ϕ(X)≡¬ψ(X) one can simply let {a mathematical formula}Tϕ=Tψ: if the truth value of {a mathematical formula}ψ(a) is retrievable from {a mathematical formula}V(Tψ(a)), then so is the truth value of {a mathematical formula}¬ψ(a). In the quantifier case {a mathematical formula}ϕ(X)≡∃Yψ(Z,Y) ({a mathematical formula}X=Z∪Y) one defines {a mathematical formula}Tϕ(X)=[⊤,(Y,Tψ(Z,Y))]. Again, the TET constructed in this step represents a counting-refinement of the Boolean feature actually required (note that while (10) has an outermost counting semantics for the variables X, there is a standard Boolean semantics for any quantifiers appearing internally in ϕ).
      </paragraph>
      <paragraph label="Example 2.11">
       Graphs are a special kind of relational models in which there is a single binary (edge) relation {a mathematical formula}e(X,Y), and, in the case of labeled graphs, multiple unary relations {a mathematical formula}li(X) representing the different node labels. An important feature considered in graph mining problems, and used, for example, to define kernel functions on graphs [9], is the number of subgraphs of a specific structure: if H is any (finite) graph, then{a mathematical formula} counts the number of subgraphs {a mathematical formula}H′ isomorphic to H that are embedded in G. The (unlabeled) graph H with nodes {a mathematical formula}{1,…,k} can be described by a type that contains one variable {a mathematical formula}Xi for each node i of H, and the literal {a mathematical formula}e(Xi,Xj) for each edge {a mathematical formula}i→j in H. Then{a mathematical formula} is just the TET feature that corresponds to {a mathematical formula}ϕH(G). The case of labeled graphs, or the case where {a mathematical formula}H′⊆G is the induced subgraph relationship (i.e., there are no edges in G between any nodes matched to nodes of {a mathematical formula}H′, other than the edges also present in {a mathematical formula}H′) are treated in a similar manner.
      </paragraph>
      <paragraph>
       The preceding examples illustrate the ability of the TET language to represent in a coherent manner a wide range of features used in a variety of different relational learning frameworks. There are some limitations to the TET language, though: TETs are essentially rooted in first-order logic, with a refined counting semantics replacing Boolean existential or universal quantification. They cannot represent features that are not first-order in nature, such as features that are defined in terms of the transitive closure of a relation, which can only be defined in suitable extensions of first-order logic, such as least fixed-point logic, or transitive closure logic [11]. For example, “paper {a mathematical formula}p1 can be found by tracing a chain of citations starting with paper {a mathematical formula}p2” is not TET-expressible. Also, the integer-valued feature “Erdös number of author a” cannot be captured by a TET, since it depends on a chain of co-authorship relations of undetermined length.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      TET discriminant function
     </section-title>
     <paragraph>
      A TET alone only defines a feature of objects in a relational domain. TET-defined features can be incorporated in many ways into existing types of predictive or descriptive models. For example, one can define distance or kernel functions on TET value spaces {a mathematical formula}V(T), thereby making TET features usable for standard clustering techniques, or SVM classifiers. We will introduce a metric on {a mathematical formula}V(T) in Section 4. In this section we first describe how to build a predictive model on a TET feature using simple discriminant functions on TET values, i.e. functions of the form [7]{a mathematical formula} Such discriminant functions directly lead to binary classification models. We use {a mathematical formula}{+,−} to denote binary class labels. Then one can learn two discriminant functions {a mathematical formula}d+, {a mathematical formula}d−, and a threshold value t, and assign class label + to a tuple a iff{a mathematical formula}
     </paragraph>
     <section label="3.1">
      <section-title>
       Defining the simple TET discriminant
      </section-title>
      <paragraph>
       We now introduce one simple type of TET-discriminant function. The motivation for the particular form of discriminant function we propose is twofold: first, for a given TET, these discriminants are efficient to learn and evaluate. Since the discriminant function is also used in TET learning, within a wrapper evaluation routine for candidate TETs (see Section 5), efficiency is an important issue. Second, as we will show below, our discriminant function definition can be motivated as a uniform generalization of the classic decision tree and Naive Bayes models.
      </paragraph>
      <paragraph label="Definition 3.1">
       Let T be a TET. A weight assignment β for T assigns a non-negative real number to all nodes of T. A weight assignment can be written as {a mathematical formula}(βr,β1,…,βm), where {a mathematical formula}βr is the weight assigned to the root, and {a mathematical formula}βi is the weight assignment to the ith sub-tree.For a TET T with node weights β we define the discriminant function {a mathematical formula}dβ as follows. Let {a mathematical formula}γ∈V(T):
      </paragraph>
      <list>
       <list-item label="•">
        If {a mathematical formula}γ=f define {a mathematical formula}dβ(γ):=0.
       </list-item>
       <list-item label="•">
        If {a mathematical formula}γ=t then {a mathematical formula}T=[τ(V)] consists of a single node, and {a mathematical formula}β=(βr). Define {a mathematical formula}dβ(γ):=βr.
       </list-item>
       <list-item label="•">
        If {a mathematical formula}γ=(t,μ1,…,μm), {a mathematical formula}μi∈multisets(V(Ti)), define{a mathematical formula}
       </list-item>
      </list>
      <paragraph>
       In the following we illustrate the nature of this discriminant function. We begin by investigating TETs in a propositional setting, where it turns out that our simple discriminant is closely related to the standard decision tree and Naive Bayes models.
      </paragraph>
      <paragraph>
       Table 2 shows a hypothetical dataset with observations of 100 cases for which three Boolean attributes a, b, c and a binary class label class with values +, − are recorded. To be consistent with our relational notation, we view the Boolean attributes as unary relations {a mathematical formula}a(X), {a mathematical formula}b(X), {a mathematical formula}c(X) defined on the observations X. The entries in the table represent pairs {a mathematical formula}n+; {a mathematical formula}n− of counts for the positive and negative class with the given attribute value combination.
      </paragraph>
      <paragraph>
       Fig. 3 on the left shows a decision tree that could be constructed for this data. The nodes are labeled with the counts {a mathematical formula}n+; {a mathematical formula}n− of positive/negative examples that reach the nodes. An example e with attribute values {a mathematical formula}a(e)=t, {a mathematical formula}b(e)=f, {a mathematical formula}c(e)=t, for instance, would be classified as positive, or, more precisely, would be estimated as being positive with probability 9/14.
      </paragraph>
      <paragraph>
       The right side of the Fig. 3 shows a propositional TET for a, b, c. This TET is labeled with a weight assignment, where the weight at each node corresponds to the empirical frequency {a mathematical formula}n+/(n++n−) of the positive class among the examples that satisfy all the conditions on the path from the root down to the node. For the example {a mathematical formula}a(e)=t, {a mathematical formula}b(e)=f, {a mathematical formula}c(e)=t the TET in Fig. 3 evaluates to{a mathematical formula} For this value, the discriminant function evaluates as{a mathematical formula}
      </paragraph>
      <paragraph>
       A Naive Bayes model learned from the data of Table 2 is shown in the top of Fig. 4, with a corresponding TET below. Again the TET nodes are labeled with weight assignments corresponding to empirical frequencies {a mathematical formula}n+/(n++n−).
      </paragraph>
      <paragraph>
       Now the example {a mathematical formula}a(e)=t, {a mathematical formula}b(e)=f, {a mathematical formula}c(e)=t evaluates to the TET value{a mathematical formula} which gives a discriminant function value equal to{a mathematical formula} While (13) is not quite equivalent to the posterior probability {a mathematical formula}P(+|a(e)=t,¬b(e)=t,c(e)=t) that one would obtain from the Naive Bayes model, one still obtains that the ratio {a mathematical formula}d+β(V(T(e)))/d−β(V(T(e))) (with {a mathematical formula}d−β similarly defined as {a mathematical formula}d+β, only using the negative class frequencies {a mathematical formula}n−/(n++n−) as weights) is equal to the odds ratio {a mathematical formula}P(+|a(e),b(e),c(e))/P(−|a(e),b(e),c(e)) in the Naive Bayes model.
      </paragraph>
      <paragraph>
       The special cases (12) and (13) for TETs emulating decision trees and Naive Bayes models, respectively, generalize to arbitrary propositional TETs as follows. To simplify matters, consider a TET T with a single free variable X, and vacuous root {a mathematical formula}⊤(X). Assume that T has n nodes besides the root, and that the ith node is labeled with a type {a mathematical formula}τi(X). We denote with {a mathematical formula}ϕip(X) the Boolean feature that is the conjunction of all types on the path from the root to node i (not including {a mathematical formula}τi(X) itself). Let node i be labeled with the weight {a mathematical formula}P(+|ϕip=t,τi=t).
      </paragraph>
      <paragraph>
       Now consider an example e. It defines a “prefix” of T consisting of all nodes i for which {a mathematical formula}ϕip(e)∧τi(e) evaluates to t. Without loss of generality, assume that the nodes in this prefix are just {a mathematical formula}i=1,…,l for some {a mathematical formula}l⩽n. Then the discriminant function value for e is{a mathematical formula}
      </paragraph>
      <paragraph>
       Let {a mathematical formula}ki⩾0 be the number of children of node {a mathematical formula}i⩽l within the prefix {a mathematical formula}{1,…,l} ({a mathematical formula}ki will usually be less than the actual number of children of {a mathematical formula}τi in the full TET T). For {a mathematical formula}j=1,…,ki let {a mathematical formula}ϕij be the Boolean feature that is the conjunction of all prefix nodes contained in the subtree rooted at the jth child of i (note that {a mathematical formula}ϕij depends on e, which defines the relevant prefix). If we now assume that for each {a mathematical formula}i⩽l the child features {a mathematical formula}ϕij ({a mathematical formula}j=1…ki) are independent given {a mathematical formula}ϕip∧τi and {a mathematical formula}class=+, then one obtains{a mathematical formula} and from that{a mathematical formula} Thus, in this way {a mathematical formula}d+β/d−β can be interpreted as an odds ratio. The independence assumption we made to arrive at this interpretation is appropriate for the decision tree and Naive Bayes emulating TETs: in the first case, it actually becomes vacuous, because here the prefix defined by an example always consists of a single branch. For the Naive Bayes TETs, it is just the regular Naive Bayes assumption. For TETs that do not have a pure decision tree or Naive Bayes structure, the independence assumption can still be reasonable, and lead to a coherent probabilistic interpretation of the {a mathematical formula}d+/d− ratio. Note, however, that one also easily can construct TETs in which our independence assumption is infeasible, due to logical dependencies between different child features {a mathematical formula}ϕij. Thus, while our analysis here leads to a general understanding of the nature of the discriminant function, it does not necessarily endow it in all cases with a coherent probabilistic semantics.
      </paragraph>
      <paragraph>
       So far, we have considered propositional TETs only. However, the analysis of the discriminant function for this case directly carries over to non-propositional TETs. For this we only have to observe that for any given concrete domain M one can transform a TET T into an equivalent propositional one by grounding all variable-introducing edges, i.e., by replacing a branch {a mathematical formula}→WT′(V,W) with the set of branches {a mathematical formula}→T′(V,a) ({a mathematical formula}a∈M|W|). If all the groundings of the original branch are labeled with copies of the original weight assignment, then the discriminant function defined by the grounded TET is the same as the discriminant function defined by the original TET. Thus, the interpretation of the discriminant function on propositional TETs also explains the discriminant function on general TETs, with two additional assumptions, or observations: our general independence assumption implies that features {a mathematical formula}T′(V,a),T′(V,a′) defined by two different substitutions of constants in a sub-TET {a mathematical formula}T′(V,W) are assumed as independent, and the {a mathematical formula}d+/d− ratio one obtains is only an approximation of (14), since the weights defined by the initial TET are not exact class frequencies for the ground features, but only shared approximations obtained from aggregating statistics from all groundings (see Section 5).
      </paragraph>
      <paragraph label="Example 3.2">
       Consider the following weight assignment for the TET from Example 2.3:{a mathematical formula} We compute the discriminant function value for {a mathematical formula}γ2 and {a mathematical formula}γ3 from Example 2.6:{a mathematical formula} Since the underlying TET consists of a single branch, here {a mathematical formula}m=1 at all levels in the recursive value definition. At the first level of recursion we obtain:{a mathematical formula} To proceed, we compute for values of the form {a mathematical formula}(t,{f:k,t:l}) for the sub-TET {a mathematical formula}T′(A,P1) (cf. (5)):{a mathematical formula} Plugging this into (15) gives{a mathematical formula} The last re-arrangement of the terms in (16) can be read as follows: each paper {a mathematical formula}P1 written by author A contributes a factor 1.5 to the discriminant function value, and each citation to a paper by A contributes a factor of 2.0/1.5. Since in {a mathematical formula}γ3 the total number of authored papers, and citations to these papers also is 3, respectively 2, the same discriminant function value is obtained:{a mathematical formula}
      </paragraph>
      <paragraph>
       The preceding example points to a limitation of the discriminant function: the function value {a mathematical formula}d(γ) depends only on certain “flat” counts contained in γ, not on the more detailed count-of-count structure. On the other hand, being a product of factors determined by simple counts, it turns out that TETs with the discriminant function can emulate Markov Logic Networks, as the following example illustrates.
      </paragraph>
      <paragraph label="Example 3.3">
       Consider a MLN knowledge base (9) in which all {a mathematical formula}ϕi are conjunctions of literals. The MLN then defines the weight of a model {a mathematical formula}M as{a mathematical formula} where {a mathematical formula}count{Xi|ϕi(Xi)}[M] is the value of the count feature (10) in {a mathematical formula}M.The same weight function on models is defined by the discriminant function on the TET{a mathematical formula} with the weight assignment {a mathematical formula}β=(1,ew1,…,ewn).MLNs whose formulas {a mathematical formula}ϕi are arbitrary quantifier-free formulas also can be emulated by a TET discriminant function. For this one may write {a mathematical formula}ϕi in a disjunctive normal form {a mathematical formula}⋁j⋀klijk with literals {a mathematical formula}lijk, such that the individual disjuncts {a mathematical formula}ϕij:=⋀klijk are mutually exclusive. Then the same construction as above applied to all formulas {a mathematical formula}ϕij with associated weights {a mathematical formula}wi yields a discriminant function representation of the MLN weight function. This representation, however, may now be of a size that is exponential in the length of the original formulas {a mathematical formula}ϕi.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      TET metric
     </section-title>
     <paragraph>
      After the simple discriminant function of the previous section, we now introduce our second tool to build predictive and descriptive models directly on TET-defined features. This consists of the definition of a metric on the value space {a mathematical formula}V(T) of a TET T.
     </paragraph>
     <paragraph>
      The metric is defined by induction on the structure of {a mathematical formula}V(T). Following Definition 2.5, the base case is:
     </paragraph>
     <list>
      <list-item label="•">
       If {a mathematical formula}V(T)={t,f}, define {a mathematical formula}dtet(t,f)=1, {a mathematical formula}dtet(t,t)=dtet(f,f)=0.
      </list-item>
     </list>
     <paragraph>
      The core of the induction step for {a mathematical formula}dtet consists of the specification of the distance between two multisets of values{a mathematical formula} where the {a mathematical formula}γi, {a mathematical formula}γj′ all come from a value space {a mathematical formula}V(T˜) of some sub-TET {a mathematical formula}T˜ of T, and {a mathematical formula}dtet is already defined on {a mathematical formula}V(T˜).
     </paragraph>
     <paragraph>
      After normalizing the counts to probability values {a mathematical formula}pi:=ki/(k1+⋯+km), {a mathematical formula}pj′:=kj′/(k1′+⋯+kl′) we can view the two values as probability distributions on the metric space {a mathematical formula}(V(T˜),dtet). A standard way to define a metric on such distributions is the well-known Wasserstein–Kantorovich or Earth-Moverʼs Distance:
     </paragraph>
     <paragraph label="Definition 4.1">
      Let {a mathematical formula}μ=(γ1:p1,…,γm:pm), {a mathematical formula}μ′=(γ1′:p1′,…,γl′′:pl′) be two multisets over {a mathematical formula}V(T˜) with counts normalized to probability distributions p, {a mathematical formula}p′.Let {a mathematical formula}dtet be a metric on {a mathematical formula}V(T˜). The Wasserstein–Kantorovich distance between μ and {a mathematical formula}μ′ is{a mathematical formula} where the infimum is taken over all probability distributions q on {a mathematical formula}V(T˜)×V(T˜) whose marginal on the first component is equal to p, and whose marginal on the second component is equal to {a mathematical formula}p′. Note that {a mathematical formula}dWK is a valid metric if the “ground distance” {a mathematical formula}dtet is a valid metric [4].
     </paragraph>
     <paragraph label="Definition 4.2">
      We now define {a mathematical formula}dtet(γ,γ′) for {a mathematical formula}γ,γ′∈V(T): Let {a mathematical formula}V(T)={f}∪{t}×⨉i=1mmultisets(V(Ti)), and assume that {a mathematical formula}dtet is defined on {a mathematical formula}V(Ti){a mathematical formula}(i=1,…,m). Let {a mathematical formula}γ,γ′∈V(T). Depending on whether one or both of {a mathematical formula}γ,γ′ are f, we define {a mathematical formula}dtet(γ,γ′) as:{a mathematical formula} In the non-trivial case {a mathematical formula}γ,γ′≠f, we have {a mathematical formula}γ=(t,μ1,…,μm),γ′=(t,μ1′,…,μm′) with{a mathematical formula} Then{a mathematical formula} where {a mathematical formula}ω0,…,ωm&gt;0 are adjustable weight parameters with {a mathematical formula}∑iωi=1.
     </paragraph>
     <paragraph label="Proposition 4.3">
      For all T,{a mathematical formula}dtetis a metric on{a mathematical formula}V(T)with values in{a mathematical formula}[0,1].
     </paragraph>
     <paragraph label="Proof">
      The statement is clearly true for the base case {a mathematical formula}V(T)={t,f}.For the case {a mathematical formula}V(T)={f}∪{t}×⨉i=1mmultisets(V(Ti)) we have the induction hypothesis that {a mathematical formula}dtet is a metric with values in {a mathematical formula}[0,1] on {a mathematical formula}V(Ti) ({a mathematical formula}i=1,…,m). Then {a mathematical formula}dWK defined on multisets {a mathematical formula}(μi,μi′) over {a mathematical formula}V(Ti) is a metric for all {a mathematical formula}i=1,…,m, and hence the convex combination in (19) defines a metric on {a mathematical formula}V(T)∖{f}. By the condition {a mathematical formula}∑iωi=1 its values lie in the interval {a mathematical formula}[0,1].It remains to show that the extension via (18) to include the f case still satisfies the properties of a metric. {a mathematical formula}dtet(γ,γ′)⩾0 with equality only for {a mathematical formula}γ=γ′, and {a mathematical formula}dtet(γ,γ′)⩽1, as well as symmetry {a mathematical formula}dtet(γ,γ′)=dtet(γ′,γ) are clearly satisfied. For the triangle inequality, consider {a mathematical formula}dtet(γ,γ′)+dtet(γ′,γ″). If {a mathematical formula}γ=γ′=γ″=f, then the sum is zero, and equal to {a mathematical formula}dtet(γ,γ″). If one of {a mathematical formula}γ,γ′,γ″ is not f, then the sum is greater or equal 1, and thus greater or equal {a mathematical formula}dtet(γ,γ″).  □
     </paragraph>
     <paragraph>
      The TET metric defined above can be computed using the transportation simplex algorithm, a specialized linear programming algorithm for solving the transportation problem [37]. Ling and Okada [25] have introduced a faster algorithm for computing the Earth Moverʼs Distance between histogram. However that algorithm assumes fixed-size histograms and cannot deal with signatures of distributions, as required by the recursive definition of {a mathematical formula}dtet.
     </paragraph>
     <paragraph>
      In our experiments, we found the CPU time required for computing the distances to be negligible compared to CPU time for computing the TET-values. A simple theoretical analysis justifies this finding. Assume we want to calculate the distance between two TET values having (for simplicity) the same shape, uniform branching factor m, and height h. Let {a mathematical formula}n=mh the number of nodes in the TET value and assume the transportation simplex (which needs to be computed on each TET-value node) takes a polynomial time {a mathematical formula}O(mk) for some k (in [37]k was empirically found to be between 3 and 4). We therefore have the recurrence for the running time {a mathematical formula}T(n) of the TET-distance calculation:{a mathematical formula} By the master theorem, if {a mathematical formula}h&gt;k then {a mathematical formula}T(n)=O(n) and if {a mathematical formula}h&lt;k then {a mathematical formula}T(n)=O(nkh).
     </paragraph>
     <paragraph>
      We now illustrate some of the properties of the {a mathematical formula}dtet metric. Our first example uses simple propositional TETs to illustrate the flexibility of the {a mathematical formula}dtet metric that derives from varying TET structures.
     </paragraph>
     <paragraph label="Example 4.4">
      Consider the four TETs (a)–(d) from Example 2.8. Table 1 gave for the four possible configurations of entity pairs {a mathematical formula}a1, {a mathematical formula}a2 the associated TET values. Table 3 now shows the distance matrices obtained from evaluating {a mathematical formula}dtet on these values. For (c) and (d) uniform weights for the different branches have been used, i.e., {a mathematical formula}ωi=1/2 in (c), and {a mathematical formula}ωi=1/4 in (d). For better readability and ease of comparison, the rows and columns in these matrices are indexed by the {a mathematical formula}a1,a2-substructures, even though the entries in the table are, of course, a function of their values.We obtain the following characteristics of the distance function defined by the four TETs:
     </paragraph>
     <list>
      <list-item label="(a)">
       This is 0/1-distance to the “reference structure” {a mathematical formula}a1→a2: any pair {a mathematical formula}a1′, {a mathematical formula}a2′ that has a different structure has distance 1 to {a mathematical formula}a1→a2, and distance 0 to any other pair that also does not have the reference structure.
      </list-item>
      <list-item label="(b)">
       This metric identifies two structures {a mathematical formula}(a1,a2), {a mathematical formula}(a1′,a2′) (i.e., assigns zero distance between them), if neither contains the edge →. Otherwise, structures have distance 0 iff they are equal, and distance 1 else.
      </list-item>
      <list-item label="(c)">
       Here the TET metric becomes the (normalized) edit distance relative to the two primitive edit operations edge insertion and edge deletion.
      </list-item>
      <list-item label="(d)">
       This is a scaled 0/1 distance: two structures {a mathematical formula}(a1,a2), {a mathematical formula}(a1′,a2′) have a constant distance &gt;0 iff they are different. Note that two distinct structures have distance 1/2 rather than 1, because their values agree on the two out of four TET branches that evaluate to f for both of them. The two other branches each return a distance of 1, which with the {a mathematical formula}ωi=1/4 weights gives a total distance 1/2.
      </list-item>
     </list>
     <paragraph label="Example 4.5">
      Table 4(a) gives the distances between the values {a mathematical formula}γi shown in (6), i.e. the distances between the authors {a mathematical formula}ai in Fig. 1 defined by the TET (1).The matrix shows that according to {a mathematical formula}dtet there are two clusters {a mathematical formula}{a1,a4} and {a mathematical formula}{a2,a3,a5} of authors: the distances between authors within each of these groups is about one order of magnitude smaller than the distance between authors from different groups.Comparing with Fig. 1 one finds that the clusters are defined by the number of papers written by an author: two for the first cluster, and three for the second. Given the difference in the number of authored papers, the citation distribution has a secondary influence on the distance value: thus, for example, {a mathematical formula}d(a1,a2)&lt;d(a1,a5), because the citation pattern of the two papers of {a mathematical formula}a1 is more similar to the one of the three papers of {a mathematical formula}a2, than the three papers of {a mathematical formula}a5.
     </paragraph>
     <paragraph>
      The preceding example highlights a potential problem with the definition of {a mathematical formula}dtet: differences in top-level counts appear to have a dominating influence on the distance values. While it often will be reasonable that primary counts have a larger impact than counts in the lower levels of the TET, it may be desirable to control the extent to which this is happening. In the following we first analyze in a more general manner the distances obtained between certain TET values, and then introduce a method for adjusting the metric so that its behavior can be adapted to fit more closely the needs in specific applications.
     </paragraph>
     <paragraph>
      We consider the generic two-level count-of-count TET{a mathematical formula} Assume that the variables {a mathematical formula}W,U are typed, such that W ranges over a sub-domain of size K, and U ranges over a sub-domain of size N. We now consider values of the form{a mathematical formula} These values are symmetric in the sense that all W with {a mathematical formula}r(V,W) have the same number n of {a mathematical formula}s()-successors U (cf. also the values for {a mathematical formula}a2,a4,a5 in (6)). Assuming {a mathematical formula}K,N to be fixed, these values are fully characterized by the two parameters {a mathematical formula}n,k, and one can derive a closed-form expression for pairs of values of this form:{a mathematical formula}
     </paragraph>
     <paragraph>
      This expression explains several potential problems that were already visible in the example of Table 4(a): first, since typically {a mathematical formula}n,n′≪N and {a mathematical formula}k,k′≪K, the distances will tend to be very small numbers. Second, the distance is dominated by the difference {a mathematical formula}|k−k′|K in counts at the first level of the TET. Furthermore, the distance is sensitive to the sizes of the domains over which the variables range: the behavior of (20) as a function of the actual counts n, k, {a mathematical formula}n′, {a mathematical formula}k′ depends on {a mathematical formula}K,N, in particular on differences in order of magnitude ({a mathematical formula}K≪N or {a mathematical formula}N≪K). Note that the third issue is akin to the situation in standard attribute-value data, where certain numeric features may dominate a distance measure due to the order of magnitude of their measuring scale. We can address all these (potential) problems by introducing a normalization operation on TET values.
     </paragraph>
     <section label="4.1">
      <section-title>
       Value normalization
      </section-title>
      <paragraph>
       In analogy to standard normalization procedures for numeric data, we introduce a normalization operation for TET values. A standard normalization procedure for numeric data would be a linear transformation {a mathematical formula}x↦ax+b, where the coefficients a, b are such that the empirical distribution in the transformed dataset has mean 0 and variance 1. Note that here the concrete coefficients a, b depend on the dataset (the original empirical mean and variance of x), but that the normalization procedure in general is defined by the two “hyper-parameters” 0 and 1. Instead of standardizing all numeric attributes to 0 mean and variance 1, one could also assign different such hyper-parameters to different attributes, and thereby adjust the impact different attributes have on the overall distance function.
      </paragraph>
      <paragraph>
       For TET values, we will perform normalization by scaling f counts, i.e., replacing occurrences of {a mathematical formula}f:k by {a mathematical formula}f:ak for some a. The normalization is guided by data-independent hyper-parameters that can be adjusted to optimize the behavior of the TET metric {a mathematical formula}dtet for specific purposes. The concrete multiplicative factors a will then depend on the hyper-parameters, and the empirical distribution of TET values for which the normalization is performed. The hyper-parameters are defined by a normalization labeling in the sense of the following definition.
      </paragraph>
      <paragraph label="Definition 4.6">
       A normalization labeling for a TET{a mathematical formula} is given by a vector {a mathematical formula}(y1,…,ym) of non-negative real numbers, and a normalization labeling for each of the sub-TETs {a mathematical formula}Ti ({a mathematical formula}i=1,…,m).
      </paragraph>
      <paragraph label="Definition 4.7">
       In the following, we assume for notational convenience {a mathematical formula}m=1, i.e., {a mathematical formula}T=[τ(V),(W,T′(V,W))]. Let {a mathematical formula}T=[τ(V),(W,T′(V,W))], and {a mathematical formula}(y,y′) be a normalization labeling ({a mathematical formula}y′ a normalization labeling for {a mathematical formula}T′). Let {a mathematical formula}Γ={γ1,…,γn}⊆V(T)∖{f}.We write the {a mathematical formula}γi as{a mathematical formula} with {a mathematical formula}γi,j≠f.Define{a mathematical formula}The normalization of {a mathematical formula}γi in Γ with hyper-parameters {a mathematical formula}(y,y′) is now given by:
      </paragraph>
      <list>
       <list-item label="•">
        replacing {a mathematical formula}kif by {a mathematical formula}y(kavg≠f/kavgf)kif.
       </list-item>
       <list-item label="•">
        replacing each {a mathematical formula}γi,j by the normalization of {a mathematical formula}γi,j in {a mathematical formula}Γ′:={γi,j|i=1,…,n;j=1,…,li} with hyper-parameters {a mathematical formula}y′.
       </list-item>
      </list>
      <paragraph>
       A normalization parameter y specifies the ratio of total f to non-f counts in all the values in the given dataset corresponding to the branch labeled with y.
      </paragraph>
      <paragraph label="Example 4.8">
       A normalization labeling for our basic bibliographic TET is{a mathematical formula}The hyper-parameters ({a mathematical formula}0.1,1.0) were found in our experiments on predicting the h-index in the DBLP dataset (cf. Section 6.1). Normalizing the dataset consisting of the 5 values in (6) gives the normalized values:{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}Observe that {a mathematical formula}0.1=(0.281+0.246+0.246+0.281+0.246)/(2+3+3+2+3), and {a mathematical formula}1=(2⋅0.878+3⋅0.78+4⋅0.976)/(1+2+1+2+2).The distance matrix obtained for these normalized values is shown in Table 4(b). One immediately sees that now the range of distance values is more spread out in the interval [{a mathematical formula}0,1], and that the clustering according to paper count has disappeared. The most dis-similar authors now (as before) are {a mathematical formula}a1 and {a mathematical formula}a5 (few papers with many citations vs. many papers without citations). However, the most similar authors now are {a mathematical formula}a3 and {a mathematical formula}a4, who previously even belonged to different clusters. Seeing that {a mathematical formula}a3 differs from {a mathematical formula}a4 only by the addition of one paper without citations, it makes intuitive sense that the distance measure optimized for predicting the h-index sees them as nearest neighbors.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      TET learning
     </section-title>
     <paragraph>
      We first describe in more detail the learning problem we want to solve. Our data consists of a model {a mathematical formula}M in the sense of Definition 2.1. In our implementation, {a mathematical formula}M is given as a relational database containing one table for each {a mathematical formula}r∈R, where the table for r contains all tuples {a mathematical formula}a∈Marity(r) for which {a mathematical formula}I(r(a))=true. Furthermore, we are given an initial target table, i.e. a table consisting of a set of examples with {a mathematical formula}+/− class labels. For example, a learning problem given by the data depicted in Fig. 1, with {a mathematical formula}a1, {a mathematical formula}a3, {a mathematical formula}a4 as positive and {a mathematical formula}a2, {a mathematical formula}a5 as negative examples would be given by the 4 leftmost tables in Table 5. Columns in the data tables are headed by synthetic identifiers {a mathematical formula}Argi. Columns in the target table (other than the class label column) are headed by variable names, which will then become the names of the free variables in the TET we construct. Thus, given the input we will want to construct a TET {a mathematical formula}T(A) over the signature {a mathematical formula}R={author,paper,authorOf,cites} that captures features of A that are predictive for the class label given in target.
     </paragraph>
     <paragraph>
      Our general approach to TET learning is a recursive top-down construction that associates with each node a local discrimination task represented by a local target table. In our running example, starting with the input data in Table 5 we would initialize the TET construction with the vacuous TET {a mathematical formula}⊤(A). If the first extension was {a mathematical formula}⊤(A)→PauthorOf(A,P), then we would associate with the node {a mathematical formula}authorOf(A,P) the local target table shown in Table 6. This construction of local target tables is essentially the same as the construction of local training sets in FOIL [34]. The construction of this new target table amounts to a problem transformation: the problem of predicting the label of an author is transformed into predicting the label of author/paper pairs in the new target table, which may be effected by taking into consideration attributes of both authors and papers, as well as additional relations between authors and papers (if any such exist in the data).
     </paragraph>
     <paragraph>
      The exact specification of the construction of local target tables is as follows. Let n be a TET node associated with a local target table {a mathematical formula}ttn(V,L) with columns for variables V and label L. Let {a mathematical formula}n′ be a child of n labeled with type {a mathematical formula}σ(V,W), and reached by an edge labeled with variables W (we include the possibility that {a mathematical formula}|W|=0, i.e. the edge is really unlabeled). Then {a mathematical formula}n′ is associated with a target table {a mathematical formula}ttn′ with columns for {a mathematical formula}V,W and L, defined as:{a mathematical formula} In the case {a mathematical formula}|W|=0, {a mathematical formula}ttn′ is just the subset of {a mathematical formula}ttn containing the elements for which {a mathematical formula}σ(a) is true.
     </paragraph>
     <paragraph>
      When building the TET, candidate tree-extensions {a mathematical formula}…→Wσ(V,W) are scored based on the relational information gain (RIG) measure proposed in [26]. RIG values represent both direct and potential informativeness of the extension: direct informativeness is provided by extensions that in one step increase the class-purity of the local target table. Potential informativeness is provided by extensions that introduce new entities into the target table which in subsequent steps might enable discrimination between positive and negative examples via additional features related to the new entities.
     </paragraph>
     <paragraph>
      High RIG values (unlike information gain in decision tree learning, for example), thus, do not give bounds on a guaranteed improvement of classification accuracy in a single construction step, but may only indicate a potential improvement that could be obtained by further construction steps. After termination of a recursive sub-tree construction, therefore, the final predictive accuracy gain of the sub-tree is evaluated, and the sub-tree is pruned if this gain does not exceed a given threshold. The evaluation of the current TETʼs accuracy has to be based on a concrete classification model built on the TET feature. It is here that we crucially use the discriminant function model of Section 3: a weight assignment defining the discriminant function is very fast to learn, and the resulting discriminant function values on the validation set are fast to compute. The overall wrapper-evaluation with the discriminant function, thus, is computationally very efficient. If the final TET is to be used in conjunction with a different classification model than the discriminant function, then it could be beneficial to already use that classification model in the wrapper-evaluation during TET learning. However, for complex models this can be computationally very expensive. Furthermore, it does not appear to be the case that TET features learned using the discriminant function are highly biased towards this particular classification model. The discriminant function, thus, can play the role of a lightweight classification model that may be used to discover features which are also useful for more complex model types, similarly as in the propositional case decision tree or logistic regression model learning can act as a feature selector also for subsequent use in more complex models, like support vector machines (cf. Fig. 2).
     </paragraph>
     <paragraph>
      Table 7 outlines the TET learning algorithm. It is implemented as a procedure that recursively expands an initial TET. It receives as arguments the data, a local target table tt to be classified, a pointer parent to the current node to be expanded and a pointer to the root of the TET being constructed. The initial call is{a mathematical formula} where {a mathematical formula}T=newTET_Node(⊤(V)) is a pointer to an initial TET with vacuous root type {a mathematical formula}⊤(V). The construction works as follows: Line 1 sets the weight for the discriminant function {a mathematical formula}d+ for the input node. It is just the relative frequency of positive examples in the target table associated with this node (the weight for {a mathematical formula}d− being one minus this weight). Thus, the discriminant function here is learned (at little extra cost) in parallel with the TET construction.
     </paragraph>
     <paragraph>
      The function {a mathematical formula}predictive_score(M,root) called in lines 2 and 11 performs the global evaluation of the current TET based on its predictive performance in conjunction with the chosen classification model. If a model other than the discriminant function here is used, then calls to {a mathematical formula}predictive_score(M,root) may require computationally expensive model training for the current TET.
     </paragraph>
     <paragraph>
      Lines 3–5 are crucial: here a subset of all the possible extensions of the current node defined by types {a mathematical formula}σ(V,W) of child nodes is constructed for further exploration. This operation is analogous to refinement operators in ILP. Our construction is in two steps: in the first step the set of possible extensions for the current node is constructed by the function possible_extensions. This function can implement various constraints and a language bias. In our implementation and experiments, we restrict possible extensions in terms of the number of literals and the number of new variables in {a mathematical formula}σ(V,W) (mostly limiting both numbers to at most one). The function can also take TILDE-style user-defined rmode declarations [3], that can force certain arguments of the new literal to be filled with variables already present in the parent node (input variable), or with a new variable introduced by this extension (output variable). As common in ILP algorithms, type predicates can be used to specify the type of arguments of the literals. In this case the learner expects a unary predicate for each variable type, being true for all and only the objects of that type. In addition to this user-defined bias, we force candidate extensions to use at least one of the latest introduced variables along their path to the root. The rationale for this constraint is that extensions introducing new variables were selected based on their RIG score and thus likely on their potential rather than direct informativeness. By focusing the search toward further refinements of the new variables, we force the algorithm to try making this potential informativeness explicit. When introducing a new variable, the algorithm automatically adds inequality constraints guaranteeing that they cannot be bound to the same value as that of any of the root variables, i.e. those identifying the entity the TET will represent features of. In case of a typed language bias, inequality constraints are added for variables of the same type only. In order to control the computational cost of adding new variables, we constraint the number of variables in each path from the root to a leaf to be within a user-defined maximum value ({a mathematical formula}θvars). Finally, possible_extensions is used to implement a termination condition: if the depth of the current parent node in the TET has reached a (user specified) maximum depth ({a mathematical formula}θdepth), then possible_extensions will return an empty set. In the next step, the relational information gain is computed for all possible extensions; the function candidate_extensions then performs a selection based on RIG values. Our current implementation of candidate_extensions selects all extensions whose RIG value exceeds a user defined threshold ({a mathematical formula}θRIG).
     </paragraph>
     <paragraph>
      A child node is then created for each candidate extension. The function construct_tt constructs the local target table for the child according to (21). Lines 8–9 add a new child labeled with the current candidate extension {a mathematical formula}σ(V,W) to parent. Line 10 continues the recursive construction at the new child, which then becomes the root of a whole new subtree. Lines 11–14 then evaluate the extension of the old TET with this new subtree, and either accept or reject it based on a user defined threshold for the required global score improvement ({a mathematical formula}θscore).
     </paragraph>
    </section>
    <section label="6">
     <section-title>
      Experiments
     </section-title>
     <section label="6.1">
      <section-title>
       DBLP
      </section-title>
      <paragraph>
       As already stated in the introduction, bibliometrics represents an ideal domain to test the capability of TETs of learning count-of-counts features: in particular, we focus on the task of predicting h-indices. The h-index of an author A is defined as the maximum number h such that A authored h papers having at least h citations each [14].
      </paragraph>
      <paragraph>
       The data set used in our experiments is taken from the DBLP Computer Science Bibliography [24]{sup:2} enhanced with citation data [41]. We extracted a set of facts and relations in the form of a MySQL database from the original data set available at http://www.arnetminer.org/citation. For the sake of reproducibility, we also provide a package with the scripts used to build the data set used in our experiments (see Supplementary Materials). From the original database we extracted the following tables:
      </paragraph>
      <list>
       <list-item label="•">
        author(A) providing the id of each author;
       </list-item>
       <list-item label="•">
        paper(P) providing the id of each paper;
       </list-item>
       <list-item label="•">
        author_of({a mathematical formula}A,P), true if A is an author of P;
       </list-item>
       <list-item label="•">
        cites({a mathematical formula}P1,P2), true if {a mathematical formula}P1 cites {a mathematical formula}P2.
       </list-item>
      </list>
      <paragraph>
       Our first goal was to learn a TET able to discriminate between authors with high and low h-index (i.e., above/below a certain threshold). To this end, we extracted a sub-graph of the whole DBLP network (see Supplementary Materials) consisting of 8726 authors and 244,265 papers, and used the learning algorithm described in Table 7, employing relational information gain [26] as the scoring function guiding the search, and the discriminant function of Definition 3.1 to evaluate the TET score after the introduction of each new literal.{sup:3} We chose an h-index threshold {a mathematical formula}h=7 to define positive and negative examples. We fixed {a mathematical formula}θRIG=0, so that each candidate extension with non-zero RIG was considered (in the order given by the RIG score), {a mathematical formula}θscore=1e−4 in order to prune away branches with low improvement, {a mathematical formula}θdepth=2 and {a mathematical formula}θvars=3. Fig. 5 shows the learned TET: it consists of three different branches, the first corresponding to the count-of-counts feature which can be used to exactly compute the h-index, and the two other branches describing features which are also correlated to the h-index of author A: the number of papers {a mathematical formula}P′ cited by each paper P written by A (second branch), and the number of co-authors {a mathematical formula}A′ for each paper P of A (third). One may wonder why the learner did not return a TET consisting only of the first branch, since this feature would be sufficient to predict the h-index in the training examples exactly. Note however that the classification accuracy on the training examples is also constrained by the prediction model that maps the TET feature value into a binary classification. The discriminant function used by the learner (as well as virtually any other conceivable prediction model) is not able to exactly map the feature value obtained from the first branch into the binary threshold function {a mathematical formula}h&gt;7, and therefore additional features represented by the additional branches can still be useful for obtaining a better fit to this threshold.
      </paragraph>
      <paragraph>
       The TET shown in Fig. 5 in combination with the discriminant function guiding the learning phase achieves an F1 of 61.3% for the binary classification task of predicting authors with {a mathematical formula}h&gt;7. As a comparison, we employed TILDE [3] to inductively learn a logical decision tree in the same setting. The task turned out to be quite difficult for TILDE: being the author_of({a mathematical formula}A,P) predicate only potentially informative but not directly informative[26], with the same language bias used by our TET learner, TILDE ended up the search with an empty tree. We therefore tried to modify the language bias, by allowing the joint introduction of the predicates author_of({a mathematical formula}A,P) and cites({a mathematical formula}P1,P2): the result was a quite complex tree learned by TILDE, which anyhow contained only three positive leaves covering a few examples, heading to an F1 of 1.8 %. We also tried to define some aggregates{sup:4} in the language bias, counting the number of papers of an author and the number of citations of a paper. We used the TILDE option which allows the introduction of multiple aggregates within the same tree branch,{sup:5} but in this case the search could not be completed due to memory requirements. We finally tried to use exhaustive lookahead, which turned out to be computationally very expensive: with only one level of lookahead, TILDE ran for over 20 days without terminating. As a comparison, the TET learning algorithm ran for about 7 minutes. Finally, in order to assess the potential of the TET metric introduced in Section 4, we also tested our TET with a k-NN classifier in a leave-one-out setting, using the TET metric between TET values: in this case, the F1 achieved by the classifier, even without value normalization, was 88.5% with a single neighbor, and up to 91.2 with {a mathematical formula}k=5.
      </paragraph>
      <paragraph>
       The second experiment is about h-index forecasting: given data up to a given year {a mathematical formula}Y0, we predict the h-index of an author in the forthcoming years. In this case, our aim was to measure the discriminative power of the metric defined on TET values and described in Section 4, with respect to several baselines and other prediction models using plain counts. The experimental framework is constructed as follows: first, we extracted from the whole DBLP the set of 8441 authors having h-index &gt;3 in {a mathematical formula}Y0=2000, and then split this set into 2/3 for the development set, and 1/3 for the test set (the development set was again split in 2/3 for training and 1/3 for validation). Our predictor was built as follows: using just the simple TET shown in the first branch of Fig. 5, which describes all the sufficient features to calculate h-index, we computed TET values for all the authors, and then run a simple k-NN algorithm employing the TET metric as distance between such values. This predictor was compared against several different competitors:
      </paragraph>
      <list>
       <list-item label="1.">
        predict future h-index as equal to current h-index (SAME);
       </list-item>
       <list-item label="2.">
        predict future h-index of a test author a as the average of future h-indices of training authors having current h-index equal to a (AVFUT);
       </list-item>
       <list-item label="3.">
        predict future h-index using a k-NN algorithm, using as distance a linear combination of plain counts features, that is the number of papers {a mathematical formula}npap and the number of citations {a mathematical formula}ncit ({a mathematical formula}hpred=wpap×npap+wcit×ncit) (k-NN counts);
       </list-item>
       <list-item label="4.">
        predict future h-index as a non-linear Support Vector Regressor taking plain counts features {a mathematical formula}npap and {a mathematical formula}ncit as input (SVR);
       </list-item>
       <list-item label="5.">
        predict future h-index using a k-NN algorithm with the TET metric, but using the whole learned TET (represented in Fig. 5) to compute TET values (complete TET k-NN).
       </list-item>
      </list>
      <paragraph>
       Note that albeit very simple, the SAME and AVFUT predictors have a significant advantage over the other methods, since they make direct use of the h-index as a materialized feature in the data, whereas the other methods only are given underlying paper and citation counts.
      </paragraph>
      <paragraph>
       The validation set was used to perform model selection over the parameters of each model: regularization parameter C and Gaussian kernel width γ were tuned for SVR (C ranges in {a mathematical formula}10−2,…,102, while γ ranges in {a mathematical formula}10−4,…,1); {a mathematical formula}wpap, {a mathematical formula}wcit (both ranging in {a mathematical formula}10−2,…,102) and the number of neighbors k for the counts-based k-NN; the normalization coefficients y and the number of neighbors k for k-NN with TET metric.{sup:6} Root Mean Squared Error (RMSE) was used to measure the performance of each predictor.
      </paragraph>
      <paragraph>
       Fig. 6 shows the results obtained as a function of the prediction horizon H over years, starting from 2000 – the year for which TET values were computed, and therefore corresponding to {a mathematical formula}H=0 – up to 2009 ({a mathematical formula}H=9).
      </paragraph>
      <paragraph>
       The SAME and AVFUT predictors, by construction, have 0 error at {a mathematical formula}H=0, and remain very accurate for short prediction horizons, as the dynamics of h-indices change slowly over time. The TET based predictor has a lower average prediction error starting from {a mathematical formula}H&gt;5, and always outperforms methods based on plain counts of features. It should also be noticed that the k-NN algorithm based on the complete learned TET performs slightly better than the simple TET for longer prediction horizons. This happens because the TET metric, although taking into account count-of-counts features (and thus performing consistently better than plain counts-based metrics), is not able to exactly compute the h-index starting from these features.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Cora
      </section-title>
      <paragraph>
       CORA is a dataset of research papers and their citations, originally collected by Andrew McCallum and used for different predictive tasks including hierarchical classification, information extraction, and citation matching. Here we focus on this latter task, namely predicting whether two bibliographic records refer to the same paper. We rely on the relational data representation and experimental setting defined by Singla and Domingos [40].
      </paragraph>
      <paragraph>
       The domain consists of entities of types title, author, venue (given by a string value), bibrec (a bibliographic record given by its author, title and venue fields), as well as title_word, author_word, venue_word, which are the constituent words appearing in title, author, and venue. Only the first author of each record is considered in this setting. The data set contains 1295 bibliographic records, referring to 132 different research papers, 50 authors, and 103 venues. Relations are all representing part-of relationships: title_of, author_of, and venue_of link bibliographic records to their constituent fields; word_in_title, word_in_author, word_in_venue link complete field strings to their constituent words. Note that relations in the first group are one-to-one, whereas the second group is one-to-many.
      </paragraph>
      <paragraph>
       The experimental setting by Singla and Domingos [40] consists of a five fold cross validation procedure over plausible candidate pairs as identified using McCallum et al.ʼs canopy approach [28], with TF-IDF cosine as the similarity measure. This results in 52,923 overall candidate pairs, with 30,971 positive and 21,952 negative pairs respectively. The compiled dataset is available at alchemy.cs.washington.edu.
      </paragraph>
      <paragraph>
       We fixed {a mathematical formula}θRIG=0, as in the DBLP experiments, and {a mathematical formula}θscore=1e−2. We ran the TET learner on each of the five different training sets with increasing values of the main parameters controlling the size of the search space, {a mathematical formula}θdepth and {a mathematical formula}θvars. Table 8, Table 9 report F1 and Area under the recall-precision curve (AURPC) values for each fold and macro-averaged on the five folds. The simple discriminant function guiding the TET learning phase (DF) is compared to k-NN with the TET metric, for different values of {a mathematical formula}k&gt;1 (only values producing non-negligible differences are reported). We did not perform any fine tuning on the TET metric, leaving all hyper-parameters to one (see Section 4.1).
      </paragraph>
      <paragraph>
       The first apparent finding is that small TETs seem to perform quite well on this dataset, and increasing their complexity does not pay much. Fig. 7(a) shows the TET learned in four (1, 2, 4, 5) out of five folds for both ({a mathematical formula}θdepth=2, {a mathematical formula}θvars=1) and ({a mathematical formula}θdepth=3, {a mathematical formula}θvars=2) parameter settings and in two (2, 5) folds for the ({a mathematical formula}θdepth=4, {a mathematical formula}θvars=3) setting. This represents the very basic feature of the two records {a mathematical formula}B0, {a mathematical formula}B1 having identical title and/or venue fields. It is noteworthy that neither here, nor in any of the other TETs, features involving the author field were constructed. This may be due to the fact that the CORA dataset contains relatively many different publications by a relatively small number of (first) authors, so that the author field becomes a quite poor predictor for the identity of papers. Note that simple pairs with clearly different first authors were preliminarily excluded by the canopy construction [28] and are not part of the plausible candidates. The only difference between the two simpler learning settings is in fold three, where the ({a mathematical formula}θdepth=3, {a mathematical formula}θvars=2) setting learns the TET in Fig. 7(b), while the TET learned by the simplest setting ({a mathematical formula}θdepth=2, {a mathematical formula}θvars=1) lacks the {a mathematical formula}→TWword_in_title(T,TW) branch. The former achieves slightly better results, possibly because of a correlation between the number of title words and the likelihood that two entries refer to the same paper.
      </paragraph>
      <paragraph>
       Learning more complex TETs does not seem to provide improvements in this setting. However, an inspection on the learned TETs gives interesting insights on the potential of the mined features. Figs. 7(c) and (d) show linear branches that constitute the main features in two of the complex TETs. (c), at first, appears to be a fairly complex feature, which because of its three variable introductions would represent a three-level hierarchical count. However, due to the one-to-one nature of the relations in the extensions {a mathematical formula}→Vvenue_of(B0,V) and {a mathematical formula}→Ttitle_of(B,T), only the extension {a mathematical formula}→Bvenue_of(B,V) introduces real counts other than 0 and 1. Roughly speaking, the feature (c) counts the number of records B that have the same venue as {a mathematical formula}B0, and the same title as {a mathematical formula}B1. In both TETs containing the branch (c), also the dual branch with the roles of {a mathematical formula}B0 and {a mathematical formula}B1 interchanged was constructed. Intuitively, this feature uses the transitivity of the same_paper relation by considering “interpolating” records B for which there is evidence that they are equal to {a mathematical formula}B0 because of agreement in the venue field, and equal to {a mathematical formula}B1 because of agreement in the title field (or vice-versa). Note that the dataset includes 103 distinct venues for the 132 papers, which makes venue almost as discriminative as title.
      </paragraph>
      <paragraph>
       (d) is a refinement of the left branch of (a): in addition to testing equality of the title field, (d) also counts the number of additional records B that have the same title. It appears, however, that the introduction of the variable {a mathematical formula}T1 in the extension {a mathematical formula}→T1title_of(B,T1) here is redundant, since {a mathematical formula}T0 at this place is already established as the unique title of B, and so the simpler extension {a mathematical formula}→title_of(B0,T0) would express the same logical feature. The reason for the roundabout way (d) ends up taking for defining this feature lies in the fact that our learning algorithm includes a constraint that variables introduced at a previous extension (B in this case) should be used in the next child node (see Section 5).
      </paragraph>
      <paragraph>
       With the exception of the {a mathematical formula}→TWword_in_title(T,TW) branch of TET (b), all features discussed so far do not use the word-related relations. Comparisons are based on identity of whole title and venue strings. This is different in (e), which shows the complete TET learned for the third fold in the ({a mathematical formula}θdepth=4, {a mathematical formula}θvars=3) learning setting. It again is a refinement of the same-title feature, but this time in its right sub-branch also introducing counts of title words, and by means of the subsequent extension {a mathematical formula}→T1word_in_title(T1,TW)→title_of(B0,T1) a count-of-count feature that incorporates an inverse-document frequency feature (cf. TET (2) in Example 2.4). To understand the meaning of this branch, consider how the sub-TET rooted at {a mathematical formula}word_in_title(T0,TW) evaluates for a title word w: if w does not occur in the title of {a mathematical formula}B1, then the value is f. If w is in the title of {a mathematical formula}B1, but not in the title of {a mathematical formula}B0, then the value is {a mathematical formula}(t,{(t,{f}):k1,f:k2}), where {a mathematical formula}k1 and {a mathematical formula}k2 are the number of titles in the domain that contain, respectively do not contain, w. If, finally, w occurs both in the title of {a mathematical formula}B1 and {a mathematical formula}B0, then the value is {a mathematical formula}(t,{(t,{t}):k1,f:k2}), with the same {a mathematical formula}k1, {a mathematical formula}k2 as before. The branch {a mathematical formula}→TW⋯ then provides for each of these possible values the count of words w with that value. In this manner, the values of the right branch provide all the count-of-count statistics required for the inverse-document-frequency feature. Note that the more parsimonious representation (2) is not learnable by our current TET learner, since we only allow single-literal nodes.
      </paragraph>
      <paragraph>
       We see, thus, that the learned complex TETs represent very reasonable features. In order to better understand whether the lack of performance gain over the simpler TETs is possibly due to lack of predictive relevance of these features, or due to a difficulty for the TET metric to utilize the feature information in the way it is presented by these TETs, we manually designed a TET that encodes the idf-like feature represented by (e) in a way that is optimized for the TET metric (Fig. 8). It provides separate branches for words in each of the titles and each of the venues (the dashed triangle indicates a copy of the title subtree, with title replaced by venue). As shown in the last rows of Table 8, Table 9, the TET achieves quite high performance when paired with k-NN employing the TET-metric, with almost perfect AURPC in most folds, while the discriminant function again fails to exploit the full potential of counts-of-counts features.
      </paragraph>
      <paragraph>
       We can conclude that the TET learner is able to discover complex features with high discriminative value. The fact that some post-processing in the representation of the features was needed to obtain the best performance results with k-NN prediction may not be very surprising, since the learner is not optimizing with regard to the TET metric. In future work this kind of post-processing may be automated by implementing efficient techniques for optimizing the parameters of the TET metric (weight parameters ω and normalization labels y), and optimizing the TET structure using post-pruning and -balancing operations.
      </paragraph>
      <paragraph>
       The results of the learned TETs are comparable with those achieved with a Markov Logic Network (MLN) that (like our TET) is language independent, i.e. does not contain rules referring to specific strings occurring in the data, which achieves an AURPC of 0.971 [40]. Note that the MLN based approach in [40] – as well as more recent approaches achieving still higher accuracy [33], [39] – perform collective classification, and therefore can exploit the fact that the binary relation on bibliographic records that one predicts is an equivalence relation. The two classification models we have used both perform independent predictions for each pair of bibliographic records, and therefore cannot be expected to achieve results that are competitive with state-of-the-art collective approaches. It should be emphasized, though, that in [33], [39] the MLN structure (i.e. the set of logical formulas) was carefully designed by hand, while in our experiments the TET structure is learned from data. A carefully crafted TET as described in Fig. 8 indeed achieves a macro-averaged AURPC of around 0.99. Additionally, TET features could equally well be used in connection with collective classification techniques. We also compared TET results with those achievable by TILDE, with and without aggregates. As for the DBLP case, the search procedure of TILDE suffers from a lack of direct informativeness of single predicates, and plain TILDE returns an empty tree for all folds. However, exhaustive lookahead allows us to overcome the problem and recover the same rules of the simple TET in Fig. 7(a), thus achieving substantially equivalent results (macro averaged {a mathematical formula}F1=91.0%, slightly better than those of the simple TET as Tilde learns these rules for the third fold too). More complex features, like the idf-like ones in Fig. 7, Fig. 8 cannot be recovered by plain Tilde, and adding aggregates in the language bias concerning counts of author, title and venue words dramatically increases learning time: the search did not finish after a week of CPU time.
      </paragraph>
      <paragraph>
       As for inductive logic programming and relational rule learning approaches, learning time strongly depends on the constraints imposed on the search space, which is otherwise exponential in the number of candidate predicates. Learning TETs takes roughly three minutes, one hour and 20 hours respectively, on average over the five folds, for the three increasingly complex learning settings ({a mathematical formula}θdepth=2, {a mathematical formula}θvars=1), ({a mathematical formula}θdepth=3, {a mathematical formula}θvars=2), ({a mathematical formula}θdepth=4, {a mathematical formula}θvars=3). As a matter of comparison, Tilde learns the tree resembling TET in Fig. 7(a) in 30 seconds or 11 hours, depending on the number of exhaustive lookaheads allowed (one and two, respectively).
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      Properties of entities in a relational domain can depend on complex combinatorial count-of-count features characterizing the entitiesʼ relational neighborhood. Examples of properties that are directly defined in terms of count-of-count features are the h-index of an author, and certain relevance measures widely used in information retrieval. Type Extension Trees are a simple, but highly expressive representation language for count-of-count features. In this article we have presented a method for learning Type Extension Trees in supervised learning settings as a means of discovering count-of-count features that are informative for the prediction of a class label.
     </paragraph>
     <paragraph>
      Most existing frameworks for statistical relational learning either are only based on simpler, “flat”, count features, or their use of count-of-count features is only implicit in the specification of conditional probability distributions, and does not include an interpretable representation of the underlying features. Examples of frameworks of the first kind are Markov Logic Networks [36] (cf. Example 2.10), and systems providing simple aggregation operators [1], [13], [18]. Examples of frameworks of the second kind are probabilistic relational models that allow the specification of conditional probability distribution using nested combination functions [16], [30].
     </paragraph>
     <paragraph>
      Kernel methods can be also applied to (implicitly) extract features from relational data. The general framework of convolution kernels [12] has originated a wealth of different approaches for defining the similarity between structured objects (see e.g. [45] and references therein). Features defined by these kernels essentially count fragments or substructures, but not counts of counts. In most cases, these methods aim to develop a suitable representation of structured data for subsequent learning, not to discover features. There are previous works, however, where the feature space itself is learned from relational data [23], [29] and is interpretable in terms of definite clauses.
     </paragraph>
     <paragraph>
      In most of these previous works relational feature construction is an integral part of a particular learning paradigm. Relational features in their own right have previously been investigated in [32]. Here a systematic view of aggregation-based features at different levels of complexity is developed. However, the focus still is on aggregation over a single level of relational dependencies.
     </paragraph>
     <paragraph>
      Discovered TET features can be used in a variety of classification models, and could be integrated into existing models such as relational probability trees [31], or inductive logic programming systems, for which simpler types of count features have already been used [1]. In this paper we have considered two approaches for directly augmenting TET features into full prediction models. The simple discriminant function is fast to learn and evaluate, but only makes limited use of the count-of-count information provided by a TET feature value. We have therefore also introduced a metric on TET values defined by a recursive application of the Wasserstein–Kantorovich metric. With this metric, distance-based methods for supervised or unsupervised learning become directly applicable.
     </paragraph>
     <paragraph>
      Our experiments have shown that our TET learning algorithm is able to discover non-trivial and interpretable count-of-count features. A comparison of the classification accuracies achieved with the discriminant function model and k-nearest neighbor classification based on the TET metric indicates that TET features learned using the discriminant function can also support other classification models, and that a model that exploits the complex count-of-count information outperforms models only using flat counts.
     </paragraph>
    </section>
    <section label="8">
     <section-title>
      Supplementary material
     </section-title>
     <paragraph>
      The software for TET learning and for the computation of Wasserstein–Kantorovich metric between TET values, together with the data used in the experiments presented in this paper, can be downloaded at http://www3.diism.unisi.it/~lippi/research/TET.html.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>