<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    The dropout learning algorithm.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Dropout is a recently introduced algorithm for training neural networks [27]. In its simplest form, on each presentation of each training example, each feature detector unit is deleted randomly with probability {a mathematical formula}q=1−p=0.5. The remaining weights are trained by backpropagation [40]. The procedure is repeated for each example and each training epoch, sharing the weights at each iteration (Fig. 1.1). After the training phase is completed, predictions are produced by halving all the weights (Fig. 1.2). The dropout procedure can also be applied to the input layer by randomly deleting some of the input-vector components—typically an input component is deleted with a smaller probability (i.e. {a mathematical formula}q=0.2).
     </paragraph>
     <paragraph>
      The motivation and intuition behind the algorithm is to prevent overfitting associated with the co-adaptation of feature detectors. By randomly dropping out neurons, the procedure prevents any neuron from relying excessively on the output of any other neuron, forcing it instead to rely on the population behavior of its inputs. It can be viewed as an extreme form of bagging [17], or as a generalization of naive Bayes [23], as well as denoising autoencoders [42]. Dropout has been reported to yield remarkable improvements on several difficult problems, for instance in speech and image recognition, using well known benchmark datasets, such as MNIST, TIMIT, CIFAR-10, and ImageNet [27].
     </paragraph>
     <paragraph>
      In [27], it is noted that for a single unit dropout performs a kind of “geometric” ensemble averaging and this property is conjectured to extend somehow to deep multilayer neural networks. Thus dropout is an intriguing new algorithm for shallow and deep learning, which seems to be effective, but comes with little formal understanding and raises several interesting questions. For instance:
     </paragraph>
     <list>
      <list-item label="1.">
       What kind of model averaging is dropout implementing, exactly or in approximation, when applied to multiple layers?
      </list-item>
      <list-item label="2.">
       How crucial are its parameters? For instance, is {a mathematical formula}q=0.5 necessary and what happens when other values are used? What happens when other transfer functions are used?
      </list-item>
      <list-item label="3.">
       What are the effects of different deletion randomization procedures, or different values of q for different layers? What happens if dropout is applied to connections rather than units?
      </list-item>
      <list-item label="4.">
       What are precisely the regularization and averaging properties of dropout?
      </list-item>
      <list-item label="5.">
       What are the convergence properties of dropout?
      </list-item>
     </list>
     <paragraph>
      To answer these questions, it is useful to distinguish the static and dynamic aspects of dropout. By static we refer to properties of the network for a fixed set of weights, and by dynamic to properties related to the temporal learning process. We begin by focusing on static properties, in particular on understanding what kind of model averaging is implemented by rules like “halving all the weights”. To some extent this question can be asked for any set of weights, regardless of the learning stage or procedure. Furthermore, it is useful to first study the effects of droupout in simple networks, in particular in linear networks. As is often the case [8], [9], understanding dropout in linear networks is essential for understanding dropout in non-linear networks.
     </paragraph>
     <paragraph label="Related work">
      Here we point out a few connections between dropout and previous literature, without any attempt at being exhaustive, since this would require a review paper by itself. First of all, dropout is a randomization algorithm and as such it is connected to the vast literature in computer science and mathematics, sometimes a few centuries old, on the use of randomness to derive new algorithms, improve existing ones, or prove interesting mathematical results (e.g. [22], [3], [33]). Second, and more specifically, the idea of injecting randomness into a neural network is hardly new. A simple Google search yields dozen of references, many dating back to the 1980s (e.g. [24], [25], [30], [34], [12], [6], [37]). In these references, noise is typically injected either in the input data or in the synaptic weights to increase robustness or regularize the network in an empirical way. Injecting noise into the data is precisely the idea behind denoising autoencoders [42], perhaps the closest predecessor to dropout, as well as more recent variations, such as the marginalized-corrupted-features learning approach described in [29]. Finally, since the posting of [27], three articles with dropout in their title were presented at the NIPS 2013 conference: a training method based on overlaying a dropout binary belief network on top of a neural network [7]; an analysis of the adaptive regularizing properties of dropout in the shallow linear case suggesting some possible improvements [43]; and a subset of the averaging and regularization properties of dropout described primarily in Sections 8 and 11 of this article [10].
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Dropout for shallow linear networks
     </section-title>
     <paragraph>
      In order to compute expectations, we must associate well defined random variables with unit activities or connection weights when these are dropped. Here and everywhere else we will consider that a unit activity or connection is set to 0 when the unit or connection is dropped.
     </paragraph>
     <section label="2.1">
      <section-title>
       Dropout for a single linear unit (combinatorial approach)
      </section-title>
      <paragraph>
       We begin by considering a single linear unit computing a weighted sum of n inputs of the form{a mathematical formula} where {a mathematical formula}I=(I1,…,In) is the input vector. If we delete inputs with a uniform distribution over all possible subsets of inputs, or equivalently with a probability {a mathematical formula}q=0.5 of deletion, then there are {a mathematical formula}2n possible networks, including the empty network. For a fixed I, the average output over all these networks can be written as:{a mathematical formula} where {a mathematical formula}N is used to index all possible sub-networks, i.e. all possible edge deletions. Note that in this simple case, deletion of input units or of edges are the same thing. The sum above can be expanded using networks of size {a mathematical formula}0,1,2,…,n in the form{a mathematical formula} In this expansion, the term {a mathematical formula}wiIi occurs{a mathematical formula} times. So finally the average output is{a mathematical formula} Thus in the case of a single linear unit, for any fixed input I the output obtained by halving all the weights is equal to the arithmetic mean of the outputs produced by all the possible sub-networks. This combinatorial approach can be applied to other cases (e.g. {a mathematical formula}p≠0.5) but it is much easier to work directly with a probabilistic approach.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Dropout for a single linear unit (probabilistic approach)
      </section-title>
      <paragraph>
       Here we simply consider that the output is a random variable of the form{a mathematical formula} where {a mathematical formula}δi is a Bernoulli selector random variable, which deletes the weight {a mathematical formula}wi (equivalently the input {a mathematical formula}Ii) with probability {a mathematical formula}P(δi=0)=qi. The Bernoulli random variables are assumed to be independent of each other (in fact pairwise independence, as opposed to global independence, is sufficient for all the results to be presented here). Thus {a mathematical formula}P(δi=1)=1−qi=pi. Using the linearity of the expectation we have immediately{a mathematical formula} This formula allows one to handle different {a mathematical formula}pi for each connection, as well as values of {a mathematical formula}pi that deviate from 0.5. If all the connections are associated with independent but identical Bernoulli selector random variables with {a mathematical formula}pi=p, then{a mathematical formula} Thus note, for instance, that if the inputs are deleted with probability 0.2 then the expected output is given by {a mathematical formula}0.8∑iwiIi. Thus the weights must be multiplied by 0.8. The key property behind Eq. (8) is the linearity of the expectation with respect to sums and multiplications by scalar values, and more generally for what follows the linearity of the expectation with respect to the product of independent random variables. Note also that the same approach could be applied for estimating expectations over the input variables, i.e. over training examples, or both (training examples and subnetworks). This remains true even when the distribution over examples is not uniform.
      </paragraph>
      <paragraph>
       If the unit has a fixed bias b (affine unit), the random output variable has the form{a mathematical formula} The case where the bias is always present, i.e. when {a mathematical formula}δb=1 always, is just a special case. And again, by linearity of the expectation{a mathematical formula} where {a mathematical formula}P(δb=1)=pb. Under the natural assumption that the Bernoulli random variables are independent of each other, the variance is linear with respect to the sum and can easily be calculated in all the previous cases. For instance, starting from the most general case of Eq. (9) we have{a mathematical formula} with {a mathematical formula}qi=1−pi. S can be viewed as a weighted sum of independent Bernoulli random variables, which can be approximated by a Gaussian random variable under reasonable assumptions.
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       Dropout for a single layer of linear units
      </section-title>
      <paragraph>
       We now consider a single linear layer with k output units{a mathematical formula} In this case, dropout applied to input units is slightly different from dropout applied to the connections. Dropout applied to the input units leads to the random variables{a mathematical formula} whereas dropout applied to the connections leads to the random variables{a mathematical formula} In either case, the expectations, variances, and covariances can easily be computed using the linearity of the expectation and the independence assumption. When dropout is applied to the input units, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula}
      </paragraph>
      <paragraph>
       When dropout is applied to the connections, we get:{a mathematical formula}{a mathematical formula}{a mathematical formula} Note the difference in covariance between the two models. When dropout is applied to the connections, {a mathematical formula}Si and {a mathematical formula}Sl are entirely independent.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Dropout for deep linear networks
     </section-title>
     <paragraph>
      In a general feedforward linear network described by an underlying directed acyclic graph, units can be organized into layers using the shortest path from the input units to the unit under consideration. The activity in unit i of layer h can be expressed as:{a mathematical formula} Again, in the general case, dropout applied to the units is slightly different from dropout applied to the connections. Dropout applied to the units leads to the random variables{a mathematical formula} whereas dropout applied to the connections leads to the random variables{a mathematical formula}
     </paragraph>
     <paragraph>
      When dropout is applied to the units, assuming that the dropout process is independent of the unit activities or the weights, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer. This formula can be applied recursively across the entire network, starting from the input layer. Note that the recursion of Eq. (24) is formally identical to the recursion of backpropagation suggesting the use of dropout during the backward pass. This point is elaborated further at the end of Section 10. Note also that although the expectation {a mathematical formula}E(Sih) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the previous layers ({a mathematical formula}l&lt;h) matter. Therefore it coincides also with the expectation taken over only all the induced subnetworks of node i (comprising only nodes that are ancestors of node i).
     </paragraph>
     <paragraph>
      Remarkably, using these expectations, all the covariances can also be computed recursively from the input layer to the output layer, by writing {a mathematical formula}Cov(Sih,Si′h′)=E(SihSi′h′)−E(Sih)E(Si′h′) and computing{a mathematical formula} under the usual assumption that {a mathematical formula}δjlδj′l′ is independent of {a mathematical formula}SjlSj′l′. Furthermore, under the usual assumption that {a mathematical formula}δjl and {a mathematical formula}δj′l′ are independent when {a mathematical formula}l≠l′ or {a mathematical formula}j≠j′, we have in this case {a mathematical formula}E(δjlδj′l′)=pjlpj′l′, with furthermore {a mathematical formula}E(δjlδjl)=pjl. Thus in short under the usual independence assumptions, {a mathematical formula}E(SihSi′h′) can be computed recursively from the values of {a mathematical formula}E(SjlSj′l′) in lower layers, with the boundary conditions {a mathematical formula}E(IiIj)=IiIj for a fixed input vector (layer 0). The recursion proceeds layer by layer, from the input to the output layer. When a new layer is reached, the covariances to all the previously visited layers must be computed, as well as all the intralayer covariances.
     </paragraph>
     <paragraph>
      When dropout is applied to the connections, under similar independence assumptions, we get:{a mathematical formula} with {a mathematical formula}E(Sj0)=Ij in the input layer. This formula can be applied recursively across the entire network. Note again that although the expectation {a mathematical formula}E(Sih) is taken over all possible subnetworks of the original network, only the Bernoulli gating variables in the previous layers ({a mathematical formula}l&lt;h) matter. Therefore it is also the expectation taken over only all the induced subnetworks of node i (corresponding to all the ancestors of node i). Furthermore, using these expectations, all the covariances can also be computed recursively from the input layer to the output layer using a similar analysis to the one given above for the case of dropout applied to the units of a general linear network.
     </paragraph>
     <paragraph>
      In summary, for linear feedforward networks the static properties of dropout applied to the units or the connections using Bernoulli gating variables that are independent of the weights, of the activities, and of each other (but not necessarily identically distributed) can be fully understood. For any input, the expectation of the outputs over all possible networks induced by the Bernoulli gating variables is computed using the recurrence equations(24)and(26), by simple feedforward propagation in the same network where each weight is multiplied by the appropriate probability associated with the corresponding Bernoulli gating variable. The variances and covariances can also be computed recursively in a similar way.
     </paragraph>
    </section>
    <section label="4">
     <section-title>
      Dropout for shallow neural networks
     </section-title>
     <paragraph>
      We now consider dropout in non-linear networks that are shallow, in fact with a single layer of weights.
     </paragraph>
     <section label="4.1">
      <section-title>
       Dropout for a single non-linear unit (logistic)
      </section-title>
      <paragraph>
       Here we consider that the output of a single unit with total linear input S is given by the logistic sigmoidal function{a mathematical formula} Here and everywhere else, we must have {a mathematical formula}c⩾0 There are {a mathematical formula}2n possible sub-networks indexed by {a mathematical formula}N and, for a fixed input I, each sub-network produces a linear value {a mathematical formula}S(N,I) and a final output value {a mathematical formula}ON=σ(N)=σ(S(N,I)). Since I is fixed, we omit the dependence on I in all the following calculations. In the uniform case, the geometric mean of the outputs is given by{a mathematical formula} Likewise, the geometric mean of the complementary outputs ({a mathematical formula}1−ON) is given by{a mathematical formula} The normalized geometric mean (NGM) is defined by{a mathematical formula} The NGM of the outputs is given by{a mathematical formula} Now for the logistic function σ, we have{a mathematical formula} Applying this identity to Eq. (31) yields{a mathematical formula} where here {a mathematical formula}E(S)=∑NS(N)/2n. Or, in more compact form,{a mathematical formula} Thus with a uniform distribution over all possible sub-networks {a mathematical formula}N, equivalent to having i.i.d. input unit selector variables {a mathematical formula}δ=δi with probability {a mathematical formula}pi=0.5, the NGM is simply obtained by keeping the same overall network but dividing all the weights by two and applying σ to the expectation {a mathematical formula}E(S)=∑i=1nwi2Ii.
      </paragraph>
      <paragraph>
       It is essential to observe that this result remains true in the case of a non-uniform distribution over the subnetworks {a mathematical formula}N, such as the distribution generated by Bernoulli gating variables that are not identically distributed, or with {a mathematical formula}p≠0.5. For this we consider a general distribution {a mathematical formula}P(N). This is of course even more general than assuming the P is the product of n independent Bernoulli selector variables. In this case, the weighted geometric means are defined by:{a mathematical formula} and{a mathematical formula} and similarly for the normalized weighted geometric mean (NWGM){a mathematical formula} Using the same calculation as above in the uniform case, we can then compute the normalized weighted geometric mean NWGM in the form{a mathematical formula}{a mathematical formula} where here {a mathematical formula}E(S)=∑NP(N)S(N). Thus in summary with any distribution{a mathematical formula}P(N)over all possible sub-networks{a mathematical formula}N, including the case of independent but not identically distributed input unit selector variables{a mathematical formula}δiwith probability{a mathematical formula}pi, the NWGM is simply obtained by applying the logistic function to the expectation of the linear input S. In the case of independent but not necessarily identically distributed selector variables{a mathematical formula}δi, each with a probability{a mathematical formula}piof being equal to one, the expectation of S can be computed simply by keeping the same overall network but multiplying each weight{a mathematical formula}wiby{a mathematical formula}piso that{a mathematical formula}E(S)=∑i=1npiwiIi.
      </paragraph>
      <paragraph>
       Note that as in the linear case, this property of logistic units is even more general. That is for any set of {a mathematical formula}S1,…,Sm and any associated probability distribution {a mathematical formula}P1,…,Pm ({a mathematical formula}∑i=1mPi=1) and associated outputs {a mathematical formula}O1,…,Om (with {a mathematical formula}O=σ(S)), we have {a mathematical formula}NWGM(O)=σ(E)=σ(∑iPiSi). Thus the NVGM can be computed over inputs, over inputs and subnetworks, or over other distributions than the one associated with subnetworks, even when the distribution is not uniform. For instance, if we add Gaussian or other noise to the weights, the same formula can be applied. Likewise, we can approximate the average activity of an entire neuronal layer, by applying the logistic function to the average input of the neurons in that layer, as long as all the neurons in the layer use the same logistic function. Note also that the property is true for any c and λ and therefore, using the analyses provided in the next sections, it will be applicable to each of the units, in a network where different units have different values of c and λ. Finally, the property is even more general in the sense that the same calculation as above shows that for any function f{a mathematical formula} and in particular, for any k{a mathematical formula}
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Dropout for a single layer of logistic units
      </section-title>
      <paragraph>
       In the case of a single output layer of k logistic functions, the network computes k linear sums {a mathematical formula}Si=∑j=1nwijIj for {a mathematical formula}i=1,…,k and then k outputs of the form{a mathematical formula} The dropout procedure produces a subnetwork {a mathematical formula}M=(N1,…,Nk) where {a mathematical formula}Ni here represents the corresponding sub-network associated with the i-th output unit. For each i, there are {a mathematical formula}2n possible sub-networks for unit i, so there are {a mathematical formula}2kn possible subnetworks {a mathematical formula}M. In this case, Eq. (39) holds for each unit individually. If dropout uses independent Bernoulli selector variables {a mathematical formula}δij on the edges, or more generally, if the sub-networks {a mathematical formula}(N1,…,Nk) are selected independently of each other, then the covariance between any two output units is 0. If dropout is applied to the input units, then the covariance between two sigmoidal outputs may be small but non-zero.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Dropout for a set of normalized exponential units
      </section-title>
      <paragraph>
       We now consider the case of one layer of normalized exponential units. In this case, we can think of the network as having k outputs obtained by first computing k linear sums of the form {a mathematical formula}Si=∑j=1nwijIj for {a mathematical formula}i=1,…,k and then k outputs of the form{a mathematical formula} Thus {a mathematical formula}Oi is a logistic output but the coefficients of the logistic function depend on the values of {a mathematical formula}Sj for {a mathematical formula}j≠i. The dropout procedure produces a subnetwork {a mathematical formula}M=(N1,…,Nk) where {a mathematical formula}Ni represents the corresponding sub-network associated with the i-th output unit. For each i, there are {a mathematical formula}2n possible sub-networks for unit i, so there are {a mathematical formula}2kn possible subnetworks {a mathematical formula}M. We assume first that the distribution {a mathematical formula}P(M) is factorial, that is {a mathematical formula}P(M)=P(N1)…P(Nk), equivalent to assuming that the subnetworks associated with the individual units are chosen independently of each other. This is the case when using independent Bernoulli selector applied to the connections. The normalized weighted geometric average of output unit i is given by{a mathematical formula} Simplifying by the numerator{a mathematical formula} Factoring and collecting the exponential terms gives{a mathematical formula}{a mathematical formula}Thus with any distribution{a mathematical formula}P(N)over all possible sub-networks{a mathematical formula}N, including the case of independent but not identically distributed input unit selector variables{a mathematical formula}δiwith probability{a mathematical formula}pi, the NWGM of a normalized exponential unit is obtained by applying the normalized exponential to the expectations of the underlying linear sums{a mathematical formula}Si. In the case of independent but not necessarily identically distributed selector variables{a mathematical formula}δi, each with a probability{a mathematical formula}piof being equal to one, the expectation of{a mathematical formula}Sican be computed simply by keeping the same overall network but multiplying each weight{a mathematical formula}wiby{a mathematical formula}piso that{a mathematical formula}E(Si)=∑j=1npjwiIj.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Dropout for deep neural networks
     </section-title>
     <paragraph>
      Finally, we can deal with the most interesting case of deep feedforward networks of sigmoidal units,{sup:1} described by a set of equations of the form{a mathematical formula} Dropout on the units can be described by{a mathematical formula} using the selector variables {a mathematical formula}δjl and similarly for dropout on the connections. For each sigmoidal unit{a mathematical formula} and the basic idea is to approximate expectations by the corresponding NWGMs, allowing the propagation of the expectation symbols from outside the sigmoid symbols to inside.{a mathematical formula}More precisely, we have the following recursion:{a mathematical formula}{a mathematical formula}{a mathematical formula}Eqs.(52),(53), and(54)are the fundamental equations underlying the recursive dropout ensemble approximation in deep neural networks. The only direct approximation in these equations is of course Eq.(52)which will be discussed in more depth in Sections8and9. This equation is exact if and only if the numbers{a mathematical formula}Oihare identical over all possible subnetworks{a mathematical formula}N. However, even when the numbers{a mathematical formula}Oihare not identical, the normalized weighted geometric mean often provides a good approximation. If the network contains linear units, then Eq. (52) is not necessary for those units and their average can be computed exactly. The only fundamental assumption for Eq. (54) is independence of the selector variables from the activity of the units or the value of the weights so that the expectation of the product is equal to the product of the expectations. Under the same conditions, the same analysis can be applied to dropout gating variables applied to the connections or, for instance, to Gaussian noise added to the unit activities.
     </paragraph>
     <paragraph>
      Finally, we measure the consistency{a mathematical formula}C(Oih,I) of neuron i in layer h for input I by the variance {a mathematical formula}Var[Oih(I)] taken over all subnetworks {a mathematical formula}N and their distribution when the input I is fixed. The larger the variance is, the less consistent the neuron is, and the worse we can expect the approximation in Eq. (52) to be. Note that for a random variable O in {a mathematical formula}[0,1] the variance is bound to be small anyway, and cannot exceed 1/4. This is because {a mathematical formula}Var(O)=E(O2)−(E(O))2⩽E(O)−(E(O))2=E(O)(1−E(O))⩽1/4. The overall input consistency of such a neuron can be defined as the average of {a mathematical formula}C(Oih,I) taken over all training inputs I, and similar definitions can be made for the generalization consistency by averaging {a mathematical formula}C(Oih,I) over a generalization set.
     </paragraph>
     <paragraph>
      Before examining the quality of the approximation in Eq. (52), we study the properties of the NWGM for averaging ensembles of predictors, as well as the classes of transfer functions satisfying the key dropout NWGM relation ({a mathematical formula}NWGM(f(x))=f(E(x))) exactly, or approximately.
     </paragraph>
    </section>
    <section label="6">
     <section-title>
      Ensemble optimization properties
     </section-title>
     <paragraph>
      The weights of a neural network are typically trained by gradient descent on the error function computed using the outputs and the corresponding targets. The error functions typically used are the squared error in regression and the relative entropy in classification. Considering a single example and a single output O with a target t, these errors functions can be written as:{a mathematical formula} Extension to multiple outputs, including classification with multiple classes using normalized exponential transfer functions, is immediate. These error terms can be summed over examples or over predictors in the case of an ensemble. Both error functions are convex up (∪) and thus a simple application of Jensen's theorem shows immediately that the error of any ensemble average is less than the average error of the ensemble components. Thus in the case of any ensemble producing outputs {a mathematical formula}O1,…,Om and any convex error function we have{a mathematical formula} Note that this is true for any individual example and thus it is also true over any set of examples, even when these are not identically distributed. Eq. (56) is the key equation for using ensembles and for averaging them arithmetically.
     </paragraph>
     <paragraph>
      In the case of dropout with a logistic output unit the previous analyses show that the NWGM is an approximation to E and on this basis alone it is a reasonable way of combining the predictors in the ensemble of all possible subnetworks. However the following stronger result holds. For any convex error function, both the weighted geometric mean WGM and its normalized version NWGM of an ensemble possess the same qualities as the expectation. In other words:{a mathematical formula}{a mathematical formula}In short, for any convex error function, the error of the expectation, weighted geometric mean, and normalized weighted geometric mean of an ensemble of predictors is always less than the expected error.
     </paragraph>
     <paragraph label="Proof">
      Finally, it must also be pointed out that in the prediction phase once can also use expected values, estimated at some computational cost using Monte Carlo methods, rather than approximate values obtained by forward propagation in the network with modified weights.
     </paragraph>
    </section>
    <section label="7">
     <section-title>
      Dropout functional classes and transfer functions
     </section-title>
     <section label="7.1">
      <section-title>
       Dropout functional classes
      </section-title>
      <paragraph>
       Dropout seems to rely on the fundamental property of the logistic sigmoidal function {a mathematical formula}NWGM(σ)=σ(E). Thus it is natural to wonder what is the class of functions f satisfying this property. Here we show that the class of functions f defined on the real line with range in{a mathematical formula}[0,1]and satisfying{a mathematical formula}for any set of points and any distribution, consists exactly of the union of all constant functions{a mathematical formula}f(x)=Kwith{a mathematical formula}0⩽K⩽1and all logistic functions{a mathematical formula}f(x)=1/(1+ce−λx). As a reminder, G denotes the geometric mean and {a mathematical formula}G′ denotes the geometric mean of the complements. Note also that all the constant functions with {a mathematical formula}f(x)=K with {a mathematical formula}0⩽K⩽1 can also be viewed as logistic functions by taking {a mathematical formula}λ=0 and {a mathematical formula}c=(1−K)/K ({a mathematical formula}K=0 is a limiting case corresponding to {a mathematical formula}c→∞).
      </paragraph>
      <paragraph label="Proof">
       To prove this result, note first that the {a mathematical formula}[0,1] range is required by the definitions of G and {a mathematical formula}G′, since these impose that {a mathematical formula}f(x) and {a mathematical formula}1−f(x) be positive. In addition, any function {a mathematical formula}f(x)=K with {a mathematical formula}0⩽K⩽1 is in the class and we have shown that the logistic functions satisfy the property. Thus we need only to show these are the only solutions.By applying Eq. (59) to pairs of arguments, for any real numbers u and v with {a mathematical formula}u⩽v and any real number {a mathematical formula}0⩽p⩽1, any function in the class must satisfy:{a mathematical formula} Note that if {a mathematical formula}f(u)=f(v) then the function f must be constant over the entire interval {a mathematical formula}[u,v]. Note also that if {a mathematical formula}f(u)=0 and {a mathematical formula}f(v)&gt;0 then {a mathematical formula}f=0 in {a mathematical formula}[u,v). As a result, it is impossible for a non-zero function in the class to satisfy {a mathematical formula}f(u)=0, {a mathematical formula}f(v1)&gt;0, and {a mathematical formula}f(v2)&gt;0. Thus if a function f in the class is not constantly equal to 0, then {a mathematical formula}f&gt;0 everywhere. Similarly (and by symmetry), if a function f in the class is not constantly equal to 1, then {a mathematical formula}f&lt;1 everywhere.Consider now a function f in the class, different from the constant 0 or constant 1 function so that {a mathematical formula}0&lt;f&lt;1 everywhere. Eq. (60) shows that on any interval {a mathematical formula}[u,v]f is completely defined by at most two parameters {a mathematical formula}f(u) and {a mathematical formula}f(v). On this interval, by letting {a mathematical formula}x=pu+(1−p)v or equivalently {a mathematical formula}p=(v−x)/(v−u) the function is given by{a mathematical formula} or{a mathematical formula} with{a mathematical formula} and{a mathematical formula} Note that a particular simple parameterization is given in terms of{a mathematical formula} [As a side note, another elegant formula is obtained from Eq. (60) for {a mathematical formula}f(0) by taking {a mathematical formula}u=−v and {a mathematical formula}p=0.5. Simple algebraic manipulations give:{a mathematical formula}.] As a result, on any interval {a mathematical formula}[u,v] the function f must be: (1) continuous, hence uniformly continuous; (2) differentiable, in fact infinitely differentiable; (3) monotone increasing or decreasing, and strictly so if f is constant; (4) and therefore f must have well defined limits at −∞ and +∞. It is easy to see that the limits can only be 0 or 1. For instance, for the limit at +∞, let {a mathematical formula}u=0 and {a mathematical formula}v′=αv, with {a mathematical formula}0&lt;α&lt;1 so that {a mathematical formula}v′→∞ as {a mathematical formula}v→∞. Then{a mathematical formula} As {a mathematical formula}v′→∞ the limit must be independent of α and therefore the limit {a mathematical formula}f(v) must be 0 or 1.Finally, consider {a mathematical formula}u1&lt;u2&lt;u3. By the above results, the quantities {a mathematical formula}f(u1) and {a mathematical formula}f(u2) define a unique logistic function on {a mathematical formula}[u1,u2], and similarly {a mathematical formula}f(u2) and {a mathematical formula}f(u3) define a unique logistic function on {a mathematical formula}[u2,u3]. It is easy to see that these two logistic functions must be identical either because of the analycity or just by taking two new points {a mathematical formula}v1 and {a mathematical formula}v2 with {a mathematical formula}u1&lt;v1&lt;u2&lt;v2&lt;u3. Again {a mathematical formula}f(v1) and {a mathematical formula}f(v2) define a unique logistic function on {a mathematical formula}[v1,v2] which must be identical to the other two logistic functions on {a mathematical formula}[v1,u2] and {a mathematical formula}[u2,v2] respectively. Thus the three logistic functions above must be identical. In short, {a mathematical formula}f(u) and {a mathematical formula}f(v) define a unique logistic function inside {a mathematical formula}[u,v], with the same unique continuation outside of {a mathematical formula}[u,v].From this result, one may incorrectly infer that dropout is brittle and overly sensitive to the use of logistic non-linear functions. This conclusion is erroneous for several reasons. First, the logistic function is one of the most important and widely used transfer functions in neural networks. Second, regarding the alternative sigmoidal function {a mathematical formula}tanh(x), if we translate it upwards and normalize it so that its range is the [{a mathematical formula}0,1] interval, then it reduces to a logistic function since {a mathematical formula}(1+tanh(x))/2=1/(1+e−2x). This leads to the formula: {a mathematical formula}NWGM((1+tanh(x))/2)=(1+tanh(E(x)))/2. Note also that the NWGM approach cannot be applied directly to tanh, or any other transfer function which assumes negative values, since G and NWGM are defined for positive numbers only. Third, even if one were to use a different sigmoidal function, such as {a mathematical formula}arctan(x) or {a mathematical formula}x/1+x2, when rescaled to {a mathematical formula}[0,1] its deviations from the logistic function may be small and lead to fluctuations that are in the same range as the fluctuations introduced by the approximation of E by NWGM. Fourth and most importantly, dropout has been shown to work empirically with several transfer functions besides the logistic, including for instance {a mathematical formula}tanh and rectified linear functions. This point is addressed in more detail in the next section. In any case, for all these reasons one should not be overly concerned by the superficially fragile algebraic association between dropout, NWGMs, and logistic functions.
      </paragraph>
     </section>
     <section label="7.2">
      <section-title>
       Dropout transfer functions
      </section-title>
      <paragraph>
       In deep learning, one is often interested in using alternative transfer functions, in particular rectified linear functions which can alleviate the problem of vanishing gradients during backpropagation. As pointed out above, for any transfer function it is always possible to compute the ensemble average at prediction time using sampling. However, we can show that the ensemble averaging property of dropout is preserved to some extent also for rectified linear transfer functions, as well for broader classes of transfer functions.
      </paragraph>
      <paragraph>
       To see this, we first note that, while the properties of the NWGM are useful for logistic transfer functions, the NWGM is not needed to enable the approximation of the ensemble average by deterministic forward propagation. For any transfer function f, what is really needed is the relation{a mathematical formula}
      </paragraph>
      <paragraph>
       Any transfer function satisfying this property can be used with dropout and allow the estimation of the ensemble at prediction time by forward propagation. Obviously linear functions satisfy Eq. (68) and this was used in the previous sections on linear networks. A rectified linear function {a mathematical formula}RL(S) with threshold t and slope λ has the form{a mathematical formula} and is a special case of a piece-wise linear function. Eq. (68) is satisfied within each linear portion and will be satisfied around the threshold if the variance of S is small. Everything else being equal, smaller value of λ will also help the approximation. To see this more formally, assume without any loss of generality that {a mathematical formula}t=0. It is also reasonable to assume that S is approximately normal with mean {a mathematical formula}μS and variance {a mathematical formula}σS2—a treatment without this assumption is given in Appendix A. In this case,{a mathematical formula} On the other hand,{a mathematical formula} and thus{a mathematical formula} where Φ is the cumulative distribution of the standard normal distribution. It is well known that Φ satisfies{a mathematical formula} when x is large. This allows us to estimate the error in all the cases. If {a mathematical formula}μS=0 we have{a mathematical formula} and the error in the approximation is small and directly proportional to λ and σ. If {a mathematical formula}μS&lt;0 and {a mathematical formula}σS is small, so that {a mathematical formula}|μS|/σS is large, then {a mathematical formula}Φ(μS/σS)≈12πσS|μS|e−μS2/2σS2 and{a mathematical formula} And similarly for the case when {a mathematical formula}μS&gt;0 and {a mathematical formula}σS is small, so that {a mathematical formula}μS/σS is large. Thus in all these cases Eq. (68) holds. As we shall see in Section 11, dropout tends to minimize the variance {a mathematical formula}σS and thus the assumption that σ be small is reasonable. Together, these results show that the dropout ensemble approximation can be used with rectified linear transfer functions. It is also possible to model a population of RL neurons using a hierarchical model where the mean{a mathematical formula}μSis itself a Gaussian random variable. In this case, the error{a mathematical formula}E(RL(S))−RL(E(S))is approximately Gaussian distributed around 0. [This last point will become relevant in Section 9.]
      </paragraph>
      <paragraph>
       More generally, the same line of reasoning shows that the dropout ensemble approximation can be used with piece-wise linear transfer functions as long as the standard deviation of S is small relative to the length of the linear pieces. Having small angles between subsequent linear pieces also helps strengthen the quality of the approximation.
      </paragraph>
      <paragraph>
       Furthermore any continuous twice-differentiable function with small second derivative (curvature) can be robustly approximated by a linear function locally and therefore will tend to satisfy Eq.(68), provided the variance of S is small relative to the curvature.
      </paragraph>
      <paragraph>
       In this respect, a rectified linear transfer function can be very closely approximated by a twice-differentiable function by using the integral of a logistic function. For the standard rectified linear transfer function, we have{a mathematical formula} With this approximation, the second derivative is given by {a mathematical formula}σ′(S)=λσ(S)(1−σ(S)) which is always bounded by {a mathematical formula}λ/4.
      </paragraph>
      <paragraph>
       Finally, for the most general case, the same line of reasoning, shows that the dropout ensemble approximation can be used with any continuous, piece-wise twice differentiable, transfer function provided the following properties are satisfied: (1) the curvature of each piece must be small; (2){a mathematical formula}σSmust be small relative to the curvature of each piece. Having small angles between the left and right tangents at each junction point also helps strengthen the quality of the approximation. Note that the goal of dropout training is precisely to make {a mathematical formula}σS small, that is to make the output of each unit robust, independent of the details of the activities of the other units, and thus roughly constant over all possible dropout subnetworks.
      </paragraph>
     </section>
    </section>
    <section label="8">
     <section-title>
      Weighted arithmetic, geometric, and normalized geometric means and their approximation properties
     </section-title>
     <paragraph>
      To further understand dropout, one must better understand the properties and relationships of the weighted arithmetic, geometric, and normalized geometric means and specifically how well the NWGM of a sigmoidal unit approximates its expectation ({a mathematical formula}E(σ)≈NWGMS(σ)). Thus consider that we have m numbers {a mathematical formula}O1,…,Om with corresponding probabilities {a mathematical formula}P1,…,Pm ({a mathematical formula}∑i=1mPi=1). We typically assume that the m numbers satisfy {a mathematical formula}0&lt;Oi&lt;1 although this is not always necessary for the results below. Cases where some of the {a mathematical formula}Oi are equal to 0 or 1 are trivial and can be examined separately. The case of interest of course is when the m numbers are the outputs of a sigmoidal unit of the form {a mathematical formula}O(N)=σ(S(N)) for a given input {a mathematical formula}I=(I1,…,In). We let E be the expectation (weighted arithmetic mean) {a mathematical formula}E=∑i=1mPiOi and G be the weighted geometric mean {a mathematical formula}G=∏i=1mOiPi. When {a mathematical formula}0⩽Oi⩽1 we also let {a mathematical formula}E′=∑i=1mPi(1−Oi) be the expectation of the complements, and {a mathematical formula}G′=∏i=1m(1−Oi)Pi be the weighted geometric mean of the complements. Obviously we have {a mathematical formula}E′=1−E. The normalized weighted geometric mean is given by {a mathematical formula}NWGM=G/(G+G′). We also let {a mathematical formula}V=Var(O). We then have the following properties.
     </paragraph>
     <list>
      <list-item label="1.">
       The weighted geometric mean is always less or equal to the weighted arithmetic mean{a mathematical formula} with equality if and only if all the numbers {a mathematical formula}Oi are equal. This is true regardless of whether the number {a mathematical formula}Oi are bounded by one or not. This results immediately from Jensen's inequality applied to the logarithmic function. Although not directly used here, there are interesting bounds for the approximation of E by G, often involving the variance, such as:{a mathematical formula} with equality only if the {a mathematical formula}Oi are all equal. This inequality was originally proved by Cartwright and Field [20]. Several refinements, such as{a mathematical formula}{a mathematical formula} as well as other interesting bounds can be found in [4], [5], [31], [32], [1], [2].
      </list-item>
      <list-item label="2.">
       Since {a mathematical formula}G⩽E and {a mathematical formula}G′⩽E′=1−E, we have {a mathematical formula}G+G′⩽1, and thus {a mathematical formula}G⩽G/(G+G′) with equality if and only if all the numbers {a mathematical formula}Oi are equal. Thus the weighted geometric mean is always less or equal to the normalized weighted geometric mean.
      </list-item>
      <list-item label="3.">
       If the numbers {a mathematical formula}Oi satisfy {a mathematical formula}0&lt;Oi⩽0.5 (consistently low), then{a mathematical formula} [Note that if {a mathematical formula}Oi=0 for some i with {a mathematical formula}pi≠0, then {a mathematical formula}G=0 and the result is still true.] This is easily proved using Jensen's inequality and applying it to the function {a mathematical formula}lnx−ln(1−x) for {a mathematical formula}x∈(0,0.5]. It is also known as the Ky Fan inequality [11], [35], [36] which can also be viewed as a special case of the Levinson's inequality [28]. In short, in the consistently low case, the normalized weighted geometric mean is always less or equal to the expectation and provides a better approximation of the expectation than the geometric mean. We will see in a later section why the consistently low case is particularly significant for dropout.
      </list-item>
      <list-item label="4.">
       If the numbers {a mathematical formula}Oi satisfy {a mathematical formula}0.5⩽Oi&lt;1 (consistently high), then{a mathematical formula} Note that if {a mathematical formula}Oi=1 for some i with {a mathematical formula}pi≠0, then {a mathematical formula}G′=0 and the result is still true. In short, the normalized weighted geometric mean is greater or equal to the expectation. The proof is similar to the previous case, interchanging x and {a mathematical formula}1−x.
      </list-item>
      <list-item label="5.">
       Note that if {a mathematical formula}G/(G+G′) underestimates E then {a mathematical formula}G′/(G+G′) overestimates {a mathematical formula}1−E, and vice versa.
      </list-item>
      <list-item label="6.">
       This is the most important set of properties. When the numbers {a mathematical formula}Oi satisfy {a mathematical formula}0&lt;Oi&lt;1, to a first order of approximation we have{a mathematical formula} Thus to a first order of approximation the WGM and the NWGM are equally good approximations of the expectation. However the results above, in particular property 3, lead one to suspect that the NWGM may be a better approximation, and that bounds or estimates ought to be derivable in terms of the variance. This can be seen by taking a second order approximation, which gives{a mathematical formula} with the differences{a mathematical formula} and{a mathematical formula} The difference {a mathematical formula}|E−NWGM| is small to a second order of approximation and over the entire range of values of E. This is because either E is close to 0.5 and then the term {a mathematical formula}1−2E is small, or E is close to 0 or 1 and then the term V is small. Before we provide specific bounds for the difference, note also that if {a mathematical formula}E&lt;0.5 the second order approximation to the NWGM is below E, and vice versa when {a mathematical formula}E&gt;0.5.Since {a mathematical formula}V⩽E(1−E), with equality achieved only for 0–1 Bernoulli variables, we have{a mathematical formula} The inequalities are optimal in the sense that they are attained in the case of a Bernoulli variable with expectation E. The function {a mathematical formula}E(1−E)|1−2E|/[1−2E(1−E)] is zero for {a mathematical formula}E=0, 0.5, or 1, and symmetric with respect to {a mathematical formula}E=0.5. It is convex down and its maximum over the interval {a mathematical formula}[0,0.5] is achieved for {a mathematical formula}E=0.5−5−2/2 (Fig. 8.1). The function {a mathematical formula}2E(1−E)|1−2E| is zero for {a mathematical formula}E=0, 0.5, or 1, and symmetric with respect to {a mathematical formula}E=0.5. It is convex down and its maximum over the interval {a mathematical formula}[0,0.5] is achieved for {a mathematical formula}E=0.5−3/6 (Fig. 8.2). Note that at the beginning of learning, with small random weights initialization, typically E is close to 0.5. Towards the end of learning, E is often close to 0 or 1. In all these cases, the bounds are close to 0 and the NWGM is close to E.Note also that it is possible to have {a mathematical formula}E=NWGM even when the numbers {a mathematical formula}Oi are not identical. For instance, if {a mathematical formula}O1=0.25, {a mathematical formula}O2=0.75, and {a mathematical formula}P1=P2=0.5 we have {a mathematical formula}G=G′ and thus: {a mathematical formula}E=NWGM=0.5.In short, in general the NWGM is a better approximation to the expectation E than the geometric mean G. The property is always true to a second order of approximation. Furthermore, it is always exact when{a mathematical formula}NWGM⩽Esince we must have{a mathematical formula}G⩽NWGM⩽E. Furthermore, in general the NWGM is a better approximation to the mean than a random sample. Using a randomly chosen {a mathematical formula}Oi as an estimate of the mean E, leads to an error that scales like the standard deviation {a mathematical formula}σ=V, whereas the NWGM leads to an error that scales like V.When {a mathematical formula}NWGM&gt;E, “third order” cases can be found where{a mathematical formula} An example is provided by: {a mathematical formula}O1=0.622459, {a mathematical formula}O2=0.731059 with a uniform distribution ({a mathematical formula}p1=p2=0.5). In this case, {a mathematical formula}E=0.676759, {a mathematical formula}G=0.674577, {a mathematical formula}G′=0.318648, {a mathematical formula}NWGM=0.679179, {a mathematical formula}E−G=0.002182 and {a mathematical formula}NWGM−E=0.002420.
      </list-item>
     </list>
     <paragraph label="Extreme cases">
      Note also that if for some i, {a mathematical formula}Oi=1 with non-zero probability, then {a mathematical formula}G′=0. In this case, {a mathematical formula}NWGM=1, unless there is a {a mathematical formula}j≠i such that {a mathematical formula}Oj=0 with non-zero probability. Likewise if for some i, {a mathematical formula}Oi=0 with non-zero probability, then {a mathematical formula}G=0. In this case, {a mathematical formula}NWGM=0, unless there is a {a mathematical formula}j≠i such that {a mathematical formula}Oj=1 with non-zero probability. If both {a mathematical formula}Oi=1 and {a mathematical formula}Oj=0 are achieved with non-zero probability, then {a mathematical formula}NWGM=0/0 is undefined. In principle, in a sigmoidal neuron, the extreme output values 0 and 1 are never achieved, although in simulations this could happen due to machine precision. In all these extreme cases, where the NWGM is a good approximation of E or not depends on the exact distribution of the values. For instance, if for some i, {a mathematical formula}Oi=1 with non-zero probability, and all the other {a mathematical formula}Oj's are also close to 1, then {a mathematical formula}NWGM=1≈E. On the other hand, if {a mathematical formula}Oi=1 with small but non-zero probability, and all the other {a mathematical formula}Oj's are close to 0, then {a mathematical formula}NWGM=1 is not a good approximation of E.
     </paragraph>
     <paragraph label="Higher order moments">
      It would be useful to be able to derive estimates also for the variance V, as well as other higher order moments of the numbers O, especially when {a mathematical formula}O=σ(S). While the NWGM can easily be generalized to higher order moments, it does not seem to yield simple estimates as for the mean (see Appendix C). However higher order moments in a deep network trained with dropout can easily be approximated, as in the linear case (see Section 9).
     </paragraph>
     <paragraph label="Proof">
      To prove these results, we compute first and second order approximations. Depending on the case of interest, the numbers {a mathematical formula}0&lt;Oi&lt;1 can be expanded around E, around G, or around 0.5 (or around 0 or 1 when they are consistently close to these boundaries). Without assuming that they are consistently low or high, we expand them around 0.5 by writing {a mathematical formula}Oi=0.5+ϵi where {a mathematical formula}0⩽|ϵi|⩽0.5. [Estimates obtained by expanding around E are given in Appendix B]. For any distribution {a mathematical formula}P1,…,Pm over the m subnetworks, we have {a mathematical formula}E(O)=0.5+E(ϵ) and {a mathematical formula}Var(O)=Var(ϵ). As usual, let {a mathematical formula}G=∏iOiPi=∏i(0.5+ϵi)Pi=0.5∏i(1+2ϵi)Pi. To a first order of approximation,{a mathematical formula} The approximation is obtained using a Taylor expansion and the fact that {a mathematical formula}2|ϵi|&lt;1. In a similar way, we have {a mathematical formula}G′≈1−E and {a mathematical formula}G/(G+G′)≈E. These approximations become more accurate as {a mathematical formula}ϵi→0. To a second order of approximation, we have{a mathematical formula} where {a mathematical formula}R3(ϵi) is the remainder of order three{a mathematical formula} and {a mathematical formula}|ui|⩽2|ϵi|. Expanding the product gives{a mathematical formula} which reduces to{a mathematical formula} By symmetry, we also have{a mathematical formula} where again {a mathematical formula}R3(ϵ) is the higher order remainder. Neglecting the remainder and writing {a mathematical formula}E=E(O) and {a mathematical formula}V=Var(O) we have{a mathematical formula} Thus the differences between the mean on one hand, and the geometric mean and the normalized geometric means on the other, satisfy{a mathematical formula} and{a mathematical formula} To know when the NWGM is a better approximation to E than the WGM, we consider when the factor {a mathematical formula}|(1−2E)/(1−2V)| is less or equal to one. There are four cases:
     </paragraph>
     <list>
      <list-item label="(1)">
       {a mathematical formula}E⩽0.5 and {a mathematical formula}V⩽0.5 and {a mathematical formula}E⩾V.
      </list-item>
      <list-item label="(2)">
       {a mathematical formula}E⩽0.5 and {a mathematical formula}V⩾0.5 and {a mathematical formula}E+V⩾1.
      </list-item>
      <list-item label="(3)">
       {a mathematical formula}E⩾0.5 and {a mathematical formula}V⩽0.5 and {a mathematical formula}E+V⩽1.
      </list-item>
      <list-item label="(4)">
       {a mathematical formula}E⩾0.5 and {a mathematical formula}V⩾0.5 and {a mathematical formula}E⩽V.
      </list-item>
     </list>
    </section>
    <section label="9">
     <section-title>
      Dropout distributions and approximation properties
     </section-title>
     <paragraph>
      Throughout the rest of this article, we let {a mathematical formula}Wil=σ(Uil) denote the deterministic variables of the dropout approximation (or ensemble network) with{a mathematical formula} in the case of dropout applied to the nodes. The main question we wish to consider is whether {a mathematical formula}Wil is a good approximation to {a mathematical formula}E(Oil) for every input, every layer l, and any unit i.
     </paragraph>
     <section label="9.1">
      <section-title>
       Dropout induction
      </section-title>
      <paragraph>
       Dropout relies on the correctness of the approximation of the expectation of the activity of each unit over all its dropout subnetworks by the corresponding deterministic variable in the form{a mathematical formula} for each input, each layer l, and each unit i. The correctness of this approximation can be seen by induction. For the first layer, the property is obvious since {a mathematical formula}Wi1=NWGM(Oi1)≈E(Oi1), using the results of Section 8. Now assume that the property is true up to layer l. Again, by the results in Section 8,{a mathematical formula} which can be computed by{a mathematical formula} The approximation in Eq. (101) uses of course the induction hypothesis. This induction, however, does not provide any sense of the errors being made, and whether these errors increase significantly with the depth of the networks. The error can be decomposed into two terms{a mathematical formula} Thus in what follows we study each term.
      </paragraph>
     </section>
     <section label="9.2">
      <section-title>
       Sampling distributions
      </section-title>
      <paragraph>
       In Section 8, we have shown that in general {a mathematical formula}NWGM(O) provides a good approximation to {a mathematical formula}E(O). To further understand the dropout approximation and its behavior in deep networks, we must look at the distribution of the difference {a mathematical formula}α=E(O)−NWGM(O). Since both E and NWGM are deterministic functions of a set of O values, a distribution can only be defined if we look at different samples of O values taken from a more general distribution. These samples could correspond to dropout samples of the output of a given neuron. Note that the number of dropout subnetworks of a neuron being exponentially large, only a sample can be accessed during simulations of large networks. However, we can also consider that these samples are associated with a population of neurons, for instance the neurons in a given layer. While we cannot expect the neurons in a layer to behave homogeneously for a given input, they can in general be separated in a small number of populations, such as neurons that have low activity, medium activity, and high activity and the analysis below can be applied to each one of these populations separately. Letting {a mathematical formula}OS denote a sample of m values {a mathematical formula}Oi,…,Om, we are going to show through simulations and more formal arguments that in general {a mathematical formula}E(OS)−NWGM(OS) has a mean close to 0, a small standard deviation, and in many cases is approximately normally distributed. For instance, if the values O originate from a uniform distribution over {a mathematical formula}[0,1], it is easy to see that both E and NWGM are approximately normally distributed, with mean 0.5, and a small variance decreasing as {a mathematical formula}1/m.
      </paragraph>
     </section>
     <section label="9.3">
      <section-title>
       Mean and standard deviation of the normalized weighted geometric mean
      </section-title>
      <paragraph>
       More generally, assume that the variables {a mathematical formula}Oi are i.i.d. with mean {a mathematical formula}μO and variance {a mathematical formula}σO2. Then the variables {a mathematical formula}Si satisfying {a mathematical formula}Oi=σ(Si) are also i.i.d. with mean {a mathematical formula}μS and variance {a mathematical formula}σS2. Densities for S when O has a Beta distribution, or for O when S has a Gaussian distribution, are derived in Appendix A Rectified linear transfer function without Gaussian assumption, Appendix B Expansion around the mean and around zero or one, Appendix C Higher order moments, Appendix D Derivatives of the logistic function and their expectations, Appendix E Distributions, Appendix F Alternative estimate of the expectation. These could be used to model in more detail non-uniform distributions, and distributions corresponding to low or high activity. For m sufficiently large, by the central limit theorem{sup:2} the means of these quantities are approximately normal with:{a mathematical formula} If these standard deviations are small enough, which is the case for instance when m is large, then σ can be well approximated by a linear function with slope t over the corresponding small range. In this case, {a mathematical formula}NWGM(OS)=σ(E(SS)) is also approximately normal with{a mathematical formula} Note that {a mathematical formula}|t|⩽λ/4 since {a mathematical formula}σ′=λσ(1−σ). Very often, {a mathematical formula}σ(μS)≈μO. This is particularly true if {a mathematical formula}μO=0.5. Away from 0.5, a bias can appear—for instance we know that if all the {a mathematical formula}Oi&lt;0.5 then {a mathematical formula}NWGM&lt;E—but this bias is relatively small. This is confirmed by simulations, as shown in Fig. 9.1 using Gaussian or uniform distributions to generate the values {a mathematical formula}Oi. Finally, note that the variance of {a mathematical formula}E(OS) and {a mathematical formula}NWGM(OS) are of the same order and behave like {a mathematical formula}C1/m and {a mathematical formula}C2/m respectively as {a mathematical formula}m→∞. Furthermore {a mathematical formula}σO2=C1≈C2 if {a mathematical formula}σO2 is small.
      </paragraph>
      <paragraph>
       If necessary, it is also possible to derive better and more general estimates of {a mathematical formula}E(O), under the assumption that S is Gaussian by approximating the logistic function with the cumulative distribution of a Gaussian, as described in Appendix F (see also [41]).
      </paragraph>
      <paragraph>
       If we sample from many neurons whose activities come from the same distribution, the sample mean and the sample NWGM will be normally distributed and have roughly the same mean. The difference will have approximately zero mean. To show that the difference is approximately normal we need to show that E and NWGM are uncorrelated.
      </paragraph>
     </section>
     <section label="9.4">
      <section-title>
       Correlation between the mean and the normalized weighted geometric mean
      </section-title>
      <paragraph>
       We have{a mathematical formula} Thus to estimate the variance of the difference, we must estimate the covariance between {a mathematical formula}E(OS) and {a mathematical formula}NWGM(OS). As we shall see, this covariance is close to null.
      </paragraph>
      <paragraph>
       In this section, we assume again samples of size m from a distribution on O with mean {a mathematical formula}E=μO and variance {a mathematical formula}V=σO2. To simplify the notation, we use {a mathematical formula}ES, {a mathematical formula}VS, and {a mathematical formula}NWGMS to denote the random variables corresponding to the mean, variance, and normalized weighted geometric mean of the sample. We have seen, by doing a Taylor expansion around 0.5, that {a mathematical formula}NWGMS≈(ES−VS)/(1−2VS).
      </paragraph>
      <paragraph>
       We first consider the case where {a mathematical formula}E=NWGM=0.5. In this case, the covariance of {a mathematical formula}NWGMS and {a mathematical formula}ES can be estimated as{a mathematical formula} We have {a mathematical formula}0.5⩽1−2VS⩽1 and {a mathematical formula}E(ES−12)2=Var(ES)=V/m. Thus in short the covariance is of order {a mathematical formula}V/m and goes to 0 as the sample size m goes to infinity. For the Pearson correlation, the denominator is the product of two similar standard deviations and scales also like {a mathematical formula}V/m. Thus the correlation should be roughly constant and close to 1. More generally, even when the mean E is not equal to 0.5, we still have the approximations{a mathematical formula} And the leading term is still of order {a mathematical formula}V/m. [Similar results are also obtained by using the expansions around 0 or 1 given in Appendix A Rectified linear transfer function without Gaussian assumption, Appendix B Expansion around the mean and around zero or one, Appendix C Higher order moments, Appendix D Derivatives of the logistic function and their expectations, Appendix E Distributions, Appendix F Alternative estimate of the expectation to model populations of neurons with low or high activity.] Thus again the covariance between NWGM and E goes to 0, and the Pearson correlation is constant and close to 1. These results are confirmed by simulations in Fig. 9.2.
      </paragraph>
      <paragraph>
       Combining the previous results we have{a mathematical formula}Thus in general{a mathematical formula}E(OS)and{a mathematical formula}NWGM(OS)are random variables with: (1) similar, if not identical, means; (2) variances and covariance that decrease to 0 inversely to the sample size; (3) approximately normal distributions. Thus{a mathematical formula}E−NWGMis approximately normally distributed around zero. The NWGM behaves like a random variable with small fluctuations above and below the mean. [Of course contrived examples can be constructed (for instance with small m or small networks) which deviate from this general behavior.]
      </paragraph>
     </section>
     <section label="9.5">
      <section-title>
       Dropout approximations: the cancellation effects
      </section-title>
      <paragraph>
       To complete the analysis of the dropout approximation of {a mathematical formula}E(Oil) by {a mathematical formula}Wil, we show by induction over the layers that {a mathematical formula}Wil=E(Oil)−ϵil where in general the error term {a mathematical formula}ϵil=αil+βil is small and approximately normally distributed with mean 0. Furthermore the error {a mathematical formula}ϵil is uncorrelated with the error {a mathematical formula}αil=E(Oil)−NWGM(Oil) for {a mathematical formula}l&gt;1.
      </paragraph>
      <paragraph>
       First, the property is true for {a mathematical formula}l=1 since {a mathematical formula}Wi1=NWGM(Oi1) and the results of the previous sections apply immediately to this case. For the induction step, we assume that the property is true up to layer l. At the following layer, we have{a mathematical formula} Using a first order Taylor expansion{a mathematical formula} or more compactly{a mathematical formula} thus{a mathematical formula} As a sum of many linear small terms, {a mathematical formula}βil+1 is approximately normally distributed. By linearity of the expectation{a mathematical formula} By linearity of the variance with respect to sums of independent random variables{a mathematical formula} This variance is small since {a mathematical formula}[σ′(E(Sil+1))]2⩽1/16 for the standard logistic function (and much smaller than 1/16 at the end of learning), {a mathematical formula}(pjh)2⩽1, and {a mathematical formula}Var(ϵjh) is small by induction. The weights {a mathematical formula}wijl+1h are small at the beginning of learning and as we shall see in Section 11 dropout performs weight regularization automatically. While this is not observed in the simulations used here, one concern is that with very large layers the sum could become large. We leave a more detailed study of this issue for future work. Finally, we need to show that {a mathematical formula}αil+1 and {a mathematical formula}βil+1 are uncorrelated. Since both terms have approximately mean 0, we compute the mean of their product{a mathematical formula} By linearity of the expectation{a mathematical formula} since {a mathematical formula}E(E(Oil+1)−NWGM(Oil+1))ϵjh=E[E(Oil+1)−NWGM(Oil+1)]E(ϵjh)≈0.
      </paragraph>
      <paragraph>
       In summary, in general both{a mathematical formula}Wiland{a mathematical formula}NGWM(Oil)can be viewed as good approximations to{a mathematical formula}E(Oil)with small deviations that are approximately Gaussians with mean zero and small standard deviations. These deviations act like noise and cancel each other to some extent preventing the accumulation of errors across layers.
      </paragraph>
      <paragraph>
       These results and those of the previous section are confirmed by simulation results given by Fig. 9.3, Fig. 9.4, Fig. 9.5, Fig. 9.6, Fig. 9.7. The simulations are based on training a deep neural network classifier on the MNIST handwritten characters dataset with layers of size 784-1200-1200-1200-1200-10 replicating the results described in [27], using {a mathematical formula}p=0.8 for the input layer and {a mathematical formula}p=0.5 for the hidden layers. The raster plots accumulate the results obtained for 10 randomly selected input vectors. For fixed weights and a fixed input vector, 10,000 Monte Carlo simulations are used to sample the dropout subnetworks and estimate the distribution of activities O of each neuron in each layer. These simulations use the weights obtained at the end of learning, except in the cases were the beginning and end of learning are compared (Fig. 9.6, Fig. 9.7). In general, the results show how well the {a mathematical formula}NWGM(Oil) and the deterministic values {a mathematical formula}Wil approximate the true expectation {a mathematical formula}E(Oil) in each layer, both at the beginning and the end of learning, and how the deviations can roughly be viewed as small, approximately Gaussian, fluctuations well within the bounds derived in Section 8.
      </paragraph>
     </section>
     <section label="9.6">
      <section-title>
       Dropout approximations: estimation of variances and covariances
      </section-title>
      <paragraph>
       We have seen that the deterministic values {a mathematical formula}Ws can be used to provide very simple but effective estimates of the values {a mathematical formula}E(O)s across an entire network under dropout. Perhaps surprisingly, the {a mathematical formula}Ws can also be used to derive approximations of the variances and covariances of the units as follows.
      </paragraph>
      <paragraph>
       First, for the dropout variance of a neuron, we can use{a mathematical formula} or{a mathematical formula} These two approximations can be viewed respectively as rough upperbounds and lower bounds to the variance. For neurons whose activities are close to 0 or 1, and thus in general for neurons towards the end of learning, these two bounds are similar to each other. This is not the case at the beginning of learning when, with very small weights and a standard logistic transfer function, {a mathematical formula}Wil=0.5 and {a mathematical formula}Var(Oil)≈0 (Fig. 9.8, Fig. 9.9). At the beginning and the end of learning, the variances are small and so “0” is the better approximation. However, during learning, variances can be expected to be larger and closer to their approximate upper bound {a mathematical formula}W(1−W) (Fig. 9.10, Fig. 9.11).
      </paragraph>
      <paragraph>
       For the covariances of two different neurons, we use{a mathematical formula} This independence approximation is accurate for neurons that are truly independent of each other, such as pairs of neurons in the first layer. However it can be expected to remain approximately true for pairs of neurons that are only loosely coupled, i.e. for most pairs of neurons in a large neural networks at all times during learning. This is confirmed by simulations (Fig. 9.12) conducted using the same network trained on the MNIST dataset. The approximation is much better than simply using 0 (Fig. 9.13).
      </paragraph>
      <paragraph>
       For neurons that are directly connected to each other, this approximation still holds but one can try to improve it by introducing a slight correction. Consider the case of a neuron with output {a mathematical formula}Ojh feeding directly into the neuron with output {a mathematical formula}Oil ({a mathematical formula}h&lt;l) through a weight {a mathematical formula}wijlh. By isolating the contribution of {a mathematical formula}Ojh, we have{a mathematical formula} with a first order Taylor approximation which is more accurate when {a mathematical formula}wijlh or {a mathematical formula}Ojh are small (conditions that are particularly well satisfied at the beginning of learning or with sparse coding). In this expansion, the first term is independent of {a mathematical formula}Ojh and its expectation can easily be computed as{a mathematical formula} Thus here {a mathematical formula}Wijlh is simply the deterministic activation of neuron i in layer l in the ensemble network when neuron j in layer h is removed from its inputs. Thus it can easily be computed by forward propagation in the deterministic network. Using a first-order Taylor expansion it can be estimated by{a mathematical formula} In any case,{a mathematical formula} Towards the end of learning, {a mathematical formula}σ′≈0 and so the second term can be neglected. A slightly more precise estimate can be obtained by writing {a mathematical formula}σ′≈λσ when σ is close to 0, and {a mathematical formula}σ′≈λ(1−σ) when σ is close to 1, replacing the corresponding expectation by {a mathematical formula}Wijlh or {a mathematical formula}1−Wijlh. In any case, to a leading term approximation, we have{a mathematical formula} The accuracy of these formula for pairs of connected neurons is demonstrated in Fig. 9.14 at the beginning and end of learning, where it is also compared to the approximation {a mathematical formula}E(OilOjh)≈WilWjh. The correction provides a small improvement at the end of learning but not at the beginning. This is because it neglects a term in {a mathematical formula}σ′ which presumably is close to 0 at the end of learning. The improvement is small enough that for most purposes the simpler approximation {a mathematical formula}WilWjh may be used in all cases, connected or unconnected.
      </paragraph>
     </section>
    </section>
    <section label="10">
     <section-title>
      The duality with spiking neurons and with backpropagation
     </section-title>
     <section label="10.1">
      <section-title>
       Spiking neurons
      </section-title>
      <paragraph>
       There is a long-standing debate on the importance of spikes in biological neurons, and also in artificial neural networks, in particular as to whether the precise timing of spikes is used to carry information or not. In biological systems, there are many examples, for instance in the visual and motor systems, where information seems to be carried by the short term average firing rate of neurons rather than the exact timing of their spikes. However, other experiments have shown that in some cases the timing of the spikes are highly reproducible and there are also known examples where the timing of the spikes is crucial, for instance in the auditory location systems of bats and barn owls, where brain regions can detect very small interaural differences, considerably smaller than 1 ms [26], [19], [18]. However these seem to be relatively rare and specialized cases. On the engineering side the question of course is whether having spiking neurons is helpful for learning or any other purposes, and if so whether the precise timing of the spikes matters or not. There is a connection between dropout and spiking neurons which might shed some, at the moment faint, light on these questions.
      </paragraph>
      <paragraph>
       A sigmoidal neuron with output {a mathematical formula}O=σ(S) can be converted into a stochastic spiking neuron by letting the neuron “flip a coin” and produce a spike with probability O. Thus in a network of spiking neurons, each neuron computes three random variables: an input sum S, a spiking probability O, and a stochastic output Δ (Fig. 10.1). Two spiking mechanisms can be considered: (1) global: when a neuron spikes it sends the same quantity r along all its outgoing connections; and (2) local or connection-specific: when a neuron spikes with respect to a specific connection, it sends a quantity r along that connection. In the latter case, a different coin must be flipped for each connection. Intuitively, one can see that the first case corresponds to dropout on the units, and the second case to droupout on the connections. When a spike is not produced, the corresponding unit is dropped in the first case, and the corresponding connection is dropped in the second case.
      </paragraph>
      <paragraph>
       To be more precise, a multi-layer network is described by the following equations. First for the spiking of each unit:{a mathematical formula} in the global firing case, and{a mathematical formula} in the connection-specific case. Here we allow the “size” of the spikes to vary with the neurons or the connections, with spikes of fixed-size being an easy special case. While the spike sizes could in principle be greater than one, the connection to dropout requires spike sizes of size at most one. The spiking probability is computed as usual in the form{a mathematical formula} and the sum term is given by{a mathematical formula} in the global firing case, and{a mathematical formula} in the connection-specific case. The equations can be applied to all the layers, including the output layer and the input layer if these layers consist of spiking neurons. Obviously non-spiking neurons (e.g. in the input or output layers) can be combined with spiking neurons in the same network.
      </paragraph>
      <paragraph>
       In this formalism, the issue of the exact timing of each spike is not really addressed. However some information about the coin flips must be given in order to define the behavior of the network. Two common models are to assume complete asynchrony, or to assume synchrony within each layer. As spikes propagate through the network, the average output {a mathematical formula}E(Δ) of a spiking neuron over all spiking configurations is equal to r times the size its average firing probability {a mathematical formula}E(O). As we have seen, the average firing probability can be approximated by the NWGM over all possible inputs S, leading to the following recursive equations:{a mathematical formula} in the global firing case, or{a mathematical formula} in the connection-specific case. Then{a mathematical formula} with{a mathematical formula} in the global firing case, or{a mathematical formula} in the connection-specific case.
      </paragraph>
      <paragraph>
       In short, the expectation of the stochastic outputs of the stochastic neurons in a feedforward stochastic network can be approximated by a dropout-like deterministic feedforward propagation, proceeding from the input layer to the output layer, and multiplying each weight{a mathematical formula}wijhlby the corresponding spike size{a mathematical formula}rjl(or{a mathematical formula}rijl)—which acts as a dropout probability parameter—of the corresponding pre-synaptic neuron. [Operating a neuron in stochastic mode is also equivalent to setting all its inputs to 1 and using dropout on its connections with different Bernoulli probabilities associated with the sigmoidal outputs of the previous layer.]
      </paragraph>
      <paragraph>
       In particular, this shows that given any feedforward network of spiking neurons, with all spikes of size 1, we can approximate the average firing rate of any neuron simply by using deterministic forward propagation in the corresponding identical network of sigmoidal neurons. The quality of the approximation is determined by the quality of the approximations of the expectations by the NWGMs. More generally, consider three feedforward networks (Fig. 10.2) with the same identical topology, and almost identical weights. The first network is stochastic, has weights {a mathematical formula}wijhl, and consists of spiking neurons: a neuron with activity {a mathematical formula}Oih sends a spike of size {a mathematical formula}rih with probability {a mathematical formula}Oih, and 0 otherwise (a similar argument can be made with connection-specific spikes of size {a mathematical formula}rjih). Thus, in this network neuron i in layer h sends out a signal that has instantaneous mean and variance given by{a mathematical formula} for fixed {a mathematical formula}Oih, and short-term mean and variance given by{a mathematical formula} when averaged over all spiking configurations, for a fixed input.
      </paragraph>
      <paragraph>
       The second network is also stochastic, has identical weights to the first network, and consists of dropout sigmoidal neurons: a neuron with activity {a mathematical formula}Oih sends a value {a mathematical formula}Oih with probability {a mathematical formula}rih, and 0 otherwise (a similar argument can be made with connection-specific dropout with probability {a mathematical formula}rjih). Thus neuron i in layer h sends out a signal that has instantaneous expectation and variance given by{a mathematical formula} for a fixed {a mathematical formula}Oih, and short-term expectation and variance given by{a mathematical formula} when averaged over all dropout configurations, for a fixed input.
      </paragraph>
      <paragraph>
       The third network is deterministic and consists of logistic units. Its weights are identical to those of the previous two networks except they are rescaled in the form {a mathematical formula}wijhl×rjl. Then, remarkably, feedforward deterministic propagation in the third network can be used to approximate both the average output of the neurons in the first network over all possible spiking configurations, and the average output of the neurons in the second network over all possible dropout configurations. In particular, this shows that using stochastic neurons in the forward pass of a neural network of sigmoidal units may be similar to using dropout.
      </paragraph>
      <paragraph>
       Note that the first and second network are quite different in their details. In particular the variances of the signals sent by a neuron to the following layer are equal only when {a mathematical formula}Oih=rih. When {a mathematical formula}rih&lt;Oih, then the variance is greater in the dropout network. When {a mathematical formula}rih&gt;Oih, which is the typical case with sparse encoding and {a mathematical formula}rih⩾0.5, then the variance is greater in the spiking network. This corresponds to the Poisson regime of relatively rare spikes.
      </paragraph>
      <paragraph>
       In summary, a simple deterministic feedforward propagation allows one to estimate the average firing rates in stochastic, even asynchronous, networks without the need for knowing the exact timing of the firing events. Stochastic neurons can be used instead of dropout during learning. Whether stochastic neurons are preferable to dropout, for instance because of the differences in variance described above, requires further investigations. There is however one more aspect to the connection between dropout, stochastic neurons, and backpropagation.
      </paragraph>
     </section>
     <section label="10.2">
      <section-title>
       Backpropagation and backpercolation
      </section-title>
      <paragraph>
       Another important observation is that the backward propagation used in the backpropagation algorithm can itself be viewed as closely related to dropout. Starting from the errors at the output layer, backpropagation uses an orderly alternating sequence of multiplications by the transpose of the forward weight matrices and by the derivatives of the activation functions. Thus backpropagation is essentially a form of linear propagation in the reverse linear network combined with multiplication by the derivatives of the activation functions at each node, and thus formally looks like the recursion of Eq. (24). If these derivatives are between 0 and 1, they can be interpreted as probabilities. [In the case of logistic activation functions, {a mathematical formula}σ′(x)=λσ(x)(1−σ(x)) and thus {a mathematical formula}σ′(x)⩽1 for every value of x when {a mathematical formula}λ⩽4.] Thus backpropagation is computing the dropout ensemble average in the reverse linear network where the dropout probability p of each node is given by the derivative of the corresponding activation. This suggests the possibility of using dropout (or stochastic spikes, or addition of Gaussian noise), during the backward pass, with or without dropout (or stochastic spikes, or addition of Gaussian noise) in the forward pass, and with different amounts of coordination between the forward and backward pass when dropout is used in both.
      </paragraph>
      <paragraph>
       Using dropout in the backward pass is still faced with the problem of vanishing gradients since units with activities close to 0 or 1, hence derivatives close to 0, lead to rare sampling. However, imagine for instance six layers of 1000 units each, fully connected, with derivatives that are all equal to 0.1 everywhere. Standard backpropagation produces an error signal that contains a factor of {a mathematical formula}10−6 by the time the first layer is reached. Using dropout in the backpropagation instead selects on average 100 units per layer and propagates a full signal through them, with no attenuation. Thus a strong error signal is propagated but through a narrow channel, hence the name of backpercolation. Backpropagation can be thought of as a special case of backpercolation, because with a very small learning rate backpercolation is essentially identical to backpropagation, since backpropagation corresponds to the ensemble average of many backpercolation passes. This approach of course would be slow on a computer since a lot of time would be spent sampling to compute an average signal that is provided in one pass by backpropagation. However it shows that exact gradients are not always necessary and that backpropagation can tolerate noise, alleviating at least some of the concerns with the biological plausibility of backpropagation. Furthermore, aside from speed issue, noise in the backward pass might help avoiding certain local minima. Finally, we note that several variations on these ideas are possible, such as using backpercolation with a fixed value of p (e.g. {a mathematical formula}p=0.5), or using backpropagation for the top layers followed by backpercolation for the lower layers and vice versa. Detailed investigation of these issues is beyond the scope of this paper and left for future work.
      </paragraph>
     </section>
    </section>
    <section label="11">
     <section-title>
      Dropout dynamics
     </section-title>
     <paragraph>
      So far, we have concentrated on the static properties of dropout, i.e. properties of dropout for a fixed set of weights. In this section we look at more dynamic properties of dropout, related to the training procedure and the evolution of the weights.
     </paragraph>
     <section label="11.1">
      <section-title>
       Dropout convergence
      </section-title>
      <paragraph>
       With properly decreasing learning rates, dropout is almost sure to converge to a small neighborhood of a local minimum (or global minimum in the case of a strictly convex error function) in a way similar to stochastic gradient descent in standard neural networks [38], [13], [14]. This is because it can be viewed as a form of on-line gradient descent with respect to the error function{a mathematical formula} of the true ensemble, where {a mathematical formula}t(I) is the target value for input I and {a mathematical formula}fw is the elementary error function, typically the squared error in regression, or the relative entropy error in classification, which depends on the weights w. In the case of dropout, the probability {a mathematical formula}P(N) of the network {a mathematical formula}N is factorial and associated with the product of the underlying Bernoulli selector variables.
      </paragraph>
      <paragraph>
       Thus dropout is “on-line” with respect to both the input examples I and the networks {a mathematical formula}N, or alternatively one can form a new set of training examples, where the examples are formed by taking the cartesian product of the set of original examples with the set of all possible subnetworks. In the next section, we show that dropout is also performing a form of stochastic gradient descent with respect to a regularized ensemble error.
      </paragraph>
      <paragraph>
       Finally, we can write the gradient of the error above as:{a mathematical formula} If the backpropagated error does not vary too much around its mean from one network to the next, which seems reasonable in a large network, then we can replace it by its mean, and similarly for the activity {a mathematical formula}Ojh. Thus the gradient of the true ensemble can be approximated by the product of the expected backpropagated error (post-synaptic terms) and the expected pre-synaptic activity{a mathematical formula}
      </paragraph>
     </section>
     <section label="11.2">
      <section-title>
       Dropout gradient and adaptive regularization: single linear unit
      </section-title>
      <paragraph>
       As for the static properties, it is instructive to first consider the simplest case of a single linear unit. In the case of a single linear unit trained with dropout with an input I, an output {a mathematical formula}O=S, and a target t, the error is typically quadratic of the form {a mathematical formula}Error=12(t−O)2. Let us consider the two error functions {a mathematical formula}EENS and {a mathematical formula}ED associated with the ensemble of all possible subnetworks and the network with dropout. In the linear case, the ensemble network is identical to the deterministic network obtained by scaling the connections by the dropout probabilities. For a single input I, these error functions are defined by:{a mathematical formula} and{a mathematical formula} Here {a mathematical formula}δi are the Bernoulli selector random variables with {a mathematical formula}P(δi=1)=pi, hence {a mathematical formula}ED is a random variable, whereas {a mathematical formula}EENS is a deterministic function. We use a single training input I for notational simplicity, otherwise the errors of each training example can be combined additively. The learning gradients are of the form {a mathematical formula}∂E∂w=∂E∂O∂O∂w=−(t−O)∂O∂w, yielding:{a mathematical formula} and{a mathematical formula} The last vector is a random vector variable and we can take its expectation. Assuming as usual that the random variables {a mathematical formula}δi's are pairwise independent, we have{a mathematical formula} which yields{a mathematical formula}Thus, in general the dropout gradient is well aligned with the ensemble gradient. Remarkably, the expectation of the gradient with dropout is the gradient of the regularized ensemble error{a mathematical formula}The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that the weights do not become too large and overfit the data. Dropout provides immediately the magnitude of the regularization term which is adaptively scaled by the square of the input terms and by the variance of the dropout variables. Note that here{a mathematical formula}pi=0.5is the value that provides the highest level of regularization and the regularization term depends only on the inputs, and not on the target outputs. Furthermore, the expected dropout gradient is on-line also with respect to the regularization term since there is one term for each training example. Obviously, the same result holds for an entire layer of linear units. The regularization effect of dropout in the case of generalized linear models is also discussed in [43] where it is also used to derive other regularizers.
      </paragraph>
     </section>
     <section label="11.3">
      <section-title>
       Dropout gradient and adaptive regularization: deep linear networks
      </section-title>
      <paragraph>
       Similar calculations can be made for deep linear networks. For instance, the previous calculation can be adapted immediately to the top layer of a linear network with T layers with{a mathematical formula} and{a mathematical formula} which corresponds again to an adaptive quadratic regularization term in {a mathematical formula}wijTl, with a coefficient associated for each input with the corresponding variance of the dropout pre-synaptic neuron {a mathematical formula}Var(δjlOjl).
      </paragraph>
      <paragraph>
       To study the gradient of any weight w in the network, let us assume without any loss of generality that the deep network has a single output unit. Let us denote its activity by S in the dropout network, and by U in the deterministic ensemble network. Since the network is linear, for a given input the output is a linear function of w{a mathematical formula} The output is obtained by summing the contributions provided by all possible paths from inputs to output. Here α and β are random variables. α corresponds to the sum of all the contributions associated with paths from the input layer to the output layer that contain the edge associated with w. β corresponds to the sum of all the contributions associated with paths from the input layer to the output layer that do not contain the edge associated with w. Thus the gradients are given by{a mathematical formula} and{a mathematical formula} The expectation of the dropout gradient is given by{a mathematical formula} This yields the remarkable expression{a mathematical formula}Thus again the expectation of the dropout gradient is the gradient of the ensemble plus an adaptive regularization term which has two components. The component{a mathematical formula}wVar(α)corresponds to a weight decay, or quadratic regularization term in the error function. The adaptive coefficient{a mathematical formula}Var(α)measures the dropout variance of the contribution to the final output associated with all the input-to-output paths which contain w. The component{a mathematical formula}Cov(α,β)measures the dropout covariance between the contribution associated with all the paths that contain w and the contribution associated with all the paths that do not contain w. In general, this covariance is small and equal to zero for a single layer linear network. Both α and β depend on the training inputs, but not on the target outputs.
      </paragraph>
     </section>
     <section label="11.4">
      <section-title>
       Dropout gradient and adaptive regularization: single sigmoidal unit
      </section-title>
      <paragraph>
       For a single sigmoidal unit something quite similar, but not identical holds. With a sigmoidal unit {a mathematical formula}O=σ(S)=1/(1+ce−λS), one typically uses the relative entropy error{a mathematical formula} We can again consider two error functions {a mathematical formula}EENS and {a mathematical formula}ED. Note that while in the linear case {a mathematical formula}EENS is exactly equal to the ensemble error, in the non-linear case we use {a mathematical formula}EENS to denote the error of deterministic network which approximates the ensemble network.
      </paragraph>
      <paragraph>
       By the chain rule, we have {a mathematical formula}∂E∂w=∂E∂O∂O∂S∂S∂w with{a mathematical formula} Thus finally grouping terms together{a mathematical formula} Thus the overall form of the derivative is similar to the linear case up to multiplication by the positive factor λ which is often fixed to one. However the outputs are non-linear which complicates the comparison of the derivatives. We use {a mathematical formula}O=σ(S) in the dropout network and {a mathematical formula}W=σ(U) in the deterministic ensemble approximation. For the ensemble network{a mathematical formula} For the dropout network{a mathematical formula} Taking the expectation of the gradient gives{a mathematical formula} Using the NWGM approximation to the expectation allows one to take the expectation inside the sigmoidal function so that{a mathematical formula} The logistic function is continuously differentiable everywhere so that one can take its first-order Taylor expansion around U:{a mathematical formula} where {a mathematical formula}σ′(x)=σ(x)(1−σ(x)) denotes the derivative of σ. So finally we obtain a result similar to the linear case{a mathematical formula}The dropout gradient is well aligned with the ensemble approximation gradient. Remarkably, and up to simple approximations, the expectation of the gradient with dropout is the gradient of the regularized ensemble error{a mathematical formula}The regularization term is the usual weight decay or Gaussian prior term based on the square of the weights and ensuring that the weights do not become too large and overfit the data. Dropout provides immediately the magnitude of the regularization term which is adaptively scaled by the square of the input terms, the gain λ of the sigmoidal function, by the variance of the dropout variables, and the instantaneous derivative of the sigmoidal function. This derivative is bounded and approaches zero when{a mathematical formula}SENSis small or large. Thus regularization is maximal at the beginning of learning and decreases as learning progresses. Note again that{a mathematical formula}pi=0.5is the value that provides the highest level of regularization. Furthermore, the expected dropout gradient is on-line also with respect to the regularization term since there is one term for each training example. Note again that the regularization term depends only on the inputs, and not on the target outputs. A similar analysis, with identical results, can be carried also for a set of normalized exponential units or for an entire layer of sigmoidal units. A similar result can be derived in a similar way for other suitable transfer functions, for instance for rectified linear functions by expressing them as integrals of logistic functions to ensure differentiability.
      </paragraph>
     </section>
     <section label="11.5">
      <section-title>
       Dropout gradient and adaptive regularization: deep neural networks
      </section-title>
      <paragraph>
       In deep neural networks with logistic transfer functions at all the nodes, the basic idea remains the same. In fact, for a fixed set of weights and a fixed input, we can linearize the network around any weight w and thus Eq. (155) applies “instantaneously”.
      </paragraph>
      <paragraph>
       To derive more specific approximations, consider a deep dropout network described by{a mathematical formula} with layers ranging from {a mathematical formula}h=0 for the inputs to {a mathematical formula}h=T for the output layer, using the selector random variables {a mathematical formula}δjl. The corresponding approximation ensemble network is described by{a mathematical formula} using a new set of U and W distinct variables to avoid any confusion. In principle each node could use a different logistic function, with different c and λ parameters, but to simplify the notation we assume that the same logistic function is used by all neurons. Then the gradient in the ensemble network can be computed by{a mathematical formula} where the backpropagated error can be computed recursively using{a mathematical formula} with the initial values at the top of the network{a mathematical formula} Here {a mathematical formula}ti is the i-th component of the target vector for the example under consideration. In addition, for the pre-synaptic term, we have{a mathematical formula} Likewise, for the dropout network,{a mathematical formula} with{a mathematical formula} and the initial values at the top of the network{a mathematical formula} and the pre-synaptic term{a mathematical formula}
      </paragraph>
      <paragraph>
       Consider unit i in the output layer T receiving a connection from unit j in a layer l (typically {a mathematical formula}l=T−1) with weight {a mathematical formula}wijTl. The gradient of the error function in the dropout network is given by{a mathematical formula} using the notation of Section 9.5: {a mathematical formula}SijTl=Sil−wijTlδjlOjl. Using a first order Taylor expansion to separate out independent terms gives:{a mathematical formula} We can now take the expectation of the gradient{a mathematical formula} Now, using the NWGM approximation {a mathematical formula}E(σ(SijTl))≈σ(E(SijTl))=σ(UijTl)=WijTl≈WiT−σ′(UiT)wijTlpjlWjl{a mathematical formula} which has the form{a mathematical formula} where A has the complex expression given by Eq. (179). Thus we see again that the expectation of the dropout gradient in the top layer is approximately the gradient of the ensemble network regularized by a quadratic weight decay with an adaptive coefficient. Towards the end of learning, if the sigmoidal functions are saturated, then the derivatives are close to 0 and {a mathematical formula}A≈0.
      </paragraph>
      <paragraph>
       Using the dropout approximation {a mathematical formula}E(Ojl)≈Wjl together with {a mathematical formula}E(σ′(SijTl))≈σ′(UiT) (Fig. 9.15, Eq. (220)) produces the more compact approximation{a mathematical formula} similar to the single layer-case and showing that dropout tends to minimize the variance {a mathematical formula}Var(δjlOjl). Also with the approximation of Section 9.5{a mathematical formula}E(OjlOjl)≈Wjl thus A can be further approximated as {a mathematical formula}A≈σ′(UiT)pjlWjl(1−pjlWjl). In this case, we can also write the expected gradient as a product of a post-synaptic backpropagated error and a pre-synaptic expectation{a mathematical formula}
      </paragraph>
      <paragraph>
       With approximations, similar results appear to be true for deeper layers. To see this, the first approximation we make is to assume that the backpropagated error is independent of the product {a mathematical formula}σ′(Sih)δjlPjl of the immediate pre- and post-synaptic terms, so that{a mathematical formula} This approximation should be reasonable and increasingly accurate for units closer to the input layer, as the presence and activity of these units bears vanishingly less influence on the output error. As in the case of the top layer, we can use a first-order Taylor approximation to separate the dependent terms in Eq. (183) so that {a mathematical formula}E(δihσ′(Sih)δjlOjl) is approximately equal to{a mathematical formula} We can approximate {a mathematical formula}E(σ′(Sijhl)) by {a mathematical formula}σ′(Uijhl) and use a similar Taylor expansion in reverse to get {a mathematical formula}E(σ′(Sijhl))≈σ′(Uih)−σ″(Uih)pjlwijhlWjl≈σ′(Uih)−σ″(Uih)pjlwijhlE(Ojl) so that{a mathematical formula} Collecting terms, finally gives{a mathematical formula} or, by extracting the variance term,{a mathematical formula} Combining this result with Eq. (183) gives{a mathematical formula} where A is an adaptive coefficient, proportional to {a mathematical formula}σ″(Uih)Var(δjlOjl). Note that it is not obvious that A is always positive—a requirement for being a form of weight decay—especially since {a mathematical formula}σ″(x) is negative for {a mathematical formula}x&gt;0.5 in the case of the standard sigmoid. Further analyses and simulations of these issues and the underlying approximations are left for future work.
      </paragraph>
      <paragraph>
       In conclusion, the approximations suggest that the gradient of the dropout approximation ensemble{a mathematical formula}∂EENS/∂wijhland the expectation of the gradient{a mathematical formula}E(∂ED/∂wijhl)of the dropout network are similar. The difference is approximately a (weight decay) term linear in{a mathematical formula}wijhlwith a complex, adaptive coefficient, that varies during learning and depends on the variance of the pre-synaptic unit and on the input. Thus dropout has a built in regularization effect that keeps the weights small. Furthermore, this regularization tends also to keep the dropout variance of each unit small. This is a form of self-consistency since small variances ensure higher accuracy in the dropout ensemble approximations. Furthermore, since the dropout variance of a unit is minimized when all its inputs are 0, dropout has also a built-in propensity towards sparse representations.
      </paragraph>
     </section>
     <section label="11.6">
      <section-title>
       Dropin
      </section-title>
      <paragraph>
       It is instructive to think about the apparently symmetric algorithm we call dropin where units are randomly and independently set to 1, rather than 0 as in dropout. Although superficially symmetric to dropout, simulations show that dropin behaves very differently and in fact does not work. The reason can be understood in terms of the previous analyses since setting units to 1 tends to maximize variances, rather then minimizing them.
      </paragraph>
     </section>
     <section label="11.7">
      <section-title>
       Learning phases and sparse coding
      </section-title>
      <paragraph>
       Finally, in light of these results, we can expect roughly three phases during dropout learning:
      </paragraph>
      <list>
       <list-item label="1.">
        At the beginning of learning, when the weights are random and very small, the total input to each unit is close to 0 for all the units and the consistency is high: the output of the units remains roughly constant across subnetworks (and equal to 0.5 if the logistic coefficient is {a mathematical formula}c=1.0).
       </list-item>
       <list-item label="2.">
        As learning progresses, the sizes of the weights increase, activities tend to move towards 0 or 1, and the consistencies decreases, i.e. for a given input the dropout variance of the units across subnetworks increases, and more so for units that move towards 1 than units that move towards 0. However, overall the regularization effect of dropout keeps the weights and variances small. To keep variances small, sparse representations tend to emerge.
       </list-item>
       <list-item label="3.">
        As learning converges, the consistency of the units stabilizes, i.e. for a given input the variance of the units across subnetworks becomes roughly constant and small for units that have converged towards 1, and very small for units that have converged towards 0. This is a consequence of the convergence of stochastic gradient.
       </list-item>
      </list>
      <paragraph>
       For simplicity, let us assume that dropout is carried only in layer h where the units have an output of the form {a mathematical formula}Oih=σ(Sih) and {a mathematical formula}Sih=∑l&lt;h∑jwijhlδjlOjl. For a fixed input, {a mathematical formula}Ojl is a constant since dropout is not applied to layer l. Thus{a mathematical formula} under the usual assumption that the selector variables {a mathematical formula}δjl are independent of each other. A similar expression is obtained if dropout is applied in the same way to the connections. Thus {a mathematical formula}Var(Sih), which ultimately influences the consistency of unit i in layer h, depends on three factors. Everything else being equal, it is reduced by: (1) Small weights which goes together with the regularizing effect of dropout, or the random initial condition; (2) Small activities, which show that dropout is not symmetric with respect to small or large activities, hence the failure of dropin. Overall, dropout tends to favor small activities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the dropout probabilities {a mathematical formula}pjl. The sparsity and learning phases of dropout are demonstrated through simulations in Fig. 11.1, Fig. 11.2, Fig. 11.3.
      </paragraph>
     </section>
    </section>
    <section label="12">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      We have developed a general framework that has enabled the understanding of several aspects of dropout with good mathematical precision. Dropout is an efficient approximation to training all possible sub-models of a given architecture and taking their average. While several theoretical questions regarding both the static and dynamic properties of dropout require further investigations, for instance its generalization properties, the existing framework clarifies the ensemble averaging properties of dropout, as well as its regularization properties. In particular, it shows that the three standard approaches to regularizing large models and avoiding overfitting: (1) ensemble averaging; (2) adding noise; and (3) adding regularization terms (equivalent to Bayesian priors) to the error functions, are all present in dropout and thus may be viewed in a more unified manner.
     </paragraph>
     <paragraph>
      Dropout wants to produce robust units that do not depend on the details of the activation of other individual units. As a result, it seeks to produce units with activities that have small dropout variance, across dropout subnetworks. This partial variance minimization is achieved by keeping the weights small and using sparse encoding, which in turn increases the accuracy of the dropout approximation and the degree of self-consistency. Thus, in some sense, by using small weights and sparse coding, dropout leads to large but energy efficient networks, which could potentially have some biological relevance as it is well known that carbon-based computing is orders of magnitude more efficient than silicon-based computing.
     </paragraph>
     <paragraph>
      It is worth to consider which other classes of models, besides, linear and non-linear feedforward networks, may benefit from dropout. Some form of dropout ought to work, for instance, with Boltzmann machines or Hopfield networks. Furthermore, while dropout has already been successfully applied to several real-life problems, many more remain to be tested. Among these, the problem of predicting quantitative phenotypic traits, such as height, from genetic data, such as single nucleotide polymorphisms (SNPs), is worth mentioning. While genomic data is growing rapidly, for many complex traits we are still in the ill-posed regime where typically the number of loci where genetic variation occurs exceeds the number of training examples. Thus the best current models are typically highly (L1) regularized linear models, and these have had limited success. With its strong regularization properties, dropout is a promising algorithm that could be applied to these questions, using both simple linear or logistic regression models, as well as more complex models, with the potential for also capturing epistatic interactions.
     </paragraph>
     <paragraph>
      Finally, at first sight dropout seems like another clever hack. More careful analysis, however reveals an underlying web of elegant mathematical properties. This mathematical structure is unlikely to be the result of chance alone and leads one to suspect that dropout is more than a clever hack and that over time it may become an important concept for AI and machine learning.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Rectified linear transfer function without Gaussian assumption
     </section-title>
     <paragraph>
      Here we consider a rectified linear transfer function RE with threshold 0 and slope λ. If we assume that S is uniformly distributed over the interval {a mathematical formula}[−a,a] (similar considerations hold for intervals that are not symmetric), then {a mathematical formula}μS=0 and {a mathematical formula}σS=a/3. We have {a mathematical formula}RL(E(S))=0 and {a mathematical formula}E(RL(S))=∫0aλx(1/2a)dx=λa/4. In this case{a mathematical formula} This difference is small when the standard deviation is small, i.e. when a is small, and proportional to λ as in the Gaussian case. Alternatively, one can also consider m input (dropout) values {a mathematical formula}S1,…,Sm with probabilities {a mathematical formula}P1,…,Pm. We then have{a mathematical formula} and{a mathematical formula} Thus{a mathematical formula} In the usual case where {a mathematical formula}Pi=1/m this yields{a mathematical formula} Again these differences are proportional to λ and it is easy to show they are small if the standard deviation is small using, for instance, Tchebycheff's inequality.
     </paragraph>
    </section>
    <section label="Appendix B">
     <section-title>
      Expansion around the mean and around zero or one
     </section-title>
     <section label="B.1">
      <section-title>
       Expansion around the mean
      </section-title>
      <paragraph>
       Using the same notation as in Section 8, we consider the outputs {a mathematical formula}O1,…,Om of a sigmoidal neuron with associated probabilities {a mathematical formula}P1,…,Pm ({a mathematical formula}∑iPi=1) and {a mathematical formula}Oi=σ(Si). The difference here is that we expand around the mean and write {a mathematical formula}Oi=E+ϵi. As a result{a mathematical formula} and{a mathematical formula} In order to use the Binomial expansion, we must further assume that for every i, {a mathematical formula}|ϵi|&lt;min(E,1−E). In this case,{a mathematical formula} where {a mathematical formula}R3(ϵi) is the remainder of order three. Expanding and collecting terms gives{a mathematical formula} Noting that {a mathematical formula}∑iPiϵi=0, we finally have{a mathematical formula} and similarly by symmetry{a mathematical formula} As a result,{a mathematical formula} where {a mathematical formula}VE(1−E)⩽1 is a measure of how much the distribution deviates from the binomial case with the same mean. Combining the results above yields{a mathematical formula} In general, this approximation is slightly more accurate than the approximation obtained in Section 8 by expanding around 0.5 (Eq. (87)), as shown by Fig. 9.4, Fig. 9.5, however its range of validity may be slightly narrower.
      </paragraph>
     </section>
     <section label="B.2">
      <section-title>
       Expansion around zero or one
      </section-title>
      <paragraph>
       Consider the expansion around one with {a mathematical formula}Oi=1−ϵi, {a mathematical formula}G=∏i(1−ϵi)Pi, and {a mathematical formula}G′=∏i(ϵi)Pi. The binomial expansion requires {a mathematical formula}ϵi&lt;1, which is satisfied for every {a mathematical formula}Oi. We have{a mathematical formula} where {a mathematical formula}R3(ϵi) is the remainder of order three. Expanding and collecting terms gives{a mathematical formula} and{a mathematical formula} As a result,{a mathematical formula} Thus{a mathematical formula} and{a mathematical formula} This yields various approximate bounds{a mathematical formula} and{a mathematical formula} Over the interval {a mathematical formula}[0,1], the function {a mathematical formula}f(E)=E(1−E)1−E(1−E) is positive and concave down. It satisfies {a mathematical formula}f(E)=0 for {a mathematical formula}E=0 and {a mathematical formula}E=1, and reaches its maximum for {a mathematical formula}E=0.5 with {a mathematical formula}f(0.5)=13. Expansion around 0 is similar, interchanging the role of G and {a mathematical formula}G′ and yields{a mathematical formula} from which similar bounds on {a mathematical formula}|E−NWGM| can be derived.
      </paragraph>
     </section>
    </section>
    <section label="Appendix C">
     <section-title>
      Higher order moments
     </section-title>
     <paragraph>
      It would be useful to have better estimates of the variance V and potentially also of higher order moments. We have seen{a mathematical formula} Since {a mathematical formula}V=E(O2)−E(O)2=E(O2)−E2, one would like to estimate {a mathematical formula}E(O2) or, more generally, {a mathematical formula}E(Ok) and it is tempting to use the NWGM approach, since we already know from the general theory that {a mathematical formula}E(Ok)≈NWGM(Ok). This leads to{a mathematical formula} For {a mathematical formula}k=2 this gives{a mathematical formula} However one would have to calculate exactly or approximately the last term in the denominator above. More or less equivalently, one can use the general fact that {a mathematical formula}NWGM(σ(f(S)))=σ(E(f(S))), which leads in particular to{a mathematical formula} By inverting the sigmoidal function, we have{a mathematical formula} which can be expanded around E or around 0.5 using {a mathematical formula}log(1+u)=∑n=1∞(−1)n+1un/n for {a mathematical formula}|u|&lt;1. Expanding around 0.5, letting {a mathematical formula}O=05+ϵ, gives{a mathematical formula} where the last approximation is obtained by retaining only up to second order terms in the expansion. Thus with this approximation, we have{a mathematical formula} We already have an estimate for {a mathematical formula}E=E(O) provided by {a mathematical formula}NWGM(O). Thus any estimate of {a mathematical formula}E(S2) obtained directly, or through {a mathematical formula}NWGM(σ(S2)) by inverting Eq. (215), leads to an estimate of {a mathematical formula}(O2) through Eq. (218), and hence to an estimate of the variance V. And similarly for all higher order moments.
     </paragraph>
     <paragraph>
      However, in all these cases, additional costly information seem to be required, in order to get estimates of V that are sharper than those in Eq. (212), and one might as well directly sample the values {a mathematical formula}Oi.
     </paragraph>
    </section>
    <section label="Appendix D">
     <section-title>
      Derivatives of the logistic function and their expectations
     </section-title>
     <paragraph>
      For {a mathematical formula}σ(x)=1/(1+ce−λx), the first order derivative is given by {a mathematical formula}σ′(x)=λσ(x)(1−σ(x))=λce−λx/(1+ce−λx)2 and the second order derivative by {a mathematical formula}σ″(x)=λσ(x)(1−σ(x))(1−2σ(x)). As expected, when {a mathematical formula}λ&gt;0 the maximum of {a mathematical formula}σ′(x) is reached when {a mathematical formula}σ(x)=0.5 and is equal to {a mathematical formula}λ/4.
     </paragraph>
     <paragraph>
      As usual, let {a mathematical formula}Oi=σ(Si) for {a mathematical formula}i=1,…,m with corresponding probabilities {a mathematical formula}P1,…,Pm. To approximate {a mathematical formula}E(σ′(S)), we can apply the definition of the derivative{a mathematical formula} using the NWGM approximation to the expectation. Note that the NWGM approximation requires {a mathematical formula}0⩽σ′(Si)⩽1 for every i, which is always satisfied if {a mathematical formula}λ⩽4. Using a first order Taylor expansion, we finally get:{a mathematical formula}
     </paragraph>
     <paragraph>
      To derive another approximation to {a mathematical formula}E(σ′(S)), we have{a mathematical formula} As in most applications, we assume now that {a mathematical formula}c=λ=1 to slightly simplify the calculations since the odd terms in the Taylor expansion of the two exponential functions in the denominator cancel each other. In this case{a mathematical formula} Now different approximations can be derived by truncating the denominator. For instance, by retaining only the term corresponding to {a mathematical formula}n=1 in the sum and using {a mathematical formula}(1+x)α≈1+αx for x small, we finally have the approximation{a mathematical formula}
     </paragraph>
    </section>
    <section label="Appendix E">
     <section-title>
      Distributions
     </section-title>
     <paragraph>
      Here we look at the distribution of O and S, where {a mathematical formula}O=σ(S) under some simple assumptions.
     </paragraph>
     <section label="E.1">
      Assuming S has a Gaussian distribution
      <paragraph>
       Under various probabilistic assumptions, it is natural to assume that the incoming sum S into a neuron has a Gaussian distribution with mean μ and variance {a mathematical formula}σ2 with the density{a mathematical formula} In this case, the distribution of O is given by{a mathematical formula} which yields the density{a mathematical formula} In general this density is bell-shaped, similar but not identical to a beta density. For instance, if {a mathematical formula}μ=0 and {a mathematical formula}λ=c=1=σ{a mathematical formula}
      </paragraph>
     </section>
     <section label="E.2">
      The mean and variance of S
      <paragraph>
       Consider a sum of the form {a mathematical formula}S=∑i=1nwiOi. Assume that the weights have mean {a mathematical formula}μw and variance {a mathematical formula}σw2, the activities have mean {a mathematical formula}μO and variance {a mathematical formula}σO2, and the weights and the activities are independent of each other. Then, for n large, S is approximately Gaussian by the central limit theorem, with{a mathematical formula} and{a mathematical formula} In a typical case where {a mathematical formula}μw=0, the variance reduces to{a mathematical formula}
      </paragraph>
     </section>
     <section label="E.3">
      Assuming O has a Beta distribution
      <paragraph>
       The variable O is between 0 and 1 and thus it is natural to assume a Beta distribution with parameters {a mathematical formula}a⩾0 and {a mathematical formula}b⩾0 with the density{a mathematical formula} with the normalizing constant {a mathematical formula}B(a,b)=Γ(a+b)/Γ(a)Γ(b). In this case, the distribution of S is given by{a mathematical formula} which yields the density{a mathematical formula} In general this density is bell-shaped, similar but not identical to a Gaussian density. For instance, in the balanced case where {a mathematical formula}a=b,{a mathematical formula} Note, for instance, how this density at +∞ decays exponentially like {a mathematical formula}e−λas with a linear term in the exponent, rather than a quadratic one as in the exact Gaussian case.
      </paragraph>
     </section>
    </section>
    <section label="Appendix F">
     <section-title>
      Alternative estimate of the expectation
     </section-title>
     <paragraph>
      Here we describe an alternative way for obtaining a closed form estimate of {a mathematical formula}E(O) when {a mathematical formula}O=σ(S) and S has a Gaussian distribution with mean {a mathematical formula}μS and variance {a mathematical formula}σS2, which is a reasonable assumption in the case of dropout applied to large networks. It is known that the logistic function can be approximated by a cumulative Gaussian distribution in the form{a mathematical formula} where {a mathematical formula}Φμ,σ2(x)=∫−∞x12πσe−t2/2dt for a suitable value of α. Depending on the optimization criterion, different but reasonably close values of α can be found in the literature such as {a mathematical formula}α=0.607[21] or {a mathematical formula}α=1/1.702=0.587[15]. Just equating the first derivatives of the two functions at {a mathematical formula}S=0 gives {a mathematical formula}α=2π/4≈0.626. In what follows, we will use {a mathematical formula}α=0.607. In any case, for the more general logistic case, we have{a mathematical formula} As a result, in the general case,{a mathematical formula} It is easy to check that{a mathematical formula} Thus{a mathematical formula} where {a mathematical formula}Z|S is normally distributed with mean {a mathematical formula}−λS+logc and variance {a mathematical formula}1/α2. Thus Z is normally distributed with mean {a mathematical formula}−λμs+logc and variance {a mathematical formula}σS2+α−2, and the expectation can be estimated by{a mathematical formula} Finally, using in reverse the logistic approximation to the cumulative Gaussian distribution, we have{a mathematical formula} In the usual case where {a mathematical formula}c=λ=1 this gives{a mathematical formula} using {a mathematical formula}α=0.607 in the last approximation. In some cases this approximation to {a mathematical formula}E(O) may be more accurate than the NWGMS approximation but there is a tradeoff. This approximation requires a normal assumption on S, as well as knowing both the mean and the variance of S, whereas the NWGM approximation uses only the mean of S in the form {a mathematical formula}E(O)≈NWGM(O)=σ(E(S)). For small values of {a mathematical formula}σS2 the two approximations are similar. For very large values of {a mathematical formula}σS2, the estimate in Eq. (242) converges to 0.5 whereas the NWGM could be arbitrarily close to 0 or 1 depending on the values of {a mathematical formula}E(S)μS. In practice this is not observed because the size of the weights remains limited due to the dropout regularization effect, and thus the variance of S is also bounded.
     </paragraph>
     <paragraph>
      Note that for non-Gaussian distributions, artificial cases can be constructed where the discrepancy between E and the NWGM is even larger and goes all the way to 1. For example there is a large discrepancy for {a mathematical formula}S=−1/ϵ with probability {a mathematical formula}1−ϵ and {a mathematical formula}S=1/ϵ3 with probability ϵ, and ϵ close to 0. In this case {a mathematical formula}E(O)≈0 and {a mathematical formula}NWGM≈1.
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>