<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    POMDPs under probabilistic semantics.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      In this work, we consider partially observable Markov decision processes (POMDPs), with limit-average payoff, and study the problem of whether there exists a strategy to ensure that the payoff is above a given threshold with probability 1. Our main results characterize the decidability frontier of the problem, and for the decidable cases we establish tight computational complexity results. We first describe the details of the problem, the previous results, and then our contribution.
     </paragraph>
     <paragraph>
      Partially observable Markov decision processes (POMDPs).Markov decision processes (MDPs) are standard models for probabilistic systems that exhibit both probabilistic and nondeterministic behavior [20]. MDPs have been used to model and solve control problems for stochastic systems [16], [38]: nondeterminism represents the freedom of the controller to choose a control action, while the probabilistic component of the behavior describes the systems response to control actions. In perfect-observation (or perfect-information) MDPs the controller can observe the current state of the system to choose the next control actions, whereas in partially observable MDPs (POMDPs) the state space is partitioned according to observations that the controller can observe, i.e., given the current state, the controller can only view the observation of the state (the partition the state belongs to), but not the precise state [35]. POMDPs provide the appropriate model to study a wide variety of applications such as speech processing [33], image processing [13], robot planning [24], [21], computational biology [14], reinforcement learning [22], to name a few. POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [39], [36] (since probabilistic finite automata (a.k.a. blind POMDPs) are a special case of POMDPs with a single observation).
     </paragraph>
     <paragraph>
      Limit-average payoff. A payoff function maps every infinite path (infinite sequence of state action pairs) of an MDP to a real value. A very well-studied payoff in the setting of MDPs is the limit-average payoff where every state action pair is assigned a real-valued reward in the interval {a mathematical formula}[0,1] and the payoff of an infinite path is the long-run average of the rewards on the path [16, Chapter 5], [38, Chapter 8]. POMDPs with limit-average payoff provide the theoretical framework to study many important problems of practical relevance, including probabilistic planning and several stochastic optimization problems [30], [16], [38].
     </paragraph>
     <paragraph>
      Expectation vs probabilistic semantics. Traditionally, MDPs with limit-average payoff have been studied with the expectation semantics, where the goal of the controller is to maximize the expected limit-average payoff. The expected payoff value can be {a mathematical formula}12 when with probability {a mathematical formula}12 the payoff is 1, and with remaining probability the payoff is 0. In many applications of system analysis (such as robot planning and control) the relevant question is the probability measure of the paths that satisfy certain criteria, e.g., whether the probability measure of the paths such that the limit-average payoff is 1 (or the payoff is at least {a mathematical formula}12) is at least a given threshold (e.g., see [2], [24]). We classify the path constraints for limit-average payoff as follows: (1) the quantitative constraint that defines the set of paths with limit-average payoff at least {a mathematical formula}λ1, for a threshold {a mathematical formula}λ1∈(0,1]; and (2) the qualitative constraint that is the special case of the quantitative constraint defining the set of paths with limit-average payoff 1 (i.e., the special case with {a mathematical formula}λ1=1). We refer to the problem where the controller must satisfy a path constraint with a probability threshold {a mathematical formula}λ2∈(0,1] as the probabilistic semantics. An important special case of probabilistic semantics is the almost-sure semantics, where the probability threshold is 1. The almost-sure semantics is of great importance because there are many applications where the requirement is to know whether the correct behavior arises with probability 1. For instance, when analyzing a randomized embedded scheduler, the relevant question is whether every thread progresses with probability 1. Even in settings where it suffices to satisfy certain specifications with probability {a mathematical formula}λ2&lt;1, the correct choice of {a mathematical formula}λ2 is a challenging problem, due to the simplifications introduced during modeling. For example, in the analysis of randomized distributed algorithms it is quite common to require correctness with probability 1 (e.g., [37], [42]). Besides its importance in practical applications, almost-sure convergence, like convergence in expectation, is a fundamental concept in probability theory, and provides stronger convergence guarantee than convergence in expectation [15].
     </paragraph>
     <paragraph>
      Previous results. There are several deep undecidability results established for probabilistic finite automata (PFA), which are a special case of POMDPs (that immediately imply undecidability for the case of POMDPs).{sup:1} The basic undecidability results are for PFA over finite words: The emptiness problem for PFA under probabilistic semantics is undecidable over finite words [39], [36], [12]; and it was shown in [30] that even the following approximation version is undecidable: for any fixed {a mathematical formula}0&lt;ϵ&lt;12, given a probabilistic automaton and the guarantee that either (a) there is a word accepted with probability at least {a mathematical formula}1−ϵ; or (ii) all words are accepted with probability at most ϵ; decide whether it is case (i) or case (ii). The almost-sure problem for probabilistic automata over finite words reduces to the non-emptiness question of universal automata over finite words and is PSPACE-complete. However, another related decision question whether for every {a mathematical formula}ϵ&gt;0 there is a word that is accepted with probability at least {a mathematical formula}1−ϵ (called the value 1 problem) is undecidable for probabilistic automata over finite words [17]. Also observe that all undecidability results for probabilistic automata over finite words carry over to POMDPs where the controller is restricted to finite-memory strategies. The importance of finite-memory strategies in applications has been established in [18], [28], [31].
     </paragraph>
     <paragraph>
      Our contributions. Since under the general probabilistic semantics, the decision problems are undecidable even for PFA, we consider POMDPs with limit-average payoff under the almost-sure semantics. We present a complete picture of decidability as well as tight complexity bounds.
     </paragraph>
     <list>
      <list-item label="1.">
       (Almost-sure winning for qualitative constraints). We first consider limit-average payoff with a qualitative constraint under almost-sure semantics. We show that belief-support-based strategies are not sufficient (where a belief-support-based strategy is based on the subset construction that remembers the possible set of current states): we show that there exist POMDPs with limit-average payoff with a qualitative constraint where finite-memory almost-sure winning strategy exists but there exists no belief-support-based almost-sure winning strategy. Our counter-example shows that standard techniques based on subset construction (to construct an exponential size perfect-observation MDP) are not adequate to solve the problem. We then show one of our main results that given a POMDP with {a mathematical formula}|S| states and {a mathematical formula}|A| actions, if there is a finite-memory almost-sure winning strategy to satisfy the limit-average payoff with a qualitative constraint, then there is an almost-sure winning strategy that uses at most {a mathematical formula}23⋅|S|+|A| memory. Our exponential memory upper bound is asymptotically optimal, as even for PFA over finite words, exponential memory is required for almost-sure winning. This follows from the fact that the shortest witness word for non-emptiness of universal finite automata is at least exponential. We then show that the problem of deciding the existence of finite-memory almost-sure winning strategy for limit-average payoff with a qualitative constraint is EXPTIME-complete for POMDPs. In contrast to our result for finite-memory strategies, we establish that the problem of deciding the existence of infinite-memory almost-sure winning strategy for limit-average payoff with a qualitative constraint is undecidable for POMDPs.
      </list-item>
      <list-item label="2.">
       (Almost-sure winning with quantitative constraints). In contrast to our decidability result under finite-memory strategies for qualitative constraints, we show that the almost-sure winning problem for limit-average payoff with a quantitative constraint is undecidable even for finite-memory strategies for POMDPs.
      </list-item>
     </list>
     <paragraph>
      In summary we establish the precise decidability frontier for POMDPs with limit-average payoff under probabilistic semantics (see Table 1). For practical purposes, the most prominent question is the problem of finite-memory strategies, and for finite-memory strategies we establish decidability with optimal EXPTIME-complete complexity for the important special case of qualitative constraints under almost-sure semantics. We have also implemented our algorithm (along with a heuristic to avoid the worst-case exponential complexity) for almost-sure winning under finite-memory strategies for qualitative constraints. We experimented with our prototype on variants of several POMDP examples from the literature, and the implementation worked reasonably efficiently.
     </paragraph>
     <paragraph>
      Technical contributions. The key technical contribution for the decidability result is as follows. Since belief-support-based strategies are not sufficient, standard subset construction techniques do not work. For an arbitrary finite-memory strategy we construct a collapsed strategy that collapses memory states based on a graph construction given the strategy. The collapsed strategy at a collapsed memory state plays uniformly over actions that were played at all the corresponding memory states of the original strategy. The main challenge is to show that the exponential size collapsed strategy, even though it has less memory elements, does not destroy the structure of the recurrent classes of the original strategy. For the computational complexity result, we show how to construct an exponential size special class of POMDPs (which we call belief-observation POMDPs where the belief support is always the current observation) and present polynomial time algorithms for the solution of the special belief-observation POMDPs of our construction. For undecidability of almost-sure winning for qualitative constraints under infinite-memory strategies we present a reduction from the value 1 problem for PFA; and for undecidability of almost-sure winning for quantitative constraints under finite-memory strategies we present a reduction from the strict emptiness problem for PFA under probabilistic semantics.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Definitions
     </section-title>
     <paragraph>
      In this section we present the definitions of POMDPs, strategies, objectives, and other basic definitions required throughout this work. We follow the standard definitions of MDPs and POMDPs [38], [26].
     </paragraph>
     <paragraph>
      Notations. Given a finite set X, we denote by {a mathematical formula}P(X) the set of subsets of X, i.e., {a mathematical formula}P(X) is the power set of X. A probability distribution f on X is a function {a mathematical formula}f:X→[0,1] such that {a mathematical formula}∑x∈Xf(x)=1, and we denote by {a mathematical formula}D(X) the set of all probability distributions on X. For {a mathematical formula}f∈D(X) we denote by {a mathematical formula}Supp(f)={x∈X|f(x)&gt;0} the support of f.
     </paragraph>
     <paragraph label="Definition 1">
      POMDPA Partially Observable Markov Decision Process (POMDP) is a tuple {a mathematical formula}G=(S,A,δ,O,γ,s0) where:
     </paragraph>
     <list>
      <list-item label="•">
       S is a finite set of states;
      </list-item>
      <list-item label="•">
       {a mathematical formula}A is a finite alphabet of actions;
      </list-item>
      <list-item label="•">
       {a mathematical formula}δ:S×A→D(S) is a probabilistic transition function that given a state s and an action {a mathematical formula}a∈A gives the probability distribution over the successor states, i.e., {a mathematical formula}δ(s,a)(s′) denotes the transition probability from state s to state {a mathematical formula}s′ given action a;
      </list-item>
      <list-item label="•">
       {a mathematical formula}O is a finite set of observations;
      </list-item>
      <list-item label="•">
       {a mathematical formula}γ:S→O is an observation function that maps every state to an observation; and
      </list-item>
      <list-item label="•">
       {a mathematical formula}s0 is the initial state.
      </list-item>
     </list>
     <paragraph>
      Given {a mathematical formula}s,s′∈S and {a mathematical formula}a∈A, we also write {a mathematical formula}δ(s′|s,a) for {a mathematical formula}δ(s,a)(s′). A state s is absorbing if for all actions a we have {a mathematical formula}δ(s,a)(s)=1 (i.e., s is never left from s). For an observation o, we denote by {a mathematical formula}γ−1(o)={s∈S|γ(s)=o} the set of states with observation o. For a set {a mathematical formula}U⊆S of states and {a mathematical formula}O⊆O of observations we denote {a mathematical formula}γ(U)={o∈O|∃s∈U.γ(s)=o} and {a mathematical formula}γ−1(O)=⋃o∈Oγ−1(o).
     </paragraph>
     <paragraph label="Remark 1">
      Unique initial stateFor technical convenience we assume that there is a unique initial state {a mathematical formula}s0 and we will also assume that the initial state has a unique observation, i.e., {a mathematical formula}|γ−1(γ(s0))|=1. In general there is an initial distribution α over initial states that all have the same observation, i.e., {a mathematical formula}Supp(α)⊆γ−1(o), for some {a mathematical formula}o∈O. However, this can be modeled easily by adding a new initial state {a mathematical formula}snew with a unique observation such that in the first step gives the desired initial probability distribution α, i.e., {a mathematical formula}δ(snew,a)=α for all actions {a mathematical formula}a∈A. Hence for simplicity we assume there is a unique initial state {a mathematical formula}s0 with a unique observation.
     </paragraph>
     <paragraph label="Remark 2">
      ObservationsWe remark about two other general cases of observations.
     </paragraph>
     <list>
      Thus both the above general cases of observation function can be reduced to observation mapping that deterministically assigns an observation to a state, and we consider such observation mapping which greatly simplifies the notation.
     </list>
     <paragraph>
      Plays and cones. A play (or a path) in a POMDP is an infinite sequence {a mathematical formula}(s0,a0,s1,a1,s2,a2,…) of states and actions such that for all {a mathematical formula}i≥0 we have {a mathematical formula}δ(si,ai)(si+1)&gt;0. We write Ω for the set of all plays. For a finite prefix {a mathematical formula}w∈(S⋅A)⁎⋅S of a play, we denote by {a mathematical formula}Cone(w) the set of plays with w as the prefix (i.e., the cone or cylinder of the prefix w), and denote by {a mathematical formula}Last(w) the last state of w.
     </paragraph>
     <paragraph>
      Belief supports and updates. For a finite prefix {a mathematical formula}w=(s0,a0,s1,a1,…,sn) we denote by {a mathematical formula}γ(w)=(γ(s0),a0,γ(s1),a1,…,γ(sn)) the observation and action sequence associated with w. For a finite sequence {a mathematical formula}ρ=(o0,a0,o1,a1,…,on) of observations and actions, the belief support{a mathematical formula}B(ρ) after the prefix ρ is the set of states in which a finite prefix of a play can be after the sequence ρ of observations and actions, i.e., {a mathematical formula}B(ρ)={sn=Last(w)|w=(s0,a0,s1,a1,…,sn),w is a prefix of a play,and for all 0≤i≤n.γ(si)=oi}. The belief-support updates associated with finite-prefixes are as follows: for prefixes w and {a mathematical formula}w′=w⋅a⋅s the belief-support update is defined inductively as {a mathematical formula}B(γ(w′))=(⋃s1∈B(γ(w))Supp(δ(s1,a)))∩γ−1(γ(s)), i.e., the set {a mathematical formula}(⋃s1∈B(γ(w))Supp(δ(s1,a))) denotes the possible successors given the belief support {a mathematical formula}B(γ(w)) and action a, and then the intersection with the set of states with the current observation {a mathematical formula}γ(s) gives the new belief support. Informally, the belief (or a belief state [5]) defines the probability distributions over the possible current states, and the belief support is the support of the belief.
     </paragraph>
     <paragraph>
      Strategies. A strategy (or a policy) is a recipe to extend prefixes of plays and is a function {a mathematical formula}σ:(S⋅A)⁎⋅S→D(A) that given a finite history (i.e., a finite prefix of a play) selects a probability distribution over the actions. Since we consider POMDPs, strategies are observation-based, i.e., for all histories {a mathematical formula}w=(s0,a0,s1,a1,…,an−1,sn) and {a mathematical formula}w′=(s0′,a0,s1′,a1,…,an−1,sn′) such that for all {a mathematical formula}0≤i≤n we have {a mathematical formula}γ(si)=γ(si′) (i.e., {a mathematical formula}γ(w)=γ(w′)), we must have {a mathematical formula}σ(w)=σ(w′). In other words, if the observation sequence is the same, then the strategy cannot distinguish between the prefixes and must play the same. We now present an equivalent definition of observation-based strategies such that the memory of the strategy is explicitly specified, and will be required to present finite-memory strategies.
     </paragraph>
     <paragraph label="Definition 2">
      Strategies with memory and finite-memory strategiesA strategy with memory is a tuple {a mathematical formula}σ=(σu,σn,M,m0) where:
     </paragraph>
     <list>
      <list-item label="•">
       (Memory set). M is a denumerable set (finite or infinite) of memory elements (or memory states).
      </list-item>
      <list-item label="•">
       (Action selection function). The function {a mathematical formula}σn:M→D(A) is the next action selection function that given the current memory state gives the probability distribution over actions.
      </list-item>
      <list-item label="•">
       (Memory update function). The function {a mathematical formula}σu:M×O×A→D(M) is the memory update function that given the current memory state, the current observation and action, updates the memory state probabilistically.
      </list-item>
      <list-item label="•">
       (Initial memory). The memory state {a mathematical formula}m0∈M is the initial memory state.
      </list-item>
     </list>
     <paragraph>
      Probability measure. Given a strategy σ, the unique probability measure obtained given σ is denoted as {a mathematical formula}Pσ(⋅). We first define the measure {a mathematical formula}μσ(⋅) on cones. For {a mathematical formula}w=s0 we have {a mathematical formula}μσ(Cone(w))=1, and for {a mathematical formula}w=s where {a mathematical formula}s≠s0 we have {a mathematical formula}μσ(Cone(w))=0; and for {a mathematical formula}w′=w⋅a⋅s we have {a mathematical formula}μσ(Cone(w′))=μσ(Cone(w))⋅σ(w)(a)⋅δ(Last(w),a)(s). By Caratheódary's extension theorem, the function {a mathematical formula}μσ(⋅) can be uniquely extended to a probability measure {a mathematical formula}Pσ(⋅) over Borel sets of infinite plays [3].
     </paragraph>
     <paragraph>
      Objectives. An objective in a POMDP G is a measurable set {a mathematical formula}φ⊆Ω of plays. We first define the limit-average payoff (a.k.a. mean-payoff) function. Given a POMDP we consider a reward function {a mathematical formula}r:S×A→[0,1] that maps every state action pair to a real-valued reward in the interval {a mathematical formula}[0,1]. The {a mathematical formula}LimAvg payoff function maps every play to a real-valued reward that is the long-run average of the rewards of the play. Formally, given a play {a mathematical formula}ρ=(s0,a0,s1,a1,s2,a2,…) we have{a mathematical formula} When the reward function {a mathematical formula}r is clear from the context, we drop it for simplicity. For a reward function {a mathematical formula}r, we consider two types of limit-average payoff constraints.
     </paragraph>
     <list>
      <list-item label="1.">
       Qualitative constraints. The qualitative constraint limit-average objective {a mathematical formula}LimAvg=1 defines the set of paths such that the limit-average payoff is 1; i.e., {a mathematical formula}LimAvg=1={ρ|LimAvg(ρ)=1}.
      </list-item>
      <list-item label="2.">
       Quantitative constraints. Given a threshold {a mathematical formula}λ1∈(0,1), the quantitative constraint limit-average objective {a mathematical formula}LimAvg&gt;λ1 defines the set of paths such that the limit-average payoff is strictly greater than {a mathematical formula}λ1; i.e., {a mathematical formula}LimAvg&gt;λ1={ρ|LimAvg(ρ)&gt;λ1}.
      </list-item>
     </list>
     <paragraph>
      Probabilistic and almost-sure winning. Given a POMDP, an objective φ, and a class {a mathematical formula}C of strategies, we say that:
     </paragraph>
     <list>
      <list-item label="•">
       a strategy {a mathematical formula}σ∈C is almost-sure winning if {a mathematical formula}Pσ(φ)=1;
      </list-item>
      <list-item label="•">
       a strategy {a mathematical formula}σ∈C is probabilistic winning, for a threshold {a mathematical formula}λ2∈(0,1), if {a mathematical formula}Pσ(φ)≥λ2.
      </list-item>
     </list>
     <paragraph label="Theorem 1">
      Results for PFA (probabilistic automata over finite words)(See[36].) The following assertions hold for the class{a mathematical formula}Cof all infinite-memory as well as finite-memory strategies: (1) the probabilistic winning problem is undecidable for PFA; and (2) the almost-sure winning problem is PSPACE-complete for PFA.
     </paragraph>
     <paragraph>
      Since PFA are a special case of POMDPs, the undecidability of the probabilistic winning problem for PFA implies the undecidability of the probabilistic winning problem for POMDPs with both qualitative and quantitative constraints limit-average objectives. The almost-sure winning problem is PSPACE-complete for PFAs, and we study the complexity of the almost-sure winning problem for POMDPs with both qualitative and quantitative constraints limit-average objectives, under infinite-memory and finite-memory strategies.
     </paragraph>
     <paragraph>
      Basic properties of Markov chains. Since our proofs will use results related to Markov chains, we start with some basic definitions and properties related to Markov chains [23].
     </paragraph>
     <paragraph>
      Markov chains and recurrent classes. A Markov chain {a mathematical formula}G¯=(S¯,δ¯) consists of a finite set {a mathematical formula}S¯ of states and a probabilistic transition function {a mathematical formula}δ¯:S¯→D(S¯). Given the Markov chain, we consider the directed graph {a mathematical formula}(S¯,E¯) where {a mathematical formula}E¯={(s¯,s¯′)|δ(s¯′|s¯)&gt;0}. A recurrent class{a mathematical formula}C¯⊆S¯ of the Markov chain is a bottom strongly connected component (scc) in the graph {a mathematical formula}(S¯,E¯) (a bottom scc is an scc with no edges out of the scc). We denote by {a mathematical formula}Rec(G¯) the set of recurrent classes of the Markov chain, i.e., {a mathematical formula}Rec(G¯)={C¯|C¯ is a recurrent class}. Given a state {a mathematical formula}s¯ and a set {a mathematical formula}U¯ of states, we say that {a mathematical formula}U¯ is reachable from {a mathematical formula}s¯ if there is a path from {a mathematical formula}s¯ to some state in {a mathematical formula}U¯ in the graph {a mathematical formula}(S¯,E¯). Given a state {a mathematical formula}s¯ of the Markov chain we denote by {a mathematical formula}Rec(G¯)(s¯)⊆Rec(G¯) the subset of the recurrent classes reachable from {a mathematical formula}s¯ in {a mathematical formula}G¯. A state is recurrent if it belongs to a recurrent class. The following standard properties of reachability and the recurrent classes will be used in our proofs:
     </paragraph>
     <list>
      <list-item label="•">
       Property 1. (a) For a set {a mathematical formula}T¯⊆S¯, if for all states {a mathematical formula}s¯∈S¯ there is a path to {a mathematical formula}T¯ (i.e., for all states there is a positive probability to reach {a mathematical formula}T¯), then from all states the set {a mathematical formula}T¯ is reached with probability 1. (b) For all states {a mathematical formula}s¯, if the Markov chain starts at {a mathematical formula}s¯, then the set {a mathematical formula}C¯=⋃C¯∈Rec(G¯)(s¯)C¯ is reached with probability 1, i.e., the set of recurrent classes is reached with probability 1.
      </list-item>
      <list-item label="•">
       Property 2. For a recurrent class {a mathematical formula}C¯, for all states {a mathematical formula}s¯∈C¯, if the Markov chain starts at {a mathematical formula}s¯, then for all states {a mathematical formula}t¯∈C¯ the state {a mathematical formula}t¯ is visited infinitely often with probability 1, and is visited with positive average frequency (i.e., positive limit-average frequency) with probability 1.
      </list-item>
     </list>
     <paragraph>
      The following lemma is an easy consequence of the above properties.
     </paragraph>
     <paragraph label="Lemma 1">
      Let{a mathematical formula}G¯=(S¯,δ¯)be a Markov chain with a reward function{a mathematical formula}r:S¯→[0,1]. Consider a state{a mathematical formula}s¯∈S¯and the Markov chain with{a mathematical formula}s¯as the initial (or starting) state. The objective{a mathematical formula}LimAvg=1is satisfied with probability 1 (almost-surely) iff for all recurrent classes{a mathematical formula}C¯∈Rec(G¯)(s¯)and for all states{a mathematical formula}s¯1∈C¯we have{a mathematical formula}r(s¯1)=1.
     </paragraph>
     <paragraph>
      Markov chains under finite memory strategies. We now define Markov chains obtained by fixing a finite-memory strategy in a POMDP G. A finite-memory strategy {a mathematical formula}σ=(σu,σn,M,m0) induces a Markov chain {a mathematical formula}(S×M,δσ), denoted {a mathematical formula}G↾σ, with the probabilistic transition function {a mathematical formula}δσ:S×M→D(S×M): given {a mathematical formula}s,s′∈S and {a mathematical formula}m,m′∈M, the transition {a mathematical formula}δσ((s′,m′)|(s,m)) is the probability to go from state {a mathematical formula}(s,m) to state {a mathematical formula}(s′,m′) in one step under the strategy σ. The probability of transition can be decomposed as follows:
     </paragraph>
     <list>
      <list-item label="•">
       first an action {a mathematical formula}a∈A is sampled according to the distribution {a mathematical formula}σn(m);
      </list-item>
      <list-item label="•">
       then the next state {a mathematical formula}s′ is sampled according to the distribution {a mathematical formula}δ(s,a); and
      </list-item>
      <list-item label="•">
       finally the new memory {a mathematical formula}m′ is sampled according to the distribution {a mathematical formula}σu(m,γ(s′),a) (i.e., the new memory is sampled according {a mathematical formula}σu given the old memory, new observation and the action).
      </list-item>
     </list>
     <paragraph>
      More formally, we have:{a mathematical formula} Given {a mathematical formula}s∈S and {a mathematical formula}m∈M, we write {a mathematical formula}(G↾σ)(s,m) for the finite state Markov chain induced on {a mathematical formula}S×M by the transition function {a mathematical formula}δσ, given the initial state is {a mathematical formula}(s,m).
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Finite-memory strategies with qualitative constraints
     </section-title>
     <paragraph>
      In this section we will establish the following three results for finite-memory strategies: (i) we show that in POMDPs with {a mathematical formula}LimAvg=1 objectives belief-support-based strategies are not sufficient for almost-sure winning; (ii) we establish an exponential upper bound on the memory required by an almost-sure winning strategy for {a mathematical formula}LimAvg=1 objectives; and (iii) we show that the decision problem is EXPTIME-complete.
     </paragraph>
     <section label="3.1">
      <section-title>
       Belief support is not sufficient
      </section-title>
      <paragraph>
       We now show with an example that there exist POMDPs with {a mathematical formula}LimAvg=1 objectives, where finite-memory randomized almost-sure winning strategies exist, but there exists no belief-support-based randomized almost-sure winning strategy (a belief-support-based strategy only uses memory that relies on the subset construction where the subset denotes the possible current states called the belief support). In our example we will present the counter-example even for POMDPs with a restricted reward function {a mathematical formula}r assigning only Boolean rewards 0 and 1 to the states (the reward does not depend on the action played but only on the states of the POMDP).
      </paragraph>
      <paragraph label="Example 1">
       We consider a POMDP with a state space {a mathematical formula}{s0,X,Y,Z} and an action set {a mathematical formula}{a,b}, and let {a mathematical formula}U={X,Y,Z}. From the initial state {a mathematical formula}s0 all the other states are reached with uniform probability in one-step, i.e., for all {a mathematical formula}s′∈U={X,Y,Z} we have {a mathematical formula}δ(s0,a)(s′)=δ(s0,b)(s′)=13. The transitions from the other states are as follows (shown in Fig. 1): (i) {a mathematical formula}δ(X,a)(Z)=1 and {a mathematical formula}δ(X,b)(Y)=1; (ii) {a mathematical formula}δ(Z,a)(Y)=1 and {a mathematical formula}δ(Z,b)(X)=1; and (iii) {a mathematical formula}δ(Y,a)(X)=δ(Y,a)(s0)=δ(Y,a)(Z)=δ(Y,b)(X)=δ(Y,b)(s0)=δ(Y,b)(Z)=13. All states in U have the same observation. The initial state {a mathematical formula}s0 has its own observation. The reward function {a mathematical formula}r assigns reward 1 to states in U and reward 0 to the initial state {a mathematical formula}s0.The belief support initially after one-step is the set {a mathematical formula}U={X,Y,Z} since from {a mathematical formula}s0 all the states in U are reached with positive probability. The belief support can only be the set {a mathematical formula}{s0} or U since every state in U has an input edge for every action, i.e., if the current belief support is U (i.e., the set of states that the POMDP is currently in with positive probability is U), then irrespective of whether a or b is chosen either the observation of the initial state {a mathematical formula}s0 is observed or all states of U are reached with positive probability and hence the belief support is again U. As playing action a in the initial state has the same effect as playing action b, there are effectively three belief-support-based strategies: (i) {a mathematical formula}σ1 that plays always a in belief support U; (ii) {a mathematical formula}σ2 that plays always b in belief support U; or (iii) {a mathematical formula}σ3 that plays both a and b in belief support U with positive probability. The Markov chains {a mathematical formula}G↾σ1 (resp. {a mathematical formula}σ2 and {a mathematical formula}σ3) are obtained by retaining the edges labeled by action a (resp. action b, and both actions a and b). For all three strategies, the Markov chains obtained contain the whole state space {a mathematical formula}{s0}∪U as the reachable recurrent class. It follows that in all the Markov chains there exists a reachable recurrent class containing a state with reward 0, and by Lemma 1 none of the belief-support-based strategies {a mathematical formula}σ1,σ2 or {a mathematical formula}σ3 is almost-sure winning for the {a mathematical formula}LimAvg=1 objective.The Markov chains {a mathematical formula}G↾σ1 and {a mathematical formula}G↾σ2 are also shown in Fig. 1, and the recurrent class is {a mathematical formula}{s0,X,Y,Z}. The graph of {a mathematical formula}G↾σ3 is the same as the POMDP G (with edge labels removed), and the recurrent class is again {a mathematical formula}{s0,X,Y,Z}. The strategy {a mathematical formula}σ4 that plays action a and b alternately gives rise to the Markov chain {a mathematical formula}G↾σ4 (shown in Fig. 2) where the recurrent class does not intersect with {a mathematical formula}s0, and is a finite-memory almost-sure winning strategy for the {a mathematical formula}LimAvg=1 objective.  □
      </paragraph>
      <paragraph>
       Significance ofExample 1. We now discuss the significance of Example 1. If we consider the problem of almost-sure winning for PFA, then it coincides with non-emptiness of universal automata, because if there is a finite word that is accepted with probability 1 then it must be accepted along all probabilistic branches (i.e., the probabilistic choices can be interpreted as universal choices). Since the non-emptiness problem of universal automata is similar to universality of non-deterministic automata, the almost-sure winning problem for PFA can be solved by the standard subset construction for automata. The implication of our counter-example (Example 1) shows that since belief-support-based strategies are not sufficient even for qualitative limit-average objectives, the standard subset construction techniques from automata theory will not suffice to solve the problem.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Strategy complexity
      </section-title>
      <paragraph>
       In this section we will present memory bounds for finite-memory almost-sure winning strategies. Hence we assume that there exists a finite-memory almost-sure winning strategy, and for the rest of the subsection we fix a finite-memory almost-sure winning strategy {a mathematical formula}σ=(σu,σn,M,m0) on the POMDP {a mathematical formula}G=(S,A,δ,O,γ,s0) with a reward function {a mathematical formula}r for the objective {a mathematical formula}LimAvg=1. Our goal is to construct an almost-sure winning strategy for the {a mathematical formula}LimAvg=1 objective with memory size (i.e., the number of memory state) at most {a mathematical formula}Mem⁎=23⋅|S|⋅2|A|. We start with a few definitions associated with strategy σ. For {a mathematical formula}m∈M:
      </paragraph>
      <list>
       <list-item label="•">
        The function {a mathematical formula}RecFunσ(m):S→{0,1} is such that {a mathematical formula}RecFunσ(m)(s) is 1 iff the state {a mathematical formula}(s,m) is recurrent in the Markov chain {a mathematical formula}G↾σ and 0 otherwise. (The {a mathematical formula}RecFun stands for recurrence function.)
       </list-item>
       <list-item label="•">
        The function {a mathematical formula}AWFunσ(m):S→{0,1} is such that {a mathematical formula}AWFunσ(m)(s) is 1 iff the state {a mathematical formula}(s,m) is almost-sure winning for the {a mathematical formula}LimAvg=1 objective in the Markov chain {a mathematical formula}G↾σ and 0 otherwise. (The {a mathematical formula}AWFun stands for almost-sure win function.)
       </list-item>
       <list-item label="•">
        We also consider {a mathematical formula}Actσ(m)=Supp(σn(m)) that for every memory element gives the support of the probability distribution over actions played at m.
       </list-item>
      </list>
      <paragraph label="Remark 3">
       Let {a mathematical formula}(s′,m′) be a state reachable from {a mathematical formula}(s,m) in the Markov chain {a mathematical formula}G↾σ. If the state {a mathematical formula}(s,m) is almost-sure winning for the {a mathematical formula}LimAvg=1 objective, then the state {a mathematical formula}(s′,m′) is also almost-sure winning for the {a mathematical formula}LimAvg=1 objective.Collapsed graph ofσ. Given the strategy σ we define the notion of a collapsedgraph{a mathematical formula}CoGr(σ)=(V,E), where the vertices of the graph are elements from the set {a mathematical formula}V={(Y,AWFunσ(m),RecFunσ(m),Actσ(m))|Y⊆S and m∈M} and the initial vertex is {a mathematical formula}({s0},AWFunσ(m0),RecFunσ(m0),Actσ(m0)). The edges in E are labeled by actions in {a mathematical formula}A. Intuitively, the action labeled edges of the graph depict the updates of the belief support and the functions upon a particular action. Formally, there is an edge{a mathematical formula} in the collapsedgraph {a mathematical formula}CoGr(σ) iff there exists an observation {a mathematical formula}o∈O such that
      </paragraph>
      <list>
       <list-item label="1.">
        the action {a mathematical formula}a∈Actσ(m),
       </list-item>
       <list-item label="2.">
        the set {a mathematical formula}Y′ is non-empty and it is the belief-support update from Y, under action a and the observation o, i.e., {a mathematical formula}Y′=⋃s∈YSupp(δ(s,a))∩γ−1(o), and
       </list-item>
       <list-item label="3.">
        {a mathematical formula}m′∈Supp(σu(m,o,a)).
       </list-item>
      </list>
      <paragraph>
       Note that the number of vertices in the graph is bounded by {a mathematical formula}|V|≤Mem⁎.
      </paragraph>
      <paragraph>
       In the following lemma we establish the connection of the functions {a mathematical formula}RecFunσ(m) and {a mathematical formula}AWFunσ(m) with the edges of the collapsedgraph. Intuitively the lemma shows that when the function {a mathematical formula}RecFunσ(m) (resp. {a mathematical formula}AWFunσ(m)) is set to 1 for a state s of a vertex of the collapsedgraph, then for all successor vertices along the edges in the collapsedgraph, the function {a mathematical formula}RecFun (resp. {a mathematical formula}AWFun) is also set to 1 for the successors of state s.
      </paragraph>
      <paragraph label="Lemma 2">
       Let{a mathematical formula}(Y,W,R,A)→a(Y′,W′,R′,A′)be an edge in the collapsedgraph{a mathematical formula}CoGr(σ)=(V,E). Then for all{a mathematical formula}s∈Ythe following assertions hold:
      </paragraph>
      <list>
       <list-item label="1.">
        If{a mathematical formula}W(s)=1, then for all{a mathematical formula}s′∈Supp(δ(s,a))∩Y′we have that{a mathematical formula}W′(s′)=1.
       </list-item>
       <list-item label="2.">
        If{a mathematical formula}R(s)=1, then for all{a mathematical formula}s′∈Supp(δ(s,a))∩Y′we have that{a mathematical formula}R′(s′)=1.
       </list-item>
      </list>
      <paragraph label="Proof">
       We present proof of both the items below.
      </paragraph>
      <list>
       <list-item label="1.">
        Let {a mathematical formula}(Y,W,R,A)→a(Y′,W′,R′,A′) be an edge in the collapsedgraph {a mathematical formula}CoGr(σ)=(V,E) and a state {a mathematical formula}s∈Y such that {a mathematical formula}W(s)=1. It follows that there exist memories {a mathematical formula}m,m′∈M and an observation {a mathematical formula}o∈O such that (i) {a mathematical formula}W=AWFunσ(m); (ii) {a mathematical formula}W′=AWFunσ(m′); (iii) {a mathematical formula}a∈Supp(σn(m)); (iv) {a mathematical formula}m′∈Supp(σu(m,o,a)); and finally (v) {a mathematical formula}O(s′)=o. From all the points above it follows that there exists an edge {a mathematical formula}(s,m)→(s′,m′) in the Markov chain {a mathematical formula}G↾σ. As the state {a mathematical formula}(s,m) is almost-sure winning (since {a mathematical formula}W(s)=1) it follows by Remark 3 that the state {a mathematical formula}(s′,m′) must also be almost-sure winning and therefore {a mathematical formula}W′(s′)=1.
       </list-item>
       <list-item label="2.">
        As in the proof of the first part we have that {a mathematical formula}(s′,m′) is reachable from {a mathematical formula}(s,m) in the Markov chain {a mathematical formula}G↾σ. As every state reachable from a recurrent state in a Markov chain is also recurrent we have {a mathematical formula}R′(s′)=1.
       </list-item>
      </list>
      <paragraph>
       We now define the collapsedstrategy for σ. Intuitively we collapse memory elements of the original strategy σ whenever they agree on all the {a mathematical formula}RecFun, {a mathematical formula}AWFun, and {a mathematical formula}Act functions. The collapsedstrategy plays uniformly all the actions from the set given by {a mathematical formula}Act in the collapsed state.
      </paragraph>
      <paragraph>
       Collapsed strategy. We now construct the collapsedstrategy{a mathematical formula}σ′=(σu′,σn′,M′,m0′) of σ based on the collapsedgraph {a mathematical formula}CoGr(σ)=(V,E). We will refer to this construction by {a mathematical formula}σ′=CoSt(σ).
      </paragraph>
      <list>
       <list-item label="•">
        The memory set {a mathematical formula}M′ is the vertices of the collapsedgraph {a mathematical formula}CoGr(σ)=(V,E), i.e., {a mathematical formula}M′=V={(Y,AWFunσ(m),RecFunσ(m),Actσ(m))|Y⊆S and m∈M}.
       </list-item>
       <list-item label="•">
        The initial memory is {a mathematical formula}m0′=({s0},AWFunσ(m0),RecFunσ(m0),Actσ(m0)).
       </list-item>
       <list-item label="•">
        The next action function given a memory {a mathematical formula}(Y,W,R,A)∈M′ is the uniform distribution over the set of actions {a mathematical formula}{a|∃(Y′,W′,R′,A′)∈M′ and (Y,W,R,A)→a(Y′,W′,R′,A′)∈E}, where E are the edges of the collapsedgraph.
       </list-item>
       <list-item label="•">
        The memory update function {a mathematical formula}σu′((Y,W,R,A),o,a) given a memory element {a mathematical formula}(Y,W,R,A)∈M′, {a mathematical formula}a∈A, and {a mathematical formula}o∈O is the uniform distribution over the set of vertices {a mathematical formula}{(Y′,W′,R′,A′)|(Y,W,R,A)→a(Y′,W′,R′,A′)∈E and Y′⊆γ−1(o)}.
       </list-item>
      </list>
      <paragraph>
       The following lemma intuitively shows that the collapsedstrategy can reach all the states the original strategy could reach.
      </paragraph>
      <paragraph label="Proof">
       Let{a mathematical formula}σ′=CoSt(σ)be the collapsedstrategy,{a mathematical formula}s,s′∈S, and{a mathematical formula}m,m′∈M. If{a mathematical formula}(s′,m′)is reachable from{a mathematical formula}(s,m)in{a mathematical formula}G↾σ, then for all belief supports{a mathematical formula}Y⊆Swith{a mathematical formula}s∈Ythere exists a belief support{a mathematical formula}Y′⊆Swith{a mathematical formula}s′∈Y′such that the state{a mathematical formula}(s′,Y′,AWFunσ(m′),RecFunσ(m′),Actσ(m′))is reachable from{a mathematical formula}(s,Y,AWFunσ(m),RecFunσ(m),Actσ(m))in{a mathematical formula}G↾σ′We will start the proof with one step reachability first. Assume there is an edge {a mathematical formula}(s,m)→(s′,m′) in the Markov chain {a mathematical formula}G↾σ and the action labeling the transition in the POMDP is a, i.e., (i) {a mathematical formula}s′∈Supp(δ(s,a)); (ii) {a mathematical formula}a∈Supp(σn(m)); and (iii) {a mathematical formula}m′∈Supp(σu(m,γ(s′),a)). Let {a mathematical formula}(s,Y,AWFunσ(m),RecFunσ(m),Actσ(m)) be a state in {a mathematical formula}G↾σ′. It follows that {a mathematical formula}a∈Actσ(m) and there exists {a mathematical formula}Y′⊆S and an edge {a mathematical formula}(Y,AWFunσ(m),RecFunσ(m),Actσ(m))→a(Y′,AWFunσ(m′),RecFunσ(m′),Actσ(m′)) in the collapsedgraph {a mathematical formula}CoGr(σ). Therefore, action a is played with positive probability by the strategy {a mathematical formula}σ′ and with positive probability the memory is updated to {a mathematical formula}(Y′,AWFunσ(m′),RecFunσ(m′),Actσ(m′)). Therefore there is an edge {a mathematical formula}(s,Y,AWFunσ(m),RecFunσ(m),Actσ(m))→(s′,Y′,AWFunσ(m′),RecFunσ(m′),Actσ(m′)) in the Markov chain {a mathematical formula}G↾σ′.We finish the proof by extending the one-step reachability to general reachability. If {a mathematical formula}(s′,m′) is reachable from {a mathematical formula}(s,m) in {a mathematical formula}G↾σ then there exists a finite path. Applying the one step reachability to every transition in the path gives us the desired result.  □
      </paragraph>
      <paragraph>
       Random variable notation. For all {a mathematical formula}n≥0 we use the following notations for random variables: let {a mathematical formula}Λn denote the random variable for the nth state of the Markov chain {a mathematical formula}G↾σ′. We have: {a mathematical formula}Xn for the projection of {a mathematical formula}Λn on the S component, {a mathematical formula}Yn for the belief support {a mathematical formula}P(S) component of {a mathematical formula}Λn, {a mathematical formula}Wn for the {a mathematical formula}AWFunσ component of {a mathematical formula}Λn, {a mathematical formula}Rn for the {a mathematical formula}RecFunσ component of {a mathematical formula}Λn, {a mathematical formula}An for the {a mathematical formula}Actσ component of {a mathematical formula}Λn, and {a mathematical formula}Ln for the nth action in the Markov chain, respectively.
      </paragraph>
      <paragraph>
       Run of the Markov chain{a mathematical formula}G↾σ′. A run of the Markov chain {a mathematical formula}G↾σ′ is an infinite sequence{a mathematical formula} such that each finite prefix of the run is generated with positive probability on the Markov chain, i.e., for all {a mathematical formula}i≥0, we have (i) {a mathematical formula}Li∈Supp(σn′(Yi,Wi,Ri,Ai)); (ii) {a mathematical formula}Xi+1∈Supp(δ(Xi,Li)); and (iii) {a mathematical formula}(Yi+1,Wi+1,Ri+1,Ai+1)∈Supp(σu′((Yi,Wi,Ri,Ai),γ(Xi+1),Li)). In the following lemma we establish important properties of the Markov chain {a mathematical formula}G↾σ′ that are essential for our proof.
      </paragraph>
      <paragraph label="Lemma 4">
       Let{a mathematical formula}(X0,Y0,W0,R0,A0)→L0(X1,Y1,W1,R1,A1)→L1⋯be a run of the Markov chain{a mathematical formula}G↾σ′, then the following assertions hold for all{a mathematical formula}i≥0:
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}Xi+1∈Supp(δ(Xi,Li))∩Yi+1;
       </list-item>
       <list-item label="2.">
        {a mathematical formula}(Yi,Wi,Ri,Ai)→Li(Yi+1,Wi+1,Ri+1,Ai+1)is an edge in the collapsedgraph{a mathematical formula}CoGr(σ);
       </list-item>
       <list-item label="3.">
        if{a mathematical formula}Wi(Xi)=1, then{a mathematical formula}Wi+1(Xi+1)=1;
       </list-item>
       <list-item label="4.">
        if{a mathematical formula}Ri(Xi)=1, then{a mathematical formula}Ri+1(Xi+1)=1; and
       </list-item>
       <list-item label="5.">
        if{a mathematical formula}Wi(Xi)=1and{a mathematical formula}Ri(Xi)=1, then{a mathematical formula}r(Xi,Li)=1.
       </list-item>
      </list>
      <paragraph label="Proof">
       We prove all the points below:
      </paragraph>
      <list>
       <list-item label="1.">
        The first point follows directly from the definition of the Markov chain and the collapsedstrategy {a mathematical formula}σ′.
       </list-item>
       <list-item label="2.">
        The second point follows from the definition of the collapsedstrategy {a mathematical formula}σ′.
       </list-item>
       <list-item label="3.">
        The third point follows from the first two points of this lemma and the first point of Lemma 2.
       </list-item>
       <list-item label="4.">
        The fourth point follows from the first two points of this lemma and the second point of Lemma 2.
       </list-item>
       <list-item label="5.">
        For the fifth point consider that {a mathematical formula}Wi(Xi)=1 and {a mathematical formula}Ri(Xi)=1. Then there exists a memory {a mathematical formula}m∈M such that (i) {a mathematical formula}AWFunσ(m)=Wi, and (ii) {a mathematical formula}RecFunσ(m)=Ri. Moreover, the state {a mathematical formula}(Xi,m) is recurrent (since {a mathematical formula}Ri(Xi)=1) and almost-sure winning state (since {a mathematical formula}Wi(Xi)=1) in the Markov chain {a mathematical formula}G↾σ. As {a mathematical formula}Li∈Actσ(m) it follows that {a mathematical formula}Li∈Supp(σn(m)), i.e., the action {a mathematical formula}Li is played with positive probability in state {a mathematical formula}Xi given memory m, and {a mathematical formula}(Xi,m) is in an almost-sure winning recurrent class. By Lemma 1 it follows that the reward {a mathematical formula}r(Xi,Li) must be 1.
       </list-item>
      </list>
      <paragraph>
       We now introduce the final notion of a collapsed-recurrent state that is required to complete the proof. A state {a mathematical formula}(X,Y,W,R,A) of the Markov chain {a mathematical formula}G↾σ′ is collapsed-recurrent, if for all memory elements {a mathematical formula}m∈M that were merged to the memory element {a mathematical formula}(Y,W,R,A), the state {a mathematical formula}(X,m) of the Markov chain {a mathematical formula}G↾σ is recurrent. It will turn out that every recurrent state of the Markov chain {a mathematical formula}G↾σ′ is also collapsed-recurrent.
      </paragraph>
      <paragraph label="Definition 3">
       A state {a mathematical formula}(X,Y,W,R,A) of the Markov chain {a mathematical formula}G↾σ′ is called collapsed-recurrent iff {a mathematical formula}R(X)=1.
      </paragraph>
      <paragraph>
       Note that due to point 4 of Lemma 4 all the states reachable from a collapsed-recurrent state are also collapsed-recurrent. In the following lemma we show that the set of collapsed-recurrent states is reached with probability 1.
      </paragraph>
      <paragraph label="Proof">
       With probability 1 a run of the Markov chain{a mathematical formula}G↾σ′reaches a collapsed-recurrent state.We show that from every state {a mathematical formula}(X,Y,W,R,A) in the Markov chain {a mathematical formula}G↾σ′ there exists a reachable collapsed-recurrent state. Consider a memory element {a mathematical formula}m∈M such that (i) {a mathematical formula}AWFunσ(m)=W, (ii) {a mathematical formula}RecFunσ(m)=R, and (iii) {a mathematical formula}Actσ(m)=A. Consider a state {a mathematical formula}(s,m) in the Markov chain {a mathematical formula}G↾σ. By Property 1(b) of Markov chains from every state in a Markov chain the set of recurrent states is reached with probability 1, in particular there exists a reachable recurrent state {a mathematical formula}(X′,m′), such that {a mathematical formula}RecFunσ(m′)(X′)=1. By Lemma 3, there exists {a mathematical formula}Y′⊆S such that the state {a mathematical formula}(X′,Y′,AWFunσ(m),RecFunσ(m′),Actσ(m′)) is reachable from {a mathematical formula}(X,Y,W,R,A) in the Markov chain {a mathematical formula}G↾σ′, and moreover the state is collapsed-recurrent. As this is true for every state, we have that there is a positive probability of reaching a collapsed-recurrent from every state in {a mathematical formula}G↾σ′. This ensures by Property 1(a) of Markov chains that collapsed-recurrent states are reached with probability 1.  □
      </paragraph>
      <paragraph label="Proof">
       The collapsedstrategy{a mathematical formula}σ′is a finite-memory almost-sure winning strategy for the{a mathematical formula}LimAvg=1objective on the POMDP G with the reward function{a mathematical formula}r.The initial state of the Markov chain {a mathematical formula}G↾σ′ is {a mathematical formula}({s0},AWFunσ(m0),RecFunσ(m0),Actσ(m0)) and as the strategy σ is an almost-sure winning strategy we have that {a mathematical formula}AWFunσ(m0)(s0)=1. It follows from the third point of Lemma 4 that every reachable state {a mathematical formula}(X,Y,W,R,A) in the Markov chain {a mathematical formula}G↾σ′ satisfies that {a mathematical formula}W(X)=1.From every state a collapsed-recurrent state is reached with probability 1. It follows that all the recurrent states in the Markov chain {a mathematical formula}G↾σ′ are also collapsed-recurrent states. As in all reachable states {a mathematical formula}(X,Y,W,R,A) we have {a mathematical formula}W(X)=1, by the fifth point of Lemma 4 it follows that every action L played in a collapsed-recurrent state {a mathematical formula}(X,Y,W,R,A) satisfies that the reward {a mathematical formula}r(X,L)=1. As this is true for every reachable recurrent class, the fact that the collapsedstrategy is an almost-sure winning strategy for {a mathematical formula}LimAvg=1 objective follows from Lemma 1.  □
      </paragraph>
      <paragraph label="Proof">
       Strategy complexityThe following assertions hold: (1) If there exists a finite-memory almost-sure winning strategy in the POMDP{a mathematical formula}G=(S,A,δ,O,γ,s0)with reward function{a mathematical formula}rfor the{a mathematical formula}LimAvg=1objective, then there exists a finite-memory almost-sure winning strategy with memory size at most{a mathematical formula}23⋅|S|+|A|. (2) Finite-memory almost-sure winning strategies for{a mathematical formula}LimAvg=1objectives in POMDPs in general require exponential memory and belief-support-based strategies are not sufficient.The first item follows from Lemma 6 and the fact that the size of the memory set of the collapsedstrategy {a mathematical formula}σ′ of any finite-memory strategy σ (which is the size of the vertex set of the collapsedgraph of σ) is bounded by {a mathematical formula}23⋅|S|+|A|. The second item is obtained as follows: (i) the exponential memory requirement follows the almost-sure winning problem for PFA as the shortest witness to the non-emptiness problem for universal automata is exponential [25]; and (ii) the fact that belief-support-based strategies are not sufficient follows from Example 1.  □
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Computational complexity
      </section-title>
      <paragraph>
       We will present an exponential time algorithm for the almost-sure winning problem in POMDPs with {a mathematical formula}LimAvg=1 objectives under finite-memory strategies. A naive double-exponential algorithm would be to enumerate all finite-memory strategies with memory bounded by {a mathematical formula}23⋅|S|+|A| (by Theorem 2). Our improved algorithm consists of two steps: (i) first we construct a special type of a belief-observation POMDP; and we show that there exists a finite-memory almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1 iff there exists a randomized memoryless almost-sure winning strategy in the belief-observation POMDP for the {a mathematical formula}LimAvg=1 objective; and (ii) then we show how to determine whether there exists a randomized memoryless almost-sure winning strategy in the belief-observation POMDP for the {a mathematical formula}LimAvg=1 objective in polynomial time with respect to the size of the belief-observation POMDP. Intuitively, a belief-observation POMDP satisfies that the current belief support is always the set of states with the current observation.
      </paragraph>
      <paragraph label="Definition 4">
       A POMDP {a mathematical formula}G=(S,A,δ,O,γ,s0) is a belief-observation POMDP iff for every finite prefix {a mathematical formula}w=(s0,a0,s1,a1,…,an−1,sn) the belief support associated with the observation sequence {a mathematical formula}ρ=γ(w) is exactly the set of states which has the observation {a mathematical formula}γ(sn), which is the last observation of the sequence ρ. Formally, a POMDP is a belief-observation POMDP iff for every finite prefix w and the observation sequence {a mathematical formula}ρ=γ(w) we have {a mathematical formula}B(ρ)=γ−1(γ(sn)).
      </paragraph>
      <paragraph>
       Overview of the approach. We will split our algorithmic analysis in three parts.
      </paragraph>
      <list>
       <list-item label="1.">
        First in Section 3.3.1 we present the construction that given a POMDP G constructs an exponential size belief-observation POMDP {a mathematical formula}G¯ such that there exists a finite-memory almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1 in G iff there exists a randomized memoryless almost-sure winning strategy in {a mathematical formula}G¯ for the {a mathematical formula}LimAvg=1 objective.
       </list-item>
       <list-item label="2.">
        Second in Section 3.3.2 we show that deciding the existence of a randomized memoryless almost-sure winning strategy in {a mathematical formula}G¯ can be reduced to the computation of almost-sure winning strategies for safety and reachability objectives.
       </list-item>
       <list-item label="3.">
        Finally, in Section 3.3.3 we show that for belief-observation POMDPs the almost-sure winning computation for safety and reachability objectives can be achieved in polynomial time.
       </list-item>
      </list>
      <paragraph>
       Combining the above three steps, we obtain an exponential-time algorithm to determine the existence of finite-memory almost-sure winning strategies for POMDPs with {a mathematical formula}LimAvg=1 objectives. Fig. 3 gives a pictorial overview of the three steps.
      </paragraph>
      <section label="3.3.1">
       <section-title>
        Construction of the belief-observation POMDP
       </section-title>
       <paragraph>
        Given a POMDP G with the {a mathematical formula}LimAvg=1 objective specified by a reward function {a mathematical formula}r we construct a belief-observation POMDP {a mathematical formula}G¯ with the {a mathematical formula}LimAvg=1 objective specified by a reward function {a mathematical formula}r¯, such that there exists a finite-memory almost-sure winning strategy ensuring the objective in the POMDP G iff there exists a randomized memoryless almost-sure winning strategy in the POMDP {a mathematical formula}G¯. We will refer to this construction as {a mathematical formula}G¯=Red(G).
       </paragraph>
       <paragraph>
        Intuition of the construction. Intuitively, the construction will proceed as follows: if there exists an almost-sure winning finite-memory strategy, then there exists an almost-sure winning strategy with memory bounded by {a mathematical formula}23⋅|S|+|A| (by Theorem 2). This allows us to consider memory elements {a mathematical formula}M=2S×{0,1}|S|×{0,1}|S|×2A; and intuitively construct the product of the memory M with the POMDP G. The key intuition that makes the construction work is as follows: by considering memory elements M we essentially consider all possible collapsed strategies of finite-memory strategies. Hence on the product of the POMDP with the memory elements, we essentially search for a witness collapsed strategy of a finite-memory almost-sure winning strategy.
       </paragraph>
       <paragraph>
        Formal construction of{a mathematical formula}G¯=Red(G). We proceed with the formal construction: let {a mathematical formula}G=(S,A,δ,O,γ,s0) be a POMDP with a reward function {a mathematical formula}r. We construct a POMDP {a mathematical formula}G¯=(S¯,A¯,δ¯,O¯,γ¯,s¯0) with a reward function {a mathematical formula}r¯ as follows:
       </paragraph>
       <list>
        <list-item label="•">
         The set of states {a mathematical formula}S¯=S¯a∪S¯m∪{s¯0,s¯l}, consists of the action selection states {a mathematical formula}S¯a=S×M; the memory selection states {a mathematical formula}S¯m=S×2S×M×A; an initial state {a mathematical formula}s¯0 and an absorbing losing state {a mathematical formula}s¯l.
        </list-item>
        <list-item label="•">
         The actions {a mathematical formula}A¯ are {a mathematical formula}A∪M, i.e, the actions {a mathematical formula}A from the POMDP G to simulate action playing and the memory elements M to simulate memory updates.
        </list-item>
        <list-item label="•">
         The observation set is {a mathematical formula}O¯=(M)∪(2S×M×A)∪{s¯0}∪{s¯l}, intuitively the first component of the state cannot be observed, i.e., the memory part of the state remains visible; and the two newly added states do have their own observations.
        </list-item>
        <list-item label="•">
         The observation mapping is then defined {a mathematical formula}γ¯((s,m))=m,γ¯((s,Y,m,a))=(Y,m,a),γ¯(s¯0)=s¯0 and {a mathematical formula}γ¯(s¯l)=s¯l.
        </list-item>
        <list-item label="•">
         As the precise probabilities do not matter for computing almost-sure winning states under finite-memory strategies for the {a mathematical formula}LimAvg=1 objective we specify the transition function only as edges of the POMDP graph and the probabilities are uniform over the support set. First we define the set of winning memories {a mathematical formula}MWin⊆M defined as follows:{a mathematical formula} Note that every reachable memory m of the collapsedstrategy of any finite-memory almost-sure winning strategy σ in the POMDP G must belong to {a mathematical formula}MWin. We define the transition function {a mathematical formula}δ¯ in the following steps:
        </list-item>
       </list>
       <paragraph>
        The reward function {a mathematical formula}r¯ is defined using the reward function {a mathematical formula}r, i.e., {a mathematical formula}r¯((s,(Y,W,R,A)),a)=r(s,a), and {a mathematical formula}r¯((s′,Y′,a,(Y,W,R,A)),a¯)=1 for all {a mathematical formula}a¯∈A¯. The reward for the initial state may be set to an arbitrary value, as the initial state is visited only once. The rewards in the absorbing state {a mathematical formula}s¯l are 0 for all actions.
       </paragraph>
       <paragraph>
        Intuition of the next lemma. In the POMDP {a mathematical formula}G¯ the belief support is already included in the state space of the POMDP itself, and the belief support represents exactly the set of states in which the POMDP is with positive probability. It follows that {a mathematical formula}G¯ is a belief-observation POMDP, which we formally establish in the following lemma.
       </paragraph>
       <paragraph label="Lemma 7">
        The POMDP{a mathematical formula}G¯is a belief-observation POMDP.
       </paragraph>
       <paragraph label="Proof">
        Note that the observations are defined in a way that the first component cannot be observed. Given a sequence {a mathematical formula}w¯ of states and actions in {a mathematical formula}G¯ with the observation sequence {a mathematical formula}ρ¯=γ¯(w¯) we will show that the possible first components of the states in the belief support {a mathematical formula}B(ρ¯) are equal to the updated belief-support components {a mathematical formula}Y′ in the observation. Intuitively the proof holds as the Y component is the belief support and the belief support already represents exactly the set of states in which the POMDP can be with positive probability. We now present the formal argument. Let us denote by {a mathematical formula}Proj1(B(ρ¯))⊆S the projection on the first component of the states in the belief support. One inclusion is trivial since for every reachable state {a mathematical formula}(s,(Y,W,R,A)) we have {a mathematical formula}s∈Y (resp. for states {a mathematical formula}(s′,Y′,(Y,W,R,A),a) we have that {a mathematical formula}s′∈Y′). Therefore we have {a mathematical formula}Proj1(B(ρ¯))⊆Y (resp. {a mathematical formula}Y′).We prove the second inclusion by induction with respect to the length of the play prefix:
       </paragraph>
       <list>
        <list-item label="•">
         Base case: We show the base case for prefixes of length 1 and 2. The first observation is always {a mathematical formula}{s0¯} which contains only a single state, so there is nothing to prove. Similarly the second observation in the game is of the form {a mathematical formula}({s0},W,R,A), and the argument is the same.
        </list-item>
        <list-item label="•">
         Induction step: Let us a consider a prefix {a mathematical formula}w¯′=w¯⋅a⋅(s′,Y′,(Y,W,R,A),a) where {a mathematical formula}a∈A and the last transition is {a mathematical formula}(s,(Y,W,R,A))→a(s′,Y′,(Y,W,R,A),a) in the POMDP {a mathematical formula}G¯.By induction hypothesis we have that {a mathematical formula}B(γ¯(w¯))={(s,(Y,W,R,A))|s∈Y}. The new belief support is computed (by definition) as{a mathematical formula}Let {a mathematical formula}sY′ be a state in {a mathematical formula}Y′, we want to show that {a mathematical formula}(sY′,Y′,(Y,W,R,A),a) is in {a mathematical formula}B(γ¯(w¯′)). Due to the definition of the belief-support update there exists a state {a mathematical formula}sY in Y such that {a mathematical formula}sY→asY′ and {a mathematical formula}(sY,(Y,W,R,A))∈B(γ¯(w¯)). As {a mathematical formula}γ(sY′)=γ(s′), it follows that {a mathematical formula}(sY′,Y′,(Y,W,R,A),a)∈⋃s¯∈B(γ¯(w¯))Supp(δ¯(s¯,a)) and as {a mathematical formula}(sY′,Y′,(Y,W,R,A),a)∈γ¯−1((Y′,(Y,W,R,A),a)), the result follows.The case when the prefix is extended with a memory action {a mathematical formula}m∈M is simpler as the first two components do not change during the transition.
        </list-item>
       </list>
       <paragraph>
        We now show that the existence of a finite-memory almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1 in POMDP G implies the existence of a randomized memoryless almost-sure winning strategy in the POMDP {a mathematical formula}G¯ and vice versa.
       </paragraph>
       <paragraph>
        Intuition of the following two lemmas. We have established in Theorem 2 that if there exists a finite-memory almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1 in a POMDP G, then there also exists a collapsedalmost-sure winning strategy with memory elements that are by construction already present in the state space of the belief-observation POMDP {a mathematical formula}G¯. In every state of the POMDP {a mathematical formula}G¯ the part of the state space which corresponds to the memory elements of the collapsedstrategy is perfectly observable. The transition function of the POMDP {a mathematical formula}G¯ respects the possible memory updates of the collapsedstrategy. Intuitively, it follows that the collapsedstrategy in the POMDP G can be adapted to a memoryless strategy in the belief-observation POMDP {a mathematical formula}G¯ by considering the observable parts of the state space instead of its memory, and vice versa.
       </paragraph>
       <paragraph label="Lemma 8">
        If there exists a finite-memory almost-sure winning strategy for the{a mathematical formula}LimAvg=1objective with the reward function{a mathematical formula}rin the POMDP G, then there exists a memoryless almost-sure winning strategy for the{a mathematical formula}LimAvg=1objective with the reward function{a mathematical formula}r¯in the POMDP{a mathematical formula}G¯.
       </paragraph>
       <paragraph label="Proof">
        Assume there is a finite-memory almost-sure winning strategy σ in the POMDP G, then the collapsedstrategy {a mathematical formula}σ′=(σu′,σn′,M,({s0},W,R,A))=CoSt(σ) is also an almost-sure winning strategy in the POMDP G. We define the memoryless strategy {a mathematical formula}σ¯:O¯→D(A¯) as follows:
       </paragraph>
       <list>
        <list-item label="•">
         In the initial observation {a mathematical formula}{s¯0} play the action {a mathematical formula}({s0},W,R,A).
        </list-item>
        <list-item label="•">
         In observation {a mathematical formula}(Y,W,R,A) play {a mathematical formula}σ¯((Y,W,R,A))=σn′((Y,W,R,A)).
        </list-item>
        <list-item label="•">
         In observation {a mathematical formula}(Y′,a,(Y,W,R,A)) we denote by o the unique observation such that {a mathematical formula}Y′⊆γ−1(o), then {a mathematical formula}σ¯(Y′,a,(Y,W,R,A))=σu′((Y,W,R,A),o,a).
        </list-item>
        <list-item label="•">
         In the observation {a mathematical formula}{s¯l} no matter what is played the losing absorbing state is not left.
        </list-item>
       </list>
       <paragraph label="Lemma 9">
        If there exists a memoryless almost-sure winning strategy for the{a mathematical formula}LimAvg=1objective with the reward function{a mathematical formula}r¯in the POMDP{a mathematical formula}G¯, then there exists a finite-memory almost-sure winning strategy for the{a mathematical formula}LimAvg=1objective withe the reward function{a mathematical formula}rin the POMDP G.
       </paragraph>
       <paragraph label="Proof">
        Let {a mathematical formula}σ¯ be a memoryless almost-sure winning strategy. Intuitively we use everything from the state except the first component as the memory in the constructed finite-memory strategy {a mathematical formula}σ=(σu,σn,M,m0), i.e.,
       </paragraph>
       <list>
        <list-item label="•">
         {a mathematical formula}σn((Y,W,R,A))=σ¯((Y,W,R,A)).
        </list-item>
        <list-item label="•">
         {a mathematical formula}σu((Y,W,R,A),o,a) we update uniformly to the elements from the set {a mathematical formula}Supp(σ¯((Y′,a,(Y,W,R,A)))), where {a mathematical formula}Y′ is the belief-support update from Y under observation o and action a.
        </list-item>
        <list-item label="•">
         {a mathematical formula}m0=σ¯({s0′}), this can be in general a probability distribution, in our setting the initial memory is deterministic. However, this can modeled by adding an additional initial state from which the required memory update is going to be modeled.
        </list-item>
       </list>
      </section>
      <section label="3.3.2">
       <section-title>
        Analysis of belief-observation POMDPs
       </section-title>
       <paragraph>
        We will present a polynomial-time algorithm to determine the set of states from which there exists a memoryless almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1 in the belief-observation POMDP {a mathematical formula}G¯=(S¯,A¯,δ¯,O¯,γ¯,s¯0). In this section we show how the computation is reduced to the analysis of safety and reachability objectives.
       </paragraph>
       <paragraph>
        For simplicity in presentation we enhance the belief-observation POMDP {a mathematical formula}G¯ with an additional function {a mathematical formula}Γ¯:O¯→P(A¯)∖∅ denoting the set of actions that are available for an observation, i.e., in this part we will consider POMDPs to be a tuple {a mathematical formula}Red(G)=G¯=(S¯,A¯,δ¯,O¯,γ¯,Γ¯,s¯0). Note that this does not increase the expressive power of the model, as an action not available in an observation may be simulated by an edge leading to a losing absorbing state.
       </paragraph>
       <paragraph>
        Almost-sure winning observations. Given a POMDP {a mathematical formula}G¯=(S¯,A¯,δ¯,O¯,γ¯,Γ¯,s¯0) and an objective ψ, let {a mathematical formula}AlmostM(ψ) denote the set of observations {a mathematical formula}o¯∈O¯, such that there exists a memoryless almost-sure winning strategy ensuring the objective from every state {a mathematical formula}s¯∈γ¯−1(o¯), i.e., {a mathematical formula}AlmostM(ψ)={o¯∈O¯|there exists a memoryless strategyσ¯,such that for alls¯∈γ¯−1(o¯)we havePs¯σ¯(ψ)=1}. Our goal is to compute the set {a mathematical formula}AlmostM(LimAvg=1) given the belief-observation POMDP {a mathematical formula}G¯=Red(G). Our algorithm will reduce the computation to safety and reachability objectives which are defined as follows:
       </paragraph>
       <list>
        <list-item label="•">
         Reachability and safety objectives. Given a set {a mathematical formula}T¯⊆S¯ of target states, the reachability objective {a mathematical formula}Reach(T¯)={(s¯0,a¯0,s¯1,a¯1,s¯2…)∈Ω|∃k≥0:s¯k∈T¯} requires that a target state in {a mathematical formula}T¯ is visited at least once. Dually, the safety objective {a mathematical formula}Safe(F¯)={(s¯0,a¯0,s¯1,a¯1,s¯2…)∈Ω|∀k≥0:s¯k∈F¯} requires that only states in {a mathematical formula}F¯ are visited.
        </list-item>
       </list>
       <paragraph>
        In the first step of the computation of the winning observations, we will restrict the set of available actions in the belief-observation POMDP {a mathematical formula}G¯. Observe, that any almost-sure winning strategy must avoid reaching the losing absorbing state {a mathematical formula}s¯l. Therefore, we restrict the actions in the POMDP to only so called allowable or safe actions.
       </paragraph>
       <list>
        <list-item label="•">
         (Allow). Given a set of observations {a mathematical formula}O¯⊆O¯ and an observation {a mathematical formula}o¯∈O¯ we define by {a mathematical formula}Allow(o¯,O¯) the set of actions that when played in observation {a mathematical formula}o¯ (in any state {a mathematical formula}γ¯−1(o¯)) ensure that the next observation is in {a mathematical formula}O¯, i.e.,{a mathematical formula}
        </list-item>
       </list>
       <paragraph>
        We will denote by {a mathematical formula}S¯good=S¯∖s¯l the set of states of the POMDP without the losing absorbing state. We compute the set of observations {a mathematical formula}AlmostM(Safe(S¯good)) from which there exists a memoryless strategy ensuring almost-surely that the losing absorbing state {a mathematical formula}s¯l is not visited.
       </paragraph>
       <paragraph>
        Formal construction of{a mathematical formula}G˜. We construct a restricted POMDP {a mathematical formula}G˜=(S˜,A˜,δ˜,O˜,γ˜,Γ˜,s˜0), where {a mathematical formula}A˜=A¯, {a mathematical formula}s˜0=s¯0 and the rest is defined as follows:
       </paragraph>
       <list>
        <list-item label="•">
         the set of states is restricted to {a mathematical formula}S˜=γ¯−1(AlmostM(Safe(S¯good))),
        </list-item>
        <list-item label="•">
         the set of safe actions available in observation {a mathematical formula}o˜ are{a mathematical formula}
        </list-item>
        <list-item label="•">
         the transition function {a mathematical formula}δ˜ is defined as {a mathematical formula}δ¯ but restricted to states {a mathematical formula}S˜, and
        </list-item>
        <list-item label="•">
         the observation {a mathematical formula}O˜=AlmostM(Safe(S¯good)) and the observation mapping {a mathematical formula}γ˜ is defined as {a mathematical formula}γ¯ restricted to states {a mathematical formula}S˜.
        </list-item>
       </list>
       <paragraph>
        Note that any memoryless almost-sure winning strategy {a mathematical formula}σ¯ for the objective {a mathematical formula}LimAvg=1 on the POMDP {a mathematical formula}G¯ can be interpreted on the restricted POMDP {a mathematical formula}G˜ as playing an action from the set {a mathematical formula}Γ¯(o)∖Γ˜(o) or reaching a state in {a mathematical formula}S¯∖S˜ leads to a contradiction to the fact that {a mathematical formula}σ¯ is an almost-sure winning strategy (the losing absorbing state is reached with positive probability). Similarly due to the fact that the POMDP {a mathematical formula}G¯ is a belief-observation POMDP it follows that the restricted POMDP {a mathematical formula}G˜ is also a belief-observation POMDP.
       </paragraph>
       <paragraph>
        We define a subset of states of the belief-observation POMDP {a mathematical formula}G˜ that intuitively correspond to winning collapsed-recurrent states (wcs), i.e., {a mathematical formula}S˜wcs={(s,(Y,W,R,A))|W(s)=1,R(s)=1}. Finally we compute the set of observations {a mathematical formula}AW˜=AlmostM(Reach(Swcs)) in the restricted belief-observation POMDP {a mathematical formula}G˜. We show that the set of observations {a mathematical formula}AW˜ is equal to the set of observations {a mathematical formula}AlmostM(LimAvg=1) in the POMDP {a mathematical formula}G¯. In the following two lemmas we establish the required inclusions:
       </paragraph>
       <paragraph label="Proof">
        {a mathematical formula}AW˜⊆AlmostM(LimAvg=1).Let {a mathematical formula}σ˜ be a memoryless almost-sure winning strategy for the objective {a mathematical formula}Reach(S˜wcs) from every state that has observation {a mathematical formula}o˜ in the POMDP {a mathematical formula}G˜, for all {a mathematical formula}o˜ in {a mathematical formula}AW˜. We will show that the strategy {a mathematical formula}σ˜ also almost-surely ensures the {a mathematical formula}LimAvg=1 objective in POMDP {a mathematical formula}G¯.Consider the Markov chain {a mathematical formula}G¯↾σ˜. As the strategy {a mathematical formula}σ˜ plays in the POMDP {a mathematical formula}G˜ that is restricted only to actions that keep the game in observations {a mathematical formula}AlmostM(Safe(S¯good)) it follows that the losing absorbing state {a mathematical formula}s¯l is not reachable in the Markov chain {a mathematical formula}G¯↾σ˜. Also the strategy ensures that the set of states {a mathematical formula}S˜wcs is reached with probability 1 in the Markov chain {a mathematical formula}G¯↾σ˜.Let {a mathematical formula}(s,(Y,W,R,A))→a(s′,Y′,a,(Y,W,R,A)) be an edge in the Markov chain {a mathematical formula}G¯↾σ˜ and assume that {a mathematical formula}(s,(Y,W,R,A))∈S˜wcs, i.e., {a mathematical formula}W(s)=1 and {a mathematical formula}R(s)=1. The only actions {a mathematical formula}(Y′,W′,R′,A′) available in the state {a mathematical formula}(s′,Y′,a,(Y,W,R,A)) are enabled actions that satisfy:
        <list>
         for all {a mathematical formula}sˆ∈Y, if {a mathematical formula}W(sˆ)=1, then for all {a mathematical formula}sˆ′∈Supp(δ(sˆ,a))∩Y′ we have {a mathematical formula}W′(sˆ′)=1, andfor all {a mathematical formula}sˆ∈Y, if {a mathematical formula}R(sˆ)=1, then for all {a mathematical formula}sˆ′∈Supp(δ(sˆ,a))∩Y′ we have {a mathematical formula}R′(sˆ′)=1.It follows that all the states reachable in one step from
        </list>
        <paragraph>
         {a mathematical formula}(s′,Y′,a,(Y,W,R,A)) are also in {a mathematical formula}S˜wcs. Similarly, for every enabled action in state {a mathematical formula}(s,(Y,W,R,A)) we have that for all {a mathematical formula}s∈Y, if {a mathematical formula}W(s)=1 and {a mathematical formula}R(s)=1, then {a mathematical formula}r(s,a)=1. Therefore, all the rewards {a mathematical formula}r¯ from states in {a mathematical formula}S˜wcs are 1, and all the intermediate states {a mathematical formula}(s′,Y′,a,(Y,W,R,A)) have reward 1 by definition. This all together ensures that after reaching the set {a mathematical formula}S˜wcs only rewards 1 are received, and as the set {a mathematical formula}S˜wcs is reached with probability 1, it follows by Lemma 1 that {a mathematical formula}σ˜ is an almost-sure winning strategy in the POMDP {a mathematical formula}G¯ for the {a mathematical formula}LimAvg=1 objective.  □
        </paragraph>
       </paragraph>
       <paragraph label="Proof">
        {a mathematical formula}AlmostM(LimAvg=1)⊆AW˜.Assume for contradiction that there exists an observation {a mathematical formula}o˜∈AlmostM(LimAvg=1)∖AW˜. As we argued before, the observation {a mathematical formula}o˜ must belong to the set {a mathematical formula}AlmostM(Safe(S¯good)), otherwise with positive probability the losing absorbing state {a mathematical formula}s¯l is reached.As {a mathematical formula}o˜∈AlmostM(LimAvg=1) there exists a memoryless almost-sure winning strategy {a mathematical formula}σ¯ in the POMDP {a mathematical formula}G¯ for the objective {a mathematical formula}LimAvg=1. By Lemma 9 there exists a finite-memory almost-sure winning strategy σ in POMDP G for the {a mathematical formula}LimAvg=1 objective and by Theorem 2 there exists a finite-memory almost-sure winning collapsedstrategy {a mathematical formula}σ′=CoSt(σ) in POMDP G.Let {a mathematical formula}(s,(Y,W,R,A)) be a reachable collapsed-recurrent state in the Markov chain {a mathematical formula}G↾σ. Then by the definition of collapsed-recurrent states we have that {a mathematical formula}R(s)=1, and as the strategy {a mathematical formula}σ′ is almost-sure winning we also have that {a mathematical formula}W(s)=1, i.e., if we consider the state {a mathematical formula}(s,(Y,W,R,A)) of the POMDP {a mathematical formula}G¯ we obtain that the state belongs to the set {a mathematical formula}S˜wcs. Note that by Lemma 5 the set of collapsed-recurrent states in the Markov chain {a mathematical formula}G↾σ′ is reached with probability 1. By the construction presented in Lemma 8 we obtain a memoryless almost-sure winning strategy {a mathematical formula}σ¯ for the objective {a mathematical formula}LimAvg=1 in the POMDP {a mathematical formula}G¯. Moreover, we have that the Markov chains {a mathematical formula}G↾σ′ and {a mathematical formula}G¯↾σ¯ are isomorphic when simplified edges are considered. In particular it follows that the set of states {a mathematical formula}S˜wcs is reached with probability 1 in the Markov chain {a mathematical formula}G¯↾σ¯ and therefore {a mathematical formula}σ¯ is a witness strategy for the fact that the observation {a mathematical formula}o˜ belongs to the set {a mathematical formula}AW˜. The contradiction follows.  □
       </paragraph>
       <paragraph>
        The algorithm. From the results of this section we obtain our algorithm (described as Algorithm 1) to determine the existence of finite-memory almost-sure winning strategies for {a mathematical formula}LimAvg=1 objectives in POMDPs. The algorithm requires two procedures, namely AlmostSafe (Algorithm 2) and AlmostReach (Algorithm 3), for the computation of almost-sure winning for reachability and safety objectives, which we present in the following Section 3.3.3.
       </paragraph>
      </section>
      <section label="3.3.3">
       <section-title>
        Polynomial-time algorithm for safety and reachability objectives for belief-observation POMDPs
       </section-title>
       <paragraph>
        To complete the computation for almost-sure winning for {a mathematical formula}LimAvg=1 objectives we now present polynomial time algorithms for almost-sure safety and almost-sure reachability objectives for randomized memoryless strategies in the belief-observation POMDP {a mathematical formula}G¯.
       </paragraph>
       <paragraph>
        Notations for algorithms. We start with a few notations below:
       </paragraph>
       <list>
        <list-item label="•">
         (Pre). The predecessor function given a set of observations {a mathematical formula}O¯ selects the observations {a mathematical formula}o¯∈O¯ such that {a mathematical formula}Allow(o¯,O¯) is non-empty, i.e.,{a mathematical formula}
        </list-item>
        <list-item label="•">
         (Apre). Given a set {a mathematical formula}Y¯⊆O¯ of observations and a set {a mathematical formula}X¯⊆S¯ of states such that {a mathematical formula}X¯⊆γ¯−1(Y¯), the set {a mathematical formula}Apre(Y¯,X¯) denotes the states from {a mathematical formula}γ¯−1(Y¯) such that there exists an action that ensures that the next observation is in {a mathematical formula}Y¯ and the set {a mathematical formula}X¯ is reached with positive probability, i.e.:{a mathematical formula}
        </list-item>
        <list-item label="•">
         (ObsCover). For a set {a mathematical formula}U¯⊆S¯ of states we define the {a mathematical formula}ObsCover(U¯)⊆O¯ to be the set of observations {a mathematical formula}o¯ such that all states with observation {a mathematical formula}o¯ are in {a mathematical formula}U¯, i.e., {a mathematical formula}ObsCover(U¯)={o¯∈O¯|γ¯−1(o¯)⊆U¯}.
        </list-item>
       </list>
       <paragraph>
        Using the above notations we present the solution of almost-sure winning for safety and reachability objectives.
       </paragraph>
       <paragraph>
        Almost-sure winning for safety objectives. Given a safety objective {a mathematical formula}Safe(F¯), for a set {a mathematical formula}F¯⊆S¯ of states, let {a mathematical formula}OF¯=ObsCover(F¯) denote the set of observations {a mathematical formula}o¯ such that {a mathematical formula}γ¯−1(o¯)⊆F¯, i.e., all states {a mathematical formula}s¯∈γ¯−1(o¯) belong to {a mathematical formula}F¯. We denote by νX the greatest fixpoint and by μX the least fixpoint. Let{a mathematical formula} be the greatest fixpoint of the function {a mathematical formula}f(Y)=OF¯∩Pre(Y). Then the set {a mathematical formula}Y⁎ is obtained by the following computation:
       </paragraph>
       <list>
        <list-item label="1.">
         {a mathematical formula}Y0←OF¯; and
        </list-item>
        <list-item label="2.">
         repeat {a mathematical formula}Yi+1←Pre(Yi) until a fixpoint is reached.
        </list-item>
       </list>
       <paragraph>
        We show that {a mathematical formula}Y⁎=AlmostM(Safe(F¯)).
       </paragraph>
       <paragraph label="Proof">
        For every observation{a mathematical formula}o¯∈Y⁎we have{a mathematical formula}Allow(o¯,Y⁎)≠∅(i.e.,{a mathematical formula}Allow(o¯,Y⁎)is non-empty).Assume for contradiction that there exists an observation {a mathematical formula}o¯∈Y⁎ such that {a mathematical formula}Allow(o¯,Y⁎) is empty. Then {a mathematical formula}o¯∉Pre(Y⁎) and hence the observation must be removed in the next iteration of the algorithm. This implies {a mathematical formula}Pre(Y⁎)≠Y⁎, we reach a contradiction that {a mathematical formula}Y⁎ is a fixpoint.  □
       </paragraph>
       <paragraph label="Lemma 13">
        The set{a mathematical formula}Y⁎is the set of almost-sure winning observations for the safety objective{a mathematical formula}Safe(F¯), i.e.,{a mathematical formula}Y⁎=AlmostM(Safe(F¯)), and can be computed in linear time.
       </paragraph>
       <paragraph label="Proof">
        We prove the two desired inclusions: (1) {a mathematical formula}Y⁎⊆AlmostM(Safe(F¯)); and (2) {a mathematical formula}AlmostM(Safe(F¯))⊆Y⁎.
       </paragraph>
       <list>
        <list-item label="1.">
         (First inclusion). By the definition of {a mathematical formula}Y0 we have that {a mathematical formula}γ¯−1(Y0)⊆F¯. As {a mathematical formula}Yi+1⊆Yi we have that {a mathematical formula}γ¯−1(Y⁎)⊆F¯. By Lemma 12, for all observations {a mathematical formula}o¯∈Y⁎ we have {a mathematical formula}Allow(o¯,Y⁎) is non-empty. A pure memoryless strategy that plays some action from {a mathematical formula}Allow(o¯,Y⁎) in {a mathematical formula}o¯, for {a mathematical formula}o¯∈Y⁎, ensures that the next observation is in {a mathematical formula}Y⁎. Thus the strategy ensures that only states from {a mathematical formula}γ¯−1(Y⁎)⊆F¯ are visited, and therefore is an almost-sure winning strategy for the safety objective.
        </list-item>
        <list-item label="2.">
         (Second inclusion). We prove that there is no almost-sure winning strategy from {a mathematical formula}O¯∖Y⁎ by induction:
        </list-item>
       </list>
       <paragraph>
        Formal description of the almost-sure safety algorithm. The above lemma establishes the correctness of the algorithm that computes the almost-sure winning for safety objectives, and the formal description of the algorithm is described as Algorithm 2.
       </paragraph>
       <paragraph>
        Almost-sure winning for reachability objectives. Consider a set {a mathematical formula}T¯⊆S¯ of target states, and the reachability objective {a mathematical formula}Reach(T¯). We will show that:{a mathematical formula} Let {a mathematical formula}Z⁎=νZ.ObsCover(μX.((T¯∩γ−1(Z))∪Apre(Z,X))). In the following two lemmas we show the two desired inclusions, i.e., {a mathematical formula}AlmostM(Reach(T¯))⊆Z⁎ and then we show that {a mathematical formula}Z⁎⊆AlmostM(Reach(T¯)).
       </paragraph>
       <paragraph label="Proof">
        {a mathematical formula}AlmostM(Reach(T¯))⊆Z⁎.Let {a mathematical formula}W⁎=AlmostM(Reach(T¯)). We first show that {a mathematical formula}W⁎ is a fixpoint of the function{a mathematical formula} i.e., we will show that {a mathematical formula}W⁎=ObsCover(μX.((T¯∩γ−1(W⁎))∪Apre(W⁎,X))). As {a mathematical formula}Z⁎ is the greatest fixpoint it will follow that {a mathematical formula}W⁎⊆Z⁎. Let{a mathematical formula} and {a mathematical formula}Xˆ⁎=ObsCover(X⁎). Note that by definition we have {a mathematical formula}X⁎⊆γ¯−1(W⁎) as the inner fixpoint computation only computes states that belong to {a mathematical formula}γ¯−1(W⁎). Assume for contradiction that {a mathematical formula}W⁎ is not a fixpoint, i.e., {a mathematical formula}Xˆ⁎ is a strict subset of {a mathematical formula}W⁎. For all states {a mathematical formula}s¯∈γ¯−1(W⁎)∖X⁎, for all actions {a mathematical formula}a¯∈Allow(γ¯(s¯),W⁎) we have {a mathematical formula}Supp(δ¯(s¯,a¯))⊆(γ¯−1(W⁎)∖X⁎). Consider any randomized memoryless almost-sure winning strategy {a mathematical formula}σ⁎ from {a mathematical formula}W⁎ and we consider two cases:
       </paragraph>
       <list>
        <list-item label="1.">
         Suppose there is a state {a mathematical formula}s¯∈γ¯−1(W⁎)∖X⁎ such that an action that does not belong to {a mathematical formula}Allow(γ¯(s¯),W⁎) is played with positive probability by {a mathematical formula}σ⁎. Then with positive probability the observations from {a mathematical formula}W⁎ are left (because from some state with same observation as {a mathematical formula}s¯ an observation in the complement of {a mathematical formula}W⁎ is reached with positive probability). Since from the complement of {a mathematical formula}W⁎ there is no randomized memoryless almost-sure winning strategy (by definition), it contradicts the claim that {a mathematical formula}σ⁎ is an almost-sure winning strategy from {a mathematical formula}W⁎.
        </list-item>
        <list-item label="2.">
         Otherwise for all states {a mathematical formula}s¯∈γ¯−1(W⁎)∖X⁎ the strategy {a mathematical formula}σ⁎ plays only actions in {a mathematical formula}Allow(γ¯(s¯),W⁎), and then the probability to reach {a mathematical formula}X⁎ is zero, i.e., {a mathematical formula}Safe(γ¯−1(W⁎)∖X⁎) is ensured. Since all target states in {a mathematical formula}γ¯−1(W⁎) belong to {a mathematical formula}X⁎ (they get included in iteration 0 of the fixpoint computation) it follows that {a mathematical formula}(γ¯−1(W⁎)∖X⁎)∩T¯=∅, and hence {a mathematical formula}Safe(γ¯−1(W⁎)∖X⁎)∩Reach(T¯)=∅, and we again reach a contradiction that {a mathematical formula}σ⁎ is an almost-sure winning strategy.
        </list-item>
       </list>
       <paragraph label="Proof">
        {a mathematical formula}Z⁎⊆AlmostM(Reach(T¯)).Since the goal is to reach the set {a mathematical formula}T¯, without loss of generality we assume the set {a mathematical formula}T¯ to be absorbing. We define a randomized memoryless strategy {a mathematical formula}σ⁎ for the objective {a mathematical formula}AlmostM(Reach(T¯)) as follows: for an observation {a mathematical formula}o¯∈Z⁎, play all actions from the set {a mathematical formula}Allow(o¯,Z⁎) uniformly at random. Since the strategy {a mathematical formula}σ⁎ plays only actions in {a mathematical formula}Allow(o¯,Z⁎), for {a mathematical formula}o¯∈Z⁎, it ensures that the set of states {a mathematical formula}γ¯−1(Z⁎) is not left (i.e., {a mathematical formula}Safe(γ¯−1(Z⁎)) is ensured). We now analyze the computation of the inner fixpoint, i.e., analyze the computation of {a mathematical formula}μX.((T¯∩γ¯−1(Z⁎))∪Apre(Z⁎,X)) as follows:
       </paragraph>
       <list>
        <list-item label="•">
         {a mathematical formula}X0=(T¯∩γ¯−1(Z⁎))∪Apre(Z⁎,∅)=T¯∩γ¯−1(Z⁎)⊆T¯ (since {a mathematical formula}Apre(Z⁎,∅) is ∅);
        </list-item>
        <list-item label="•">
         {a mathematical formula}Xi+1=(T¯∩γ¯−1(Z⁎))∪Apre(Z⁎,Xi)
        </list-item>
       </list>
       <paragraph label="Proof">
        The set{a mathematical formula}AlmostM(Reach(T¯))can be computed in quadratic time for belief-observation POMDPs, for target set{a mathematical formula}T¯⊆S¯.Follows directly from Lemma 14 and Lemma 15.  □
       </paragraph>
       <paragraph>
        Formal description of the almost-sure reachability algorithm. The previous three lemmas establish the correctness of the algorithm that computes the almost-sure winning for reachability objectives, and the formal description of the algorithm is described as Algorithm 3.
       </paragraph>
       <paragraph>
        The EXPTIME-completeness. In this section we first showed that given a POMDP G with a {a mathematical formula}LimAvg=1 objective we can construct an exponential size belief-observation POMDP {a mathematical formula}G¯ and the computation of the almost-sure winning set for {a mathematical formula}LimAvg=1 objectives is reduced to the computation of the almost-sure winning set for safety and reachability objectives, for which we established linear and quadratic time algorithms respectively. This gives us a {a mathematical formula}2O(|S|+|A|) time algorithm to decide (and construct if one exists) the existence of finite-memory almost-sure winning strategies in POMDPs with {a mathematical formula}LimAvg=1 objectives. The EXPTIME-hardness for almost-sure winning easily follows from the result of Reif for two-player partial-observation games with safety objectives [40]: (i) First observe that in POMDPs, if almost-sure safety is violated, then it is violated with a finite prefix which has positive probability, and hence for almost-sure safety, the probabilistic player can be treated as an adversary. This shows that the almost-sure safety problem for POMDPs is EXPTIME-hard. (ii) The almost-sure safety problem reduces to almost-sure winning for limit-average objectives by assigning reward 1 to safe states, reward 0 to non-safe states and make the non-safe states absorbing. It follows that POMDPs with almost-sure winning for {a mathematical formula}LimAvg=1 objectives under finite-memory strategies is EXPTIME-hard.
       </paragraph>
       <paragraph label="Theorem 3">
        The following assertions hold:
       </paragraph>
       <list>
        <list-item label="1.">
         Given a POMDP G with{a mathematical formula}|S|states,{a mathematical formula}|A|actions, and a{a mathematical formula}LimAvg=1objective, the existence (and the construction if one exists) of a finite-memory almost-sure winning strategy can be achieved in{a mathematical formula}2O(|S|+|A|)time.
        </list-item>
        <list-item label="2.">
         The decision problem of given a POMDP and a{a mathematical formula}LimAvg=1objective whether there exists a finite-memory almost-sure winning strategy is EXPTIME-complete.
        </list-item>
       </list>
       <paragraph label="Remark 4">
        We highlight one important aspect of our results. Several algorithms for POMDPs are based on the belief states, which are probability distributions over the states of the POMDPs, and thus the algorithms require numerical data structures. In contrast, our algorithm is based only on the support of the belief states (i.e., as a set of states rather than a probability distribution). Our algorithm, though exponential time in the worst case, is a discrete algorithm, and we present an implementation in Section 6.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="4">
     <section-title>
      Finite-memory strategies with quantitative constraints
     </section-title>
     <paragraph>
      We will show that the problem of deciding whether there exists a finite-memory (as well as an infinite-memory) almost-sure winning strategy for the objective {a mathematical formula}LimAvg&gt;12 is undecidable. We present a reduction from the standard undecidable problem for probabilistic finite automata (PFA). A PFA {a mathematical formula}P=(S,A,δ,F,s0) is a special case of a POMDP {a mathematical formula}G=(S,A,δ,O,γ,s0) with a single observation {a mathematical formula}O={o} such that for all states {a mathematical formula}s∈S we have {a mathematical formula}γ(s)=o. Moreover, the PFA proceeds for only finitely many steps, and has a set F of desired final states. The strict emptiness problem asks for the existence of a strategy w (a finite word over the alphabet {a mathematical formula}A) such that the measure of the runs ending in the desired final states F is strictly greater than {a mathematical formula}12; and the strict emptiness problem for PFA is undecidable [36].
     </paragraph>
     <paragraph>
      Reduction. Given a PFA {a mathematical formula}P=(S,A,δ,F,s0) we construct a POMDP {a mathematical formula}G=(S′,A′,δ′,O,γ,s0′) with a Boolean reward function {a mathematical formula}r such that there exists a word {a mathematical formula}w∈A⁎ accepted with probability strictly greater than {a mathematical formula}12 in {a mathematical formula}P iff there exists a finite-memory almost-sure winning strategy in G for the objective {a mathematical formula}LimAvg&gt;12. Intuitively, the construction of the POMDP G is as follows: for every state {a mathematical formula}s∈S of {a mathematical formula}P we construct a pair of states {a mathematical formula}(s,1) and {a mathematical formula}(s,0) in {a mathematical formula}S′ with the property that {a mathematical formula}(s,0) can only be reached with a new action $ (not in {a mathematical formula}A) played in state {a mathematical formula}(s,1). The transition function {a mathematical formula}δ′ from the state {a mathematical formula}(s,0) mimics the transition function δ, i.e., {a mathematical formula}δ′((s,0),a)((s′,1))=δ(s,a)(s′). The reward {a mathematical formula}r of {a mathematical formula}(s,1) (resp. {a mathematical formula}(s,0)) is 1 (resp. 0), ensuring the average of the pair to be {a mathematical formula}12. We add a new available action # that when played in a final state reaches a state {a mathematical formula}good∈S′ with reward 1, and when played in a non-final state reaches a state {a mathematical formula}bad∈S′ with reward 0, and for states {a mathematical formula}good and {a mathematical formula}bad given action # the next state is the initial state. An illustration of the construction on an example is depicted on Fig. 4. Whenever an action is played in a state where it is not available, the POMDP reaches a losing absorbing state, i.e., an absorbing state with reward 0, and for brevity we omit transitions to the losing absorbing state. The formal construction of the POMDP G is as follows:
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}S′=(S×{0,1})∪{good,bad},
      </list-item>
      <list-item label="•">
       {a mathematical formula}s0′=(s0,1),
      </list-item>
      <list-item label="•">
       {a mathematical formula}A′=A∪{#,$},
      </list-item>
      <list-item label="•">
       The actions {a mathematical formula}a∈A∪{#} in states {a mathematical formula}(s,1) (for {a mathematical formula}s∈S) lead to the losing absorbing state; the action $ in states {a mathematical formula}(s,0) (for {a mathematical formula}s∈S) leads to the losing absorbing states; and the actions {a mathematical formula}a∈A∪{$} in states {a mathematical formula}good and {a mathematical formula}bad lead to the losing absorbing state. The other transitions are as follows: For all {a mathematical formula}s∈S: (i) {a mathematical formula}δ′((s,1),$)((s,0))=1, (ii) for all {a mathematical formula}a∈A we have {a mathematical formula}δ′((s,0),a)((s′,1))=δ(s,a)(s′), and (iii) for action # we have{a mathematical formula}{a mathematical formula}
      </list-item>
      <list-item label="•">
       there is a single observation {a mathematical formula}O={o}, and all the states {a mathematical formula}s∈S′ have {a mathematical formula}γ(s)=o.
      </list-item>
     </list>
     <paragraph>
      We define the Boolean reward function {a mathematical formula}r only as a function of the state, i.e., {a mathematical formula}r:S′→{0,1} and show the undecidability even for this special case of reward functions. For all {a mathematical formula}s∈S the reward is {a mathematical formula}r((s,0))=0, and similarly {a mathematical formula}r((s,1))=1, and the remaining two states have rewards {a mathematical formula}r(good)=1 and {a mathematical formula}r(bad)=0. Note that though the rewards are assigned as a function of states, the rewards appear on the transitions. We now establish the correctness of the reduction.
     </paragraph>
     <paragraph label="Proof">
      If there exists a word{a mathematical formula}w∈A⁎accepted with probability strictly greater than{a mathematical formula}12in{a mathematical formula}P, then there exists a pure finite-memory almost-sure winning strategy in the POMDP G for the objective{a mathematical formula}LimAvg&gt;12.Let {a mathematical formula}w∈A⁎ be a word accepted in {a mathematical formula}P with probability {a mathematical formula}μ&gt;12 and let the length of the word be {a mathematical formula}|w|=n−1. We construct a pure finite-memory almost-sure winning strategy for the {a mathematical formula}LimAvg&gt;12 objective in the POMDP G as follows: We denote by {a mathematical formula}w[i] the ith action in the word w. The finite-memory strategy we construct is specified as an ultimately periodic word {a mathematical formula}($w[1]$w[2]…$w[n−1]$##)ω. Observe that by the construction of the POMDP G, the sequence of rewards (that appear on the transitions) is {a mathematical formula}(10)n followed by (i) 1 with probability μ (when F is reached), and (ii) 0 otherwise; and the whole sequence is repeated ad infinitum. Also observe that once the pure finite-memory strategy is fixed we obtain a Markov chain with a single recurrent class since the starting state belongs to the recurrent class and all states reachable from the starting state form the recurrent class. We first establish the almost-sure convergence of the sequence of partial averages of the periodic blocks, and then of the sequences inside the periodic blocks as well.Almost-sure convergence of periodic blocks. Let {a mathematical formula}r1,r2,r3,… be the infinite sequence of rewards and {a mathematical formula}sj=1j⋅∑i=1jri. The infinite sequence of rewards can be partitioned into blocks of length {a mathematical formula}2⋅n+1, intuitively corresponding to the transitions of a single run on the word {a mathematical formula}($w[1]$w[2]…$w[n−1]$##). We define a random variable {a mathematical formula}Xi denoting average of rewards of the ith block in the sequence, i.e., with probability μ for each i the value of {a mathematical formula}Xi is {a mathematical formula}n+12⋅n+1 and with probability {a mathematical formula}1−μ the value is {a mathematical formula}n2⋅n+1. The expected value of {a mathematical formula}Xi is therefore equal to {a mathematical formula}E(Xi)=μ+n2⋅n+1, and as we have that {a mathematical formula}μ&gt;12 it follows that {a mathematical formula}E(Xi)&gt;12. The fact that we have a single recurrent class and after the ## the initial state is reached implies that the random variable sequence {a mathematical formula}(Xi)i≥0 is an infinite sequence of i.i.d's. By the Strong Law of Large Numbers (SLLN) [15, Theorem 7.1, page 56] we have that{a mathematical formula}Almost-sure convergence inside the periodic blocks. It follows that with probability 1 the {a mathematical formula}LimAvg of the partial averages on blocks is strictly greater than {a mathematical formula}12. As the blocks are of fixed length {a mathematical formula}2⋅n+1, if we look at the sequence of averages at every {a mathematical formula}2⋅n+1 step, i.e., the sequence {a mathematical formula}sj⋅(2n+1) for {a mathematical formula}j&gt;0, we have that this sequence converges with probability 1 to a value strictly greater than {a mathematical formula}12. It remains to show that all {a mathematical formula}si converge to that value. As the elements of the subsequence converging with probability 1 are always separated by exactly {a mathematical formula}2⋅n+1 elements (i.e., constant number of elements) and due to the definition of {a mathematical formula}LimAvg the deviation introduced by these {a mathematical formula}2⋅n+1 elements ultimately gets smaller than any {a mathematical formula}ϵ&gt;0 as the length of the path increases (the average is computed from the whole sequence so far, and deviation caused by a fixed length is negligible as the length increases). Therefore the whole sequence {a mathematical formula}(si)i&gt;0 converges to a value strictly greater than {a mathematical formula}12 with probability 1. It follows that the strategy ensures {a mathematical formula}LimAvg&gt;12 with probability 1.  □
     </paragraph>
     <paragraph>
      In the next two lemmas we first show the other direction for pure finite-memory strategies and then extend the result to the class of randomized infinite-memory strategies.
     </paragraph>
     <paragraph label="Proof">
      If there exists a pure finite-memory almost-sure winning strategy in the POMDP G for the objective{a mathematical formula}LimAvg&gt;12, then there exists a word{a mathematical formula}w∈A⁎accepted with probability strictly greater than{a mathematical formula}12in{a mathematical formula}P.Assume there exists a pure finite-memory almost-sure winning strategy σ for the objective {a mathematical formula}LimAvg&gt;12. Observe that as there is only a single observation in the POMDP G the strategy σ can be viewed as an ultimately periodic infinite word of the form {a mathematical formula}u⋅vω, where {a mathematical formula}u,v are finite words from {a mathematical formula}A′. Note that v must contain the subsequence ##, as otherwise the {a mathematical formula}LimAvg would be only {a mathematical formula}12. Similarly, before every letter {a mathematical formula}a∈A in the words {a mathematical formula}u,v, the strategy must necessarily play the $ action, as otherwise the losing absorbing state is reached.In the first step we align the ## symbols in v. Let us partition the word v into two parts {a mathematical formula}v=y⋅x such that y is the shortest prefix ending with ##. Then the ultimately periodic word {a mathematical formula}u⋅y⋅(x⋅y)ω=u⋅vω is also a strategy ensuring almost-surely {a mathematical formula}LimAvg&gt;12. Due to the previous step we consider {a mathematical formula}u′=u⋅y and {a mathematical formula}v′=x⋅y, and thus have that {a mathematical formula}v′ is of the form:{a mathematical formula} We extract the set of words {a mathematical formula}W={w1,w2,…,wm} from {a mathematical formula}v′. Assume for contradiction that all the words in the set W are accepted in the PFA {a mathematical formula}P with probability at most {a mathematical formula}12. As in Lemma 17 we define a random variable {a mathematical formula}Xi denoting the average of rewards after reading {a mathematical formula}v′. It follows that the expected value of {a mathematical formula}E(Xi)≤12 for all {a mathematical formula}i≥0. By using SLLN we obtain that almost-surely the {a mathematical formula}LimAvg of {a mathematical formula}u⋅vω is {a mathematical formula}E(Xi), and hence it is not possible as {a mathematical formula}u⋅vω is an almost-sure winning strategy for the objective {a mathematical formula}LimAvg&gt;12. We reach a contradiction to the assumption that all the words in W are accepted with probability at most {a mathematical formula}12 in {a mathematical formula}P. Therefore there exists a word {a mathematical formula}w∈W that is accepted in {a mathematical formula}P with probability strictly greater than {a mathematical formula}12, which concludes the proof.  □
     </paragraph>
     <paragraph>
      To complete the reduction we show in the following lemma that pure strategies are sufficient for the POMDPs constructed in our reduction.
     </paragraph>
     <paragraph label="Proof">
      Given the POMDP G of our reduction, if there is a randomized (possibly infinite-memory) almost-sure winning strategy for the objective{a mathematical formula}LimAvg&gt;12, then there exists a pure finite-memory almost-sure winning strategy{a mathematical formula}σ′for the objective{a mathematical formula}LimAvg&gt;12.Let σ be a randomized (possibly infinite-memory) almost-sure winning strategy for the objective {a mathematical formula}LimAvg&gt;12. As there is a single observation in the POMDP G constructed in the reduction, the strategy does not receive any useful feedback from the play, i.e., the memory update function {a mathematical formula}σu always receives as one of the parameters the unique observation. Note that the strategy needs to play the pair ## of actions infinitely often with probability 1, i.e., with probability 1 the resolving of the probabilities in the strategy σ leads to an infinite word {a mathematical formula}ρ=w1##w2##…, as otherwise the limit-average payoff is at most {a mathematical formula}12 with positive probability. From each such run ρ we extract the finite words {a mathematical formula}w1,w2,… that occurs in ρ, and then consider the union of all such words as W. We consider two options:
     </paragraph>
     <list>
      <list-item label="1.">
       If there exists a word v in W such that the expected average of rewards after playing this word is strictly greater than {a mathematical formula}12, then the pure strategy {a mathematical formula}vω is also a pure finite-memory almost-sure winning strategy for the objective {a mathematical formula}LimAvg&gt;12.
      </list-item>
      <list-item label="2.">
       Assume for contradiction that all the words in W have the expected reward at most {a mathematical formula}12. Then with probability 1 resolving the probabilities in the strategy σ leads to an infinite word {a mathematical formula}w¯=w1##w2##…, where each word {a mathematical formula}wi belongs to W, that is played on the POMDP G. Let us define a random variable {a mathematical formula}Xi denoting the average between i and {a mathematical formula}(i+1)th occurrence of ##. The expected average {a mathematical formula}E(Xi) is at most {a mathematical formula}12 for all i. Therefore the expected {a mathematical formula}LimAvg of the sequence {a mathematical formula}wˆ is at most:{a mathematical formula}Since {a mathematical formula}Xi's are non-negative measurable functions, by Fatou's lemma [15, Theorem 3.5, page 16] that shows the integral of limit inferior of a sequence of non-negative measurable functions is at most the limit inferior of the integrals of these functions, we have the following inequality:{a mathematical formula}Note that since the strategy σ is almost-sure winning for the objective {a mathematical formula}LimAvg&gt;12, then the expected value of rewards must be strictly greater than {a mathematical formula}12. Thus we arrive at a contradiction. Hence there must exist a word in W that has an expected payoff strictly greater than {a mathematical formula}12 in G.
      </list-item>
     </list>
     <paragraph label="Theorem 4">
      The problem whether there exists a finite (or infinite-memory) almost-sure winning strategy in a POMDP for the objective{a mathematical formula}LimAvg&gt;12is undecidable.
     </paragraph>
    </section>
    <section label="5">
     <section-title>
      Infinite-memory strategies with qualitative constraints
     </section-title>
     <paragraph>
      In this section we show that the problem of deciding the existence of infinite-memory almost-sure winning strategies in POMDPs with {a mathematical formula}LimAvg=1 objectives is undecidable. We prove this fact by a reduction from the value 1 problem in PFA, which is undecidable [17]. The value 1 problem given a PFA {a mathematical formula}P asks whether for every {a mathematical formula}ϵ&gt;0 there exists a finite word w such that the word is accepted in {a mathematical formula}P with probability at least {a mathematical formula}1−ϵ (i.e., the limit of the acceptance probabilities is 1).
     </paragraph>
     <paragraph>
      Reduction. Given a PFA {a mathematical formula}P=(S,A,δ,F,s0), we construct a POMDP {a mathematical formula}G′=(S′,A′,δ′,O′,γ′,s0′) with a reward function {a mathematical formula}r′, such that {a mathematical formula}P satisfies the value 1 problem iff there exists an infinite-memory almost-sure winning strategy in {a mathematical formula}G′ for the objective {a mathematical formula}LimAvg=1. Intuitively, the construction adds two additional states {a mathematical formula}good and {a mathematical formula}bad. We add an edge from every state of the PFA under a new action $, this edge leads to the state {a mathematical formula}good when played in a final state, and to the state {a mathematical formula}bad otherwise. In the states {a mathematical formula}good and {a mathematical formula}bad we add self-loops under a new action #. The action $ in the states {a mathematical formula}good or {a mathematical formula}bad leads back to the initial state. An example of the construction is illustrated with Fig. 5. All the states belong to a single observation, and we will use Boolean reward function on states. The reward for all states except the newly added state {a mathematical formula}good is 0, and the reward for the state {a mathematical formula}good is 1. The formal construction is as follows:
     </paragraph>
     <list>
      <list-item label="•">
       {a mathematical formula}S′=S∪{good,bad},
      </list-item>
      <list-item label="•">
       {a mathematical formula}s0′=s0,
      </list-item>
      <list-item label="•">
       {a mathematical formula}A′=A∪{#,$},
      </list-item>
      <list-item label="•">
       For all {a mathematical formula}s,s′∈S and {a mathematical formula}a∈A we have {a mathematical formula}δ′(s,a)(s′)=δ(s,a)(s′),{a mathematical formula}{a mathematical formula}{a mathematical formula}
      </list-item>
      <list-item label="•">
       there is a single observation {a mathematical formula}O={o}, and all the states {a mathematical formula}s∈S′ have {a mathematical formula}γ(s)=o.
      </list-item>
     </list>
     <paragraph>
      When an action is played in a state without an outgoing edge for the action, the losing absorbing state is reached; i.e., for action # in states in S and actions {a mathematical formula}a∈A for states {a mathematical formula}good and {a mathematical formula}bad, the next state is the losing absorbing state. The Boolean reward function {a mathematical formula}r′:S′→{0,1} assigns all states {a mathematical formula}s∈S∪{bad} the reward {a mathematical formula}r′(s)=0, and {a mathematical formula}r′(good)=1.
     </paragraph>
     <paragraph label="Lemma 20">
      If the PFA{a mathematical formula}Psatisfies the value 1 problem, then there exists an infinite-memory almost-sure winning strategy ensuring the{a mathematical formula}LimAvg=1objective.
     </paragraph>
     <paragraph label="Proof">
      We construct an almost-sure winning strategy σ that we describe as an infinite word. As {a mathematical formula}P satisfies the value 1 problem, there exists a sequence of finite words {a mathematical formula}(wi)i≥1, such that each {a mathematical formula}wi is accepted in {a mathematical formula}P with probability at least {a mathematical formula}1−12i+1. We construct an infinite word {a mathematical formula}w1⋅$⋅#n1⋅w2⋅$⋅#n2⋯, where each {a mathematical formula}ni∈N is a natural number that satisfies the following condition: let {a mathematical formula}ki=|wi+1⋅$|+∑j=1i(|wj⋅$|+nj) be the length of the word sequence before {a mathematical formula}#ni+1, then we must have {a mathematical formula}niki≥1−1i. In other words, the length {a mathematical formula}ni is large enough such that even if the whole sequence of length {a mathematical formula}∑j=1i(|wj⋅$|+nj) before {a mathematical formula}ni and the sequence of length {a mathematical formula}|wi+1⋅$| is 0s, the sequence {a mathematical formula}ni 1s ensures the average is at least {a mathematical formula}1−1i. Intuitively the condition ensures that even if the rewards are always 0 for the prefix up to {a mathematical formula}wi⋅$ of a run, even then a single visit to the {a mathematical formula}good state after {a mathematical formula}wi⋅$ can ensure the average to be greater {a mathematical formula}1−1i from the end of {a mathematical formula}#ni up to the point {a mathematical formula}wi+1⋅$ ends. We first argue that if the state {a mathematical formula}bad is visited finitely often with probability 1, then with probability 1 the objective {a mathematical formula}LimAvg=1 is satisfied.Almost-sure winning if{a mathematical formula}badoccurs only finitely often. Observe that if the state {a mathematical formula}bad appears only finitely often, then from some {a mathematical formula}j≥0, for all {a mathematical formula}ℓ≥j, the state visited after {a mathematical formula}w1⋅$⋅#n1⋅w2⋅$⋅#n2⋯wℓ⋅$ is the state {a mathematical formula}good, and then the sequence {a mathematical formula}#nℓ ensures that the payoff from the end of {a mathematical formula}#nℓ up to the end of {a mathematical formula}wℓ+1⋅$ is at least {a mathematical formula}1−1ℓ. If the state {a mathematical formula}good is visited, then the sequence of # gives reward 1, and thus after a visit to the state {a mathematical formula}good, the average only increases in the sequence of #'s. Hence it follows that lim-inf average of the rewards is at least {a mathematical formula}1−1i, for all {a mathematical formula}i≥0 (i.e., for every {a mathematical formula}i≥0, there exists a point in the path such that the average of the rewards never falls below {a mathematical formula}1−1i). Since this holds for all {a mathematical formula}i≥0, it follows that {a mathematical formula}LimAvg=1 is ensured with probability 1 (provided with probability 1 the state {a mathematical formula}bad occurs only finitely often).The state{a mathematical formula}badoccurs only finitely often. We now need to show that the state {a mathematical formula}bad is visited infinitely often with probability 0 (i.e., occurs only finitely often with probability 1). We first upper bound the probability {a mathematical formula}uk+1 of visiting the state {a mathematical formula}bad at least {a mathematical formula}k+1 times, given k visits to state {a mathematical formula}bad. The probability {a mathematical formula}uk+1 is at most {a mathematical formula}12k+1(1+12+14+⋯). The above bound for {a mathematical formula}uk+1 is obtained as follows: following the visit to {a mathematical formula}bad for k times, the words {a mathematical formula}wj, for {a mathematical formula}j≥k are played; and hence the probability to reach {a mathematical formula}bad decreases by {a mathematical formula}12 every time the next word is played; and after k visits the probability is always smaller than {a mathematical formula}12k+1. Hence the probability to visit {a mathematical formula}bad at least {a mathematical formula}k+1 times, given k visits, is at most the sum above, which is {a mathematical formula}12k. Let {a mathematical formula}Ek denote the event that {a mathematical formula}bad is visited at least {a mathematical formula}k+1 times given k visits to {a mathematical formula}bad. Then we have {a mathematical formula}∑k≥0P(Ek)≤∑k≥112k&lt;∞. By Borel–Cantelli lemma [15, Theorem 6.1, page 47] we know that if the sum of probabilities is finite (i.e., {a mathematical formula}∑k≥0P(Ek)&lt;∞), then the probability that infinitely many of them occur is 0 (i.e., {a mathematical formula}P(lim⁡supk→∞⁡Ek)=0). Hence the probability that {a mathematical formula}bad is visited infinitely often is 0, i.e., with probability 1 {a mathematical formula}bad is visited finitely often.It follows that the strategy σ that plays the infinite word {a mathematical formula}w1⋅$⋅#n1⋅w2⋅$⋅#n2⋯ is an almost-sure winning strategy for the objective {a mathematical formula}LimAvg=1.  □
     </paragraph>
     <paragraph label="Proof">
      If there exists an infinite-memory almost-sure winning strategy for the objective{a mathematical formula}LimAvg=1, then the PFA{a mathematical formula}Psatisfies the value 1 problem.We prove the contrapositive. Consider that the PFA {a mathematical formula}P does not satisfy the value 1 problem, i.e., there exists a constant {a mathematical formula}c&gt;0 such that for all {a mathematical formula}w∈A⁎ we have that the probability that w is accepted in {a mathematical formula}P is at most {a mathematical formula}1−c&lt;1. We will show that there is no almost-sure winning strategy. Assume for contradiction that there exists an infinite-memory almost-sure winning strategy σ in the POMDP {a mathematical formula}G′. In POMDPs, for all measurable objectives, infinite-memory pure strategies are as powerful as infinite-memory randomized strategies [7, Lemma 1].{sup:2} Therefore we may assume that the almost-sure winning strategy is given as an infinite word {a mathematical formula}w¯. Note that the infinite word {a mathematical formula}w¯ must necessarily contain infinitely many $ and can be written as {a mathematical formula}w¯=w1⋅$⋅#n1⋅$⋅w2⋅$⋅#n2⋅$⋯. Moreover, there must be infinitely many {a mathematical formula}i&gt;0 such that the number {a mathematical formula}ni is positive (since only such segments yield reward 1).Consider the Markov chain {a mathematical formula}G′↾σ and the rewards on the states of the chain. By the definition of the reward function {a mathematical formula}r′ all the rewards that correspond to words {a mathematical formula}wi for {a mathematical formula}i&gt;0 are equal to 0. Rewards corresponding to the word segment {a mathematical formula}#ni for {a mathematical formula}i&gt;0 are with probability at most {a mathematical formula}1−c equal to 1 and 0 otherwise. Let us for the moment remove the 0 rewards corresponding to words {a mathematical formula}wi for all {a mathematical formula}i&gt;0 (removing the reward 0 segments only increases the limit-average payoff), and consider only rewards corresponding to the segments {a mathematical formula}#ni for all {a mathematical formula}i&gt;0, we will refer to this infinite sequence of rewards as {a mathematical formula}wˆ. Let {a mathematical formula}Xi denote the random variable corresponding to the value of the ith reward in the sequence {a mathematical formula}wˆ. Then we have that {a mathematical formula}Xi=1 with probability at most {a mathematical formula}1−c and 0 otherwise. The expected {a mathematical formula}LimAvg of the sequence {a mathematical formula}wˆ is then at most:{a mathematical formula} Since {a mathematical formula}Xi's are non-negative measurable function, by Fatou's lemma [15, Theorem 3.5, page 16] that shows the integral of limit inferior of a sequence of non-negative measurable functions is at most the limit inferior of the integrals of these functions, we have the following inequality:{a mathematical formula}If we put back the rewards 0 from words {a mathematical formula}wi for all {a mathematical formula}i&gt;0, then the expected value can only decrease. It follows that {a mathematical formula}Eσ(LimAvg)≤1−c. Note that if the strategy σ was almost-sure winning for the objective {a mathematical formula}LimAvg=1 (i.e., {a mathematical formula}Pσ(LimAvg=1)=1), then the expectation of the {a mathematical formula}LimAvg payoff would also be 1 (i.e., {a mathematical formula}Eσ(LimAvg)=1). Therefore we have reached a contradiction to the fact that the strategy σ is almost-sure winning, and the result follows.  □
     </paragraph>
     <paragraph label="Theorem 5">
      The problem whether there exists an infinite-memory almost-sure winning strategy in a POMDP with the objective{a mathematical formula}LimAvg=1is undecidable.
     </paragraph>
    </section>
    <section label="6">
     <section-title>
      Implementation and experimental results
     </section-title>
     <paragraph>
      We implemented in Java the presented algorithm that given a POMDP with a qualitative constraint decides whether there exists a finite-memory almost-sure winning strategy. Along with the algorithm we also employed a heuristic to avoid the worst-case exponential complexity.
     </paragraph>
     <paragraph>
      Heuristics. Our main heuristic to overcome the exponential complexity of the algorithm is based on an idea to reduce the size of the memory set of the collapsed strategy. Instead of storing the mappings {a mathematical formula}RecFun and {a mathematical formula}AWFun for every state of the POMDP, we store the mappings restricted to the current belief support. Intuitively, for all states that are not in the belief support, the probability of being in them is 0. Therefore, the information stored about these states is not relevant for the strategy. The current belief support is often significantly smaller than the subset construction of the state space, as the size of the belief support is bounded by the size of the largest observation in the POMDP. This heuristic dramatically reduces the size of {a mathematical formula}G¯←Red(G) of Algorithm 1 in our example case studies.
     </paragraph>
     <paragraph>
      Experiments. We evaluated our algorithm on the examples that are discussed below:
     </paragraph>
     <paragraph>
      Avoid the observer (AV). Inspired by the maze POMDPs from [27] we consider a POMDP that models a maze where an intruder can decide to stay in the same position or move deterministically in all four directions, whenever the obstacles in the maze allow the movement. There is an observer moving probabilistically through the maze that checks that there is no intruder present. The intruder cannot see the observer whenever it is behind an obstacle.
     </paragraph>
     <paragraph>
      We consider two settings of the problem: (1) In the capture mode all actions of the intruder have reward 1 with the exception of the moves when the intruder is captured by the observer, i.e, the intruder and the observer are in the same position in the maze. The latter moves have reward 0. (2) In the detect mode we assume the observer is equipped with sensors that can also detect the intruder on a neighboring position (with no walls in between). The rewards are defined as follows: (i) for every move of the intruder that is undetected by the observer the reward is 1; (ii) the reward for a move where the intruder is detected but not captured is 0.5 (the intruder and the observer are on neighboring positions); and (iii) the reward for a move where the intruder is captured is 0 (the intruder and the observer are on the same position).
     </paragraph>
     <paragraph>
      We consider three variants small, medium, and large that differ in the size and the structure of the maze. The variants are depicted in Fig. 6, Fig. 7, Fig. 8, respectively. The POMDP corresponding to variant AV – small has 50 states and the intruder starts at position 0 and the observer at position 7. The POMDP corresponding to variant AV – medium has 145 states and the intruder starts at position 0 and the observer at position 11. The last POMDP corresponding to variant AV – large has 257 states and the intruder starts at position 0 and the observer at position 15.
     </paragraph>
     <paragraph>
      Supply the space station (SP). We consider an extension of the space shuttle docking problem that originally comes from [11], [27]. In the original setting the POMDP models a simple supply delivery problem between two space stations, where a space shuttle must deliver supplies to two stations in space. There are four actions available to the space shuttle: move forwards, turn around, back up, and drop supplies. A successful supply delivery to a station is modeled by first backing up into the space station and then by dropping the supplies in the space station.
     </paragraph>
     <paragraph>
      In the original setting the goal is to supply both stations infinitely often in an alternate fashion. We extend the model by introducing a time bound on the number of turns the space shuttle is allowed to spend traveling in space (without delivering any supplies). We assume there is a given amount of supplies in a space station that decreases in every turn by one, and whenever the space shuttle delivers supplies, the amount of supplies is restored to its maximum. We consider two different bounds: 3 – step SP introduces a 3 turn time bound on the traveling time (supplies for 6 turns in every station), and 6 – step SP introduces a 6 turn time bound on the traveling time (supplies for 12 turns in every station). The schematic representation of the space shuttle is depicted in Fig. 9 and originally comes from [27]. The leftmost and rightmost states in Fig. 9 are the docking stations, the most recently visited docking station is labeled with MRV, and the least recently visited docking station is labeled with LRV. The property of the model is that whenever a LRV station is visited it automatically changes its state to MRV. The actions move forward, turn around, and drop supplies are deterministic. The movement under back up action is probabilistic when the space shuttle is in space, e.g., the space shuttle can move backwards as intended, not move at all, or even turn around, with positive probability.
     </paragraph>
     <paragraph>
      States: The POMDP is a synchronized product of two components: the space shuttle, and the counter that keeps track of the remaining supplies. The state of the space shuttle are as follows: 0 – docked in LRV; 1 – just outside space station MRV, front of ship facing station; 2 – space, facing MRV; 3 – just outside space station LRV, back of ship facing station; 4 – just outside space station MRV, back of ship facing station; 5 – space, facing LRV; 6 – just outside space station LRV, front of ship facing station; 7 – docked in MRV, the initial state. The states of the counter are simply the remaining amount of supplies. The space shuttle cannot distinguish the states in space, with the following exception: the states where the shuttle is facing a station, i.e., states 2, 3, 4, and 5 have the same observation different from the observation of other states in the space.
     </paragraph>
     <paragraph>
      Rewards: The rewards of actions are defined as follows: (1) when the shuttle is facing a station and plays action move forward it bumps into a space station and receives reward 0.1; (2) dropping the supplies outside of the space station receives reward 0.5; (3) every other action as long as the stations are supplied receives reward 1; (4) if the shuttle is traveling too long and the amount of supplies in one of the stations reaches 0 every action receives reward 0 until the station is supplied.
     </paragraph>
     <paragraph>
      RockSample (RS). We consider a modification of the RockSample problem introduced in [41] and used later in [4]. It is a scalable problem that models rover science exploration. The rover is equipped with a limited amount of fuel and can increase the amount of fuel by sampling rocks in the immediate area. The positions of the rover and the rocks are known, but only some of the rocks can increase the amount of fuel; we will call these rocks good. The type of the rock is not known to the rover, until the rock is sampled. Once a good rock is used to increase the amount of fuel, it becomes temporarily a bad rock until all other good rocks are sampled. We consider variants with different maximum capacity of the rover's fuel tank. An instance of the RockSample problem is parametrized with two parameters {a mathematical formula}[n,k]: map size {a mathematical formula}n×n and k rocks is described as RockSample[n,k]. The POMDP model of RockSample[n,k] is as follows:
     </paragraph>
     <paragraph>
      States: The state space is the cross product of {a mathematical formula}2k+1+c features: {a mathematical formula}Position={(1,1),(1,2),…,(n,n)}, {a mathematical formula}2⁎k binary features {a mathematical formula}RockTypei={Good,Bad} that indicate which of the rocks are good and which rocks are temporarily not able to increase the amount of fuel, and c is the amount of fuel remaining in the fuel tank. The rover can select from four actions: {a mathematical formula}{N,S,E,W}. All the actions are deterministic single-step motion actions. A rock is sampled whenever the rock is at the rover's current location.
     </paragraph>
     <paragraph>
      Rewards: If the rock is good, the fuel amount is increased to the maximum capacity and the rock becomes temporarily bad. For every move the rover receives reward 1 with the following two exceptions:
     </paragraph>
     <list>
      <list-item label="•">
       When the rover samples a bad rock (also temporarily bad rocks) the reward is 0.
      </list-item>
      <list-item label="•">
       If the fuel amount decreases to 0, every move has reward 0 until the fuel is increased by sampling a good rock.
      </list-item>
     </list>
     <paragraph>
      The instance RS[4,2] (resp. RS[4,3]) is depicted on Fig. 10 (resp. Fig. 11), the arrow indicates the initial position of the rover and the filled rectangles denote the fixed positions of the rocks.
     </paragraph>
     <paragraph>
      Results. We have run our implementation on all the examples described above. The results are summarized in Table 2. For every POMDP and its variant we show the number of states {a mathematical formula}|S|, the number of actions {a mathematical formula}|A|, the number of observations {a mathematical formula}|O| of the POMDP, the running time of our tool in seconds, and finally the last column denotes whether there exists a finite-memory almost-sure winning strategy for the {a mathematical formula}LimAvg=1 objective. Our implementation could solve all the examples in less than 5 minutes.
     </paragraph>
    </section>
    <section label="7">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      We now discuss our work in comparison to several important works in the literature. We study almost-sure winning for limit-average objectives with qualitative and quantitative constraints, which are infinite-horizon objectives. Two special cases of limit-average objectives with qualitative constraints are (i) reachability objectives, where the target states are absorbing and the only states with reward 1 and all other rewards are 0; and (ii) safety objectives, where the non-safe states are absorbing and have reward 0 and all other rewards are 1.
     </paragraph>
     <paragraph>
      (Un)Decidability frontier. A very important and strong negative result of Madani et al. [30] shows that approximating the maximal acceptance probability for PFA is undecidable. The result of Madani et al. [30] implies that for POMDPs with reachability objectives approximating the maximal probability to reach the target set both for finite-memory and infinite-memory strategies is undecidable. This implies that for limit-average objectives with a qualitative constraint, under expected semantics or probabilistic winning (i.e., to approximate the maximal probability), the decision problems are undecidable both for finite-memory and infinite-memory strategies. The problem of almost-sure winning with limit-average objectives was not studied before. We show that the almost-sure winning problem with a qualitative constraint is EXPTIME-complete for finite-memory strategies, whereas undecidable for infinite-memory strategies (in contrast to Madani et al. [30] where the undecidability result holds uniformly both for finite-memory and infinite-memory strategies). We also show that the almost-sure winning problem with quantitative constraints is undecidable even for finite-memory strategies.
     </paragraph>
     <paragraph>
      Strategy complexity. It was shown by Lusena et al. [29] that for POMDPs with finite-horizon or infinite-horizon average reward objectives, state-based memoryless strategies are not sufficient. However, for almost-sure winning, for PFA or POMDPs with safety and reachability objectives, belief-support-based memoryless strategies are sufficient: the result for safety objectives follows from the work of Reif [40] and the result for reachability objectives follows from a reduction of POMDPs to partial-observation games by Chatterjee et al. [7]. Sufficiency of belief-support-based (randomized) memoryless strategies was established by Chatterjee et al. [9]. In contrast, we show that for almost-sure winning of limit-average objectives with qualitative constraints, belief-support-based strategies are not sufficient (Example 1). It was shown by Meuleau et al. [31] that for fixed finite-memory strategies, solving POMDPs is isomorphic to finding memoryless strategies in the “cross-product” POMDP. However, note that without an explicit bound on the size of the finite-memory strategies no decidability results can be obtained (for example, approximating the maximal acceptance probability for PFA is undecidable even for finite-memory strategies [30]). Thus our exponential upper bound on the size of the finite memory is the first crucial step in establishing the decidability result. Moreover, it follows from the results of Chatterjee et al. [10] that for POMDPs deciding almost-sure winning under memoryless strategies is NP-complete, even for reachability objectives. Thus using the exponential cross-product construction and an NP algorithm would yield a NEXPTIME algorithm, whereas we present an EXPTIME algorithm for almost-sure winning in POMDPs with limit-average objectives under qualitative constraints.
     </paragraph>
     <paragraph>
      Finite-horizon objectives. There are several works that consider the stochastic shortest path in POMDPs [32] and the finite-horizon objectives [34], [19]. It was shown by Mundhenk et al. [34] that the complexity of computing the optimal finite-horizon expected reward is EXPTIME for POMDPs. Our work is very different, since we consider infinite-horizon objectives in contrast to finite-horizon objectives, and almost-sure winning instead of expected semantics. Moreover, limit-average objectives are fundamentally different from finite-horizon and shortest path objectives: in finite-horizon and shortest path objectives every reward contributes to the payoff, whereas in limit-average objectives a finite prefix of rewards does not change the payoff (for example, a finite sequence of zero rewards followed by all one rewards still has the limit-average payoff of 1). In other words, limit-average objectives model the long-run behavior of POMDPs, as compared to finite-horizon objectives.
     </paragraph>
     <paragraph>
      Almost-sure winning for special cases. The problem of almost-sure winning for POMDPs with safety and reachability objectives has been considered before. For safety objectives, if the almost-sure winning is not satisfied, then the violation is witnessed by a finite path which has positive probability. With this intuition it was shown by Alur et al. [1] that the almost-sure winning for POMDPs with safety objectives can be reduced to partial-observation games with deterministic strategies, and EXPTIME-completeness was established using the results of Reif [40]. For almost-sure winning with reachability objectives, the problem can be reduced to partial-observation games with randomized strategies [7], the EXPTIME upper bound for randomized strategies in games follows from Chatterjee et al. [9], and the EXPTIME lower bound for POMDPs was established by Chatterjee et al. [8]. In both cases for reachability and safety objectives, the EXPTIME upper bound crucially relies on the existence of belief-support-based strategies, that does not hold for limit-average objectives with qualitative constraints (Example 1). Our main contribution for limit-average objectives with a qualitative constraint is the EXPTIME upper bound (the lower bound follows from existing results); and all our undecidability proofs for almost-sure winning are completely different from the EXPTIME lower bounds for reachability and safety objectives.
     </paragraph>
     <paragraph>
      Comparison with Chatterjee et al.[6]. A preliminary version of our work was presented by Chatterjee et al. [6]. In comparison our present version contains full and detailed proofs of all the results, better description of our algorithm, and Section 6 on implementation and experimental results is completely new.
     </paragraph>
    </section>
    <section label="8">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      We studied POMDPs with limit-average objectives under probabilistic semantics. Since for general probabilistic semantics, the problems are undecidable even for PFA, we focus on the very important special case of almost-sure winning. For almost-sure winning with a qualitative constraint, we show that belief-support-based strategies are not sufficient for finite-memory strategies, and establish EXPTIME-complete complexity for the existence of finite-memory strategies. Given our decidability result, the next natural questions are whether the result can be extended to infinite-memory strategies, or to quantitative path constraints. We show that both these problems are undecidable, and thus establish the precise decidability frontier with tight complexity bounds. Also observe that contrary to other classical results for POMDPs where both the finite-memory and infinite-memory problems are undecidable, for almost-sure winning with a qualitative constraint, we show that the finite-memory problem is decidable (EXPTIME-complete), but the infinite-memory problem is undecidable. We also present a prototype implementation of our EXPTIME-complete algorithm along with a heuristic, and the implementation works reasonably efficiently on example POMDPs from the literature. An interesting direction of future work would be to consider discounted reward payoff instead of limit-average payoff.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>