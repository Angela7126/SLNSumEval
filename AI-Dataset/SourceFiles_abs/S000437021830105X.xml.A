<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    The complexity and generality of learning answer set programs.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Over the last two decades there has been a growing interest in Inductive Logic Programming (ILP) [1], where the goal is to learn a logic program called a hypothesis, which together with a given background knowledge base, explains a set of examples. The main advantage that ILP has over traditional statistical machine learning approaches is that the learned hypotheses can be easily expressed in plain English and explained to a human user, so facilitating a closer interaction between humans and machines. Traditional ILP frameworks have focused on learning definite logic programs [1], [2], [3], [4], [5], [6] and normal logic programs [7], [8]. On the other hand, Answer Set Programming [9] is a powerful language for knowledge representation and reasoning. ASP is closely related to other declarative paradigms such as SAT, SMT and Constraint Programming, which have each been used for inductive reasoning [10], [11], [12]. Compared with these other paradigms, due to its non-monotonicity, ASP is particularly suited for common-sense reasoning [13], [14], [15]. Because of its expressiveness and efficient solving, ASP is also increasingly gaining attention in industry [16]; for example, in decision support systems [17], in e-tourism [18] and in product configuration [19]. Consequently, the scope of ILP has recently been extended to learning answer set programs from examples of partial solutions of a given problem, with the intention being to provide algorithms that support automated learning of complex declarative knowledge. Learning ASP programs allows us to learn a variety of declarative non-monotonic, common-sense theories, including for instance Event Calculus [20] theories [21] and theories for scheduling problems and agents' preference models, both from real user data [22] and from synthetic data [23], [24].
     </paragraph>
     <paragraph>
      Learning ASP programs has several advantages when compared to learning Prolog programs. Firstly, when learning Prolog programs, the goal directed SLDNF procedure of Prolog must be taken into account. Specifically, when learning programs with negation, it must be ensured that the programs are stratified, or otherwise the learned program may loop under certain queries. As ASP is declarative, no such consideration need be taken into account when learning ASP programs. A second, more fundamental advantage of learning ASP programs, is that the theory learned can be expressed using extra types of rules that are not available in Prolog, such as choice rules and weak constraints. Learning choice rules allows us to learn non-deterministic concepts; for instance, we may learn that a coin may non-deterministically land on either heads or tails, but never both. This could be achieved by learning the simple choice rule {a mathematical formula}1{heads,tails}1. Learning choice rules is different from probabilistic ILP settings such as [25], [26], [27] where, in similar coins problems the focus would be on learning the probabilities of the two outcomes of are coin. Learning weak constraints enables a natural extension of ILP to preference learning [23], which has resulted to be effective in problem domains such as learning preference models for scheduling [23] and for urban mobility [24].
     </paragraph>
     <paragraph>
      Several algorithms, aimed at learning under the answer set semantics, and different frameworks for learning ASP programs have been recently introduced in the literature. [28] presented the notions of brave induction ({a mathematical formula}ILPb) and cautious induction ({a mathematical formula}ILPc), based respectively on the well established notions of entailment under the answer set semantics [13], [29] of brave entailment (when an atom is true in at least one answer set) and cautious entailment (when and an atom is true in all answer sets). In brave induction, at least one answer set must cover the examples, whereas in cautious induction, every answer set must cover the examples. Brave induction is actually a special case of an earlier learning framework, called induction of stable models ({a mathematical formula}ILPsm) [30], in which examples are partial interpretations. A hypothesis is a solution of an induction of stable models task if for each of the example partial interpretations, there is an answer set of the hypothesis combined with the background knowledge, that covers that partial interpretation. Brave induction is equivalent to induction of stable models with exactly one (partial interpretation) example.
     </paragraph>
     <paragraph>
      Each of the above frameworks for learning ASP programs is unable to learn some types of ASP programs [31]; for example, brave induction alone cannot learn programs containing hard constraints. In [31], we presented a learning framework, called Learning from Answer Sets ({a mathematical formula}ILPLAS), which unifies brave and cautious induction and is able to learn ASP programs containing normal rules, choice rules and hard constraints. In spite of the increased expressivity, none of the above approaches can learn weak constraints, which are able to capture preference learning. Informally, learning weak constraints consists on identifying conditions for ordering answer sets. The learning task in this case would require examples of orderings over partial interpretations. To tackle this aspect of learning ASP programs, we have extended the Learning from Answer Sets framework to Learning from Ordered Answer Sets ({a mathematical formula}ILPLOAS) [23] and demonstrated that our algorithm{sup:1} is able to learn preferences in a scheduling domain. More recently, we have extended the {a mathematical formula}ILPLOAS framework to {a mathematical formula}ILPLOAScontext, with context-dependent examples, which come together with extra contextual information [24].
     </paragraph>
     <paragraph>
      In this paper, we explore both the expressive power and the computational complexity of each framework. The former is important, as it allows us to identify the class of problems that each framework can solve, whereas the latter gives an indication of the price paid for using each framework. We characterise the expressive power of a framework in terms of new notions called one-to-one-distinguishability, one-to-many-distinguishability and many-to-many-distinguishability. The intuition of one-to-one-distinguishability is that, given some fixed background knowledge B and sufficient examples, the framework should be able to distinguish a target hypotheses {a mathematical formula}H1 from another, unwanted, hypotheses {a mathematical formula}H2. This means that there should be at least one task T (of the given framework) with background knowledge B, such that {a mathematical formula}H1 is a solution of T, and {a mathematical formula}H2 is not. We characterise the one-to-one-distinguishability class of a framework {a mathematical formula}F (written {a mathematical formula}D11(F)) as the set of tuples {a mathematical formula}〈B,H1,H2〉 for such B's, {a mathematical formula}H1's and {a mathematical formula}H2's, and state that a framework {a mathematical formula}F1 is more {a mathematical formula}D11-general than another {a mathematical formula}F2 if {a mathematical formula}F2's one-to-one-distinguishability class is a strict subset of {a mathematical formula}F1's one-to-one-distinguishability class.
     </paragraph>
     <paragraph>
      One-to-many-distinguishability relates to the task of finding a single target hypothesis from within a set of possible hypotheses. It upgrades the notion of one-to-one-distinguishability classes to one-to-many-distinguishability classes. These are tuples of the form {a mathematical formula}〈B,H,S〉 for which a framework has at least one task that includes H and none of the (unwanted) hypotheses in S as an inductive solution. Many-to-many-distinguishability upgrades this notion to many-to-many-distinguishability classes. These contain tuples of the form {a mathematical formula}〈B,S1,S2〉, where {a mathematical formula}S1 is a set of target hypotheses, for which a framework must have a task that accepts each hypothesis in {a mathematical formula}S1 and no hypothesis in {a mathematical formula}S2 as inductive solution. We show that, under these three measures, {a mathematical formula}ILPLOAScontext is more general than {a mathematical formula}ILPLOAS, which is more general than {a mathematical formula}ILPLAS. We also show that {a mathematical formula}ILPLAS is more general than both {a mathematical formula}ILPsm and {a mathematical formula}ILPc. Although {a mathematical formula}ILPsm is equally {a mathematical formula}D11-general to {a mathematical formula}ILPb, we show that {a mathematical formula}ILPsm is more general than {a mathematical formula}ILPb under the one-to-many and many-to-many generality measures.
     </paragraph>
     <paragraph>
      Despite the different generalities of {a mathematical formula}ILPc, {a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext, we show that the computational complexity of all four frameworks is the same, both for the decision problem of verifying that a given hypothesis is a solution of a given learning task, and for the problem of deciding whether a given learning task has any solutions. Similarly, we also show that {a mathematical formula}ILPsm and {a mathematical formula}ILPb have the same computational complexities for both decision problems, despite the former being more general than the latter under two of our generality measures.
     </paragraph>
     <paragraph>
      We begin, in Section 2, by reviewing the background material necessary for the rest of the paper. In Section 3 we recall the definitions of each of the learning frameworks and in Sections 4 and 5 we prove the complexities and generalities (respectively) of each learning framework. We conclude the paper with a discussion of the related and future work.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background
     </section-title>
     <section label="2.1">
      <section-title>
       Answer Set Programming
      </section-title>
      <paragraph>
       In this section we introduce the concepts needed in the paper. Given any atoms {a mathematical formula}h,h1,…,hk,b1,…,bn,c1,…,cm, {a mathematical formula}h:-b1,…,bn,notc1,…,notcm is called a normal rule, with h as the head and {a mathematical formula}b1,…,bn,notc1,…,notcm (collectively) as the body (“not” represents negation as failure); a rule {a mathematical formula}:-b1,…,bn,notc1,…,notcm, with an empty head, is a hard constraint; a choice rule is a rule {a mathematical formula}l{h1,…,hk}u←b1,…,bn,notc1,…,notcm (where l and u are integers) and its head is called an aggregate. A rule R is safe if each variable in R occurs in at least one positive literal in the body of R. In this paper we will use {a mathematical formula}ASPch to denote the set of choice programsP, which are programs composed of safe normal rules, choice rules, and hard constraints. Given a rule R, we will write {a mathematical formula}head(R) to denote the head of R, {a mathematical formula}body(R) to denote the body of R and {a mathematical formula}body+ (resp. {a mathematical formula}body−(R)) to denote the atoms that occur positively (resp. negatively) in the body of R. Given a program P, we will also write {a mathematical formula}Atoms(P) to denote the atoms in P. We will also extend this notation to fragments of a program.
      </paragraph>
      <paragraph>
       The Herbrand Base of any program {a mathematical formula}P∈ASPch, denoted {a mathematical formula}HBP, is the set of variable free (ground) atoms that can be formed from predicates and constants in P. The subsets of {a mathematical formula}HBP are called the (Herbrand) interpretations of P. A ground aggregate {a mathematical formula}l{h1,…,hk}u is satisfied by an interpretation I iff {a mathematical formula}l≤|I∩{h1,…,hk}|≤u.
      </paragraph>
      <paragraph>
       As we restrict our programs to sets of normal rules, (hard) constraints and choice rules, we can use the simplified definitions of the reduct for choice rules presented in [33]. Given a program P and an Herbrand interpretation {a mathematical formula}I⊆HBP, the reduct {a mathematical formula}PI is constructed from {a mathematical formula}ground(P) (the set of ground instances of rules in P) in 4 steps: firstly, remove rules whose bodies contain the negation of an atom in I; secondly, remove all negative literals from the remaining rules; thirdly, replace the head of any hard constraint, or any choice rule whose head is not satisfied by I with ⊥ (where {a mathematical formula}⊥∉HBP); and finally, replace any remaining choice rule {a mathematical formula}l{h1,…,hm}u:-b1,…,bn with the set of rules {a mathematical formula}{hi:-b1,…,bn|hi∈I∩{h1,…,hm}}. Any {a mathematical formula}I⊆HBP is an answer set of P if it is the minimal model of the reduct{a mathematical formula}PI. Throughout the paper we denote the set of answer sets of a program P with {a mathematical formula}AS(P).
      </paragraph>
      <paragraph>
       We say a program Pbravely entails an atom a (written {a mathematical formula}P⊨ba) if there is at least one answer set A of P such that {a mathematical formula}a∈A. Similarly, Pcautiously entailsa (written {a mathematical formula}P⊨ca) if for every answer set A of P, {a mathematical formula}a∈A.
      </paragraph>
      <paragraph>
       Unlike hard constraints in ASP, weak constraints do not affect what is, or is not, an answer set of a program P. Hence the above definitions also apply to programs with weak constraints. Weak constraints create an ordering over {a mathematical formula}AS(P) specifying which answer sets are “preferred” to others. A weak constraint is of the form {a mathematical formula}:∼b1,…,bn,{a mathematical formula}notc1,…,notcm.[w@l,t1,…,tk] where {a mathematical formula}b1,…,bn, {a mathematical formula}c1,…,cm are atoms, w and l are terms specifying the weight and the level, and {a mathematical formula}t1,…,tk are terms. A weak constraint W is safe if every variable in W occurs in at least one positive literal in the body of W. At each priority levell, the aim is to discard any answer set which does not minimise the sum of the weights of the ground weak constraints with level l whose bodies are true. The higher levels are minimised first. The terms {a mathematical formula}t1,…,tk specify which ground weak constraints should be considered unique [34]. For any program P and an interpretation A, {a mathematical formula}weak(P,A) is the set of tuples {a mathematical formula}(w,l,t1,…,tk) for which there is some {a mathematical formula}:∼b1,…,bn,notc1,…,notcm.[w@l,t1,…,tk] in {a mathematical formula}ground(P) such that A satisfies {a mathematical formula}b1,…,bn,notc1,…,notcm. For each level l, the score of the interpretation A is the sum of the weights of tuples with level l, formally {a mathematical formula}PAl=∑(w,l,t1,…,tk)∈weak(P,A)w. For {a mathematical formula}A1,A2∈AS(P), {a mathematical formula}A1dominates{a mathematical formula}A2 (written {a mathematical formula}A1≻PA2) iff ∃l such that {a mathematical formula}PA1l&lt;PA2l and {a mathematical formula}∀m&gt;l,PA1m=PA2m. An answer set {a mathematical formula}A∈AS(P) is optimal if it is not dominated by any {a mathematical formula}A2∈AS(P).
      </paragraph>
      <paragraph label="Example 1">
       Let P be the program {a mathematical formula}{0{p(1),p(2),p(3)}1.}. P has 8 answer sets, which are the various combinations of making each of the three p atoms true or false. Consider the two weak constraints {a mathematical formula}:∼p(X).[1@1] and {a mathematical formula}:∼p(X).[1@1,X]. The first weak constraint states that if any of the p atoms is true then a penalty of one must be paid. This penalty is only paid once, regardless whether 1, 2 or 3 of the p atoms are true. Conversely, the second weak constraint says that a penalty of 1 must be paid for each of the p atoms that is true. In both cases, ∅ is the only optimal answer set; however, in the first case, none of the remaining answer sets dominate each other, whereas in the second case, the answer sets with only one p atom dominate those with 2 p atoms, which in turn each dominate the single answer set with 3 p atoms.
      </paragraph>
      <paragraph>
       Note that the definition of weak constraints used in this paper is in line with the recent ASP standard established in [34]. The syntax of some previous definitions of weak constraints such as [13] do not include the terms {a mathematical formula}t1,…,tk and considered every ground instance of every weak constraint individually. This semantics can be achieved using the notion of weak constraints in [34]. Any weak constraint {a mathematical formula}:∼body.[w:l]{sup:2} can be mapped to the weak constraint {a mathematical formula}:∼body.[w@l,V1,…,Vn], where {a mathematical formula}V1,…,Vn is the set of all variables that occur in body. If there are multiple weak constraints, to exactly preserve the semantics of [13], a unique term must be added to each weak constraint. For example, {a mathematical formula}{:∼p(X).[1:1];:∼q(X).[1:1]} would become {a mathematical formula}{:∼p(X).[1@1,X,1];:∼q(X).[1@1,X,2]}. With this additional term, {a mathematical formula}Weak(P,{p(a),q(a)}) (where P is the program containing the two weak constraints) would be equal to {a mathematical formula}{(1,1,a,1),(1,1,a,2)}, leading to a score of 2 at level 1; without the additional term, {a mathematical formula}Weak(P,{p(a),q(a)}) would equal {a mathematical formula}{(1,1,a)}, leading to a score of 1 at level 1.
      </paragraph>
      <paragraph>
       Unless otherwise stated, when we refer to an ASP program in this paper, we mean a program consisting of a finite set of normal rules, choice rules, hard and weak constraints.
      </paragraph>
      <paragraph>
       We now introduce some extra notation which will be useful in later sections. Given a set of interpretations S, the set {a mathematical formula}ord(P,S) captures the ordering of the interpretations given by the weak constraints in P. It generalises the dominates relation; so it not only includes {a mathematical formula}〈A1,A2,&lt;〉 if {a mathematical formula}A1≻PA2, but it also includes tuples for other binary comparison operators. Formally, {a mathematical formula}〈A1,A2,&lt;〉∈ord(P,S) if {a mathematical formula}A1,A2∈S and {a mathematical formula}A1≻PA2; {a mathematical formula}〈A1,A2,&gt;〉∈ord(P,S) if {a mathematical formula}A1,A2∈S and {a mathematical formula}A2≻PA1; {a mathematical formula}〈A1,A2,≤〉∈ord(P,S) if {a mathematical formula}A1,A2∈S and {a mathematical formula}A2⊁PA1; {a mathematical formula}〈A1,A2,≥〉∈ord(P,S) if {a mathematical formula}A1,A2∈S and {a mathematical formula}A1⊁PA2; {a mathematical formula}〈A1,A2,=〉∈ord(P,S) if {a mathematical formula}A1,A2∈S, {a mathematical formula}A1⊁PA2 and {a mathematical formula}A2⊁PA1; {a mathematical formula}〈A1,A2,≠〉∈ord(P,S) if {a mathematical formula}A1,A2∈S and {a mathematical formula}A1≻PA2 or {a mathematical formula}A2≻PA1. Given an ASP program, we write {a mathematical formula}ord(P) as a shorthand for {a mathematical formula}ord(P,AS(P)). Two ASP programs P and Q are strongly equivalent (written {a mathematical formula}P≡sQ) if for every ASP program R, {a mathematical formula}AS(P∪R)=AS(Q∪R).
      </paragraph>
      <paragraph>
       We now recall the splitting set theorem from [35], which we use in the proofs throughout the paper. This theorem relies on the notions of a splitting set and the partial evaluation of a logic program. Given a program P, a set {a mathematical formula}U⊆HBP is a splitting set of P if and only if for every rule {a mathematical formula}R∈ground(P) such that {a mathematical formula}Atoms(head(R))∩U≠∅, {a mathematical formula}Atoms(R)⊆U. Given a ground rule R and a set of atoms U, we write {a mathematical formula}R\U to denote the rule R with all (positive or negative) occurrences of atoms in U removed from the body of R. Given a program P a splitting set U of P and a set {a mathematical formula}X⊆U, the partial evaluation of P with respect to U and X, written {a mathematical formula}eU(P,X), is the program {a mathematical formula}{R\U|R∈ground(P),Atoms(head(R))∩U=∅,(body+(R)∩U)⊆X,body−(R)∩X=∅}.
      </paragraph>
      <paragraph label="Theorem 1">
       Given any ground ASP program P, and splitting set U of P,{a mathematical formula}AS(P)={X∪Y|X∈AS({R∈P|Atoms(head(R))∩U≠∅}),Y∈AS(eU(P,X))}.
      </paragraph>
      <paragraph>
       The intuition behind the splitting set theorem is that if a set of atoms U is known to split the program P, then we can find the answer sets of the subprogram that defines the atoms in U first. For each of these answer sets X, we can partially evaluate P using X and solve this partially evaluated program for answer sets. The splitting set theorem then guarantees that for each answer set Y of the partially evaluated program, {a mathematical formula}X∪Y is an answer set of P. Furthermore, every answer set of P can be constructed in this way.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Complexity theory
      </section-title>
      <paragraph>
       We assume the reader is familiar with the fundamental concepts of complexity, such as Turing machines and reductions; for a detailed explanation, see [36].
      </paragraph>
      <paragraph>
       Many of the decision problems for ASP are known to be complete for classes in the polynomial hierarchy [37]. The classes of the polynomial hierarchy are defined as follows: P is the class of all problems which can be solved in polynomial time by a Deterministic Turing Machine (DTM); {a mathematical formula}Σ0P=Π0P=Δ0P=P; {a mathematical formula}Δk+1P=PΣkP is the class of all problems which can be solved by a DTM in polynomial time with a {a mathematical formula}ΣkP oracle; {a mathematical formula}Σk+1P=NPΣkP is the class of all problems which can be solved by a non-deterministic Turing Machine in polynomial time with a {a mathematical formula}ΣkP oracle; finally, {a mathematical formula}Πk+1P= co-{a mathematical formula}NPΣkP is the class of all problems whose complement can be solved by a non-deterministic Turing Machine in polynomial time with a {a mathematical formula}ΣkP oracle. {a mathematical formula}Σ1P and {a mathematical formula}Π1P are NP and co-NP (respectively), where NP is the class of problems which can be solved by a non-deterministic Turing machine in polynomial time and co-NP is the class of problems whose complement is an NP problem.
      </paragraph>
      <paragraph>
       DP is the class of problems D that can be mapped to a pair of problems {a mathematical formula}D1 and {a mathematical formula}D2 such that {a mathematical formula}D1∈NP, {a mathematical formula}D2∈ co-NP, and for each instance I of D, I answers “yes” if and only if both of the mapped instances {a mathematical formula}I1 and {a mathematical formula}I2 (of {a mathematical formula}D1 and {a mathematical formula}D2, respectively) answer “yes”. It is well known [36] that the following inclusions hold: {a mathematical formula}P⊆NP⊆DP⊆Δ2P⊆Σ2P and P⊆ co-{a mathematical formula}NP⊆DP⊆Δ2P⊆Π2P.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Learning frameworks
     </section-title>
     <paragraph>
      In this section, we give the definitions of the six learning frameworks we analyse in this paper. The first three – brave induction, cautious induction and induction of stable models – are not our own. We reformulate, but preserve the meaning of, the original definitions for easier comparison with our own.
     </paragraph>
     <paragraph>
      It is common in ILP for a task to have a hypothesis space (the set of all rules which can appear in hypotheses). The purpose of the hypothesis space is two-fold: firstly, it allows the task to be restricted to those solutions which are in some way interesting; secondly, it aids the computational search for inductive solutions. Tasks for brave and cautious induction and for induction of stable models were originally presented with no hypothesis space [28], [30] as they were mainly considered theoretically without the specifications of efficient algorithmic computations. The only publicly available algorithms for brave induction [38], [39] make use of a hypothesis space defined by mode declarations [40]. In this paper, we “upgrade” each of brave induction, cautious induction and induction of stable models with a hypothesis space {a mathematical formula}SM.
     </paragraph>
     <section label="3.1">
      <section-title>
       Notation and terminology
      </section-title>
      <paragraph>
       An ILP learning framework{a mathematical formula}F defines what a learning task of {a mathematical formula}F is and what an inductive solution is for a given learning task of {a mathematical formula}F. For each framework a task is a tuple {a mathematical formula}〈B,SM,E〉, where B is an ASP program called the background knowledge, {a mathematical formula}SM is a set of ASP rules called the hypothesis space, and E is a tuple called the examples. The structure of E depends on the type of ILP framework. Each of the papers [28], [30], [31] and [23] presented learning frameworks with different languages for B and {a mathematical formula}SM; for example, induction of stable models was presented only for normal logic programs. It would be unfair to say that induction from stable models is not general enough to learn programs with choice rules, simply because they were not considered in the original paper (in fact, induction from stable models is general enough to learn some programs with choice rules). For a fair comparison we therefore assume in this paper that every learning framework has a background knowledge B and hypothesis space {a mathematical formula}SM that consist of normal rules, choice rules, hard constraints and weak constraints.
      </paragraph>
      <paragraph>
       Given a framework {a mathematical formula}F and a learning task {a mathematical formula}TF=〈B,SM,E〉 of {a mathematical formula}F, a hypothesis is any subset of the hypothesis space {a mathematical formula}SM. In Section 5, we consider tasks with unrestricted hypothesis spaces (written {a mathematical formula}〈B,E〉), in which case any ASP program can be called a hypothesis. An inductive solution is a hypothesis that, together with the background knowledge B, satisfies some conditions on E (given by the particular learning framework {a mathematical formula}F). We write {a mathematical formula}ILPF(TF) to denote the set of all inductive solutions of {a mathematical formula}TF. Throughout the paper, we use the term covers to apply to any kind of example: i.e. given a {a mathematical formula}F task {a mathematical formula}〈B,SM,E〉, we say that a hypothesis H covers an example e (any element of any component of E), if it meets the particular conditions that the framework {a mathematical formula}F puts on H and e.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Framework definitions
      </section-title>
      <paragraph>
       Brave induction ({a mathematical formula}ILPb), first presented in [28], defines an inductive task in which all examples are ground atoms that should be covered in at least one answer set, i.e. entailed under brave entailment in ASP. The original definition did not consider atoms which should not be present in an answer set, namely negative examples. The two publicly available algorithms that realise brave induction, on the other hand, do allow for negative examples. We therefore upgrade the definition in this paper to allow negative examples{sup:3} as follows.
      </paragraph>
      <paragraph label="Definition 1">
       A brave induction ({a mathematical formula}ILPb) task {a mathematical formula}Tb is a tuple {a mathematical formula}〈B,SM,〈E+,E−〉〉, where B is an ASP program called the background knowledge, {a mathematical formula}SM is the hypothesis space and {a mathematical formula}E+ and {a mathematical formula}E− are sets of ground atoms called the positive and negative examples (respectively). A hypothesis {a mathematical formula}H⊆SM is said to be an inductive solution of {a mathematical formula}Tb (written {a mathematical formula}H∈ILPb(Tb)) if and only if {a mathematical formula}∃A∈AS(B∪H) such that {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅.
      </paragraph>
      <paragraph label="Definition 2">
       Cautious induction ({a mathematical formula}ILPc) was also first presented in [28]. It defines an inductive task where all of the examples should be covered in every answer set (i.e. entailed under cautious entailment in ASP) and that {a mathematical formula}B∪H should be satisfiable (have at least one answer set). Similarly to brave induction, the original definition did not consider negative examples, but in Definition 2 we upgrade the framework to include negative examples. A cautious induction ({a mathematical formula}ILPc) task {a mathematical formula}Tc is a tuple {a mathematical formula}〈B,SM,〈E+,E−〉〉, where B is an ASP program called the background knowledge, {a mathematical formula}SM is the hypothesis space and {a mathematical formula}E+ and {a mathematical formula}E− are sets of ground atoms called the positive and negative examples (respectively). A hypothesis {a mathematical formula}H⊆SM is said to be an inductive solution of {a mathematical formula}Tc (written {a mathematical formula}H∈ILPc(Tc)) if and only if {a mathematical formula}AS(B∪H)≠∅ and {a mathematical formula}∀A∈AS(B∪H), {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅.
      </paragraph>
      <paragraph>
       Brave induction alone can only reason about what should be true (or false) in a single answer set of {a mathematical formula}B∪H. It cannot specify other brave tasks such as enforcing that two atoms are both bravely entailed, but not necessarily in the same answer set. Induction of stable models[30] ({a mathematical formula}ILPsm), on the other hand, generalises the notion of brave induction as shown in Definition 4. The following terminology is first introduced.
      </paragraph>
      <paragraph label="Definition 3">
       A partial interpretatione is a pair of sets of ground atoms {a mathematical formula}〈einc,eexc〉. An interpretation I is said to extende iff {a mathematical formula}einc⊆I and {a mathematical formula}eexc∩I=∅.
      </paragraph>
      <paragraph label="Definition 4">
       An induction of stable models ({a mathematical formula}ILPsm) task {a mathematical formula}Tsm is a tuple {a mathematical formula}〈B,SM,〈E〉〉, where B is an ASP program called the background knowledge, {a mathematical formula}SM is the hypothesis space and E is a set of partial interpretations called the examples. A hypothesis H is said to be an inductive solution of {a mathematical formula}Tsm (written {a mathematical formula}H∈ILPsm(Tsm)) if and only if {a mathematical formula}H⊆SM and {a mathematical formula}∀e∈E, {a mathematical formula}∃A∈AS(B∪H) such that A extends e.
      </paragraph>
      <paragraph>
       Note that a brave induction task can be thought of as a special case of induction of stable models, with exactly one (partial interpretation) example.
      </paragraph>
      <paragraph>
       We now consider the Learning from Answer Sets framework introduced in [31]. This is the first framework capable of unifying the concepts of brave and cautious induction. The idea is to use examples of partial interpretations which should or should not be extended by answer sets of {a mathematical formula}B∪H.
      </paragraph>
      <paragraph label="Definition 5">
       A Learning from Answer Sets task is a tuple {a mathematical formula}T=〈B,SM,〈E+,E−〉〉 where B is an ASP program called the background knowledge, {a mathematical formula}SM is the hypothesis space and {a mathematical formula}E+ and {a mathematical formula}E− are sets of partial interpretations called, respectively, the positive and negative examples. A hypothesis {a mathematical formula}H⊆SM is an inductive solution of T (written {a mathematical formula}H∈ILPLAS(T)) if and only if:
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}∀e+∈E+{a mathematical formula}∃A∈AS(B∪H) such that A extends {a mathematical formula}e+
       </list-item>
       <list-item label="2.">
        {a mathematical formula}∀e−∈E−{a mathematical formula}∄A∈AS(B∪H) such that A extends {a mathematical formula}e−
       </list-item>
      </list>
      <paragraph>
       Note that this definition combines properties of both the brave and cautious semantics: the positive examples must each be bravely entailed, whereas the negation of each negative example must be cautiously entailed.
      </paragraph>
      <paragraph label="Example 2">
       Consider an {a mathematical formula}ILPLAS learning task whose background knowledge B contains definitions of the structure of a 4x4 Sudoku board; i.e. definitions of cell, same_row, same_col and same_block (where same_row, same_col and same_block are true only for two different cells in the same row, column or block).{a mathematical formula}For the purposes of this example, we will consider only a small hypothesis space {a mathematical formula}SM but in practice this would be much larger.{sup:4}{a mathematical formula}We need to be able to say that there should be at least one answer set that assigns a value to a cell, or otherwise the empty hypothesis would be sufficient. This is captured by our positive example which causes at least one of the choice rules to be part of a solution in order to be covered. Our first three negative examples require the three constraints to be also included in a solution. Without each one of these negative examples, at least one constraint could be left out of the solution. The fourth negative example means that the upper bound of the counting aggregate in the choice rule must be 1, as otherwise there would be answer sets in which cell {a mathematical formula}(1,1) was assigned to both 1 and 2. Finally, the fifth negative example forces that the lower bound of the choice rule should be 1 as otherwise there would be answer sets in which {a mathematical formula}(1,1) was not assigned to any of the values between 1 and 4. Hence, one possible inductive solution is:{a mathematical formula}The only other solutions within the hypothesis space {a mathematical formula}SM are those that contain H and also extra redundant choice rules, such as {a mathematical formula}0{value(C,1),value(C,2),{a mathematical formula}value(C,3),value(C,4)}1:-cell(C).Note that we need {a mathematical formula}ILPLAS's combination of brave and cautious induction to separate the correct hypothesis from the incorrect hypotheses.
      </paragraph>
      <list>
       <list-item label="•">
        If we instead use brave induction, whichever examples we use, if H is a solution, then any of the choice rules on their own is also a solution. For instance, consider the hypothesis {a mathematical formula}H′, containing only the choice rule {a mathematical formula}0{value(C,1),value(C,2),{a mathematical formula}value(C,3),value(C,4)}1:-cell(C). For any examples {a mathematical formula}〈E+,E−〉 such that {a mathematical formula}H∈ILPb(〈B,〈E+,E−〉〉), there must be an answer set A of {a mathematical formula}B∪H such that {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅. As {a mathematical formula}AS(B∪H)⊂AS(B∪H′), any such answer set is also an answer set of {a mathematical formula}B∪H′; and hence, {a mathematical formula}H′ is also a solution of the task.
       </list-item>
       <list-item label="•">
        If we use cautious induction, we have to give examples which are either true in every answer set, or false in every answer set. Therefore, we could not give any examples about the value predicate – for each atom {a mathematical formula}value(x,y) (where x and y range from 1 to 4), there is at least one answer set of {a mathematical formula}B∪H that contains {a mathematical formula}value(x,y) and at least one that does not; this means that if {a mathematical formula}value(x,y) is given as either a positive or negative example, H will not be a solution of the task.This means that for any {a mathematical formula}ILPc task {a mathematical formula}Tc such that H is a solution, any subset of the hypothesis space {a mathematical formula}SM is also a solution of {a mathematical formula}Tc.
       </list-item>
      </list>
      <paragraph>
       Note that none of the learning frameworks we have considered so far ({a mathematical formula}ILPLAS included) can incentivise learning a weak constraint. This is because the frameworks only have examples of what should be in some, all or none of the answer sets of {a mathematical formula}B∪H. Any solution H containing a weak constraint W will have the same answer sets with W removed and {a mathematical formula}H\{W} would therefore be a shorter (more optimal{sup:5}) solution. The notion of ordering examples is needed to incentivise learning weak constraints, in order to enforce which answer sets of {a mathematical formula}B∪H should dominate other answer sets.
      </paragraph>
      <paragraph label="Definition 6">
       An ordering example is a tuple {a mathematical formula}o=〈e1,e2,op〉 where {a mathematical formula}e1 and {a mathematical formula}e2 are partial interpretations and op is a binary comparison operator (&lt;, &gt;, =, ≤, ≥ or ≠). An ASP program Pbravely respectso iff {a mathematical formula}∃A1,A2∈AS(P) such that all of the following conditions hold: (i) {a mathematical formula}A1 extends {a mathematical formula}e1; (ii) {a mathematical formula}A2 extends {a mathematical formula}e2; and (iii) {a mathematical formula}〈A1,A2,op〉∈ord(P). Pcautiously respectso iff {a mathematical formula}∄A1,A2∈AS(P) such that all of the following conditions hold: (i) {a mathematical formula}A1 extends {a mathematical formula}e1; (ii) {a mathematical formula}A2 extends {a mathematical formula}e2; and (iii) {a mathematical formula}〈A1,A2,op〉∉ord(P).
      </paragraph>
      <paragraph>
       Note that Definition 6 generalises our initial definition of ordering examples given in [23], where ordering examples had only the operator &lt;, and we could not express examples of pairs of answer sets which were equally preferred. In Section 5 we show that this extension allows us to learn a wider class of programs. We now define the notion of Learning from Ordered Answer Sets ({a mathematical formula}ILPLOAS).
      </paragraph>
      <paragraph label="Definition 7">
       A Learning from Ordered Answer Sets task is a tuple {a mathematical formula}T=〈B,SM,〈E+,E−,Ob,Oc〉〉 where B is an ASP program, called the background knowledge, {a mathematical formula}SM is the hypothesis space, {a mathematical formula}E+ and {a mathematical formula}E− are sets of partial interpretations called, respectively, positive and negative examples, and {a mathematical formula}Ob and {a mathematical formula}Oc are sets of ordering examples over {a mathematical formula}E+ called brave and cautious orderings. A hypothesis {a mathematical formula}H⊆SM is an inductive solution of T (written {a mathematical formula}H∈ILPLOAS(T)) if and only if:
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}H∈ILPLAS(〈B,SM,〈E+,E−〉〉)
       </list-item>
       <list-item label="2.">
        {a mathematical formula}∀o∈Ob{a mathematical formula}B∪H bravely respects o
       </list-item>
       <list-item label="3.">
        {a mathematical formula}∀o∈Oc{a mathematical formula}B∪H cautiously respects o
       </list-item>
      </list>
      <paragraph>
       Note that the orderings are only over positive examples. We chose to make this restriction as there does not appear to be any scenario where a hypothesis would need to respect orderings which are not extended by any pair of answer sets of {a mathematical formula}B∪H.
      </paragraph>
      <paragraph label="Example 3">
       Consider the {a mathematical formula}ILPLOAS task {a mathematical formula}T=〈B,SM,〈E+,E−,Ob,Oc〉〉 where the individual components of the task are as follows:
       <list>
        {a mathematical formula}B={0{p,q}2.}{a mathematical formula}SM is unrestricted (i.e. {a mathematical formula}SM is the set of all normal rules, choice rules and hard and weak constraints).{a mathematical formula}E+={e1+,e2+} where {a mathematical formula}e1+=〈{p},∅〉 and {a mathematical formula}e2+=〈∅,{p}〉{a mathematical formula}E−=∅{a mathematical formula}Ob={〈e1+,e2+,&lt;〉}{a mathematical formula}Oc={〈e1+,e1+,=〉}The positive examples of this task are already satisfied by the background knowledge, which has the answer sets ∅,
       </list>
       <paragraph>
        {a mathematical formula}{p}, {a mathematical formula}{q} and {a mathematical formula}{p,q}. As there are no negative examples, it remains to find a set of weak constraints such that there is at least one answer set which contains p which is preferred to at least one answer set which does not contain p and all answer sets which contain p are equally optimal.One such hypothesis is the single weak constraint {a mathematical formula}:∼notp.[1@1].
       </paragraph>
      </paragraph>
      <paragraph>
       The frameworks discussed so far have examples which can only express the properties of a learned hypothesis H together with a fixed background knowledge B. These properties are on the answer sets of {a mathematical formula}B∪H (and the ordering of these answer sets). In [24], we presented a new learning framework that uses context-dependent examples. Each example comes with its own context, which is an {a mathematical formula}ASPch program C. Examples then express properties of {a mathematical formula}B∪H∪C, meaning that by using multiple examples (with different contexts), we can express that {a mathematical formula}B∪H∪C1 should have some properties and that {a mathematical formula}B∪H∪C2 should have different properties.
      </paragraph>
      <paragraph label="Definition 8">
       A context-dependent partial interpretation (CDPI) is a pair {a mathematical formula}〈e,C〉, where e is a partial interpretation and C is an {a mathematical formula}ASPch program, called a context. A context-dependent ordering example (CDOE) o is a tuple {a mathematical formula}〈〈e1,C1〉,〈e2,C2〉,op〉, where the first two elements are CDPIs and op is a binary comparison operator (&lt;, &gt;, =, ≤, ≥ or ≠). P is said to bravely respecto if {a mathematical formula}∃A1∈AS(P∪C1),∃A2∈AS(P∪C2) such that {a mathematical formula}A1 extends {a mathematical formula}e1, {a mathematical formula}A2 extends {a mathematical formula}e2 and {a mathematical formula}〈A1,A2,op〉∈ord(P,AS(P∪C1)∪AS(P∪C2)). A program P is said to cautiously respecto if {a mathematical formula}∀A1∈AS(P∪C1),∀A2∈AS(P∪C2) such that {a mathematical formula}A1 extends {a mathematical formula}e1 and {a mathematical formula}A2 extends {a mathematical formula}e2, {a mathematical formula}〈A1,A2,op〉∈ord(P,AS(P∪C1)∪AS(P∪C2)).
      </paragraph>
      <paragraph>
       When examples are given with empty contexts, they are equivalent to examples in {a mathematical formula}ILPLOAS. Note also that contexts do not contain weak constraints. In fact, the operator {a mathematical formula}≻P defines the ordering over two answer sets based on the weak constraints in one program P. So, given a CDOE {a mathematical formula}〈〈e1,C1〉,〈e2,C2〉〉 such that {a mathematical formula}C1 and {a mathematical formula}C2 contain different weak constraints, it is not clear which program to consider for computing the ordering of answer sets – i.e. whether they should be checked against the weak constraints in P, {a mathematical formula}P∪C1, {a mathematical formula}P∪C2 or {a mathematical formula}P∪C1∪C2.
      </paragraph>
      <paragraph>
       We now present a formal definition of the {a mathematical formula}ILPLOAScontext framework.
      </paragraph>
      <paragraph label="Definition 9">
       A Context-dependent Learning from Ordered Answer Sets ({a mathematical formula}ILPLOAScontext) task is a tuple {a mathematical formula}T=〈B,SM,〈E+,E−,Ob,Oc〉〉 where B is an ASP program called the background knowledge, {a mathematical formula}SM is the set of rules allowed in the hypotheses (the hypothesis space), {a mathematical formula}E+ and {a mathematical formula}E− are finite sets of CDPIs called, respectively, positive and negative examples, and {a mathematical formula}Ob and {a mathematical formula}Oc are finite sets of CDOEs over {a mathematical formula}E+ called, respectively, brave and cautious context-dependent orderings. A hypothesis {a mathematical formula}H⊆SM is an inductive solution of T (written {a mathematical formula}H∈ILPLOAScontext(T)) if and only if:
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}∀〈e+,C〉∈E+, {a mathematical formula}∃A∈AS(B∪C∪H) st A extends {a mathematical formula}e+
       </list-item>
       <list-item label="2.">
        {a mathematical formula}∀〈e−,C〉∈E−, {a mathematical formula}∄A∈AS(B∪C∪H) st A extends {a mathematical formula}e−
       </list-item>
       <list-item label="3.">
        {a mathematical formula}∀o∈Ob, {a mathematical formula}B∪H bravely respects o
       </list-item>
       <list-item label="4.">
        {a mathematical formula}∀o∈Oc, {a mathematical formula}B∪H cautiously respects o
       </list-item>
      </list>
      <paragraph>
       In [24], we showed that context-dependent examples could be used to simplify the encoding of certain tasks, by splitting the background knowledge into contexts that were only relevant to particular examples. Although any {a mathematical formula}ILPLOAScontext task can be transformed into an {a mathematical formula}ILPLOAS task, in general this requires parts of the examples to be encoded in the background knowledge. Example 4 shows such a transformation.
      </paragraph>
      <paragraph label="Example 4">
       Consider a simple scenario where we have a machine that has a single configuration parameter a, which is allowed to take any natural number as its value. A user is allowed to input another natural number b, and if {a mathematical formula}a&gt;b, the machine should beep.Two example scenarios could be encoded as the context-dependent positive examples {a mathematical formula}〈〈{beep},∅〉,{value(a,3).value(b,2).}〉, and {a mathematical formula}〈〈∅,{beep}〉,{value(a,4).value(b,20).}〉. A task containing these examples and an empty background knowledge requires an inductive solution that when combined with the context of the first example would have at least one answer set containing beep, and when combined with the second example would have at least one answer set not containing beep. If we were expressing the same task in {a mathematical formula}ILPLOAS the above two scenarios would be represented considering the background knowledge:{a mathematical formula}The context-dependent examples could then be mapped to the non context-dependent examples {a mathematical formula}〈{value(a,3),value(b,2),beep},∅〉 and {a mathematical formula}〈{value(a,4),value(b,20)},{beep}〉. In fact, in [24], we show that there is a general mapping from {a mathematical formula}ILPLOAScontext to {a mathematical formula}ILPLOAS. This mapping, just as the simplified mapping here, depends on encoding the examples in the background knowledge, which abuses the purpose of the background knowledge. The contexts in context-dependent examples allow us instead to separate information that is truly background knowledge, which applies in all scenarios, from information that is part of a particular example.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Systems for learning under the answer set semantics
      </section-title>
      <paragraph>
       The current publicly available systems for ILP can be categorised according to the 6 frameworks presented in this section (Table 1). It should be noted that although there are no systems which directly solve {a mathematical formula}ILPc or {a mathematical formula}ILPsm tasks, both can be simply translated into {a mathematical formula}ILPLAS tasks, and can therefore be solved by the ILASP system.
      </paragraph>
      <paragraph>
       The ILED [21] system is an incremental extension of XHAIL, that is specifically targeted at learning Event Calculus [20] theories. The underlying mechanism is based on brave induction, but each of its examples are in terms of two sequential time points.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Complexity
     </section-title>
     <paragraph>
      In this section, we discuss the complexity of each of the learning frameworks presented in Section 3 with respect to two decision problems: verification, deciding whether a given hypothesis H is an inductive solution of a task T; and satisfiability, deciding whether a learning task T has any inductive solutions. A summary of the results is shown in Table 2. To aid readability, the proofs of the propositions stated in this section are given in appendix. All complexities discussed in this section are for propositional versions of the frameworks (both the background knowledge and hypothesis space of each learning task is ground).
     </paragraph>
     <section label="4.1">
      <section-title>
       Learning from answer sets with stratified summing aggregates
      </section-title>
      <paragraph>
       As there are existing results on the complexity of solving aggregate stratified programs, it is useful to introduce a new learning framework {a mathematical formula}ILPLASs, which is a generalization of {a mathematical formula}ILPLAS, that allows summing aggregates in the bodies of rules, as long as they are stratified. The existing results on the complexity of these programs then allow us to prove the complexity of {a mathematical formula}ILPLASs. Hence, as we can show that {a mathematical formula}ILPLOAS reduces to {a mathematical formula}ILPLASs, this is helpful in proving the complexity of {a mathematical formula}ILPLOAS.
      </paragraph>
      <paragraph>
       A summing aggregate s is of the form {a mathematical formula}l#sum{a1=w1,…,an=wn}u, where l, u and {a mathematical formula}w1,…,wn are integers and {a mathematical formula}a1,…,an are atoms. s is satisfied by an interpretation I if and only if {a mathematical formula}l≤(∑wi∈WSwi)≤u, where WS is the set {a mathematical formula}{wi|i∈[0..n],ai∈I}. We now recall the definition of aggregate stratification from [44]. We slightly simplify the definition by considering only propositional programs without disjunction.
      </paragraph>
      <paragraph label="Definition 10">
       A propositional logic program P, in which aggregates occur only in bodies of rules, is stratified on an aggregateagg if there is a level mapping ‖ ‖ from {a mathematical formula}Atoms(P) to ordinals, such that for each rule {a mathematical formula}R∈P, the following holds:
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}∀b∈Atoms(body(R)):‖b‖≤‖head(R)‖
       </list-item>
       <list-item label="2.">
        If {a mathematical formula}agg∈body(R), then {a mathematical formula}∀b∈Atoms(agg):‖b‖&lt;‖head(R)‖
       </list-item>
      </list>
      <paragraph>
       The intuition is that aggregate stratification forbids recursion through aggregates. In general aggregate stratified programs have a lower complexity than non-aggregate stratified programs. Aggregate stratification has nothing to do with negation as failure, and therefore, whether a program is aggregate stratified is unrelated to whether it is stratified in the usual sense. Note that constraints and choice rules can be added in to any aggregate stratified program without breaking stratification so long as no atoms in the head of the choice rule are on a lower level than any atom in the body. This is illustrated by the following example.
      </paragraph>
      <paragraph label="Example 5">
       Any constraint {a mathematical formula}:-b1,…,bn,notc1,…,notcm can be rewritten as {a mathematical formula}s:-b1,…,bn,notc1,…,notcm,nots where s is a new atom. s can then be mapped to a higher level than any other atom.A choice rule {a mathematical formula}l{h1,…,ho}u:-b1,…,bn,notc1,…,notcm can be rewritten as:{a mathematical formula} where {a mathematical formula}h1′,…,ho′,s,s′ are all new atoms. s and {a mathematical formula}s′ can both be given a new highest level and each {a mathematical formula}hi′ can be given the same level as {a mathematical formula}hi (if they did not occur in the previous program then they should be given a new level one below s and {a mathematical formula}s′). Provided the previous program was aggregate stratified, then this new one is too. To avoid constantly using this mapping, we will refer to programs with choice rules and constraints as also being aggregate stratified.
      </paragraph>
      <paragraph label="Lemma 1">
       [44]Deciding whether an aggregate stratified propositional program without disjunction cautiously entails an atom is co-NP-complete.
      </paragraph>
      <paragraph label="Corollary 1">
       Deciding whether an aggregate stratified propositional program without disjunction bravely entails an atom is NP-complete.
      </paragraph>
      <paragraph label="Proof">
       We first show that deciding whether an aggregate stratified propositional program without disjunction bravely entails an atom is in NP. We do this by showing that there is a polynomial reduction from this problem to the complement of the problem in Lemma 1 (which by definition of co-NP must be in NP). The complement of the problem in Lemma 1 is deciding whether a non disjunctive aggregate stratified program does not cautiously entail an atom. Take any non-disjunctive aggregate stratified program P and any atom a and let neg_a be an atom that does not occur in P. {a mathematical formula}P⊨ba if and only if {a mathematical formula}P∪{neg_a:-nota.}⊭cneg_a. So the decision problem is in NP.It remains to show that deciding whether an aggregate stratified propositional program without disjunction bravely entails an atom is NP-hard. We do this by showing that any problem in NP can be reduced in polynomial time to deciding the satisfiability of an aggregate stratified propositional program without disjunction.Consider an arbitrary NP problem D. The complement of D, {a mathematical formula}D¯, must be in co-NP (by definition of co-NP). Hence, by Lemma 1, there is a polynomial reduction from {a mathematical formula}D¯ to deciding whether an aggregate stratified propositional program without disjunction cautiously entails an atom. We define the polynomial reduction from D to deciding whether an aggregate stratified propositional program without disjunction bravely entails an atom as follows: for any instance I of D, let P and a be the program and atom given by the polynomial reduction from the complement of I to deciding cautious entailment; define {a mathematical formula}P′ as the program {a mathematical formula}P∪{neg_a:-nota.} (where neg_a is a new atom). I returns true if and only if {a mathematical formula}P⊭ca if and only if {a mathematical formula}P′⊨bneg_a. Hence, as {a mathematical formula}P′ is still aggregate stratified (the new atom neg_a can be put in the top strata), this is a polynomial reduction from D to deciding whether an aggregate stratified propositional program without disjunction bravely entails an atom. Hence, the decision problem is NP-hard. □
      </paragraph>
      <paragraph>
       We can now introduce our extra learning task, Learning from Answer Sets with Stratified Aggregates ({a mathematical formula}ILPLASs). It is the same as Learning from Answer Sets, except for allowing summing aggregates in the bodies of the rules in B and {a mathematical formula}SM, as long as {a mathematical formula}B∪SM is aggregate stratified. Note that the condition of {a mathematical formula}B∪SM being aggregate stratified implies that for any hypothesis {a mathematical formula}H⊆SM, {a mathematical formula}B∪H is aggregate stratified.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Relationships between the learning tasks
      </section-title>
      <paragraph>
       In this section we prove for both decision problems that {a mathematical formula}ILPb and {a mathematical formula}ILPsm both reduce to each other polynomially. We also show that for both decision problems there is a chain of polynomial reductions from {a mathematical formula}ILPc to {a mathematical formula}ILPLAS to {a mathematical formula}ILPLOAScontext to {a mathematical formula}ILPLOAS to {a mathematical formula}ILPLASs. This chain of reductions is then used in proving that all four tasks share the same complexity for both decision problems. By proving that {a mathematical formula}ILPc is {a mathematical formula}O-hard and {a mathematical formula}ILPLASs is in {a mathematical formula}O for some complexity class {a mathematical formula}O, we prove that all four tasks are {a mathematical formula}O-complete. Similarly as {a mathematical formula}ILPb and {a mathematical formula}ILPsm both reduce polynomially to each other for both decision problems, if for one of the problems {a mathematical formula}ILPb is {a mathematical formula}O-complete for some class then so is {a mathematical formula}ILPsm. The chains of reductions are shown in Fig. 1.
      </paragraph>
      <paragraph>
       Proposition 1 shows that the complexity of {a mathematical formula}ILPb and {a mathematical formula}ILPsm coincide for both decision problems.
      </paragraph>
      <paragraph label="Proposition 1">
       <list>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPbreduces polynomially to the corresponding{a mathematical formula}ILPsmdecision problem.
        </list-item>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPsmreduces polynomially to the corresponding{a mathematical formula}ILPbdecision problem.
        </list-item>
       </list>
      </paragraph>
      <paragraph>
       Proposition 2 shows that there is a chain of polynomial reductions from {a mathematical formula}ILPc to {a mathematical formula}ILPLAS to {a mathematical formula}ILPLOAS to {a mathematical formula}ILPLOAScontext to {a mathematical formula}ILPLASs for both decision problems.
      </paragraph>
      <paragraph label="Proposition 2">
       <list>
        <list-item label="1.">
         Deciding both verification and satisfiability for{a mathematical formula}ILPcreduces polynomially to the corresponding{a mathematical formula}ILPLASdecision problem.
        </list-item>
        <list-item label="2.">
         Deciding both verification and satisfiability for{a mathematical formula}ILPLASreduces polynomially to the corresponding{a mathematical formula}ILPLOAScontextdecision problem.
        </list-item>
        <list-item label="3.">
         Deciding both verification and satisfiability for{a mathematical formula}ILPLOAScontextreduces polynomially to the corresponding{a mathematical formula}ILPLOASdecision problem.
        </list-item>
        <list-item label="4.">
         Deciding both verification and satisfiability for{a mathematical formula}ILPLOASreduces polynomially to the corresponding{a mathematical formula}ILPLASsdecision problem.
        </list-item>
       </list>
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Complexity of deciding verification and satisfiability for each framework
      </section-title>
      <paragraph>
       For each of the learning frameworks, we prove the complexity of deciding verification and satisfiability. We start with the {a mathematical formula}ILPb and {a mathematical formula}ILPsm frameworks, for which both decision problems are NP-complete.
      </paragraph>
      <paragraph label="Proposition 3">
       Verifying whether a given H is an inductive solution of a general{a mathematical formula}ILPbtask is NP-complete.
      </paragraph>
      <paragraph label="Corollary 2">
       Verifying whether a given H is an inductive solution of a general{a mathematical formula}ILPsmtask is NP-complete.
      </paragraph>
      <paragraph label="Proposition 4">
       Deciding the satisfiability of a general{a mathematical formula}ILPbtask is NP-complete.
      </paragraph>
      <paragraph label="Corollary 3">
       Deciding the satisfiability of a general{a mathematical formula}ILPsmtask is NP-complete.
      </paragraph>
      <paragraph>
       We have now proven the complexity of deciding verification and satisfiability for {a mathematical formula}ILPb and {a mathematical formula}ILPsm, proving the corresponding entries in Table 2. It remains to show the complexities for {a mathematical formula}ILPc, {a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext.
      </paragraph>
      <paragraph>
       As we have shown that {a mathematical formula}ILPc reduces to {a mathematical formula}ILPLAS which, in turn, reduces to {a mathematical formula}ILPLOAS, which reduces to {a mathematical formula}ILPLOAScontext and that {a mathematical formula}ILPLOAScontext reduces to {a mathematical formula}ILPLASs (all in polynomial time), to prove the complexity of verifying a hypothesis for each framework, it suffices to show that {a mathematical formula}ILPc is DP-hard (thus also proving the hardness for each of the other frameworks) and that {a mathematical formula}ILPLASs is a member of DP (thus proving membership for the other frameworks). This shows that each framework is both a member of DP and also DP-hard, and therefore must be DP-complete.
      </paragraph>
      <paragraph label="Proposition 5">
       Deciding verification for{a mathematical formula}ILPLASsis a member of DP.
      </paragraph>
      <paragraph label="Proposition 6">
       Deciding verification for{a mathematical formula}ILPcis DP-hard.
      </paragraph>
      <paragraph>
       We can now prove the complexity of deciding verification for {a mathematical formula}ILPc, {a mathematical formula}ILPLAS and {a mathematical formula}ILPLOAS. This proves the corresponding entries in Table 2.
      </paragraph>
      <paragraph label="Theorem 2">
       Deciding whether a given H is a solution of any{a mathematical formula}ILPc,{a mathematical formula}ILPLAS,{a mathematical formula}ILPLOASor{a mathematical formula}ILPLOAScontexttask is DP-complete in each case.
      </paragraph>
      <paragraph label="Proof">
       By Proposition 6, deciding the verification for {a mathematical formula}ILPc is DP-hard. By Proposition 2, deciding the verification for {a mathematical formula}ILPc reduces to deciding verification for {a mathematical formula}ILPLAS which, in turn, reduces to deciding verification for {a mathematical formula}ILPLOAScontext, which reduces to deciding satisfiability for {a mathematical formula}ILPLOAS, which again reduces to deciding verification for {a mathematical formula}ILPLASs and by Proposition 5, deciding verification for {a mathematical formula}ILPLASs is a member of DP. Deciding verification for each of these learning frameworks must therefore be both a member of DP and must be DP-hard. Hence, deciding verification for each framework is DP-complete. □
      </paragraph>
      <paragraph>
       Similarly, to show that deciding satisfiability is {a mathematical formula}Σ2P-complete for each framework, we only need to show that {a mathematical formula}ILPLASs is a member of {a mathematical formula}Σ2P and {a mathematical formula}ILPc is {a mathematical formula}Σ2P-hard.
      </paragraph>
      <paragraph label="Proposition 7">
       Deciding satisfiability for{a mathematical formula}ILPLASsis in{a mathematical formula}Σ2P.
      </paragraph>
      <paragraph label="Proposition 8">
       Deciding satisfiability for{a mathematical formula}ILPcis{a mathematical formula}Σ2P-hard.
      </paragraph>
      <paragraph>
       We can now prove the complexity of deciding satisfiability for {a mathematical formula}ILPc, {a mathematical formula}ILPLAS and {a mathematical formula}ILPLOAS. This proves the remaining entries in Table 2.
      </paragraph>
      <paragraph label="Theorem 3">
       Deciding the satisfiability of any{a mathematical formula}ILPc,{a mathematical formula}ILPLAS,{a mathematical formula}ILPLOASor{a mathematical formula}ILPLOAScontexttask is{a mathematical formula}Σ2P-complete in each case.
      </paragraph>
      <paragraph label="Proof">
       (similar to the proof of Theorem 2) By Proposition 8, deciding satisfiability for {a mathematical formula}ILPc is {a mathematical formula}Σ2P-hard. By Proposition 2, deciding satisfiability for {a mathematical formula}ILPc reduces to deciding satisfiability for {a mathematical formula}ILPLAS which, in turn, reduces to deciding satisfiability for {a mathematical formula}ILPLOAScontext, which reduces to deciding satisfiability for {a mathematical formula}ILPLOAS, which again reduces to deciding satisfiability of {a mathematical formula}ILPLASs. By Proposition 7, deciding satisfiability for {a mathematical formula}ILPLASs is in {a mathematical formula}Σ2P. Deciding satisfiability for each of these learning frameworks is therefore both a member of {a mathematical formula}Σ2P and is {a mathematical formula}Σ2P-hard. Hence, deciding satisfiability for each framework is {a mathematical formula}Σ2P-complete. □
      </paragraph>
     </section>
     <section label="4.4">
      <section-title>
       Considering noisy examples
      </section-title>
      <paragraph>
       Although the frameworks considered in this paper were originally presented under the assumption that all examples were perfectly labeled (i.e. there is no noise in the examples), some of the systems for solving these tasks do consider noise when searching for an optimal solution.
      </paragraph>
      <paragraph>
       A common approach, used by both XHAIL [42] and ILASP [32] is to penalise hypotheses for the examples they do not cover. In ILASP, some examples can be labeled together with a penalty that must be paid if a hypothesis does not cover the example. Any example that is not labeled with a penalty must be covered by any inductive solution. Given a set of examples E, the score of a hypothesis H is said to be {a mathematical formula}|H|+p(H,E), where {a mathematical formula}|H| is the length of the hypothesis, and {a mathematical formula}p(H,E) is the sum of the weights of all examples that are not covered by H. As a hypothesis is an inductive solution if and only if it covers all the examples that are labeled with a penalty, that were not labeled with an explicit penalty, the two decision problems of verification and satisfiability can be reduced to the corresponding decisions for non-noisy tasks (by simply removing any example with a penalty).
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Generality
     </section-title>
     <paragraph>
      In this section, we present a new notion of the generality of a learning framework. The aim is to get a sense of which class of ASP programs a framework is capable of learning, if given sufficient examples. Language biases tend, in general, to impose their own restrictions on the classes of program that can be learned. They are primarily used to aid the performance of the computation, rather than to capture intrinsic properties of a learning framework. In this section we will therefore consider learning tasks with unrestricted hypothesis spaces: hypotheses can be constructed from any set of (first order) normal rules, choice rules and hard and weak constraints. We assume each learning framework {a mathematical formula}F to have a task consisting of a pair {a mathematical formula}〈B,EF〉, where B is the (first order ASP) background knowledge and {a mathematical formula}EF is a tuple consisting of the examples for this framework; for example {a mathematical formula}ELAS=〈E+,E−〉 where {a mathematical formula}E+ and {a mathematical formula}E− are sets of partial interpretations.
     </paragraph>
     <paragraph label="Example 6">
      Allowing an unrestricted hypothesis space raises the question of whether a learning framework is general enough to define tasks that lead to a particular set of hypotheses as the inductive solutions. On a first instance, we could say that a framework {a mathematical formula}F is general enough to learn a hypothesis H if there is at least one task {a mathematical formula}TF in this framework such that H is an inductive solution of {a mathematical formula}TF. However, as shown in Example 6, such a “loose notion” of generality may lead to the trivial learning framework, whose learning tasks have no examples, as the most general framework possible. Consider the trivial learning framework {a mathematical formula}ILP⊤ whose learning tasks are pairs {a mathematical formula}〈B,E⊤〉, where {a mathematical formula}E⊤ is the empty tuple and B is an ASP program. {a mathematical formula}ILP⊤(〈B,E⊤〉) is then the set of all ASP programs, i.e., every ASP program is a solution of every {a mathematical formula}ILP⊤ task. Although for every hypothesis H, given any background knowledge B there is clearly a set of examples {a mathematical formula}E⊤ such that {a mathematical formula}H∈ILP⊤(〈B,E⊤〉), every other possible hypothesis is also a solution of this same task, making it impossible to distinguish any hypothesis from another.
     </paragraph>
     <paragraph>
      It is clearly not sufficient to say that a framework is general enough to learn some target hypothesis (denoted from now on as {a mathematical formula}HT) if we can find at least one learning task with {a mathematical formula}HT as a solution. What this definition lacks is a way to express that {a mathematical formula}HT is a solution of a task T, but that some other (unwanted) hypothesis is not a solution of T. To capture this property of a learning framework we should be able to say that a task T can distinguish a hypothesis {a mathematical formula}HT from the unwanted hypothesis. Pairs of target and unwanted hypotheses, which can be distinguished from each other, are an interesting starting point when considering generality of a learning framework. But this again might not be the only property of generality. Frameworks, such as brave induction, can distinguish the target hypothesis {a mathematical formula}HT from two (or more) unwanted hypotheses, e.g., {a mathematical formula}H1′ and {a mathematical formula}H2′, in two separate learning tasks, but they may not have a single learning task capable of accepting {a mathematical formula}HT as inductive solution but neither {a mathematical formula}H1′ nor {a mathematical formula}H2′. Consider for instance the following example.
     </paragraph>
     <paragraph label="Example 7">
      Imagine the scenario where we are observing a coin being tossed several times. Obviously there are two outcomes, and we would like to learn an ASP program whose answer sets correspond to these two different outcomes. Consider the background knowledge B to be empty, and the atoms heads and tails to be true when the coin lands on heads or tails respectively. Our target hypothesis {a mathematical formula}HT is an ASP program such that {a mathematical formula}AS(B∪H)={{heads},{tails}}. One such hypothesis could be the program {a mathematical formula}HT={1{heads,tails}1.}. Consider now the two hypotheses {a mathematical formula}H1′={heads.} and {a mathematical formula}H2′={tails.}, which correspond to the coin always landing on heads or tails respectively. Neither of these hypothesis correctly represent the behaviour of the coin, so they are unwanted hypotheses. There is one answer set, {a mathematical formula}{heads}, of {a mathematical formula}B∪H1′ and one answer set, {a mathematical formula}{tails}, of {a mathematical formula}B∪H2′. {a mathematical formula}ILPb can distinguish {a mathematical formula}HT from {a mathematical formula}H1′ and from {a mathematical formula}H2′ separately with the tasks {a mathematical formula}〈B,〈{tails},∅〉〉 and {a mathematical formula}〈B,〈{heads},∅〉〉, respectively. But there is, however, no learning task for {a mathematical formula}ILPb for which {a mathematical formula}HT is an inductive solution and neither {a mathematical formula}H1′ nor {a mathematical formula}H2′ is.
     </paragraph>
     <paragraph>
      A more general notion of generality of learning framework can be considered, which looks at distinguishing a target hypothesis {a mathematical formula}HT from a set of unwanted hypotheses S. In Section 5.2 we introduce the notion of one-to-many-distinguishability class of a learning framework. This corresponds to the class of pairs of single hypothesis {a mathematical formula}HT's and set S's of hypotheses for which a learning framework has at least one task that distinguishes {a mathematical formula}HT from each hypothesis in S. Informally, this notion expresses the generality of a framework in finding a single target hypothesis in the presence of many unwanted hypotheses. In Section 5.3, we extend one-to-many-distinguishability class of a learning framework to many-to-many-distinguishability, which in turns captures the notion of distinguishing a set of target hypotheses {a mathematical formula}S1 from another set of unwanted hypotheses {a mathematical formula}S2, with a single task.
     </paragraph>
     <paragraph>
      In the remainder of this section we explore these three new measures of generality, expressed as three different learning problems. One-to-one-distinguishability determines the hypotheses that a framework is general enough to learn, while ruling out another unwanted hypothesis; one-to-many-distinguishability determines the hypotheses that can be learned from within a space of unwanted hypotheses; and finally, many-to-many-distinguishability determines exactly which sets of hypotheses can be learned. We will prove properties of our three classes of generalities making use of a definition of strong reduction from one framework to another. Strong reduction is different from the concept of reduction presented in [45]. Definition 11, Definition 12 present, respectively, a reformulation of the notion of reduction introduced in [45] and of our new concept of strong reduction.
     </paragraph>
     <paragraph label="Definition 11">
      A framework {a mathematical formula}F1reduces to{a mathematical formula}F2 (written {a mathematical formula}F1→rF2) if for every {a mathematical formula}F1 task {a mathematical formula}TF1 there is an {a mathematical formula}F2 task {a mathematical formula}TF2 such that {a mathematical formula}ILPF1(TF1)=ILPF2(TF2). A framework {a mathematical formula}F1 is at least as r-general as{a mathematical formula}F2 if {a mathematical formula}F2→rF1; and {a mathematical formula}F1 is more r-general than{a mathematical formula}F2 if {a mathematical formula}F2→rF1 and {a mathematical formula}F1↛rF2.
     </paragraph>
     <paragraph label="Example 8">
      Consider the {a mathematical formula}ILPb and {a mathematical formula}ILPc learning frameworks. {a mathematical formula}ILPb→rILPc, as any {a mathematical formula}ILPb task {a mathematical formula}〈B,〈E+,E−〉〉 maps to the {a mathematical formula}ILPc task {a mathematical formula}〈B∪{:-note.|e∈E+}∪{:-e.|e∈E−},〈∅,∅〉〉. {a mathematical formula}ILPc does not, however, reduce to {a mathematical formula}ILPb. Consider, for instance, the {a mathematical formula}ILPc task {a mathematical formula}Tc=〈∅,〈{p},∅〉〉 and assume that there is a task {a mathematical formula}Tb=〈B,〈E+,E−〉〉 such that {a mathematical formula}ILPb(Tb)=ILPc(Tc). The hypothesis {a mathematical formula}H1={p.}∈ILPc(Tc), and, given the assumption, {a mathematical formula}H1 is also in {a mathematical formula}ILPb(Tb). But consider now the hypothesis {a mathematical formula}H2={0{p}1.}. Since {a mathematical formula}AS(B∪H1)⊆AS(B∪H2), if {a mathematical formula}B∪H1 has an answer set extending {a mathematical formula}〈E+,E−〉, then so does {a mathematical formula}B∪H2. Thus, if {a mathematical formula}H1∈ILPb(Tb) then {a mathematical formula}H2∈ILPb(Tb). But, although {a mathematical formula}H2∈ILPb(Tb), it is easy to see that {a mathematical formula}H2∉ILPc(Tc), so making {a mathematical formula}ILPb(Tb) not equal to {a mathematical formula}ILPc(Tc). Hence, {a mathematical formula}ILPc does not reduce to {a mathematical formula}ILPb, and {a mathematical formula}ILPc is more r-general than {a mathematical formula}ILPb.
     </paragraph>
     <paragraph>
      We discuss the relationship between reductions and our own measures of generality in Section 6. Our notion of strong reduction differs from the above notion of reduction, in the fact that the reduced task must have the same background knowledge as the original task.
     </paragraph>
     <paragraph label="Definition 12">
      A framework {a mathematical formula}F1strongly reduces to{a mathematical formula}F2 (written {a mathematical formula}F1→srF2) if for every {a mathematical formula}F1 task {a mathematical formula}TF1=〈B,EF1〉 there is an {a mathematical formula}F2=〈B,EF2〉 task {a mathematical formula}TF2 such that {a mathematical formula}ILPF1(TF1)=ILPF2(TF2). A framework {a mathematical formula}F1 is at least as sr-general as{a mathematical formula}F2 if {a mathematical formula}F2→srF1; and {a mathematical formula}F1 is more sr-general than{a mathematical formula}F2 if {a mathematical formula}F2→srF1 and {a mathematical formula}F1↛srF2.
     </paragraph>
     <paragraph>
      Proposition 9 shows the strong reduction relations between the frameworks considered in this paper. Note that although {a mathematical formula}ILPc is more r-general than {a mathematical formula}ILPb (as shown in Example 8), it is not more sr-general than {a mathematical formula}ILPb. This is because without changing the background knowledge, {a mathematical formula}ILPc cannot represent the same {a mathematical formula}ILPb tasks.
     </paragraph>
     <paragraph label="Proposition 9">
      <list>
       <list-item label="1.">
        {a mathematical formula}ILPb→srILPsm→srILPLAS→srILPLOAS→srILPLOAScontext
       </list-item>
       <list-item label="2.">
        {a mathematical formula}ILPc→srILPLAS
       </list-item>
      </list>
     </paragraph>
     <paragraph label="Proof">
      <list>
       <list-item label="1.">
        For any {a mathematical formula}ILPb task {a mathematical formula}Tb=〈B,〈E+,E−〉〉, {a mathematical formula}ILPb(Tb)=ILPsm(〈B,〈{〈E+,E−〉}〉〉)For any {a mathematical formula}ILPsm task {a mathematical formula}Tsm=〈B,〈{e1,…,en}〉〉, {a mathematical formula}ILPsm(Tsm)=ILPLAS(〈B,〈{e1,…,en},∅〉〉)For any {a mathematical formula}ILPLAS task {a mathematical formula}TLAS=〈B,〈E+,E−〉〉, {a mathematical formula}ILPLAS(TLAS)=ILPLOAS(〈B,〈E+,E−,∅,∅〉〉)For any partial interpretation e, let {a mathematical formula}c(e) be the CDPI {a mathematical formula}〈e,∅〉. For any {a mathematical formula}ILPLOAS task {a mathematical formula}TLOAS=〈B,〈E+,E−,Ob,Oc〉〉, {a mathematical formula}ILPLOAS(TLOAS)=ILPLOAScontext(〈B,〈{c(e)|e∈E+},{c(e)|e∈E−},{〈c(e1),c(e2)〉|〈e1,e2〉∈Ob},{〈c(e1),c(e2)〉|〈e1,e2〉∈Oc}〉〉)
       </list-item>
       <list-item label="2.">
        For any {a mathematical formula}ILPc task {a mathematical formula}Tc=〈B,〈{e1+,…,em+},{e1−,…,en−}〉〉, {a mathematical formula}ILPc(Tc)=ILPLAS(〈B,〈{〈∅,∅〉},{〈∅,{e1+}〉,…,〈∅,{em+}〉,〈{e1−},∅〉,…,〈{en−},∅〉}〉〉). Note that the empty {a mathematical formula}ILPLAS positive example enforces that there is at least one answer set, and both the {a mathematical formula}ILPc positive and negative examples are mapped to {a mathematical formula}ILPLAS negative examples which enforce in the case of positive (resp. negative) examples that they are not false (resp. not true) in any answer set, and hence true (resp. false) in every answer set. □
       </list-item>
      </list>
     </paragraph>
     <section label="5.1">
      <section-title>
       Distinguishability
      </section-title>
      <paragraph>
       A one-to-one-distinguishability class captures those pairs of hypotheses {a mathematical formula}H1 and {a mathematical formula}H2 that can be distinguished from each other with respect to a given possible background knowledge.
      </paragraph>
      <paragraph label="Definition 13">
       The one-to-one-distinguishability class of a learning framework {a mathematical formula}F (denoted {a mathematical formula}D11(F)) is the set of tuples {a mathematical formula}〈B,H1,H2〉 of ASP programs for which there is at least one task {a mathematical formula}TF=〈B,EF〉 such that {a mathematical formula}H1∈F(TF) and {a mathematical formula}H2∉F(TF). For each {a mathematical formula}〈B,H1,H2〉∈D11(F), {a mathematical formula}TF is said to distinguish{a mathematical formula}H1 from {a mathematical formula}H2 with respect to B. Given two frameworks {a mathematical formula}F1 and {a mathematical formula}F2, we say that {a mathematical formula}F1 is at least as (resp. more) {a mathematical formula}D11-general as (resp. than) {a mathematical formula}F2 if {a mathematical formula}D11(F2)⊆D11(F1) (resp. {a mathematical formula}D11(F2)⊂D11(F1)).
      </paragraph>
      <paragraph>
       Note that the one-to-one-distinguishability relationship is not symmetric; i.e. there are pairs of hypotheses {a mathematical formula}H1 and {a mathematical formula}H2 such that, given a background knowledge B, {a mathematical formula}H1 can be distinguished from {a mathematical formula}H2, but {a mathematical formula}H2 can not be distinguished from {a mathematical formula}H1. This is illustrated by Example 9.
      </paragraph>
      <paragraph label="Example 9">
       Consider a background knowledge B that defines the concepts of cell, same_block, same_row and same_column for a 4x4 Sudoku grid.Let {a mathematical formula}H1 be the incomplete description of the Sudoku rules:
       <list>
        1{value(C,1),value(C,2),value(C,3),value(C,4)}1:-cell(C).:-value(C1,V),value(C2,V),same_row(C1,C2).:-value(C1,V),value(C2,V),same_col(C1,C2).Also let
       </list>
       <paragraph>
        {a mathematical formula}H2 be the complete description of the Sudoku rules:
       </paragraph>
       <list>
        <list-item>
         1{value(C,1),value(C,2),value(C,3),value(C,4)}1:-cell(C).:-value(C1,V),value(C2,V),same_row(C1,C2).:-value(C1,V),value(C2,V),same_col(C1,C2).:-value(C1,V),value(C2,V),same_block(C1,C2).
        </list-item>
       </list>
       <paragraph>
        {a mathematical formula}ILPb can distinguish {a mathematical formula}H1 from {a mathematical formula}H2 with respect to B. This can be seen using the task {a mathematical formula}〈B,〈{value((1,1),1),value((2,2),1)},∅〉〉. On the other hand, {a mathematical formula}ILPb cannot distinguish {a mathematical formula}H2 from {a mathematical formula}H1. Whatever examples are given in a learning task to learn {a mathematical formula}H2, it must be the case that {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅, where A is an answer set of {a mathematical formula}B∪H2. But answer sets of {a mathematical formula}B∪H2 are also answer sets of {a mathematical formula}B∪H1. So A is also an answer set of {a mathematical formula}B∪H1, which implies that {a mathematical formula}H1 satisfies the same examples and is a solution of the same learning task.
       </paragraph>
      </paragraph>
      <paragraph>
       In fact, Proposition 10 generalises Example 9 showing that {a mathematical formula}ILPb cannot distinguish any program containing a constraint from the same program without the constraint.
      </paragraph>
      <paragraph label="Proposition 10">
       {a mathematical formula}ILPbcannot distinguish any hypothesis H which contains a constraint C from{a mathematical formula}H\{C}, with respect to any background knowledge.
      </paragraph>
      <paragraph label="Proof">
       Assume for contradiction that there is a hypothesis {a mathematical formula}H=H′∪C where C is a constraint and an {a mathematical formula}ILPb task {a mathematical formula}Tb=〈B,〈E+,E−〉〉 such that {a mathematical formula}H∈ILPb(Tb) and {a mathematical formula}H′∉ILPb(Tb).
      </paragraph>
      <list>
       <list-item label="⇒">
        {a mathematical formula}∃A∈AS(B∪H) such that {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅. But as C is a constraint {a mathematical formula}AS(B∪H)⊆AS(B∪H′) and so {a mathematical formula}A∈AS(B∪H′).
       </list-item>
       <list-item label="⇒">
        {a mathematical formula}∃A∈AS(B∪H′) such that {a mathematical formula}E+⊆A and {a mathematical formula}E−∩A=∅.
       </list-item>
       <list-item label="⇒">
        {a mathematical formula}H′∈ILPb(Tb). Contradiction! □
       </list-item>
      </list>
      <paragraph>
       One useful property is that if there is a strong reduction from one framework {a mathematical formula}F1 to another framework {a mathematical formula}F2 then {a mathematical formula}D11(F1)⊆D11(F2). Note that {a mathematical formula}F2 is not guaranteed to be more {a mathematical formula}D11-general than {a mathematical formula}F1, even in the case when there is no reduction from {a mathematical formula}F2 to {a mathematical formula}F1.
      </paragraph>
      <paragraph label="Proposition 11">
       For any two frameworks{a mathematical formula}F1and{a mathematical formula}F2:{a mathematical formula}F1→srF2⇒D11(F1)⊆D11(F2).
      </paragraph>
      <paragraph label="Proof">
       Assume that {a mathematical formula}F1→srF2. Take any {a mathematical formula}〈B,H1,H2〉∈D11(F1). There must be some task {a mathematical formula}TF1, with background knowledge B, such that {a mathematical formula}H1∈ILPF1(TF1) and {a mathematical formula}H2∉ILPF1(TF1). Hence, as {a mathematical formula}F1→srF2, there must be some task {a mathematical formula}TF2, with background knowledge B, such that {a mathematical formula}H1∈ILPF2(TF2) and {a mathematical formula}H2∉ILPF2(TF2). So {a mathematical formula}〈B,H1,H2〉∈D11(F2). Hence, {a mathematical formula}D11(F1)⊆D11(F2). □
      </paragraph>
      <paragraph>
       As there are clear strong reductions (shown by Proposition 9), an ordering of the one-to-one-distinguishability classes of the frameworks emerges (shown in Corollary 4).
      </paragraph>
      <paragraph label="Corollary 4">
       <list>
        <list-item>
         {a mathematical formula}D11(ILPb)⊆D11(ILPsm)⊆D11(ILPLAS)⊆D11(ILPLOAS)⊆D11(ILPLOAScontext)
        </list-item>
        <list-item>
         {a mathematical formula}D11(ILPc)⊆D11(ILPLAS)
        </list-item>
       </list>
      </paragraph>
      <paragraph>
       While this does give us information about the ordering of the power of the frameworks to distinguish between hypotheses, it does not tell us, for example, what the relationship is between the distinguishability classes of {a mathematical formula}ILPb and {a mathematical formula}ILPc. It does not tell us which of the ⊆'s are strict (in fact, {a mathematical formula}D11(ILPb)=D11(ILPsm), but the rest are strict subset relations). For each framework, Table 3 shows the necessary and sufficient condition needed to be able to distinguish hypotheses. In the case of the cautious induction framework, the condition makes use of a new notation. Given a program P, {a mathematical formula}Eb(P)={i1∧…∧im∧note1∧…∧noten|∃A∈AS(P) st i1,…,im∈A and e1,…,en∉A}, i.e. {a mathematical formula}Eb(P) denotes the set of conjunctions of literals that are true in at least one answer set of P. Similarly, we use {a mathematical formula}Ec(P) to denote the set of conjunctions of literals that are true in every answer set of P. The following property holds.
      </paragraph>
      <paragraph label="Proposition 12">
       For any programs{a mathematical formula}P1and{a mathematical formula}P2,{a mathematical formula}Eb(P1)⊆Eb(P2)if and only if{a mathematical formula}AS(P1)⊆AS(P2).
      </paragraph>
      <paragraph>
       Proposition 13, Proposition 14, Proposition 15, Proposition 16, Proposition 17, Proposition 18 prove the one-to-one-distinguishability classes of {a mathematical formula}ILPb, {a mathematical formula}ILPsm, {a mathematical formula}ILPc, {a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext, showing also the sufficient and necessary conditions for distinguishability presented in Table 3. To aid readability, the proofs are in the appendix rather than the main paper.
      </paragraph>
      <paragraph label="Proposition 13">
       {a mathematical formula}D11(ILPb)={〈B,H1,H2〉|AS(B∪H1)⊈AS(B∪H2)}.
      </paragraph>
      <paragraph>
       Interestingly, although {a mathematical formula}ILPsm↛srILPb, {a mathematical formula}D11(ILPb)=D11(ILPsm). This is shown by Proposition 14. The reason for this is that if {a mathematical formula}ILPsm can distinguish one hypothesis {a mathematical formula}H1 from another hypothesis {a mathematical formula}H2 then, there must be some task {a mathematical formula}Tsm such that {a mathematical formula}H1 is a solution of {a mathematical formula}Tsm and {a mathematical formula}H2 is not. This means that {a mathematical formula}H1 must cover all of the examples of {a mathematical formula}Tsm and there must be at least one (partial interpretation) example of {a mathematical formula}Tsm which is not covered by {a mathematical formula}H2. This partial interpretation example can be given as the set of positive and negative examples in an {a mathematical formula}ILPb task. This {a mathematical formula}ILPb task will then distinguish {a mathematical formula}H1 from {a mathematical formula}H2.
      </paragraph>
      <paragraph label="Proposition 14">
       {a mathematical formula}D11(ILPb)=D11(ILPsm).
      </paragraph>
      <paragraph>
       To better compare the conditions for {a mathematical formula}ILPb and {a mathematical formula}ILPc, we can express the necessary and sufficient condition of {a mathematical formula}ILPb in terms of the notion {a mathematical formula}Eb(P). Specifically, in {a mathematical formula}ILPb for one hypothesis {a mathematical formula}H1 to be distinguishable from another hypothesis {a mathematical formula}H2 (with respect to a background knowledge B) it is both necessary and sufficient for {a mathematical formula}Eb(B∪H1) to contain at least one conjunction that is not in {a mathematical formula}Eb(B∪H2). This is because the extra conjunction can be used to generate a set of examples that are covered by {a mathematical formula}H1 but not {a mathematical formula}H2. This is demonstrated by Example 10.
      </paragraph>
      <paragraph label="Example 10">
       Consider again the programs {a mathematical formula}B=∅, {a mathematical formula}H1={1{heads,tails}1.} and {a mathematical formula}H2={heads.}. {a mathematical formula}Eb(B∪H1) contains the conjunction {a mathematical formula}notheads∧tails, whereas {a mathematical formula}Eb(B∪H2) does not. This conjunction can be mapped into the positive example tails and the negative example heads, which {a mathematical formula}B∪H1 covers, but {a mathematical formula}B∪H2 does not – i.e. the task {a mathematical formula}〈B,〈{tails},{heads}〉〉 distinguishes {a mathematical formula}H1 from {a mathematical formula}H2. So, as the one-to-one-distinguishability condition for {a mathematical formula}ILPb could also be expressed as {a mathematical formula}Eb(B∪H1)⊈Eb(B∪H2), it might be expected that the one-to-one-distinguishability condition for {a mathematical formula}ILPc would be that {a mathematical formula}Ec(B∪H1)⊈Ec(B∪H2). Indeed this would be the case, if it were not for the extra condition that {a mathematical formula}ILPc imposes on any inductive solution: that is, any inductive solution H must be such that {a mathematical formula}B∪H is satisfiable. Although this extra condition may seem unnecessary at first sight, its importance becomes clear when considering distinguishability. Without this extra condition, no hypothesis would be distinguishable from the hypothesis given by the empty constraint “{a mathematical formula}:-.” – i.e. there would be no hypothesis H such that {a mathematical formula}〈B,H,{:-.}〉∈D11(ILPc) (for any B). This is because there cannot be any answer set of {a mathematical formula}B∪{:-.} that does not cover the examples (as there are no answer sets). As {a mathematical formula}ILPc has the extra condition that {a mathematical formula}B∪H must be satisfiable, its distinguishability condition is slightly more complicated than {a mathematical formula}Ec(B∪H1)⊈Ec(B∪H2), as shown in Proposition 15.
      </paragraph>
      <paragraph label="Proposition 15">
       {a mathematical formula}D11(ILPc)={〈B,H1,H2〉|AS(B∪H1)≠∅∧(AS(B∪H2)=∅∨Ec(B∪H2)⊈Ec(B∪H1))}.
      </paragraph>
      <paragraph>
       We now prove the one-to-one-distinguishability classes of our own frameworks, {a mathematical formula}ILPLAS and {a mathematical formula}ILPLOAS. {a mathematical formula}D11(ILPLAS) contains both {a mathematical formula}D11(ILPb) and {a mathematical formula}D11(ILPc) as {a mathematical formula}ILPLAS can distinguish any two hypotheses which, combined with the background knowledge, have different answer sets.
      </paragraph>
      <paragraph label="Proposition 16">
       {a mathematical formula}D11(ILPLAS)={〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)}. As shown in Theorem 4, {a mathematical formula}ILPLOAS is more {a mathematical formula}D11-general than {a mathematical formula}ILPLAS. This is because {a mathematical formula}ILPLOAS is able to use its ordering examples to distinguish any two hypotheses that, when combined with the background knowledge, order their answer sets differently, even if the two programs have the same answer sets.
      </paragraph>
      <paragraph label="Proposition 17">
       {a mathematical formula}D11(ILPLOAS)={〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)orord(B∪H1)≠ord(B∪H2)}.
      </paragraph>
      <paragraph>
       Note that we assume {a mathematical formula}ILPLOAS to be able to give ordering examples with any of the binary ordering operators. The slightly more restrictive version of {a mathematical formula}ILPLOAS, presented in [23] where the operator is only the &lt;, has a smaller one-to-one-distinguishability class. This is shown in Example 11.
      </paragraph>
      <paragraph label="Example 11">
       Consider the heads and tails problem again, where {a mathematical formula}B={1{heads,tails}1.}, and two potential hypotheses:
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}H1=∅
       </list-item>
       <list-item label="•">
        {a mathematical formula}H2={:∼heads.[1@1]}
       </list-item>
      </list>
      <paragraph>
       {a mathematical formula}ILPLOAS can distinguish any two hypotheses that, when combined with a fixed background knowledge, behave differently. It cannot distinguish hypotheses that are different but behave the same with respect to the background knowledge. This means that there are some hypotheses that are not strongly equivalent (when combined with the background knowledge), but {a mathematical formula}ILPLOAS cannot distinguish one from the other. We now show that {a mathematical formula}ILPLOAScontextcan distinguish between any two hypotheses, {a mathematical formula}H1 and {a mathematical formula}H2, that, when combined with the background knowledge, are not strongly equivalent, or there is at least one program {a mathematical formula}C∈ASPch (consisting of normal rules, choice rules and hard constraints), such that {a mathematical formula}ord(B∪H1∪C)≠ord(B,∪H2∪C).
      </paragraph>
      <paragraph label="Proposition 18">
       {a mathematical formula}
      </paragraph>
      <paragraph>
       Now that we have proven the distinguishability classes for each learning framework, we can strengthen the statement of Corollary 4 and more precisely state the relationship between the distinguishability classes of the frameworks. Apart from the case of {a mathematical formula}ILPb and {a mathematical formula}ILPsm, each of the subset relations in Corollary 4 are in fact strict subsets.
      </paragraph>
      <paragraph label="Theorem 4">
       Consider the learning frameworks{a mathematical formula}ILPb,{a mathematical formula}ILPc,{a mathematical formula}ILPsm,{a mathematical formula}ILPLAS,{a mathematical formula}ILPLOASand{a mathematical formula}ILPLOAScontext.
      </paragraph>
      <list>
       <list-item>
        {a mathematical formula}D11(ILPb)=D11(ILPsm)⊂D11(ILPLAS)⊂D11(ILPLOAS)⊂D11(ILPLOAScontext)
       </list-item>
       <list-item>
        {a mathematical formula}D11(ILPc)⊂D11(ILPLAS)
       </list-item>
      </list>
      <paragraph label="Proof">
       <list>
        <list-item label="1.">
         The fact that {a mathematical formula}D11(ILPb)=D11(ILPsm) was shown in Proposition 14. By Corollary 4, {a mathematical formula}D11(ILPsm)⊆D11(ILPLAS)⊆D11(ILPLOAS)⊆D11(ILPLOAScontext); hence, it remains to show that {a mathematical formula}D11(ILPsm)≠D11(ILPLAS)≠D11(ILPLOAS)≠D11(ILPLOAScontext)
        </list-item>
        <list-item label="2.">
         By Corollary 4, {a mathematical formula}D11(ILPc)⊆D11(ILPLAS). Hence, it remains to show that {a mathematical formula}D11(ILPc)≠D11(ILPLAS). Consider the tuple {a mathematical formula}〈B,H1,H2〉, where {a mathematical formula}B={p:-notp}, {a mathematical formula}H1=∅ and {a mathematical formula}H2={p.}. {a mathematical formula}AS(B∪H1)=∅ and {a mathematical formula}AS(B∪H1)≠AS(B∪H2). By the conditions in Table 3, {a mathematical formula}〈B,H1,H2〉 is in {a mathematical formula}D11(ILPLAS) but is not in {a mathematical formula}D11(ILPc). Hence, {a mathematical formula}D11(ILPc)≠D11(ILPLAS). □
        </list-item>
       </list>
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       The one-to-many-distinguishability class of a learning framework
      </section-title>
      <paragraph>
       In practice an ILP task has a search space of possible hypotheses, and it is important to know the cases in which one particular hypothesis can be distinguished from the rest. In what follows, we analyse the conditions under which a learning framework can distinguish an hypothesis from a set of other hypotheses. As mentioned at the beginning of Section 5, this corresponds to the new notion we call the one-to-many-distinguishability class of a learning framework, which is a generalisation of the notion of the one-to-one-distinguishability class described above.
      </paragraph>
      <paragraph label="Definition 14">
       The one-to-many-distinguishability class of a learning framework {a mathematical formula}F (denoted {a mathematical formula}Dm1(F)) is the set of all tuples {a mathematical formula}〈B,H,{H1,…,Hn}〉 such that there is a task {a mathematical formula}TF which distinguishes H from each {a mathematical formula}Hi with respect to B. Given two frameworks {a mathematical formula}F1 and {a mathematical formula}F2, we say that {a mathematical formula}F1 is at least as (resp. more) {a mathematical formula}Dm1-general than {a mathematical formula}F2 if {a mathematical formula}Dm1(F2)⊆Dm1(F1) (resp. {a mathematical formula}Dm1(F2)⊂Dm1(F1)).
      </paragraph>
      <paragraph>
       The one-to-many-distinguishability class tells us the circumstances in which a framework is general enough to distinguish some target hypothesis from a set of unwanted hypotheses. Note that, although the tuples in a one-to-many-distinguishability class that have a singleton set as third argument correspond to the tuples in a one-to-one-distinguishability class of that framework, it is not always the case that if {a mathematical formula}F1 is more {a mathematical formula}Dm1-general than {a mathematical formula}F2 then {a mathematical formula}F1 is also more {a mathematical formula}D11-general than {a mathematical formula}F2. For example, we will see that {a mathematical formula}ILPsm is more {a mathematical formula}Dm1-general than {a mathematical formula}ILPb, but we have already shown in Proposition 14 that the {a mathematical formula}ILPb and {a mathematical formula}ILPsm are equally {a mathematical formula}D11-general. Proposition 19 shows, however, that if {a mathematical formula}F1 is at least as {a mathematical formula}Dm1-general as {a mathematical formula}F2 then {a mathematical formula}F1 is at least as {a mathematical formula}D11-general as {a mathematical formula}F2.
      </paragraph>
      <paragraph label="Proposition 19">
       For any two frameworks{a mathematical formula}F1and{a mathematical formula}F2such that{a mathematical formula}F1is at least as{a mathematical formula}Dm1-general as{a mathematical formula}F2,{a mathematical formula}F1is at least as{a mathematical formula}D11-general as{a mathematical formula}F2(i.e.{a mathematical formula}Dm1(F2)⊆Dm1(F1)⇒D11(F2)⊆D11(F1)).
      </paragraph>
      <paragraph label="Proof">
       Assume that {a mathematical formula}F1 is at least as {a mathematical formula}Dm1-general as {a mathematical formula}F2 and let {a mathematical formula}〈B,H1,H2〉∈D11(F2). To show that {a mathematical formula}F1 is at least as {a mathematical formula}D11-general as {a mathematical formula}F2, we must show that {a mathematical formula}〈B,H1,H2〉∈D11(F1).As {a mathematical formula}〈B,H1,H2〉∈D11(F2), {a mathematical formula}〈B,H1,{H2}〉∈Dm1(F2); hence, as {a mathematical formula}F1 is at least as {a mathematical formula}Dm1-general as {a mathematical formula}F2, {a mathematical formula}〈B,H1,{H2}〉∈Dm1(F1); and hence, {a mathematical formula}〈B,H1,H2〉∈D11(F1). □
      </paragraph>
      <paragraph>
       We have already seen that if there is a strong reduction from {a mathematical formula}F1 to {a mathematical formula}F2 then {a mathematical formula}F2 is at least as {a mathematical formula}D11-general as {a mathematical formula}F1. Proposition 20 shows that a similar result holds for {a mathematical formula}Dm1-generality. Similarly to {a mathematical formula}D11-generality, however, a strong reduction from {a mathematical formula}F1 to {a mathematical formula}F2 does not imply that {a mathematical formula}F2 is more {a mathematical formula}Dm1-general than {a mathematical formula}F1, even in the case that there is no strong reduction from {a mathematical formula}F2 to {a mathematical formula}F1.
      </paragraph>
      <paragraph label="Proposition 20">
       For any two frameworks{a mathematical formula}F1and{a mathematical formula}F2:{a mathematical formula}F1→srF2⇒Dm1(F1)⊆Dm1(F2).
      </paragraph>
      <paragraph label="Proof">
       Assume that {a mathematical formula}F1→srF2. Take any {a mathematical formula}〈B,H,S〉∈Dm1(F1). There must be some task {a mathematical formula}TF1, with background knowledge B, such that {a mathematical formula}H∈ILPF1(TF1) and {a mathematical formula}S∩ILPF1(TF1)=∅. Hence, as {a mathematical formula}F1→srF2, there must be some {a mathematical formula}F2 task {a mathematical formula}TF2, with background knowledge B, such that {a mathematical formula}H∈ILPF2(TF2) and {a mathematical formula}S∩ILPF2(TF2)=∅. So {a mathematical formula}〈B,H,S〉∈Dm1(F2). Hence, {a mathematical formula}Dm1(F1)⊆Dm1(F2). □
      </paragraph>
      <paragraph>
       Due to the strong reductions shown in Proposition 9, an ordering of the one-to-many-distinguishability classes of the frameworks emerges (shown in Corollary 5).
      </paragraph>
      <paragraph label="Corollary 5">
       <list>
        <list-item>
         {a mathematical formula}Dm1(ILPb)⊆Dm1(ILPsm)⊆Dm1(ILPLAS)⊆Dm1(ILPLOAS)⊆Dm1(ILPLOAScontext)
        </list-item>
        <list-item>
         {a mathematical formula}Dm1(ILPc)⊆Dm1(ILPLAS)
        </list-item>
       </list>
      </paragraph>
      <paragraph>
       This time, we will see that each of the ⊆'s in Corollary 5 can be upgraded to a strict ⊂. Rather than proving the one-to-many-distinguishability classes from scratch, we now present a useful result. For some frameworks, the one-to-one-distinguishability class of a learning framework can be used to construct the one-to-many-distinguishability class. This is the case when the framework has closed one-to-many-distinguishability (formalised by Definition 15). Proposition 21 and Corollary 6 show how the one-to-many-distinguishability class of a framework can be constructed using its one-to-one-distinguishability class if it has closed one-to-many-distinguishability.
      </paragraph>
      <paragraph label="Definition 15">
       Given any learning framework {a mathematical formula}F, the closure of the one-to-many-distinguishability class, written {a mathematical formula}Dm1(F)‾, is the set {a mathematical formula}{〈B,H,S1∪…∪Sn〉|〈B,H,S1〉,…,〈B,H,Sn〉∈Dm1(F)}. We say that {a mathematical formula}F has closed one-to-many-distinguishability if and only if {a mathematical formula}Dm1(F)‾=Dm1(F).
      </paragraph>
      <paragraph label="Proposition 21">
       For any learning framework{a mathematical formula}F,{a mathematical formula}Dm1(F)‾={〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}.
      </paragraph>
      <paragraph label="Corollary 6">
       For any learning framework{a mathematical formula}F,{a mathematical formula}Dm1(F)⊆{〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}. The equality holds if and only if{a mathematical formula}Fhas closed one-to-many-distinguishability.
      </paragraph>
      <paragraph>
       Note that not all learning frameworks have closed one-to-many-distinguishability; for instance, Example 12 shows that brave induction does not. We will show that induction of stable models, on the other hand, does have closed one-to-many-distinguishability.
      </paragraph>
      <paragraph label="Example 12">
       {a mathematical formula}ILPb does not have closed one-to-many-distinguishability. We can see this by reconsidering the programs {a mathematical formula}B=∅, {a mathematical formula}H={1{heads,tails}1.}, {a mathematical formula}H1={heads.} and {a mathematical formula}H2={tails.}. {a mathematical formula}〈B,H,{H1}〉∈Dm1(ILPb) ({a mathematical formula}〈B,〈{tails},∅〉〉 distinguishes H from {a mathematical formula}H1 wrt the background knowledge B). Similarly {a mathematical formula}〈B,H,{H2}〉∈Dm1(ILPb) ({a mathematical formula}〈B,〈{heads},∅〉〉 distinguishes H from {a mathematical formula}H2 wrt the background knowledge B). If {a mathematical formula}ILPb had closed one-to-many-distinguishability then {a mathematical formula}〈B,H,{H1,H2}〉 would be in {a mathematical formula}Dm1(ILPb); hence, to show that {a mathematical formula}ILPb does not have closed one-to-many-distinguishability it is sufficient to show that {a mathematical formula}〈B,H,{H1,H2}〉∉Dm1(ILPb). Hence it remains to show that there is no task {a mathematical formula}Tb=〈B,〈E+,E−〉〉 such that {a mathematical formula}H∈ILPb(Tb) and {a mathematical formula}{H1,H2}∩ILPb(Tb)=∅.Assume for contradiction that there is such a task {a mathematical formula}Tb. As {a mathematical formula}H∈ILPb(Tb) and {a mathematical formula}AS(B∪H)={{heads},{tails}}, {a mathematical formula}E+⊂{heads,tails} and {a mathematical formula}E−⊂{heads,tails} (neither can be equal to {a mathematical formula}{heads,tails} or H would not be a solution).
      </paragraph>
      <list>
       <list-item>
        {a mathematical formula}E+=∅
       </list-item>
       <list-item>
        {a mathematical formula}E+={heads}{a mathematical formula}heads∉E− as otherwise the task would have no solutions (and we know that H is a solution). In this case {a mathematical formula}H1 would be an inductive solution (regardless of what else is in {a mathematical formula}E−). Contradiction.
       </list-item>
       <list-item>
        {a mathematical formula}E+={tails}Similarly to above case, {a mathematical formula}tails∉E− as otherwise the task would have no solutions. In this case {a mathematical formula}H2 would be an inductive solution (regardless of what else is in {a mathematical formula}E−). Contradiction.
       </list-item>
      </list>
      <paragraph>
       In contrast to {a mathematical formula}ILPb, {a mathematical formula}ILPsm (which we will see does have closed one-to-many-distinguishability), can distinguish H from {a mathematical formula}H1 and {a mathematical formula}H2 with the task {a mathematical formula}〈B,〈{〈{heads},∅〉,〈{tails},∅〉}〉〉. Note that this is a combination of the two brave tasks which distinguish H from {a mathematical formula}H1 and from {a mathematical formula}H2. We will show that the ability to combine tasks in this way is a sufficient condition for a framework to have closed one-to-many-distinguishability. Proposition 22 shows the one-to-many-distinguishability class of {a mathematical formula}ILPb.
      </paragraph>
      <paragraph label="Proposition 22">
       {a mathematical formula}Dm1(ILPb)={〈B,H,{h1,…,hm}〉|AS(B∪H)⊈AS(B∪h1)∪…∪AS(B∪hm)}.
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="1.">
         Let {a mathematical formula}B,H,h1,…,hm be ASP programs such that {a mathematical formula}AS(B∪H)⊈AS(B∪h1)∪…∪AS(B∪hm). This implies that there is an interpretation A that is an answer set of {a mathematical formula}B∪H but not an answer set of any of the programs {a mathematical formula}B∪h1,…,B∪hm. Let L be the set of atoms which occur in at least one answer set of at least one of the programs {a mathematical formula}B∪H,B∪h1,…B∪hm; then {a mathematical formula}B∪H has an answer set that extends {a mathematical formula}〈A,L\A〉, but none of {a mathematical formula}B∪h1,…B∪hm do. So the task {a mathematical formula}〈B,〈A,L\A〉〉 distinguishes H from {a mathematical formula}h1 to {a mathematical formula}hm. Hence, {a mathematical formula}〈B,H,{h1,…,hm}〉∈Dm1(ILPb).
        </list-item>
        <list-item label="2.">
         Assume {a mathematical formula}〈B,H,{h1,…,hm}〉∈Dm1(ILPb). Then there is an {a mathematical formula}ILPb task {a mathematical formula}Tb=〈B,〈E+,E−〉〉 such that {a mathematical formula}B∪H has an answer set extending {a mathematical formula}〈E+,E−〉 and none of {a mathematical formula}B∪h1,…,B∪hm do. Hence, there must be at least one answer set of {a mathematical formula}B∪H, which is not an answer set of any of {a mathematical formula}B∪h1,…,B∪hm. Therefore {a mathematical formula}AS(B∪H)⊈AS(B∪h1)∪…∪AS(B∪hm). □
        </list-item>
       </list>
      </paragraph>
      <paragraph>
       For a framework {a mathematical formula}F to have closed one-to-many-distinguishability it is sufficient (but not necessary) that for every two {a mathematical formula}F tasks, there is a third {a mathematical formula}F task whose solutions are exactly those hypotheses which are solutions to both of the original two tasks. This is formalised and proved in Lemma 2. This condition is not necessary in general, but it holds for the frameworks considered in this paper that have closed one-to-many-distinguishability.
      </paragraph>
      <paragraph label="Proof">
       For any learning framework{a mathematical formula}Fto have closed one-to-many-distinguishability, it is sufficient that for every pair of learning tasks{a mathematical formula}TF1=〈B,EF1〉and{a mathematical formula}TF2=〈B,EF2〉it is possible to construct a new learning task{a mathematical formula}TF3=〈B,EF3〉such that{a mathematical formula}ILPF(TF3)=ILPF(TF1)∩ILPF(TF2).Assume that for every pair of learning tasks {a mathematical formula}TF1=〈B,EF1〉 and {a mathematical formula}TF2=〈B,EF2〉 it is possible to construct a new learning task {a mathematical formula}TF3=〈B,EF3〉 such that {a mathematical formula}ILPF(TF3)=ILPF(TF1)∩ILPF(TF2). Let {a mathematical formula}〈B,H,S1〉,…,〈B,H,Sn〉∈Dm1(F). To prove that {a mathematical formula}F has closed one-to-many-distinguishability, we must show that {a mathematical formula}〈B,H,S1∪…∪Sn〉∈Dm1(F). We prove this by showing (by mathematical induction) that for each {a mathematical formula}k∈[1..n], {a mathematical formula}〈B,H,S1∪…∪Sk〉∈Dm1(F).
      </paragraph>
      <list>
       <list-item>
        {a mathematical formula}k=1. {a mathematical formula}〈B,H,S1〉∈Dm1(F) by the initial assumptions.
       </list-item>
       <list-item>
        Assume that for some {a mathematical formula}0≤k&lt;n, {a mathematical formula}〈B,H,S1∪…∪Sk〉∈Dm1(F).
       </list-item>
       <list-item>
        We must show that {a mathematical formula}〈B,H,S1∪…∪Sk+1〉∈Dm1(F).
       </list-item>
      </list>
      <paragraph label="Proposition 23">
       {a mathematical formula}ILPc,{a mathematical formula}ILPsm,ILPLAS,{a mathematical formula}ILPLOASand{a mathematical formula}ILPLOAScontextall have closed one-to-many-distinguishability.
      </paragraph>
      <paragraph label="Theorem 5">
       Given two frameworks{a mathematical formula}F1and{a mathematical formula}F2,{a mathematical formula}Dm1(F1)‾⊆Dm1(F2)‾if and only if{a mathematical formula}D11(F1)⊆D11(F2).
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}Dm1(F1)‾⊆Dm1(F2)‾{a mathematical formula} □
      </paragraph>
      <paragraph label="Corollary 7">
       Given two frameworks{a mathematical formula}F1and{a mathematical formula}F2with closed one-to-many-distinguishability:{a mathematical formula}Dm1(F1)⊂Dm1(F2)if and only if{a mathematical formula}D11(F1)⊂D11(F2).
      </paragraph>
      <paragraph label="Theorem 6">
       Consider the learning frameworks{a mathematical formula}ILPb,{a mathematical formula}ILPc,{a mathematical formula}ILPsm,{a mathematical formula}ILPLAS,{a mathematical formula}ILPLOASand{a mathematical formula}ILPLOAScontext.
      </paragraph>
      <list>
       <list-item>
        {a mathematical formula}Dm1(ILPb)⊂Dm1(ILPsm)⊂Dm1(ILPLAS)⊂Dm1(ILPLOAS)⊂Dm1(ILPLOAScontext)
       </list-item>
       <list-item>
        {a mathematical formula}Dm1(ILPc)⊂Dm1(ILPLAS)
       </list-item>
      </list>
      <paragraph label="Proof">
       Firstly, as shown in Example 12, {a mathematical formula}Dm1(ILPb) is a strict subset of {a mathematical formula}Dm1(ILPb‾). Hence, by Theorem 5, {a mathematical formula}Dm1(ILPb)⊂Dm1(ILPsm)‾; and hence as {a mathematical formula}ILPsm has closed one-to-many-distinguishability, {a mathematical formula}Dm1(ILPb)⊂Dm1(ILPsm). The other results all follow from Corollary 7 and Proposition 23. □
      </paragraph>
      <paragraph>
       Even if two frameworks {a mathematical formula}F1 and {a mathematical formula}F2 both have closed one-to-many-distinguishability, it might not be the case that their combination has closed one-to-many-distinguishability. Example 13 shows, for example, that this is not the case for {a mathematical formula}ILPsm and {a mathematical formula}ILPc. We define first what we mean by combination framework constructed from two given frameworks.
      </paragraph>
      <paragraph label="Definition 16">
       Given two frameworks {a mathematical formula}F1 and {a mathematical formula}F2, the combination framework {a mathematical formula}comb(F1,F2) allows any task {a mathematical formula}〈B,〈1,E1〉〉, where {a mathematical formula}〈B,E1〉 is an {a mathematical formula}F1 task, and any task {a mathematical formula}〈B,〈2,E2〉〉, where {a mathematical formula}〈B,E2〉 is an {a mathematical formula}F2 task.Given any {a mathematical formula}comb(F1,F2) task {a mathematical formula}T=〈B,〈x,E〉〉: {a mathematical formula}ILPcomb(F1,F2)(T)={ILPF1(〈B,E〉)if x=1ILPF2(〈B,E〉)if x=2
      </paragraph>
      <paragraph label="Example 13">
       Consider the frameworks {a mathematical formula}ILPsm and {a mathematical formula}ILPc (both of which have closed one-to-many-distinguishability). Consider the programs {a mathematical formula}B=∅,H={0{p}1.},H1=∅,H2={0{p,q}1.}. {a mathematical formula}〈B,H,H1〉∈D11(ILPsm) (using the task {a mathematical formula}〈B,〈{〈{p},∅〉}〉〉), and {a mathematical formula}〈B,H,H2〉∈D11(ILPc) (using the task {a mathematical formula}〈B,〈∅,{q}〉〉). This shows that both {a mathematical formula}〈B,H,H1〉 and {a mathematical formula}〈B,H,H2〉 are in {a mathematical formula}D11(ILPsm)∪D11(ILPc). Hence by Definition 16 they must be in {a mathematical formula}D11(comb(ILPsm,ILPc)). But using the distinguishability conditions proven in the previous section, it can be seen that neither framework can distinguish H from both {a mathematical formula}H1 and {a mathematical formula}H2. Therefore, {a mathematical formula}〈B,H,{H1,H2}〉∉Dm1(comb(ILPsm,ILPc)). This also means that {a mathematical formula}Dm1(ILPsm)∪Dm1(ILPc) is a strict subset of {a mathematical formula}Dm1(ILPLAS). This is because {a mathematical formula}Dm1(ILPLAS) contains both {a mathematical formula}〈B,H,{H1}〉 and {a mathematical formula}〈B,H,{H2}〉 (as it contains both {a mathematical formula}Dm1(ILPsm) and {a mathematical formula}Dm1(ILPc)) and has closed one-to-many-distinguishability, so must also contain {a mathematical formula}〈B,H,{H1,H2}〉.
      </paragraph>
     </section>
     <section label="5.3">
      <section-title>
       The many-to-many-distinguishability class of a learning framework
      </section-title>
      <paragraph>
       So far, we have considered two main classes to define how general a learning framework is. Firstly, we discussed the one-to-one-distinguishability class, which is made up of tuples {a mathematical formula}〈B,H,H′〉 such that the framework can distinguish H from {a mathematical formula}H′ with respect to B. We showed that this has limitations and cannot separate {a mathematical formula}ILPb and {a mathematical formula}ILPsm even though {a mathematical formula}ILPb is clearly a special case of {a mathematical formula}ILPsm. This motivated upgrading the notion of a one-to-one-distinguishability class, changing the third element of each tuple from a single hypothesis to a set of hypotheses to give the notion of a one-to-many-distinguishability class.
      </paragraph>
      <paragraph>
       This naturally leads to the question of whether it is possible to upgrade generality classes by allowing the second element of the tuple to also be a set of hypotheses. Each tuple would then be of the form {a mathematical formula}〈B,S1,S2〉, where B is a background knowledge, and {a mathematical formula}S1 and {a mathematical formula}S2 are sets of hypotheses. For each tuple in this new class, a framework would be required to have at least one task T with the background knowledge B such that every hypothesis in {a mathematical formula}S1 is an inductive solution of T, and no hypothesis in {a mathematical formula}S2 is an inductive solution of T. Definition 17 formalises this many-to-many-distinguishability class.
      </paragraph>
      <paragraph label="Definition 17">
       The many-to-many-distinguishability class of a learning framework {a mathematical formula}F (denoted {a mathematical formula}Dmm(F)) is the set of all tuples {a mathematical formula}〈B,S1,S2〉, where B is a program and {a mathematical formula}S1 and {a mathematical formula}S2 are sets of hypotheses for which there is a task {a mathematical formula}TF, with background knowledge B, such that {a mathematical formula}S1⊆ILPF(TF) and {a mathematical formula}S2∩ILPF(TF). Given two frameworks, {a mathematical formula}F1 and {a mathematical formula}F2, we say that {a mathematical formula}F1 is at least as (resp. more) {a mathematical formula}Dmm-general than {a mathematical formula}F2 if and only if {a mathematical formula}Dmm(F2)⊆Dmm(F1) (resp. {a mathematical formula}Dmm(F2)⊂Dmm(F1)).
      </paragraph>
      <paragraph>
       We have already seen that for any two frameworks, {a mathematical formula}F1 and {a mathematical formula}F2, {a mathematical formula}F1→srF2⇒Dm1(F1)⊆Dm1(F2)⇒D11(F1)⊆D11(F2). We have also seen that for {a mathematical formula}D11-generality and {a mathematical formula}Dm1-generality, even if there is no corresponding strong reduction from {a mathematical formula}F2 to {a mathematical formula}F1 these subset relations are not necessarily strict. Proposition 24 and Corollary 8 show that {a mathematical formula}Dmm-generality is equivalent to strong reductions.
      </paragraph>
      <paragraph label="Proposition 24">
       For any two learning frameworks{a mathematical formula}F1and{a mathematical formula}F2,{a mathematical formula}F1→srF2⇔Dmm(F1)⊆Dmm(F2).
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="1.">
         Assume that {a mathematical formula}F1→srF2. Let {a mathematical formula}〈B,S1,S2〉 be an arbitrary element of {a mathematical formula}Dmm(F1). By definition of {a mathematical formula}Dmm(F1), there is a task {a mathematical formula}TF1 with background knowledge B such that {a mathematical formula}S1⊆ILPF1(TF1) and {a mathematical formula}S2∩ILPF1(TF1)=∅. Hence, as {a mathematical formula}F1→srF2, there is an {a mathematical formula}F2 task {a mathematical formula}TF2 with background knowledge B such that {a mathematical formula}S1⊆ILPF2(TF2) and {a mathematical formula}S2∩ILPF2(TF2)=∅. Hence {a mathematical formula}〈B,S1,S2〉∈Dmm(F2).
        </list-item>
        <list-item label="2.">
         Assume that {a mathematical formula}Dmm(F1)⊆Dmm(F2). Let {a mathematical formula}TF1 be an arbitrary {a mathematical formula}F1 task. We must show that there is a {a mathematical formula}F2 task with the same background knowledge and the same inductive solutions. Let B be the background knowledge of {a mathematical formula}TF1, {a mathematical formula}S1=ILPF1(TF1) and {a mathematical formula}S2 be the (possibly infinite) set of ASP programs which are not in {a mathematical formula}S1. {a mathematical formula}〈B,S1,S2〉∈Dmm(F1); and hence, {a mathematical formula}〈B,S1,S2〉∈Dmm(F2). Therefore, there must be at least one task {a mathematical formula}TF2 with the background knowledge B such that {a mathematical formula}ILPF2(TF2)=S1. Hence, {a mathematical formula}F1→srF2. □
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Corollary 8">
       For any two learning frameworks{a mathematical formula}F1and{a mathematical formula}F2,{a mathematical formula}F1is more{a mathematical formula}Dmm-general than{a mathematical formula}F2if and only if{a mathematical formula}F2→srF1and{a mathematical formula}F1↛srF2.
      </paragraph>
      <paragraph label="Proposition 25">
       For any two frameworks{a mathematical formula}F1and{a mathematical formula}F2:{a mathematical formula}Dmm(F1)⊆Dmm(F2)⇒Dm1(F1)⊆Dm1(F2).
      </paragraph>
      <paragraph label="Proof">
       Assume that {a mathematical formula}Dmm(F1)⊆Dmm(F2) and let {a mathematical formula}〈B,H,S〉∈Dm1(F1). Then {a mathematical formula}〈B,{H},S〉∈Dmm(F1), and so {a mathematical formula}〈B,{H},S〉∈Dmm(F2). Hence, {a mathematical formula}〈B,H,S〉∈Dm1(F2). □
      </paragraph>
      <paragraph>
       Theorem 7 shows that one framework being more {a mathematical formula}Dm1-general than another implies that it is also more {a mathematical formula}Dmm-general if there is a strong reduction from the second framework to the first.
      </paragraph>
      <paragraph label="Theorem 7">
       For any two frameworks{a mathematical formula}F1and{a mathematical formula}F2, if{a mathematical formula}F1is more{a mathematical formula}Dm1-general than{a mathematical formula}F2and{a mathematical formula}F2→srF1then{a mathematical formula}F1is more{a mathematical formula}Dmm-general than{a mathematical formula}F2.
      </paragraph>
      <paragraph label="Proof">
       Assume that {a mathematical formula}F1 is more {a mathematical formula}Dm1-general than {a mathematical formula}F2 and that {a mathematical formula}F2→srF1. By Proposition 24, {a mathematical formula}F1 is at least as {a mathematical formula}Dmm-general as {a mathematical formula}F2. It remains to show that {a mathematical formula}F2 is not at least as {a mathematical formula}Dmm-general as {a mathematical formula}F1. Assume for contradiction that {a mathematical formula}F2 is at least as {a mathematical formula}Dmm-general as {a mathematical formula}F1. Then by Proposition 25, {a mathematical formula}F2 is at least as {a mathematical formula}Dm1-general as {a mathematical formula}F1, contradicting the fact that {a mathematical formula}F1 is more {a mathematical formula}Dm1-general than {a mathematical formula}F2. □
      </paragraph>
      <paragraph label="Corollary 9">
       Consider the learning frameworks{a mathematical formula}ILPb,{a mathematical formula}ILPc,{a mathematical formula}ILPsm,{a mathematical formula}ILPLAS,{a mathematical formula}ILPLOASand{a mathematical formula}ILPLOAScontext.
      </paragraph>
      <list>
       <list-item>
        {a mathematical formula}Dmm(ILPb)⊂Dmm(ILPsm)⊂Dmm(ILPLAS)⊂Dmm(ILPLOAS)⊂Dmm(ILPLOAScontext)
       </list-item>
       <list-item>
        {a mathematical formula}Dmm(ILPc)⊂Dmm(ILPLAS)
       </list-item>
      </list>
      <paragraph label="Proof">
       Each result follows directly from Theorem 6, Theorem 7 and Proposition 9. □
      </paragraph>
      <paragraph>
       Note that although for each pair of frameworks discussed in this paper, one being more {a mathematical formula}Dm1-general than another implies that it is also more {a mathematical formula}Dmm-general, this result does not hold in general. Example 14 shows such a pair of frameworks.
      </paragraph>
      <paragraph label="Example 14">
       Consider a new learning framework {a mathematical formula}ILPd that takes as examples a pair of sets of atoms {a mathematical formula}E+ and {a mathematical formula}E− such that a hypothesis H is an inductive solution of a task if {a mathematical formula}B∪H has exactly one answer set and this answer set contains all of the {a mathematical formula}E+'s and none of the {a mathematical formula}E−'s. The one-to-one-distinguishability class {a mathematical formula}D11(ILPd)⊆D11(ILPc). This can be seen as follows: assume that {a mathematical formula}〈B,H,H′〉∈D11(ILPd). Then there is a task {a mathematical formula}Td=〈B,〈E+,E−〉〉 such that {a mathematical formula}H∈ILPd(Td) but {a mathematical formula}H′∉ILPd(Td).
       <list>
        {a mathematical formula}AS(B∪H′)=∅Let {a mathematical formula}Tc=〈B,〈E+,E−〉〉. As {a mathematical formula}B∪H has exactly one answer set, and this answer set covers the examples, {a mathematical formula}H∈ILPc(Tc). As {a mathematical formula}AS(B∪H′)=∅, {a mathematical formula}H′∉ILPc(Tc). Hence, {a mathematical formula}〈B,H,H′〉∈D11(ILPc).{a mathematical formula}B∪H′ has exactly one answer set, and this answer set does not cover the examples.Let {a mathematical formula}Tc=〈B,〈E+,E−〉〉. As {a mathematical formula}B∪H has exactly one answer set, and this answer set covers the examples, {a mathematical formula}H∈ILPc(Tc). As {a mathematical formula}B∪H′ has an answer set that does not cover the examples, {a mathematical formula}H′∉ILPc(Tc). Hence, {a mathematical formula}〈B,H,H′〉∈D11(ILPc).{a mathematical formula}B∪H′ has multiple answer sets.There must be at least one answer set {a mathematical formula}A⁎ of {a mathematical formula}B∪H′ that is not an answer set of {a mathematical formula}B∪H (as {a mathematical formula}B∪H only has one answer set). There must either be an atom {a mathematical formula}a∈A⁎ that is not in the unique answer set of {a mathematical formula}B∪H, or an atom a that is not in {a mathematical formula}A⁎, but is in the unique answer set of {a mathematical formula}B∪H. In the first case, let {a mathematical formula}Ec+=∅ and {a mathematical formula}Ec−={a}. In the second case, let {a mathematical formula}Ec+={a} and {a mathematical formula}Ec−=∅. Then let {a mathematical formula}Tc=〈B,〈Ec+,Ec−〉〉. {a mathematical formula}H∈ILPc(Tc) as the only answer set of {a mathematical formula}B∪H covers the examples, whereas {a mathematical formula}H′∉ILPc(Tc) as {a mathematical formula}B∪H′ has at least one answer set that does not cover the examples. Hence, {a mathematical formula}〈B,H,H′〉∈D11(ILPc).{a mathematical formula}ILPc
       </list>
       <paragraph>
        is not, however, more {a mathematical formula}Dmm-general than {a mathematical formula}ILPd. Take, for instance, the tuple {a mathematical formula}t=〈∅,{{heads.},{tails.}},{{1{heads,tails}1.}}〉. The empty set of examples are sufficient for {a mathematical formula}ILPd to distinguish both hypotheses containing facts from the choice rule (as the choice rule has multiple answer sets). However, there is no {a mathematical formula}ILPc task such that both facts are solutions, but the choice rule is not. Hence, {a mathematical formula}t∈Dmm(ILPd) but {a mathematical formula}t∉Dmm(ILPc); and so, {a mathematical formula}ILPc is not at least as {a mathematical formula}Dmm-general as {a mathematical formula}ILPd. In fact, as {a mathematical formula}ILPd is not as {a mathematical formula}Dmm-general as {a mathematical formula}ILPc either, the two have incomparable {a mathematical formula}Dmm-generalities.
       </paragraph>
      </paragraph>
      <paragraph>
       Example 14 shows that {a mathematical formula}Dmm-generality may not be able to compare two frameworks even when there is a clear {a mathematical formula}Dm1-generality relation between the two. In the next section, we discuss relationships between, and the relative merits of using, each measure of generality.
      </paragraph>
     </section>
     <section label="5.4">
      <section-title>
       Discussion
      </section-title>
      <paragraph>
       Table 4 summarises the relationships between the different measures of generality presented in this paper. It shows that equal one-to-one-distinguishability is weaker than equal one-to-many-distinguishability, which is weaker than equal many-to-many-distinguishability. This can be seen from the first section of the table, as equal many-to-many-distinguishability implies equal one-to-many-distinguishability, which implies equal one-to-one-distinguishability, but the converse implications do not hold in general. On the other hand different one-to-one-distinguishability is stronger than different one-to-many-distinguishability, which in turn is stronger than different many-to-many-distinguishability. This means that many-to-many-distinguishability (resp. one-to-many-distinguishability) will be able to “separate” frameworks that one-to-many-distinguishability (resp. one-to-one-distinguishability) can not; but, there are more frameworks that are incomparable under many-to-many-distinguishability (resp. one-to-many-distinguishability) than one-to-many-distinguishability (resp. one-to-one-distinguishability).
      </paragraph>
      <paragraph>
       The different notions of generalities will never be inconsistent, in the sense that one will never say that {a mathematical formula}F1 is more general than {a mathematical formula}F2, while the other says that {a mathematical formula}F2 is more general than {a mathematical formula}F1. It is useful, however, to explain the tasks that the different measures of generality correspond to.
      </paragraph>
      <list>
       <list-item label="1.">
        One-to-one-distinguishability describes how general a framework is at distinguishing one hypothesis from another.
       </list-item>
       <list-item label="2.">
        One-to-many-distinguishability describes how general a framework is at the task of identifying one target hypothesis within a space of unwanted hypotheses.
       </list-item>
       <list-item label="3.">
        Many-to-many-distinguishability describes how general a framework is for the task of identifying a set of target hypotheses – for any background knowledge B and set of hypotheses S, there is a task {a mathematical formula}TF with background knowledge B such that {a mathematical formula}ILPF(TF)=S if and only if {a mathematical formula}〈B,S,S¯〉∈Dmm(F), where {a mathematical formula}S¯ is the (infinite) set of hypotheses which are not in S.
       </list-item>
      </list>
      <paragraph>
       In practice, as ILP usually addresses the task of finding a single target hypothesis from a space of other hypotheses, one-to-many-distinguishability is likely to be the most useful measure; however, one-to-one-distinguishability classes are useful for finding the one-to-many-distinguishability classes of frameworks, and many-to-many-distinguishability is interesting as a theoretical property.
      </paragraph>
      <section label="5.4.1">
       <section-title>
        More general learning frameworks
       </section-title>
       <paragraph>
        We have shown in this section that {a mathematical formula}ILPLOAScontext is more general (under every measure) than any of the other tasks presented for learning under the answer set semantics. The obvious question is whether it is possible to go further and define more general learning tasks.
       </paragraph>
       <paragraph>
        The most {a mathematical formula}D11-general learning task possible would be able to distinguish between any two different ASP programs {a mathematical formula}H1 and {a mathematical formula}H2 with respect to any background knowledge B. This would require the learning task to distinguish between programs which are strongly equivalent, such as {a mathematical formula}{p.q:-p.} and {a mathematical formula}{p:-q.q.}. We would argue that this level of one-to-one-distinguishability is unnecessary as in ILP, we aim to learn programs whose output explains the examples. As two strongly equivalent programs will always have the same output, even when combined with additional programs providing “context”, we can not see any reason for going further under {a mathematical formula}D11-generality. As {a mathematical formula}ILPLOAScontext has closed one-to-many-distinguishability, the same argument can be made for {a mathematical formula}Dm1-generality.
       </paragraph>
       <paragraph>
        One outstanding question is whether it is worth going any further under {a mathematical formula}Dmm-generality. Note that it is possible to define the notion of the closure of many-to-many-distinguishability classes; however, none of the frameworks considered in this paper have closed many-to-many-distinguishability. It is unclear whether having closed many-to-many-distinguishability is a desirable property for a framework. Closed one-to-many-distinguishability means that a framework can distinguish a target hypothesis H from any set of hypotheses S such that it can distinguish H from each element of S: this means that the sets of examples that distinguish H from each element of S can be combined to form a single set of examples, ruling out each element of S. For a framework to have closed many-to-many-distinguishability, however, given two (or more) target hypotheses {a mathematical formula}h1, {a mathematical formula}h2 that can be distinguished from an undesirable hypothesis {a mathematical formula}h3, it would need to be able to find a task which distinguished both {a mathematical formula}h1 and {a mathematical formula}h2 from {a mathematical formula}h3. For example, as both {a mathematical formula}〈∅,{heads.},{1{heads,tails}1.}〉 and {a mathematical formula}〈∅,{tails.},{1{heads,tails}1.}〉 are in {a mathematical formula}D11(ILPLAS), for {a mathematical formula}ILPLAS to have closed many-to-many-distinguishability it would need to be able to find a task with an empty background knowledge that distinguishes both {a mathematical formula}{heads.} and {a mathematical formula}{tails.} from {a mathematical formula}{1{heads,tails}1.}. It is difficult to imagine a scenario, however, where we should learn either the hypothesis that a coin is always heads or always tails, when the choice rule is not a desirable hypothesis.
       </paragraph>
      </section>
      <section label="5.4.2">
       <section-title>
        The generality of noisy frameworks
       </section-title>
       <paragraph>
        As discussed in Section 4.4 some learning systems are able to solve tasks where examples are potentially noisy – in this case, not all examples should necessarily be covered, and there is a trade off between maximising coverage and not over-fitting the examples. One method, used by the XHAIL [42] and ILASP [32] systems is to penalise a hypothesis for each example that is not covered. Examples are given a positive integer penalty, which must be paid if the example is not covered.
       </paragraph>
       <paragraph>
        The three measures of generality presented in this section could be extended to cover the noisy tasks. For instance, in the case of one-to-one-distinguishability we could define the “noisy” one-to-one-distinguishability class of a learning framework as the set of tuples {a mathematical formula}〈B,H1,H2〉, for which there is a set of examples E such that {a mathematical formula}p(H1,E)&lt;p(H2,E), where {a mathematical formula}p(H,E) is the total penalty paid by a hypothesis H (together with the background knowledge B) over the examples E. In fact, we now show that this extended notion of one-to-one-distinguishability class would be equivalent to the standard “non-noisy” one-to-one-distinguishability class.
       </paragraph>
       <paragraph>
        As all penalties are positive, {a mathematical formula}p(H1,E)&lt;p(H2,E) implies that {a mathematical formula}p(H1,E′)&lt;p(H2,E′), where {a mathematical formula}E′ the set of all examples in E that are covered by H. Hence, there is a set of examples {a mathematical formula}E′ such that {a mathematical formula}H1 every example in {a mathematical formula}E′, but {a mathematical formula}H2 does not. This means that any {a mathematical formula}〈B,H1,H2〉 that would be in the “noisy” one-to-one-distinguishability class is in the standard “non-noisy” one-to-one-distinguishability class. Similarly for any tuple {a mathematical formula}〈B,H1,H2〉 in the standard one-to-one-distinguishability class, there is a set of examples E such that {a mathematical formula}H1 covers every example in E, and {a mathematical formula}H2 does not; hence {a mathematical formula}p(H1,E)&lt;p(H2,E), and so {a mathematical formula}〈B,H1,H2〉 would be in the “noisy” one-to-one-distinguishability class.
       </paragraph>
       <paragraph>
        A similar argument holds for the one-to-many-distinguishability class; however, it is worth noting that it does not hold true for the many-to-many-distinguishability class. If we upgrade the many-to-many-distinguishability class in the same way then there are some tuples which are in the “noisy” many-to-many-distinguishability class for a framework, but not in the standard many-to-many-distinguishability class. Take for instance the example discussed in the previous section: {a mathematical formula}〈∅,{{heads.},{tails.}},{1{heads,tails}1.}〉 is not in {a mathematical formula}Dmm(ILPLAS). However, if we consider the {a mathematical formula}ILPLAS examples {a mathematical formula}E=〈∅,{〈{heads},∅〉,〈{tails},∅〉〉, then {a mathematical formula}p({heads.},E)=1, {a mathematical formula}p({tails.},E)=1 and {a mathematical formula}p({1{heads,tails}1.},E)=2, meaning that {a mathematical formula}〈∅,{{heads.},{tails.}},{1{heads,tails}1.}〉 would be in the “noisy” {a mathematical formula}Dmm(ILPLAS).
       </paragraph>
      </section>
     </section>
    </section>
    <section label="6">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      The complexity of {a mathematical formula}ILPb and {a mathematical formula}ILPc for verification and satisfiability were investigated in [28]. However, in that work, the results on satisfiability are for deciding whether or not a task has any solutions with no restrictions on the hypothesis space. This means that for both {a mathematical formula}ILPb and {a mathematical formula}ILPc deciding whether a task is satisfiable is equivalent to checking whether there is a model of B in which the examples are covered (a simpler decision problem). For this reason, the complexity of satisfiability for {a mathematical formula}ILPc in [28] was NP-complete, rather than {a mathematical formula}Σ2P-complete. The complexities given of verification of a hypothesis given in [28] are also different from the ones in this paper, as they consider a different language for {a mathematical formula}B∪H. They consider disjunctive logic programs, whereas we investigated the complexity of learning programs without disjunction. The reason we chose not to consider disjunctive logic programs is that the systems available for ILP under the answer set semantics do not allow disjunction. For example, the systems for {a mathematical formula}ILPb[38], [39] do not allow disjunction, and allowing disjunction would raise the complexity beyond the complexity of the tasks that are actually solved in practice by the existing systems.
     </paragraph>
     <paragraph>
      As discussed in Section 5, the generality of a learning framework has been investigated before. In [45], the author defined generality in terms of reductions – one framework {a mathematical formula}F1 was said to be more general than another framework {a mathematical formula}F2 if and only if {a mathematical formula}F2→rF1 and {a mathematical formula}F1↛rF2. We showed in Section 5 that our final notion of generality (many-to-many-distinguishability) coincides with a similar notion of strong reductions. The difference with strong reductions, as compared to the reductions in [45], is that strong reductions do not allow the background knowledge to be modified as part of the reduction. We showed in Example 8 that {a mathematical formula}ILPb reduces to {a mathematical formula}ILPc, but {a mathematical formula}ILPb does not strongly reduce to {a mathematical formula}ILPc. This is because any reduction from {a mathematical formula}ILPb to {a mathematical formula}ILPc must encode the examples in the background knowledge, which we would argue abuses the purpose of the background knowledge.
     </paragraph>
     <paragraph>
      Aside from the differences in strong reductions and reductions, we discussed in Section 5 that one-to-many-distinguishability is more relevant when comparing the generalities of frameworks with respect to the task of finding a single hypothesis within a space of hypotheses. The reductions of [45] are closer to the notion of many-to-many-distinguishability, because they compare the set of solutions.
     </paragraph>
     <paragraph>
      One key advantage to using our three notions of generality, rather than strong reductions or reductions, is for comparing the relative generalities of frameworks that do not strongly reduce to one another. For instance, we have seen that {a mathematical formula}ILPb and {a mathematical formula}ILPc are incomparable under {a mathematical formula}D-generality, but we can still reason that {a mathematical formula}ILPb is never {a mathematical formula}D-general enough to distinguish a hypothesis containing a constraint from the same hypothesis without the constraint. On the other hand, {a mathematical formula}ILPc may be {a mathematical formula}D-general enough to do so (for example, {a mathematical formula}ILPc can distinguish {a mathematical formula}{:-p.} from ∅ with respect to the background knowledge {a mathematical formula}{0{p}1.}, with the task {a mathematical formula}〈{0{p}1.},〈∅,{p}〉〉).
     </paragraph>
     <section label="6.1">
      <section-title>
       Other learning frameworks
      </section-title>
      <paragraph>
       Traditional ILP aims to learn Prolog style logic programs, often restricted to learning definite programs (with no negation as failure). For the shared subset of the languages learned by these ILP frameworks and the ASP frameworks (definite rules, not including lists), a definite learning task can be expressed as either a brave, or as a cautious task with the same examples as the definite task, and hypothesis space restricted to definite logic programs. As these frameworks do not support features such as choice rules or constraints or negation, and ASP frameworks do not support lists, a comparison of the generality is not very informative. A review of early efforts to extend ILP to learn normal logic programs was presented in [8]. The techniques discussed in [8] that operate under the stable model (or answer set) semantics require that all examples are covered in all stable models (or answer sets). This corresponds to cautious induction.
      </paragraph>
      <paragraph>
       We have already discussed most of the other frameworks for ILP which work under the answer set semantics and shown in sections 4 and 5 how the complexity and generality of these frameworks compare to our own frameworks. In particular, we have shown that although the complexities of our three learning frameworks ({a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext) are the same as cautious induction, there are some learning problems which can be represented in learning from answer sets that cannot be represented in either brave or cautious induction. One example of this is the learning of the rules of Sudoku. This is because brave induction cannot incentivise learning the constraints in the rules of Sudoku, and there are no useful examples that can be given to a cautious learner about the values of cells, since no cell has the same value in every valid Sudoku board.
      </paragraph>
      <paragraph>
       Another early work on learning frameworks under the answer set semantics is Induction from Answer Sets[46]. In the paper, two learning algorithms {a mathematical formula}IASpos and {a mathematical formula}IASneg are presented. The task of {a mathematical formula}IASpos is to learn a hypothesis that cautiously entails a set of examples. This corresponds to the task of cautious induction. {a mathematical formula}IASneg on the other hand aims to find a hypothesis that does not cautiously entail each of a set of examples (i.e. there should be at least one answer set that does not contain each example). This is (in some sense reversed) brave induction. As shown in the paper, in general the {a mathematical formula}IASpos and {a mathematical formula}IASneg procedures are cannot be combined in general to compute a correct hypothesis.
      </paragraph>
      <paragraph>
       Another framework, under the supported model semantics rather than the answer set semantics, is Learning from Interpretation Transitions (LFIT) [47]. In LFIT, the examples are pairs of interpretations {a mathematical formula}〈I,J〉 where J is the set of immediate consequences of I given {a mathematical formula}B∪H. In [24], we presented a mapping from any LFIT task to an {a mathematical formula}ILPLAScontext task. This shows that the complexity of deciding both satisfiability and verification for LFIT is at most {a mathematical formula}Σ2P-complete. The generality, on the other hand would be different to the tasks we have considered, since there are programs that are strongly equivalent under the answer sets semantics that have different supported models. Example 15 demonstrates a pair of such programs, and an example that learning from interpretations could use to distinguish between them.
      </paragraph>
      <paragraph label="Example 15">
       Consider the programs {a mathematical formula}P1 and {a mathematical formula}P2.{a mathematical formula}{a mathematical formula}P1 and {a mathematical formula}P2 are strongly equivalent under the answer set semantics. However, {a mathematical formula}P1 has the supported model {a mathematical formula}{p}, whereas {a mathematical formula}P2 does not. LFIT can distinguish {a mathematical formula}P1 from {a mathematical formula}P2 (with respect to an empty background knowledge) with the example {a mathematical formula}〈{p},{p}〉.
      </paragraph>
      <paragraph>
       Example 15 shows that {a mathematical formula}ILPLOAScontext has a distinguishability class which does not contain LFIT's distinguishability class. Conversely, LFIT cannot have a distinguishability class which contains {a mathematical formula}D11(ILPLOAScontext), as it cannot distinguish hypotheses containing weak constraints from the same hypotheses without the weak constraints. In fact, it does not even contain {a mathematical formula}D11(ILPLAS), as shown in Example 16.
      </paragraph>
      <paragraph label="Example 16">
       Consider the programs {a mathematical formula}P1 and {a mathematical formula}P2.{a mathematical formula} For both programs {a mathematical formula}P1 and {a mathematical formula}P2, the immediate consequences of any interpretation I is the set {a mathematical formula}{p}. This means that no example could possibly distinguish {a mathematical formula}P1 from {a mathematical formula}P2 with respect to an empty background knowledge. Under the answer set semantics, however, {a mathematical formula}P1 has one answer set {a mathematical formula}{p}, but {a mathematical formula}P2 has no answer sets. {a mathematical formula}ILPLAS can therefore distinguish {a mathematical formula}P1 from {a mathematical formula}P2 (with respect to the empty background knowledge), with the positive example {a mathematical formula}〈{p},∅〉.
      </paragraph>
      <paragraph>
       {a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext have different distinguishability classes to LFIT, but none is either more or less {a mathematical formula}D11-general than LFIT. This is an interesting observation, as it demonstrates that even when two frameworks are incomparable under our measures of generality, we can still reason about their individual distinguishability classes and discuss hypotheses which one framework is powerful enough to distinguish between and another is not. For instance, {a mathematical formula}ILPLAS cannot distinguish between any two hypotheses that are strongly equivalent under the answer set semantics, but Example 15 shows that there are some cases where {a mathematical formula}ILPLFIT can.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Relation to probabilistic ILP
      </section-title>
      <paragraph>
       One of the advantages to learning ASP programs rather than Prolog programs is that ASP allows the modeling of non-determinism, either through unstratified negation or through choice rules. The latter can be seen in the coin examples throughout the paper, where we have shown that our {a mathematical formula}ILPLAS framework can learn that a coin can be either heads or tails, but not both.
      </paragraph>
      <paragraph>
       Another method for achieving non-determinism in ILP is by adding probabilities. Probabilistic Inductive Logic Programming [48] is a combination of ILP with probabilistic reasoning. Its aim is to learn a logic program that is annotated with probabilities. The task of PILP is often divided into structure learning, where the underlying logic program is learned, and parameter estimation or weight learning, where the probabilities are learned. A key difference between {a mathematical formula}ILPLAS and PILP is that while both aim to learn programs which are non-deterministic, {a mathematical formula}ILPLAS aims to learn programs whose answer sets capture the set of possibilities, whereas PILP aims to learn a probability distribution over these possibilities.
      </paragraph>
      <paragraph>
       Although there has been significant progress in the field of PILP [25], [49], [50], [51], [26] for learning annotated Prolog programs, PILP under the answer set semantics is still relatively young, and thus, there are few approaches. PrASP [52], [53], [27] considers the problem of weight learning, and in fact uses a similar example of learning about coins. This example illustrates the difference between weight learning and standard ILP. In ILP our task is to learn that there are exactly two possibilities (heads and tails); whereas in weight learning, the goal is to estimate probabilities of each possibility. PROBXHAIL [54] does attempt to combine structure learning and weight learning, but can only learn definite logic programs.
      </paragraph>
      <paragraph>
       While the coin example used in this paper may be viewed as inherently probabilistic, there are situations in practice where we may wish to learn non-deterministic programs without considering probability; for instance, in policy learning. A policy may well permit many valid actions in a given scenario, and impose some constraints on these actions. The task is to learn a program whose answer sets reflect the set of valid options, rather than to estimate the probability of each action being taken.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      In this paper we have investigated the complexity and generality of the state of the art frameworks for learning answer set programs. We have shown, for the two decision problems of verification that a hypothesis is an inductive solution of a task and deciding whether a given task is satisfiable, that brave induction ({a mathematical formula}ILPb) and induction of stable models ({a mathematical formula}ILPsm) have the same complexities, and that cautious induction ({a mathematical formula}ILPc), learning from answer sets ({a mathematical formula}ILPLAS), learning from ordered answer sets ({a mathematical formula}ILPLOAS) and context dependent learning from ordered answer sets ({a mathematical formula}ILPLOAScontext) also have the same complexities as each other, but higher than {a mathematical formula}ILPb and {a mathematical formula}ILPsm. Studying the complexity of decision problems for the learning frameworks is important, as it gives a sense of the price paid for choosing a particular framework. In contrast, generality is important, as it shows the advantages of choosing one framework over another, by specifying which hypotheses can be learned by each framework. When using ILP in practice, a trade off must be made between the complexity and generality of the framework. The generality classes presented in this paper can inform this decision, as it is likely to be influenced by the class of programs that must be learned.
     </paragraph>
     <paragraph>
      We have introduced three new measures of generality ({a mathematical formula}D11-generality, {a mathematical formula}Dm1-generality and {a mathematical formula}Dmm-generality), and shown that, both under our own measures of generality, and by using the concept of strong reductions, there is an ordering of the generalities of the frameworks considered in this paper. Although {a mathematical formula}ILPc, {a mathematical formula}ILPLAS, {a mathematical formula}ILPLOAS and {a mathematical formula}ILPLOAScontext have the same computational complexities, {a mathematical formula}ILPc is less general than {a mathematical formula}ILPLAS, which is less general than {a mathematical formula}ILPLOAS, which is less general than {a mathematical formula}ILPLOAScontext, under each measure of generality. This ordering could have been seen using strong reductions, but our measures go further. They allow us to reason about why one framework is more {a mathematical formula}D11-general than another, for example, by studying the class of tuples which are in one framework's distinguishability class, but not the others. They also allow us to discuss the generalities of frameworks which are incomparable under strong reductions; for example, there is no strong reduction from {a mathematical formula}ILPc to {a mathematical formula}ILPb, or from {a mathematical formula}ILPb to {a mathematical formula}ILPc. Our measures allow us to show, however, that {a mathematical formula}ILPb is not {a mathematical formula}D11-general enough to distinguish a hypothesis containing a constraint from the same program without the constraint, but in some cases {a mathematical formula}ILPc is {a mathematical formula}D11-general enough to do so.
     </paragraph>
     <paragraph>
      In this paper, most of the results we have presented have addressed non-noisy learning frameworks. In general our ILASP systems do support noise, by allowing examples to be labeled with a penalty. In this case, ILASP searches for a hypothesis that minimises the sum {a mathematical formula}|H|+p(H,E), where {a mathematical formula}p(H,E) is the sum of all examples in a set E that are not covered by a hypothesis H. Such a hypothesis is called an optimal solution. For the two decision problems of verification and satisfiability, we have shown that the complexity results are unaffected. In current work we are investigating whether the complexities of the non-noisy frameworks and the noisy frameworks differ for the decision problem of verifying that a hypothesis is an optimal solution of a given task. In future work, we also hope to “upgrade” the propositional complexity results presented in this paper to apply to the learning of first order answer set programs.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Proofs
     </section-title>
     <section label="A.1">
      Proofs from Section 4
      <paragraph>
       In this section, we present the proofs that were omitted from Section 4. In some of the proofs, we make use of predicate symbols to avoid continually introducing new atoms and to aid readability. As this section is restricted to propositional programs, any first order atom should be interpreted as a new propositional atom.
      </paragraph>
      <paragraph label="Proposition 1">
       <list>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPbreduces polynomially to the corresponding{a mathematical formula}ILPsmdecision problem.
        </list-item>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPsmreduces polynomially to the corresponding{a mathematical formula}ILPbdecision problem.
        </list-item>
       </list>
       <list>
        <list-item label="1.">
         Let {a mathematical formula}Tb=〈B,SM,〈E+,E−〉〉 be any arbitrary {a mathematical formula}ILPb task.Consider the task {a mathematical formula}Tsm=〈B,SM,〈{〈E+,E−〉}〉〉. ∀H, {a mathematical formula}H∈ILPsm(Tsm) if and only if {a mathematical formula}H∈ILPb(Tb) and hence deciding verification for {a mathematical formula}ILPb reduces polynomially to deciding verification for {a mathematical formula}ILPsm. Similarly, as {a mathematical formula}ILPsm(Tsm)=ILPb(Tb), {a mathematical formula}Tsm is satisfiable if and only if {a mathematical formula}Tb is satisfiable; hence, deciding satisfiability for {a mathematical formula}ILPb reduces to deciding satisfiability for {a mathematical formula}ILPsm.
        </list-item>
        <list-item label="2.">
         Let {a mathematical formula}Tsm=〈B,SM,〈E〉〉 be any arbitrary {a mathematical formula}ILPsm task.Let {a mathematical formula}n=|E|, where {a mathematical formula}E={e1,…,en}.For each integer i from 1 to n, let {a mathematical formula}fi be a function which maps each atom a in {a mathematical formula}B∪SM to a new atom {a mathematical formula}ai. We also extend this notation to work on sets of atoms and rules (and parts of rules) by replacing each atom a in the set or rule with {a mathematical formula}fi(a).For each rule {a mathematical formula}R∈SM, define a new atom {a mathematical formula}in_hR.Consider the task {a mathematical formula}Tb=〈Bb,SMb,〈E+,E−〉〉 where the components of the task are as follows ({a mathematical formula}append(R,a) is the rule R with the atom a appended to the body).{a mathematical formula} For any solution H of {a mathematical formula}Tb, define {a mathematical formula}g(H) to be {a mathematical formula}{R|in_hR∈H}. We now show that {a mathematical formula}ILPsm(Tsm)={g(H′)|H′∈ILPb(Tb)}.Assume {a mathematical formula}H∈ILPsm(Tsm)
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proposition 2">
       <list>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPcreduces polynomially to the corresponding{a mathematical formula}ILPLASdecision problem.
        </list-item>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPLASreduces polynomially to the corresponding{a mathematical formula}ILPLOAScontextdecision problem.
        </list-item>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPLOAScontextreduces polynomially to the corresponding{a mathematical formula}ILPLOASdecision problem.
        </list-item>
        <list-item>
         Deciding both verification and satisfiability for{a mathematical formula}ILPLOASreduces polynomially to the corresponding{a mathematical formula}ILPLASsdecision problem.
        </list-item>
       </list>
       <list>
        <list-item label="1.">
         Let {a mathematical formula}Tc be any {a mathematical formula}ILPc task {a mathematical formula}〈B,SM,〈E+,E−〉〉.Consider the {a mathematical formula}ILPLAS task {a mathematical formula}TLAS=〈B,SM,〈{〈∅,∅〉},{〈∅,{e+}〉|e+∈E+}∪{〈{e−},∅〉|e−∈E−}〉〉.By the definition of {a mathematical formula}ILPLAS, {a mathematical formula}H∈ILPLAS if and only if {a mathematical formula}H⊆SM; {a mathematical formula}∃A∈AS(B∪H) such that A extends {a mathematical formula}〈∅,∅〉; {a mathematical formula}∀e+∈E+, {a mathematical formula}∄A∈AS(B∪H) such that A extends {a mathematical formula}〈∅,e+〉; and finally, {a mathematical formula}∀e−∈E−, {a mathematical formula}∄A∈AS(B∪H) such that A extends {a mathematical formula}〈e−,∅〉.This is true if and only if {a mathematical formula}H⊆SM, {a mathematical formula}B∪H is satisfiable, {a mathematical formula}∀e+∈E+{a mathematical formula}∀A∈AS(B∪H), {a mathematical formula}e+∈A and {a mathematical formula}∀e−∈E−{a mathematical formula}∀A∈AS(B∪H){a mathematical formula}e−∉A. This is the definition of H being a member of {a mathematical formula}ILPc(Tc); hence, {a mathematical formula}ILPc(Tc)=ILPLAS(TLAS).As {a mathematical formula}ILPc(Tc)=ILPLAS(TLAS), {a mathematical formula}Tc is satisfiable if and only if {a mathematical formula}TLAS is satisfiable. Hence deciding the satisfiability of an {a mathematical formula}ILPc task can be reduced to deciding the satisfiability of an {a mathematical formula}ILPLAS task in polynomial time. Similarly, for any hypothesis H, {a mathematical formula}H∈ILPc(Tc) if and only if {a mathematical formula}H∈ILPLAS(TLAS); and so deciding verification for {a mathematical formula}ILPc reduces polynomially to deciding verification for {a mathematical formula}ILPLAS.
        </list-item>
        <list-item label="2.">
         Let {a mathematical formula}TLAS be any {a mathematical formula}ILPLAS task {a mathematical formula}〈B,SM,〈E+,E−〉〉. Consider the {a mathematical formula}ILPLOAS task {a mathematical formula}TLOAS=〈B,SM,〈E+,E−,∅,∅〉〉.{a mathematical formula}ILPLAS(TLAS)=ILPLOAS(TLOAS) and hence, {a mathematical formula}TLAS is satisfiable if and only if {a mathematical formula}TLOAS is satisfiable. Hence deciding the satisfiability of {a mathematical formula}ILPLAS reduces polynomially to deciding satisfiability for {a mathematical formula}ILPLOAS. Similarly, for any hypothesis H, {a mathematical formula}H∈ILPLAS(TLAS) if and only if {a mathematical formula}H∈ILPLOAS(TLOAS); and so deciding verification for {a mathematical formula}ILPLAS reduces polynomially to deciding verification for {a mathematical formula}ILPLOAS.
        </list-item>
        <list-item label="3.">
         In [24], we presented a mapping from any {a mathematical formula}ILPLOAScontext task to an {a mathematical formula}ILPLOAS task. The correctness of this mapping is proven in Theorem 1 of [24]. Given any {a mathematical formula}ILPLOAScontext task, we can decide its satisfiability by using this mapping and checking the satisfiability of the resulting {a mathematical formula}ILPLOAS task. Similarly, given any hypothesis and {a mathematical formula}ILPLOAScontext task, we can verify that the hypothesis is an inductive solution of the task by using the mapping. Hence, both satisfiability and verification for {a mathematical formula}ILPLOAScontext reduce to satisfiability and verification (respectively), for {a mathematical formula}ILPLOAS.
        </list-item>
        <list-item label="4.">
         We do this by translating an arbitrary {a mathematical formula}ILPLOAS task {a mathematical formula}TLOAS=〈B,SM,〈E+,E−,Ob,Oc〉〉 to an {a mathematical formula}ILPLASs task. Before we do this, we define several new atoms used in our meta representation.For {a mathematical formula}i∈{1,2}, let {a mathematical formula}fi be a function which maps each atom a in {a mathematical formula}B∪SM to a new atom {a mathematical formula}ai. We also extend this notation to work on sets of atoms and rules (and parts of rules) by replacing each atom a in the set or rule with {a mathematical formula}fi(a).For each rule {a mathematical formula}R∈SM, define a new atom {a mathematical formula}in_hR.For each weak constraint {a mathematical formula}W∈B∪SM let {a mathematical formula}id1(W) and {a mathematical formula}id2(W) be two new (propositional) atoms and let {a mathematical formula}wt(W) be the weight of W and {a mathematical formula}priority(W) be the priority level of W.For any two terms {a mathematical formula}t1 and {a mathematical formula}t2, {a mathematical formula}dominates(t1,t2) is defined as below.{a mathematical formula} Consider the task {a mathematical formula}TLASs=〈B′,SM′,〈E+′,E−′〉〉 where the individual components are defined below. For the positive and negative examples, it is a simple reification so that the examples relate to the new {a mathematical formula}B′ and {a mathematical formula}SM′. The brave orderings are mapped to positive examples which can only be covered by a hypothesis H if {a mathematical formula}B∪H bravely respects the ordering example. For any hypothesis {a mathematical formula}H′∈SM′, the {a mathematical formula}f1 and {a mathematical formula}f2 in a single answer set of {a mathematical formula}B′∪H′ represent two answer sets of {a mathematical formula}B∪H where H is the hypothesis in {a mathematical formula}SM corresponding to {a mathematical formula}H′. Similarly, cautious orderings are mapped to negative examples, such that there is an answer set of {a mathematical formula}B′∪H′ which extends the example if there is a pair of answer sets of the corresponding {a mathematical formula}B∪H which are ordered incorrectly (i.e. if {a mathematical formula}B∪H does not cautiously respect the ordering).{a mathematical formula} By using the splitting set theorem [35], it can be shown that for any {a mathematical formula}H′∈SM′:{a mathematical formula} Hence, as the rules in {a mathematical formula}dominates(t1,t2) describe exactly the behaviour of the weak constraints in {a mathematical formula}B∪H for two answer sets (with {a mathematical formula}dom(t1,t2) being true if and only if the first answer set dominates the second):{a mathematical formula}AS(B′∪H′)={A′|A=f1(A1)∪f2(A2),A1,A2∈AS(B∪H)}, where {a mathematical formula}A′ is A augmented with dom and {a mathematical formula}dom(1,2) when {a mathematical formula}A1 dominates {a mathematical formula}A2 and dom and {a mathematical formula}dom(2,1) when {a mathematical formula}A2 dominates {a mathematical formula}A1.For any hypothesis {a mathematical formula}H′∈SM′, let H be the corresponding hypothesis in {a mathematical formula}SM. The answer sets of {a mathematical formula}B′∪H′ correspond to the pairs of answer sets of {a mathematical formula}B∪H.Each positive example {a mathematical formula}e+∈E+ is mapped to an example in {a mathematical formula}E+′ ensuring that at least one of the pairs of answer sets' first answer set covers {a mathematical formula}e+. Note that as each answer set of {a mathematical formula}B∪H must be the first element of one of these pairs at least once, this is true if and only if {a mathematical formula}B∪H covers each positive example.Similarly each negative example {a mathematical formula}e−∈E− is mapped to an example in {a mathematical formula}E−′ ensuring that none of the pairs of answer sets' first answer set covers {a mathematical formula}e−. This is true if and only if {a mathematical formula}B∪H does not cover any negative examples.Each brave ordering example {a mathematical formula}〈e1,e2,op〉∈Ob is mapped to a positive example ensuring that there is a pair of answer sets {a mathematical formula}〈A1,A2〉 of {a mathematical formula}B∪H such that {a mathematical formula}A1 covers {a mathematical formula}e1, {a mathematical formula}A2 covers {a mathematical formula}e2 and {a mathematical formula}〈A1,A2,op〉∈ord(B∪H). This is true if and only if {a mathematical formula}B∪H bravely respects the ordering example.Each cautious ordering example {a mathematical formula}〈e1,e2,op〉∈Oc is mapped to a negative example ensuring that there is no pair of answer sets {a mathematical formula}〈A1,A2〉 of {a mathematical formula}B∪H such that {a mathematical formula}A1 covers {a mathematical formula}e1, {a mathematical formula}A2 covers {a mathematical formula}e2 and {a mathematical formula}〈A1,A2,op〉∉ord(B∪H). This is true if and only if {a mathematical formula}B∪H cautiously respects the ordering example.Hence, {a mathematical formula}H′ is an inductive solution of {a mathematical formula}ILPLASs(〈B′,SM′,〈E+′,E−′〉〉) if and only if H is an inductive solution of {a mathematical formula}ILPLOAS(〈B,SM,〈E+,E−,Ob,Oc〉〉).This means that we can check the satisfiability of any {a mathematical formula}ILPLOAS task (and similarly verify a solution) by mapping the task to an {a mathematical formula}ILPLASs task as above. Note that this is a well defined {a mathematical formula}ILPLASs task as B contains only stratified aggregates.As this mapping is polynomial in size of the original task, this means that verification and satisfiability for {a mathematical formula}ILPLOAS each reduces polynomially to the corresponding decision problem for {a mathematical formula}ILPLASs. □
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proof">
       Verifying whether a given H is an inductive solution of a general{a mathematical formula}ILPbtask is NP-complete.Let {a mathematical formula}Tb be any {a mathematical formula}ILPb task {a mathematical formula}〈B,SM,〈E+,E−〉〉. For any {a mathematical formula}H⊆SM, {a mathematical formula}H∈ILPb(Tb) if and only if {a mathematical formula}B∪H∪{:-note+.|e+∈E+}∪{:-e−.|e−∈E−} is satisfiable. As deciding the satisfiability of this program is NP-complete ({a mathematical formula}B∪H contains only normal rules, choice rules and constraints), and the program can be constructed in polynomial time, this means that deciding verification for {a mathematical formula}ILPb is in NP.It remains to show that deciding verification is NP-hard. We do this by showing that deciding satisfiability for any ASP program P containing normal rules, choice rules and constraints can be reduced polynomially to deciding verification for an {a mathematical formula}ILPb task. Consider the {a mathematical formula}ILPb task {a mathematical formula}Tb=〈P,∅,〈∅,∅〉〉. Let {a mathematical formula}H=∅. {a mathematical formula}H∈ILPb(Tb) if and only if there is an answer set of {a mathematical formula}P∪H, and hence, if and only if P is satisfiable. □
      </paragraph>
      <paragraph label="Proposition 4">
       Deciding the satisfiability of a general{a mathematical formula}ILPbtask is NP-complete.
      </paragraph>
      <paragraph label="Proof">
       First we will show that deciding the satisfiability of a general {a mathematical formula}ILPb task is in NP. We do this by mapping an arbitrary task {a mathematical formula}T=〈B,SM,〈E+,E−〉〉 to an ASP program whose answer sets can be mapped to the solutions of T. This program will be satisfiable if and only if T is satisfiable and as the program is aggregate stratified, checking whether the program is satisfiable is in NP. Hence, if we can construct such a program then we will have proved that deciding satisfiability for {a mathematical formula}ILPb is in NP.For each {a mathematical formula}Ri∈SM we define a new atom {a mathematical formula}in_hRi. Also, let {a mathematical formula}meta(Ri) be the rule {a mathematical formula}Ri with the additional atom {a mathematical formula}in_hRi added to the body.We define the meta encoding {a mathematical formula}Tmeta as follows:{a mathematical formula}For any answer set A, let {a mathematical formula}M−1(A)={Ri|Ri∈SM,in_hRi∈A}.{a mathematical formula}A∈AS(Tmeta) if and only if {a mathematical formula}(A\{in_hRi|Ri∈SM})∈AS(B∪M−1(A)∪{:-note.|e∈E+}∪{:-e.|e∈E−}). (This can be seen by using the splitting set theorem, with {a mathematical formula}{in_hRi|Ri∈SM} as the splitting set.)Hence {a mathematical formula}A∈AS(Tmeta) if and only if {a mathematical formula}∃H⊆SM such that {a mathematical formula}H=M−1(A), {a mathematical formula}(A\{in_hRi|Ri∈SM})∈AS(B∪H) and A respects the examples.Hence {a mathematical formula}Tmeta is satisfiable if and only if {a mathematical formula}∃H⊆SM such that {a mathematical formula}∃A∈AS(B∪H) such that A respects the examples. This is the case if and only if T is satisfiable.It remains to show that deciding the satisfiability of a general {a mathematical formula}ILPb task is NP-hard. Deciding the satisfiability of a normal logic program is NP-hard, so demonstrating that a deciding the satisfiability of a normal program P can be mapped to a {a mathematical formula}ILPb task is sufficient.Let P be any normal logic program. Let T be the {a mathematical formula}ILPb task {a mathematical formula}〈P,∅,〈∅,∅〉〉. T is satisfiable if and only if {a mathematical formula}∃H⊆∅ such that {a mathematical formula}∃A∈AS(P∪H) such that {a mathematical formula}∅⊆A and {a mathematical formula}A∩∅=∅. This is true if and only if P is satisfiable.Hence, deciding the satisfiability of a general {a mathematical formula}ILPb task is NP-complete. □
      </paragraph>
      <paragraph label="Proposition 5">
       Deciding verification for{a mathematical formula}ILPLASsis a member of DP.
      </paragraph>
      <paragraph label="Proof">
       Checking whether H is an inductive solution of an {a mathematical formula}ILPLASs task {a mathematical formula}T=〈B,SM,〈E+,E−〉〉 can be achieved by mapping T to two aggregate stratified ASP programs {a mathematical formula}P+ and {a mathematical formula}P−, such that {a mathematical formula}H∈ILPLAS(T) if and only if {a mathematical formula}P+ bravely entails an atom and {a mathematical formula}P− cautiously an atom.
       <list>
        Let n be the integer {a mathematical formula}|E+|.For any integer {a mathematical formula}i∈[1,n], let {a mathematical formula}fi be a function mapping the atoms a in {a mathematical formula}B∪H to new atoms {a mathematical formula}ai. We extend the notation to allow {a mathematical formula}fi to act on ASP programs (substituting all atoms in the program).Let {a mathematical formula}P+ be the program:{a mathematical formula}{a mathematical formula}P+ can be split into n sub programs {a mathematical formula}P1…Pn where each program {a mathematical formula}Pi contains the rules containing the atoms generated by {a mathematical formula}fi, plus the rule for {a mathematical formula}covered(i).As the atoms in each sub program are disjoint from the atoms of all other subprograms, {a mathematical formula}AS(P+)={A1∪…∪An|A1∈AS(P1),…,An∈AS(Pn)}. (This follows from applying the splitting set theorem {a mathematical formula}n−1 times).For each {a mathematical formula}i∈[1,n], {a mathematical formula}Pi⊨bcovered(i) if and only if {a mathematical formula}∃A∈B∪H such that A extends {a mathematical formula}ei (where {a mathematical formula}ei is the {a mathematical formula}ith positive example). Hence {a mathematical formula}P+∪{covered:-covered(1),…,covered(n).}⊨bcovered if and only if all the positive examples are covered. Therefore, checking whether all the positive examples are covered is in NP by Corollary 1.As checking that {a mathematical formula}H⊆SM can be done in polynomial time, this means that checking both that {a mathematical formula}H∈SM and all the positive examples are covered is in NP.Let {a mathematical formula}P− be the program:{a mathematical formula}{a mathematical formula}P−⊨ccovered if and only if {a mathematical formula}∄A∈AS(B∪H) such that {a mathematical formula}∃e−∈E− such that A extends {a mathematical formula}e−. Hence checking that all negative examples are covered is in co-NP by Lemma 1.Hence as
       </list>
       <paragraph label="Proposition 6">
        {a mathematical formula}H∈ILPLASs(T) if and only if {a mathematical formula}H⊆SM, all the positive examples are covered and all the negative examples are covered, verifying that {a mathematical formula}H∈ILPLASs(T) can be reduced to checking one problem in NP and another problem in co-NP. This means that verifying a hypothesis is a solution of an {a mathematical formula}ILPLASs task is in DP. □Deciding verification for{a mathematical formula}ILPcis DP-hard.
       </paragraph>
      </paragraph>
      <paragraph label="Proof">
       To prove that verification that a hypothesis is a solution of an {a mathematical formula}ILPc task is DP-hard, we must prove that any problem in DP can be reduced to the verification task.Let D be any arbitrary decision problem which is in DP.By the definition of DP, this is the case if and only if there exist two decision problems {a mathematical formula}D1 and {a mathematical formula}D2 such that {a mathematical formula}D1 is in NP, {a mathematical formula}D2 is in co-NP and D returns yes if and only if both {a mathematical formula}D1 and {a mathematical formula}D2 return yes.By Lemma 1 and Corollary 1, this is the case if and only if there are two programs {a mathematical formula}P1 and {a mathematical formula}P2 and two atoms {a mathematical formula}a1 and {a mathematical formula}a2 such that both {a mathematical formula}P1⊨ba1 and {a mathematical formula}P2⊨ca2 if and only if D returns yes. Without loss of generality we can assume that the atoms in {a mathematical formula}P1 (together with {a mathematical formula}a1) are disjoint from the atoms in {a mathematical formula}P2 (together with {a mathematical formula}a2).Take {a mathematical formula}Tc to be the {a mathematical formula}ILPc task {a mathematical formula}〈B,SM,〈E+,E−〉〉, where the individual components of the task are defined as follows:
       <list>
        {a mathematical formula}B=P1∪append(P2,a3)∪{:-nota1.0{a3}1.a2:-nota3.} (where we assume {a mathematical formula}a3 to be a new atom and {a mathematical formula}append(P,a) to add the atom a to the body of all rules in P){a mathematical formula}SM=∅{a mathematical formula}E+={a2}{a mathematical formula}E−=∅{a mathematical formula}∅∈ILPc(Tc)
       </list>
       <paragraph>
        if and only if {a mathematical formula}(P1∪{:-nota1.}) is satisfiable and {a mathematical formula}append(P2,a3)∪{0{a3}1.a2:-nota3.}⊨ca2. This is the case as the two subprograms {a mathematical formula}P1∪{:-nota1.} and {a mathematical formula}append(P2,a3)∪{0{a3}1.a2:-nota3.} are disjoint, and the latter is guaranteed to be satisfiable (it will always have the answer set {a mathematical formula}{a2}).Hence {a mathematical formula}∅∈ILPc(Tc) if and only if {a mathematical formula}P1⊨ba1, and {a mathematical formula}P2⊨ca2. But this is the case if and only if D returns yes.Hence any problem in DP can be reduced to verifying that a hypothesis is an inductive solution of an {a mathematical formula}ILPc task.Hence verification for {a mathematical formula}ILPc is DP-hard. □
       </paragraph>
      </paragraph>
      <paragraph label="Proof">
       Deciding satisfiability for{a mathematical formula}ILPLASsis in{a mathematical formula}Σ2P.Given an {a mathematical formula}ILPLASs task {a mathematical formula}T=〈B,SM,〈E+,E−〉〉, we show that a non-deterministic Turing Machine with access to an NP oracle could check satisfiability of T in polynomial time.A non-deterministic Turing Machine can have {a mathematical formula}|SM| choices to make (corresponding to selecting each rule as part of the hypothesis). This hypothesis can then be verified in polynomial time using an NP oracle, with two queries similar to the NP query and (the complement of) the co-NP query in Proposition 5, answering yes if and only if the first query returned yes and the second query returned no.Such a Turing Machine would terminate answering yes if and only if the task is satisfiable (as there is a path through the Turing Machine which answers yes if and only if there is an hypothesis in {a mathematical formula}SM which is an inductive solution of the task).Hence, deciding the existence of a solution for an {a mathematical formula}ILPLASs task is in {a mathematical formula}Σ2P. □
      </paragraph>
      <paragraph label="Proposition 8">
       Deciding satisfiability for{a mathematical formula}ILPcis{a mathematical formula}Σ2P-hard.
      </paragraph>
      <paragraph label="Proof">
       Hence, deciding whether a disjunctive logic program is satisfiable can in general be mapped to the decision problem of checking the existence of solutions of a learning from answer sets task.Therefore, deciding the existence of solutions of a ground {a mathematical formula}ILPc task is {a mathematical formula}Σ2P-hard. □
      </paragraph>
     </section>
     <section label="A.2">
      Proofs from Section 5
      <paragraph label="Proposition 12">
       For any programs{a mathematical formula}P1and{a mathematical formula}P2,{a mathematical formula}Eb(P1)⊆Eb(P2)if and only if{a mathematical formula}AS(P1)⊆AS(P2).
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="•">
         Assume that {a mathematical formula}AS(P1)⊆AS(P2){a mathematical formula}Assume {a mathematical formula}c=i1∧…∧im,∧note1,…,noten∈Eb(P1). Then there must be an answer set A of {a mathematical formula}P1 which contains all of the i's and none of the e's. Hence, there is also such an answer set of {a mathematical formula}P2 which contains all of the i's and none of the e's. Hence, {a mathematical formula}c∈Eb(P2).
        </list-item>
        <list-item label="•">
         Conversely, assume that {a mathematical formula}Eb(P1)⊆Eb(P2). Let {a mathematical formula}A∈AS(P1), we must show that {a mathematical formula}A∈AS(P2).Let {a mathematical formula}L be the set {a mathematical formula}HBP1∪HBP2.As {a mathematical formula}A∈AS(P1), {a mathematical formula}c=i1∧…∧im,∧note1,…,noten∈Eb(P1), where the i's are the set of atoms in A and the e's are the set of atoms in {a mathematical formula}L\A. As {a mathematical formula}c∈Eb(P1), {a mathematical formula}c∈Eb(P2) and hence there is an answer set {a mathematical formula}A′ of {a mathematical formula}P2 which contains each {a mathematical formula}i∈A but no atom {a mathematical formula}e∈L\A, and hence as {a mathematical formula}HBP2⊆L, {a mathematical formula}A′=A. Hence {a mathematical formula}A∈AS(P2). □
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proposition 13">
       {a mathematical formula}D11(ILPb)={〈B,H1,H2〉|AS(B∪H1)⊈AS(B∪H2)}.
      </paragraph>
      <paragraph label="Proof">
       We prove this by showing that {a mathematical formula}D11(ILPb)={〈B,H1,H2〉|Eb(B∪H1)⊈Eb(B∪H2)}, which is equal to the set {a mathematical formula}{〈B,H1,H2〉|AS(B∪H1)⊈AS(B∪H2)} by Proposition 12.
      </paragraph>
      <list>
       <list-item label="•">
        We first show that if {a mathematical formula}〈B,H1,H2〉∈D11(ILPb) then there must be a conjunction in {a mathematical formula}Eb(B∪H1) that is not in {a mathematical formula}Eb(B∪H2).As {a mathematical formula}〈B,H1,H2〉∈D11(ILPb), there is an {a mathematical formula}ILPb task {a mathematical formula}T=〈B,〈{i1,…,im},{e1,…,en}〉〉 such that {a mathematical formula}H1∈ILPb(T) and {a mathematical formula}H2∉ILPb(T).Hence, there must be an answer set of {a mathematical formula}B∪H1 such that {a mathematical formula}{i1,…,im}⊆A and {a mathematical formula}{e1,…,em}∩A=∅, but no such answer set of {a mathematical formula}B∪H2.Hence the conjunction {a mathematical formula}c=i1∧…∧im∧note1∧…∧noten∈Eb(B∪H1) but {a mathematical formula}c∉Eb(B∪H2).
       </list-item>
       <list-item label="•">
        Next we show that if there exists a conjunction {a mathematical formula}c=i1∧…∧im∧note1∧…∧noten such that {a mathematical formula}c∈Eb(B∪H1) but {a mathematical formula}c∉Eb(B∪H2), then {a mathematical formula}〈B,H1,H2〉∈D11(ILPb).Assume that there is such a conjunction c. Then {a mathematical formula}B∪H1 has an answer set that extends {a mathematical formula}〈{i1,…,im},{e1,…,en}〉 and {a mathematical formula}B∪H2 does not. Hence {a mathematical formula}H1∈ILPb(〈B,〈{i1,…,im},{e1,…,en}〉〉) but {a mathematical formula}H2∉ILPb(〈B,〈{i1,…,im}, {a mathematical formula}{e1,…,en}〉〉). So {a mathematical formula}〈B,H1,H2〉∈D11(ILPb). □
       </list-item>
      </list>
      <paragraph label="Proof">
       {a mathematical formula}D11(ILPb)=D11(ILPsm).
      </paragraph>
      <list>
       <list-item label="•">
        First we show that {a mathematical formula}D11(ILPb)⊆D11(ILPsm). Assume {a mathematical formula}〈B,H1,H2〉∈D11(ILPb). Then there is a task {a mathematical formula}Tb=〈B,〈E+,E−〉〉 such that {a mathematical formula}H1∈ILPb(Tb) and {a mathematical formula}H2∉ILPb(Tb). Let {a mathematical formula}Tsm=〈B,{〈E+,E−〉}〉. {a mathematical formula}H1∈ILPsm(Tsm) but {a mathematical formula}H2∉ILPsm(Tsm). Hence, {a mathematical formula}〈B,H1,H2〉∈D11(ILPsm).
       </list-item>
       <list-item label="•">
        Next we show that {a mathematical formula}D11(ILPb)⊇D11(ILPsm). Assume {a mathematical formula}〈B,H1,H2〉∈D11(ILPsm). There must be a task {a mathematical formula}Tsm=〈B,{〈E1+,E1−〉,…,〈En+,En−〉}〉 such that {a mathematical formula}H1∈ILPsm(Tsm) and {a mathematical formula}H2∉ILPsm(Tsm). There must be at least one partial interpretation {a mathematical formula}〈Ei+,Ei−〉 such that there is an answer set A of {a mathematical formula}B∪H1 such that {a mathematical formula}Ei+⊆A and {a mathematical formula}Ei−∩A=∅ and there is no such answer set of {a mathematical formula}B∪H2. Hence, letting {a mathematical formula}Tb=〈B,〈Ei+,Ei−〉〉, {a mathematical formula}H1∈ILPb(Tb) but {a mathematical formula}H2∉ILPb(Tb). So {a mathematical formula}〈B,H1,H2〉∈D11(ILPb). □
       </list-item>
      </list>
      <paragraph label="Proposition 15">
       {a mathematical formula}D11(ILPc)={〈B,H1,H2〉|AS(B∪H1)≠∅∧(AS(B∪H2)=∅∨Ec(B∪H2)⊈Ec(B∪H1))}.
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="•">
         First we show that for any {a mathematical formula}〈B,H1,H2〉∈D11(ILPc), {a mathematical formula}AS(B∪H1)≠∅ and either {a mathematical formula}AS(B∪H2)=∅ or {a mathematical formula}Ec(B∪H1)⊈Ec(B∪H2).Let {a mathematical formula}〈B,H1,H2〉 be an arbitrary element of {a mathematical formula}D11(ILPc). As {a mathematical formula}H1∈ILPc(Tc), {a mathematical formula}AS(B∪H1)≠∅. Assume that {a mathematical formula}Ec(B∪H1)⊆Ec(B∪H2). We must show that {a mathematical formula}AS(B∪H2)=∅. As {a mathematical formula}〈B,H1,H2〉∈D11(ILPc), {a mathematical formula}∃Tc=〈B,〈E+,E−〉〉 such that {a mathematical formula}H1∈ILPc(Tc) and {a mathematical formula}H2∉ILPc(Tc).As {a mathematical formula}H1∈ILPc(Tc),∀A∈AS(B∪H1):E+⊆A and {a mathematical formula}E−∩A≠∅, hence the conjunction {a mathematical formula}E+∧{note−|e−∈E−}∈Ec(B∪H1); hence by our initial assumption that {a mathematical formula}Ec(B∪H1)⊆Ec(B∪H2), the conjunction is also in {a mathematical formula}Ec(B∪H2); hence, {a mathematical formula}∀A∈AS(B∪H2),E+⊆A and {a mathematical formula}E−∩A=∅. But as {a mathematical formula}H2∉ILPc(Tc) this means that {a mathematical formula}AS(B∪H2)=∅.
        </list-item>
        <list-item label="•">
         We now show that for any B, {a mathematical formula}H1 and {a mathematical formula}H2, if {a mathematical formula}AS(B∪H1)≠∅∧(AS(B∪H2)=∅∨Ec(B∪H1)⊈Ec(B∪H2), then {a mathematical formula}〈B,H1,H2〉∈D11(ILPc).
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proposition 16">
       {a mathematical formula}D11(ILPLAS)={〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)}
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="•">
         We first show that {a mathematical formula}D11(ILPLAS)⊆{〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)}. For any {a mathematical formula}〈B,H1,H2〉∈D11(ILPLAS) there is an {a mathematical formula}ILPLAS task {a mathematical formula}T=〈B,〈E+,E−〉〉 such that {a mathematical formula}H1∈ILPLAS(T) and {a mathematical formula}H2∉ILPLAS(T).
        </list-item>
        <list-item label="•">
         It remains to show that {a mathematical formula}D11(ILPLAS)⊇{〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)}. Take B, {a mathematical formula}H1, {a mathematical formula}H2 to be any ASP programs such that {a mathematical formula}AS(B∪H1)≠AS(B∪H2)Let L be the set of atoms which appear in answer sets of {a mathematical formula}B∪H1 and {a mathematical formula}B∪H2.
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}D11(ILPLOAS)={〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2)orord(B∪H1)≠ord(B∪H2)}.
      </paragraph>
      <list>
       <list-item label="•">
        We first show that {a mathematical formula}D11(ILPLOAS)⊆{〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2) or ord(B∪H1)≠ord(B∪H2)}. For any {a mathematical formula}〈B,H1,H2〉∈D11(ILPLOAS) there is an {a mathematical formula}ILPLOAS task {a mathematical formula}T=〈B,〈E+,E−,Ob,Oc〉〉 such that {a mathematical formula}H1∈ILPLOAS(T) and {a mathematical formula}H2∉ILPLOAS(T).
       </list-item>
       <list-item label="•">
        It remains to show that {a mathematical formula}D11(ILPLOAS)⊇{〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2) or ord(B∪H1)≠ord(B∪H2)}. Take B, {a mathematical formula}H1, {a mathematical formula}H2 to be any ASP programs such that {a mathematical formula}AS(B∪H1)≠AS(B∪H2) or {a mathematical formula}ord(B∪H1)≠ord(B∪H2).Let L be the set of literals which appear in answer sets of {a mathematical formula}B∪H1 and {a mathematical formula}B∪H2.
       </list-item>
      </list>
      <paragraph label="Proposition 18">
       {a mathematical formula}D11(ILPLOAScontext)={〈B,H1,H2〉|B∪H1≢sB∪H2or∃C∈ASPchsuch thatord(B∪H1∪C)≠ord(B∪H2∪C)}.
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="•">
         We first show that {a mathematical formula}D11(ILPLOAScontext)⊆{〈B,H1,H2〉|B∪H1≢sB∪H2 or∃C∈ASPch st ord(B∪H1∪C)≠ord(B∪H2∪C)}. For any {a mathematical formula}〈B,H1,H2〉∈D11(ILPLOAScontext) there is an {a mathematical formula}ILPLOAScontext task {a mathematical formula}T=〈B,〈E+,E−,Ob,Oc〉〉 such that {a mathematical formula}H1∈ILPLOAScontext(T) and {a mathematical formula}H2∉ILPLOAScontext(T).
        </list-item>
        <list-item label="•">
         It remains to show that {a mathematical formula}D11(ILPLOAS)⊇{〈B,H1,H2〉|AS(B∪H1)≠AS(B∪H2) or ord(B∪H1)≠ord(B∪H2)}. Take B, {a mathematical formula}H1, {a mathematical formula}H2 to be any ASP programs such that {a mathematical formula}AS(B∪H1)≠AS(B∪H2) or {a mathematical formula}ord(B∪H1)≠ord(B∪H2).
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proposition 19">
       For any learning framework{a mathematical formula}F,{a mathematical formula}Dm1(F)‾={〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}.
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="•">
         We first show that {a mathematical formula}Dm1(F)‾⊆{〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}. Take an arbitrary {a mathematical formula}〈B,H,S〉∈Dm1(F)‾. We must show that {a mathematical formula}〈B,H,S〉∈{〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}. To do this, we need to show that {a mathematical formula}∀H′∈S, {a mathematical formula}〈B,H,H′〉∈D11(F).Take an arbitrary {a mathematical formula}H′∈S. It remains to show that {a mathematical formula}〈B,H,H′〉∈D11(F). By definition of {a mathematical formula}Dm1(F)‾, there must be some subset {a mathematical formula}S′⊆S such that {a mathematical formula}H′∈S′ and {a mathematical formula}〈B,H,S′〉∈Dm1(F). Hence, {a mathematical formula}∃TF such that {a mathematical formula}H∈ILPF(TF) and {a mathematical formula}S′∩ILPF(TF)=∅. Hence, as {a mathematical formula}H′∈S′, {a mathematical formula}〈B,H,H′〉∈D11(F).
        </list-item>
        <list-item label="•">
         We now show that {a mathematical formula}Dm1(F)‾⊇{〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}. Take an arbitrary {a mathematical formula}〈B,H,{H1,…,Hn}〉∈{〈B,H,{H1,…,Hn}〉|〈B,H,H1〉,…,〈B,H,Hn〉∈D11(F)}. For each {a mathematical formula}i∈[1..n], {a mathematical formula}〈B,H,Hi〉∈D11(F), and hence, {a mathematical formula}〈B,H,{Hi}〉∈Dm1(F). Hence, by definition of {a mathematical formula}Dm1(F)‾, {a mathematical formula}〈B,H,{H1,…,Hn}〉∈Dm1(F)‾.  □
        </list-item>
       </list>
      </paragraph>
      <paragraph label="Proposition 20">
       {a mathematical formula}ILPc,{a mathematical formula}ILPsm,ILPLAS,{a mathematical formula}ILPLOASand{a mathematical formula}ILPLOAScontextall have closed one-to-many-distinguishability.
      </paragraph>
      <paragraph label="Proof">
       <list>
        <list-item label="1.">
         Consider any two {a mathematical formula}ILPc tasks, {a mathematical formula}Tc1=〈B,〈E1+,E1−〉〉 and {a mathematical formula}Tc2=〈B,〈E2+,E2−〉〉. Let {a mathematical formula}Tc3=〈B,〈E1+∪E2+,E1−∪E2−〉〉.{a mathematical formula}H∈ILPc(Tc1)∩ILPc(Tc2) if and only if {a mathematical formula}AS(B∪H) is non-empty and {a mathematical formula}∀A∈AS(B∪H): {a mathematical formula}E1+⊆A, {a mathematical formula}E2+⊆A, {a mathematical formula}E1−∩A=∅ and {a mathematical formula}E2−∩A=∅. This is the case if and only if {a mathematical formula}(E1+∪E2+)⊆A, and {a mathematical formula}(E1−∪E2−)∩A=∅ which holds if an only if {a mathematical formula}H∈ILPc(Tc3).Hence, by Lemma 2, {a mathematical formula}ILPc has closed one-to-many-distinguishability.
        </list-item>
        <list-item label="2.">
         For any tasks {a mathematical formula}Tsm1=〈B,{e11,…en1}〉 and {a mathematical formula}Tsm2=〈B,{e12,…em2}〉, let {a mathematical formula}Tsm3=〈B,{e11,…en1,e12,…,em2}〉. {a mathematical formula}ILPsm(Tsm3)=ILPsm(Tsm1)∩ILPsm(Tsm2).Hence, by Lemma 2, {a mathematical formula}ILPsm has closed one-to-many-distinguishability.
        </list-item>
        <list-item label="3.">
         For any tasks {a mathematical formula}TLAS1=〈B,〈E1+,E1−〉〉 and {a mathematical formula}TLAS2=〈B,〈E2+,E2−〉〉, let {a mathematical formula}TLAS3=〈B,〈E1+∪E2+,E1−∪E2−〉〉. {a mathematical formula}ILPLAS(TLAS3)=ILPLAS(TLAS1)∩ILPLAS(TLAS2).Hence, by Lemma 2, {a mathematical formula}ILPLAS has closed one-to-many-distinguishability.
        </list-item>
        <list-item label="4.">
         For any tasks {a mathematical formula}TLOAS1=〈B,〈E1+,E1−,O1b,O1c〉〉 and {a mathematical formula}TLOAS2=〈B,〈E2+,E2−,O2b,O2c〉〉, let {a mathematical formula}TLOAS3=〈B,〈E1+∪E2+,E1−∪E2−,O1b∪O2b,O1c∪O2c〉〉. {a mathematical formula}ILPLOAS(TLOAS3)=ILPLOAS(TLOAS1)∩ILPLOAS(TLOAS2).Hence, by Lemma 2, {a mathematical formula}ILPLOAS has closed one-to-many-distinguishability.
        </list-item>
        <list-item label="5.">
         For any tasks {a mathematical formula}T1LOAScontext=〈B,〈E1+,E1−,O1b,O1c〉〉 and {a mathematical formula}T2LOAScontext=〈B,〈E2+,E2−,O2b,O2c〉〉, let {a mathematical formula}T3LOAScontext=〈B,〈E1+∪E2+,E1−∪E2−,O1b∪O2b,O1c∪O2c〉〉. {a mathematical formula}ILPLOAScontext(T3LOAScontext)=ILPLOAScontext(T1LOAScontext)∩ILPLOAScontext(T2LOAScontext).Hence, by Lemma 2, {a mathematical formula}ILPLOAScontext has closed one-to-many-distinguishability. □
        </list-item>
       </list>
      </paragraph>
     </section>
    </section>
   </appendices>
  </root>
 </body>
</html>