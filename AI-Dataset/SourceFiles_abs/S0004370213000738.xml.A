<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Speeding up many-objective optimization by Monte Carlo approximations.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Multi-objective optimization, also known as multi-criteria or vector optimization, is the basis of multiple criteria decision making [19], [22], [32], [33]. It is concerned with the optimization of vector-valued objective functions. The goal is to find or to approximate the set of Pareto-optimal solutions. A solution is Pareto-optimal if it cannot be improved in one objective without getting worse in another one. In recent years, it has become apparent that stochastic, population-based search algorithms such as evolutionary computing techniques are particularly well suited for solving vector optimization problems (e.g., see [15], [16]).
     </paragraph>
     <paragraph>
      Multi-objective evolutionary algorithms (MOEAs) have become broadly accepted methods in multi-criteria decision making and multiple criteria mathematical programming. It is known that the performance of MOEAs tends to deteriorate with an increasing number of objectives [14]. This is a general problem of vector optimization algorithms. For few objectives, MOEAs relying on the contributing hypervolume as the (second-level) sorting criterion are the methods of choice. These include the evolution strategy with probabilistic mutation for multi-objective optimization (ESP, [26]), the multi-objective covariance matrix adaptation evolution strategy (MO-CMA-ES, [27], [28], [34]), the SMS-EMOA [4], and variants of the indicator-based evolutionary algorithm (IBEA, [41]). Despite the progress in developing algorithms for hypervolume computation (e.g., [3], [6], [7], [10], [21], [38], [40]), the computational complexity of calculating the contributing hypervolume prevents the broad application of these powerful MOEAs to objective functions with many (say, more than four) objectives.
     </paragraph>
     <paragraph>
      Recently, several approximation algorithms for determining the least hypervolume contributor of a given Pareto-front approximation have been presented, for example, in [8], [11] or as part of the HypE MOEA [2]. Because of the good performance of HypE and encouraged by a preliminary study [36], we hypothesize that using such an approximation instead of the exact contributing hypervolume will make the aforementioned MOEAs applicable to problems with many objectives and that the resulting algorithms will push the boundaries of MOEAs for many-objective optimization.
     </paragraph>
     <paragraph>
      In theory, the approximation allows for the application of hypervolume-based MOEAs to optimization problems with an arbitrary number of objectives. While there exist comparisons of approximation-based algorithms with other MOEAs [2], the effects of replacing the exact hypervolume calculation with an approximation algorithm on the overall performance of MOEAs have not been investigated in isolation. Apart from [36], there has been no empirical comparison of state-of-the-art MOEAs in which the exact hypervolume computation has been replaced by an approximation while fixing the other components of the algorithm.
     </paragraph>
     <paragraph>
      Against this background, we employ the approximation within the steady-state MO-CMA-ES while all other components are kept fixed to empirically investigate whether the Monte Carlo approximation is actually useful in practice. In our experiments, using approximations indeed considerably reduced the computation time in the case of multiple objectives without impairing the quality of the obtained solutions.
     </paragraph>
     <paragraph>
      The remainder of the document is structured as follows. The next section introduces the problem of determining the least hypervolume contributor. We briefly review results on Monte Carlo approximation of the least hypervolume contributor as well as the exact hypervolume algorithm. Then, we present our empirical evaluation before concluding with the results of our experiments and suggestions for future work.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Vector optimization and the least hypervolume contributor
     </section-title>
     <paragraph>
      We consider multi-objective optimization problems of the form{a mathematical formula} where X denotes the search space of the optimization problem and m refers to the number of objectives. Without loss of generality, we assume that all objectives are to be minimized. The number of objectives of all considered test problems is {a mathematical formula}m⩾3. Pareto-dominance is the fundamental concept for comparing candidate solutions of a multi-objective optimization problem. The candidate solution {a mathematical formula}x′weakly dominates x and we write {a mathematical formula}x′⪯x if{a mathematical formula} The solution {a mathematical formula}x′strictly dominates x and we write {a mathematical formula}x′≺x if additionally{a mathematical formula} hold.
     </paragraph>
     <paragraph>
      Using the notion of dominance, the goal of multi-objective optimization can be defined as finding or approximating the set{a mathematical formula} which is called the Pareto-optimal set. The image of {a mathematical formula}X′ under {a mathematical formula}f→ is referred to as the corresponding Pareto-optimal front.
     </paragraph>
     <paragraph>
      The concept of dominance can be extended to sets. Let A and B be sets of candidate solutions. Then A weakly dominates B and we write {a mathematical formula}A⪯B if every element in B is weakly dominated by at least one element in A.
     </paragraph>
     <section label="2.1">
      <section-title>
       Evolutionary vector optimization
      </section-title>
      <paragraph>
       Evolutionary algorithms (EAs, [20]) are iterative direct search heuristics that maintain a set of μ candidate solutions, the so-called parent population. In each iteration, λ new offspring solutions are generated. Then a new parent population is assembled from both the offspring and the former parent population. Candidate solutions with better objective function values are preferentially selected. In the elitist EA considered in this study, the parent population of the next generation is formed by the best μ of the new solutions and their parents. This requires sorting the solutions. However, the Pareto-dominance relation does not establish a total order. Therefore, incomparable candidate solutions need to be sorted by a so-called second-level sorting criterion. Given two incomparable individuals a and b (i.e. neither {a mathematical formula}a⪯b nor {a mathematical formula}b⪯a holds) and a Pareto front F, the second-level sorting criterion determines whether a or b is more valuable in the context of F (cf. [41]). The contributing hypervolume is one of the most popular second-level sorting criteria due to its attractive theoretical properties, and it is deployed in most recent multi-objective evolutionary algorithms.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       The contributing hypervolume
      </section-title>
      <paragraph>
       The hypervolume measure or {a mathematical formula}S-metric (see [42]) of a population A is the volume of the union of regions of the objective space which are dominated by A and bounded by some appropriately chosen reference point {a mathematical formula}r→∈Rm, that is,{a mathematical formula} with {a mathematical formula}VOL(⋅) being the Lebesgue measure. One of the unique features of the hypervolume indicator is that it is, up to weighting objectives, the only known indicator which is strictly Pareto compliant [43], that is, given two sets A and B the indicator values A higher than B if A dominates B. It has further been shown by Bringmann and Friedrich [13] that the worst-case approximation factor of all possible Pareto fronts obtained by any hypervolume-optimal set of fixed size μ is asymptotically equal to the best worst-case approximation factor achievable by any set of size μ, namely {a mathematical formula}Θ(1/μ) for additive approximation and {a mathematical formula}1+Θ(1/μ) for relative approximation. The authors have shown in [23] that by considering a transformed variant of the hypervolume indicator, the logarithmic hypervolume indicator, a close-to-optimal multiplicative approximation ratio can be achieved. For these reasons, the hypervolume indicator is a popular second-level sorting criterion in many recent multi-objective evolutionary algorithms (MOEAs).
      </paragraph>
      <paragraph>
       When using the hypervolume as a second-level sorting criterion for comparing incomparable individuals, we measure the respective contribution of each individual to the total hypervolume. The contributing hypervolume of an individual {a mathematical formula}a∈A is given by{a mathematical formula} Note that the contributing hypervolume of a dominated individual is zero. Thus, in the following we assume that all dominated individuals have been removed from A before contributing hypervolumes are computed or estimated. The contribution {a mathematical formula}CON(a,A) is an important measure since instead of using the hypervolume directly, most hypervolume-based algorithms such as the steady-state MO-CMA-ES or the SMS-EMOA remove, in the selection step, the individual{a mathematical formula} contributing the least hypervolume to the population A.{sup:1}
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Computing the contributing hypervolume
     </section-title>
     <paragraph>
      In this section, we summarize an exact algorithm and two approximation schemes for calculating the contributing hypervolume.
     </paragraph>
     <section label="3.1">
      <section-title>
       Exact computation
      </section-title>
      <paragraph>
       In order to determine {a mathematical formula}a1, the usual way is calculating {a mathematical formula}HYP(A) and {a mathematical formula}HYP(A∖{a}) for all {a mathematical formula}a∈A. This can be done by {a mathematical formula}(|A|+1) hypervolume calculations with one of the many available hypervolume algorithms. Unfortunately, as the problem is #P-hard [9], none of them can run in time polynomial in m unless {a mathematical formula}P=NP. In fact, assuming the widely believed exponential time hypothesis [30], the runtime of all algorithms computing the hypervolume must be {a mathematical formula}|A|Ω(m)[12]. Note that this only holds in the worst-case. The average-case complexity can be polynomial in the number of objectives [12].
      </paragraph>
      <paragraph>
       Many algorithms have been present recently for calculating the hypervolume (e.g. [3], [6], [7], [10], [21], [38], [40]). We use the algorithm of Bringmann and Friedrich [10] which computes all contributions {a mathematical formula}CON(a,A), {a mathematical formula}a∈A, in only one pass. This saves a factor of {a mathematical formula}|A| compared to most other hypervolume algorithms and gives a total runtime of {a mathematical formula}O(|A|m/2log(|A|)) to compute all {a mathematical formula}|A| hypervolume contributions. For dimension {a mathematical formula}m=3 there is an even faster algorithm by Emmerich and Fonseca [21], which computes all hypervolume contributions in time {a mathematical formula}O(|A|log(|A|)). As we are more interested in higher-dimensional problems ({a mathematical formula}m⩾5), we use the algorithm of Bringmann and Friedrich [10] for all dimensions.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Probably approximately correct approximation
      </section-title>
      <paragraph>
       In our experiments we want to compare this exact calculation of the hypervolume with an approximation algorithm. It is known that the hypervolume can be approximated very efficiently by an FPRAS (fully polynomial-time randomized approximation scheme) [9]. Unfortunately, an approximation of the hypervolume does not yield an approximation of CON. Even worse, {a mathematical formula}CON(a,A) is not only #P-hard to calculate exactly, it is also NP-hard to approximate by a factor of {a mathematical formula}2m1−ε for all {a mathematical formula}ε&gt;0[8], [11]. Though CON is therefore not approximable in time polynomial in {a mathematical formula}|A| and m, there are still a few approximation algorithms for CON (see [2], [8], [11]). The approximation algorithms presented in [2], [8], [11] are Monte Carlo algorithms based on different sampling techniques. The algorithm HypE [2] is a MOEA using hypervolume estimations at a user-specified confidence level to guide the search. However, here we want to compare a standard MOEA with and without hypervolume approximation to study the effects of the approximation. For this, we use the approximation algorithm of Bringmann and Friedrich [8], [11]. We now briefly describe this approach, which we will later improve in Section 3.3. This Monte Carlo algorithm returns, for a population A and arbitrary small {a mathematical formula}ε,δ&gt;0, an individual {a mathematical formula}a1˜ with the property{a mathematical formula} The algorithm samples in the minimal bounding boxes of all contributions and conducts a race between the different candidates until an individual {a mathematical formula}a1˜ is found which (with high probability) has a contribution very close to the hypervolume contribution of {a mathematical formula}a1.
      </paragraph>
      <paragraph>
       The runtime of this algorithm is bounded by {a mathematical formula}O(m|A|(|A|+H)), where H is a measure of hardness of the instance. It is polynomial in m and {a mathematical formula}|A| for most practical instances, but unbounded in the worst-case. More precisely, H is defined as{a mathematical formula} where {a mathematical formula}a1 denotes the individual with the smallest contribution, {a mathematical formula}a2 denotes the individual with the second smallest contribution, and {a mathematical formula}BB(a,A) denotes the volume of the smallest bounding box of the contribution {a mathematical formula}CON(a,A). By definition, H is unbounded and can even be undefined if there is no unique least contributor. However, in such cases an abortion criterion bounds the runtime. In general, H is small if {a mathematical formula}BB(a,A)≈CON(a,A) and {a mathematical formula}CON(a1,A)≪CON(a,A) for all {a mathematical formula}a∈A∖{a1}. On the other hand, H is large if either (i) there is an individual with a large bounding box {a mathematical formula}BB(a,A) but a small contribution {a mathematical formula}CON(a,A), or (ii) there are two or more boxes contributing the minimal contribution or only slightly more than it, that is, for all {a mathematical formula}CON(a,A)−CON(a1,A), {a mathematical formula}a≠a1, is very small.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Fast approximate computation
      </section-title>
      <paragraph>
       As has been pointed out, approximating the least hypervolume contributor is an NP-hard problem [8], [11]. For fixed error bounds and error probabilities, the above described approximation scheme can degenerate to an exponential runtime. For guiding the search in a randomized search heuristic this seems inappropriate. Despite the fact that the approximation algorithm is reported to have a very fast empirical average-case performance [8], [11], we observed that difficult situations do indeed occur for typical benchmark problems, and sometimes very many samples are needed to achieve the specified error bound and error probability. These slow instances have a very large hardness value H (see Eq. (2)), for example, because the contribution is extremely small compared to the bounding box and most samples do not lie in the contribution. This is unavoidable for a probably approximately correct approximation, but undesirable for a practical optimization algorithm.
      </paragraph>
      <paragraph>
       To address these situations, we propose a heuristic that stops the overall selection process whenever a certain threshold of total samples has been reached. On early stopping, the current estimate of the least contributor is considered further by the selection scheme. Note that, in contrast to the approximation scheme described in Section 3.2, this algorithm is not anymore probably approximately correct with parameters ε and δ (cf. Eq. (1)), but it results in a very fast algorithm which still gives competitive results with respect to the quality of the final Pareto-front approximation.
      </paragraph>
      <paragraph>
       This new approximation scheme comes with an additional threshold parameter. In order to determine a good value for this parameter, we conducted a preliminary study of the threshold parameter for the DTLZ benchmark set [18]. As a testbed, we chose the objective function DTLZ 2 with eight objectives and considered a maximum number of 10{sup:3}, 10{sup:4}, 10{sup:5} and 10{sup:6} samples for the hypervolume approximation scheme. We conducted 25 independent trials for every parameter setting and analyzed the quality of the resulting Pareto-front approximations in terms of the absolute hypervolume indicator. Moreover, we recorded the required running time for every parameter setup.
      </paragraph>
      <paragraph>
       The results of the parameter study are presented in Fig. 1. Limiting the number of samples directly affects the running time of the selection scheme, and thus, of the overall algorithm. For a threshold of 10{sup:3} and 10{sup:4} samples, respectively, the quality of the resulting Pareto-front approximations is negatively affected and the absolute hypervolume starts to fluctuate as it nears the Pareto-optimal front (see Fig. 1, bottom). In case of higher thresholds (10{sup:5} and 10{sup:6} samples), the quality remains stable and is on par with the quality obtained when considering the approximation scheme without a sample threshold. We therefore use sample threshold 10{sup:5} for our fast approximation algorithm.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Empirical evaluation
     </section-title>
     <paragraph>
      We compared the two approximated hypervolume indicators to the exact hypervolume indicator w.r.t. the influence on the performance (in terms of the quality of the final Pareto-front approximation) and the running time of MOEAs. To this end, we deploy all three indicators in the steady-state variant of the multi-objective covariance matrix evolution strategy (MO-CMA-ES, [27], [34], [37]). However, we expect the results to carry over to any MOEA also relying on the hypervolume indicator as second-level sorting criterion.
     </paragraph>
     <section label="4.1">
      <section-title>
       MO-CMA-ES
      </section-title>
      <paragraph>
       The MO-CMA-ES relies on the Pareto-dominance relation and a second-level sorting criterion for selection, which is state-of-the-art in MOEAs since [17]. The algorithm builds on the principles of the single-objective covariance matrix adaptation strategy (CMA-ES, [24], [31], [34]), which is a variable metric algorithm adapting the shape and strength of its Gaussian search distribution. The claim that the “CMA-ESs represent the state-of-the-art in evolutionary optimization in real-valued {a mathematical formula}Rn search spaces” [5] is backed up by many performance comparisons across different suites of benchmark problems (e.g., see the competition results in [1], [25]). Here, we use the most recent variant of the {a mathematical formula}(μ+1)-MO-CMA-ES presented in [37]. For empirical evaluations of the MO-CMA-ES see, for example, [27], [28], [37].
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Experimental setup
      </section-title>
      <paragraph>
       We compared the MO-CMA-ES using an approximation of the least hypervolume contributor to the results of the original MO-CMA-ES with exact hypervolume computation. More precisely, we compare the following indicators:
      </paragraph>
      <list>
       <list-item label="•">
        Exact computation as described in Section 3.1.
       </list-item>
       <list-item label="•">
        Probably approximately correct as described in Section 3.2.
       </list-item>
       <list-item label="•">
        Fast approximation with sample threshold 10{sup:5} as described in Section 3.3.
       </list-item>
      </list>
      <paragraph>
       This allows us to isolate the influence of the indicator by altering only the environmental selection procedure. We applied the algorithms to several classes of benchmark functions that are scalable to an arbitrary number of objectives m. We considered the seven constrained functions DTLZ 1–7 [18] with search space dimension 30. The number of objectives was chosen to be 3, 5, and 7. In addition, we considered two real-world many-objective optimization problems. The first one deals with the design of 2D airfoil shapes that are encoded by 10 real-valued parameters, the so-called PARSEC 10 parameter set. An airfoil shape is then evaluated with respect to six objectives by means of a computational fluid dynamics simulation (see [39]). The second one considers the optimization of centrifugal pump designs with respect to eight objectives (see [35]).
      </paragraph>
      <paragraph>
       For all experiments, the number of parent individuals was set to {a mathematical formula}μ=50. We conducted 25 independent trials with 50,000 objective function evaluations each. For both approximation algorithms, we used the parameters {a mathematical formula}ε=10−2 and {a mathematical formula}δ=10−2 (cf. Eq. (1)). The experiments were carried out on a cluster of technically equivalent workstations with 2.93 GHz Intel Quad-Core Xeon processors running Linux. We used the GNU compiler chain and enabled compiler optimizations according to the following command line arguments: -O3 -ffast-math -msse4 -mtune=core2. The reported runtimes refer to the overall CPU times.
      </paragraph>
      <paragraph>
       We monitored the performance of the algorithms after every 5,000th objective function evaluation and carried out the statistical evaluation after 25,000 and 50,000 function evaluations. We relied on the hypervolume indicator to compare the Pareto-front approximations obtained by the three optimizers. In case of the benchmark functions DTLZ 1–7 and the centrifugal pump design problem, the reference point was determined from the union of all Pareto-front approximations. In case of the airfoil shape optimization problem, we chose the reference point {a mathematical formula}r→=(0.00516,0.00606,0.00982,0.30806,0.92314,0.65460) as suggested in [39]. We applied the statistical evaluation procedure described in [37] to evaluate our experiments employing the Wilcoxon rank-sum test to verify statistical significance. All experiments were implemented using the Shark machine learning library [29], which is available online.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Results
      </section-title>
      <paragraph>
       Table 1 summarizes our experimental results. In terms of the achieved hypervolume, the MO-CMA-ES relying on an approximation of the least hypervolume contributor performed at least on par with the variant employing the exact hypervolume indicator across the set of benchmark functions. More precisely, the differences in achieved hypervolumes were not statistically significant according to a Wilcoxon rank-sum test at a confidence level of {a mathematical formula}α=0.05 for all test functions except DTLZ 7 with 7 objectives. The final objective values are therefore omitted in Table 1.
      </paragraph>
      <paragraph>
       Table 1 shows clearly that the MO-CMA-ES achieves an enormous speed-up if the indicator is only approximated, especially for more than 4 dimensions. The speed-up is the largest for the heuristically improved approximation introduced in Section 3.3 of this paper. However, we also observe that for very complicated test functions such as the disconnected fronts of DTLZ 7 in 7 dimensions, a sample threshold of 10{sup:5} might not suffice. Here, we observe a tradeoff between speed (sample threshold 10{sup:5}) and quality (probably approximately correct computation).
      </paragraph>
      <paragraph>
       Fig. 2 illustrates the typical behavior of the algorithms as a function of time and number of objective function evaluations. Shown are the medians over the 25 trials and the corresponding lower and upper quartiles (i.e., 25th and 75th percentiles). As expected, the hypervolume as a function of the number of objective function evaluations behaves similarly. On the other hand, both variants with approximated hypervolume finished all 50,000 objective functions evaluations in less than 10 hours, while the exact version needed more than 10 hours for only the first 5000 objective function evaluations.
      </paragraph>
      <paragraph>
       A closer look at Fig. 2 reveals that the algorithms using the Monte Carlo approximation perform even a bit better. The final medians are slightly higher, and, more prominently, the lower quartiles are larger. That is, there are fewer trials reaching only low hypervolume values within the given budget. A possible explanation might be that the approximation adds a little noise to the otherwise deterministic selection operator, which turns out to be beneficial for the evolutionary process.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Conclusions
     </section-title>
     <paragraph>
      We empirically investigated the effects of replacing the exact hypervolume indicator with two different Monte Carlo approximations on the performance of multi-objective evolutionary algorithms (MOEAs). We evaluated whether a state-of-the-art MOEA relying on hypervolume-indicator-based selection is affected by the potential errors made by approximating the least hypervolume contributor. The results show that the performance of the algorithms in terms of the quality of the Pareto-front approximation given a budget of objective function evaluations does not suffer from the additional noise introduced by the Monte Carlo approximation. In some trials, the approximation, which introduces noise into the otherwise deterministic, greedy indicator-based selection scheme, led to better performance. We observed a vast reduction of the running time even for few objectives, when relying on the approximation scheme. In general, the higher the number of objectives the more pronounced the performance advantage of using Monte Carlo approximation becomes. Hence, by employing Monte Carlo approximations, hypervolume-based MOEAs become applicable to many-objective optimization.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>