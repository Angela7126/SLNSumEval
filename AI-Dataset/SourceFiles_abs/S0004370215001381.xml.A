<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Exploiting meta features for dependency parsing and part-of-speech tagging.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Discriminative methods have been highly successful in solving structured prediction tasks in natural language processing (NLP), such as parsing, part-of-speech tagging, and word segmentation [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. An important reason is that discriminative models accommodate rich features without constraints such as probabilistic independence assumptions between features. Taking dependency parsing as an example, recent advances in parsing accuracies have been driven by the incorporation of rich non-local features in discriminative models [4], [5], [6].
     </paragraph>
     <paragraph>
      One drawback of rich and complex features, however, is that they can be sparse and rare in unseen test data. For example, a lexicalized feature, which contains a specific word in the training data, cannot be instantiated on out-of-vocabulary words in test data. Given a limited amount of manually annotated training data, it is even harder for the more complex bilexical or trilexical features, which contain more than one word, to be fully utilized in unknown test data.
     </paragraph>
     <paragraph>
      Several methods have been proposed to address the sparseness issue by leveraging large-scale unannotated data. In particular, word clusters [11], [12] have been used as additional features in discriminative models to alleviate the sparseness of lexicalized features, leading to improved accuracies in named entity recognition [13] and dependency parsing [14]. Recently, word embeddings [15] have also been used as less-sparse word features to improve part-of-speech tagging [16], [17], named entity recognition [18], [19], and dependency parsing [20], leading to similar improvements compared with Brown clusters. Obtained from large-scale data, word clusters and embeddings effectively enlarge the vocabulary, allowing words out of the training data to be better handled at test time.
     </paragraph>
     <paragraph>
      There has been work on reducing the sparseness of structures beyond words. In particular, Chen et al. [21] extract subtree structures from auto-parsed dependency trees, groups the subtrees into clusters by their frequencies and incorporates the resulting clusters into a dependency parser as less-sparse subtree features. The method leads to accuracy improvements that are comparable to the use of Brown word clusters. The same idea has been applied to word segmentation and pos-tagging by making less-sparse representations of word ngrams from automatically processed data [22]. Experiments show that the simple method gives significant improvements in accuracies.
     </paragraph>
     <paragraph>
      We extend the idea above from task-specific structures to arbitrary features, obtaining less-sparse features by clustering feature instances from automatically annotated data. Specifically, the feature instances under each feature template are bucketed by their frequencies, and each bucket is taken as a cluster. The clusters contain information on structured feature templates, and statistical distributions of the structures over large automatically annotated data. They are used to form a set of less-sparse features, which we call meta features. Compared with task-specific clusters such as subtree features, meta features have two main advantages. First, they are general and task independent. As a result, the method of this article can be used for any structured prediction tasks. Second, they are relatively more comprehensive by covering all the feature templates in a discriminative model, which are designed to include the most important structured patterns for a task.
     </paragraph>
     <paragraph>
      We apply meta features to two typical structured prediction tasks in NLP, namely dependency parsing and part-of-speech (POS) tagging. For the dependency parsing task, our method significantly outperforms the method of [21], achieving accuracies comparable to the best reported in the literature. For the POS tagging task, our method also perform better than strong baselines.
     </paragraph>
     <paragraph>
      This article is a significant extension of a conference version [23], which focuses on the dependency parsing task. We reformulate the method as a general framework for structured prediction, and demonstrate the effectiveness by evaluation on POS tagging in addition to dependency parsing. We give more details of the method, and in-depth analysis of the results.
     </paragraph>
     <paragraph>
      The rest of this article is organized as follows. Section 2 gives a overview of the learning framework for discriminative structured prediction. In Section 3 shows the application of the proposed framework to dependency parsing. In Section 4 shows the application of the framework to part-of-speech tagging. Section 3.4 and 4.4 describe the experiment settings and reports the experimental results of dependency parsing and part-of-speech tagging respectively. Section 5 discusses related work. Finally, in Section 6 we summarize the proposed approach.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Learning framework of feature transformation
     </section-title>
     <paragraph>
      As shown in Fig. 1, the learning framework consists of four steps: 1) Base training: training a baseline system using annotated data and base features; 2) Processing: using the baseline system to annotate a large amount of raw sentences and obtain auto-labeled data; 3) Feature bucketing: performing feature transformation from base features to meta features; 4) Final training: training a new system with both the base and meta features.
     </paragraph>
     <paragraph>
      The key step of the framework is feature bucketing, in which we use a transformation function to group the base feature instances from automatically annotated data into clusters, and define a set of meta features based on the clusters. The feature transformation can be treated as a clustering problem that groups the features with similar properties into the same cluster. Different clustering methods can be used for bucketing similar base features. We use frequency-based bucketing, putting the base features that have similar frequency levels in automatically annotated data into the same bucket, and use the bucket to form meta features. The frequency-based bucketing assumes that the base feature instances with higher frequencies are relatively more reliable, inspired by the work of [24], in which different bucketing methods are well studied. The use of frequency bucketing to reduce sparseness has also been studied, mainly for smoothing n-gram language models in the field of Machine Learning [25], [26], [27], [28]. Based on the frequency-based bucketing method, we define different functions for feature transformation and different meta features for different tasks.
     </paragraph>
     <paragraph>
      In the following sections, we apply the framework to dependency parsing and part-of-speech tagging.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Case 1: Dependency parsing
     </section-title>
     <section label="3.1">
      <section-title>
       Background of dependency parsing
      </section-title>
      <paragraph>
       The task of dependency parsing is to assign head-dependent relations between the words in a sentence. Fig. 2 shows a simple example, where arcs indicate dependency relations, and “ROOT” is an artificial root token inserted at the beginning of the sentence which does not depend on any other token in the sentence. For example, the arc between “ate” and “fish” indicates a dependency, where “ate” is the head and “fish” is the dependent.
      </paragraph>
      <paragraph>
       For dependency parsing, there are two main types of models [29]: graph-based models [8], [5] and transition-based models [9], [30]. We study graph-based parsing in this article, although the SSFT framework can be applied in both parsing models.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Baseline parser
      </section-title>
      <section label="3.2.1">
       <section-title>
        Graph-based parsing model
       </section-title>
       <paragraph>
        Given an input sentence, the task of dependency parsing is to build a dependency tree. We define X as the set of possible input sentences, Y as the set of possible dependency trees, and {a mathematical formula}D=(x1,y1),…,(xi,yi),…,(xn,yn) as a training set where {a mathematical formula}xi∈X and {a mathematical formula}yi∈Y. A sentence is denoted by {a mathematical formula}x=(w0,w1,…,wi,…,wm), where {a mathematical formula}w0 is ROOT and does not depend on any other word and {a mathematical formula}wi refers to a word.
       </paragraph>
       <paragraph>
        Denoting a dependency relation in tree y from word {a mathematical formula}wi to word {a mathematical formula}wj ({a mathematical formula}wi is the head and {a mathematical formula}wj is the dependent) as a ordered pair {a mathematical formula}(wi,wj)∈y, a graph {a mathematical formula}Gx consists of a set of nodes {a mathematical formula}Vx={w0,w1,…,wi,…,wm} and a set of arcs (edges) {a mathematical formula}Ex={(wi,wj)|i≠j,wi∈Vx,wj∈(Vx−w0)}. The parsing model of [8] is to search for the maximum spanning tree (MST) in the graph {a mathematical formula}Gx. Denoting the set of all the subgraphs of {a mathematical formula}Gx that are valid dependency trees [31] for sentence x as {a mathematical formula}Y(Gx), the score of a dependency tree {a mathematical formula}y∈Y(Gx) is the sum of the subgraph scores,{a mathematical formula} where g is a spanning subgraph of y, which can be a single arc or adjacent arcs. A discriminative model scores each subgraph g using a linear equation,{a mathematical formula} where {a mathematical formula}f(x,g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features.
       </paragraph>
       <paragraph>
        The maximum spanning tree {a mathematical formula}y⁎ is the highest scoring tree in {a mathematical formula}Y(Gx). Given an input sentence x, the task of the decoding algorithms is to find {a mathematical formula}y⁎, where{a mathematical formula}
       </paragraph>
       <paragraph>
        We use the decoding algorithm proposed by [5], which is a second-order CKY-style algorithm [32], and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) [33], [3].
       </paragraph>
      </section>
      <section label="3.2.2">
       <section-title>
        Base features
       </section-title>
       <paragraph>
        Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in [8], the second-order parent-siblings features defined in [3], and the second-order parent-child-grandchild features defined in [5]. Bohnet [7] explorers a richer set of features than the above sets. Based on the above work, we defined a set of base feature templates listed in Table 1, where h, d refer to the head, the dependent respectively, c refers to d's sibling or child, +1 (−1) refers to the next (previous) word, w and p refer to the surface word and part-of-speech tag respectively, {a mathematical formula}dir(h,d) is the direction of the dependency relation between h and d, {a mathematical formula}[wp] refers to w or p, and {a mathematical formula}dir(h,d,c) is the directions of the relation among h, d, and c. For example, for the words {a mathematical formula}h=ate, {a mathematical formula}d=with, and {a mathematical formula}c=fish in Fig. 2, {a mathematical formula}dir(h,d,c)=SIB_RIGHT_RIGHT, where SIB refers to the parent-sibling structure and RIGHT refers to the right direction.
       </paragraph>
      </section>
      <section label="3.2.3">
       <section-title>
        Baseline parser
       </section-title>
       <paragraph>
        We train a baseline parser with the base features in Table 1. Defining {a mathematical formula}fb(x,g) as the base features and {a mathematical formula}wb as the corresponding weights, the scoring function is,{a mathematical formula}
       </paragraph>
      </section>
     </section>
     <section label="3.3">
      <section-title>
       Feature transformation
      </section-title>
      <section label="3.3.1">
       <section-title>
        Template-based transformation function
       </section-title>
       <paragraph>
        The baseline parser is used to process large scale raw text automatically, and the output is used for feature transformation. We define a template-based function for grouping the base features into clusters and use discrete values to represent the clusters.
       </paragraph>
       <paragraph>
        Denote the set of base feature templates with {a mathematical formula}TB. For each template {a mathematical formula}Ti∈TB, a set of base features {a mathematical formula}Fi is generated from the set of automatically annotated dependency trees. We add the features to a list {a mathematical formula}L(Ti) and count their frequencies in the auto-parsed data. The collected features in {a mathematical formula}L(Ti) are sorted in decreasing order of frequencies. The transformation function for a base feature {a mathematical formula}fb generated from {a mathematical formula}Ti is defined as follows,{a mathematical formula} where {a mathematical formula}R(fb) is the position index of {a mathematical formula}fb in the sorted list {a mathematical formula}L(Ti), “Others” is defined for the base features that are not included in the list, and TOP10 and TOP30 refer to the position numbers of top 10% and top 30%, respectively. Following the work of [24], we also use the numbers, 10% and 30%, and the number of categories in our systems. The preliminary experiments on our development sets show that the setting works very well. Note that {a mathematical formula}OTi is defined for the features that are not in the list. But in our final systems, we do not use {a mathematical formula}OTi.{sup:1} Therefore, for a feature generated from the template {a mathematical formula}Ti, we have three possible values: {a mathematical formula}HTi, {a mathematical formula}MTi, and {a mathematical formula}LTi. In total, we have {a mathematical formula}3×N(TB) possible values for all the base features, where {a mathematical formula}N(TB) refers to the number of the base feature templates, which is usually small. We can obtain the mapped values of all the collected features via the transformation function.
       </paragraph>
      </section>
      <section label="3.3.2">
       <section-title>
        Meta feature templates
       </section-title>
       <paragraph>
        Based on the mapped values, we define meta feature templates in {a mathematical formula}FM for dependency parsing. The meta feature templates are listed in Table 2, where {a mathematical formula}fb is a base feature of {a mathematical formula}FB, {a mathematical formula}hp refers to the part-of-speech tag of the head and {a mathematical formula}hw refers to the surface word of the head. Of the table, the first template uses the mapped value only, the second and third templates combine the value with the head information. We remove any feature related to the surface form if the word is not one of the Top-K{sup:2} most frequent words in the training data. This method can reduce the size of the feature sets. The number of the meta features is relatively small. It is possible to have {a mathematical formula}3×N(TB) for the first type, {a mathematical formula}3×N(TB)×N(POS) for the second type, and {a mathematical formula}3×N(TB)×N(WORD) for the third one, where {a mathematical formula}N(POS) refers to the number of part-of-speech tags, {a mathematical formula}N(WORD) refers to the number of words. The empirical statistics of the feature sizes at Section 3.4.2 show that the number of meta features is only 1.2% of base features.
       </paragraph>
      </section>
      <section label="3.3.3">
       <section-title>
        Generating meta features
       </section-title>
       <paragraph>
        We use an example to demonstrate how to generate the meta features based on the meta feature templates. Consider the sentence “I ate the meat with a fork.” Fig. 3 shows the example for generating the meta features for the relation among “ate”, “meat”, and “with”, where “ate” is the head, “meat” is the dependent, and “with” is the closest left sibling of “meat”.
       </paragraph>
       <paragraph>
        We demonstrate the generating procedure using the template {a mathematical formula}Tk= “{a mathematical formula}hw,dw,cw,dir(h,d,c)” (the last template of Table 1-(b)), which contains the surface forms of the head, the dependent, its sibling, and the directions of the dependencies among h, d, and c. A base feature can be “ate, meat, with, SIB_ RIGHT_RIGHT”, where “SIB_RIGHT_RIGHT” refers to the parent-siblings structure with the right direction. In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30. According to the mapping function, we obtain the mapped value {a mathematical formula}MTk. Finally, we have the three meta features “{a mathematical formula}[MTk]”, “{a mathematical formula}[MTk],VV”, and “{a mathematical formula}[MTk],ate”, where VV is the part-of-speech tag of the word “ate”.
       </paragraph>
      </section>
      <section label="3.3.4">
       <section-title>
        Meta parser
       </section-title>
       <paragraph>
        We combine the base features with the meta features by a new scoring function,{a mathematical formula} where {a mathematical formula}fb(x,g) refers to the base features, {a mathematical formula}fm(x,g) refers to the meta features, and {a mathematical formula}wb and {a mathematical formula}wm are their corresponding weights respectively.
       </paragraph>
       <paragraph>
        We use the same decoding algorithm in the new parser as in the Baseline parser. The new parser is referred to as the meta parser.
       </paragraph>
      </section>
     </section>
     <section label="3.4">
      <section-title>
       Experiments
      </section-title>
      <paragraph>
       We evaluate the effect of the meta features for the graph-based parsers on English and Chinese data.
      </paragraph>
      <section label="3.4.1">
       <section-title>
        Experimental settings
       </section-title>
       <paragraph>
        In our experiments, we use the Penn Treebank (PTB) [34] for English and the Chinese Treebank version 5.1 (CTB5) [35] for Chinese. Tool “Penn2Malt”{sup:3} is used to convert the data into dependency structures with the English head rules of [9] and the Chinese head rules of [36]. We follow the standard data splits as shown in Table 3. Following the work of [14], we use a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and use 10-way jackknifing to generate part-of-speech tags for the training set. We use the MXPOST [37] tagger for English and the CRF-based tagger for Chinese. We use gold standard segmentation in the CTB5. The data partition of Chinese is chosen to match previous work [38], [39], [40].
       </paragraph>
       <paragraph>
        For the unannotated data in English, we use the Brown WSJ corpus [41] containing about 43 million words.{sup:4} We use the MXPOST tagger trained on the training data to assign part-of-speech tags and use the Baseline parser to process the sentences of the Brown corpus. For the unannotated data in Chinese, we use the Xinhua portion of Chinese Gigaword{sup:5} Version 2.0 (LDC2009T14) [42], which has approximately 311 million words. We use the MMA system [43] trained on the training data to perform word segmentation and POS tagging, and use the Baseline parser to parse the sentences in the Gigaword data.
       </paragraph>
       <paragraph>
        For collecting base feature instances, we remove the features which occur only once in the English data and less than four times in the Chinese data. The feature occurrences of one time and four times are based on the development data performance.
       </paragraph>
       <paragraph>
        We measure the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency trees evaluation (COMP).
       </paragraph>
      </section>
      <section label="3.4.2">
       <section-title>
        Feature selection on development sets
       </section-title>
       <paragraph>
        We evaluate the parsers with different settings on the development sets to select the meta features.
       </paragraph>
       <paragraph>
        1) Different models vs meta features
       </paragraph>
       <paragraph>
        We investigate the effect of different types of meta features for the models trained on different sizes of training data on English. There are too many base feature templates to test one by one, and we divide the templates into several categories. In Table 1, some templates are related to only part-of-speech tags (P), some are related to only surface words (W), and the others contain both part-of-speech tags and surfaces (M). Table 4 shows the categories, where numbers N[1–4] refer to the numbers of words involved in templates. For example, the templates of N3WM are related to three words and contain the templates of W and M. Based on different categories of base templates, we have different sets of meta features.{sup:6}
       </paragraph>
       <paragraph>
        We randomly select 1% and 10% of the sentences, respectively, from the training data and trained the POS taggers and Baseline parsers on these small training data and use them to process the unannotated data. Then, we generate the meta features based on the newly auto-parsed data. The meta parsers are trained on the different subsets of the training data with different sets of meta features. Finally, we have three meta parsers: MP1, MP10, MPFULL, which are trained on 1%, 10% and 100% of the training data.
       </paragraph>
       <paragraph>
        Table 5 shows the results. From the table, we find that the meta features that are related to only part-of-speech tags do not always help, while the ones related to the surface words are very useful. This suggest that the more sparse the base features are, the more effective the corresponding meta features are. Thus, we build the final parsers by adding the meta features of N1WM, N2WM, N3WM, and N4WM, marked by {sup:♡} in Table 1. The results show that OURS achieve better performance than the systems with individual sets of meta features.
       </paragraph>
       <paragraph>
        2) Different meta feature types
       </paragraph>
       <paragraph>
        There are three types of meta feature templates, as shown in Table 2. Here, the results of the parsers with different settings are shown in Table 6, where CORE refers to the first type of meta features, WithPOS refers to the second, and WithWORD refers to the third. The results show that with all the types of meta features, the parser (OURS) achieve the best accuracy.
       </paragraph>
       <paragraph>
        Then, we count the numbers of the meta features based on Table 1, where {sup:♡} refers to the templates used in OURS. The detail of generating meta features is as follows: 1) For each first-order template, we can have 3 possible values (described in Section 3.3.1). Thus, we have 30 ({a mathematical formula}=10×3) meta features for first-order. 2) For each second-order template, we consider the types of structures, shown in Fig. 4, where “SIB” refers to the parent-siblings and “GC” refers to the parent-child-grandchild structures. In the tree structures, c can be a word or empty node. Thus, we have 468 ({a mathematical formula}=13×6×2×3) meta features for the 13 second-order templates. Totally, we have 498 meta features for CORE. Then, these are combined with part-of-speech tags and word surfaces. Finally, only 327,864 (or 1.2%) features are added into OURS. Thus, we use all the three types of meta features in our final meta parsers.
       </paragraph>
       <paragraph>
        We also record the speed of Baseline and OURS, as shown in Table 6. The results show that OURS is slightly slower (20.88%) than Baseline.
       </paragraph>
      </section>
      <section label="3.4.3">
       <section-title>
        Main results on test sets
       </section-title>
       <paragraph>
        The results on English are shown in Table 7, where MetaParser refers to the meta parser. We find that the meta parser outperforms the baseline with an absolute improvement of 1.01 points (UAS). The improvement is significant by McNemar's Test ({a mathematical formula}p&lt;10−7).
       </paragraph>
       <paragraph>
        The results on Chinese are shown in Table 8. As in the experiment on English, the meta parser outperforms the baseline. We obtain an absolute improvement of 2.07 points (UAS). The improvement is significant by McNemar's Test ({a mathematical formula}p&lt;10−8).
       </paragraph>
      </section>
      <section label="3.4.4">
       <section-title>
        Different sizes of unannotated data
       </section-title>
       <paragraph>
        Here, we consider the improvement relative to the sizes of the unannotated data used to generate the meta features. We randomly select the 0.1%, 1%, and 10% of the sentences from the full data. Table 9 shows the results, where P0.1, P1, and P10 correspond to 0.1%, 1%, and 10% respectively. From the table, we find that the parsers obtain more benefits as we use more raw sentences. We also try generating the meta features from the training data only, shown as TrainData in Table 9. However, the results shows that the parsers perform worse than the baselines. The reason might be that only the known base features are included in the training data.
       </paragraph>
      </section>
      <section label="3.4.5">
       <section-title>
        Comparison with previous work
       </section-title>
       <paragraph>
        Table 10 shows the performance of the previous systems that are compared on English. We add the cluster-based features of [14]{sup:7} to our baseline system listed as “Baseline+CLU” in the table. The results show that our meta parser outperforms most of the previous systems and obtain the comparable accuracy with the best result of Suzuki11 [44] which combine the clustering-based word representations of [14] and a condensed feature representation.
       </paragraph>
       <paragraph>
        Table 11 shows the comparative results on Chinese. We add the cluster-based features of [14]{sup:8} to our baseline system listed as “Baseline+CLU” in the table. The reported scores on this data are produced by the supervised learning methods and our Baseline (supervised) parser provides a comparable accuracy. We find that our meta parser achieves state-of-the-art accuracy and performs significantly better than the previous scores. Note that we use the auto-assigned POS tags in the test set to match previous studies.
       </paragraph>
      </section>
      <section label="3.4.6">
       <section-title>
        Domain adaptation
       </section-title>
       <paragraph>
        We use the data from the shared task of SANCL-2012{sup:9} to perform out-of-domain evaluation. Note that we do not perform text normalization in our experiments, which contributes a lot in the evaluated systems in the shared task. There are five domains in the task: answers, emails, newsgroups, reviews, and weblogs. The participants in the shared task are provided with five sets of unlabeled sentences (about 27,000 to 2,000,000 per domain), shown in Table 12. Two settings are used in the experiments. The first one is that we use the induced model (used in Section 3.4.3) to parse the out-of-domain test data sets, shown as “MetaParser” in Table 12. The second is that we use the out-of-domain unlabeled sentences to generate new meta features and then retrain new parsers, shown as “adMetaParser” in Table 12. From the table, we find that MetaParser and adMetaParser perform better than Baseline. AdMetaParser provided the best scores on 4 of 5 data sets. Because there is only 27,274 unlabeled sentences for the answers domain, it is reasonable that adMetaParser only provides slightly better results than Baseline. We also find that the more unlabeled sentences we provide, the more benefits we obtain. Note that compared with in-domain evaluation, out-of-domain evaluation here has less unlabeled sentences.
       </paragraph>
      </section>
      <section label="3.4.7">
       <section-title>
        Analysis
       </section-title>
       <paragraph>
        Here, we analyze the effect of the meta features on the data sparseness problem.
       </paragraph>
       <paragraph>
        We first check the effect of unknown features on the parsing accuracy. We calculate the number of unknown features in each sentence and compute the average number per word. The average numbers are used to eliminate the influence of varied sentence sizes. We sort the test sentences in increasing orders of these average numbers, and divide equally into five bins. BIN 1 is assigned the sentences with the smallest numbers and BIN 5 is with the largest ones. Fig. 5 shows the average accuracy scores of the Baseline parsers against to the bins. From the figure, we find that for both two languages the Baseline parsers perform worse while the sentences contain more unknown features.
       </paragraph>
       <paragraph>
        Then, we investigate the effect of the meta features. We calculate the average number of active meta features per word that are transformed from the unknown features for each sentence. We sort the sentences in increasing order of the average numbers of active meta features and divide them into five bins. Fig. 6, Fig. 7 show the results, where “Better” is for the sentences where the meta parsers provide better results than the baselines and “Worse” is for those where the meta parsers provide worse results. We find that the gap between “Better” and “Worse” becomes larger while the sentences contain more active meta features for the unknown features. The gap means performance improvement. This indicates that the meta features are very effective in processing the unknown features.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="4">
     <section-title>
      Case 2: POS tagging
     </section-title>
     <section label="4.1">
      <section-title>
       The tagging model
      </section-title>
      <paragraph>
       Given an input sentence, the task of POS tagging is to assign a POS tag to each word in the sentence. Define X as the set of input sentences, Y as the set of possible tag sequences, and {a mathematical formula}D=(x1,y1),…,(xi,yi),…,(xn,yn) as a training set, where {a mathematical formula}xi∈X and {a mathematical formula}yi∈Y. A sentence is denoted by {a mathematical formula}x=(w1,…,wi,…,wm), where {a mathematical formula}wi refers to the ith word. Let {a mathematical formula}y=(t1,…,ti,…,tm) be a POS tag sequence for x, and {a mathematical formula}Y(x) be the set of all the possible tag sequences for x, where {a mathematical formula}ti is the tag of {a mathematical formula}wi.
      </paragraph>
      <paragraph>
       A linear model to compute the score of a tag sequence {a mathematical formula}y∈Y(x) is defined as,{a mathematical formula} where {a mathematical formula}f(x,y) is a high-dimensional feature vector based on a set of predefined feature templates over y and x, and w is the weight vector for the features. For word {a mathematical formula}wi in the sentence x, the set of features {a mathematical formula}fi(x,ti,ti−1) considers the combination of the surrounding context and the possible tag of {a mathematical formula}wi.
      </paragraph>
      <paragraph>
       The task of the tagging algorithms for a given sentence x is to find {a mathematical formula}y⁎,{a mathematical formula}
      </paragraph>
      <paragraph>
       We use the Viterbi algorithm to obtain the optimal tag sequence. The weights of the features are trained by the averaged perceptron algorithm [50].
      </paragraph>
      <section label="4.1.1">
       <section-title>
        Base features
       </section-title>
       <paragraph>
        We study part-of-speech tagging for both English and Chinese, following previous studies [51], [52], [37] for the definition of the feature templates. For English, we adopt the feature templates used in [51] as listed in Table 13, where ‘#’ refers the number of template types in each line, wc refers to a word class such as capitalization, digits, and punctuation, {a mathematical formula}Nwi is the number of characters of {a mathematical formula}wi, prefix({a mathematical formula}wi,k) refers to the first k characters of {a mathematical formula}wi, and suffix({a mathematical formula}wi,k) refers to the last k characters of {a mathematical formula}wi. The templates consider the surrounding context of {a mathematical formula}wi and tags {a mathematical formula}ti,ti−1, and there are 46 types of the templates in the table in total.
       </paragraph>
       <paragraph>
        Here, we use an example to show how to generate the base features for {a mathematical formula}wi based on the five templates in the first line in the table. Suppose that we have sentence “I got a new monitor yesterday.” and {a mathematical formula}wi is “monitor” that is tagged as “NN”. Fig. 8 shows the example of generating base features. For each template, we generate the corresponding base feature. Finally, we obtain five features, which are “T:NN, W_2:a”, “T:NN, W_1:new”, “T:NN, W0:monitor”, “T:NN, W1:yesterday”, and “T:NN, W2:.”.
       </paragraph>
       <paragraph>
        For Chinese, we adopt the templates proposed by [52] and [37]. Table 14 shows the templates, where {a mathematical formula}ci,k refers to the kth Chinese character of {a mathematical formula}wi, while {a mathematical formula}ci,0 and {a mathematical formula}ci,−1 refer to the first and last character, respectively, {a mathematical formula}Nwi is the number of Chinese characters in {a mathematical formula}wi, prefix({a mathematical formula}wi,k) refers to the first k characters of {a mathematical formula}wi, and suffix({a mathematical formula}wi,k) refers to the last k characters of {a mathematical formula}wi. In total, there are 21 types of the base feature templates for Chinese.
       </paragraph>
       <paragraph>
        With the base features, the score function of the base tagger is,{a mathematical formula} where {a mathematical formula}fb is the base features and {a mathematical formula}wb is the corresponding weights.
       </paragraph>
      </section>
     </section>
     <section label="4.2">
      <section-title>
       Meta tagger
      </section-title>
      <paragraph>
       A meta tagger is implemented by following the learning framework. Firstly, we perform word segmentation (if applicable) and POS tagging for the unlabeled data using the baseline tagger. Subsequently, we collect base feature instances from the auto-processed data and transform them into meta features. Finally, a new tagger is trained with the base features and the meta features on the training data.
      </paragraph>
      <section label="4.2.1">
       <section-title>
        Feature bucketing
       </section-title>
       <paragraph>
        Fig. 9 shows an example for base feature instantiation using the sentences “He bought the monitors” and “He monitors the kids”. For each word in the sentences, we generate the base features based on the templates in Table 13. Fig. 9-(a) shows the tagging results, and Fig. 9-(b) shows some generated features when the current word {a mathematical formula}wi is “monitors” which can be tagged as “NNS” and “VBZ” in different sentences. For example, in sentence “He bought the monitors.”, we have “T:NNS W_1:the”, “T:NNS W_1:the W0:monitors”, and other features. In the final step shown in Fig. 9-(c), we merge the same features and take their counts.
       </paragraph>
       <paragraph>
        As for dependency parsing, we perform feature transformation by considering the frequencies of the base features. However, a slightly different strategy is taken for the bucketing of feature instances.{sup:10} The number of POS tags is often more than 30 (36 tags in PTB and 33 tags in CTB), much larger than the number of directions for dependency parsing (only LEFT and RIGHT). This makes us to divide one base feature into two parts: feature tag and feature context. The feature tag is the current tag {a mathematical formula}ti and the feature context is the remaining content of a feature instance. For example, in feature “T:NNS W_1:the”, the feature tag is “T:NNS” and the feature context is “W_1:the”. The preferences of the base features is measured by ranking the co-occurrence frequencies between the feature tags and the feature contexts. The basic idea behind is that if a feature context often co-occurs with a feature tag, there is a clue to assign the tag to the current word under this feature context.
       </paragraph>
       <paragraph>
        We put the features that have the same feature context into one list and consider two factors for computing preferences. The first factor considers the ranking positions in each list. The features are sorted in decreasing order and mapped to different values. Suppose that we have base feature {a mathematical formula}fb∈L(Ci), where {a mathematical formula}Ci is the feature context of {a mathematical formula}fb and {a mathematical formula}L(Ci) is the list of {a mathematical formula}Ci. Then {a mathematical formula}fb is mapped by the following function,{a mathematical formula}
       </paragraph>
       <paragraph>
        The second factor considers the size of the list {a mathematical formula}L(Ci). Then {a mathematical formula}fb is mapped by the following function,{a mathematical formula} where {a mathematical formula}|L(Ci)| is the size of {a mathematical formula}L(Ci).
       </paragraph>
       <paragraph>
        We represent the feature preference as a string that concatenates the two factors with the index {a mathematical formula}Idxfb of the template from which {a mathematical formula}fb is generated. The preference {a mathematical formula}Pfb is “{a mathematical formula}Φ(fb)Ψ(fb)Idxfb”.{sup:11} Note that TOPO and NTO are defined for the features that are not in the list. But in our final systems, we do not use them.{sup:12}
       </paragraph>
       <paragraph>
        Compared with the mapping function of feature transformation for dependency parsing, the function for part-of-speech tagging is different. However, they share the similar idea that we rank the base features according to the frequencies and map to discrete values.
       </paragraph>
      </section>
      <section label="4.2.2">
       <section-title>
        Meta features
       </section-title>
       <paragraph>
        We define meta feature templates based on the feature buckets. The feature templates are listed in Table 15, where {a mathematical formula}fb is a base feature. The first template uses the preference value itself, while the second and third templates combine the value with tag {a mathematical formula}ti and word surface {a mathematical formula}wi. The number of the new features is small. The number of the possible values of {a mathematical formula}Pfb is at most {a mathematical formula}6×N(TB), where {a mathematical formula}N(TB) refers to the number of the base feature templates that is small. Thus it is possible to have {a mathematical formula}6×N(TB) for the first type, {a mathematical formula}6×N(TB)×N(POS) for the second type, and {a mathematical formula}6×N(TB)×N(WORD)×N(POS) for the third one, where {a mathematical formula}N(POS) refers to the number of part-of-speech tags and {a mathematical formula}N(WORD) refers to the number of words. We remove any meta feature related to word form if the word is not one of the Top-K{sup:13} most frequent words in the training data. This method can further reduce the size of the feature sets.
       </paragraph>
      </section>
      <section label="4.2.3">
       <section-title>
        Meta tagger
       </section-title>
       <paragraph>
        Using the meta features based on the templates defined in Table 15, the score of a tag sequence {a mathematical formula}y∈Y(x) is,{a mathematical formula} where {a mathematical formula}fm(x,y) is a feature vector which is based on the meta features, and {a mathematical formula}wm is the corresponding weights.
       </paragraph>
      </section>
     </section>
     <section label="4.3">
      <section-title>
       Feature selection
      </section-title>
      <paragraph>
       According to the types of the base feature templates, the meta features are grouped into different sets. There are 46 types of the base feature templates for English (Table 13) and 21 for Chinese (Table 14). Although the numbers are small, the number of possible combinations is large. How to choose the effective features is an important problem. Here, we propose a simple but effective algorithm to select the features. Table 16 shows the algorithm. The input of the algorithm is base features {a mathematical formula}Fb and meta features {a mathematical formula}Fm={F1,…,Fi,…,FNT}, where {a mathematical formula}Fi refers to the set generated from the ith base feature template, and the output is the selected features. The procedure is listed as follows: 1) Evaluate feature set {a mathematical formula}Fi∈Fm. We combine {a mathematical formula}Fb with {a mathematical formula}Fi and train a tagger with them. The tagger is evaluated on the development data set and we get the accuracy score {a mathematical formula}si. 2) Sort the feature sets according to the evaluation results. 3) Add the feature sets one by one until the performance stops increasing.
      </paragraph>
     </section>
     <section label="4.4">
      <section-title>
       Experiments
      </section-title>
      <paragraph>
       We evaluate the effect of the meta features for the POS tagging model and conduct the experiments on English and Chinese data.
      </paragraph>
      <section label="4.4.1">
       <section-title>
        Data sets
       </section-title>
       <paragraph>
        For English, the Penn Treebank (PTB) [34] is used in our experiments. We create a standard data split: sections 0–18 is used for training, sections 19–21 for development, and sections 22–24 for testing. For the unlabeled data, we use the Brown WSJ corpus [41]. We use the Baseline Tagger to process the sentences of the Brown corpus.
       </paragraph>
       <paragraph>
        For Chinese, the Chinese Treebank version 5 (CTB5) is used in the experiments. There are two widely used data splits for CTB5 [53], [40], [54]: 1) CTB5D [38]: files 001–815 and files 1001–1136 for training, files 816–885 and 1137–1147 for testing, and files 886–931 and 1148–1151 for development. 2) CTB5J [54]: files 1–260, 400–931, and 1001–1151 for training, files 271–300 for testing, and files 301–325 for development. For the unlabeled data, we use the XIN_CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14) [42], which has around 311 million words whose segmentation and POS tags are given. We discard the annotations due to the differences in annotation policy between CTB and the Gigaword data. MMA [43] is used to train on the training data to perform word segmentation and the Baseline Tagger is used to process all the sentences in the unlabeled data.
       </paragraph>
       <paragraph>
        To reduce the size of the collected base features, we remove the entries which occur only once in the English data and less than four times in the Chinese data. We choose the numbers based on the initial results on the development sets.
       </paragraph>
       <paragraph>
        We measure the tagger quality by the accuracy, i.e. the percentage of tokens with the correct tag. We also use the error reduction to show the relative improvements.
       </paragraph>
      </section>
      <section label="4.4.2">
       <section-title>
        Results on development sets
       </section-title>
       <paragraph>
        In this section, we select the best feature combination via tuning on the development sets. The algorithm in Section 4.3 is applied to select the feature types. Table 17, Table 18 show the tagging results with different settings on the development data sets. We list the results with the most two effective feature types in the tables. For English, E1 is the tagger with the most effective type {a mathematical formula}&lt;ti,w0&gt; and E2 is the one with the second most effective type {a mathematical formula}&lt;ti,w1&gt;. For Chinese, the most two effective types are {a mathematical formula}&lt;ti,w0&gt; and {a mathematical formula}&lt;ti,ci,−1&gt;. For both English and Chinese, the most effective feature type is the one based on template {a mathematical formula}&lt;ti,w0&gt;. We build the final taggers (OURS) by adding the meta features selected by the algorithm. The results indicate that OURS achieve higher accuracy than the Baseline Taggers for both English and Chinese.
       </paragraph>
      </section>
      <section label="4.4.3">
       <section-title>
        Main results
       </section-title>
       <paragraph>
        In this section, we evaluate the final taggers (OURS) on the test data for English and Chinese and compare our results with the state-of-the-art results in the previous studies.
       </paragraph>
       <paragraph>
        For English, the results are shown in Table 19, where “err.red” refers to error reduction, “-sup” refers to the results of the supervised methods in the previous studies, and “-semi” refers to the results of the semi-supervised methods.{sup:14} From the table, we find that the final tagger outperforms the Baseline Tagger and the error reduction is 5.01%. The improvement is significant by McNemar's Test ({a mathematical formula}p&lt;0.001). Compared with the previous work, OURS performs better than the supervised methods of [56], [51] and provides the comparable accuracy with the semi-supervised systems of [56], [51], [55], [57]. As we mentioned before, Søgaard [56] and Spoustová et al. [55] select new sentences as labeled data with the help of more than one taggers. Suzuki and Isozaki [51] use 1G-word unlabeled data that is much larger than the Brown corpus we use. Søgaard [57] combines a unsupervised POS tagger with the semi-supervised condensed nearest neighbor algorithm and achieves the best reported result. Compared with theirs, our approach is much simpler. We believe that the performance of our tagger can be further enhanced by integrating their methods with our approach.
       </paragraph>
       <paragraph>
        For Chinese, the results are listed in Table 20, where the abbreviations used are the same as those in Table 19. The results indicate that the final tagger outperforms the Baseline Tagger and the error reduction was 4.89% on CTB5D and 4.87% on CTB5J. The improvement is significant by McNemar's Test ({a mathematical formula}p&lt;0.001). Compared with the previous work, OURS provides better results than the supervised methods{sup:15} of [53] and [40].
       </paragraph>
      </section>
      <section label="4.4.4">
       <section-title>
        Feature sizes and speed
       </section-title>
       <paragraph>
        We check the sizes of the added meta features and the speed in the testing stage. Table 21 shows the feature sizes and the testing speed of the Baseline taggers and OURS. From the table, we find that only a small size of the new features are added and the speed slows down a little bit (13.18% for English and 5.22% for Chinese). The feature size only increases by 0.16% for English and 0.15% for Chinese. Note that we train the Baseline tagger and our tagger with different feature sets on the same training data. The approaches of [56] and [55] enlarge the training data and result in slow training speed.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="5">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      Previous work on using large scale unannotated text to improve structured prediction tasks can be classified into two main categories. The first category of methods collects statistical information from raw text, and use them as features to enhance a baseline model. The second category of methods first uses a baseline model to process raw texts, and then collect statistical information from the automatically processed results. Our method described in this article belongs to the second one.
     </paragraph>
     <section label="5.1">
      <section-title>
       Information from raw data
      </section-title>
      <paragraph>
       Typical information collected from raw text include word clusters [14], embeddings [18], n-grams [58], word co-occurrence patterns [59], selectional preference [48], mutual information [60], and crossing entropy [61]. Such information has been used to improve structured prediction tasks such as parsing, named entity recognition and word segmentation. The advantage of this category of methods is that they only rely on raw data that is usually less noisy than the auto-processed data. Moreover, it is easy to be used in different tasks using the same information, such as word clusters for parsing and named entity recognition [14], [13]. However, most methods in this category only learn the relations between words.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Information from auto-processed data
      </section-title>
      <paragraph>
       More complex information can be collected from automatically processed data, although statistical errors exist in such data. This category can be further divided into two types: much previous work in this category uses the whole auto-processed sentences as newly-labeled data to re-train models, while others learn information from partial sentences and represent the information as additional features to the systems.
      </paragraph>
      <paragraph>
       1) Using whole sentences: The line of methods that exploits automatically processed data is connected to self-training [62], which uses automatically annotated data as additional training examples, co-training [63], which filters the additional training examples by agreement between the baseline methods, and tri-training [64], which filters the additional training examples by voting three baseline models. Different from the above methods, our method collects statistical information from automatically processed data, but do not use the whole auto-processed sentences as additional training data. The effect of noise in automatic analysis can be reduced by frequency-based bucketing of feature instances.
      </paragraph>
      <paragraph>
       2) Using partial structures: Our method belongs to this type. In our method, we transform the base features, which are partial structures from sentences, into the meta features. Similar in spirit to our method, Chang and Han [65] collects {a mathematical formula}χ2 information to improve segmentation, Want et al. [22] collect n-gram information to improve segmentation and POS tagging, Mirroshandel et al. [66] collect lexical affinities to improve dependency parsing, and Chen et al. [21], [24] collect sub-tree distributions to improve dependency parsing. The previous methods in [21], [24] represent features on bigram and trigram subtrees, while our method represents meta features on any base features defined over arbitrary structures. Our method here is more general and the meta features used in our method can represent more complex structures. In addition, our method is easy to be applied to a new task, such as the POS tagging task demonstrated in this article. The meta parser outperforms the parsers of [21], [24]. In addition to the types of statistics above, automatically processed data have also been used to train simple sub-models, which are used as features to enhance a base model trained on annotated data [67], [51]. The use of simple sub-models can also be regarded as a type of statistics. The advantage of using statistics as additional features is that they address noise in automatically processed data.
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      In this article, we have presented a simple yet empirically highly effective framework of training discriminative models. The main idea is to reduce the sparseness of features by frequency bucketing over large-scale automatically processed data, and defining meta features from the resulting feature clusters. When applied to dependency parsing and part-of-speech tagging, the proposed approach significantly improves the accuracies. In the dependency parsing task, the meta parser achieves comparable accuracy with the best known parsers on the English data and the best accuracy on the Chinese data so far. In the part-of-speech tagging task, our tagger provided better results than the state-of-the-art Baseline systems by about 5% error reduction when tested on both English and Chinese data. Further analysis indicate that the meta features are very effective in processing the unknown features.
     </paragraph>
     <paragraph>
      For future work, there are several ways in which this research could be extended. First, we plan to experiment with alternative clustering algorithms for feature transformation in the current tasks. Second, we plan to use larger raw data to perform feature transformation. Third, we plan to combine other semi-supervised methods, such as the approach of [44], with ours to enhance our parsers. Finally, we could apply the framework to other NLP tasks (for example constituency parsing and NER).
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>