<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Automated aerial suspended cargo delivery through reinforcement learning.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Unmanned aerial vehicles (UAVs) show potential for use in remote sensing, transportation, and search and rescue missions [1]. One such UAV, the quadrotor, is an ideal candidate for autonomous cargo delivery due to its high maneuverability, vertical takeoff and landing, single-point hover, and ability to carry loads 50% to 100% of their body weight. For example, cargoes may consist of food and supply delivery in disaster struck areas, patient transport, or spacecraft landing. The four rotor blades of a quadrotors make them easier to maneuver than helicopters. However, they are still inherently unstable systems with complicated non-linear dynamics. The addition of a suspended load further complicates the system's dynamics, posing a significant control challenge. Planning motions that control the load position is difficult, so automated learning methods are necessary for mission safety and success.
     </paragraph>
     <paragraph>
      Recent research has begun to develop control policies for mini UAVs, including approaches that incorporate learning [1]. Learning system dynamics model parametrization has been successful for adaptation to changes in aerodynamics conditions and system calibration [2]. Other approaches, such as iterative learning methods for policy development, have been shown to be effective for aggressive maneuvers [3]. Another learning method, expectation–maximization, has been applied to the problem of quadrotor trajectory tracking with a linear model [4]. Even the task of suspended load delivery has been addressed for UAVs [5], [6], [7], [8], [9]. Our recent work used reinforcement learning (RL) in discrete action spaces to generate swing-free trajectories for quadrotors between two pre-defined, obstacle-free waypoints [5]. RL provided several advantages over previous work with dynamic programming [9]: a single learning phase leading to the generation of multiple trajectories, better compensation for accumulated error resulting from model approximation, and lack of knowledge of the detailed system dynamics. In this article, we extend the work in [5] through the development of an autonomous agent for aerial cargo delivery that works in environments with static obstacles. This method plans and generates trajectories through sampling of the environment and the system dynamics. In contrast to work currently under submission [10], which is concerned with computationally efficient continuous action selection and reinforcement learning for systems with high-dimensional continuous inputs in obstacle-free spaces, the work presented here delivers fully-automated planning agent for aerial cargo delivery in environments with static obstacles that relies on the discrete action space to perform trajectory generation along a collision-free path. The distinction between discrete and continuous action spaces is important because it is a basis for aerial cargo delivery that is both collision-free and with bounded load displacement. Manipulation of a suspended load has been performed with both RL [6] and model predictive control [7]. However, both of these methods require a pre-planned trajectory for the load to track and generate input that controls the UAV. They have an implicit assumption of the obstacle-free space in the area around the reference trajectory.
     </paragraph>
     <paragraph>
      To learn control policy for minimizing the load displacement, we rely on approximate value iteration [11] with a specifically designed feature vector for value function approximation. There are two challenges we face. First, the approximate value iteration requires a state-space sampling domain to learn the policy. The learning must have sufficient sample-density to be successful. To provide sufficient sample-density while learning, we sample in the small state subspace around the goal but choose a feature vector that is defined beyond the sampling subspace. The second challenge we face, is that the planning for swing-free motion consists of a large action space (over 10{sup:6} actions). Such a large action space is impractical for learning, thus we learn in an action subspace, and plan with the larger action space. Relying on Lyapunov stability theory [12], the article contributes sufficient conditions that the Markov decision process (MDP) formulation (system dynamics, action space, and state-value function approximation) must meet to ensure the cargo delivery with minimal residual oscillations. Within the conditions we are free to change MDP properties as it suits our needs and to transfer learning to compatible MDPs. For example, for practicality we learn in a 2-dimensional action subspace and transfer the state-value function approximation to MDP with a 3-dimensional action space to plan altitude changing trajectories. As another example, we develop a novel path-following agent that minimizes the load displacement by action-space adaptation at every planning step. In the context of related work in the obstacle-free case [13], [14], we transfer to MDPs with state and action supersets and noisy dynamics using a behavior transfer function that transfers directly the learned value function approximation to the new domain with no further learning. Another approach examined action transfer between the tasks, learned the optimal policy and transferred only the most relevant actions from the optimal policy [15]. In obstacle-free spaces, we take the opposite approach; to save computational time, we learn a sub-optimal policy on a subset of actions, and transfer it to the expanded action space to produce a more refined plan. When planning a path-following trajectory, we work with a most relevant subset of the expanded action space. Partial policy learning for fixed start and goal states, manages state space complexity by focusing on states that are more likely to be encountered [16]. We are interested in finding minimal residual oscillations trajectories from different start states, but we do have a single goal state. Thus, all trajectories will pass near the goal state, and we learn the partial policy only in the vicinity of the goal state. Then, we apply the learned policy to an arbitrary start state.
     </paragraph>
     <paragraph>
      Motion planning methods, which define a valid, collision-free path for a robot, are often solved in configuration space ({a mathematical formula}Cspace), the space of all configurations. The valid, collision-free path lies in the collision-free portion of {a mathematical formula}Cspace, {a mathematical formula}Cfree. Many planning methods work by either learning and approximating the topology of {a mathematical formula}Cspace, e.g., Probabilistic Roadmap Methods (PRMs) [17], or by traversing a continuous path in {a mathematical formula}Cfree, e.g., Rapidly-Exploring Random Tree (RRT) and Expansive Space Tree (EST) methods [18], [19], [20]. One primary difference between PRMs and RRT/EST methods is that PRMs were designed to learn {a mathematical formula}Cfree topology once and then use this knowledge to solve multiple planning queries, where RRTs/ESTs expand from a single start and/or goal position for a single planning query. PRMs have been modified to work well for complex robotic problems, including moving obstacles [21], [22], noisily modeled environments (e.g., with sensor) [23], and localization errors [24], [25]. Also, PRMs have been previously integrated with RL, e.g., a manipulator moving in a dynamic space [26]. In contrast, we use the RL agent as a local planner to plan trajectories along a collision-free path obtained with PRMs.
     </paragraph>
     <paragraph>
      For the problem of suspended load swing reduction, we apply RL to automatically generate, test, and follow swing-free trajectories for a quadrotor. RL is integrated into both the motion planning and trajectory generation for a rotorcraft equipped with a suspended load and in an environment with static obstacles. In [5], we showed that RL could be used to reduce load displacement at the arrival to the goal state. Here, the agent is placed in a larger context of time-sensitive cargo delivery tasks. In this class of applications, the payload should be delivered free of collision as soon as possible, possibly following a reference path, while bounded load displacement is maintained throughout the trajectory. Beyond aerial robotics, the methods presented here are applicable to any constraint balancing task posed on a dynamical system, because aerial cargo delivery problem is such a task.
     </paragraph>
     <paragraph>
      The result and contribution of this work is a fully automated software system that plans and creates trajectories in obstacle-laden environments for aerial cargo delivery. Its essence is a RL agent that creates trajectories with minimal residual oscillations for a suspended load bearing UAV in obstacle-free spaces [5], and is described in detail in Sections 3.1 and 3.2. Further, to develop the automated system, we develop several novel methods: a RL path following agent that reduces the load displacement (Section 3.3), a framework for trajectory generation with constraints that avoids obstacles (Section 3), RL agent's integration with sampling-based path planning (Section 3.5), and development of an efficient rigid body model to represent a quadrotor carrying a suspended load (Section 3.4). We test the proposed methodology in Section 4 both in simulation and experimentally on a physical system (Fig. 1a).
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Preliminaries
     </section-title>
     <paragraph label="Definition 2.1">
      This article is concerned with joint UAV-suspended load systems. The load is suspended from the UAV's center of mass with an inelastic cable. A ball joint models the connection between the UAV's body and the suspension cable. We now define several terms that are used in this paper extensively. Load displacement or swingIs the position of the load, at time t, expressed in coordinates {a mathematical formula}η(t)=[ϕ(t)θ(t)]T with the origin in the center of the mass of the joint quadrotor–load system (see Fig. 1b).
     </paragraph>
     <paragraph label="Definition 2.2">
      Minimal residual oscillations trajectoryA trajectory of duration T is a minimal residual oscillations trajectory if for a given constant {a mathematical formula}ϵ&gt;0 there is a time {a mathematical formula}0≤t1≤T, such that for all {a mathematical formula}t≥t1, the load displacement is bounded by ϵ, i.e. {a mathematical formula}‖η(t)‖&lt;ϵ.
     </paragraph>
     <paragraph label="Definition 2.3">
      Bounded load displacement or Swing-free trajectoryA trajectory is a swing-free trajectory if it is the minimal residual oscillation trajectory for time {a mathematical formula}t1=0, in other words if the load displacement is bounded by a given constant throughout the entire trajectory, i.e. {a mathematical formula}‖η(t)‖&lt;ϵ,t≥0.
     </paragraph>
     <paragraph>
      Now, we present problem formulations for two aerial cargo delivery tasks, which specify the trajectory characteristics, task completion criteria, and dynamical constraints on the system.
     </paragraph>
     <paragraph label="Problem 2.1">
      Minimal residual oscillations taskGiven start and goal positional states {a mathematical formula}xs,xg∈{a mathematical formula}Cfree, and a swing-constraint ϵ, find a minimal residual oscillations trajectory {a mathematical formula}x=τ(t), ({a mathematical formula}0≤t≤T) from {a mathematical formula}xs to {a mathematical formula}xg for a holonomic UAV carrying a suspended load. The task is completed when the system comes to rest at the goal state: for some small positive constants {a mathematical formula}ϵd,ϵv, starting at time {a mathematical formula}tg≤T, {a mathematical formula}‖τ(t)−xg‖≤ϵd and {a mathematical formula}‖τ˙(t)‖≤ϵv, for {a mathematical formula}tg≤t≤T. The dynamical constraints of the system are the bounds on the acceleration vector.
     </paragraph>
     <paragraph label="Problem 2.2">
      In the above definition, the system completes the task if it reaches and remains inside a small interval around origin in the system's position-velocity space. The small constants {a mathematical formula}ϵd, and {a mathematical formula}ϵv define the interval size. In contrast, the swing-constraint can be met at a different time during the trajectory. Aerial cargo delivery, described next, requires the load displacement to be bounded by ϵ throughout the trajectory, while the position and velocity constants, {a mathematical formula}ϵd, {a mathematical formula}ϵv, must be reached at the end of the trajectory. Cargo delivery taskGiven start and goal positional states {a mathematical formula}xs,xg∈Cfree, and a swing-constraint ϵ, find a swing-free trajectory {a mathematical formula}x=τ(t) from {a mathematical formula}xs to {a mathematical formula}xg for a holonomic UAV carrying a suspended load. The task is completed when the system comes to rest at the goal state: for some {a mathematical formula}ϵd,ϵv, starting at time {a mathematical formula}tg≤T, {a mathematical formula}‖τ(t)−xg‖≤ϵd and {a mathematical formula}‖τ˙(t)‖≤ϵv, for {a mathematical formula}tg≤t≤T. The dynamical constraints of the system are the bounds on the acceleration vector.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Methods
     </section-title>
     <paragraph>
      Our goal is to develop a fully automated agent that calculates aerial cargo delivery (Problem 2.2) trajectories in environments with static obstacles. The trajectory must be collision-free, the load displacement must be bounded, and the UAV must arrive at the given goal coordinates. Within these constraints, the task needs to complete in the shortest time given the physical limitations of the system combined with the task constraints.
     </paragraph>
     <paragraph>
      Fig. 2 presents the proposed architecture. We learn the policy that reduces load displacement separately from the space geometry, and then combine them to generate trajectories with both characteristics. The minimal residual oscillations policy, described in Section 3.1, performs minimal residual oscillations task (Problem 2.1). Once the policy is learned, it can be used with varying state and action spaces to refine system performance according to the task specifications. Based on Lyapunov theory [12], Section 3.2 discusses the sufficient conditions for these modifications. In Section 3.3, we adapt the policy to follow a path, by modifying the action space at every step. To enable obstacle-avoidance, we use a planner to generate a collision-free path. Section 3.4 describes the geometric model of the cargo-bearing UAV that we use for efficient collision detection. After the policy is learned and a roadmap constructed, we generate trajectories that are both collision-free, and guarantee load displacement below a given upper bound. For given start and goal coordinates, the planner calculates a collision-free path between them. A path-following trajectory that maintains the acceptable load displacement is calculated using the method described in Section 3.5.
     </paragraph>
     <paragraph>
      The architecture has two clear phases: learning and planning. The learning for a particular payload is performed once, and used many times in any environment. The problem geometry is learned, and PRMs are constructed once, for a given environment and a maximum allowed load displacement. When constructed, roadmaps can be used for multiple queries in the environment for tasks requiring the same or smaller load displacement. The distinct learning phases and the ability to reuse both, the policy and the roadmap, are desirable and of practical use, because the policy learning and roadmap construction are time consuming.
     </paragraph>
     <section label="3.1">
      <section-title>
       Learning minimal residual oscillations policy
      </section-title>
      <paragraph>
       In this section, our goal is to find fast trajectories with minimal residual oscillations for rotorcraft aerial robots carrying suspended loads in obstacle-free environments. We assume that we know the goal state of the vehicle; the initial state may be arbitrary. Furthermore, we assume that we have a black box system's simulator (or a generative model) available, but our algorithm makes no assumptions about the system's dynamics.
      </paragraph>
      <paragraph>
       The approximate value iteration algorithm (AVI) [11] produces an approximate solution to a Markov Decision Process (MDP) defined with {a mathematical formula}(S,A,D,R), in continuous state spaces with a discrete action set. A linearly parametrized feature vector approximates the state-value function. It is in an expectation–maximization (EM) algorithm that relies on a sampling of the state space transitions, an estimation of the state value function using the Bellman equation [27], and a linear regression to find the parameters that minimize the least squared error.
      </paragraph>
      <paragraph>
       In our implementation, we use a deterministic MDP. The state space S is set of all vectors {a mathematical formula}s∈S, such that {a mathematical formula}s=[xyzx˙y˙z˙ϕLθLϕ˙Lθ˙L]T (Fig. 1b). Vector {a mathematical formula}p=[xyz]T is the position of the vehicle's center of mass, relative to the goal state. The vehicle's linear velocity is vector {a mathematical formula}v=[x˙y˙z˙]T. Vector {a mathematical formula}η=[ϕLθL]T represents the angles that the suspension cable projections onto xz and yz planes form with the z-axis (see Fig. 1b). The vector of the load's angular velocities is {a mathematical formula}η˙L=[ϕL˙θL˙]T. L is the length of the suspension cable. Since L is constant in this work, it will be omitted. To simplify, we also refer to the state as {a mathematical formula}s=[pTvTηTη˙T]T when we do not need to differentiate between particular dimensions in s. The action space, A is a set of linear acceleration vectors {a mathematical formula}a=[x¨y¨z¨]T discretized using equidistant steps centered around zero acceleration.
      </paragraph>
      <paragraph>
       The reward function, R, penalizes the distance from the goal state, and the load displacement angle. It also penalizes the negative z coordinate to provide a bounding box and enforce that the vehicle must stay above the ground. Lastly, the agent is rewarded when it reaches the goal. The reward function {a mathematical formula}R(s)=cTr(s) is a linear combination of basis rewards {a mathematical formula}r(s)=[r1(s)r2(s)r3(s)]T, weighted with vector {a mathematical formula}c=[c1c2c3]T, for some constants {a mathematical formula}a1 and {a mathematical formula}a2, where:{a mathematical formula}
      </paragraph>
      <paragraph>
       To obtain the state transition function samples {a mathematical formula}D(s0,a)=s, we rely on a simplified model of the quadrotor–load system, where the quadrotor is represented by a holonomic model of a UAV [1], [5], [28]. The simulator returns the next system state {a mathematical formula}s=[pTvTηTη˙]T when an action a is applied to a state {a mathematical formula}s0=[p0Tv0Tη0Tη˙0T]. Equations{a mathematical formula} describe the simulator. A vector {a mathematical formula}g′=[00g]T represents the gravity force vector, and △t is the duration of the time step. L is the length of the suspension cable measured as distance between the cargo's and joint quadrotor–load system's centers of mass. Because the cargo's mass determines the center of the mass of the joint system, it influences the load's motion indirectly, through the effective cable length L.
      </paragraph>
      <paragraph>
       The state value function V is approximated with a weighted feature vector {a mathematical formula}F(s). The feature vector chosen for this problem consists of four basis functions, corresponding to features we wish to minimize for task completion. In our case, it consists of squares of the vehicle's distance to the goal, its velocity magnitude, and load's angular displacement and velocity magnitude:{a mathematical formula} where {a mathematical formula}Θ∈R4.
      </paragraph>
      <paragraph>
       To learn the approximation of the state value function, AVI learns parametrization Θ. Starting with an arbitrary vector Θ, in each iteration, the state space is randomly sampled to produce a set of new state samples M. The samples are uniformly, randomly drawn from a 10-dimensional interval centered around the goal state. A new estimate of the state value function is calculated according to {a mathematical formula}V(s)=R(s)+γmaxa⁡ΘTF∘D(s,a) for all samples {a mathematical formula}s∈M. {a mathematical formula}0&lt;γ&lt;1 is a discount factor, and {a mathematical formula}D(s,a) is sampled transition when action a is applied to state s. A linear regression then finds a new value of Θ that fits the calculated estimates {a mathematical formula}V(s) into a quadratic form {a mathematical formula}ΘTF(s). The process repeats until a maximum number of iterations is performed. Note, that the state-value function approximation (2) is defined outside of the sampling interval.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Minimal residual oscillations trajectory generation
      </section-title>
      <paragraph>
       After learning the parametrization Θ of value function approximation, we can plan trajectories using greedy policy {a mathematical formula}π:S→A induced by the approximated value function, V,{a mathematical formula} where {a mathematical formula}D(s,a) is the sampled state resulting from applying action a to a state s. When applied to the system, the resulting action moves the system to the state associated with the highest estimated value. The algorithm starts with an arbitrary initial state. Then it finds an action according to (3). The action is used to transition to the next state. The process repeats until the goal is reached or the trajectory exceeds a maximum number of steps. Proposition 3.1 gives sufficient conditions that the state-value function approximation, action state space and system dynamics need to meet to guarantee a plan that leads to the goal state.
      </paragraph>
      <paragraph label="Proposition 3.1">
       Let{a mathematical formula}sgbe the goal state. If:
      </paragraph>
      <list>
       <list-item label="1.">
        All components of vectorΘare negative,{a mathematical formula}Θi&lt;0, for{a mathematical formula}∀i∈{1,2,3,4},
       </list-item>
       <list-item label="2.">
        Action space, A, allows transitions to a higher-valued state,{a mathematical formula}∀s∈S∖{sg},∃a∈Athat{a mathematical formula}V(πA(s))&gt;V(s), and
       </list-item>
       <list-item label="3.">
        {a mathematical formula}sgis global maximum of function V,
       </list-item>
      </list>
      <paragraph label="Proof">
       To show that {a mathematical formula}sq is an asymptotically stable point, we need to find a discrete time Lyapunov control function {a mathematical formula}W(s), such that
       <list>
        {a mathematical formula}W(s(k))&gt;0,for∀s(k)≠0{a mathematical formula}W(sg)=0,{a mathematical formula}△W(s(k))=W(s(k+1))−W(s(k))&lt;0,∀k≥0{a mathematical formula}△W(sg)=0,wheresg=[0000000000]TLet
       </list>
       <paragraph>
        {a mathematical formula}W(s)=−V(s)=−ΘT[‖p‖2‖(v)‖2‖η‖2‖η˙‖2]. Then {a mathematical formula}W(0)=0, and for all {a mathematical formula}s≠sg, {a mathematical formula}W(s)&gt;0, since {a mathematical formula}Θi&lt;0.{a mathematical formula}△W(s(k))=−(V(s(k+1))−V(s(k)))&lt;0 because of the assumption that for each state there is an action that takes the system to a state with a higher value. Lastly, since {a mathematical formula}V(sg) is global maxima, {a mathematical formula}W(sg) is global minima, and {a mathematical formula}△W(sg)=−(V(sg)−V(sg))=0.Thus, W is a Lyapunov function with no constraints on s, and is globally asymptotically stable. Therefore, any policy-following function W (or V) will lead the system to the unique equilibrium point.  □
       </paragraph>
      </paragraph>
      <paragraph>
       Proposition 3.1 connects the MDP problem formulation (states, actions, and transition function) and state-value approximation with Lyapunov stability analysis theory [12]. If MDP and V satisfy the conditions, the system is globally uniformly stable, i.e. a policy generated under these conditions will drive the quadrotor–load system from any initial state s to the goal state {a mathematical formula}sg=0. We empirically show that the conditions are met. Proposition 3.1 requires all components of vector Θ to be negative. As we will see in the 4.2, the empirical results show that is the case. These observations lead to several practical properties of the induced greedy policy that we will verify empirically:
      </paragraph>
      <list>
       <list-item label="•">
        The policy is agnostic to the simulator used; the simulator defines the transition function, and along with the action space, defines the set of reachable states. Thus, as long as the conditions of Proposition 3.1 are met, we can switch the simulators we use. This means that we can train on a simple simulator and generate a trajectory on a more sophisticated model that would predict the system better.
       </list-item>
       <list-item label="•">
        The policy can be learned on a state space subset that contains the goal state, and the resulting policy will work on the whole domain where the conditions above hold, i.e., where the value function does not have other maxima. We show this property in Sections 4.2.1 and 4.2.2.
       </list-item>
       <list-item label="•">
        The induced greedy policy is robust to noise; as long as there is a transition to a state with a higher value, the action will be taken and the goal will be attained. Section 4.2.1 presents the empirical evidence for this property.
       </list-item>
       <list-item label="•">
        The action space between learning and the trajectory generation can change, and the algorithm will still produce a trajectory to the goal state. For example, to save computational time, we can learn on a smaller, more coarse discretization of the action space to obtain the value function parameters, and generate a trajectory on a more refined action space which produces a smoother trajectory. We demonstrate this property during the altitude changing flight experiment in Section 4.2.2. This property also allows us to create a path-following algorithm in Section 3.3 by restricting the action space only to actions that maintain proximity to the reference trajectory.
       </list-item>
      </list>
      <paragraph>
       Since we use an approximation to represent a value function and obtain an estimate iteratively, the question of algorithm convergence is twofold. First, the parameters that determine the value function must converge to a fixed point. Second, the fixed point of the approximator must be close to the true value function. Convergence of the algorithm is not guaranteed in the general case. Thus, we show empirically that the approximator parameters stabilize. To show that the policy derived from a stabilized approximator is sound, we examine the resulting trajectory. The trajectory needs to be with minimal residual oscillations at the arrival at the goal state, and be suitable for the system.
      </paragraph>
      <paragraph>
       Thus far, we used RL to learn the minimal residual oscillations task (Problem 2.1). The learning requires a feature vector, and a generative model of system dynamics, to come up with the feature vector parametrization. Then, in the distinct trajectory generation phase, using the same feature vector, and possibly different simulators, states, and action spaces, we create trajectories. The learning is done once for a particular payload, and the learned policy is used for any number of trajectories. Once the trajectory is created, it is passed to the lower-level vehicle controller.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Swing-free path-following
      </section-title>
      <paragraph>
       To plan a path-following trajectory that reduces load's oscillations, we develop a novel approach that takes advantage of findings from Section 3.2 by applying constraints on the system. In particular, we rely on the ability to change the action space and still complete the task.
      </paragraph>
      <paragraph label="Problem 3.1">
       Let {a mathematical formula}P=[r1,..,rn]T be a reference path, given as the list of quadrotor's center of mass Cartesian coordinates in 3D space, and let {a mathematical formula}dP(s) be shortest Euclidean distance between the reference path and the system's state {a mathematical formula}s=[pTvTηTη˙T]T:{a mathematical formula} Then we can formulate the swing-free path-following task, as: Swing-free path-following taskGiven a reference path P, and a start and a goal positional states {a mathematical formula}xs,xg∈P, find a minimum residual oscillations trajectory from {a mathematical formula}xs to {a mathematical formula}xg that minimizes the accumulated squared error {a mathematical formula}∫0∞(dP(s))2dt.
      </paragraph>
      <paragraph>
       To keep the system in the proximity of the reference path, we restrict the action space {a mathematical formula}As⊂A to only the actions that transition the system in the proximity of the reference path, using proximity constant {a mathematical formula}δ&gt;0{a mathematical formula} In the case {a mathematical formula}As=∅, we use an alternative action subset that transitions the system to the k closest position to the reference trajectory,{a mathematical formula} where {a mathematical formula}argmink denotes k smallest elements. We call k the candidate action set size constant. Admissible action set {a mathematical formula}As⊂A represents constraints on the system.
      </paragraph>
      <paragraph>
       The action selection step, or the policy, becomes the search for action that transitions the system to the highest valued state chosen from the {a mathematical formula}As subset,{a mathematical formula} The policy given in (7) ensures state transitions in the vicinity of the reference path P. If it is not possible to transition the system within given ideal proximity δ, the algorithm selects k closest positions and selects an action that produces the best minimal residual oscillations characteristics upon transition (see Algorithm 1). Varying δ, which is the desired error margin, controls how close we desire the system to be to the reference path. Parameter k gives the weight to whether we prefer the proximity to the reference trajectory, or load displacement reduction. Choosing very small k and δ results in trajectories that are as close to the reference trajectory as the system dynamics allows, at the expense of the load swing. Larger values of the parameters allow more deviation from the reference trajectory, and better load displacement results.
      </paragraph>
     </section>
     <section label="3.4">
      <section-title>
       UAV geometric model
      </section-title>
      <paragraph>
       We use PRMs to obtain a collision-free path. The PRMs require a geometric model of the physical space, and of a robot, to construct an approximate model of {a mathematical formula}Cspace. In order to simplify the robot model, we use its bounding volume. The selection of the robot model has implications for the computational cost of the collision detection [29]. The bounding volume can be represented with fewer degrees of freedom by omitting certain joints, and to address local uncertainty of the configuration space [30].
      </paragraph>
      <paragraph>
       For the cargo delivery task that bounds the load displacement with a given fixed limit, we consider the quadrotor–load system to be two rigid bodies joined with a ball joint and an inelastic suspension cable. This system is geometrically contained in a joint cylinder-cone volume. Modeling the quadrotor's body with a cylinder that encompasses it, allows us to ignore yaw for the path planning purposes. Similarly, cargo delivery is a non-aggressive maneuver, and quadrotor's pitch and roll are negligible, and are contained in a cylinder. For the load, we need only to consider the set of its possible positions, which is contained in a right circular cone with an apex that connects to the quadrotor's body, and with its axis perpendicular to quadrotor's body. The relationship between the cone's aperture, ρ, and load displacement {a mathematical formula}η=[ϕθ]T that need to be satisfied for collision-free trajectories is {a mathematical formula}cos⁡(ρ/2)&gt;(1+tan2⁡ϕ+tan2⁡θ)−0.5. Connecting the cylindrical model of the quadrotor body with the cone model of its load gives us the geometrical model of the quadrotor carrying a suspended load. Since the quadrotor's body stays orthogonal to the z-axis, the cylinder-cone model can be treated as a single rigid body with 3 degrees of freedom. Fig. 1c shows the fit of the AscTec Hummingbird quadrotor carrying a suspended load fitting into the cylinder-cone bounding volume. The clearance surrounding the quadrotor's body area needs greater than the maximal path-following error, {a mathematical formula}maxt⁡dP(s(t)). The model, will produce paths that leave enough clearance between the obstacles to accommodate for the maximum allowable load displacement.
      </paragraph>
      <paragraph>
       Approaching the problem in this manner, allows PRM to perform fast collision-checking without burdening path planning with the knowledge of complicated system dynamics. Because the bounding volumes for tasks with smaller bounds on the load displacement are contained within volumes for tasks that allow greater swing, a more conservative model can be used for multitude of tasks having lesser upper bounds (see Fig. 3). The limitation of the reduced model is that it does not take a direction of the load displacement into account. This could be easily remedied with replacing a circular cone with elliptic one for example, and then pairing with a compatible RL agent that ensures the trajectory adheres to the geometric model.
      </paragraph>
     </section>
     <section label="3.5">
      <section-title>
       Path planning and trajectory generation integration
      </section-title>
      <paragraph>
       Fig. 2 shows the flowchart of the trajectory generation process. This method learns the dynamics of the system through AVI, as outlined in Section 3.1. Monte Carlo selection picks the best policy out of the multiple learning trials. Independently and concurrently, a planner learns the space geometry. When the system is queried with particular start and stop goals, the planner returns a collision-free path. The PRM edges are reference paths, and the nodes are waypoints. AVI provides the policy. The trajectory generation module generates a swing-free trajectory along each edge in the path, using Algorithm 1 and the modified AVI policy. If the trajectory exceeds the maximum allowed load displacement, the edge is bisected, and the midpoint is inserted in the waypoint list (see Algorithm 2). The system comes to a stop at each waypoint and starts the next trajectory from the stopped state. All swing-free trajectories are translated and concatenated to produce the final multi-waypoint, collision-free trajectory. This trajectory is sent to the low-level quadrotor controller [9] that performs trajectory tracking. The result is fully automated method that solves cargo delivery task (Problem 2.2).
      </paragraph>
      <paragraph>
       Line 12 in Algorithm 2 creates a trajectory segment that follows the PRM calculated path between adjacent nodes {a mathematical formula}(wi,wi+1). The local planer determines the path geometry. Although any local planner can be used with PRMs, in this setup we choose a straight line local planner to construct the roadmap. Thus, the collision-free paths are line segments. Prior to creating a segment, we translate the coordinate system such that the goal state is in the origin. Upon trajectory segment calculation, the segment is translated so that it ends in {a mathematical formula}wi+1. The swing-free path-following corresponds to Algorithm 1. When the reference path is a line segment with start in {a mathematical formula}s0 and end in the origin, such as in this setup, the distance d calculation defined in (4), and needed to calculate the action space subsets (5), and (6), can be simplified and depends only the start state {a mathematical formula}s0:{a mathematical formula}
      </paragraph>
      <paragraph>
       Algorithm 2 leads the system to the goal state. This is an implication of the asymptotic stability of the AVI algorithm. It means that the policy produces trajectories that, starting in any initial state {a mathematical formula}s0, come to rest at the origin. Because Algorithm 2 translates the coordinate system such that the next waypoint is always in the origin, the system passes through all waypoints until it reaches the end of the path.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Results
     </section-title>
     <paragraph>
      To evaluate the aerial cargo delivery software architecture, we first check each of the components separately, and then evaluate the architecture as a whole. Section 4.1 validates the learning and its convergence to a single policy. Section 4.2 evaluates policy for minimal residual oscillation trajectory generation and its performance under varying state spaces, action spaces, and system simulators, as Proposition 3.1 predicts. In Section 4.3, we address the quality of the swing-free path-following algorithm. After results for all components are evaluated, we verify the end-to-end process in Section 4.4.
     </paragraph>
     <paragraph>
      All of the computations were performed on a single core of an Intel i7 system with 8 GB of RAM, running Linux operating system. AVI and trajectory calculations were obtained with Matlab 2011. The experiments are performed using an AscTec quadrotor UAV carrying a small ball or a paper cup filled with water in a MARHES multi-aerial vehicle testbed [31]. This testbed and its real-time controller are described in detail in [9]. The testbed's real-time controller tracks the planned trajectory and controls the quadrotor. The testbed is equipped with a high-precision motion capture system to sense and collect the position of vehicle and the load during the experiments. This system provides a resolution of 1 mm at a minimum of 100 Hz. The quadrotor is 36.5 cm in diameter, weighs 353 g without a battery, and its payload is up to 350 g [32]. Meanwhile, the ball payload used in the experiments weighs 47 g and its suspension link length is 0.62 m. The suspended load is attached to the quadrotor at all times during the experiments. In experiments where the coffee cup is used, the weight of the payload is 100 g.
     </paragraph>
     <section label="4.1">
      <section-title>
       Learning minimal residual oscillations policy
      </section-title>
      <paragraph>
       To empirically verify the learning convergence, we run AVI in two configurations: 2D and 3D. Both configurations use the same discount parameter {a mathematical formula}γ&lt;1 to ensure that the value function is finite and are learning in the sampling box 1 m around the goal state with the load displacements under {a mathematical formula}10∘. The configurations also share the deterministic simulator (2). The 3D configuration trains the agent with a coarse three-dimensional action vector. Each direction of the linear acceleration is discretized in 13 steps, resulting in over 2000 total actions. In this phase of the algorithm, we are shaping the value function, and this level of coarseness is sufficient and practical. The most computationally intensive part of the learning is greedy policy (3). Performed for each sample in all iterations, the greedy policy evaluation scales with the number of actions. Learning in the coarse action space, then transferring for planning in the fine-grain action space, speeds up learning 1000 times. Thus, the transfer makes the learning practically feasible. AVI's approximation error decays exponentially with the number of iterations. A gradual increase in the sampling over iterations yields less error as the number of iterations increases [33]. Thus, we increase sampling linearly with the number of iterations in the 3D configuration.
      </paragraph>
      <paragraph>
       To assess the stability of the approximate value iteration, we ran the AVI 100 times, for 1000 iterations in the 3D configuration. Fig. 4a shows the trend of the norm of value parameter vector Θ with respect to {a mathematical formula}L2 norm. We can see that the {a mathematical formula}‖Θ‖ stabilizes after about 200 iterations with the mean of 361170. The empirical results show that the algorithm is stable and produces a consistent policy over different trials. The mean value of {a mathematical formula}Θ=[−86290−350350−1430−1160]T has all negative components, which means that the assumption for Proposition 3.1 holds. To evaluate learning progress, a trajectory generation episode was run after every learning iteration. Fig. 4b depicts the accumulated reward per episode, averaged over 100 trials. The accumulated reward converges after 200 iterations as well. Fig. 4 depicts trajectories with the quadrotor's starting position at {a mathematical formula}(−2,−2,1) over 100 trials after 1000 learning iterations. The trajectories are created as described in Section 3.2 with (2) simulating the movement at 50 Hz. Although there are slight variations in duration (see Fig. 4c), all the trajectories are similar in shape and are consistent, giving us confidence that the AVI converges. The load initially lags behind as the vehicle accelerates (see Fig. 4d), but then stabilizes to end in minimal swing. We can also see that the swing is bounded throughout the trajectory, maintaining the displacement under {a mathematical formula}10∘ for the duration of the entire flight (see Fig. 4d).
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Minimal residual oscillations trajectory generation
      </section-title>
      <paragraph>
       In this section we evaluate effectiveness of the learned policy. We show the policy's viability in the expanded state and action spaces in simulation in Section 4.2.1. Section 4.2.2 assesses the discrepancy between the load displacement predictions in simulation and encountered experimentally during the flight. The section also compares experimentally the trajectory created using the learned policy to two other methods: a cubic spline trajectory, which is a minimum time {a mathematical formula}C3-class trajectory without any pre-assumptions about the load swing, and, to a dynamic programming trajectory [9], an optimal trajectory for a fixed start position with respect to its MDP setup.
      </paragraph>
      <section label="4.2.1">
       <section-title>
        State and action space expansion
       </section-title>
       <paragraph>
        We evaluate the quality and robustness of a trained agent in simulation by generating trajectories from different distances for two different simulators. The first simulator is a deterministic simulator described in (2) and used in the learning phase. The second simulator is a stochastic simulator that adds up to 5% uniform noise to the state predicted with the deterministic simulator. Its intent is to simulate the inaccuracies and uncertainties of the physical hardware motion between two time steps. We compare the performance of our learned, generated trajectories with model-based dynamic programming (DP) and cubic trajectories. The cubic and DP trajectories are generated using methods described in [9], but instead of relying on the full quadrotor–load system, we use the simplified model given by (2). The cubic and DP trajectories are of the same duration as corresponding learned trajectories. The agent is trained in 3D configuration. For trajectory generation, we use a fine-grain, discretized 3D action space {a mathematical formula}A=(−3:0.05:3)3. This action space is ten times per dimension finer, and contains over 10{sup:6} different actions. The trajectories are generated at 50 Hz with a maximum trajectory duration of 15 seconds. All trajectories were generated and averaged over 100 trials. To assess how well a policy adapts to different starting positions, we choose two different fixed positions, {a mathematical formula}(−2,−2,1) and {a mathematical formula}(−20,−20,15), and two variable positions. The variable positions are randomly drawn from between 4 and 5 meters, and within 1 meter from the goal state. The last position measures how well the agent performs within the sampling box. The rest of the positions are well outside of the sampling space used to learn the policy, and evaluate generalization to an extended state space.
       </paragraph>
       <paragraph>
        Table 1 presents the averaged results with their standard deviations. We measure the end state and the time when the agent reaches the goal, the percentage of trajectories that reach the goal state within 15 seconds, and the average maximum swing experienced among all 100 trials. With the exception of the stochastic simulator at the starting position {a mathematical formula}(−20,−20,15), all experiments complete the trajectory within 4 cm of the goal, and with a swing of less than {a mathematical formula}0.6∘, as Proposition 3.1 predicts. The trajectories using the stochastic simulator from a distance of 32 meters {a mathematical formula}(−20,−20,15) do not reach within 5 cm because 11% of the trajectories exceed the 15-second time limit before the agent reaches its destination. However, we still see that the swing is reduced and minimal at the destination approach, even in that case. The results show that trajectories generated with stochastic simulator on average take {a mathematical formula}5% longer to reach the goal state, and the standard deviations associated with the results is larger. This is expected, given the random nature of the noise. However, all of the stochastic trajectories approach the goal with about the same accuracy as the deterministic trajectories. This finding matches our prediction from Section 3.2.
       </paragraph>
       <paragraph>
        The maximum angle of the load during its entire trajectory for all 100 trials depends on the inverse distance from the initial state to the goal state. For short trajectories within the learning sampling box, the swing always remains within {a mathematical formula}4∘, while for very long trajectories it could go up to {a mathematical formula}46∘. As seen in Fig. 4, the peak angle is reached at the beginning of the trajectory during the initial acceleration, and as the trajectory proceeds, the swing reduces. This makes sense, given that the agent is minimizing the combination of the swing and distance. When very far away from the goal, the agent moves quickly toward the goal state and produces increased swing. Once the agent is closer to the goal state, the swing component becomes dominant in the value function, and the swing reduces.
       </paragraph>
       <paragraph>
        Fig. 5 shows the comparison of the trajectories with the same starting position {a mathematical formula}(−2,−2,1) and the same Θ parameter, generated using the models above (AVI trajectories) compared to cubic and DP trajectories. First, we see that the AVI trajectories share a similar velocity profile (Fig. 5a) with two velocity peaks, both occurring in the first half of the flight. Velocities in DP and cubic trajectories have a single maximum in the second half of the trajectory. The resulting swing predictions (Fig. 5b) shows that in the last 0.3 seconds of the trajectory, the cubic trajectory exhibits a swing of {a mathematical formula}10∘, while the DP trajectory ends with a swing of less than {a mathematical formula}5∘. The AVI generated trajectories produce load displacement within {a mathematical formula}2∘ in the same time period. To assess energy of the load's motion and compare different trajectories in that way, we consider power spectral density (PSD). PSD, a signal processing tool, calculates amount of energy per frequency in a time series. Smaller energy per frequency value and narrower frequency range correspond to less load displacement. The area below the curve is total energy needed to produce the motion of the load. We calculated the PSD over the entire load trajectory signal, using Matlab's periodogram method from the Signal Processing Toolbox. The average energy of AVI deterministic ({a mathematical formula}E([ϕθ])=[0.00740.0073]), stochastic AVI ({a mathematical formula}E([ϕθ])=[0.00500.0050]), and DP trajectories ({a mathematical formula}E([ϕθ])=[0.00810.0081) load position signals, we find that AVI deterministic trajectory requires the least energy over the entire trajectory.
       </paragraph>
      </section>
      <section label="4.2.2">
       <section-title>
        Experimental results
       </section-title>
       <paragraph>
        As another approach to addressing the learning practicality, we first train the agent in fine-grain 2D configuration. The configuration uses a 0.05 action tile size, although only in the x and y directions. There are 121 actions in each direction, totaling to 121{sup:2} actions in the discretized space. Note that this action space, although it has over 10{sup:4} actions, is still two orders of magnitude smaller that the fine-grain 3D action space used for the planning with over 10{sup:6} actions. This configuration uses a fixed sampling methodology. The approximation error stabilizes to a roughly constant level after the parameters stabilize [33]. Once the agent is trained, we generate trajectories with the same planning parameters as in Section 4.2.1, for two experiments: constant altitude flight and flight with changing altitude. The planned trajectories are sent the physical quadrotor in the testbed.
       </paragraph>
       <paragraph>
        In the constant altitude flight, the quadrotor flew from {a mathematical formula}(−1,−1,1) to {a mathematical formula}(1,1,1). Fig. 6 compares the vehicle and load trajectories for the learned trajectory as flown and in simulation, with cubic and DP trajectories of the same length and duration. The vehicle trajectories in Fig. 6a suggest a difference in the velocity profile, with the learned trajectory producing a slightly steeper acceleration between 1 and 2.5 seconds. The learned trajectory also contains a 10 cm vertical move up toward the end of the flight. To compare the flown trajectory with the simulated trajectory, we look at the load trajectories in Fig. 6b. We notice the reduced swing, especially in the second half of the load's ϕ coordinate. The trajectory in simulation never exceeds {a mathematical formula}10∘, and the actual flown trajectory reaches its maximum at {a mathematical formula}12∘. Both learned load trajectories follow the same profile with three distinct peaks around 0.5 seconds, 2.2 seconds, and 3.1 seconds into the flight, followed by rapid swing control and reduction to under {a mathematical formula}5∘. The actual flown trajectory naturally contains more oscillations that the simulator did not model. Despite that, the limits, boundaries, and profiles of the load trajectories are close between the simulation and flown trajectories. This verifies the validity of the simulation results: the load trajectory predictions in the simulator are reasonably accurate. Comparing the flown learned trajectory with a cubic trajectory, we see a different swing profile. The cubic load trajectory has higher oscillation, four peaks within 3.5 seconds of flight, compared to three peaks for the learned trajectory. The maximum peak of the cubic trajectory is {a mathematical formula}14∘ at the beginning of the flight. The most notable difference happens after the destination is reached during the hover (after 3.5 seconds in Fig. 6b). In this part of the trajectory, the cubic trajectory shows a load swing of {a mathematical formula}5–12∘, while the learned trajectory controls the swing to under {a mathematical formula}4∘. Fig. 6b shows that the load of the trajectory learned with RL stays within the load trajectory generated using dynamic programming at all times: during the flight (the first 3.4 seconds) and the residual oscillation after the flight. Power spectral density in Fig. 6c shows that, during experiments, the learned trajectory uses less energy per frequency than cubic and DP trajectories.
       </paragraph>
       <paragraph>
        In the second set of experiments, the same agent was used to generate changing altitude trajectories that demonstrate ability to expand action space between. Note that the trajectories generated for this experiment used value approximator parameters learned on a 2D action space, in the xy-plane, and produced a viable trajectory that changes altitude because the trajectory generation phase used 3D action space. This property was predicted by Proposition 3.1 since the extended 3D action space allows transitions to the higher value states. The experiment was performed three times and the resulting quadrotor and load trajectories are depicted in Fig. 7. The trajectories are consistent between three trials and follow closely simulated trajectory (Fig. 7a). The load displacement (Fig. 7b) remains under {a mathematical formula}10∘ and exhibits minimal residual oscillations. Fig. 7c shows that the energy profile between the trials remains consistent. The supplemental video submission contains videos of both experiments.
       </paragraph>
       <paragraph>
        This section evaluated in detail AVI trajectories both in simulation and experimentally. The evaluations show that the method is robust to motion noise, can learn in small and plan in large spaces, and the simulation predictions are within {a mathematical formula}5∘ from the experimental observations.
       </paragraph>
      </section>
     </section>
     <section label="4.3">
      <section-title>
       Swing-free path-following
      </section-title>
      <paragraph>
       Path-following with reduced load displacement evaluation compares the load displacement and path-following errors of Algorithm 1 with two other methods: a minimal residual oscillations method described in Section 3.2, and path-following with no load displacement reduction. The path-following only method creates minimum time path-following trajectories, by choosing actions that transition the system as close as possible to the reference path in the direction of the goal state with no consideration to the load swing. We use three reference trajectories: a straight line, a two-line segment, and a helix. To generate a trajectory, we use action space discretized in 0.1 equidistant steps, and the same value function parametrization, Θ, used in the evaluations in Section 4.2 and learned in Section 3.1.
      </paragraph>
      <paragraph>
       Table 2 examines the role of the proximity parameter δ, and candidate actions set size parameter k. Recall that Algorithm 1 uses δ as a distance from the reference trajectory where all actions that transition the system within δ distance are be considered for load swing reduction. If there were no such actions, then the algorithm selects k actions that transition the system the closest to the reference trajectory, regardless of the actual physical distance. We look at two proximity factors and two action set size parameters. In all cases, the proposed method, swing-free path-following, exhibits smaller path-following error than minimal residual oscillations method, and smaller load displacement results than path-following only method. Also for each reference path, there is a set of parameters that provides good balance between path-following error and load displacement reduction.
      </paragraph>
      <paragraph>
       Fig. 8 presents results of the path-following and load displacement errors for the line and helix reference paths. We compare them to path-following only and minimal residual oscillations AVI algorithms. Fig. 8a displays the trajectories. In all three cases, the minimal residual oscillations algorithm took a significantly different path from the other two. Its goal was to minimize the distance to the goal state as soon as possible while reducing the swing, and it does not follow the reference path. Fig. 8b quantifies the accumulated path-following error. Path-following error for Algorithm 1 remains close to the path-following only trajectory. For both trajectories, the accumulated path-following error is one to two orders of magnitude smaller than for the minimal residual oscillations method. Examining the load displacement characteristics though power spectral analysis of the vector {a mathematical formula}η=[ϕθ]T time series, we notice that frequency profile of the trajectory generated with Algorithm 1 resembles closely that of the minimal residual oscillations trajectory (Fig. 8c). In contrast, power spectral density of path-following only trajectories contain high frequencies absent in the trajectories created with the other two methods. Thus, the proposed method for swing-free path-following, with its path-following error characteristics similar to the path-following only method, and its load displacement characteristics similar to the minimal residual oscillations method, offers a solid compromise between the two extremes.
      </paragraph>
     </section>
     <section label="4.4">
      <section-title>
       Automated aerial cargo delivery
      </section-title>
      <paragraph>
       In this section we evaluate the methods for an aerial cargo delivery task developed in Section 3.5. Our goal is to verify that the method finds collision-free paths and creates trajectories that closely follow the paths while not exceeding the given maximal load displacement. The method's performance in simulation is discussed in Section 4.4.1, and its experimental evaluation, in Section 4.4.2.
      </paragraph>
      <paragraph>
       The simulations and experiments were performed using the same setup as for minimal residual oscillations tasks in obstacle-free environments, described in Section 4.2. PRMs work by first sampling configurations (nodes), connecting those samples with local transitions (edges), thus creating a roadmap of valid paths. In our PRM set up, we use uniform random sampling of configurations for node generation, identify the 10 nearest neighbors to each configuration using Euclidean distance, and attempt connections between neighbors using a straight line planner with a resolution of 5 cm for edge creation. The path planning was done using the Parasol Motion Planning Library from Texas A&amp;M University [34].
      </paragraph>
      <section label="4.4.1">
       <section-title>
        Trajectory planning in Cafe
       </section-title>
       <paragraph>
        To explore conceptual applications of the quadrotors to domestic and assistive robotics and to test the method in a more challenging environment, we choose a virtual coffee shop setting for our simulation testing. In this setting, the cargo delivery tasks (Problem 2.2) include: delivering coffee, delivering checks to the counter, and fetching items from high shelves (see Fig. 9a). The UAV needs to pass a doorway, change altitude, and navigate between the tables, shelves, and counters. In all these tasks, both speed and load displacement are important factors. We want the service to be in a timely manner, but it is important that the swing of the coffee cup, which represents the load, is minimized so that the drink is not spilled. The Cafe is 30 meters long, 10 meters wide, and 5 meters tall.
       </paragraph>
       <paragraph>
        We generate the paths, and create trajectories for three different maximal load displacements ({a mathematical formula}1∘, {a mathematical formula}10∘, and {a mathematical formula}25∘). Since the path planning is the most complex for the largest bounding volume, and the same path can be reused for smaller maximal load displacements, we create the path once using the bounding volume with a {a mathematical formula}25∘ cone half aperture, and create path-following trajectories requiring the same or lower maximal load displacement. The proximity factor is {a mathematical formula}δ=5cm and the candidate action sets size is {a mathematical formula}k=500. We evaluate the number of added waypoints to meet the load displacement requirement, the trajectory duration, the maximum swing, and the maximum path-following error.
       </paragraph>
       <paragraph>
        Table 3 summarizes the results of the trajectory characteristics for the three tasks in the coffee shop for different maximal load displacement. It demonstrates that we can follow the path with an arbitrary small load displacement. The maximum swing along a trajectory always stays under the required load displacement bound. The path-following error stays within 10 cm, regardless of the path and decreases as the maximal load displacement bound decreases. Therefore, by designing the bounding volume with 10 cm of clearance, the quadrotor–load system will not collide with the environment. The delivery time and number of added waypoints increase with the decrease of the required load displacement bound, as expected. Although, it is common sense that slower trajectories produce less swing, the agent automatically chooses waypoints so not to needlessly slow down the trajectories.
       </paragraph>
       <paragraph>
        Fig. 10 depicts the trajectory of the quadrotor and the load during the coffee delivery task for required maximal load displacements of {a mathematical formula}1∘, {a mathematical formula}10∘, and {a mathematical formula}25∘. The trajectories are smooth, and the load's residual oscillations are minimal in all of them. The trajectory and path overlay is presented in Figs. 11a–11c. The number of inserted waypoints increases for the smaller angles. The path-following error over the trajectory length is displayed in Fig. 11d. The accumulated squared error profiles differ based on the load displacement angle bound, with the {a mathematical formula}1∘ trajectory having accumulated error significantly smaller than other two trajectories.
       </paragraph>
      </section>
      <section label="4.4.2">
       <section-title>
        Experimental evaluation
       </section-title>
       <paragraph>
        The goal of the experiment is to show the discrepancy between the simulation results and the observed experimental trajectories, and to demonstrate the safety and feasibility of the method by using a quadrotor to deliver a cup of water. To check the discrepancy between the simulation predictions of the maximum load displacement in simulation and experimentally, we run queries in two testbed environments to generate trajectories in simulation. The testbed environments are all 2.5 meters by 3 meters, with the ceiling height varying from 1.5 meters to 2.5 meters. The obstacles are uniform triangular prisms with 60 cm sides. Shorter obstacles are 60 cm tall, while the tall ones are 1.2 meters tall. They differ in number of obstacles, their sizes, and locations. The first testbed environment contains three obstacles positioned diagonally across the room and the same landing platform (see Fig. 9b). Two obstacles are 0.6 meters tall, while the third one is 1.2 meters tall. The second testbed environment is filled with five 1.2 meters tall obstacles. They are in the corners and in the middle of a 2.5 by 2 meters rectangle centered in the room (Fig. 9c). This is a challenging, cluttered space that allows us to experimentally test an urban environment setting.
       </paragraph>
       <paragraph>
        Table 4 summarizes the difference in observed versus predicted maximum load displacements for these tasks over three trials. The observed maximum displacement is between {a mathematical formula}4∘ and {a mathematical formula}5∘ higher experimentally than in simulation. This is expected and due to unmodeled system dynamics, noise, wind influence and other factors, and it matches load displacement observed during hover.
       </paragraph>
       <paragraph>
        Fig. 12 shows three trials of the experimentally flown trajectory in Environment 2 (Fig. 9c), with the predicted simulation. The vehicle trajectory (Fig. 12a) matches very closely between trials and the simulation. The load's trajectories (Fig. 12b) show higher uncertainty over the position of the load at any given time. However, the load displacement is bounded and stays within {a mathematical formula}10∘. The accumulated path-following error (Fig. 12d) is slightly larger in experiments than in simulation, but follows the similar profile. The power spectral density (Fig. 12c) is consistent between the three trials, and the simulated trajectory lacks higher frequencies, which is expected.
       </paragraph>
       <paragraph>
        The demonstration of the practical feasibility and safety of this method was demonstrated by using the system to deliver a cup of water to a static human subject. In this demonstration, the quadrotor's load is a 250 ml paper cup filled with 100 ml of water. In Environment 1 (see Fig. 9b), a quadrotor needs to fly diagonally through the room, avoiding a set of three obstacles. The path and trajectory used for this demonstration are the same referenced in Table 4. A human subject was seated at the table. As the quadrotor completed the flight, it set the cup of water in front of the human, who detached it from the quadrotor. A video of the human–quadrotor interaction and other experiments can be found in the supplemental materials. As the experiment demonstrated, the small amount of the liquid does not negatively impact the trajectory. The dynamics depends on the load's center of the mass. Given that the liquid is confined to a small container, its center of the mass does not move significantly. Moreover, since we are ensuring swing-free trajectory tracking, we are minimizing the movement of the fluid inside of the container as well. For a larger container, it would be beneficial to thoroughly analyze the effect of liquid transport in comparison with solid objects.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="5">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      In this work we proposed an autonomous aerial cargo delivery agent that works in environments with static obstacles to plan and create trajectories with bounded load displacements. At the heart of the method is the RL policy for minimal residual oscillations trajectories. Planning swing-free trajectories consists of a large action space (over 10{sup:6} actions) that deems learning impractical. For that reason, we find conditions that allow us to learn the swing-free policy in an action subspace several orders of magnitude smaller. In addition, the learning generalizes to an extended state space. Then, we show how a RL policy learned through a single training can be adapted to perform different tasks leveraging different discretized action spaces. We modified the policy by restricting the domain's action space to plan a path-following trajectory with a bounded load displacement. Finally, we integrated the swing-free path-following and sampling-based path planning to solve the aerial cargo delivery task. We evaluated each of the modules separately, and showed that learning converges to a single policy, and that performs minimal residual oscillation delivery task, that the policy is viable with expending state and action spaces. The simulations quality is assessed through the comparison with experimental load displacement on a physical robot. Then we evaluated the swing-free path-following on three reference paths for varying values of the path-following parameters. The results demonstrated that the proposed method attains both good path-following and good load displacement characteristics. Lastly, results of the integration with sampling-based motion planning show that the method creates collision-free trajectories with bounded load displacement for arbitrarily small bounds. We experimentally demonstrated the feasibility and safety of the method by having a quadrotor deliver a cup of water to a human subject. This article lays a foundation for aerial cargo delivery in environments with static obstacles. In further work, trajectory smoothing can be applied to the generated trajectories to accelerate them by not stopping in the waypoints while at the same time ensuring that the trajectories remain collision-free. Moving obstacles can be handled through an online trajectory tracking that adapts to dynamical changes in environment.
     </paragraph>
     <paragraph>
      Beyond aerial cargo delivery, this article address three important question relevant to AI. First, it applies reinforcement learning to a problem with very large action space. To address the curse of dimensionality, it proposes learning in relevant subspaces several orders of magnitude smaller, and planning in the full action space. The article contributes methods for finding the suitable subspaces. Second, it shows that using the feature vectors defined on a larger domain, the learned policy generalizes well outside of the training domain. Lastly, the article proposes learning constraint-balancing tasks on systems with non-linear dynamics by designing feature vectors that are linear maps over the constraints. AI researchers can use the approaches and theory presented in this article to solve other constraint-balancing problems for non-linear systems using RL in continuous state and very large action spaces.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Supplementary material
     </section-title>
     <paragraph>
      The following is the Supplementary material related to this article.{a mathematical formula}
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>