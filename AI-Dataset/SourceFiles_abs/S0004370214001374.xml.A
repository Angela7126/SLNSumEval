<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Model-based contextual policy search for data-efficient generalization of robot skills.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Learning is a successful alternative to hand-designing robot controllers to solve complex tasks in robotics. Algorithms that learn such controllers need to take several important challenges into consideration. First, robots typically operate in high-dimensional continuous state-action spaces. Thus, the learning algorithm has to scale well to higher dimensional robot tasks. Second, running experiments with real robots typically has a high cost. An experiment rollout is time consuming, usually requires expert supervision and it might lead to robot damage. Thus, the learning algorithm is required to operate with a limited number of evaluations. Furthermore, when learning with real robots, safety becomes an important factor. To avoid robot and environmental damage, the learning algorithm has to provide robot controllers that generate robot trajectories close to the already explored and, therefore, safe trajectory space. Lastly, robot skills have to be able to adapt to changing environmental conditions. For example, if the task is defined as throwing a ball at varying target positions, the controller has to be adapted to the current target position. In the following, we will refer to such task variables as context s. In the throwing example, the context is represented as the target position to throw to. In this paper, we introduce a new model-based policy search method to generalize a learned skill to a new context. For example, if we have learned to throw a ball to a specific location, we want to generalize this skill such that we can throw the ball to multiple locations.
     </paragraph>
     <paragraph>
      Policy Search (PS) methods are one of the most successful Reinforcement Learning (RL) algorithms for learning complex movement tasks in robotics [31], [38], [22], [23], [21], [8], [30], [20], [27], [13]. PS algorithms typically optimize the parameters ω of a parametrized control policy, which generates the control commands for the robot, such that the policy obtains maximum reward. A common approach to parametrize the policy is to use a compact representation of a movement with a moderate amount of parameters, such as movement primitives [17], [21]. In many approaches to movement primitives, the parameters ω specify the shape of a desired trajectory. The policy is then defined as trajectory tracking controller that follows this desired trajectory. Such a desired trajectory, represented by a single parameter vector ω, can be used to solve one specific task, characterized by the context vector s. The goal in contextual policy search is to learn how to choose the parameter vector ω of the control policy as a function of the context s. To do so, it is convenient to define two different levels of policies that are used in policy search. At the lower level, the control policy specifies the controls of the robot as a function of its state. The lower level policy is parametrized by the parameter vector ω. The lower-level policy can, for example, be implemented as movement primitive [17]. On the upper-level, a policy that chooses the parameters ω of the lower-level policy is used. We will denote this policy as upper-level policy. Given the current task description s, the upper-level policy chooses the parameters ω of the lower-level policy. The lower level policy is subsequently executed with the given parameters ω for the whole episode. Although PS algorithms can be applied to learn a large variety of robot skills, in this paper we focus on learning stroke-based movements, such as throwing, hitting, etc.
     </paragraph>
     <paragraph>
      Most of the existing contextual policy search methods are model-free [20], [27], i.e., they try to optimize the policy without estimating a model of the robot and the environment. Model-free PS algorithms execute rollouts on the real robot to evaluate parameter vectors ω. These evaluations are finally used to improve the policy. Most model-free PS algorithms require hundreds if not thousands of real robot interactions until converging to a high quality policy. For many robot learning problems, such data inefficiency is impractical, as executing real robot experiments is time consuming, requires expert supervision and it might lead to robot wear, or even robot damage. It has been shown that the data-efficiency of policy search methods can be considerably improved by learning forward models of the robot and its environment. These models are used to predict the experiment outcome, which allows more robust and efficient policy updates. We refer to such algorithms as model-based policy search algorithms [10], [1], [4], [34], [2], [18], [29]. However, current model-based policy search methods such as PILCO [9], [12] suffer from severe limitations that make it hard to apply these methods for learning generalized robot skills. PILCO uses computationally demanding deterministic approximate inference techniques that assume a specific structure of the reward function as well as of the used lower-level policy. These assumptions do not hold for many applications that occur in contextual policy search and have hindered the use of model-based policy search for learning contextual upper level policies. Moreover, the deterministic approximate inference method adds a bias in the prediction of the experiment outcome. Recently, the PILCO algorithm has been extended for learning generalized lower-level controllers [11]. Promising results have been demonstrated for learning robot controllers for hitting and box stacking tasks. Still, PILCO suffers from the restrictions on the structure of the used reward function and lower-level controllers.
     </paragraph>
     <paragraph>
      In this paper we introduce a new model-based policy search method that relaxes these assumptions from current model-based approaches and can therefore be used to efficiently generalize lower level-robot control policies to new contexts s. We rely on the contextual extension to Relative Entropy Policy Search (REPS), an information-theoretic PS algorithm [30]. REPS maximizes the expected reward of the upper-level policy, while staying close to the observed data, given by the parameter samples of the old upper-level policy. Our approach is to extend REPS to be a model-based method. Due to the closeness-bound, REPS is well suited for this extension as REPS will not explore areas of the parameter space where it has not seen data and the learned models are of poor quality. We learn probabilistic forward models of the robot and its environment and exploit expert knowledge about the task setup to decompose the model of the environment into several simpler dynamic and contact models. We use the learned forward models to generate artificial experiment outcomes, which we subsequently use for the policy updates. The benefit of using models is two-fold. First, by using artificial samples for policy updates, we significantly improve the data-efficiency of the learning framework. Second, we use models to compute the expected return and thus, avoiding the risk sensitive bias in the original REPS algorithm. We call the resulting algorithm Gaussian Process Relative Entropy Policy Search (GPREPS). We show in three complex simulated robotic tasks and in one real experiment that GPREPS reduces real robot interactions with two orders of magnitude, while learning policies of higher quality.
     </paragraph>
     <paragraph>
      This paper is an extension to the results published in [24], with the novel contributions as follows. First, we present a more detailed description of the technical background and thoroughly discuss related work. Second, we present the full derivation of the contextual REPS algorithm. Third, we present a qualitative comparison of moment-matching and sampling using GP models in terms of prediction accuracy and computation times. Fourth, we compare the efficiency of the full and the sparse GP model for learning robot dynamics. Finally, we present a novel evaluation of GPREPS in a table tennis learning scenario with a simulated Biorob robot arm.
     </paragraph>
     <paragraph>
      In the following, we present the problem formulation in Section 2. In Section 3, we discuss related work. In Section 4, we introduce the contextual extension to REPS and show how we can learn upper-level policies. In Section 5 we introduce the GPREPS algorithm and explain how model learning and trajectory prediction is integrated in the policy updates of contextual REPS. In Section 6, we show experimental results, while Section 7 concludes our work.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Problem formulation
     </section-title>
     <paragraph>
      In this paper, we denote the state of the robot and its environment as x. Typically, this state is composed of the joint angles {a mathematical formula}q∈Rd and joint velocities {a mathematical formula}q˙∈Rd, where d is the number of degrees of freedom,{sup:1} but it can also contain external variables such as the position of a ball. The vector of control torques {a mathematical formula}u∈Rd is computed by the lower-level policy {a mathematical formula}u=π(x;ω){sup:2} parametrized by {a mathematical formula}ω∈Ω. The space of lower-level policy parameters is denoted by Ω. A typical approach in policy search is to use simple parametrizations of the lower-level policy with a small number of parameters. Depending on the task, we can use linear feedback controllers, movement primitives [17] or torque profiles [28]. Such parametrizations are easier to learn as, for example, neural network controllers, however, they are usually limited to solve a single task. In order to generalize the lower-level controllers to different contexts s, we need to choose different parameters ω in each context.
     </paragraph>
     <paragraph>
      The trajectory of the robot is defined as the set of state-action pairs at each time step of the episode of length T, {a mathematical formula}τ={x1,u1,…,xT,uT}. In our formulation, the upper-level policy is represented by a search distribution over Ω, {a mathematical formula}π(ω). It is typically defined as a Gaussian {a mathematical formula}π(ω)=N(ω|μω,Σω).We consider the episode-based policy search framework [20], [27], [33] where search for the optimal upper-level policy by solving{a mathematical formula} where the expected episode reward is denoted by {a mathematical formula}Rω=∫τp(τ|ω)R(τ)dτ, {a mathematical formula}R(τ) is the trajectory reward and {a mathematical formula}p(τ|ω) is the probability of the trajectory given the parameters ω of the low-level controller. In many cases, the reward function might be defined as the sum of immediate rewards {a mathematical formula}r(xt,ut), {a mathematical formula}R(τ)=∑t=1Tr(xt,ut). However, some objectives can only be defined as the function of the whole trajectory.
     </paragraph>
     <paragraph>
      As an example, consider a throwing task in Fig. 2(a), where the robot has to throw a ball at a specific target position while maintaining balance. The robot executes the throwing motion using controller {a mathematical formula}π(x;ω) and releases the ball at a specific time point {a mathematical formula}tr. Then, we record the ball trajectory {a mathematical formula}τb, which contains the ball position at each time step {a mathematical formula}bt=[bx,t,by,t]T. For the throwing task the reward function can be defined as the minimum distance of the ball trajectory to a target position p, {a mathematical formula}R(τ)=−mint⁡‖p−bt‖2. Additionally, we can include other objectives in the reward function, such as torque penalty {a mathematical formula}−∑t=1TutTut, or deviation from a target configuration {a mathematical formula}−∑t=1T(xt−xg)T(xt−xg), where {a mathematical formula}xg is the target state.
     </paragraph>
     <paragraph>
      In contextual policy search [21], [27], our goal is to generalize the lower-level control policy {a mathematical formula}u=π(x;ω) to multiple contexts. The context vector s for a learning problem is defined as the set of variables that fully specify the task. It typically contains the objectives of the agent, e.g., for the throwing task it might refer to the target position of the target, {a mathematical formula}s=[px,py]T, but it can also contain properties of the environment, e.g., the weight of a mass that needs to be lifted. We assume that the continuous context variable is drawn from an unknown distribution {a mathematical formula}s∼μ(s) in the beginning of an episode, and that the context variable is fully observable. The context distribution {a mathematical formula}μ(s) is defined by the learning problem. Solving a contextual problem with the standard episode-based PS approach would require us to learn {a mathematical formula}πs(ω) for each context s, which is clearly a tedious and data inefficient approach.
     </paragraph>
     <paragraph>
      Instead, we follow a hierarchical approach, where we learn an upper-level policy {a mathematical formula}π(ω|s), which provides the lower-level controller parametrization ω given the context s. Our goal is to find the optimal policy {a mathematical formula}π⁎(ω|s), such that it maximizes the expected reward{a mathematical formula} where {a mathematical formula}Rsω denotes the expected reward when executing the lower-level policy with parameter ω in context s. As the dynamics of the robot and its environment are stochastic, the reward {a mathematical formula}Rsω is given by the expected reward over all trajectories{a mathematical formula} The probability of a trajectory {a mathematical formula}p(τ|s,ω) now also depends on the context and the reward function {a mathematical formula}R(τ,s) now also depend on the context s. Using our motivating example, for the throwing task, we might define the reward function as the minimal distance to the target that is now defined by the context, i.e., {a mathematical formula}R(τ,s)=−mint⁡‖s−bt‖2, where now the context defines the target position to throw to, {a mathematical formula}s=[px,py]T. We also illustrate the basic concept of (model-free) contextual policy search in Fig. 2(b). In the beginning of the episode, the lower-level policy parameter is drawn from {a mathematical formula}π(ω|s) given the observed context variable {a mathematical formula}s∼μ(s). Subsequently, the policy parameter is evaluated on the robot and the reward is obtained. We collect N sample rollouts, which we use then to update the policy.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      While policy search became highly successful in recent years, there are multiple approaches to acquire robot skills. Standard reinforcement learning tools that rely on the Markov Decision Process (MDP) framework share the same goal as policy search, that is, maximize the reward by interacting with the environment. However, one major disadvantage of the MDP formulation is that the number of states, and thus, the number of (action-)value functions grow exponentially with the state dimensionality. As robots typically operate in high dimensional continuous state-action spaces, standard RL techniques are difficult to apply with success.
     </paragraph>
     <paragraph>
      The goal and methods of RL closely relate to that of optimal control. However, there is a significant difference in the assumed prior knowledge. In optimal control, the exact model of the controllable system, or at least the structure of the model is usually assumed to be known. While we can learn the models from data in the same way done in this paper, the optimal control solution can only be obtained for linear systems with Gaussian noise. For all other systems, optimal control has to rely on approximations that might lead to a poor quality of the optimal policy. Policy search avoids these approximations as we directly search in the policy parameter space and is therefore likely to produce policies of higher quality.
     </paragraph>
     <paragraph>
      In the following we give a brief overview of related work in the fields of robot skill generalization, contextual model-free policy search methods and model-based policy search for robot learning.
     </paragraph>
     <section>
      <section>
       <section>
        <section-title>
         Robot skill generalization
        </section-title>
        <paragraph>
         Robot skill generalization has been investigated by many researchers in recent years [40], [15], [7], [26], [20], [27]. In [15], [40], robot skills are represented by Dynamic Movement Primitives [17] or DMPs. The DMPs are generalized using demonstrated trajectories. First, the DMP parameters ω are extracted from a human expert's demonstration in a specific context s. Using multiple demonstrations, a library of context-parameter pairs {a mathematical formula}{s,ω}i=1N is built up. Subsequently, the DMP parameters {a mathematical formula}ω⁎ for a query context {a mathematical formula}s⁎ is chosen using regression techniques. While generalization has proven to be accurate in humanoid reaching, grasping and drumming, the parameters are not improved by reinforcement learning. Thus, the quality of the reproduced skill inherently depends on the quality and quantity of the expert demonstration. To account for skill improvement, policy search methods have been applied [20], [27]. For example, Kober et al. [20] proposed the Cost regularized Kernel Regression (CRKR) algorithm to learn DMP parameters for throwing a dart at different targets, and for robot table tennis. However, CRKR does not scale well to higher dimensional learning problems due to its uncorrelated exploration strategy. Another PS algorithm is proposed in [27] for robot skill generalization. The algorithm uses a probabilistic approach based in variational inference to solve the underlying RL problem. The proposed method was able to generalize a lower-level controller that balances a 4-DOF planar robot around the upright position after a random initial push. However, the solution for the proposed PS algorithm cannot be computed in closed form for most upper-level policies and is computationally very costly to obtain. Recently, the Mixture of Movement Primitives (MoMP) algorithm has been introduced [26] for robot table tennis. The MoMP approach first initializes a library of movement primitives and contexts using human demonstrations. Then, given a query context, a gating network is used to combine the demonstrated DMPs into a single one, which is then executed on the robot. The gating network parameters can be adapted depending how successful the movement primitives are in the given context. Promising results in real robot table tennis have been presented [26]. However, the used formulation of the learning problem required a lot of prior knowledge and it is unclear how the algorithm scales to domains where this prior knowledge is not applicable. Finally, a general robot skill learning framework has been proposed by da Silva et al. [7]. The proposed approach separates generalization and policy learning to a classification and a regression problem. The resulting policy is a mixture of finite number of local policies. However, choosing the amount of local policies is not straightforward and separating the generalization and policy improvement step into two distinct algorithms seem data inefficient and counter intuitive.
        </paragraph>
        <paragraph>
         The contextual REPS algorithm presented in this paper is highly related to the hierarchical (Hi-)REPS algorithm presented in [8]. Hi-REPS is applied to learn multiple options to solve a given task. It has been used for learning versatile robot skills for the tetherball game. While the contextual REPS algorithm presented in this paper can be seen as special case of Hi-REPS with only one option and without state transitions, the Hi-REPS algorithm used in [8] is model-free and therefore needs many evaluations on the real robot system.
        </paragraph>
       </section>
       <section>
        <section-title>
         Model-free policy search algorithms
        </section-title>
        <paragraph>
         The key concept of model-free policy search algorithms is to update the policy solely based on parameter-reward samples obtained using the real system, without any assumption, or knowledge about the system dynamics. Model-free policy search methods can be coarsely categorized according to the exploration and the policy update strategy. Exploration can be either implemented at the level of the actions, i.e., we add exploration noise to the controls at each time step, or, at the level of the parameters ω of the control policy.
        </paragraph>
        <paragraph>
         Some of the earliest successful policy search methods were policy gradient (PG) algorithms [42], [5], [37]. PG algorithms use the likelihood-ratio trick to compute the expected performance gradient. The policy parameters are updated using the gradient and a user defined learning rate. Some of the most important extensions to PG algorithm was the introduction of the natural policy gradient [31], [3]. The natural policy gradient bounds the relative entropy or Kullback–Leibler (KL) divergence of two subsequent policies by using a second order approximation of the KL divergence. Due to this bound, the algorithm achieves uniform convergence in the whole parameter space which typically results in an increased learning speed in comparison to standard PG algorithms. Natural policy gradient algorithms are currently one of the most commonly used PG algorithms. While the early PG algorithms were developed for action-based exploration, the ideas have been adopted to parameter-based exploration algorithms [43], [35], [33] as well. One significant challenge when using PG methods is the choice of an appropriate learning rate, which is crucial for good performance, but usually difficult to find.
        </paragraph>
        <paragraph>
         An alternative approach to learn upper-level policies is to treat the policy search problem as a latent variable estimation problem and use Expectation Maximization to solve it [27], [20], [41], [21]. In episodic Monte-Carlo EM approaches [20], [41], [21], the new upper-level policy parameters are found by using the Moment-projection of the reward weighted old policy, which is essentially using a weighted maximum likelihood estimation for the new policy parameters. Alternatively, we can perform the Information-projection of the reward weighted old policy [27]. By doing so, we avoid the typical problem of MC-EM algorithms, that is, averaging over multiple modes of the reward weighted parameter distribution. However, when using I-projection, the new policy parameters cannot be computed in closed form for most policies [27]. The idea of avoiding failed and low quality experiments during exploration has also been investigated by Grollman and Billard [16]. They assume that the expert's demonstration of the task is unsuccessful, but gives us an idea how the solution should look like. Thus, the robot should try skills that are not exactly the same as the failed demonstrations, but similar. The distribution of exploratory trajectories is encoded in a Donut Mixture Model (DMM), which can be regarded as the pseudo-inverse of a Gaussian Mixture Model [16] representing the failed trajectory distribution.
        </paragraph>
        <paragraph>
         A significant advantage of EM-based PS methods compared to PG algorithms is that no user defined learning rate is required and that they can be applied to learn contextual upper-level policies [20]. To combine the uniform convergence property of the natural gradient approaches [31] and the closed form policy update of EM-based PS approaches, the information theoretic Relative Entropy Policy Search (REPS) algorithm has been proposed in [30]. The REPS algorithm limits the information loss between subsequent policies, while maximizing the expected reward, resulting in smooth convergence. The only required parameter for REPS is the upper bound on the information loss, which is significantly easier to choose than a learning rate with PG algorithms. We will investigate in details the contextual extension to REPS in Section 3.
        </paragraph>
       </section>
       <section>
        <section-title>
         Model-based policy search methods
        </section-title>
        <paragraph>
         To improve the data-efficiency of model free methods, several model-based approaches have been proposed in the literature [9], [12], [34], [4]. Model-based methods learn the forward model of the controllable system and its environment, which is subsequently used in simulations to predict the experiment outcome. While in [1], a time dependent forward model is used, we typically learn the dynamic model of the real hardware [34], [4], [29], [10], [18], [14]. Time-dependent models fail to generalize to unseen situations, and only provide accurate models along the observed trajectories [1], however, they might be able to predict the system dynamics more accurately especially if the state of the system includes unobserved variables. Forward models are typically used to provide long term trajectory predictions. The most common approach is to learn the discrete-time stochastic state transition model {a mathematical formula}xt+1=f(xt,ut)+ϵ using measurement data, where {a mathematical formula}ϵ∼N(0,Σ) is i.i.d. Gaussian noise.
        </paragraph>
        <paragraph>
         A common method to learn the non-linear models of the system dynamics is to use the Locally Weighted Bayesian Regression (LWBR) algorithm [4], [34], [29]. LWBR learns local linear models {a mathematical formula}xt+1=[1,xtT,utT]Tβ+ϵ of the state transition where the parameter vector β is re-estimated locally for every query point {a mathematical formula}y⁎=[x⁎T,u⁎T]T. LWBR has been applied to learn the forward model of a helicopter [4], [29] as well as the inverted pendulum [34].
        </paragraph>
        <paragraph>
         An alternative successful method is to use Gaussian Process (GP) models that have proven to be efficient in learning a stochastic model of the dynamics [9], [10], [18], [14]. With GP models [32], we can compute the posterior distribution of the successor state {a mathematical formula}xt+1 in closed form given the query input {a mathematical formula}[xtT,utT]T and the measurement data {a mathematical formula}{xi+1,xi,ui}i=1K. As GP models integrate out the model uncertainty, the model becomes less biased. GP models also provide us with a variance of the prediction. The variance/uncertainty of the prediction typically decreases with the number of data points in the neighborhood. Consequently, we can avoid overly confident predictions in unexplored state spaces. The state of the art model-based policy search algorithm is the Probabilistic Inference for Learning Control (PILCO) algorithm [9], [12], which uses GP models. PILCO first learns a GP model of the dynamics of the robot. Subsequently, it predicts the expected trajectory, its variance and the distribution of the future rewards following the current control policy. However, computing the successor state distribution with GP models given a non-deterministic query input is not straightforward. PILCO solves this problem by matching the first and second moment of the predictive distribution. Using the approximated trajectory distribution and reward distribution, PILCO computes the gradient of the long term rewards w.r.t. the controller parameters in closed form. This process is repeated until the optimal policy is found using the current GP model. Subsequently, the policy is executed on the real robot to obtain new measurement data to improve the learned forward models. PILCO has been successfully applied for learning the controllers for a low-cost robot arm [10] and a robotic unicycle [9] with unprecedented data-efficiency. Recently, it also has been applied for imitation learning [14]. However, as PILCO directly optimizes lower-level controller parameters, it cannot be straightforwardly applied to learn upper-level policies. Moreover, the class of representable low-level controllers is restricted to functions through which a Gaussian distribution can be mapped in closed form.
        </paragraph>
       </section>
      </section>
     </section>
    </section>
    <section label="4">
     <section-title>
      Contextual episode-based REPS
     </section-title>
     <paragraph>
      The intuition of REPS [30] is to maximize the expected reward, while staying close to the observed data to balance out exploration and experience loss. The constraint of staying close to the data is implemented by bounding the relative entropy, also called Kullback–Leibler (KL) Divergence, between the old trajectory distribution and the trajectory distribution induces by the new policy that we want to estimate. The use of the KL-bound provides an intuitive way to define the exploration–exploitation tradeoff. With a very small KL-bound, we favor exploration and we will continue to explore with the old exploration policy. Hence, we obtain a slower learning speed, but we are likely to find good solutions. With a high ϵ, we favor exploitation. The resulting policy will be more greedy and reduce exploration. We will get a fast learning speed, but the quality of the found solution will be worse on average due to premature convergence of the algorithm [8] In the episodic learning setting, the context s and the parameter ω uniquely determine the trajectory distribution [8]. For this reason, the trajectory distribution can be abstracted as the joint distribution over the parameter vector ω and the context s, i.e., {a mathematical formula}p(s,ω)=μ(s)π(ω|s). To bound the relative entropy between consecutive trajectory distributions, REPS uses the constraint{a mathematical formula} where {a mathematical formula}p(s,ω) represent the updated and {a mathematical formula}q(s,ω) the previously used context-parameter distribution. The parameter {a mathematical formula}ϵ∈R+ is the upper bound of the relative entropy. A smaller value of ϵ results in more conservative policy updates, while a higher ϵ leads to faster converging policies.
     </paragraph>
     <paragraph>
      As the context distribution {a mathematical formula}μ(s) is defined by the learning problem and cannot be chosen by the learning algorithm, the constraints {a mathematical formula}∫ωp(s,ω)=μ(s),∀s must also be satisfied. However, in the case of continuous context variables, we would have an infinite number of instances of this constraint. To keep the optimization problem tractable, we require only to match feature averages instead of single probability values, i.e.,{a mathematical formula} where {a mathematical formula}p(s)=∫ωp(s,ω)dω. The feature vector is denoted as {a mathematical formula}ϕ(s), while {a mathematical formula}ϕˆ denotes the observed average feature vector. For example, if the feature vector contains all linear and quadratic terms of the context, the above constraint translates to matching the mean and the variance of the distributions {a mathematical formula}p(s) and {a mathematical formula}μ(s). The contextual episode-based REPS learning problem is now given by{a mathematical formula} The emerging constrained optimization problem can be solved by the Lagrange multiplier method. The closed form solution for the new distribution is given by{a mathematical formula} Here, {a mathematical formula}V(s)=θTϕ(s) is a context dependent baseline, while η and θ are Lagrangian parameters. Subtracting the baseline from the reward is corrected from its context dependent part and it allows is to evaluate the parameter ω independently from the context s. The temperature parameter η scales the advantage term such that the relative entropy bound is met after the policy update. The Lagrangian parameters are found by optimizing the dual function{a mathematical formula} The dual function is convex in θ and η, and the corresponding gradients can be obtained in closed form. In difference to recent EM-based policy search methods [21], [27], the exponential weighting emerges from the relative entropy bound and does not require additional assumptions. For a details of the derivation we refer to Appendix A.
     </paragraph>
     <section label="4.1">
      <section-title>
       Sample-based REPS
      </section-title>
      <paragraph>
       As the relationship between the context-policy parameter pair {a mathematical formula}{s,ω} and the corresponding expected reward {a mathematical formula}Rsω is not known, sample evaluations are used to approximate the integral given in the dual function [8], [24]. We denote these evaluations as rollouts. To execute the ith rollout, we first observe the context {a mathematical formula}s[i]∼μ(s). Subsequently, we sample the lower-level controller parameter using the upper-level policy {a mathematical formula}ω[i]∼π(ω|s[i]). Finally, we execute the lower-level policy with parametrization {a mathematical formula}ω[i] in context {a mathematical formula}s[i] to obtain {a mathematical formula}Rsω[i]. This process is repeated for N rollouts {a mathematical formula}{s[i],ω[i],Rsω[i]}, {a mathematical formula}i=1,…,N such that we can approximate the integral given in the dual function with samples. The sample-based approximation of the dual function is given in Appendix A in Eq. (A.8).
      </paragraph>
      <paragraph>
       As we sampled the controller parameters with the old policy, the samples have been generated from {a mathematical formula}q(s,ω). Using the optimized Lagrangian parameters θ and η, we can compute the probabilities of the updated context-parameter distribution for our finite set of samples using{a mathematical formula}
      </paragraph>
      <paragraph>
       However, in order to generate new samples, we need a parametric model to estimate {a mathematical formula}π(ω|s). Thus, we estimate the parameters of this model given the samples and using {a mathematical formula}p[i] as weight for these samples. For example, for a linear Gaussian model {a mathematical formula}π(ω|s)=N(ω|a+As,Σ), we can compute the parameters {a mathematical formula}{a,A,Σ} with weighted maximum likelihood estimation, that is,{a mathematical formula}{a mathematical formula}{a mathematical formula} where {a mathematical formula}S=[sˆ[1],…,sˆ[N]]T is the context matrix with {a mathematical formula}sˆ[i]=[1,s[i]T]T, {a mathematical formula}B=[ω[1],…,ω[N]]T is the parameter matrix and {a mathematical formula}Pii=p[i] is the diagonal weighting matrix. When updating the policy parameters, we are not restricted only to use the last N samples. To improve the accuracy of policy updates, we can define {a mathematical formula}q(s,a) to be a mixture of the last H policies, and thus, reuse old samples without the need of importance weighting. The model-free contextual REPS algorithm is summarized in Table 1.
      </paragraph>
      <paragraph>
       However, model-free REPS produces biased policy updates as the expected reward {a mathematical formula}Rsω is evaluated using a single rollout. This bias can be seen by looking at the exponential sample weighting of REPS given in Eq. (9). For example, if we only have two actions {a mathematical formula}a1 and {a mathematical formula}a2, and the expected reward of {a mathematical formula}a1 is lower than the expected reward of {a mathematical formula}a2. However, if the variance of the reward for {a mathematical formula}a1 is higher, such that there will be samples from {a mathematical formula}a1 with higher reward then all samples from {a mathematical formula}a2, REPS will prefer {a mathematical formula}a1. Thus, the resulting policy is risk-seeking, a behavior that we want in general to avoid. The same bias is inherent to all other PS methods that are based on weighting samples with an exponential function, for example PoWER [21], CrKR [20] and PI{sup:2}{sup:3}[38].
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Model-based contextual policy search
     </section-title>
     <paragraph>
      Our main motivation to use models with contextual policy search is two-fold. First, we want to improve the data-efficiency of the model-free REPS using artificial rollouts. Second, we want to obtain an accurate estimate of the expected reward {a mathematical formula}Rsω for a given context-policy parameter pair to avoid the bias in the sample-based REPS formulation. The expected reward {a mathematical formula}Rsω=Eτ[R(τ,s)|s,ω] can be estimated by multiple samples from the trajectory distribution {a mathematical formula}p(τ|s,ω), that is,{a mathematical formula} where the trajectories are now generated using the learned forward models in computer simulation and we will assume that the trajectory-dependent reward function {a mathematical formula}R(τ,s) is known. We generate M artificial context-parameter samples using an estimate of the context distribution and the upper level policy. Using the learned models, we evaluate these artificial samples in computer simulation.
     </paragraph>
     <paragraph>
      To gain the most benefit out of the learned forward models, we use structural knowledge about the task to decompose the monolithic forward model of the system in smaller forward models that are easier to learn. For example, when throwing a ball with a robot, we can learn individual forward models to predict the movement of the robot, the initial configuration of the ball when it is released from the robot and the flight of the ball. We show such a decomposition of the forward models for the ball throwing example in Fig. 3(b), the illustration of the experiment is shown in Fig. 3(a).
     </paragraph>
     <paragraph>
      In Fig. 4, we show the learning framework of the GPREPS algorithm. One of the key difference compared to the model-free case is that now we evaluate the policy on the real-robot only to obtain measurement data. Using our ball throwing example, such measurement data might consist of the joint trajectory {a mathematical formula}τr, the trajectory of the thrown ball {a mathematical formula}τb, etc. Using the obtained measurement data, we learn models of the dynamics and discrete events, such as releasing the ball.
     </paragraph>
     <paragraph>
      Note that in the model-based formulation we use solely the artificial samples to update the policy. Thus, in case of unbiased models, we can avoid the possibly noisy reward samples evaluated on the real system and, hence, eliminate the risk sensitive bias inherent to the REPS algorithm. Additionally, we can increase the number of artificial samples significantly {a mathematical formula}N≪M to further improve the accuracy of policy updates. Due to the recent success of using Gaussian Process models to reduce the model bias when learning complex system dynamics [9], we use GP models to learn the forward models of the robot and its environment. Therefore, our method is called Gaussian Process Relative Entropy Policy Search (GPREPS).
     </paragraph>
     <section label="5.1">
      <section-title>
       Gaussian Process REPS
      </section-title>
      <paragraph>
       In each iteration, first we collect measurement data from the robot and its environment. For data collection, we observe the context {a mathematical formula}s[i] and sample the parameters {a mathematical formula}ω[i] using the upper-level policy {a mathematical formula}π(ω|s[i]). Subsequently, we use the lower-level control policy {a mathematical formula}π(x;ω[i]) to obtain the trajectory sample {a mathematical formula}τ[i]. It is important to evaluate sufficiently many samples to obtain enough measurement data, such that the GP models produce accurate predictions over the relevant part of the state space. Therefore, we repeat the data collection step N times. To favor data-efficiency, we want to keep N as low as possible, while learning high quality models. In experiments, we usually choose a higher N in the first iteration, e.g. {a mathematical formula}N=20, to obtain an accurate GP model. After the first policy update, we decrease N to significantly lower value, e.g., {a mathematical formula}N=1. This way, we keep updating the GP models with relevant measurement data without compromising the data-efficiency. We retrain the GP models using the so far obtained measurement data. The GPREPS algorithm is summarized in Table 2.
      </paragraph>
      <paragraph>
       In the prediction step, we predict the rewards for M randomly sampled context-policy parameter pairs. We refer to these samples as artificial samples. To generate artificial samples, we need to obtain an estimate of the context distribution {a mathematical formula}μˆ(s) using the observed data. Depending on the learning problem, we typically approximate the context distribution with a Gaussian or a uniform distribution. Given the observed context variables {a mathematical formula}{si}i=1N, we fit the distribution parameters with maximum likelihood estimation. After updating the estimated context distribution {a mathematical formula}μˆ(s), we draw a context parameter {a mathematical formula}s[j]∼μˆ(s) for each artificial sample. Subsequently, we sample from the upper-level policy {a mathematical formula}ω[j]∼π(ω|s[j]) and produce L sample trajectories {a mathematical formula}τ[j,l], {a mathematical formula}l=1,…,L and the corresponding rewards {a mathematical formula}R(τ[j,l],s[j]) for this given context-parameter pair. To update the policy, we first minimize the dual function {a mathematical formula}g(η,θ) (Eq. (A.8)) using the artificially generated samples and compute the new weight {a mathematical formula}p[j] (Eq. (9)) for each artificial sample. Note, that we use solely the artificial context-parameter samples {a mathematical formula}{s[j],ω[j]}j=1M to update the policy as to use the expected reward for the REPS algorithm instead of a single sample estimate of the reward. Consequently, if the models produce unbiased rewards {a mathematical formula}Rsω, the final policy will also be unbiased. Finally, we update the policy by the weighted maximum likelihood estimate using Eqs. (10), (11). In the following, we will explain in more detail how to learn the models and how to sample the trajectories.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Learning GP forward models
      </section-title>
      <paragraph>
       We use forward models to simulate a trajectory τ given the context s and the lower-level policy parameters ω. We learn a forward model that is given by {a mathematical formula}yt+1=f(yt,ut)+ϵ, where {a mathematical formula}y=[xT,bT]T is composed of the state of the robot and the state of the environment b, for instance the position of a ball. The vector ϵ denotes zero-mean Gaussian noise. In order to simplify the learning task, we decompose the forward model f into simpler models, which are easier to learn. To do so, we exploit prior structural knowledge of how the robot interacts with the environment. For example, when learning to play the table tennis game, we use the prior knowledge that the trajectory of the ball is described by its free dynamics and the contacts with the table and the racket.
      </paragraph>
      <paragraph>
       Gaussian Processes are efficient non-parametric Bayesian regression tools [32] that explicitly represent model uncertainty. For learning the model, the set of training data is given by {a mathematical formula}D={vi,wi}i=1N, with {a mathematical formula}vi and {a mathematical formula}wi being the training input and target values. We use individual GP models for each output dimension and, therefore, we will assume scalar target values w in the subsequent discussion. For a new test input {a mathematical formula}v⁎, the predictive distribution {a mathematical formula}p(w⁎|v⁎,D) of the posterior Gaussian process is a Gaussian {a mathematical formula}N(w⁎|μ⁎,σ⁎2) with mean and variance{a mathematical formula}{a mathematical formula} respectively, where I is the identity matrix, K is the {a mathematical formula}N×N kernel matrix with elements {a mathematical formula}Kij=k(vi,vj) and k denotes the kernel vector for the query input with {a mathematical formula}ki=k(vi,v⁎). The parameter {a mathematical formula}σϵ2 represents the variance of the system noise ϵ. The covariance function {a mathematical formula}k(⋅,⋅) defines a similarity measure between the input data. We use the squared exponential function as covariance function{a mathematical formula} where {a mathematical formula}L=diag(l2) is the diagonal matrix containing the bandwidth parameters of the squared exponential kernel. The parameter {a mathematical formula}σf2 represents the variance of the function. We refer to parameters {a mathematical formula}{l,σf,σϵ} as hyper-parameters of the GP. To obtain accurate predictions, the hyper-parameters have to be set properly. To do so, we optimize the hyper-parameters of the GP by maximizing the marginal log-likelihood using gradient-based optimizers [32]. The computational complexity of optimizing the hyper-parameters is dominated by the {a mathematical formula}O(N3) matrix inversion in Eqs. (14), (15). To reduce the computational demands, we use sparse GP models [36], [39].
      </paragraph>
      <paragraph>
       GPREPS can be regarded as a combination of information theoretic PS, contextual PS and model-based RL to address the important problem of learning real world robot skill efficiently. Current model-based policy search methods rely on deterministic approximate inference methods. While such approximate inference is efficient for computing the policy gradient, it also suffers from severe limitations. The structure of the used policy representations and the reward functions is limited to a specific type of functions and the approximation may cause a severe bias in the policy update, GPREPS uses sampling to evaluate a new context-parameter pair. By the use of sampling, we do not rely on any assumption on the policy representation as well as on the reward function except that we can sample from the policy and evaluate the reward function for a given state action pair. Due to the increased generality, efficient model-based policy search can now be used in a much wider range of applications, including the contextual policy search application introduced in this paper. However, other types of structured policies are also possible. For example, we could learn mixture of experts models similar as the one introduced in the HiREPS method [8].
      </paragraph>
     </section>
     <section label="5.3">
      <section-title>
       Trajectory and reward predictions
      </section-title>
      <paragraph>
       Using the learned GP forward model, we need to predict the expected reward{a mathematical formula} for a given parameter vector ω executed in context s. The expectation over the trajectories is now estimated using the learned forward models.
      </paragraph>
      <paragraph>
       To obtain the trajectory distribution in closed form, at each time step we have to compute the GP predictive distribution{a mathematical formula} However, if the query input is Gaussian {a mathematical formula}N([xtT,utT]T|μxu,Σxu), and the model {a mathematical formula}f(⋅) is non-linear, the predictive distribution over the next state {a mathematical formula}p(xt+1) becomes non-Gaussian. Thus, in general, we cannot obtain an analytic solution for {a mathematical formula}p(xt+1). To overcome this problem, we use samples to solve the integral. Using the sample state {a mathematical formula}xt, we first compute the control {a mathematical formula}ut=π(xt;ω) and, subsequently, sample the next state from the predictive distribution {a mathematical formula}p(xt+1|xt,ut). We repeat this procedure until we obtain the complete trajectory.
      </paragraph>
      <paragraph>
       An alternative approach to solve the integral in Eq. (17) is to use moment matching [9], [10]. Moment matching computes the first and second moment of the predictive distribution {a mathematical formula}p(xt+1), i.e., it approximates it by a Gaussian. Moment matching is a deterministic approximate inference technique that provides a closed form solution. However, the Gaussian approximation of the predictive distribution might result in a biased trajectory distribution {a mathematical formula}p(τ|ω,s), and therefore, in a biased estimate of the expected reward {a mathematical formula}Rsω. Furthermore, when using moment matching, the class of the lower-level controllers is restricted to all functions through which we can map a Gaussian analytically, i.e., linear controllers, squared exponentials and trigonometric functions. Thus, many policies are infeasible, for example policies with a hard limit on the controls. Such hard limit needs to be approximated by the use of trigonometric functions. Nevertheless, obtaining the trajectory (and reward) distribution with moment matching is the method of choice, in particular, when using gradient based policy search [9] where accurate analytical estimates of the policy gradient are required.
      </paragraph>
      <paragraph>
       When using sampling, the lower-level policy class is not restricted and torque limits can be applied with ease. On the other hand, to accurately approximate the trajectory distribution, the number of sample trajectories L must be relatively high and typically needs to increase with the dimensionality of the system. As the moment matching is also computationally highly demanding, the question arises which method can be implemented more efficiently. Note that sampling multiple trajectories at the same time only consists of simple computations with large matrices, and thus, the computations can be executed in parallel. Thus, we can speed up computations significantly using high through-put processors, such as GPUs. This is particularly effective when evaluating multiple artificial samples with GPREPS. Such parallelization is not straightforward with the moment matching approach.
      </paragraph>
      <paragraph>
       In the following, we evaluate the sampling approach for trajectory prediction and we compare it to the moment matching algorithm in terms of accuracy and computation time.
      </paragraph>
      <section label="5.3.1">
       <section-title>
        Quantitative comparison of sampling and moment matching
       </section-title>
       <paragraph>
        To compare moment matching and sampling, we evaluated both approaches on predicting the joint trajectory of a 4-link simulated non-linear pendulum. The lower-level policy was given by a linear trajectory-tracking PD controller. We used 1500 observed data points and 300 pseudo-inputs to train the sparse GP models. The prediction horizon was set to 100 time steps.
       </paragraph>
       <paragraph>
        We sampled 1000 trajectories and estimated a Gaussian state distribution {a mathematical formula}p(xt) for each time step from these samples. These distributions are used as ground truth. Subsequently, we compute the Kullback–Leibler divergence {a mathematical formula}KL(p(xt)‖p˜(xt)) of the approximations {a mathematical formula}p˜(xt) obtained by either using less samples or moment matching. This procedure was done for an increasing number of samples for the sampling approach. To improve the accuracy of the comparison, we evaluated the prediction for 100 independently chosen starting state with varying initial variance. To facilitate the comparison of the two prediction methods, we normalized the KL divergence values, such that the moment matching prediction accuracy remains constant, i.e., {a mathematical formula}∑t=1100KL(p(xt)‖pMM(xt))=100, where {a mathematical formula}pMM(xt) is the state distribution predicted by moment matching. The result is shown in Fig. 5(a). As the figure shows, the best 95% of the experiments required approximately 50 sample trajectories to reach the accuracy of the moment matching approach. However, in most cases (top 75%), it was enough to sample not more than 20 trajectories to reach the moment matching performance. The inaccuracies of the moment matching approach results from outliers and non-Gaussian state distributions which violate the Gaussian approximation assumption of moment matching.
       </paragraph>
       <paragraph>
        We also evaluated the computation time of both approaches. For the sampling approach, we evaluated the computation time for different types of parallelization. As can be observed from Fig. 5(b), already the single CPU core implementation outperforms moment matching and can produce approximately 1000 samples in the same computation time. This number can be increased to 7000 samples when using a high-end graphics card.
       </paragraph>
       <paragraph>
        We showed that only a few sample trajectories are needed to meet the accuracy of the moment matching approach while we are able to generate thousands of trajectories in the same computation time. Thus, using sampled trajectories results in a moderate speed-up already for main stream computers and can further be improved when using a GPU implementation. In addition to the improved computation speed, sampling avoids the approximations involved in the moment matching approach, for example, using trigonometric functions to approximate torque limits [9]. Sampling produces unbiased estimates of the expected reward, and thus, we can improve the accuracy of the prediction by increasing the number of samples. Nevertheless, moment matching is still favorable for algorithms that require the computation of the gradient of the state distribution w.r.t. the policy-parameters, e.g., for PILCO [9].
       </paragraph>
      </section>
      <section label="5.3.2">
       <section-title>
        Comparison of Gaussian process models
       </section-title>
       <paragraph>
        When using GP models for trajectory prediction, we have to take the computation times into consideration. As discussed earlier, the training time of the hyper-parameters in the standard GP approach scales cubically with the number of training samples. The prediction time of the posterior mean scales linearly, while the computation of the posterior variance scales quadratically with the number of training samples. When learning dynamic models with high sampling rate, the number of training samples can quickly increase to thousands, and thus, the computation time might become impractically large. To mitigate the computational demand while learning accurate models, sparse Gaussian process methods were proposed, e.g. [36], [39]. In general, sparse GP methods maintain a set of {a mathematical formula}M&lt;N highly representative training samples, where N is the total number of training samples. When using sparse methods, the training time scales only {a mathematical formula}O(M2N), while the posterior mean and variance prediction time scales {a mathematical formula}O(M) and {a mathematical formula}O(M2) respectively.
       </paragraph>
       <paragraph>
        When learning GP forward models, numerical problems may emerge. In order to learn accurate models, we often need thousands of training samples {a mathematical formula}{v,w}. When using this many training samples, the matrix {a mathematical formula}(K+σϵ2I) used in the GP prediction and model training, might have an overly high condition number. Thus, computing the inverse of this matrix might result in numerical problems which can easily lead to an inaccurate trajectory prediction. To avoid this problem, a common strategy is to add noise to the training target data {a mathematical formula}wi=wi+ϵadd, {a mathematical formula}i=1,…,N. This will naturally increase the value of {a mathematical formula}σϵ, and thus, decrease the condition number of {a mathematical formula}(K+σϵ2I). In our experiments, we added i.i.d. Gaussian noise {a mathematical formula}ϵadd∼N(0,σadd2) to the training data with standard deviation {a mathematical formula}σadd=10−2std(w). The amount of additive noise has proven to be efficient in balancing out the prediction accuracy and numerical instability.
       </paragraph>
       <paragraph>
        In the following, we compare the accuracy of reward prediction of a control task when using the sparse GP method with pseudo inputs [36] and the standard GP approach [32]. We also tested the sparse method presented in [39], but ran into numerical problems with this method. The control problem is to balance a simulated planar 4-link pendulum to the upright position. The lower-level control policy is set such that the pendulum is robustly balanced to the optimal upright position from a random set of initial positions around the upright position. We collect measurement data by executing a certain amount of experiment rollouts. Then, we train both GP models, the standard and the sparse model [36], with the same measurement data. Subsequently, we use the GP models to predict the reward of 50 context-parameter pairs for 20 time steps. We use 20 trajectories per context-parameter pair. The corresponding reward of a single trajectory was given by {a mathematical formula}r(τˆ)=−∑t=1T(xt−xr)T(xt−xr), where {a mathematical formula}xr represents the upright position. Finally, we measure the accuracy of the GP models, by computing the average quadratic error of the mean reward prediction {a mathematical formula}Es,ω[e2]=Es,ω[(rˆ(s,ω)−r(s,ω))2], where {a mathematical formula}rˆ(s,ω) denotes the mean predicted reward and {a mathematical formula}r(s,ω) the real reward for that context parameter pair.
       </paragraph>
       <paragraph>
        First, we investigate the influence of the amount of additive noise on the prediction performance. We set the additive noise to {a mathematical formula}σadd=α10−2std(w), where α is a scaler. Second, we investigate the amount of hyper-parameter optimization steps required to learn accurate models. In the third experiment, we evaluate how well the models can capture the stochasticity in the dynamics by adding noise to the control input. Finally, we investigate how well the models can generalize with only a limited amount of training data. For the first three experiments we use 50 sample trajectories to learn the model while we varied the number of sample trajectories in this experiment.
       </paragraph>
       <paragraph>
        In each experiment we only vary one parameter and keep the remaining parameters at their optimal value. We set the optimal values such that the GP models provide the best prediction performance. In particular, we have chosen the standard deviation of the additive noise value as {a mathematical formula}σadd=10−2std(w), that is, {a mathematical formula}α=1. We optimized the hyper-parameters of the models for 150 optimization steps and we assumed 0.5 Nm standard deviation for additive torque noise. We used 50 observed trajectories for training, that is, a total of 1000 training data points. For the sparse method, we used 25% of the observed data points as pseudo inputs, that is, {a mathematical formula}M=N/4.
       </paragraph>
       <paragraph>
        The results of the model comparison tasks can be seen in Fig. 6. With increasing additive noise factor we gain more numerical stability, but the accuracy of reward prediction decreases slightly. However, we observed that the sparse method often overfits the data, which results in the worse performance with higher additive noise factor. When comparing the amount of hyper-parameter optimization steps, we can conclude that after only optimization 50 steps we can already obtain accurate models. However, we also see a small overfitting effect for the sparse models as we continue the optimization. When we add additional control input noise to the system, the standard GP approach could capture the uncertainty well. However, just as with the additive noise experiment, the sparse method tends to overfit the data and produces inaccurate predictions. Finally, an increasing number of sample trajectories clearly has a positive effect on the prediction accuracy. However, the training time steeply increases with a higher amount of training data.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="6">
     <section-title>
      Results
     </section-title>
     <paragraph>
      We evaluated our data-efficient contextual policy search method on a context-free comparison task and three contextual motor skill learning tasks. In our context-free task, we compare the GPREPS approach with the competing PILCO method on a simulated 4-link pendulum balancing task. In the contextual learning tasks, we learn how to throw a ball to distinct targets with a simulated 4-link robot. In the second task, a 7-DoF robot arm has to learn how to move a target puck in a hockey game. Here we present simulated results as well as real robot results. We also compare our approach with CrKR [20] and to contextual model-free REPS. In the third experiment we use GPREPS to learn to play table tennis with a simulated robot arm. As the lower-level controllers needs to scale to anthropomorphic robotics, we implement them using the Dynamic Movement Primitive [17] approach, which we will now briefly review.
     </paragraph>
     <section label="6.1">
      <section-title>
       Dynamic Movement Primitives
      </section-title>
      <paragraph>
       To parametrize the lower-level policy we use an extension [19] to the Dynamic Movement Primitives (DMPs) introduced in [17]. A dynamic movement primitive is defined as second order dynamical system that acts like a spring-damper system which is activated by a non-linear forcing function f{a mathematical formula}{a mathematical formula} with constant parameters {a mathematical formula}αx, {a mathematical formula}βx and {a mathematical formula}αz. Typically, a separate DMP is used for each joint of the robot. The phase variable {a mathematical formula}zt acts as internal clock of the movement. It is shared between all joints and synchronizes the joints. It is simulated by a separate first order dynamical system and is initialized as {a mathematical formula}z0=1. It converges to 0 as {a mathematical formula}t→∞ and drives the non-linear forcing function f. The parameter g is the unique point attractor of the system. The spring-damper system is modulated by the function {a mathematical formula}f(zt;v)=ϕ(zt)Tv that is linear in its weights v, but non-linear in the phase {a mathematical formula}zt. The weights v specify the shape of the movement and can be initialized with expert demonstrations. The basis functions {a mathematical formula}ϕi(zt), {a mathematical formula}i=1,…,K activate the weights as the trajectory evolves. The basis functions are defined as{a mathematical formula} where {a mathematical formula}ci is the center of the basis center and {a mathematical formula}σi the bandwidth. The squared exponential basis functions are multiplied by {a mathematical formula}zt such that f vanishes for {a mathematical formula}t→∞. Thus, for {a mathematical formula}t→∞, the DMP will behave as linear, stable system with point attractor g. The speed of the trajectory execution can be regulated by the time scaling factor {a mathematical formula}τ∈R+. The weight parameters v of the DMP can be initialized from observed trajectories {a mathematical formula}{xobs,x˙obs,x¨obs} by solving{a mathematical formula} where {a mathematical formula}Φt,⋅=ϕT(zt) is the matrix of basis vectors at time step t. In a robot skill learning task, we can adapt the weight parameters v, the goal attractor g and the time scaling factor τ to optimize the trajectory. Additionally, we can also adapt the final desired velocity {a mathematical formula}g˙ of the movement with the extension given in [19].
      </paragraph>
      <paragraph>
       To reduce the dimensionality of the learning problem we usually learn only a subset of the DMP hyper-parameters. For example, when learning to return balls in table tennis, we initialize and fix the weights v from expert demonstration. Subsequently, we adapt the goal attractor g and the final velocity {a mathematical formula}g˙ of the DMPs to maximize the reward. After obtaining a desired trajectory by the DMP, the trajectory is followed by a feedback controller which is part of the lower-level control policy. In the presented tasks, the motor primitive is always executed for a predefined amount of time. For a more detailed description of the DMP framework we refer to [19].
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Exploiting reward models as prior knowledge
      </section-title>
      <paragraph>
       A simple approach to improve the data-efficiency of the model-free contextual REPS algorithm is to use the known reward model {a mathematical formula}R(τ,s) evaluate a single outcome trajectory in multiple contexts s. Such a strategy is possible if the evaluated trajectories {a mathematical formula}τ[i] do not depend on the context variables s. For example, if the context specifies a desired target for throwing a ball, we can use the ball trajectory to evaluate the reward for multiple targets {a mathematical formula}s[i].
      </paragraph>
     </section>
     <section label="6.3">
      <section-title>
       4-link pendulum balancing task
      </section-title>
      <paragraph>
       In this task, the goal is to find a PD controller that balances a simulated 4-link planar pendulum around the upright position. The pendulum has a total length of 2 m and a total mass of 70 kg. The reward for a sample trajectory is the sum of quadratic rewards along the trajectory {a mathematical formula}R(τ,s)=−∑tTx˜tTQx˜t, where {a mathematical formula}x˜t is the deviation from the upright position at time t. We chose the initial state distribution around the upright position to be Gaussian. We compare GPREPS to the model-free REPS [30] and PILCO [9], [12], a state of the art model-based policy search algorithm. As PILCO cannot learn contextual policies, we learn context-free upper-level policies with REPS and GPREPS to have a fair comparison. Thus, the upper-level policy is given by a Gaussian, {a mathematical formula}π(ω)=N(ω|μ,Σ). The lower-level controller is represented as a PD controller {a mathematical formula}ut=G[x˜tTx˜˙tT]T. The gain matrix {a mathematical formula}G4×8 is obtained by reshaping the parameter vector {a mathematical formula}ω32×1. We initialize the models of the model-based approaches with 6 seconds of real experience, which was collected by a random policy. We use sparse GP models [36] for PILCO and GPREPS.
      </paragraph>
      <paragraph>
       In Table 3 we show the required amount of real experience to reach certain reward limits. As shown in the table, GPREPS requires two orders of magnitude fewer trials than REPS to converge to the optimal solution. PILCO achieved faster convergence as the gradient-based optimizer computes greedy updates while GPREPS continued to explore. The difference to PILCO might be decreased by updating the policy more than once between the data collections. However, the difference is negligible compared to the difference to the model-free method.
      </paragraph>
      <paragraph>
       Despite the fact that PILCO and GPREPS significantly outperformed the model-free REPS in terms of data-efficiency, the model-based algorithms typically require a higher amount of computational time for policy update. In Fig. 7, we show the learning progress against computational time required by the algorithms. In this evaluation we only consider the amount of time taken by the algorithms, but we omit the time taken for policy evaluation on the robot. In this regard REPS is superior compared to the model-based algorithms, as the samples are already evaluated on the robot. Note that this comparison is heavily biased towards favoring the model-free method. Nevertheless, the experiment gives an intuition of the scale of real world computational times when working with real robots.
      </paragraph>
      <paragraph>
       For both GPREPS and PILCO we use GP models, for which model training takes a significant amount of time. Note that in our learning scenario 1 second of real experience corresponds to 50 data points, with 12 dimensional training input and 8 dimensional training target. Other than model training, the model-based methods require additional computational time for policy update. For GPREPS, we use the sampling approach to evaluate 500 artificial policy parameter samples, where each predicted trajectory consists of 100 steps and we use 20 trajectories for a single parametrization. In other words, GPREPS requires the prediction of 10,000 trajectories per policy update, which can be evaluated efficiently on GPUs (Fig. 5(b)). For PILCO on the other hand, we cannot use parallelization straightforwardly and the computations are more involved due to the moment-matching approach and gradient computations. Additionally, PILCO repeatedly recomputes the policy gradient until it converges to a local minimum. Thus, computational times may vary significantly for PILCO. As we can see in the figure, GPREPS requires roughly an order of magnitude higher computational time compared to the model-free REPS, while for PILCO this number varies between 1.5 to 2 orders of magnitude.
      </paragraph>
      <paragraph>
       Although computational times are generally higher with model-based algorithms, especially when using GP models, in many real-world scenario model-based approaches might still learn the task faster compared to model-free methods. Some real robot experiments might require minutes to evaluate, which would make learning times impractically large with model-free algorithms. Furthermore, the aforementioned concerns with robot experiments, such as robot wear and the requirement for expert supervision, are not directly addressed with model-free methods.
      </paragraph>
     </section>
     <section label="6.4">
      <section-title>
       Ball-throwing task
      </section-title>
      <paragraph>
       In this task, a 4-link robot has to learn to throw a ball at a target position. The target position {a mathematical formula}s=[x,y] is uniformly distributed in a specified range, and we learn a contextual upper-level policy {a mathematical formula}π(ω|s). The context is varied from {a mathematical formula}x∈[5,15] m and {a mathematical formula}y∈[0,3] m. The lengths and the masses of the links were set to {a mathematical formula}l=[0.5,0.5,1.0,1.0] m and {a mathematical formula}m=[17.5,17.5,26.5,8.5] kg respectively. The robot coarsely models a human with the joints representing the ankle, knee, hip and shoulder.
      </paragraph>
      <paragraph>
       In this experiment, we used GPREPS to find DMP shape parameters v for throwing a ball to multiple targets while maintaining balance. The reward function was defined as{a mathematical formula} The first term punishes minimum distance of the ball trajectory b to the target s. We make the learning problem more challenging by penalizing joint angles that would be unrealistic for a human-like throwing motion. Thus, the second term describes a punishment term to force the robot to stay in given joint limits such that a human-like throwing motion is learned. The penalty term is defined as{a mathematical formula} where {a mathematical formula}Il and {a mathematical formula}Iu are diagonal weighting matrices, where the diagonal elements take the value 0 if the joint limit constraints are not violated and 1 if the joint limits are violated{a mathematical formula} The joint angle and angular velocity limits are defined as{a mathematical formula}{a mathematical formula} and finally {a mathematical formula}xl=[qlT,q˙lT]T and {a mathematical formula}xu=[quT,q˙uT]T. The last term of the reward function favors energy-efficient movement. In our experiment we set the reward weighting factors to {a mathematical formula}c1=102, {a mathematical formula}c2=103 and {a mathematical formula}c3=10−8.
      </paragraph>
      <paragraph>
       As lower-level controllers, we used DMPs with 10 basis functions per joint. We modified the shape parameters, but fixed the final position and velocity of the DMP to be the upright position and zero velocity. In addition, the lower-level policy also contained the release time {a mathematical formula}tr of the ball as a free parameter, resulting in a 41-dimensional parameter vector ω. After generating the reference trajectory with the DMPs, we use a PD trajectory tracker controller to generate the control inputs {a mathematical formula}ut.
      </paragraph>
      <paragraph>
       To produce trajectory rollouts with the model-based GPREPS, we learn three distinct models of the environment. The first model represents the dynamics of the robot. We use the observed state transitions as training samples for this model. The second model is used to predict the initial position and velocity of the ball at the release time {a mathematical formula}tr. The third GP model represents the free dynamics of the ball while in flight. It is used to predict the trajectory of the ball using its initial state predicted by the second GP model.
      </paragraph>
      <paragraph>
       The policy {a mathematical formula}π(ω|s) was initialized such that the robot is expected to throw the ball approximately 5 m without maintaining balance, which led to high penalties. We found this policy by applying the context-free REPS. GPREPS learned to accurately hit the target for the given range of targets. Fig. 8 shows the learned motion sequence for two different targets. The displacement for targets above {a mathematical formula}s=[13,3]T m could raise up to 0.5 m, otherwise the maximal error was smaller than 10 cm. The policy chose different DMP parametrizations and release times for different target positions. To illustrate this effect we show two target positions {a mathematical formula}s1=[6,1] m, {a mathematical formula}s2=[12,3] m in Fig. 8. When the target was farther away the robot showed a more distinctive throwing movement and released the ball slightly later.
      </paragraph>
      <paragraph>
       The learning curves for REPS and GPREPS are shown in Fig. 9. In addition, we evaluated REPS using the known reward model {a mathematical formula}R(τ,s) to generate additional samples with randomly sampled contexts {a mathematical formula}s[i]. We denote these experiments as extra context. We also evaluated GPREPS when learning the expected reward model directly {a mathematical formula}Rsω=f(s,ω) as a function of the context and parameters ω with a GP model (denoted by direct). Additionally, we investigated a learning scenario where we observe the individual terms of the decomposed reward function and learn models that predict the individual terms from policy parameters ω and context s. The terms we observe are the distance penalty {a mathematical formula}Rdist=−c1mint⁡‖bt−s‖2, the state constraint penalty term {a mathematical formula}Rx=−c2∑tfc(xt) and the torque penalty term {a mathematical formula}Ru=−c3∑tutTut. This experiment represents a transition between the standard GPREPS and the GPREPS direct approach where we only use a limited amount of expert knowledge about the task. Thus, this method is easy to implement on any new test setting. In the following we refer to this approach as GPREPS (reward decomposition).
      </paragraph>
      <paragraph>
       Fig. 9 shows that REPS converged to a good solution after 5000 episodes in most cases. In a few instances, however, we observed premature convergence resulting is suboptimal performance. The performance of REPS could be improved by using extra samples generated with the known reward model (extra context). In this case, REPS always converged to good solutions. For GPREPS, we sampled ten trajectories initially to obtain a confident GP model. We evaluated only one sample after each policy update and, subsequently, updated the learned forward models. We used 500 artificial samples and 20 sample trajectories per sample to obtain the expectation. GPREPS converged to a good solution in all cases after 30–40 real evaluations. Directly learning {a mathematical formula}Rsω also resulted in an improved learning speed, but we observed a highly varying, on average lower quality in the resulting policies (GPREPS (direct)). However, we observe that the final results with the GPREPS (reward decomposition) approach is better than GPREPS direct, but worse than GPREPS. Interestingly, the final solution is consistently better than REPS. Thus, we can conclude that even a limited amount of expert knowledge about the task can provide better results compared to the model-free REPS. Note that this approach is easy to implement on any novel test scenario. This results confirms our intuition that decomposing the forward model into multiple components simplifies the learning task.
      </paragraph>
      <section label="6.4.1">
       <section-title>
        Influence of the number of artificial samples
       </section-title>
       <paragraph>
        In the following, we investigate the influence of the number of artificial samples used to update the policy on the learning performance. To obtain an accurate estimation of the distribution {a mathematical formula}p(s,ω), we are interested in a high number of artificial samples. However, the computation time between policy updates linearly increases with the number of artificial samples generated by the GP models. While a long policy update interval does not influence the data efficiency of GPREPS, from a practical point of view we prefer to keep it as low as possible, without affecting the learning performance.
       </paragraph>
       <paragraph>
        In Fig. 10(a) we show the learning curves with GPREPS when using different amount of artificial samples. As the figure shows, increasing the amount of artificial samples always has a positive influence on the learning performance. However, we do not see much difference above using 500 samples. Using a lower number of artificial samples resulted in lower quality final policies with highly varying performance.
       </paragraph>
      </section>
      <section label="6.4.2">
       <section-title>
        Learning with stochastic dynamics
       </section-title>
       <paragraph>
        For learning problems where the dynamics of the robot and its environment are stochastic, the variance of the resulting trajectory τ, and thus, the variance of the reward {a mathematical formula}R(τ,s) might be considerably large. As discussed earlier, with the model-free REPS algorithm we typically require a single rollout evaluation for a given context-parameter pair, which we then assume to be the expected outcome. While this is a reasonable assumption for deterministic problems, most real-world robot tasks contain some level of stochasticity. GPREPS avoids the problem of learning with stochastic outcomes by averaging over multiple samples of the same context-parameter pair evaluation, as in Eq. (13).
       </paragraph>
       <paragraph>
        In the following we demonstrate the effect of noise in the dynamics on the learning performance of the robot throwing task. We implement the stochasticity of the dynamics by adding Gaussian noise to the initial state of the ball at the release time. In a real robot experiment, this stochasticity might emerge from the unknown dynamics of ball release with a robot hand. The standard deviation of the initial position of the ball is set to α7.5 cm, while for the initial velocity it is set to α30 cm/s, where α is a scaler. We investigate the effect of increasing amount noise on the learning performance, by varying {a mathematical formula}α∈[0,1].
       </paragraph>
       <paragraph>
        In Fig. 10(b), we can see the quality of the converged policy of REPS and GPREPS with an increasing level of stochasticity in the dynamics. As the figure shows, GPREPS already outperforms REPS in the deterministic case. However, with an increasing level of system stochasticity, the quality of the final policy learned by REPS gets significantly worse. The reason for this phenomena is that REPS uses noisy samples of the reward instead of the expected reward. In contrast, the forward models of GPREPS learns the stochasticity of the system, and thus, it is able to approximate the expected reward well. Even with a significant amount of noise on the system ({a mathematical formula}α=1), GPREPS is able to avoid converging to local optima, and provides good quality policies with relatively low variance.
       </paragraph>
      </section>
     </section>
     <section label="6.5">
      <section-title>
       Robot hockey with simulated environment
      </section-title>
      <paragraph>
       In this task we learn a robot hockey game using the KUKA lightweight robot arm in Fig. 1. The goal of the robot is to shoot a hockey puck using the attached hockey stick to move a target puck, which is located at a certain distance. The robot can move the target puck by hitting it with another puck, that we denote as control puck. The initial position of the target puck {a mathematical formula}[bx,by]T is varied in both dimensions between experiments. As an additional goal, we require the displacement of the target puck {a mathematical formula}dt to be as close as possible to the desired distance {a mathematical formula}d⁎. We also vary the distance {a mathematical formula}d⁎ between experiments. Thus, the robot not only has to learn to shoot the provided puck in the direction of the target puck, but also with the appropriate force. The simulated hockey task is depicted in Fig. 11. The context variable is defined as {a mathematical formula}s=[bx,by,d⁎]T.
      </paragraph>
      <paragraph>
       We first evaluated our method in simulation. We encoded the hitting motion into a DMP. The weight parameters were set by imitation learning. We learn only the final position g, final velocity {a mathematical formula}g˙ and the time scaling parameter τ of the DMP. As the robot has seven degrees of freedom, we get a 15-dimensional parameter vector ω of the lower-level policy. For trajectory tracking, we used an inverse dynamics controller. We chose the initial position of the target puck to be uniformly distributed from the robot's base with displacements {a mathematical formula}bx∈[1.5,2.5] m and {a mathematical formula}by∈[0.5,1] m. The desired displacement context parameter {a mathematical formula}d⁎ is also uniformly distributed {a mathematical formula}d⁎∈[0,1] m. The reward function{a mathematical formula} consists of two terms with equal weighting. The first term penalizes missing the target puck located at position {a mathematical formula}b=[bx,by]T, where the control puck trajectory is {a mathematical formula}x1:T. The second term penalizes the error in the desired displacement of the target puck, where {a mathematical formula}dT is the resulting displacement of the target puck after the shot.
      </paragraph>
      <paragraph>
       In the robot hockey task, modeling the contact of the stick and the control puck is challenging due to the curved shape of the hockey stick. When shooting the puck, the stick might push, or hit the puck multiple times. To avoid extensive modeling of these contacts, we use a GP model to directly predict the state of the puck at a constant distance of 0.2 m in the x direction from the robot's base, where contact between the stick and the puck is no longer possible. We use solely the DMP parameters ω as the input to this model. We learn another model for the free dynamics of the sliding pucks, which are used for predicting the puck trajectories. For predicting the contact of the pucks, we assume that we know the radius of the pucks, and thus, we can always predict when a contact is happening. For modeling the effect of the contact, we also learn a separate GP model that predicts the state of the pucks after the contact given the state of the pucks before the contact.
      </paragraph>
      <paragraph>
       We compared GPREPS to model-free REPS and CrKR [20], a state-of-the-art model-free contextual policy search method. Furthermore, we evaluated GPREPS without decomposing the experiment, and directly predict the reward {a mathematical formula}Rsω with a GP model using the policy parameter ω and the context s as input (GPREPS (direct)). The resulting learning curves are shown in Fig. 12(a). GPREPS learned the task already after 120 interactions with the environment, while the model-free version of REPS needed approximately 10,000 interactions. Moreover, the policies learned by model-free REPS were of lower quality. The GPREPS (direct) algorithm resulted in faster convergence than the model-free REPS version, but the resulting policies had lower quality. CrKR uses a kernel-based representation of the policy. For a fair comparison, we used a linear kernel for CrKR. The results show that CrKR could not compete with model-free REPS. We believe the reason for the worse performance of CrKR lies in its uncorrelated exploration strategy. The resulting policy of CrKR is a Gaussian with a diagonal covariance matrix, while REPS estimates a full covariance matrix. Moreover, CrKR does not use an information-theoretic bound to determine the weightings of the samples. The learned movement is shown in Fig. 11 for two different contexts. After 100 evaluations, GPREPS placed the target puck accurately at the desired distance with a displacement error ≤5 cm.
      </paragraph>
      <paragraph>
       We also evaluated the performance of GPREPS on the hockey task using a real KUKA lightweight arm, see Fig. 1. A Kinect sensor was used to track the position of the two pucks at a frame rate of 30 Hz. We smoothed the trajectories in a pre-processing step with a Butterworth filter. We slightly changed the context variable ranges to meet the physical constraints of the test environment. We decreased the range of the position variables in both dimensions to {a mathematical formula}bx∈[1.5,2] m and to {a mathematical formula}by∈[0.4,0.8] m from the robot's base. Furthermore, we decreased the desired distance range to {a mathematical formula}d⁎∈[0,0.6] m. We kept the reward function unchanged, but we slightly altered the modeling of the environment. Due to the low sampling frequency of the Kinect sensor, we did not receive enough information about the exact contact model of the pucks. To avoid the errors coming from a crude model, we exchange the contact model by a model that directly predicts the displacement of the second puck using the incoming puck's relative position and velocity.
      </paragraph>
      <paragraph>
       The resulting learning curve of GPREPS is shown in Fig. 12(b). As we can see, the robot adapts the lower-level policy parameters towards the optimum within a small low number of interactions with the real environment. The final reward is slightly different compared to the simulated environment due to the altered modeling and slightly distorted measurement data of the Kinect sensor. Despite these effects the GP models could average over the uncertainty and produce accurate predictions of the expected rewards.
      </paragraph>
     </section>
     <section label="6.6">
      <section-title>
       Robot table tennis
      </section-title>
      <paragraph>
       In this task, we learn hitting strokes in a table tennis game with a simulated Biorob [25] arm (Fig. 13(a)). The robot is mounted on two linear axis for moving in the horizontal plane. The robot itself has rotational joints, resulting in 8 actuated joints. A racket is mounted at the endeffector of the robot. The Biorob is a lightweight tendon-driven robot arm that, due to its small weight, can perform highly dynamic movements. The simulated robot can be seen in Fig. 13(b). The construction of the real robot platform is ongoing work. In simulation, we simulated the ball with a standard ballistic flight model with air drag, but neglected simulating the spin or measurement noise. The goal of the robot is to return the incoming ball at a target position on the opponent's side of the table. However, the incoming ball has a changing initial velocity v and the return target position b is also varied uniformly on the opponent's side of the table. Thus, the context is defined as {a mathematical formula}s=[vx,vy,vz,bx,by]T. For an illustration of the task see Fig. 13. We chose the range of the initial velocities such that the incoming ball bounces only once on the forehand side of the table. To learn the task, we use DMPs where we initialize the DMP weight parameters by kinesthetic teach-in, such that the movement resembles a forehand hitting motion. We only learn the final positions and final velocities of the DMP trajectories, furthermore the τ time-scaling parameter and the starting time point of the movement, altogether 18 parameters. The initialized policy is able to execute only the demonstrated movement without adapting to the incoming ball.
      </paragraph>
      <paragraph>
       We decompose the whole experiment into five distinct models. With the first model, we predict the landing position, landing velocity and the landing time of the incoming ball using the observed initial velocities v of the ball. Such a model is sufficient for our modeling as we want to learn to return only balls that land exactly once on the table. The second model predicts the trajectory of the ball given its position and velocity predicted by the first model. The third and fourth model predicts the trajectory and orientation of the racket mounted at the endeffector. To avoid the complex modeling of the 8-DOF robot dynamics, we use time dependent GP models to directly predict the position and orientation (in the quaternion representation) from policy parameters. To create time dependent models, we fit a linear basis function model with 40 local basis functions {a mathematical formula}ϕ(t) per dimension to the trajectory of the racket, where the basis functions only depend on the execution time of the trajectory. The task of the GP is now to predict the weights of the basis functions given the policy parameters ω. The training input for each model is the lower-level policy parameter ω, the training target is the local model weight ν. Finally, the fifth model predicts the landing position of the returned ball in case of a contact, which is detected by an SVM classifier with a linear kernel. The input to the classifier is the relative velocity of the ball and the racket, the position and orientation of the racket at the time point when the absolute distance between the racket and the ball is minimal. For the contact model, we use the same training input data as for the classifier and the observed landing position {a mathematical formula}p=[px,py] as the training target.
      </paragraph>
      <paragraph>
       The reward function is defined by the sum of penalties for missing the ball and missing the target return position{a mathematical formula} where {a mathematical formula}c=[c1,c2]T are weighting parameters, {a mathematical formula}τb and {a mathematical formula}τr is the incoming ball and the racket trajectories, while b is the target and p is the returned ball landing position.
      </paragraph>
      <paragraph>
       As learning a good contact model between the racket and the ball requires many samples, in the first few iterations we only have to focus on learning to hit the ball. As soon as we learned a good hitting stroke and we have enough contact samples, we can use the learned contact model to provide confident predictions. Thus, we change the weighting parameters {a mathematical formula}c=[c1,c2]T, such that in the beginning of the learning {a mathematical formula}c2 is negligible compared to {a mathematical formula}c1, but we add an extra constant penalty term. By doing so, the algorithm focuses only on learning to hit the ball. After collecting enough samples to learn a good contact model, we set {a mathematical formula}c1=c2 and disable the constant penalty term. Now, the algorithm focuses both on hitting the ball and returning it close to the target position b. Note, that we always use {a mathematical formula}c1=c2 for the model-free algorithms.
      </paragraph>
      <paragraph>
       We compared GPREPS with the model-free REPS and a model-based REPS, where we directly predict the reward from context-policy parameter pairs. The learning curves are depicted in Fig. 14. We can clearly see that GPREPS outperforms the other two algorithms. GPREPS consistently provides high quality policies after only 150 evaluations. The final policy avoids hitting into the net and the displacement from the desired target return position remains below 30 cm. An example for a complete experiment outcome prediction is depicted in Fig. 16.
      </paragraph>
      <paragraph>
       When using the GPREPS (direct) approach that does not use the prior knowledge of the structure of the experiment, we obtain policies that often get stuck in a local optima. Typically we observe policies that hit the ball to the net, or even miss the ball. The model-free REPS approach results in a good performance in general, but with the disadvantage of being data-inefficient. Model-free REPS requires at least 4000 evaluations on average to learn a policy that consistently returns the ball, with only a few instances of hitting the ball into the net. An animation of two different strikes learned with GPREPS is shown in Fig. 15.
      </paragraph>
      <paragraph>
       This experiment concludes that GPREPS is applicable to learn complex robotic tasks, even in the presence of contact models that are, in general, more difficult to learn. In future work, we will investigate how we can use a GP model-based version HiREPS [8], to learn not only forehand, but backhand hitting motion as well. Furthermore, in order to learn to play a general table tennis game, we will extend the context that allows balls with varying initial positions and we will evaluate GPREPS on the real robot platform.
      </paragraph>
     </section>
     <section label="6.7">
      <section-title>
       Initialization and limitations of the upper-level policy
      </section-title>
      <paragraph>
       To ensure safety and efficiency during the learning process, we have to properly initialize the upper level policy parameters. In our experiments we used a linear Gaussian model for representing of the upper level policy {a mathematical formula}π(ω|s)=N(ω|a+sA,Σ), with parameters {a mathematical formula}{a,A,Σ}. Initially, we set the linear model {a mathematical formula}A=0 to zero and obtain the offset parameter a by imitation learning. We set the initial exploration covariance Σ as diagonal matrix such that we get enough initial exploration of the parameter space, but exploration is still safe for the robot. As REPS typically decreases the exploration variance at each policy update step until Σ collapses and the policy parameters converge to the final solution. Thus, the initial Σ has to be chosen carefully, such that the optimal solution is within the range of the initial exploration range.
      </paragraph>
      <paragraph>
       Model based solutions with gradient-based policy updates, such as PILCO, can scale to higher dimensions as they neglect the exploration problem. With REPS we usually learn policies with not more than hundred parameters as the computation of the covariance matrix gets intractable. Thus, currently GPREPS cannot be applied for complex robot learning tasks with more than 100 parameters.
      </paragraph>
     </section>
     <section label="6.8">
      <section-title>
       Learning with real robots
      </section-title>
      <paragraph>
       For all the evaluations presented in this paper we used robot simulators except for one task where we used a KUKA lightweight arm. For most cases it is convenient to use a robot simulator to demonstrate the learning efficiency, but ultimately our goal is to provide good real world results. As simulators typically use hand tuned models of the robot and its environment, skills learned with simulators might perform poorly on the real robot. This is mostly due to the inaccuracies of the hand crafted models, limited expert knowledge and the lack of model adaptation.
      </paragraph>
      <paragraph>
       In our approach, we propose to learn the models by interacting with the environment. The resulting data driven modeling approach addresses the inaccuracies resulting from imperfectly tuned models and the lack of accurate expert knowledge. Moreover, it is able to adapt the learned models by continuously updating the model parameters using the most recent measurement data. We chose Gaussian Process regression as modeling approach, which can significantly reduce the bias coming from the limited representational power of mathematical models.
      </paragraph>
      <paragraph>
       When working on real robot experiments, obtaining the measurement data for model learning is one of the most important challenge. With real world experiments the measured quantities are typically corrupted by noise, which need to be filtered to have a more reliable estimate. Furthermore, devices with lower sampling frequency provide less measurement data, which ultimately increases the complexity of the learned model. Interpolation might help to build more reliable models, but it can introduce a higher model bias. To avoid using poor quality models, it is advisable to validate the learned models before using them to simulate experiment rollouts.
      </paragraph>
      <paragraph>
       In some tasks, it might not even be necessary to thoroughly model the whole experiment in order to infer the reward of a rollout, but it is sufficient to predict the relevant quantities for computing the reward from the policy and context parameters. We illustrated this idea for the ball throwing task, where we decomposed the reward function and solved several regression problems to obtain the models. This approach typically has a lower accuracy but a significantly faster computational time assuming long-term trajectory prediction with GP models is not necessary. On the other hand, due to the presumably lower quality models, the number of required evaluations might slightly increase. Nevertheless, for a new experiment it is advisable to begin with the simpler modeling approach to reduce the modeling effort and the amount of expert knowledge required.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      We presented GPREPS, a novel model-based contextual policy search algorithm based on GP models and information-theoretic policy search. We learn an upper level policy that efficiently generalizes the lower level policy parameters ω over multiple contexts. GPREPS is based on REPS, an information-theoretic policy search algorithm. It exploits learned probabilistic forward models of the robot and its environment to predict expected rewards of artificially generated data points. For evaluating the expected reward, GPREPS samples trajectories using the learned models. Unlike deterministic inference methods used in state-of-the art approaches for policy evaluation, trajectory sampling is easy to implement, easy to parallelize and does not limit the policy class or the used reward model.
     </paragraph>
     <paragraph>
      With simulated and real robot experiments, we demonstrated that GPREPS significantly reduces the required amount of measurement data to learn high quality policies compared to state-of-the-art model free contextual policy search approaches. Moreover, the GP models are able to incorporate the model uncertainty and produce accurate trajectory distributions. Thus, with GPREPS we avoid the risk of learning from noisy reward samples that results in a bias in the model-free REPS formulation. The increased data efficiency makes GPREPS applicable to learning contextual policies in real-robot tasks. Since existing model-based policy search methods cannot be applied to the contextual setup, GPREPS allows for many new applications of model-based policy search.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Derivation of contextual episode-based REPS
     </section-title>
     <paragraph>
      The constrained optimization problem of episode-based REPS for contextual policy search is given by{a mathematical formula} We can write up the Lagrangian of the corresponding constrained optimization problem in the form{a mathematical formula}{a mathematical formula} By setting the gradient of {a mathematical formula}L(p,η,θ) w.r.t. {a mathematical formula}p(s,ω) to zero we obtain the solution{a mathematical formula} with the base line {a mathematical formula}V(s)=θTϕ(s). Due to the constraint {a mathematical formula}∫∫s,ωp(s,ω)dsdω=1, we also have that{a mathematical formula} The dual function is obtained by setting the solution for {a mathematical formula}p(s,ω) back into the Lagrangian. After rearranging terms, we obtain{a mathematical formula} Setting Eq. (A.5) into the dual we can eliminate the λ parameter and obtain the dual function{a mathematical formula} Using a dataset {a mathematical formula}D={s[i],ω[i],Rsω[i]}i=1…N where the context parameter pairs have been sampled from {a mathematical formula}q(s,ω), the integral in the dual function can be approximated as{a mathematical formula}
     </paragraph>
     <paragraph>
      The dual function is convex in η and θ[30]. To solve the original optimization problem, we need to minimize {a mathematical formula}g(η,θ;D) such that {a mathematical formula}η&gt;0[6], hence, we have to solve another constrained optimization problem, which is, however, much easier to solve. We can use any solver for such problems, e.g., the interior point algorithm. For an efficient optimization of the dual, also the corresponding gradients of the dual are required. They are given by{a mathematical formula}{a mathematical formula}
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>