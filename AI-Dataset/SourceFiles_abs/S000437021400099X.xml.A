<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    The unbiased black-box complexity of partition is polynomial.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Complexity theory aims at determining the difficulty of computational problems. In classical theoretical computer science, the fruitful interplay between complexity theory, aiming at proving that a certain effort is necessary to solve a problem, and theory of algorithms, giving an algorithmic solution for a problem and showing that it can be solved with a certain computational effort, was a driving force to develop the field.
     </paragraph>
     <paragraph>
      Developing a similarly thorough theory for heuristic search methods is at the heart of the theory of randomized search heuristics community. The latter has been steadily growing in the last twenty years. Many tight or near-tight run time analyses for various problems and algorithms exist, see, e.g., the recent textbooks [26], [4], [21]. In contrast to this, the complexity theory of randomized search heuristics is still in its infancy.
     </paragraph>
     <section>
      <section>
       <section>
        <section-title>
         A complexity theory for randomized search heuristics
        </section-title>
        <paragraph>
         Applications of search heuristics typically do not consider the problem as explicitly given, in contrast to traditional optimization algorithms which operate on a problem instance containing all relevant information. Instead, a search heuristic supposes access to the problem instance via an oracle/as a black box and the heuristic can learn about the concrete instance at hand only by learning the function value of the search points it generates. Search heuristics are therefore called black-box optimization algorithms. The reasons for this restricted access are manifold: sometimes no explicit representation of the problem exists (a function evaluation might correspond to a real world experiment), sometimes we do not want to fully exploit the given representation (e.g., due to its size or complexity). The efficiency of heuristic search methods is measured by the number of function evaluations until an optimal search point is evaluated for the first time, or, depending on the context, until a solution of a certain quality has been found. This is very different from classical complexity notions (e.g., the Turing model of computation), which assume that the algorithm is fully aware of the complete problem instance, and where efficiency of an algorithm is measured by counting the number of arithmetic operations that it performs given the input data.
        </paragraph>
        <paragraph>
         As the run time (also referred to as the optimization time) of a search heuristic is measured by the number of function evaluations, this should be reflected in a corresponding complexity model. This is what black-box complexity models are developed for. They try to give a reasonable estimate of how easily a problem can be optimized by typical search heuristic methods. A useful black-box complexity notion thus provides lower bounds for a sufficiently large class of black-box search methods. In the simplest black-box complexity model, the so-called unrestricted black-box complexity, one just counts the number of function evaluations that are necessary to solve a problem, no single restriction is made on how the search points are being generated. The unrestricted black-box complexity gives thus a lower bound for all black-box algorithms, i.e., all algorithms that do not exploit knowledge about the problem instance other than those obtained from the function evaluations—this class in particular contains all stochastic hill-climbing algorithms but also bio-inspired search heuristics like evolutionary algorithms, physics-inspired algorithms like simulated annealing, and many other classes of search heuristics. Hence, the unrestricted black-box complexity is a very general notion, independently studied in a number of different scientific communities (see Section 2.3 for more details).
        </paragraph>
        <paragraph>
         While theoretically very pleasing, it quickly turned out that the class of all black-box algorithms is possibly too wide to deduce powerful statements for the most commonly used randomized search heuristics. Already in the seminal work of Droste, Jansen, Tinnefeld, and Wegener [9] (see [10] for the journal version), it was observed that there are black-box algorithms solving the MaxClique problem (the optimization version of the {a mathematical formula}NP-complete decision problem Clique) using a polynomial number of function evaluations.{sup:1}
        </paragraph>
        <paragraph>
         To increase the practical relevance of black-box complexity theory, a number of more restrictive models have been developed. A promising direction is the so-called unbiased black-box complexity model by Lehre and Witt in [24], restricting the class of admitted black-box optimization algorithms in a natural way which still includes a large class of commonly used randomized search heuristics. In this model, all solution candidates are required to be obtained by variation operators. These variation operators must be unbiased, that is, treat the bit positions and the bit entries 0 and 1 in an unbiased way (see Section 2 for a precise definition). The unbiased black-box model admits a notion of arity in a natural way: A k-ary unbiased black-box algorithm is one that employs only variation operators that take up to k arguments. This allows for talking about mutation-only algorithms (unary algorithms, i.e., having arity one) or crossover-based algorithms (having arity at least two) from a complexity-theoretic perspective.
        </paragraph>
       </section>
       <section>
        <section-title>
         Known results
        </section-title>
        <paragraph>
         For several function classes the unbiased black-box complexity model leads to more realistic complexities. While in the unrestricted model any function class consisting of a single function has a black-box complexity of one, in the unbiased model more function evaluations are needed to generate an optimal solution from applying the variation operators. For example, the mutation-only black-box complexity of any class of functions having a unique global optimum is {a mathematical formula}Ω(nlog⁡n)[24]. The (permutation-invariant) LeadingOnes function class, which is also one of the classic test problems in the theory of randomized search heuristic community, has a mutation-only black-box complexity of {a mathematical formula}Θ(n2)[24], matching the run time of standard randomized search heuristics.{sup:2} The unrestricted black-box complexity of this LeadingOnes problem is known to be of order {a mathematical formula}Θ(nlog⁡log⁡n)[1]. Hence, for both the unimodal and the LeadingOnes test problems, the unary unbiased black-box model leads to much better complexity estimates than the unrestricted one.
        </paragraph>
        <paragraph>
         When higher-arity variation operators are used, i.e., when the algorithm may combine two or more search points to generate a new one, smaller, but still not completely unrealistic complexities were observed in [8]. For the classic test problem OneMax, which assigns to each bit string the number of ones in it, the unary unbiased black-box complexity is {a mathematical formula}Ω(nlog⁡n) by the above mentioned lower bound, but it drops to {a mathematical formula}O(n) for unbiased black-box algorithms of arity two.{sup:3} For LeadingOnes the binary unbiased complexity is shown to be {a mathematical formula}O(nlog⁡n) (as opposed to the {a mathematical formula}Ω(n2) bound for the unary unbiased complexity by Lehre and Witt [24] mentioned above). For larger arity {a mathematical formula}k≤log2⁡n, the k-ary unbiased black-box complexity of OneMax is {a mathematical formula}O(n/k) as shown in [15]. For LeadingOnes, it is known [13] that 3-ary operators reduce the unbiased black-box complexity to {a mathematical formula}O(nlog⁡n/log⁡log⁡n). In the light of [1], it seems likely that larger arities again yield further improvements, but no proof in this direction exists.
        </paragraph>
       </section>
       <section>
        <section-title>
         Our result
        </section-title>
        <paragraph>
         The above-mentioned results show that the (unary) unbiased black-box model for several classic test problems gives complexity estimates much closer to the performance of commonly used randomized search heuristics. We show that this observation does not extend in general to difficult combinatorial optimization problems. We demonstrate an {a mathematical formula}NP-hard subclass of the classic combinatorial optimization problem Partition which has a small polynomial unbiased black-box complexity, even if the arity of the variation operators is restricted to one. More precisely, we consider the subclass {a mathematical formula}Partition≠ of all Partition instances with pairwise different weights. For two natural formulations of this problem (one using a signed and one using an unsigned objective function, see Sections 3.1 and 3.2) we prove that there exists a unary unbiased black-box algorithm that solves any of such instances using only {a mathematical formula}O(nlog⁡n) function evaluations. This result shows that there are problems for which no efficient optimization algorithm exists (making the common assumption that the complexity classes {a mathematical formula}P and {a mathematical formula}NP are not identical), but that have a (small) polynomial unbiased black-box complexity.
        </paragraph>
        <paragraph>
         This paper is based on the conference paper [12].
        </paragraph>
       </section>
      </section>
     </section>
    </section>
    <section label="2">
     <section-title>
      Preliminaries
     </section-title>
     <paragraph>
      In this section we first introduce the notation used in this paper, followed by a formal definition of the unrestricted and the unbiased black-box models and a brief discussion of the black-box complexity concept.
     </paragraph>
     <section label="2.1">
      <section-title>
       Notation
      </section-title>
      <paragraph>
       The positive integers are denoted by {a mathematical formula}N. For any {a mathematical formula}k∈N, we abbreviate {a mathematical formula}[k]:={1,…,k}. Analogously, we define {a mathematical formula}[0..k]:=[k]∪{0}.
      </paragraph>
      <paragraph>
       For a bit string {a mathematical formula}x=x1⋯xn∈{0,1}n we denote by {a mathematical formula}x¯ the bit-wise complement of x (i.e., for all {a mathematical formula}i∈[n] we have {a mathematical formula}x¯i=1−xi). The bit-wise exclusive-OR is denoted by ⊕. We say that y is created from x by flipping the ith bit in x to define y as {a mathematical formula}x⊕ei, where {a mathematical formula}ei denotes the ith unit vector. For any bit string x we let {a mathematical formula}OneMax(x)=|x|1 denote the number of 1s in x (also known as the Hamming-weight of x). Let {a mathematical formula}|x|0:=n−|x|1 denote the number of zeros in x.
      </paragraph>
      <paragraph>
       For any set S we denote by {a mathematical formula}2S the power set of S, i.e., the set of all subsets of S. For any set of pseudo-Boolean functions C and any function {a mathematical formula}f:R→R we let {a mathematical formula}f(C)={f∘g|g∈C}.
      </paragraph>
      <paragraph>
       For {a mathematical formula}n∈N, we let {a mathematical formula}Sn be the set of all permutations of {a mathematical formula}[n]. For {a mathematical formula}σ∈Sn and {a mathematical formula}x∈{0,1}n we abbreviate {a mathematical formula}σ(x):=xσ(1)⋯xσ(n).
      </paragraph>
      <paragraph>
       Lastly, with ln we denote the natural logarithm to base {a mathematical formula}e:=exp⁡(1).
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Unrestricted and unbiased black-box model
      </section-title>
      <paragraph>
       A usual way to measure the complexity of a problem is to measure the performance of the best algorithm out of some class of algorithms (e.g., all those algorithms which can be implemented on a Turing machine [18], [20]) on the (for this algorithm) most difficult problem instance. As we would like to measure the complexity of a problem's optimizability by randomized search heuristics, we restrict the class of permissible algorithms to those which obtain information about the problem only by learning the objective values of possible solutions (“search points”). Thus the objective function in this setting is given as an oracle or as a black-box. Using this oracle, the algorithm may query the objective value of any solution; such a query does only return this search point's function value but no other information about the problem instance.
      </paragraph>
      <paragraph>
       Here in this work we will be concerned only with so-called pseudo-Boolean functions, i.e., real-valued objective functions defined on the set {a mathematical formula}{0,1}n of bit strings of length n. This is motivated by the fact that many randomized search heuristics, in particular evolutionary algorithms, use such a representation. Black-box complexity notions are meaningful also in other search domains and objective spaces, but for the sake of clarity, we restrict our definitions to the pseudo-Boolean settings. Results and models for more general search spaces can be found in [27], [11].
      </paragraph>
      <paragraph>
       Naturally, we do allow that the algorithms use random decisions. It follows from the black-box concept that the only type of action the algorithm may perform is, based on the objective values learned so far, deciding on a probability distribution on {a mathematical formula}{0,1}n, sampling a search point {a mathematical formula}x∈{0,1}n according to this distribution, and querying its function value (often referred to as “fitness” in the evolutionary computation community) from the oracle. This leads to the scheme of Algorithm 1, which we call an unrestricted black-box algorithm.
      </paragraph>
      <paragraph>
       As the performance measure of black-box algorithms we take the number of queries to the oracle performed by the algorithm until it first queries an optimal solution. We call this the run time, or optimization time, of the black-box algorithm. This is justified by the observation that, in typical applications of randomized search heuristics, evaluating the function value of a search point is more costly than the generation of a new search point. Since we mainly talk about randomized algorithms, we are interested in the expected number of queries.
      </paragraph>
      <paragraph>
       We can now follow the usual approach in complexity theory. Let {a mathematical formula}F be a class of pseudo-Boolean functions. The complexity of an algorithm A for {a mathematical formula}F is the maximum expected run time of A on a function {a mathematical formula}f∈F (worst-case run time). The complexity of {a mathematical formula}F with respect to a class {a mathematical formula}A of algorithms is the minimum (“best”) complexity among all {a mathematical formula}A∈A for {a mathematical formula}F. The unrestricted black-box complexity of {a mathematical formula}F is the complexity of {a mathematical formula}F with respect to the class of all (unrestricted) black-box algorithms. This is the black-box complexity as defined by Droste, Jansen, Tinnefeld, and Wegener [9], [10].
      </paragraph>
      <paragraph>
       Obviously, the class of all black-box algorithms is very powerful. For example, for any function class {a mathematical formula}F={f} consisting of one single function, the unrestricted black-box complexity of {a mathematical formula}F is 1—the algorithm that simply queries an optimal solution of f at the first opportunity verifies this bound. Also, there are black-box algorithms solving MaxClique using a polynomial number of queries in expectation [10]; see Section 2.3 for a brief discussion of this algorithm.
      </paragraph>
      <paragraph label="Definition 1">
       These drawbacks of the unrestricted black-box model inspired Lehre and Witt [24] to introduce a more restrictive black-box model, where algorithms may generate new solution candidates only from random or previously generated search points and only by using unbiased variation operators. Still this model includes most of the commonly studied search heuristics, such as many {a mathematical formula}(μ+λ) and {a mathematical formula}(μ,λ) evolutionary algorithms (EAs), simulated annealing, the Metropolis algorithm, and the Randomized Local Search algorithm. For all {a mathematical formula}k∈N, a k-ary unbiased distribution{a mathematical formula}(D(⋅|y(1),…,y(k)))y(1),…,y(k)∈{0,1}n is a family of probability distributions over {a mathematical formula}{0,1}n such that for all inputs{a mathematical formula}y(1),…,y(k)∈{0,1}n the following two conditions hold.{a mathematical formula}We refer to the first condition as ⊕-invariance and to the second as permutation-invariance. An operator sampling from a k-ary unbiased distribution is called a k-ary unbiased variation operator.
      </paragraph>
      <paragraph>
       Note that the only 0-ary unbiased distribution over {a mathematical formula}{0,1}n is the uniform distribution. 1-ary, also called unary operators are sometimes referred to as mutation operators, in particular in the field of evolutionary computation. 2-ary, also called binary operators are often referred to as crossover operators. If we allow arbitrary arity, we call the corresponding model the ⁎-ary unbiased black-box model.
      </paragraph>
      <paragraph>
       k-ary unbiased black-box algorithms can now be described via the scheme of Algorithm 2. The k-ary unbiased black-box complexity of some class of functions {a mathematical formula}F is the complexity of {a mathematical formula}F with respect to all k-ary unbiased black-box algorithms.
      </paragraph>
      <paragraph>
       Note that, for all {a mathematical formula}k≤ℓ, each k-ary unbiased black-box algorithm is contained in the ℓ-ary unbiased black-box model. For any set C of pseudo-Boolean functions we write {a mathematical formula}UBBk(C) to denote the k-ary unbiased black-box complexity of C.
      </paragraph>
      <paragraph>
       As mentioned in the introduction, Lehre and Witt [24] proved, among other results, that all functions with a single global optimum have a unary unbiased black-box complexity of {a mathematical formula}Ω(nlog⁡n). For several standard test problems this bound is met by different unary randomized search heuristics, such as the {a mathematical formula}(1+1) EA or the Randomized Local Search algorithm. Recall that, as pointed out above, the unrestricted black-box complexity of any single such function is 1. For results on higher arity models refer to [8], [15].
      </paragraph>
      <paragraph>
       Rather than bounding the expected run time of an algorithm, it is sometimes easier to show that it solves the given problem with good probability in some fixed number of iterations. If we are only interested in asymptotic black-box complexities, the following remark allows us to use such statements for computing upper bounds.
      </paragraph>
      <paragraph label="Remark 2">
       Suppose for a problem P there exists a black-box algorithm A that, with constant success probability, solves P in s iterations (that is, queries an optimal solution within s queries). Then the black-box complexity of P is at most {a mathematical formula}O(s).
      </paragraph>
      <paragraph label="Proof">
       Let c be an upper bound for the failure probability of algorithm A after s iterations. We let {a mathematical formula}A′ be the algorithm which performs independent runs of s iterations of A indefinitely one after the other. If {a mathematical formula}Xi denotes the indicator variable for the event that the ith independent run of A is successful (i.e., computes an optimum), then {a mathematical formula}Pr⁡[Xi=1]≥1−c. Clearly, {a mathematical formula}Y:=min⁡{k∈N|Xk=1} is a geometric random variable with success probability at least {a mathematical formula}1−c. Hence, {a mathematical formula}E[Y]=(1−c)−1, i.e., the expected number of independent runs of A until success is at most {a mathematical formula}(1−c)−1. Thus, we can optimize P in an expected number of at most {a mathematical formula}(1−c)−1s iterations. Since c is constant, the claim follows.  □
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       Discussion of the black-box complexity concept
      </section-title>
      <paragraph>
       In this section, we briefly discuss the differences between classical complexity models and the black-box models introduced above. The reader only interested in the result on Partition can skip this section without loss.
      </paragraph>
      <paragraph>
       What distinguishes black-box complexity from classical complexity is the fact that in the black-box setting
      </paragraph>
      <list>
       <list-item label="•">
        the algorithms have no access to the problem instance other than by sampling search points and learning their function values, and that
       </list-item>
       <list-item label="•">
        the cost measure is the number of such function evaluations that are needed to optimize a problem.
       </list-item>
      </list>
      <paragraph>
       Recall that in classical complexity settings the problem instances are typically given as a white box, and instead of function evaluations, one counts the number of arithmetic operations that are needed to compute an optimal solution (or the answer to questions like satisfiability of the instance etc.). The motivation for the usage of the black-box model in evolutionary computation is that they are typically applied to problems with very large or very complex descriptions. Instead of analyzing the particular problem instance at hand, the idea is to learn about it by evaluating solution candidates. Since such evaluations are typically the most costly part of an evolutionary algorithm, the cost of an algorithm is measured by the number of such evaluations.
      </paragraph>
      <paragraph>
       The black-box setting is a standard notion much beyond the evolutionary computation community. Black-box complexity is also referred to as query complexity in classic computer science [2], [19], [23]. Very similar notions are used in learning theory [3], [5] and numerical integration theory (information-based complexity, see [25]).
      </paragraph>
      <paragraph>
       In this short discussion, we would like to emphasize the fact that a polynomial black-box complexity of a problem does not contradict standard complexity assumptions like {a mathematical formula}P≠NP. To illustrate this, let us consider the already mentioned MaxClique problem.
      </paragraph>
      <paragraph>
       The optimization variant of the well-known {a mathematical formula}NP-complete decision problem Clique is the following task: given a graph {a mathematical formula}G=(V,E), find a subset {a mathematical formula}W⊆V of maximal size such that W forms a clique, i.e., such that all nodes in W are mutually connected. We can model the MaxClique problem in the black-box setting by assigning to each subset W of V the objective value {a mathematical formula}f(W):=|W| if W is a clique, and {a mathematical formula}f(W):=0 otherwise. Optimizing MaxClique then corresponds in a natural way to maximizing f. To model f as a pseudo-Boolean function, enumerate the vertices in V and identify a bit string {a mathematical formula}x∈{0,1}n with the set of vertices for which the corresponding entry in x is one.
      </paragraph>
      <paragraph>
       As has already been pointed out in [10], the unrestricted black-box complexity of MaxClique is at most {a mathematical formula}(n2)+1. An algorithm achieving this complexity does the following. In the first {a mathematical formula}(n2) iterations, it queries the function values of all possible subsets of size two (i.e., all bit strings with exactly two ones). This corresponds to asking whether or not an edge between the two queried search points exists. Once this has been done for all {a mathematical formula}(n2) possible edges, the algorithm has full knowledge about G. It can thus compute offline, i.e., without any further function evaluations, a clique of maximal size. This can be done, for example, by a brute force algorithm. The algorithm then terminates by asking in the {a mathematical formula}((n2)+1)st iteration the search point that corresponds to the computed optimal solution. Clearly, it is not known how or if the offline computation can be done in polynomial (CPU) time.
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Partition
     </section-title>
     <paragraph>
      The fact that optimization versions of {a mathematical formula}NP-hard problems (like the MaxClique problem mentioned in Section 2.3) can be solved efficiently in the unrestricted black-box model is one of the main criticism received by it.
     </paragraph>
     <paragraph>
      In this section we prove that in the unbiased black-box model, too, there are {a mathematical formula}NP-hard problems whose optimization version have a small polynomial black-box complexity. We consider the Partition problem, which is a well known, and probably one of the most famous, {a mathematical formula}NP-hard problem, cf. [22], [18]. Given a multiset {a mathematical formula}I of positive integers (“weights”), the decision version of Partition asks whether or not it is possible to split the set into two disjoint subsets {a mathematical formula}I=I0∪˙I1 such that {a mathematical formula}∑w∈I0w=∑w∈I1w. The corresponding optimization variant of Partition asks to find a partition {a mathematical formula}(I0,I1) of {a mathematical formula}I such that the difference {a mathematical formula}|∑w∈I0w−∑w∈I1w| is minimized. To be more precise, we consider here an {a mathematical formula}NP-hard subclass of Partition, which will be described below.
     </paragraph>
     <paragraph>
      Partition is also one of the few {a mathematical formula}NP-hard problems for which theoretical investigations of randomized search heuristics exist. In fact, it is known that Partition permits heuristics which solve many instances of the problem in a polynomial number of function evaluations. For example, Frenk and Kan [17] showed that the greedy approach converges to optimality almost surely for reasonably chosen random instances. Furthermore, greedy approaches are known to deliver in polynomial number of queries solutions of good approximation quality. Witt [28] has shown that both the Randomized Local Search algorithm and the {a mathematical formula}(1+1) EA need at most {a mathematical formula}O(n2) iterations until they reach for the first time a solution of approximation quality 4/3. More results for the performance of greedy heuristics on Partition can be found in [6] and [28].
     </paragraph>
     <paragraph>
      Despite these positive results on the performance of greedy strategies on random instances and their approximation quality, Partition is an {a mathematical formula}NP-hard problem, and it is thus widely believed that there is no algorithm solving it in polynomial time. As we shall demonstrate below, we show that the difficulty of Partition is not sufficiently captured by the unary unbiased black-box model. In fact, it is possible to solve Partition using only a small polynomial number of queries to the problem instance.
     </paragraph>
     <paragraph>
      As mentioned, we consider an {a mathematical formula}NP-hard subclass of Partition. In fact, it is not difficult to see that Partition remains {a mathematical formula}NP-hard if we restrict the problem to instances with pairwise different weights. The proof is routine, and we state it here only for the sake of completeness.
     </paragraph>
     <paragraph label="Lemma 3">
      FolklorePartitionremains{a mathematical formula}NP-hard when restricted to instances{a mathematical formula}Iwith{a mathematical formula}v≠wfor all{a mathematical formula}v,w∈I.
     </paragraph>
     <paragraph label="Proof">
      Let {a mathematical formula}I be an instance of the general problem class Partition (i.e., {a mathematical formula}I may contain multiple instances of the same integer). We create a new instance {a mathematical formula}I′ from {a mathematical formula}I by the procedure described below. {a mathematical formula}I′ will have pairwise different weights only and we will prove that an optimal partition for {a mathematical formula}I′ immediately yields an optimal partition for the original input {a mathematical formula}I.To construct {a mathematical formula}I′, we first (arbitrarily) enumerate the values in {a mathematical formula}I by φ, i.e., {a mathematical formula}φ:I→[n] is a bijection. We then set {a mathematical formula}c:=n3 and we let {a mathematical formula}I′:={cw+φ(w)|w∈I}∪[n]. By construction, all integers in {a mathematical formula}I′ do have different weights.Let {a mathematical formula}(I0′,I1′) be an optimal partition for {a mathematical formula}I′. We set {a mathematical formula}I0:={(w−φ(w))/c|w∈I0′\[n]} and {a mathematical formula}I1:={(w−φ(w))/c|w∈I1′\[n]}. We use contraposition to show that {a mathematical formula}(I0,I1) is an optimal solution for {a mathematical formula}I. More precisely, we show that any partition of {a mathematical formula}I which is better than {a mathematical formula}(I0,I1) gives rise to a partition of {a mathematical formula}I′ which is better than {a mathematical formula}(I0′,I1′), contradicting the choice of {a mathematical formula}(I0′,I1′). Thus, if {a mathematical formula}(I0′,I1′) is optimal, so is {a mathematical formula}(I0,I1).To prove the claim, assume that there exists a solution {a mathematical formula}(O0,O1) for {a mathematical formula}I with {a mathematical formula}|∑w∈O0w−∑w∈O1w|&lt;|∑w∈I0w−∑w∈I1w|. We set {a mathematical formula}O0′:={c⋅w+φ(w)|w∈O0}∪{φ(w)|w∈O1} and {a mathematical formula}O1′:={c⋅w+φ(w)|w∈O1}∪{φ(w)|w∈O0}. Note that {a mathematical formula}c(∑w∈O0w−∑w∈O1w)=∑w∈O0′w−∑w∈O1′w.We assume without loss of generality that {a mathematical formula}∑w∈I0w−∑w∈I1w&gt;0. Then {a mathematical formula}∑w∈I0′w−∑w∈I1′w&gt;0 since {a mathematical formula}c(∑w∈I0w−∑w∈I1w)&gt;0, {a mathematical formula}∑w∈I0φ(w)−∑w∈I1φ(w)+∑w∈I0′∩[n]w−∑w∈I1′∩[n]w&gt;∑w∈I0φ(w)−∑w∈I1φ(w)−∑i∈[n]i&gt;−(n2+n)&gt;−c (by definition of c and assuming {a mathematical formula}n≥2) and, thus,{a mathematical formula} Similarly we obtain{a mathematical formula} contradicting the optimality of {a mathematical formula}(I0′,I1′) for {a mathematical formula}I′ (which implies {a mathematical formula}|∑w∈I0′w−∑w∈I1′w|≤|∑w∈O0′w−∑w∈O1′w|).  □
     </paragraph>
     <paragraph>
      In the following, let {a mathematical formula}Partition≠ be the subclass of Partition instances with pairwise different weights.
     </paragraph>
     <paragraph>
      There is no one best way of how to consider {a mathematical formula}Partition≠ as an optimization problem. For two different models of {a mathematical formula}Partition≠, different with respect to the objective function, we show (Theorem 4, Theorem 6) that the unary unbiased black-box complexity is {a mathematical formula}O(nlog⁡n). That is, we give a unary unbiased black-box algorithm which solves any instance of {a mathematical formula}Partition≠ in {a mathematical formula}O(nlog⁡n) queries.
     </paragraph>
     <paragraph>
      In Section 3.1 we consider a signed objective function where we learn from the function values which of the sums {a mathematical formula}∑w∈I0w and {a mathematical formula}∑w∈I1w is the larger one.
     </paragraph>
     <paragraph>
      In Section 3.2 we then consider an unsigned objective function. This gives us, a priori, less information than the signed case. However, we are still able to prove the same asymptotic bound. This second objective function is probably the more natural one but note that the key arguments for our upper bound are essentially the same.
     </paragraph>
     <section label="3.1">
      <section-title>
       The signed objective function
      </section-title>
      <paragraph>
       As mentioned, we first consider a signed objective function for {a mathematical formula}Partition≠. To this end, we define the sets {a mathematical formula}FI:={(I0,I1)∈2I×2I|I0∪˙I1=I} of feasible solutions for instance {a mathematical formula}I. The signed objective function to measures the quality of the queried solutions is defined as{a mathematical formula} Note that we aim at minimizing the absolute value {a mathematical formula}|fI⁎|.
      </paragraph>
      <paragraph>
       Next we describe how we model Partition as a pseudo-Boolean problem. We fix as enumeration {a mathematical formula}σ:I→[n] the ordering of the elements in {a mathematical formula}I, i.e., {a mathematical formula}σ(v)&lt;σ(w) for all {a mathematical formula}v,w∈I with {a mathematical formula}v&lt;w. (All arguments that follow below could be easily carried out using an arbitrary enumeration, but for the sake of readability we restrict ourselves to considering as σ the natural ordering.) For any {a mathematical formula}x∈{0,1}n let {a mathematical formula}I0(x):={w∈I|xσ(w)=0} and, accordingly, {a mathematical formula}I1(x):={w∈I|xσ(w)=1}. Note that {a mathematical formula}{0,1}n→FI,x↦(I0(x),I1(x)) is a bijection between {a mathematical formula}{0,1}n and the original search space {a mathematical formula}FI. It is therefore natural to consider as objective function the function {a mathematical formula}fI which is defined by{a mathematical formula}
      </paragraph>
      <paragraph label="Theorem 4">
       The unary unbiased black-box complexity of{a mathematical formula}Partition≠modeled via the signed objective functions{a mathematical formula}fIis{a mathematical formula}O(nlog⁡n), where{a mathematical formula}n:=|I|denotes the size of the input set{a mathematical formula}I.
      </paragraph>
      <paragraph label="Remark 5">
       Interestingly, the algorithm certifying Theorem 4 requires only two different variation operators, namely {a mathematical formula}uniform(), which samples a bit string {a mathematical formula}x∈{0,1}n uniformly at random and {a mathematical formula}RLS(⋅) (randomized local search) which, given some {a mathematical formula}x∈{0,1}n, creates from x a new bit string {a mathematical formula}y∈{0,1}n by flipping exactly one bit in x, the bit position being chosen uniformly at random. The following is straightforward to verify from the definition of unbiased variation operators. {a mathematical formula}uniform() is a (0-ary) unbiased variation operator. {a mathematical formula}RLS(⋅) is a unary unbiased variation operator.
      </paragraph>
      <paragraph label="Proof of Theorem 4">
       We show that Algorithm 3 is an unbiased algorithm that, for any instance {a mathematical formula}I of {a mathematical formula}Partition≠, needs an expected {a mathematical formula}O(|I|log⁡|I|) function evaluations to compute a partition {a mathematical formula}(O0,O1)∈2I×2I such that {a mathematical formula}|∑w∈O0w−∑w∈O1w| is minimized. Since it only employs the two variation operators {a mathematical formula}uniform() and {a mathematical formula}RLS(⋅), it is an unbiased black-box algorithm of arity one.Fix a problem instance {a mathematical formula}I of {a mathematical formula}Partition≠, let {a mathematical formula}n:=|I| denote its size, and abbreviate {a mathematical formula}f:=fI.The algorithm works as follows. It starts with a uniformly sampled solution {a mathematical formula}x(0). In a first phase, the algorithm learns all n different weights of the problem instance. To this end, it samples from {a mathematical formula}x(0) new search points {a mathematical formula}x(t) which differ from {a mathematical formula}x(0) in exactly one position. The steps in lines 9 to 11 are needed to remember which weights are in the same equivalence class of the partition induced by {a mathematical formula}x(0). Note that they do not require any additional queries.After an expected number of {a mathematical formula}(1+o(1))nln⁡n iterations, we have learned the weights of the problem instance as follows. First note that in the tth iteration of the algorithm, the weight of the flipped bit is {a mathematical formula}|f(x(0))−f(x(t))|/2. Therefore, let {a mathematical formula}Wt:={|f(x(0))−f(x(s))|/2|s∈[t]}. By a coupon collector argument (cf., e.g., Theorem 1.21 in [4]) the expected number of queries until we have flipped each bit position of {a mathematical formula}x(0) at least once is {a mathematical formula}(1+o(1))nln⁡n. Thus, we can expect that we need {a mathematical formula}t⁎∈(1+o(1))nln⁡n queries until {a mathematical formula}Wt⁎=I. That is, we can assume to have learned all n different weights in {a mathematical formula}I in {a mathematical formula}(1+o(1))nln⁡n queries.Knowing the problem instance {a mathematical formula}I we can compute an optimal partition {a mathematical formula}(O0,O1) for {a mathematical formula}Ioffline, i.e., we do not need to query any further search points for this step. The computation can be done, e.g., by applying the brute force algorithm which compares all {a mathematical formula}2n possible solutions. All we need to do now is to create a representation of {a mathematical formula}(O0,O1) via unbiased variation operators of arity at most 1.To this end let us define {a mathematical formula}I0′(x(0)):={|f(x(0))−f(x(s))|/2|s∈[t⁎],f(x(0))&gt;f(x(s))} and, accordingly, {a mathematical formula}I1′(x(0)):={|f(x(0))−f(x(s))|/2|s∈[t⁎],f(x(0))&lt;f(x(s))}. It is easily verified that {a mathematical formula}x(0) is a binary representation of the partition {a mathematical formula}(I0′(x(0)),I1′(x(0))).To create {a mathematical formula}(O0,O1) we set {a mathematical formula}M:={w∈O0|w∉I0′(x(0))}∪{w∈O1|w∉I1′(x(0))}, the set of all weights that, in order to generate the optimal solution {a mathematical formula}(O0,O1), need to be moved from one of the sets {a mathematical formula}I0′(x(0)),I1′(x(0)) to the other one.In the optimization phase we do the following. In each iteration we create a new solution y from the current solution z by flipping exactly one bit of z. If {a mathematical formula}w:=|f(y)−f(z)|/2∈M, we update {a mathematical formula}z←y and {a mathematical formula}M←M\{w}.By the same coupon collector argument as above we can expect that after {a mathematical formula}(1+o(1))nln⁡n such one bit flips we have flipped each bit position {a mathematical formula}i∈[n] at least once. That is, after an expected number of {a mathematical formula}(1+o(1))nln⁡n queries, we have {a mathematical formula}M=∅ and we have thus created {a mathematical formula}(O0,O1).This shows how to optimize an arbitrary instance {a mathematical formula}I of the {a mathematical formula}Partition≠ problem modeled via the signed objective function in an expected number of {a mathematical formula}2(1+o(1))nln⁡n=O(nlog⁡n) queries by an unbiased algorithm of arity one.  □
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       The unsigned objective function
      </section-title>
      <paragraph>
       One might dislike the fact that in the proof of Theorem 4 we neither minimize nor maximize {a mathematical formula}fI itself but only its absolute value {a mathematical formula}|fI|. However, we can achieve the same asymptotic optimization complexity as in the statement of Theorem 4 if we only allow the latter, unsigned objective function. Although the algorithm itself does not become more difficult to define, the proof of correctness is more technical. The difficulty for the analysis stems from the fact that, given two bit strings x and y which differ in only one bit position, we cannot unambiguously learn from the corresponding function values {a mathematical formula}|fI(x)| and {a mathematical formula}|fI(y)| the weight of the flipped bit, cf. Remark 7. This results in a slightly more complex procedure to learn the different weights.
      </paragraph>
      <paragraph label="Theorem 6">
       The unary unbiased black-box complexity{a mathematical formula}Partition≠with respect to{a mathematical formula}|fI|is{a mathematical formula}O(nlog⁡n), where{a mathematical formula}n:=|I|denotes the size of the input set{a mathematical formula}I.
      </paragraph>
      <paragraph>
       For a clearer presentation of the proof, we defer some technical elements used in the proof of Theorem 6 to lemmas that will be presented after the proof of the main theorem.
      </paragraph>
      <paragraph label="Proof of Theorem 6">
       By Remark 2 it suffices to show that there exists an algorithm that, for an arbitrary instance {a mathematical formula}I of {a mathematical formula}Partition≠, has at least constant success probability to compute an optimal search point within {a mathematical formula}O(|I|log⁡|I|) function evaluations. We present a unary unbiased algorithm, Algorithm 4 that does so even with high probability (w.h.p.), that is, with probability {a mathematical formula}1−o(1).For the remainder of the proof, we fix again an instance {a mathematical formula}I of {a mathematical formula}Partition≠, let {a mathematical formula}n:=|I|, and we abbreviate {a mathematical formula}f:=|fI|, where {a mathematical formula}fI is defined as in Section 3.1.For readability purposes we introduce the following notation, which—this is important to note—are a priori not identifiable for the algorithm. Using the notation from Section 3.1, each {a mathematical formula}x∈{0,1}n clearly corresponds to a partition {a mathematical formula}(I0(x),I1(x))∈FI. We set {a mathematical formula}S0(x):=∑w∈I0(x)w and {a mathematical formula}S1(x):=∑w∈I1(x)w, the corresponding sums of the weights in the equivalence classes of that partition. Let {a mathematical formula}Imax(x):=I0(x) if {a mathematical formula}S0(x)≥S1(x) and let {a mathematical formula}Imax(x)=I1(x) otherwise. We call {a mathematical formula}Imax(x) the “heavier” bin and we call the other one the “lighter” bin. Lastly, let {a mathematical formula}wmax=max⁡I be the largest weight appearing in instance {a mathematical formula}I.The general approach of Algorithm 4 is the following. First we produce a string which represents a solution where all weights are in the same class of the partition, i.e., at the end of this phase we have {a mathematical formula}Imax(x)=I. With high probability this can be achieved with {a mathematical formula}4nln⁡n queries. Next, we perform {a mathematical formula}2nln⁡nRLS steps (i.e., random one-bit flips). Through this we learn all n different weights in {a mathematical formula}I w.h.p. After that, we compute an optimal solution offline. A representation of this solution can be generated in another {a mathematical formula}3nln⁡n iterations w.h.p.If in any iteration of Algorithm 4 we have constructed a solution s with {a mathematical formula}f(s)=0 we are obviously done and do not need to run the algorithm any further. Therefore, we assume in the following that for all search points s but possibly the last one we have {a mathematical formula}f(s)≠0.Like the algorithm that we described in the previous subsection, Algorithm 4 employs only two different variation operators, {a mathematical formula}uniform() and {a mathematical formula}RLS(⋅), which (cf. Remark 5) are unbiased and of arity 0 and 1, respectively. It remains to show that w.h.p. Algorithm 4 queries an optimal solution after {a mathematical formula}O(nlog⁡n) queries. We show correctness for the three phases. The high probability statement follows from a simple union bound over the failure probabilities.Phase 1: Shifting all weights to the same bin. We query in the first step of this first phase {a mathematical formula}2nln⁡n random bit strings {a mathematical formula}x(1,ti) that differ from the initial uniform solution {a mathematical formula}x(1,0) in exactly one bit. By the coupon collector argument, with probability at least {a mathematical formula}1−n−1, there exists for each {a mathematical formula}i∈[n] at least one {a mathematical formula}ti≤2nln⁡n such that {a mathematical formula}x(1,0) and {a mathematical formula}x(1,ti) differ exactly in the ith bit. Lemma 8, which will be presented below, shows that for each string {a mathematical formula}x(1,ℓ)∈{x(1,t)|t∈[0..2nln⁡n]} of maximal function value {a mathematical formula}f(x(1,ℓ))=max⁡{f(x(1,t))|t∈[0..2nln⁡n]} it holds that {a mathematical formula}wmax∈Imax(x(1,ℓ)). In lines 6 and 7 of Algorithm 4 we fix one such ℓ and set {a mathematical formula}x:=x(1,ℓ).Lemma 8 and Lemma 9 verify the following. If y is created from x by flipping the ith bit of x, then {a mathematical formula}f(y)&gt;f(x) if and only if the ith heaviest weight is not in the heavier bin, i.e., {a mathematical formula}σ(i)∉Imax(x).In the second step of the first phase we aim at creating a string {a mathematical formula}x′ with {a mathematical formula}Imax(x′)=I. We do that by querying {a mathematical formula}y=RLS(x) and updating {a mathematical formula}x←y if and only if {a mathematical formula}f(y)&gt;f(x). From the statement of the previous paragraph this is the case only if the bit flip has moved the corresponding weight from the lighter to the heavier bin. Again from the coupon collector argument it follows that after an additional {a mathematical formula}2nln⁡n iterations we have {a mathematical formula}Imax(x)=I, with probability at least {a mathematical formula}1−n−1.Hence, after a total number of {a mathematical formula}4nln⁡n+1 iterations, we have created a bit string x with {a mathematical formula}Imax(x)=I, with probability at least {a mathematical formula}1−2n−1.Phase 2: Learning instance{a mathematical formula}I. As in the previous subsection we learn the different weights by performing random one-bit flips. Since the objective function reveals only unsigned function values, we need to argue how the weights of the elements that have been shifted can be derived from the function values. We distinguish two cases. Either we have {a mathematical formula}wmax≥∑w∈Iw/2, in which case, by the coupon collector argument, one of the sampled strings {a mathematical formula}x(2,t), {a mathematical formula}t∈[2nln⁡n] is optimal (i.e., {a mathematical formula}{wmax}=Imax(x(2,t)) for some {a mathematical formula}t∈[2nln⁡n]) with probability at least {a mathematical formula}1−n−1. In this case we are immediately done. Otherwise we have that for any {a mathematical formula}t≤2nln⁡n it holds that {a mathematical formula}f(x(2,t))&lt;f(x) (since we are always shifting exactly one weight from the bin containing all weights to the empty one) and that the corresponding weight which has been flipped from one bin to the other is of weight {a mathematical formula}(f(x)−f(x(2,t)))/2. In this case we have, again by the coupon collector argument, that {a mathematical formula}I′:={(f(x)−f(x(2,t)))/2|t∈[2nln⁡n]} equals {a mathematical formula}I, with probability at least {a mathematical formula}1−n−1.Phase 3: Creating the optimal solution. Knowing instance {a mathematical formula}I, the algorithm computes an optimal solution {a mathematical formula}(O0,O1) for {a mathematical formula}I offline, e.g., by the brute force algorithm. Note that for each {a mathematical formula}y∈{0,1}n and its bitwise complement {a mathematical formula}y¯ it holds that {a mathematical formula}f(y¯)=f(y). Thus, we can assume without loss of generality that {a mathematical formula}(O0,O1) is chosen such that {a mathematical formula}wmax∈O1.For creating the bit string which corresponds to {a mathematical formula}(O0,O1), we initialize {a mathematical formula}M:=O1. Throughout this phase {a mathematical formula}M denotes the set of all weights that, in order to create the string corresponding to {a mathematical formula}(O0,O1), still need to be “moved” from one bin to the other. The key idea here is that we required {a mathematical formula}wmax∈M and that we do not accept the weight {a mathematical formula}wmax to be flipped too early. This is important for the following reason.Recall from Remark 7 that if y is created from x by flipping the ith bit in x and if {a mathematical formula}f(y)&lt;f(x) then the corresponding weight {a mathematical formula}σ−1(i)∈{((f(x)−f(y))/2),(f(x)+f(y))/2}. But, as long as {a mathematical formula}f(x)&gt;2wmax we have {a mathematical formula}(f(x)+f(y))/2&gt;wmax (unless {a mathematical formula}f(y)=0 in which case we are done). That is, as long as {a mathematical formula}f(x)&gt;2wmax it holds in the situation above that {a mathematical formula}σ−1(i)=(f(x)−f(y))/2.It is easy to verify that as soon as {a mathematical formula}f(x)≤2wmax we have {a mathematical formula}M={wmax}. It again is the coupon collector argument which ensures with probability at least {a mathematical formula}1−n−1 that after {a mathematical formula}2nln⁡n iterations of the third phase we are in this situation. Thus, all we need to do now is to put {a mathematical formula}wmax from bin {a mathematical formula}Imax(x) to the other one, i.e., we need to flip {a mathematical formula}σ(wmax). As for each iteration the probability to flip this position is {a mathematical formula}1/n, we can bound the probability that we have flipped it after an additional {a mathematical formula}nln⁡n iterations from below by {a mathematical formula}1−(1−1/n)nln⁡n≥1−1/n. Here we have used that for all {a mathematical formula}r∈R we have {a mathematical formula}1+r≤exp⁡(r).  □
      </paragraph>
      <paragraph>
       Let us now prove the statements omitted in the proof of Theorem 6. We use the same notation as above.
      </paragraph>
      <paragraph label="Remark 7">
       Let {a mathematical formula}I be an instance of {a mathematical formula}Partition≠ equipped with the ordering {a mathematical formula}σI and objective function {a mathematical formula}f=|fI|. If y has been created from x by flipping the ith bit of x and {a mathematical formula}0≠f(y)≠f(x)≠0, we cannot uniquely identify the corresponding weight {a mathematical formula}wi=σ−1(i). More precisely, if we do not have further knowledge on the size of the weights, there are the two possibilities{a mathematical formula}
      </paragraph>
      <paragraph label="Proof">
       This enumerates all possible combinations and the claim follows.  □
      </paragraph>
      <paragraph label="Lemma 8">
       Let{a mathematical formula}Ibe an instance of{a mathematical formula}Partition≠, let{a mathematical formula}σ=σIbe its ordering, and let{a mathematical formula}f=|fI|. Furthermore, let{a mathematical formula}x(0)∈{0,1}nand for each{a mathematical formula}i∈[n]let{a mathematical formula}x(i)be created from{a mathematical formula}x(0)by flipping the ith bit.If we choose{a mathematical formula}ℓ∈[0..n]such that{a mathematical formula}f(x(ℓ))=max⁡{f(x(t))|t∈[0..n]}, then{a mathematical formula}wmax∈Imax(x(ℓ)).
      </paragraph>
      <paragraph label="Proof">
       We assume that {a mathematical formula}wmax∉Imax(x(ℓ)) to show the contrapositive. If {a mathematical formula}ℓ=0, we can flip the bit corresponding to {a mathematical formula}wmax (by our assumption on σ this is the nth bit) in {a mathematical formula}x(0) to get {a mathematical formula}f(x(n))=f(x(0))+2wmax&gt;f(x(0)). Similarly, if {a mathematical formula}ℓ=n then {a mathematical formula}f(x(0))=f(x(n))+2wmax. All other values of ℓ imply {a mathematical formula}wmax∉Imax(x(0)) (by the assumption that {a mathematical formula}wmax∉Imax(x(ℓ))) and thus, {a mathematical formula}f(x(n))−f(x(0))=2wmax. But since the weights are pairwise different, {a mathematical formula}σ−1(ℓ)&lt;wmax and thus {a mathematical formula}f(x(ℓ))−f(x(0))&lt;2wmax.  □
      </paragraph>
      <paragraph label="Lemma 9">
       The previous lemma has shown that the largest weight {a mathematical formula}wmax is in the larger of the two bins of {a mathematical formula}x(ℓ). The following lemma shows that if we have iteratively increased the value of {a mathematical formula}x(ℓ) through 1-bit flips, we only have shifted weights from the smaller bin to the larger one. Let{a mathematical formula}Ibe an instance of{a mathematical formula}Partition≠, let{a mathematical formula}σ:=σIbe the ordering of{a mathematical formula}I, and{a mathematical formula}f:=|fI|.(i) If{a mathematical formula}x∈{0,1}nwith{a mathematical formula}f(x)≥wmax, then for all{a mathematical formula}i∈[n]we have{a mathematical formula}f(x⊕ei)&gt;f(x)if and only if{a mathematical formula}wi:=σ−1(i)∉Imax(xℓ).(ii) For{a mathematical formula}x(0),…,x(n)and ℓ as inLemma 8we have{a mathematical formula}f(x(ℓ))≥wmax.
      </paragraph>
      <paragraph label="Proof">
       (i). Let {a mathematical formula}x∈{0,1}n with {a mathematical formula}f(x)≥wmax and let {a mathematical formula}i∈[n]. Clearly, if {a mathematical formula}wi∉Imax(x) then {a mathematical formula}f(x⊕ei)=f(x)+2wi&gt;f(x). On the other hand, if {a mathematical formula}wi∈Imax(x) then either {a mathematical formula}f(x⊕ei)=f(x)−2wi&lt;f(x) or {a mathematical formula}f(x⊕ei)=2wi−f(x)≤2wi−wmax≤wmax≤f(x).(ii). If {a mathematical formula}wmax∉Imax(x(0)), then {a mathematical formula}ℓ=n since for all {a mathematical formula}i∈[n] we have{a mathematical formula} The above calculation immediately yields {a mathematical formula}f(x(ℓ))&gt;wmax.Therefore, we may thus assume that {a mathematical formula}wmax∈Imax(x(0)). To show the contrapositive, let us also assume that {a mathematical formula}f(x(ℓ))&lt;wmax. Then {a mathematical formula}f(x(0))≤f(x(ℓ))&lt;wmax and thus {a mathematical formula}f(x(n))=2wmax−f(x(0))&gt;wmax&gt;f(x(ℓ)), contradicting the choice of ℓ. Thus, {a mathematical formula}f(x(ℓ))≥wmax.  □
      </paragraph>
      <paragraph>
       It is not difficult to see that already with 3-ary variation operations it is possible to access every bit position in a linear number of iterations. Hence, a small modification of Algorithm 3 solves {a mathematical formula}Partition≠ and even Partition in a linear number of steps, using only unbiased variation operators of arity at most 3. This implies, in particular, that the unrestricted black-box complexity of Partition is linear in {a mathematical formula}|I|. The latter can be seen alternatively by the fact that we can learn the weights by querying first the all-zeros bit string and then the n different unit vectors {a mathematical formula}(0,…,0,1,0,…,0).
      </paragraph>
      <paragraph label="Remark 10">
       The 3-ary unbiased black-box complexity of Partition is at most linear in the size {a mathematical formula}|I| of the input set {a mathematical formula}I.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Conclusions
     </section-title>
     <paragraph>
      We have shown that the unbiased black-box model allows for algorithms which solve the optimization version of the {a mathematical formula}NP-hard partition problem in a polynomial number of queries, even if the arity of the algorithms is restricted to one. Our result indicates that the unbiased black-box model, while clearly closer to the truth than the unrestricted one, still does not provide a complete picture on how difficult it is to solve a given problem via randomized search heuristics. It seems that further restrictions to the power of the algorithms are needed to obtain meaningful results. A recent step into this direction is the work by the first two authors [14], who, following a suggestion by Nikolaus Hansen, investigate a black-box model where the algorithm can only compare the quality of solutions, but has no access to the absolute function values. We do not know yet the black-box complexity of, e.g., Partition in this new model.
     </paragraph>
     <paragraph>
      We should note, though, that observing smaller-than-expected black-box complexities does not always reveal a weakness of the black-box complexity model regarded. In [7], the authors exhibit the first {a mathematical formula}O(nlog⁡n) crossover-based (unbiased) evolutionary algorithm for the OneMax function class, which shows that the {a mathematical formula}O(nlog⁡n) bound for the 2-ary unbiased black-box complexity of OneMax found in [8] is not as unnatural as it might have seemed at first.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>