<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    POMDP-based control of workflows for crowdsourcing.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      The ready availability of the Internet across the world has had far-reaching consequences. Not only has widespread online connectivity revolutionized communication, politics, entertainment, and other aspects of everyday life, it has enabled the ability to easily unite large groups of people around the world (a crowd) for a common purpose. This ability to crowdsource has in turn opened up radical new possibilities and resulted in novel successes, like Wikipedia{sup:1} – a whole encyclopedia written by the crowd, and platforms that are rapidly impacting the worldʼs laborforce by bringing them together in online marketplaces that provide work on-demand.
     </paragraph>
     <paragraph>
      We believe that Crowdsourcing, “the act of taking tasks traditionally performed by an employee or contractor, and outsourcing them to a group (crowd) of people or community in the form of an open call,”{sup:2} has the potential to revolutionize information-processing services by coupling human workers with intelligent machines in productive workflows [21].
     </paragraph>
     <paragraph>
      While the word “crowdsourcing” was only coined in 2006, the area has grown rapidly in economic significance with the emergence of general-purpose platforms such as Amazonʼs Mechanical Turk,{sup:3} task-specific sites for call centers,{sup:4} programming jobs{sup:5} and more [37], [4], [7], [33], [24].
     </paragraph>
     <paragraph>
      Crowdsourced workers are motivated by a variety of incentives. Common incentives include entertainment, e.g., in the context of playing games with a purpose [63], [10], contribution to science,{sup:6} a sense of community, and monetary rewards. While our work is applicable to several crowdsourcing scenarios, we focus on financially motivated micro-crowdsourcing – crowdsourcing of small jobs in exchange for monetary payments. Labor markets, like oDesk, handle medium and large-sized tasks requiring a diverse range of skills and micro-crowdsourcing has become very popular with requesters, who use a concert of small-sized jobs to handle a wide variety of higher-level tasks, such as audio transcription, language translation, calorie counting [42], and helping blind people navigate unknown surroundings [7]. It has also been immensely popular with workers in both developed and developing countries [49].
     </paragraph>
     <paragraph>
      On popular micro-crowdsourcing platforms, like Mechanical Turk, requesters often use workflows, a series of steps, to complete tasks. For instance, for a simple image classification task (e.g. “Is there a lion in this picture?”), a workflow as simple as asking several workers the same binary-choice question will suffice. For a more difficult task like the handwriting recognition task shown in Fig. 1, a requester might create a more complicated workflow like iterative improvement, in which workers incrementally refine each othersʼ solutions until no further improvement is necessary [37]. A plethora of research shows that for simple binary classification workflows, micro-crowdsourcing can achieve high-quality results [58], [60]. Similarly, iterative improvement, find–fix–verify [3] and other workflows have been shown to yield excellent output even when individual workers err.
     </paragraph>
     <paragraph>
      But the use of these workflows raises many important questions. For example, when designing a workflow to handle problems like the handwriting recognition task shown in Fig. 1, we do not know answers to questions like: (1) What is the optimal number of iterations for such a task? (2) How many ballots should be used for voting? (3) How do these answers change depending on workersʼ skill levels?
     </paragraph>
     <paragraph>
      Our paper offers answers to these questions by constructing AI agents for workflow optimization and control. We study three distinct crowdsourcing scenarios. We start by considering the problem of dynamically controlling the aggregation of the simplest possible workflow: binary classification. Next, we extend our model to control the significantly more complex iterative improvement workflow. Finally, we show how our model can be used to dynamically switch between alternative workflows for a given task.
     </paragraph>
     <paragraph>
      We use a shared toolbox of techniques on each of these crowdsourcing scenarios; there are two key components. First, we propose a probabilistic model for worker responses. This model relates worker ability, task difficulty and worker response quality by means of a Bayesian network. Second, we view the problem of workflow control as a problem of decision-theoretic planning and execution, and cast it as a Partially-Observable Markov Decision Process (POMDP) [6]. The Bayes net provides the model for the POMDP, and the POMDP policy controls the workflow to obtain a high-quality output in a cost-efficient manner.
     </paragraph>
     <paragraph>
      We make several important contributions. First, we provide a principled solution to dynamically decide the number of votes for a task based on the exact history of workers who worked on a task instance, their answers and a notion of varying problem difficulty. We also provide a framework for answering more complex questions in larger workflows (e.g. the number of iterations in an iterative improvement workflow). The commonality of approaches in these diverse tasks suggests an underlying abstraction, which may be exploited for optimizing and controlling many different kinds of workflows. Our experiments consistently outperform best known baselines by large margins, e.g., obtaining 50% error reduction in the scenario of multiple workflows, or 30% cost savings for iterative improvement.
     </paragraph>
     <paragraph>
      Moreover, our work reveals some surprising discoveries. First, for the iterative improvement scenario, our AI agent proposes a rather unexpected voting policy, which was not predicted by any human expert (see Section 4). Second, we demonstrate that judiciously using alternative workflows for a task is much better than using a single “best” workflow. While the base idea is not novel, the fact that we can do this dynamic switching between workflows automatically to obtain high-quality results is a surprising discovery.
     </paragraph>
     <paragraph>
      The rest of the paper is structured as follows. Section 2 formally defines Markov Decision Processes and Partially-Observable Markov Decision Processes (POMDP). Then we review and propose planning algorithms for solving POMDPs. Section 3 details how we model and control the aggregation of a simple binary classification workflow using our first agent, TurKontrol0. Section 4 extends our model and agent to iterative improvement workflows to create our second agent, TurKontrol. We present some simulation-based investigation of the performance of TurKontrol, illustrate how to learn the model parameters, and finally demonstrate the usefulness of TurKontrol on Mechanical Turk, with real tasks. Section 5 details how we model the availability of multiple workflows and the design of our third agent, AgentHunt, which dynamically selects the next best workflow to use. We illustrate how to learn model parameters and then demonstrate the usefulness of AgentHunt in simulation and on Mechanical Turk. Finally, we present related work, propose future work, and make conclusions. We note software packages of our implementations are available for general use at http://cs.washington.edu/node/7714.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background
     </section-title>
     <section label="2.1">
      <section-title>
       Markov decision processes
      </section-title>
      <paragraph>
       AI researchers typically use Markov Decision Processes (MDPs) to formulate fully-observable decision making under uncertainty problems [40].
      </paragraph>
      <paragraph label="Definition 1">
       An MDP is a four-tuple {a mathematical formula}〈S,A,T,R〉, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}S is a finite set of discrete states.
       </list-item>
       <list-item label="•">
        {a mathematical formula}A is a finite set of all actions.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T:S×A×S→[0,1] is the transition function describing the probability that taking an action in a given state will result in another state.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R:S×A→R is the reward for taking an action in a state.
       </list-item>
      </list>
      <paragraph>
       An agent executes its actions in discrete time steps starting from some initial state. At each step, the system is at one distinct state {a mathematical formula}s∈S. The agent can execute any action a from a set of actions{a mathematical formula}A, and receives a reward {a mathematical formula}R(s,a). The action takes the system to a new state {a mathematical formula}s′ stochastically. The transition process exhibits the Markov property, i.e., the new state {a mathematical formula}s′ is independent of all previous states given the current state s. The transition probability is defined by {a mathematical formula}Ta(s′|s). The model assumes full observability, i.e., after executing an action and transitioning stochastically to a next state as governed by {a mathematical formula}T, the agent has full knowledge of the state.
      </paragraph>
      <paragraph>
       Any solution to an MDP problem is in the form of a policy.
      </paragraph>
      <paragraph label="Definition 2">
       A policy, {a mathematical formula}π:S→A, of an MDP is a mapping from the state space to the action space.
      </paragraph>
      <paragraph>
       A policy is static if the action taken at each state is the same at every time step. {a mathematical formula}π(s) indicates which action to execute when the system is at state s. To solve an MDP we need to find an optimal policy ({a mathematical formula}π⁎:S→A), a probabilistic execution plan that achieves the maximum expected utility, or sum of rewards. We evaluate any policy π by its value function, the set of values that satisfy the following equation:{a mathematical formula}
      </paragraph>
      <paragraph>
       {a mathematical formula}β∈(0,1] is the discount factor, which controls the value of future rewards. Any optimal policyʼs value function must satisfy the following system of Bellman equations:{a mathematical formula} The corresponding optimal policy can be extracted from the value function:{a mathematical formula} Given an implicit optimal policy {a mathematical formula}π⁎ in the form of its optimal value function {a mathematical formula}V⁎(⋅), we measure the superiority of an action by a Q-function: {a mathematical formula}S×A→R.
      </paragraph>
      <paragraph label="Definition 3">
       The {a mathematical formula}Q⁎-value of a state–action pair {a mathematical formula}(s,a) is the value of state s, if an immediate action a is performed, followed by {a mathematical formula}π⁎ afterwards. More concretely,{a mathematical formula} Therefore, the optimal value function can be expressed by:{a mathematical formula}
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Solving an MDP
      </section-title>
      <paragraph>
       Many optimal MDP algorithms are based on dynamic programming. A simple, yet powerful such algorithm, value iteration[2], is also the basis of many heuristic search and approximate algorithms. It first initializes the value function arbitrarily. Then, the values are updated iteratively using an operator called the Bellman backup to create successively better approximations for the value of each state per iteration. The Bellman residual of a state is the absolute difference of a state value before and after a Bellman backup. Value iteration stops when the value function converges. In implementation, it is typically signaled by when the Bellman error, the largest Bellman residual of all states, becomes less than some pre-defined threshold.
      </paragraph>
      <paragraph>
       A simple way to approximate solutions to large MDPs is to use Monte Carlo (MC) simulation algorithms. Such algorithms repeatedly perform simulation trials originating from the initial state. During each simulation, actions are chosen based on some heuristic function h, and the resulting states are chosen stochastically based on the transition probabilities of the action. Different heuristics can be used, depending on the desired tradeoff between exploration and exploitation. For example, a heuristic that purely exploits knowledge of the value function would always pick a greedy action that maximizes the value function. On the other hand, a heuristic that solely explores would always choose a random action. One simulation trial terminates when a terminal state{sup:7} is encountered. At termination, the Q-values of all visited state–action pairs are updated in reverse order.
      </paragraph>
      <paragraph>
       Upper Confidence Bounds Applied on Trees (UCT) [29] is an MC algorithm, whose power has been demonstrated by its application to several challenging problems like Go [18] and Real-time Strategy Games [1]. UCT uses a heuristic that considers both actions that have already been proven valuable and under-explored branches, in case better policies can be found. It remembers the total number of times a state s has been visited, {a mathematical formula}ns, and the number of times action a is picked when s is visited, {a mathematical formula}n(s,a). Its heuristic function, {a mathematical formula}hUCT(s,a), is defined as follows:{a mathematical formula} The second term increases the value of actions that are visited less frequently. The exploration parameter κ is used to balance between exploration and exploitation. Theoretical error bounds given a sufficient number of trials have been proven [29].
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       Partially-Observable Markov Decision Processes
      </section-title>
      <paragraph>
       Partially-Observable MDPs (POMDPs) [25] provide a more flexible framework with which to model decision-making under uncertainty by relaxing the assumption that an agent has complete knowledge of the world and can only make noisy observations about its current state. This generality can model many single-agent, real-world problems, since perfect information about the world is rarely available to an agent.
      </paragraph>
      <paragraph label="Definition 4">
       A POMDP is a six-tuple {a mathematical formula}〈S,A,O,T,P,R〉, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}S is a finite set of discrete states.
       </list-item>
       <list-item label="•">
        {a mathematical formula}A is a finite set of all actions.
       </list-item>
       <list-item label="•">
        {a mathematical formula}O is a finite set of observations.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T:S×A×S→[0,1] is the transition function describing the probability that taking an action in a given state will result in another state.
       </list-item>
       <list-item label="•">
        {a mathematical formula}P:S×O→[0,1] is the observation function describing the probability that taking an action in a given state will result in an observation.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R:S×A→R is the reward for taking an action in a state.
       </list-item>
      </list>
      <paragraph>
       We see that a POMDP extends an MDP by adding a finite set of observations {a mathematical formula}O and a corresponding observation model {a mathematical formula}P. Since the agent is unable to directly observe the worldʼs current state, it maintains a probability distribution over states, called a belief stateb, reflecting its estimate of their likelihood:{a mathematical formula} where {a mathematical formula}b(s) is the probability that the current state is s, inferred from the previous belief state, the most recent action, and the resulting observation.
      </paragraph>
      <paragraph>
       A POMDP can be cast as an MDP by letting the state space of the MDP be the space of all possible belief states. Since the transition between two belief states is completely determined by the action and resulting observation, the resulting model is a fully-observable MDP – but over a vastly larger space. Even when a POMDP is defined over only two world states, {a mathematical formula}s1 and {a mathematical formula}s2, the space of belief states is infinite, since a belief state can be an arbitrary combination of {a mathematical formula}b(s1)=p and {a mathematical formula}b(s2)=1−p where p can be any real number in {a mathematical formula}[0,1].
      </paragraph>
      <paragraph>
       Solving a POMDP is a hard problem. For the simplest case where the set of {a mathematical formula}A, {a mathematical formula}S and {a mathematical formula}O are finite, Sondik [59] proves that the optimal value function for a finite-horizon problem is piecewise linear and convex (PWLC). Therefore, value iteration-type algorithms converge on these problems. However, the number of reachable belief states grows exponentially in the number of observations {a mathematical formula}|O|. The complexity of one update iteration is exponential in the total number of observations [25]. For an infinite-horizon POMDP one needs to perform an infinite number of iterations in the worst case, as the total number of reachable belief states can be infinite. Therefore, the problem is undecidable [38] in the worst case. Porta et al. [45] later proved that finite-horizon continuous state POMDPs are PWLC.
      </paragraph>
      <paragraph>
       Researchers seek various approximation methods to solve POMDPs efficiently. Point-based value iteration [61], [44], [54] is a very successful approach. The basic idea is to approximate the value function of the belief space, {a mathematical formula}B, by only estimating the values of a fixed, sampled subset, {a mathematical formula}B¯⊆B. For all the other belief states, the values are approximately estimated according to the values of the sampled space, e.g., inferred from integration [45] or a mixture of Gaussians representation [8]. For a tractable, finite-horizon problem, the algorithm iteratively computes an approximately optimal t-horizon value function. The PWLC properties of a discrete POMDP guarantee that the error of the value function computed by the point-based algorithms is bounded [45].
      </paragraph>
     </section>
     <section label="2.4">
      <section-title>
       Planning algorithms
      </section-title>
      <paragraph>
       We now present three algorithms that we use in this paper for solving POMDPs.
      </paragraph>
      <section label="2.4.1">
       <section-title>
        Limited lookahead
       </section-title>
       <paragraph>
        Our first decision-making algorithm is an l-step lookahead. The goal is to evaluate all sequences of up to l decisions and find the best sequence. To determine the value of a given sequence of l decisions, we calculate the expected utility from stopping at the lth decision based on the agentʼs current belief. Based on these expected utility estimations, the agent picks the sequence with the best utility, executes the first action of that sequence, and then repeats the process until a terminal state is reached.
       </paragraph>
      </section>
      <section label="2.4.2">
       <section-title>
        A discretized POMDP algorithm
       </section-title>
       <paragraph>
        We also try a discretization-based POMDP algorithm, which we call ADBS, short for approximation and discretization of belief space. The method approximates the belief distribution by the values of its mean and standard deviation. Similar techniques have been used in the robot navigation domain [50]. ADBS discretizes the range of each variable into small, equal-sized intervals. With discretization and known bounds (we show later that we can bound these variables for our application), the space of belief states is finite. ADBS performs a reachability search from the initial state to build an MDP model of the approximated belief space. It then optimally solves the discretized MDP by value iteration.
       </paragraph>
      </section>
      <section label="2.4.3">
       <section-title>
        A UCT variant
       </section-title>
       <paragraph>
        For our third algorithm, we use a variant of UCT applied to a space whose states are a simplified history of actions and observations, which we call condensed histories and define in Section 4.4. Although using condensed histories has the unfortunate property that different histories may represent the same belief state, this approximation technique excels in several ways. It saves computation, since we do not need to maintain posterior beliefs. Maintaining beliefs is tricky for our problems, since the state space is continuous, and a belief over continuous state space is non-trivial to represent and update. For the same reason, implementing history-based POMDPs is much easier. Finally, it makes execution faster too, since one can directly map the history to the action. Monte Carlo simulation-based POMDP algorithms have been proved to be effective on very large problems [56].
       </paragraph>
      </section>
     </section>
    </section>
    <section label="3">
     <section-title>
      Decision-theoretic optimization of a binary classification workflow
     </section-title>
     <paragraph>
      We now begin the study of decision-theoretic control of crowdsourcing. We first consider one of the simplest of workflows, the binary classification, or the binary ballot job. These workflows present workers with a question and ask them to pick the best answer from two choices. For instance, a workflow might show workers a picture and ask them if there is a human face in the picture. Then some method of aggregation is used, like a majority vote, to account for worker variability. Such workflows might be used to collect training data for a computer vision algorithm.
     </paragraph>
     <paragraph>
      The agentʼs control problem is defined as follows. As input the agent is given the task, and the agent is asked to return the correct answer. In pursuit of this goal, the agent is allowed to create jobs on a crowdsourcing platform. We model the agentʼs decision problem as a POMDP.
     </paragraph>
     <paragraph>
      To do this, we must first define a generative model for worker responses.
     </paragraph>
     <section label="3.1">
      <section-title>
       Probabilistic model for a binary classification response
      </section-title>
      <paragraph>
       We assume workers are diligent, so they answer all ballots to the best of their abilities. In other words, we assume workers are not adversarial. Additionally, we also assume that workers do not collaborate.
      </paragraph>
      <paragraph>
       We define a workerʼs accuracy to be{a mathematical formula} where {a mathematical formula}d∈[0,1] is the intrinsic difficulty of the task, which we will motivate shortly, and {a mathematical formula}γx∈[0,∞] is the error parameter of the worker.
      </paragraph>
      <paragraph>
       As a workerʼs error parameter and/or the workflowʼs difficulty increases, a approaches 1/2, suggesting that worker is randomly guessing. On the other hand, as the parameters decrease, a approaches 1, when the worker always produces the correct answer. As γ decreases, the accuracy curve becomes more concave, and thus the expected accuracy increases for a fixed d. Fig. 2 is an illustration of accuracy.
      </paragraph>
      <paragraph>
       The answer {a mathematical formula}bw that worker w with error parameter {a mathematical formula}γw provides is governed by the following equations:{a mathematical formula}{a mathematical formula}
      </paragraph>
      <paragraph>
       Fig. 7 illustrates the plate notation for our generative model, which encodes a Bayes Net for responses made by W workers on T tasks. The correct answer, v, the difficulty parameter, d, and the error parameter, γ, influence the final answer, b, that a worker provides, which is the only observed variable.
      </paragraph>
      <paragraph>
       We note that given our assumption that workers do not collaborate, one might believe that the workersʼ responses {a mathematical formula}bw are independent of each other given the true answer v. However, they are not because of the following subtlety. Even though the different workers are not collaborating, a mistake by one worker changes the error probability of others, because a mistake gives evidence that the question may be intrinsically hard and hence, difficult for others to get it right as well. Thus, as shown in the graphical model, we make the assumption that the worker responses are independent of each other only after we are additionally given the difficulty d.
      </paragraph>
      <paragraph>
       With this generative model, we can define our POMDP.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       The POMDP
      </section-title>
      <paragraph label="Definition 5">
       The POMDP for our simple binary classification workflow is a six-tuple {a mathematical formula}〈S,A,R,T,O,P〉, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}S={(d,v)|d∈[0,1],v∈{0,1}}d is the difficulty of the task and v is the true answer.
       </list-item>
       <list-item label="•">
        {a mathematical formula}A={createanotherjob,submittrue,submitfalse}.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R:S×A→R is specified below.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T:S×A×S→[0,1]=((d,v),a,(d,v))↦1. All other probabilities are 0.
       </list-item>
       <list-item label="•">
        {a mathematical formula}O={true,false} is a Boolean response by a worker.
       </list-item>
       <list-item label="•">
        {a mathematical formula}P:S×O→[0,1] is defined by our generative model.
       </list-item>
      </list>
      <paragraph>
       The reward function maintains the value of submitting a correct answer and the penalty for submitting an incorrect answer. Additionally, it maintains a cost that TurKontrol0 incurs when it creates a job. We can modify the reward function to match our desired budgets and accuracies.
      </paragraph>
      <paragraph>
       We note that this POMDP is a purely sensing-POMDP. None of the actions changes the state of the POMDP.
      </paragraph>
      <paragraph>
       In many crowdsourcing platforms, such as Mechanical Turk, we cannot preselect the workers for a job. However, in order to specify our observation probabilities, which are defined by our generative model, we need access to future workersʼ parameters. To simplify the computation, our POMDP calculates observation probabilities using an average γ. In other words, it assumes that every future worker is an average worker. Formally, every future worker has an error parameter equal to {a mathematical formula}γ¯=1W∑wγw where W is the number of known workers.
      </paragraph>
      <paragraph>
       However, our agent can keep around knowledge about worker accuracy in order to use accurate estimates of γ when updating its belief state. In particular, if someone answers a question correctly (according to the agentʼs belief), then she is a good worker (and her {a mathematical formula}γw should decrease) and if someone made an error in a question her {a mathematical formula}γw should increase. Moreover the increase/decrease amounts should depend on the difficulty of the question. The following simple update strategy may work:
      </paragraph>
      <list>
       <list-item label="1.">
        If a worker answers a question of difficulty d correctly then {a mathematical formula}γw←γx−dδ.
       </list-item>
       <list-item label="2.">
        If a worker makes an error when answering a question then {a mathematical formula}γw←γw+(1−d)δ.
       </list-item>
      </list>
      <paragraph>
       We use δ to represent the learning rate, which we can slowly reduce over time so that the accuracy of a worker approaches an asymptote.
      </paragraph>
      <paragraph>
       More sophisticately, one may update the parameters using Bayesian updates, in the same flavor as discussed in Section 3.4.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Simulated experiments
      </section-title>
      <paragraph>
       We evaluate our model in simulation against an agent that uses a majority-vote strategy to assess the viability of using a POMDP to control such a simple workflow for crowdsourcing.
      </paragraph>
      <paragraph>
       The POMDP must manage a belief state over the cross product of the Boolean answer and one continuous variable – the task difficulty. Since solving a POMDP with a continuous state space is challenging, we discretize difficulty into eleven possible values, leading to a (world) state space of size {a mathematical formula}2×11=22. To solve POMDPs, we run the ZMDP package{sup:8} for 300 s using the default Focused Real-Time Dynamic Programming search strategy [57].
      </paragraph>
      <paragraph>
       On each run, the simulator draws difficulty uniformly. We fix the reward of returning the correct answer to 0, and vary the reward (penalty) of returning an incorrect answer between the following values: −10, −100, and −1000. We set the cost of creating a job for a (simulated) worker to −1. We use a discount factor of 0.9999 in the POMDP so that the POMDP solver converges quickly. We ensure that majority vote uses a number of ballots that is at least as many and within two of the average number of ballots that TurKontrol0 uses. Difficulties are drawn uniformly randomly and workersʼ γ are drawn from Normal {a mathematical formula}(1.0,0.2). For each setting of reward values we run 1000 simulations and report mean net utilities (Fig. 3) and the percent reduction in error that TurKontrol0 achieves when comparing its accuracy to that of majority voting (Fig. 4).
      </paragraph>
      <paragraph>
       We see that our model works well when the average difficulty of the task is not too difficult and not too easy and the variance is wide. To gain further insight, we investigate the effectiveness of our model at varying settings of task difficulty. We run 1000 simulations at nine equally spaced difficulty settings, starting from {a mathematical formula}d=0.1. Workersʼ γ are drawn from Normal {a mathematical formula}(1.0,0.2), and we set the reward for an incorrect answer to be −1000. However, we assume that TurKontrol0 has no knowledge of worker γ, always receives observations from new workers, and only has access to the average γ. To further place our agent in the most unfavorable testing conditions as possible, we compare TurKontrol0 against an agent (MV) that uses the following adaptive majority-vote strategy. For every difficulty setting, we determine, on average, how many ballots TurKontrol0 uses. Then we set the majority-vote strategy to use at least as many ballots, and at most 1 more.
      </paragraph>
      <paragraph>
       Fig. 5 shows the percent reduction in error that TurKontrol0 achieves when comparing its accuracy to that of MV. Since TurKontrol0 always achieves equal or better accuracy than MV, and MV always uses equal or more ballots, TurKontrol0 also always achieves equal or better net utility than MV. Interestingly, the figure shows an illuminating trend. TurKontrol0 only achieves slightly better results than MV at the extremes of the difficulty spectrum, but shows larger gains in the middle difficulty settings. Such a result intuitively makes sense because of the following reasons. When the problems are easy, there is no need to do anything dynamic. Similarly, when the problems are extremely difficult, there is nothing TurKontrol0 can do without more knowledge about the workers. However, when the problem difficulty is in the middle range, sometimes the workers will agree quickly and sometimes they will not, and the ability of our agent to make dynamic decisions contributes to both higher accuracy and higher net utility.
      </paragraph>
     </section>
     <section label="3.4">
      <section-title>
       Learning the model
      </section-title>
      <paragraph>
       Extensive simulation results in the previous section show the usefulness of decision-theoretic techniques for a simple binary classification workflow. This section addresses learning the model from real Mechanical Turk data [13]. We will use the following task. Given an image and a pair of descriptions, we ask workers to select the better description. This learning problem is challenging due to the large number of parameters and the sparse, noisy training data.
      </paragraph>
      <paragraph>
       We begin by defining the supervised learning problem. Fig. 6 presents our generative model of ballot jobs; we can observe the ballots, the true answer, and the difficulty of the task. We seek to learn the error parameters γ where {a mathematical formula}γw is the parameter for worker w. To generate training data for our task we select T images and corresponding pairs of descriptions and post W copies of a ballot job for each task. We use {a mathematical formula}bi,w to denote worker wʼs ballot on the ith question. Let {a mathematical formula}vi denote whether the first artifact of the ith pair is better than the second, and {a mathematical formula}di denote the difficulty of answering such a question. The ballot answer of each worker depends on her error parameter, as well as the difficulty of the job, d, and its real truth value, v.
      </paragraph>
      <paragraph>
       For our learning problem, we collect values of v and d for the T ballot questions from the consensus of three human experts and treat these values as observed. In our experiments we assume γ are drawn from a uniform prior, though our model can incorporate more informed priors.{sup:9} We use the standard maximum a posteriori approach to estimate the γ parameters:{a mathematical formula} Under the uniform prior and conditional independence of different workers given difficulty and truth value of the task, Eq. (11) can be simplified to:{a mathematical formula}
      </paragraph>
      <paragraph>
       Taking the log, the maximum a posteriori problem is transformed to the following optimization problem:{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}
      </paragraph>
      <paragraph>
       We also try an unsupervised learning algorithm. The plate notation is shown in Fig. 7. We donʼt assume knowledge of correct answers or the difficulties. Instead, we adopt an EM-style algorithm based on Whitehill et al.ʼs learning mechanism [68]. We initialize the difficulty values and error parameters with the corresponding average values learned from the supervised learning algorithm. We find applying prior distributions on the parameters does not help, so we do not use a prior. During the expectation step, we compute the probability of the true answers given the workersʼ answers, and the current values of the difficulty and error parameters. During the maximization step, we update the difficulty and error parameters based on the likelihood function (Eq. (12)).
      </paragraph>
      <section label="3.4.1">
       <section-title>
        Experiments on ballot model
       </section-title>
       <paragraph>
        We evaluate the effectiveness of our learning procedures against each other and against a majority-vote baseline on the image description task. We select 20 pairs of descriptions ({a mathematical formula}T=20) and collect sets of ballots from 50 workers. 5 spammers were detected manually and dropped from learning ({a mathematical formula}W=45). We spend $4.50 on the supervised learning process. We solve the optimization problem using the NLopt package{sup:10} and implement the unsupervised learning algorithm based on Whitehillʼs framework [68].
       </paragraph>
       <paragraph>
        We run a five-fold cross-validation experiment. We take 4/5th of the image pairs and learn error parameters over them, and then use these parameters to estimate the true ballot answer for the images in the fifth fold. Our supervised learning algorithm obtains an accuracy of 80.01%, our unsupervised learning algorithm obtains an accuracy of 80.0%, and a majority-voting baseline obtains an accuracy of 80%. We investigate these similar accuracies and see that the four ballots frequently missed by the models are those in which the mass opinion differs from our expert labels.
       </paragraph>
       <paragraph>
        We also compare the confidence, or the degree of belief in the correctness of an answer, for two approaches. For the majority-vote baseline, we calculate confidence by dividing the number of votes for the inferred correct answer by the total number of votes. For our supervised learning approach, we use the average posterior probability of the inferred answer. The average confidence values derived from the supervised learning approach is much higher than the majority vote (82.2% against 63.6%). Thus, even though the two approaches achieve the same accuracy on all 45 votes, our ballot model has superior belief in its answer.
       </paragraph>
       <paragraph>
        Although the confidence values are different, the ballot models (learned from both supervised and unsupervised learning) seem to offer no distinct advantage over the simple majority-voting baseline given a large number of votes. In hindsight, this result is not surprising, since we are using a large number of workers. In other work, researchers have shown that a simple average of a large number of non-experts often beats even the expert opinion [58].
       </paragraph>
       <paragraph>
        However, one will rarely have the resources to use 45 voters per question, so we consider the effect of varying the number of available voters. For each image pair, we randomly sample, without replacement, 50 000 sets of 3 to 11 ballots and compute the average accuracies of the three approaches. Fig. 8 shows that using our model (learned from both the supervised and the unsupervised learning algorithms) consistently outperforms the majority-vote baseline. Furthermore, applying the supervised learning algorithm consistently achieves higher accuracy than applying the unsupervised learning algorithm, which shows the usefulness of using expert labeling. The unsupervised learning algorithm gradually catches up as the number of votes increases, which distinguishes itself from majority voting. In contrast, the model learned from supervised learning always outperforms majority voting by a wide margin. With just 11 votes, it is able to achieve an accuracy of 79.3%, which is very close to the accuracy achieved using all 45 votes. Also, the supervised ballot model with only 5 votes achieves an accuracy similar to the one achieved by a majority vote with 11 votes. Our ballot model significantly reduces the number of votes and thus the amount of money needed for a given desired accuracy. Since the unsupervised learning algorithm does not require any labeled data, it can be useful when data labeling is expensive.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="4">
     <section-title>
      Decision-theoretic optimization of an iterative improvement workflow
     </section-title>
     <section label="4.1">
      <section-title>
       Motivation example
      </section-title>
      <paragraph>
       We now move to the decision-theoretic control of the iterative improvement workflow [12], introduced by Little et al. [37]. This workflow is depicted in Fig. 9. While the ideas in our paper are applicable to many complex workflows, we choose to apply them to this one because it is representative of a number of flows in commercial use today; at the same time, it is moderately complex making it ideal for a first investigation.
      </paragraph>
      <paragraph>
       Iterative text improvement works as follows. There is an initial job, which presents the worker with an image and requests an English description of the imageʼs contents. A subsequent iterative process consists of an improvement job and multiple ballot jobs. In the improvement job, a (different) worker is shown this same image as well as the current description and is requested to generate an improved English description (see Fig. 22). Next, {a mathematical formula}n⩾1 ballot jobs are posted (“Which text best describes the picture?”). See Fig. 23, Fig. 24, Fig. 25, Fig. 26 for our user interface design. Based on a majority opinion, the best description is selected and the loop continues. Little et al. have shown that this iterative process generates better descriptions for a fixed amount than allocating the total reward to a single author.
      </paragraph>
      <paragraph>
       Little et al. support an open-source toolkit, TurKit, that provides a high-level mechanism for defining moderately complex, iterative workflows with voting-controlled conditionals. However, TurKit does not have built-in methods for monitoring the accuracy of workers; nor does it automatically determine the ideal number of voters or estimate the appropriate number of iterations before returns diminish.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Problem overview
      </section-title>
      <paragraph>
       The agentʼs control problem for a workflow like iterative text improvement is defined as follows. As input the agent is given an initial artifact (or a job description for requesting one), and the agent is asked to return an artifact which maximizes some payoff based on the quality of the submission. In our initial model, we assume that requesters will express their utility as a function U from quality to dollars. To accomplish the task, the agent is allowed to post an improvement job or a ballot job.
      </paragraph>
      <paragraph>
       To fully specify the problem, we must define what “quality” means. Intuitively, something is high-quality if it is better than most things of the same type. For engineered artifacts (including English descriptions) one may say that an artifact is high quality if it is difficult to improve. Therefore, we define the quality of an artifact as follows. Let the quality be {a mathematical formula}q∈[0,1]. An artifact with quality q means an average dedicated worker has probability {a mathematical formula}1−q of improving the artifact.
      </paragraph>
      <paragraph>
       The agent never exactly knows the quality of an artifact. At best, it can estimate q based on domain dynamics and observations (like ballot results). Therefore, we can model the agentʼs control problem as a POMDP. Since quality is a real number, the state space of the POMDP is continuous [8].
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       The POMDP
      </section-title>
      <paragraph label="Definition 6">
       The POMDP for the iterative improvement workflow is a six-tuple {a mathematical formula}〈S,A,T,R,O,P〉, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}S={(q,q′)|q,q′∈[0,1]}.
       </list-item>
       <list-item label="•">
        {a mathematical formula}A={createaballotjob,createanimprovementjob,submitbestartifact}.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R((q,q′),submit) is the reward received from submitting an artifact with quality {a mathematical formula}max{q,q′}.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R((q,q′),create) is the cost of creating a job.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T: defined below.
       </list-item>
       <list-item label="•">
        {a mathematical formula}O={true,false} is a Boolean answer received for a ballot question.
       </list-item>
       <list-item label="•">
        P: defined below.
       </list-item>
      </list>
      <section label="4.3.1">
       <section-title>
        The transition function
       </section-title>
       <paragraph>
        We first note that the underlying state of the POMDP only changes when the agent requests an improvement. The qualities of the artifacts do not change when the agent requests votes. Votes only change the belief state of the agent.
       </paragraph>
       <paragraph>
        Suppose we currently have artifacts {a mathematical formula}α,α″, with unknown qualities q and {a mathematical formula}q″. Therefore, the current state is {a mathematical formula}(q,q″). Suppose without loss of generality that the agent posts an improvement job for α, and a worker x submits another artifact {a mathematical formula}α′, whose quality is denoted by {a mathematical formula}q′. Since {a mathematical formula}α′ is a suggested improvement of α, {a mathematical formula}q′ depends on the initial quality q. Moreover, a higher accuracy worker x may improve it much more, so {a mathematical formula}q′ also depends on x. In order to determine the next state, we must compute {a mathematical formula}P(q′|q,x), the conditional distribution of {a mathematical formula}q′ when worker x improves an artifact of quality q. We describe how to learn this conditional distribution later. The current state {a mathematical formula}(q,q″) transitions to the new state {a mathematical formula}(q,q′) with probability {a mathematical formula}P(q′|q,x). However, since the agent does not know a priori the worker who will submit the improvement, it must assume, for the purpose of lookahead, that the worker has an average ability.
       </paragraph>
      </section>
      <section label="4.3.2">
       <section-title>
        The observation function
       </section-title>
       <paragraph>
        Our observation function is only relevant when the agent requests ballot jobs. If the current (hidden) state is {a mathematical formula}(q,q′) with corresponding artifacts α and {a mathematical formula}α′, the agent can either observe a vote for α or {a mathematical formula}α′. The observation function is defined by {a mathematical formula}P(o|q,q′). Since a ballot job is simply an instantiation of a binary classification task, we can use our already defined generative model by defining the difficulty of the task.
       </paragraph>
       <paragraph>
        To make this definition, we observe that the difficulty of the ballot job depends on the relative closeness of the two qualities of the underlying state. If the artifacts are of similar quality, it is difficult to judge whether one is better or not. Thus, we define the relationship between the difficulty and qualities as{a mathematical formula} where {a mathematical formula}M is the difficulty constant.
       </paragraph>
      </section>
      <section label="4.3.3">
       <section-title>
        Discussion
       </section-title>
       <paragraph>
        Fig. 10 is a high-level flow describing our plannerʼs decisions. At each step we track our belief in qualities (q and {a mathematical formula}q′) of the previous (α) and the current artifact ({a mathematical formula}α′) respectively. Each improvement attempt or vote gives us new information, which is reflected in the quality posteriors. These distributions also depend on the accuracy of workers, which we also incrementally estimate based on their previous work.
       </paragraph>
       <paragraph>
        Our POMDP lets us answer questions like (1) when to terminate the voting phase (thus switching attention to artifact improvement), (2) which of the two artifacts is the best basis for subsequent improvements, and (3) when to stop the whole iterative process and submit the result to the requester.
       </paragraph>
       <paragraph>
        We note that with this general POMDP model, the belief in the quality of the previous artifact (posterior of α) can change based on ballots comparing it with the new artifact. Why should this be the case? We make the following subtle, but important point. If the improvement worker (who has a good accuracy) was unable to create a much better {a mathematical formula}α′ in the improvement phase, that must be because α already has a high quality and is not easily improvable. Under such evidence we should increase our quality estimation of α. Similarly, if all voting workers unanimously thought that {a mathematical formula}α′ is much better than α, then {a mathematical formula}α′ likely incorporates significant improvements over α and the qualities should reflect such knowledge.
       </paragraph>
      </section>
      <section label="4.3.4">
       <section-title>
        Updating difficulty and worker accuracy
       </section-title>
       <paragraph>
        To update its knowledge about the quality of each worker after each voting phase, the agent first estimates the expected difficulty of the voting using its estimates of quality as follows:{a mathematical formula}
       </paragraph>
       <paragraph>
        Then, it uses {a mathematical formula}d⁎ and what it believes to be the correct answer to update worker quality records as we discussed in Section 3.2.
       </paragraph>
      </section>
      <section label="4.3.5">
       <section-title>
        Implementation
       </section-title>
       <paragraph>
        For efficient storage and computation TurKontrol0 could employ the piecewise constant/piecewise linear value function representations or use particle filters. Although approximate, both these techniques are very popular in the literature for efficiently maintaining continuous distributions [39], [17] and can provide arbitrarily close approximations. Because some of our equations require double integrals and can be time-consuming (e.g., Eq. (14)) these compact representations will help in overall efficiency of the implementation.
       </paragraph>
       <paragraph>
        Our piecewise constant function over a distribution density function, {a mathematical formula}P(q), divides the domain into fixed number of intervals and uses the average value {a mathematical formula}1u−l∫luf(q)dq as the value for each interval {a mathematical formula}[l,u]. In implementation, a continuous function is represented by an array of values, where each value equals the constant function value of its designated interval.
       </paragraph>
      </section>
     </section>
     <section label="4.4">
      <section-title>
       Simulations
      </section-title>
      <paragraph>
       This section aims to empirically answer the following questions about the POMDP-solving algorithms we introduced in Section 2.4: (1) For a simple lookahead approach, how deep should an agentʼs lookahead be to best tradeoff between computation time and utility? (2) How well does the ABDS algorithm perform and scale? (3) What is the error of the UCT algorithm using a history approximation? (4) How well does the UCT algorithm perform? (5) What is the best planner for TurKontrol0? (6) Does TurKontrol0 make better decisions compared to a non-adaptive workflow? (7) Can our planner outperform an agent following a well-informed, fixed policy? (8) How well does our planner handle workers that are less effective at completing ballot jobs than improvement jobs?
      </paragraph>
      <section label="4.4.1">
       <section-title>
        Experimental setup
       </section-title>
       <paragraph>
        We define reward for submitting an artifact of quality q to be the convex function {a mathematical formula}R(q)=1000eq−1e−1 with {a mathematical formula}R(0)=0 and {a mathematical formula}R(1)=1000. We assume the quality of the initial artifact follows a Beta distribution, {a mathematical formula}Beta(1,9), which implies that the mean quality of the first artifact is 0.1.
       </paragraph>
       <paragraph>
        We model results of an improvement task in a manner akin to ballot tasks. We know that the higher the quality of an artifact, the less likely that artifact can be improved. Suppose the quality of the current artifact is q, we define the conditional distribution {a mathematical formula}P(q′|q,x) to be {a mathematical formula}Beta(10μQ′|q,x,10(1−μQ′|q,x)), with mean {a mathematical formula}μQ′|q,x, where{a mathematical formula} Thus, the resulting distribution of qualities is influenced by the workerʼs accuracy and the hardness of an improvement, indicated by the quality of the original artifact, q.
       </paragraph>
       <paragraph>
        We fix the ratio of the costs of improvements and ballots to be {a mathematical formula}cimp/cb=3, because ballots take less time. We set the difficulty constant {a mathematical formula}M=0.5. In each of the simulation runs, we build a pool of 1000 workers, whose error coefficients, {a mathematical formula}γw, follow a bell shaped distribution with a fixed mean {a mathematical formula}γ¯. We also distinguish the accuracies of performing an improvement and answering a ballot by using one half of {a mathematical formula}γw when worker w is answering a ballot, since answering a ballot is an easier task, and therefore a worker should have higher accuracy.{sup:11}
       </paragraph>
      </section>
      <section label="4.4.2">
       The l-step lookahead algorithm
       <paragraph>
        We first try to find the optimal depth for the l-step lookahead algorithm we presented earlier. When {a mathematical formula}l=2, such an algorithm considers the following set of action sequences {a mathematical formula}{〈stop〉,〈ballot,stop〉,〈improvement,stop〉,〈ballot,ballot〉,〈ballot,improvement〉,〈improvement,ballot〉,〈improvement,improvement〉}.
       </paragraph>
       <paragraph>
        We run 10 000 simulation trials with average error coefficient {a mathematical formula}γ¯=1 on three pairs of improvement and ballot costs – {a mathematical formula}(30,10), {a mathematical formula}(3,1), and {a mathematical formula}(0.3,0.1) – trying to find the best lookahead depth l for the l-step lookahead algorithm. Fig. 11 shows the average utility, achieved by different lookahead depths, denoted by TurKontrol(l). Note that there is always a performance gap between TurKontrol(1) and TurKontrol(2), but the curves of TurKontrol(3) and TurKontrol(4) generally overlap. We also observe that when the costs are high, such that the process usually finishes in a few iterations, the performance difference between TurKontrol(2) and deeper step lookaheads is negligible. Since each additional step of lookahead increases the computational overhead by an order of magnitude, we limit TurKontrolʼs lookahead to depth 2 in subsequent experiments.
       </paragraph>
      </section>
      <section label="4.4.3">
       <section-title>
        The ADBS algorithm
       </section-title>
       <paragraph>
        Next, we try our ADBS algorithm. The states of the MDP are four-tuples {a mathematical formula}〈μ,σ,μ′,σ′〉, an average and a standard deviation for each hidden quality. Since quality is a real number in {a mathematical formula}[0,1], we have that {a mathematical formula}μ∈[0,1] and {a mathematical formula}σ∈[0,1], and we can finitely discretize these intervals.
       </paragraph>
       <paragraph>
        We try different resolutions of discretization, i.e. various constant interval lengths, and run with average error coefficient {a mathematical formula}γ¯=1 on three pairs of improvement and ballot costs – {a mathematical formula}(30,10), {a mathematical formula}(3,1), and {a mathematical formula}(0.3,0.1). Table 1 lists the average utility of TurKontrol(2) on 10 000 simulations (results incorporated from Fig. 11). For the ADBS algorithm, we report the size of the reachable belief states under each resolution, {a mathematical formula}|S|, and the value of the initial state, {a mathematical formula}V⁎(s0), calculated by value iteration.
       </paragraph>
       <paragraph>
        We find TurKontrol(2) outperforms ADBS in all settings. Moreover, the smaller the costs, the bigger the performance gap. The poor performance of ADBS is probably due to the errors generated during approximation and discretization. Also notice that with more refined discretization, the reachable state space grows very quickly (at approximately a rate of an order of magnitude per doubly refined interval), yet the optimal value of the initial state increases very slowly. This indicates that the error most likely comes from the approximation as opposed to the discretization. We also try an interval length of 0.0025, but the algorithm terminates prematurely when the reachability search runs out of memory – indicating the limited scalability of the ADBS algorithm. This experiment shows that a simple POMDP method does not work as well as the 2-step lookahead algorithm, yet we cannot yet draw the conclusion that POMDP algorithms do not work on this planning problem.
       </paragraph>
      </section>
      <section label="4.4.4">
       <section-title>
        The UCT algorithm
       </section-title>
       <paragraph>
        Finally, we try the UCT variant. We first evaluate the amount of suboptimality introduced by the our condensed history approximation. An example of a condensed history is: (Step 1) An initial artifact {a mathematical formula}α1 is provided. (Step 2) The agent creates an improvement HIT and obtains {a mathematical formula}α2. (Step 3) The agent creates a ballot job, and receives a vote that {a mathematical formula}α2 is better than {a mathematical formula}α1. (Step 4) The agent submits {a mathematical formula}α2.
       </paragraph>
       <paragraph>
        Since the worker identities are ignored, the belief state space size is drastically reduced.
       </paragraph>
       <paragraph>
        In a new TurKontrol(2) version, we do everything the same as TurKontrol(2) except we regard every worker as an anonymous worker of the same, average accuracy, {a mathematical formula}γ¯. We denote this new variant TurKontrol(2, anonymous). Note that in this case, the policy is non-adaptive, as there is no uncertainty in workersʼ accuracies. We perform 10 000 simulations on the following two configurations{sup:12}:
       </paragraph>
       <list>
        <list-item label="1.">
         {a mathematical formula}γ¯=2 and ballot accuracies are higher than improvement accuracies: TurKontrol(2) gets 4.5% more utility than TurKontrol(2, anonymous).
        </list-item>
        <list-item label="2.">
         {a mathematical formula}γ¯=1 and ballot accuracies are the same as improvement accuracies: TurKontrol(2) gets 3.7% more utility than TurKontrol(2, anonymous).
        </list-item>
       </list>
       <paragraph>
        This shows using the average {a mathematical formula}γ¯ is not a bad approximation, because it only reduces the expected utility of TurKontrol(2) by less than 5% in both cases. We expect it to produce even better results if the approximation is more refined (e.g., if we group the workers by accuracy).
       </paragraph>
       <paragraph>
        In investigating the UCT algorithm, we use a fixed {a mathematical formula}κ=50 and a dynamically decreasing learning parameter {a mathematical formula}θ=0.2×log210n(s,a). As an action has been visited more frequently, the effect of randomness decreases, so less weight should be given to the new observation. Fig. 12 plots the average utility of the UCT algorithm as a function of the number of trials completed (from 10 000 to 500 000). The horizontal lines are not x-axes, but represent the average utility of TurKontrol(2) (data as in Fig. 11). First notice that the performance of UCT improves with the increase in the number of completed trials. We also observe that UCT outperforms TurKontrol(2) after very few (10 000) trials on the two problems where the costs are small, but underperforms TurKontrol(2) on the problem where the costs are high. This behavior is probably because TurKontrol(2) already performs close to optimal on the high cost problem. We also find UCT consistently underperforms TurKontrol(3) (from comparing against Fig. 11). This is because TurKontrol(3) computes close-to-optimal values, while UCTʼs performance is influenced by random noises during search trials. This experiment shows that UCT is useful for quickly finding a good suboptimal policy but has limited power in finding a close-to-optimal policy.
       </paragraph>
      </section>
      <section label="4.4.5">
       <section-title>
        Choosing the best planning algorithm
       </section-title>
       <paragraph>
        From comparing the results of the three algorithms: the l-step lookahead, ADBS and UCT, we conclude that the l-step lookahead is the most efficient planning algorithm for the domain.
       </paragraph>
      </section>
      <section label="4.4.6">
       <section-title>
        The effect of poor workers
       </section-title>
       <paragraph>
        We now consider the effect of worker accuracy on the effectiveness of agent control policies. Using fixed costs of {a mathematical formula}(30,10), we compare the average net utility of three control policies. The first is TurKontrol(2). The second, TurKit, is a non-adaptive policy from the literature [37]; it performs as many iterations as possible until its fixed allowance (400 in our experiment) is depleted and on each iteration it requests at least two ballots, invoking a third only if the first two disagree. Our third policy, TurKontrol(fixed), combines elements from decision theory with a fixed policy. After simulating the behavior of TurKontrol(2), we compute the integer mean number of iterations, {a mathematical formula}μimp and mean number of ballots, {a mathematical formula}μb, and use these values to drive a fixed control policy ({a mathematical formula}μimp iterations each with {a mathematical formula}μb ballots). Thus this represents non-adaptive policy whose parameters are tuned to costs and worker accuracies.
       </paragraph>
       <paragraph>
        Fig. 13 (top) shows that both decision-theoretic methods work better than the TurKit policy, partly because TurKit runs more iterations than needed. A Studentʼs t-test shows all differences are statistically significant with {a mathematical formula}p&lt;0.01. We also note that the performance of TurKontrol(fixed) is very similar to that of TurKontrol(2), when workers are very inaccurate, {a mathematical formula}γ¯=4. Indeed, in this case TurKontrol(2) executes a nearly fixed policy itself. In all other cases, however, TurKontrol(fixed) consistently underperforms TurKontrol(2). A Studentʼs t-test confirms the differences are all statistically significant for {a mathematical formula}γ¯&lt;4. We attribute this difference to the fact that the dynamic policy makes better use of ballots, e.g., it requests more ballots in late iterations, when the (harder) improvement tasks are more error-prone. The biggest performance gap between the two policies manifests when {a mathematical formula}γ¯=2, where TurKontrol(2) generates 19.7% more utility than TurKontrol(fixed).
       </paragraph>
      </section>
      <section label="4.4.7">
       <section-title>
        Robustness in the face of bad voters
       </section-title>
       <paragraph>
        As a final study, we consider the sensitivity of the previous three policies to increasingly noisy voters. Specifically, we repeat the previous experiment using the same error coefficient, {a mathematical formula}γw, for each workerʼs improvement and ballot behavior (Fig. 13 (bottom)). (We previously set the error coefficient for ballots to one half {a mathematical formula}γw to model the fact that voting is easier.) Fig. 13 (bottom) has the same shape as that of Fig. 13 (top) but with lower overall utility. Once again, TurKontrol(2) continues to achieve the highest average utility across all settings. Interestingly, the utility gap between the two TurKontrol variants and TurKit is consistently bigger for all {a mathematical formula}γ¯ than in the previous experiment. In addition, when {a mathematical formula}γ¯=1, TurKontrol(2) generates 25% more utility than TurKontrol(fixed) – a bigger gap seen in the previous experiment. A Studentʼs t-test shows all that the differences between TurKontrol(2) and TurKontrol(fixed) are significant when {a mathematical formula}γ¯&lt;2 and the differences between both TurKontrol variants and TurKit are significant at all settings.
       </paragraph>
      </section>
     </section>
     <section label="4.5">
      <section-title>
       Learning the improvement model
      </section-title>
      <paragraph>
       Extensive simulation results in the previous section show the usefulness of decision-theoretic techniques for iterative improvement workflows. This section addresses learning the model from real Mechanical Turk data [13]. We already explored the learning of our ballot model in Section 3.4, so now we learn our improvement model. To learn the effect of a worker trying to improve an artifact, we first need a method for determining the ground truth for the quality of an arbitrary artifact.
      </paragraph>
      <section label="4.5.1">
       <section-title>
        Estimating artifact quality gold standards
       </section-title>
       <paragraph>
        Since quality is a partially-observable statistical measure, we consider three ways to approximate it: simulating the definition, direct expert estimation, and averaged worker estimation.
       </paragraph>
       <paragraph>
        Our first technique simply simulates the definition. We ask W workers to improve an artifact α and as before use multiple ballots, say l, to judge each improvement. We define the quality of α to be 1 minus the fraction of workers that are able to improve it. Unfortunately, this method requires {a mathematical formula}W+Wl jobs in order to estimate the quality of a single artifact; thus, it is both slow and expensive in practice. As an alternative, direct expert estimation is less complex. We teach a statistically-sophisticated computer scientist the definition of quality and ask her to estimate the quality to the nearest decile.{sup:13} Our final method, averaged worker estimation, is similar, but averages the judgments from several Mechanical Turk workers via scoring jobs. These scoring jobs provide a definition of quality along with a few examples, mapped to a 0–10 integer scale; the workers are then asked to score several more artifacts. See Fig. 27, Fig. 28, Fig. 29 for our user interface design.
       </paragraph>
       <paragraph>
        We collect data on 10 images from the Web and use Mechanical Turk to generate multiple descriptions for each. We then select one description for each image, carefully ensuring that the chosen descriptions span a wide range of detail and language fluency. We also modified a description to obtain one that, we felt, was very hard to improve, thereby accounting for the high-quality region. When simulating the definition, we average over {a mathematical formula}W=22 workers.{sup:14} We use a single expert for direct expert estimation and an average of 10 worker scores for averaged worker estimation. See Table 2.
       </paragraph>
       <paragraph>
        Our hope, following the work of [58], is that averaged worker estimation, definitely the cheapest method, proves comparable to expert estimates and especially to the simulated definition. Indeed, we find that all three methods produce similar results. They agree on the two best and worst artifacts, and on average both expert and worker estimates are within 0.1 of the score produced by simulating the definition. We conclude that averaged worker estimation is equally effective and additionally easier and more economical (1 cent per scoring job), so we adopt this method to assess qualities in subsequent experiments.
       </paragraph>
      </section>
      <section label="4.5.2">
       <section-title>
        Actually learning the model
       </section-title>
       <paragraph>
        Finally, we describe our approach for learning a model for the improvement phase. Our objective is to estimate {a mathematical formula}P(q′|q,w), the probability distribution that describes the quality {a mathematical formula}q′ of a new artifact, {a mathematical formula}α′, when worker w improves artifact α of quality q. Moreover, we also learn a prior, {a mathematical formula}P(q′|q), in order to model work by a previously unseen worker.
       </paragraph>
       <paragraph>
        There are two main challenges in learning this model: first, these functions are over a two-dimensional continuous space, and second, the training data is scant and noisy. To alleviate these difficulties, we break the task into two learning steps: (1) learn a mean value for quality using regression, and (2) fit a conditional density function given the mean. We make the second learning task tractable by choosing parametric representations for these functions. Our full solution follows the following steps:
       </paragraph>
       <list>
        <list-item label="1.">
         Generate an improvement job that contains T original artifacts {a mathematical formula}α1,…,αT.
        </list-item>
        <list-item label="2.">
         Crowdsource W workers to improve each artifact to generate WT new artifacts.
        </list-item>
        <list-item label="3.">
         Estimate the qualities {a mathematical formula}qi and {a mathematical formula}qi,w′ for all artifacts in the set (see previous section). {a mathematical formula}qi is the quality of {a mathematical formula}αi and {a mathematical formula}qi,w′ denotes the quality of the new artifact produced by worker w. This data is our training data.
        </list-item>
        <list-item label="4.">
         Learn a worker-dependent distribution {a mathematical formula}P(q′|q,w) for every participating worker w.
        </list-item>
        <list-item label="5.">
         Learn a worker-independent distribution {a mathematical formula}P(q′|q) to act as a prior for unseen workers.
        </list-item>
       </list>
       <paragraph>
        We now describe the last two steps in detail: the learning algorithms. We first estimate the mean of worker wʼs improvement distribution, denoted by {a mathematical formula}μqw′(q).
       </paragraph>
       <paragraph>
        We assume that {a mathematical formula}μqw′ is a linear function of the quality of the original artifact.{sup:15} By introducing {a mathematical formula}μQw′, we separate the variance in a workerʼs ability in improving all artifacts of the same quality from the variance in our training data, which is due to her starting out from artifacts of different qualities. To learn this we perform linear regression on the training data {a mathematical formula}(qi,qi,w′). This yields {a mathematical formula}qw′=awq+bw as the line of regression with standard error {a mathematical formula}ew, which we truncate for values outside {a mathematical formula}[0,1].
       </paragraph>
       <paragraph>
        To model a workerʼs variance when improving artifacts with the same quality, we consider three parametric representations for {a mathematical formula}P(q′|q,w): Triangular, Beta, and Truncated Normal. While clearly making an approximation, restricting attention to these distributions significantly reduces the parameter space and makes our learning problem tractable. Note that we assume the mean of each of these distributions is given by the line of regression. We consider each distribution in turn.
       </paragraph>
       <paragraph label="Triangular">
        The triangular-shaped probability density function has two fixed vertices {a mathematical formula}(0,0) and {a mathematical formula}(1,0). We set the x-coordinate of the third vertex to {a mathematical formula}μqw′(q), yielding the following probability density function {a mathematical formula}fqw′|q(qw′):{a mathematical formula}
       </paragraph>
       <paragraph label="Beta">
        We wish the Beta distributionʼs mean to be {a mathematical formula}μqw′ and its standard deviation to be proportional to {a mathematical formula}ew. Therefore, we train a constant, {a mathematical formula}c1, using gradient descent that maximizes the log-likelihood of observing the training data for all workers.{sup:16} resulting in the distribution {a mathematical formula}Beta(c1ew×μqw′(q),c1ew×(1−μqw′(q))). The error {a mathematical formula}ew appears in the denominator because the two parameters for the Beta distribution are approximately inversely related to its standard deviation.
       </paragraph>
       <paragraph label="Truncated Normal">
        As before we set the mean to {a mathematical formula}μqw′ and the standard deviation to be {a mathematical formula}c2×ew where {a mathematical formula}c2 is a constant, trained to maximize the log-likelihood of the training data. This yields the distribution {a mathematical formula}=TruncatedNormal(μqw′(q),c22ew2) where the truncated interval is {a mathematical formula}[0,1].
       </paragraph>
       <paragraph>
        We use similar approaches to learn the worker-independent model {a mathematical formula}P(q′|q), except that training data is of the form {a mathematical formula}(qi,qi′¯) where {a mathematical formula}qi′¯ is the average improved quality for ith artifact, i.e., the mean of {a mathematical formula}qi,w′ over all workers. Denote the standard deviation of this set as {a mathematical formula}σqi′|qi. As before, we start with linear regression, {a mathematical formula}q′=aq+b. The Triangular distribution is defined exactly as before. For the other two distributions, their standard deviations depend on {a mathematical formula}σqi′|qi. We assume that the conditional standard deviation {a mathematical formula}σq′|q is quadratic in q, and then infer an unknown conditional standard deviation using the existing ones, by running a quadratic regression. As before, we use gradient descent to train constants for the Beta and Truncated Normal distributions.
       </paragraph>
       <paragraph>
        We seek to determine which of the three distributions best models the data, and we employ leave-one-out cross validation. We set the number of original artifacts and number of workers to be ten each ({a mathematical formula}T=W=10), and spend $16.50 for this data collection. The algorithm iteratively trains on nine training examples, e.g. {a mathematical formula}{(qi,qi′¯)} for the worker-independent case, and measures the probability density of observing the tenth. We score a model by summing the ten log probability densities.
       </paragraph>
       <paragraph>
        Our results show that Beta distribution with {a mathematical formula}c1=3.76 is the best conditional distribution for worker-dependent models. For the worker-independent model, with an intercept of 0.35 and slope of 0.34 from the linear regression, Truncated Normal with {a mathematical formula}c2=1.00 performs the best. We suspect this is the case because most workers have average performance and Truncated Normal has a thinner tail than the Beta. In all cases, the Triangular distribution performs worst. This is probably because Triangular assumes a linear probability density, whereas, in reality, workers tend to provide reasonably consistent results, which translates to higher probabilities around the conditional mean. We use these best performing distributions in all subsequent experiments. In case of a returning worker, we use the corresponding worker-dependent model. Otherwise, we assume the worker performs averagely, and instead, use the worker-independent model.
       </paragraph>
      </section>
     </section>
     <section label="4.6">
      <section-title>
       TurKontrol on Mechanical Turk
      </section-title>
      <paragraph>
       Now that we have learned the POMDP model, our final evaluation assesses the benefits of the dynamic workflow controlled by TurKontrol versus a non-adaptive workflow (as originally used in TurKit [37]) under similar monetary consumption settings. We aim to answer the following questions: (1) Is there a significant quality difference between artifacts produced using TurKontrol and TurKit? (2) What are the qualitative differences between the two workflows?
      </paragraph>
      <paragraph>
       As before, we run our experiments using the image description task. We use 40 fresh pictures from the Web and employ iterative improvement to generate descriptions for these. For each picture, we restrict a worker to take part in at most one task in each setting (i.e., non-adaptive or dynamic). We set the user interfaces to be identical for both settings and randomize the order in which the two conditions are presented to workers in order to eliminate human learning effects. Altogether there are 655 participating workers, of which 57 take part in both settings.
      </paragraph>
      <paragraph>
       We devise automated rules to detect spammers. We reject an improvement job if the new artifact is identical to the original. We reject ballot and scoring jobs if they are returned so quickly that the worker could not have made a reasonable judgment.
      </paragraph>
      <paragraph>
       Note that our system does not need to learn a model for a new worker before assigning them jobs; instead, it uses the worker-independent parameters {a mathematical formula}γ¯ and {a mathematical formula}P(q′|q) as a prior. These parameters are incrementally updated as TurKontrol obtains more information.
      </paragraph>
      <paragraph>
       Recall that TurKontrol performs decision-theoretic control based on a user-defined reward function. We define the reward for submitting an artifact of quality q to be {a mathematical formula}R(q)=$25q for our experiments. We set the cost of an improvement job to be 5 cents and a ballot job to be 1 cent. We use the 3-step lookahead algorithm for the controller. Under these parameters, TurKontrol-workflows run an average of 6.25 iterations with an average of 2.32 ballots per iteration, costing about 46 cents per image description on average.
      </paragraph>
      <paragraph>
       We use TurKitʼs original non-adaptive policy for ballots, which requests a third ballot if the first two voters disagree. We compute the number of iterations for TurKit so that the total money spent matches TurKontrolʼs. Since this number comes to be 6.47 we compare against three cases: TurKit6 with 6 iterations, TurKit7 with 7 iterations and TurKit67 a weighted average of the two that equalizes monetary consumption.
      </paragraph>
      <paragraph>
       For each final description we create a scoring job in which multiple workers score the descriptions. Fig. 14 compares the artifact qualities generated by TurKontrol and by TurKit67 for the 40 images. We note that most points are below the {a mathematical formula}y=x line, indicating that the dynamic workflow produces superior descriptions. Furthermore, the quality produced by TurKontrol is greater on average than TurKitʼs, and the difference is statistically significant: {a mathematical formula}p&lt;0.01 for TurKit6, {a mathematical formula}p&lt;0.01 for TurKit67 and {a mathematical formula}p&lt;0.05 for TurKit7, using the Studentʼs t-test.
      </paragraph>
      <paragraph>
       Using our parameters, TurKontrol generates some of the highest-quality descriptions with an average quality of 0.67. TurKit67ʼs average quality is 0.60; furthermore, it generates the two worst descriptions with qualities below 0.3. Finally, the standard deviation for TurKontrol is lower (0.09) than TurKitʼs (0.12). These results demonstrate overall superior performance of decision-theoretic control on live workflows.
      </paragraph>
      <paragraph>
       While the {a mathematical formula}11% average quality increase produced by TurKontrol is statistically significant, some wonder if it is material. To better illustrate the importance of quality, we include another experiment. We run the nonadaptive, TurKit policy for additional improvement iterations, until it produces artifacts with an average quality equal to that produced by TurKontrol. Fixing the quality threshold, the TurKit policy has to run an average of 8.76 improvements, compared to the 6.25 improvement iterations used by TurKontrol. As a result the nonadaptive policy spends {a mathematical formula}28.7% more money than TurKontrol to achieve the same quality results. Note that final artifact quality is neither linear in the number of iterations nor total cost. Intuitively, it is much easier to improve an artifact when its quality is low than when it is high.
      </paragraph>
      <paragraph>
       We also qualitatively study TurKontrolʼs behavior compared to TurKitʼs and find an interesting difference in the use of ballots. Fig. 15 plots the average number of ballots per iteration number. Since TurKitʼs ballot policy is fixed, it consistently uses about 2.45 ballots per iteration. TurKontrol, on the other hand, uses ballots much more intelligently. In the first two improvement iterations TurKontrol does not bother with ballots because it expects that most workers will improve the artifact. As iterations increase, TurKontrol increases its use of ballots, because the artifacts are harder to improve in later iterations, and hence TurKontrol needs more information before deciding which artifact to promote to the next iteration. The eighth iteration is an interesting exception; at this point improvements have become so rare that if even the first voter rates the new artifact as a loser, then TurKontrol often believes the verdict.
      </paragraph>
      <paragraph>
       Besides using ballots intelligently we believe that TurKontrol adds two other kinds of reasoning. First, six of the seven pictures that TurKontrol finished in 5 iterations have higher qualities than TurKitʼs. This suggests that its quality tracking is working well. Perhaps due to the agreement among various voters, TurKontrol is able to infer that a description already has quality high enough to warrant termination. Secondly, TurKontrol has the ability to track individual workers, and this also affects its posterior calculations. For example, in one instance TurKontrol decided to trust the first vote because that worker had superior accuracy as reflected in a low error parameter. We expect that for repetitive tasks this will be an enormously valuable ability, since TurKontrol will be able to construct more informed worker models and make more superior decisions.
      </paragraph>
      <paragraph>
       We present an image description example in Fig. 16. It is interesting to note that both processes managed to find the origin of the image. However, the TurKontrol version exemplifies better use of language, factuality and level of detail. In retrospect, we find the nonadaptive workflow probably made a wrong ballot decision in the sixth iteration, where a decision was critical yet only three voters were consulted. TurKontrol  on the other hand, reached a decision after 6 unanimous votes at the same stage.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Decision-theoretic optimization of multiple workflows
     </section-title>
     <paragraph>
      Now that we have explored the decision-theoretic control of a single workflow, we consider the scenario when the requester has more than one [36]. In our exploration, for simplicity, we will only consider binary classification workflows. However, we note that the ideas we present are equally applicable for more complex workflows, like the iterative improvement workflow we considered earlier.
     </paragraph>
     <paragraph>
      To see why it is useful to switch between workflows, consider the process task designers use to design their workflows. Typically, they will quantitatively experiment with several alternative workflows to accomplish the task, but choose a single one for the production runs (e.g. the workflow that achieves the best performance during early testing). In the simplest case, alternative workflows may differ only in their user interfaces or instructions; in other cases, workers may be asked to solve a problem with a very different set of steps.
     </paragraph>
     <paragraph>
      Unfortunately, this seemingly natural design paradigm does not achieve the full potential of crowdsourcing. Selecting a single best workflow is suboptimal, because alternative workflows can compose synergistically to attain higher-quality results. While a given workflow may have the best performance on average, it is not necessarily best on every problem.
     </paragraph>
     <paragraph>
      Suppose after gathering some answers for a task, one wishes to further increase oneʼs confidence in the results; which workflow should be invoked? Due to the very fact that it is different, an alternative workflow may offer independent evidence, and this can significantly bolster oneʼs confidence in the answer. If the “best” workflow is giving mixed results for a task, then an alternative workflow is often the best way to disambiguate. Instead of selecting one a priori best workflow, a better solution should reason about this potential synergy. Our POMDP model automatically tracks the expected accuracy of alternative workflows, switching away from the “on average best” workflow if intermediate results lead it to conclude another might be superior for the particular instance of the task at hand.
     </paragraph>
     <section label="5.1">
      <section-title>
       Probabilistic model for multiple workflows
      </section-title>
      <paragraph>
       We extend our generative model of worker responses to incorporate the existence of multiple workflows to complete the same task. It includes multiple, workflow-specific error parameters for each worker and workflow-specific difficulties.
      </paragraph>
      <paragraph>
       Now, there are K alternative workflows that a worker could use to arrive at an answer. Let {a mathematical formula}dk∈[0,1] denote the inherent difficulty of completing a task using workflow k, and let {a mathematical formula}γwk∈[0,∞) be worker wʼs error parameter for workflow k. Notice that every worker has K error parameters. Having several parameters per worker incorporates the insight that some workers may perform well when asked the question in one way (e.g., visually) but not so well when asked in a different way (e.g., when asked in English, since their command of that language may not be great).
      </paragraph>
      <paragraph>
       The accuracy of a worker w, {a mathematical formula}a(dk,γwk), is the probability that she produces the correct answer using workflow k. We rewrite our original definition of worker accuracy accordingly:{a mathematical formula}Fig. 17 illustrates the plate notation for our generative model, which encodes a Bayes Net for responses made by W workers on T tasks, all of which can be solved using a set of K alternative workflows. The correct answer, v, the difficulty parameter, d, and the error parameter, γ, influence the final answer, b, that a worker provides, which is the only observed variable. d is generated by δ, a K-dimensional random variable describing a joint distribution on workflow difficulties. The answer {a mathematical formula}bwk that worker w with error parameter {a mathematical formula}γwk provides for a task using workflow k is governed by the following equations:{a mathematical formula}{a mathematical formula} As before, an underlying assumption is that given the workflow difficulty, {a mathematical formula}dk, and the true answer v, the {a mathematical formula}bwkʼs are independent of each other. δ encodes the assumption that workflows may not be independent of each other. The fact that one workflow is easy might imply that a related workflow is easy. Finally, we still assume that the workers do not collaborate with each other and that they are not adversarial, i.e., they do not purposely submit incorrect answers.
      </paragraph>
      <paragraph>
       Now we discuss how to dynamically switch between workflows to obtain the highest accuracy for a given task.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       A decision-theoretic agent
      </section-title>
      <paragraph>
       In this section, we answer the following question: Given a specific task that can be accomplished using alternative workflows, how do we design an agent that can leverage the availability of these alternatives by dynamically switching between them, in order to achieve a high-quality solution? We design an automated agent, named AgentHunt, that uses the following POMDP.
      </paragraph>
      <paragraph label="Definition 7">
       The POMDP is a six-tuple {a mathematical formula}〈S,A,T,R,O,P〉, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}S={(d1,d2,…,dK,v)|dk∈[0,1],v∈{0,1}}{a mathematical formula}dk is the difficulty of the kth workflow. v is the true answer.
       </list-item>
       <list-item label="•">
        {a mathematical formula}A={ballot1,ballot2,…,ballotK,submittrue,submitfalse}.
       </list-item>
       <list-item label="•">
        {a mathematical formula}R:S×A→R is described below.
       </list-item>
       <list-item label="•">
        {a mathematical formula}T: Identity map.
       </list-item>
       <list-item label="•">
        {a mathematical formula}O={true,false} is a Boolean answer received for a ballot question.
       </list-item>
       <list-item label="•">
        {a mathematical formula}P:S×O→[0,1] is defined by our generative model.
       </list-item>
      </list>
      <paragraph>
       As in the POMDP describing our simple binary classification workflow, the reward function maintains the value of submitting a correct answer and the penalty for submitting an incorrect answer. Additionally, it maintains a cost that AgentHunt incurs when it creates a job. We can modify the reward function to match our desired budgets and accuracies.
      </paragraph>
      <paragraph>
       Fig. 18 is a flow-chart of decisions that AgentHunt has to take.
      </paragraph>
      <paragraph>
       As before, we assume that every future worker is an average worker. In other words, for a given workflow k, every future worker has an error parameter equal to {a mathematical formula}γ¯k=1W∑wγwk where W is the number of workers.
      </paragraph>
      <paragraph>
       Also as before, after submitting an answer, AgentHunt updates its records about all the workers who participated in the task using what it believes to be the correct answer, using the following modified update rules. For a worker w who submitted an answer using workflow k, {a mathematical formula}γwk←γwk−dkα, should the worker answer correctly, and {a mathematical formula}γwk←γwk+(1−dk)α, should the worker answer incorrectly, where α is a learning rate. Any worker that AgentHunt has not seen previously begins with the average {a mathematical formula}γ¯k.
      </paragraph>
     </section>
     <section label="5.3">
      <section-title>
       Learning the model
      </section-title>
      <paragraph>
       In order to behave optimally, AgentHunt needs to learn all γ values, average worker error parameters {a mathematical formula}γ¯, and the joint workflow difficulty prior δ, which is a part of its initial belief. We consider two unsupervised approaches to learning – offline batch learning and online RL.
      </paragraph>
      <section label="5.3.1">
       <section-title>
        Offline learning
       </section-title>
       <paragraph>
        In this approach we first collect training data by having a set of workers complete a set of tasks using a set of workflows. This generates a set of worker responses, b. Since the true answer values v are unknown, an option is supervised learning, where experts label true answers and difficulties. However, this option requires significant expert time upfront, so instead, we use an EM algorithm and learn all parameters jointly.
       </paragraph>
       <paragraph>
        For EM purposes, we simplify the model by removing the joint prior δ, and treat the variables d and γ as parameters. In the E-step, we keep parameters fixed to compute the posterior probabilities of the hidden true answers: {a mathematical formula}p(vt|b,d,γ) for each task t. The M-step uses these probabilities to maximize the standard expected complete log-likelihood Q over d and γ:{a mathematical formula} where the expectation is taken over v given the old values of γ and d.
       </paragraph>
       <paragraph>
        After estimating all the hidden parameters, AgentHunt can compute {a mathematical formula}γ¯k for each workflow k by taking the average of all the learned {a mathematical formula}γk parameters. Then, to learn δ, we can fit a Truncated Multivariate Normal distribution to the learned d. This difficulty prior determines a part of the initial belief state of AgentHunt. We complete the initial belief state by assuming the correct answer is distributed uniformly among the 2 alternatives.
       </paragraph>
      </section>
      <section label="5.3.2">
       <section-title>
        Online reinforcement learning
       </section-title>
       <paragraph>
        Offline learning of our model can be very expensive, both temporally and monetarily. Moreover, we cannot be sure how much training data is necessary before the agents are ready to act in the real-world. An ideal AI agent will learn while acting in the real-world, tune its parameters as it acquires more knowledge, while still producing meaningful results. We modify AgentHunt to build its RL twin, AgentHuntRL, which is able to accomplish tasks right out of the box.
       </paragraph>
       <paragraph>
        AgentHuntRL starts with uniform priors on difficulties of all workflows. When it begins a new task, it uses the existing parameters to recompute the best policy and uses that policy to guide the next set of decisions. After completing the task, AgentHuntRL recalculates the maximum-likelihood estimates of the parameters γ and d using EM as above. The updated parameters define a new POMDP for which our agent computes a new policy for the future tasks. This relearning and POMDP-solving can be time-consuming, but we do not have to relearn and resolve after completing every task. We can easily speed the process by solving a few tasks before launching a relearning phase.
       </paragraph>
       <paragraph>
        As in all of RL, AgentHuntRL must also make a tradeoff between taking possibly suboptimal actions in order to learn more about its model of the world (exploration), or taking actions that it believes to be optimal (exploitation). AgentHuntRL uses a modification of the standard ϵ-greedy approach [62]. With probability ϵ, AgentHuntRL will uniformly choose between suboptimal actions. The exception is that it will never submit an answer that it believes to be incorrect, since doing so would not help it learn anything about the world.
       </paragraph>
      </section>
     </section>
     <section label="5.4">
      <section-title>
       Experiments
      </section-title>
      <paragraph>
       This section addresses the following three questions. (1) In practice, how much value can be gained from switching between different workflows for a task? (2) What is the tradeoff between cost and accuracy? and (3) Previous decision-theoretic crowdsourcing systems have required an initial training phase; can reinforcement learning provide similar benefits without such training? We choose an NLP labeling task, for which we create {a mathematical formula}K=2 alternative workflows (described below). To answer the first two questions, we compare two agents: TurKontrol0, which we will now refer to from now on as TurKontrol, a state-of-the-art controller for optimizing the execution of a single (best) workflow, and our AgentHunt, which can switch between the two workflows dynamically. We first compare them in simulation; then we allow the agents to control live workers on Amazon Mechanical Turk.
      </paragraph>
      <paragraph>
       We answer the third question by comparing AgentHunt with AgentHuntRL.
      </paragraph>
      <section label="5.4.1">
       <section-title>
        Implementation
       </section-title>
       <paragraph>
        The POMDP must manage a belief state over the cross product of the Boolean answer and two continuous variables – the difficulties of the two workflows. Since solving a POMDP with a continuous state space is challenging, we discretize difficulty into eleven possible values, leading to a (world) state space of size {a mathematical formula}2×11×11=242. To solve POMDPs, we run the ZMDP package{sup:17} for 300 s using the default Focused Real-Time Dynamic Programming search strategy [57]. Since we can cache the complete POMDP policy in advance, AgentHunt can control workflows in real time.
       </paragraph>
       <paragraph>
        Since we have discretized difficulty, we also modify the learning process slightly. After we learn all values of d, we round the values to the nearest discretizations and construct a histogram to count the number of times every state appears in the training data. Then, before we use the implicit joint distribution as the agentʼs starting belief state, we smooth it by adding 1 to every bin (Laplace smoothing).
       </paragraph>
      </section>
      <section label="5.4.2">
       <section-title>
        Evaluation task: NER tagging
       </section-title>
       <paragraph>
        In order to test our agents, we select a task that is needed by several colleagues: labeling training data for named-entity recognition (NER) [52]. NER tagging is a common problem in NLP and information extraction: given a body of text (e.g., “Barack Obama thinks this research is not bad.”) and a subsequence of that text (e.g., “Barack Obama”) that specifies an entity, output a set of tags that classify the type of the entity (e.g., person, politician). Since machine learning techniques are used to create production NER systems, large amounts of labeled data (of the form described above) are needed. Obtaining the most accurate training data at minimal cost, is therefore, an excellent test of our methods.
       </paragraph>
       <paragraph>
        In consultation with NER domain experts we develop two workflows for the task (Fig. 19). Both workflows begin by providing users with a body of text and an entity, like “Nixon concluded five days of private talks with Chinese leaders in Beijing.” The first workflow, called “WikiFlow,” first uses Wikification[41], [46] to find a set of possible Wikipedia articles describing the entity, such as “Nixon (film)” and “Richard Nixon.” It displays these articles (including the first sentence of each article) and asks workers to choose the one that best describes the entity. Finally, it returns the Freebase{sup:18} tags associated with the Wikipedia article selected by the worker.
       </paragraph>
       <paragraph>
        The second workflow, “TagFlow,” asks users to choose the best set of Freebase tags directly. For example, the Freebase tags associated with “Nixon (film)” is {a mathematical formula}{/film/film}, while the tags associated with “Richard Nixon” includes {a mathematical formula}{/people/person,/government/us−congressperson}. TagFlow displays the tag sets corresponding to the different options and asks the worker to choose the tag set that best describes the entity mentioned in the sentence.
       </paragraph>
      </section>
      <section label="5.4.3">
       <section-title>
        Experimental setup
       </section-title>
       <paragraph>
        First, we gather training data using Mechanical Turk. We generate 50 NER tasks. For each task, we submit 40 identical WikiFlow jobs and 40 identical TagFlow jobs to Mechanical Turk. At $0.01 per job, the total cost is $60.00 including Amazon commission. Using our EM technique, we then calculate average worker accuracies, {a mathematical formula}γ¯WF and {a mathematical formula}γ¯TF, corresponding to WikiFlow and TagFlow respectively. Somewhat to our surprise, we find that {a mathematical formula}γ¯TF=0.538&lt;γ¯WF=0.547 – on average, workers found TagFlow to be very slightly easier than WikiFlow. Note that this result implies that AgentHunt will always create a TagFlow job to begin a task. We also note that the difference between {a mathematical formula}γ¯TF and {a mathematical formula}γ¯WF influences the switching behavior of AgentHunt. Intuitively, if AgentHunt were given two workflows whose average difficulties were further apart, AgentHunt would become more reluctant to switch to a harder workflow. Because we find TagFlow jobs to be slightly easier, for all experiments, we set TurKontrol so it creates TagFlow jobs. We also use this training data to construct both agentsʼ initial beliefs.
       </paragraph>
      </section>
      <section label="5.4.4">
       <section-title>
        Experiments using simulation
       </section-title>
       <paragraph>
        We first run our agents in a simulated environment. On each run, the simulator draws states from the agentsʼ initial belief distributions. We fix the reward of returning the correct answer to 0, and vary the reward (penalty) of returning an incorrect answer between the following values: −10, −100, −1000, and {a mathematical formula}−10000. We set the cost of creating a job for a (simulated) worker to −1. We use a discount factor of 0.9999 in the POMDP. For each setting of reward values we run 1000 simulations and report mean net utilities (Fig. 21).
       </paragraph>
       <paragraph>
        We find that when the stakes are low, the two agents behave almost identically. However, as the penalty for an incorrect answer increases, AgentHuntʼs ability to switch between workflows allows it to capture much more utility than TurKontrol. As expected, both agents submit an increasing number of jobs as the importance of answer correctness rises and in the process, both of their accuracies rise too (Fig. 20). However, while both agents become more accurate, TurKontrol does not increase its accuracy enough to compensate for the exponentially growing penalties. Instead, as AgentHunt experiences an almost sublinear decline in net utility, TurKontrol sees an exponential drop (Fig. 21).
       </paragraph>
       <paragraph>
        A Studentʼs t-test shows that for all settings of penalty except −10, the differences between the two systemsʼ average net utilities are statistically significant. When the penalty is −10, {a mathematical formula}p&lt;0.4, and at all other reward settings, {a mathematical formula}p&lt;0.0001. Thus we find that at least in simulation, AgentHunt outperforms TurKontrol on all our metrics.
       </paragraph>
       <paragraph>
        We also analyze the systemsʼ behaviors qualitatively. As expected, AgentHunt always starts by creating a TagFlow job, since {a mathematical formula}γ¯TF&lt;γ¯WF implies that TagFlows lead to higher worker accuracy on average. Interestingly, although AgentHunt has more available workflows, it creates fewer actual jobs than TurKontrol, even as correct answers become increasingly important. We also split our problems into three categories to better understand the agentsʼ behaviors: 1) TagFlow is easy, 2) TagFlow is hard, but WikiFlow is easy, and 3) Both workflows are difficult.
       </paragraph>
       <paragraph>
        In the first case, both agents terminate quickly, though AgentHunt spends a little more money since it also requests WikiFlow jobs to double-check what it learns from TagFlow jobs. In the second case, TurKontrol creates an enormous number of jobs before it decides to submit an answer, while AgentHunt terminates much faster, since it quickly deduces that TagFlow is hard and switches to creating easy WikiFlow jobs. In the third case, AgentHunt expectedly creates more jobs than TurKontrol before terminating, but AgentHunt does not do too much worse than TurKontrol, since it correctly deduces that gathering more information is unlikely to help.
       </paragraph>
      </section>
      <section label="5.4.5">
       <section-title>
        Experiments using Mechanical Turk
       </section-title>
       <paragraph>
        We next run the agents on real data gathered from Mechanical Turk. We generate 106 new NER tasks for this experiment, and use gold labels supplied by a single expert. Since in our simulations we found that the agents spend on average about the same amount of money when the reward for an incorrect answer is −100, we use this reward value in our real-world experiments.
       </paragraph>
       <paragraph>
        As Table 3 shows, AgentHunt fares remarkably better in the real-world than TurKontrol. A Studentʼs t-test shows that the difference between the average net utilities of the two agents is statistically significant with {a mathematical formula}p&lt;0.03. However, we see that TurKontrol spends less, leading one to naturally wonder whether the difference in utility can be accounted for by the cost discrepancy. Thus, we modify the reward for an incorrect answer (to −300) for TurKontrol to create TurKontrol300, which spends about the same amount of money as AgentHunt.
       </paragraph>
       <paragraph>
        But even after the modification, the accuracy of AgentHunt is still much higher. A Studentʼs t-test shows that the difference between the average net utilities of AgentHunt and TurKontrol300 is statistically significant at {a mathematical formula}p&lt;0.01 showing that in the real-world, given similar budgets, AgentHunt produces significantly better results than TurKontrol. Indeed, AgentHunt reduces the error of TurKontrol by 45% and the error of TurKontrol300 by 50%. Surprisingly, the accuracy of TurKontrol300 is lower than that of TurKontrol despite the additional jobs; we attribute this to statistical variance.
       </paragraph>
      </section>
      <section label="5.4.6">
       <section-title>
        Adding reinforcement learning
       </section-title>
       <paragraph>
        Finally, we compare AgentHunt to AgentHuntRL. AgentHuntRLʼs starting belief state is a uniform distribution over all world states and it assumes that {a mathematical formula}γ¯k=1 for all workflows. To encourage exploration, we set {a mathematical formula}ϵ=0.1. We test it using the same 106 tasks described above. Table 4 reproduces the results of AgentHunt from Table 3 alongside those of AgentHuntRL. We see that while AgentHuntRL achieves a slightly higher accuracy than AgentHunt, the difference between their net utilities is not statistically significant ({a mathematical formula}p=0.4), which means AgentHuntRL is comparable to AgentHunt, suggesting that AgentHunt can perform in an “out of the box” mode, without needing a training phase.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="6">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      Modeling repeated labeling in the face of noisy workers has received significant attention, but we are the first to use decision theory to dynamically poll for more labels as necessary. Dawid et al. [14] and Romney et al. [48] are one of the firsts to incorporate a worker accuracy model to improve label quality. Raykar et al. [47] propose a model in which the parameters for worker accuracy depend on the true answer. Whitehill et al. [67] address the concern that worker labels should not be modeled as independent of each other unless given problem difficulty. Welinder et al. [66] design a multidimensional model for workers that takes into account competence, expertise, and annotator bias. Kamar et al. [27] extract features from the task at hand and use Bayesian Structure Learning to learn the worker response model. Parameswaran et al. [43] conduct a policy search to find an optimal dynamic control policy with respect to constraints like cost or accuracy. Karger et al. [28] develop an algorithm to generate a graph that represents an assignment of tasks to workers and infers correct answers based on low-rank matrix approximation. Given the assumptions that all tasks share an equal level of difficulty and that workers are either always correct or randomly guess, they analytically prove the optimality of their algorithm at minimizing a budget for a target error rate with respect to any other possible algorithm using their assumptions. Our methods use more general models which fall outside their assumptions, but provide the utility maximization guarantees that any POMDP model inherits. Wauthier et al. [64] treat the entire crowdsourcing pipeline from data collection to learning within a Bayesian framework. Kajino et al. [26] formulate the repeated labeling problem as a convex optimization problem. Lin et al. [35] address the case when either requesters cannot enumerate all possible answers for the worker or when the solution space is infinitely large.
     </paragraph>
     <paragraph>
      Other innovative workflows have been designed for complex tasks, for example, find–fix–verify for an intelligent editor [5], iterative dual pathways for speech-to-text transcription [34] and others for counting calories on a food plate [42]. Lasecki et al. [32] design a system that allows multiple users to control the same interface in real-time. Control can be switched between users depending on who is doing better. Kulkarni et al. [30] show the crowd itself can help with the design and execution of complex workflows.
     </paragraph>
     <paragraph>
      Ipeirotis et al. [23] observe that workers tend to have bias on multiple-choice, annotation tasks. They learn a confusion matrix to model the error distribution of individual workers. However, their model assumes workersʼ errors are completely independent, whereas, our model handles situations where workers make correlated errors due to the intrinsic difficulty of the task.
     </paragraph>
     <paragraph>
      Kulkarni et al. [31] design an alternative crowdsourcing platform that is not a marketplace. Instead, it intelligently routes work to workers and controls for accuracy and quality. Ho et al. [20] consider how to assign tasks to workers on arrival.
     </paragraph>
     <paragraph>
      Huang et al. [22] look at the problem of designing a task under budget and time constraints. They illustrate their approach on an image-tagging task. By wisely setting variables, such as reward per task and the number of labels requested per image, they increase the number of useful tags acquired. Donmez et al. [16] observe that workersʼ accuracies often change over time (e.g., due to fatigue, mood, task similarity, etc). Rzeszotarski et al. [51] propose to use micro-breaks to overcome fatigue effects. As these approaches are orthogonal to ours, we would like to integrate the methods in the future.
     </paragraph>
     <paragraph>
      Shahaf and Horvitz [53] develop an HTN-planner style decomposition algorithm to find a coalition of workers, each with different skill sets, to solve a task. Some members of the coalition may be machines and others humans; different skills may command different prices. In contrast to our work, Shahaf and Horvitz do not consider methods for learning models of their workers.
     </paragraph>
     <paragraph>
      The benefits from combining disparate workflows have been previously observed. Babbageʼs Law of Errors suggests that the accuracy of numerical calculations can be increased by comparing the outputs of two or more methods [19]. However, in previous work these workflows have been combined manually; AgentHunt embodies the first method for automatically evaluating potential synergy and dynamically switching between workflows.
     </paragraph>
    </section>
    <section label="7">
     <section-title>
      Discussion and future work
     </section-title>
     <paragraph>
      We believe that AI has the potential to impact the growing thousands of requesters who use crowdsourcing to make their processes more efficient. Our work can be extended in several important directions towards realizing this vision. We list a few below.
     </paragraph>
     <paragraph label="Task economic uncertainty">
      We have so far assumed that the price for each task is a constant and an input. However, figuring out the actual value of a task is an important problem. This problem is difficult due to the highly dynamic nature of crowdsourcing markets. At the supply end, the value of a job can be influenced by workersʼ expertise, motivation of work, and personal interests. First, we plan to explore approaches that estimate value based on passively observing workflow activity. As a first step we will develop models that allow for the estimation of an effective hourly wage for a job. This problem is non-trivial given the many sources of variance including network delay, a workerʼs technology adequacy, whether a worker is multi-tasking, etc. Second, we will explore active algorithms for estimating these models. These approaches will actively experiment with payment choices in order to more efficiently learn a model. Further, for both the active and passive cases, we plan to extend this model to handle tasks with time constraints. This will require modeling the relationship between the amount of payment and the task completion speed.
     </paragraph>
     <paragraph label="Incentives">
      Paying an optional bonus for exceptional work can be a viable way of motivating high-quality results and forming long-term collaborations with competent workers. Questions that need to be answered are; (1) When to pay a bonus? (2) Who should receive a bonus? and (3) How much should the bonus be? An intuitive way is to pay a portion of the monetary savings of a good result. Another form of incentive involves recognition, such as leader boards. Investigations need to be carried out to determine which (combination of) bonus forms (and magnitude) can be the most effective. Here again we plan to study both passive and active approaches for modeling incentive structure in workflows.
     </paragraph>
     <paragraph label="A universal worker model">
      A workerʼs productivity varies over time. This is due to her fatigue level, variance of mood and work efficiency during the day, or the skillfulness acquired during her experiences with similar tasks. We plan to generalize our worker model by integrating several temporal parameters: (1) the time of the day and the day of the week or year, (2) a workerʼs consecutive working hours, and (3) her cumulative hours on the same type of tasks. Further, the performance of any crowdsourcing system can be lowered by noisy answers provided by spammers, or even malicious workers. An important aspect of our model will be to support the inference of spamming and malicious intent.
     </paragraph>
     <paragraph label="Example selection for reinforcement learning">
      Our work does not consider the problem of carefully curating the reinforcement learning process whether it be online or offline. For example, given a set of tasks, we do not know how large a subset of tasks we should solve before we update parameters, nor do we know which subset of tasks to use, and in which order. All these factors can affect the ability of our agent to learn parameters accurately and quickly. We wish to investigate these questions in future research.
     </paragraph>
     <paragraph label="Additive utility function">
      Our utility function is additive, in that each task instance is treated independently of each other. The total utility that we achieve for a set of tasks is equal to the sum of the utilities we achieve for each task. Such an assumption may not hold in other scenarios, e.g., if we are trying to train a machine learning classifier using crowdsourced labels. Here, an active learning formalism may be more appropriate [15], [55], [69].
     </paragraph>
     <paragraph label="Abstractness of utility">
      To make our work usable for the vast majority, we must be able to elicit a utility function from our users. This is not an issue just with our approach, but with all decision-theoretic formalizations. The concept of utility is abstract, and as such, cannot be easily constructed even with domain knowledge. An extension of this work could consider ways to derive utilities from budgetary or accuracy constraints, which are far more concrete and comprehensible. Alternatively, we may cast this as a utility elicitation problem [9].
     </paragraph>
     <paragraph label="General-purpose crowdsourcing control">
      Our long-term research goal is to automatically control workflows consisting of a broader range of tasks [65]. To achieve this goal, we need to specify a general language for workflows, and develop methods for translating workflow specifications into decision problems. We must also build a worker model for each task type. However, we believe that many crowdsourcing tasks can be captured with a relatively small number of task classes, such as tasks with discrete alternatives, content generation, etc. By having a task library we can share parameters across similar tasks. Given a new task, we can transfer knowledge from related tasks in the library, thereby simplifying and expediting the model training process. We are currently developing a general-purpose workflow controller called CLOWDER, which, given any new workflow, will automatically convert it into a POMDP and control it for the requester. Such a software will be extremely important in realizing our vision, since then our techniques will be easily accessible to requesters who do not have advanced AI knowledge.
     </paragraph>
    </section>
    <section label="8">
     <section-title>
      Conclusions
     </section-title>
     <paragraph>
      This work applies modern planning and machine learning techniques to the quality control problem of crowdsourcing, a new and rapidly-growing method for accomplishing various tasks, and demonstrates the usefulness of decision theory in a real-world setting, by controlling dynamic workflows that successfully achieve statistically-significant, higher-quality results than traditional, non-adaptive workflows. In particular, we make the following contributions.{sup:19}
     </paragraph>
     <list>
      <list-item label="1.">
       As complex workflows have become more commonplace in crowdsourcing and are regularly employed for high-quality output, we introduce an exciting new application for artificial intelligence, the control of these workflows.
      </list-item>
      <list-item label="2.">
       We use POMDPs to model single workflows as well as multiple workflows, and define generative models that govern various observation models of the processes. Our agents implement our mathematical framework and use it to optimize and control selection and execution of workflows. Extensive simulations and real-world experiments show that our agents are robust in a variety of scenarios and parameter settings, and achieve higher utilities than previous, non-adaptive policies.
      </list-item>
      <list-item label="3.">
       We present efficient and cheap mechanisms for learning model parameters from limited and noisy training data. We validate the parameters independently and show that our learned models significantly outperform the popular majority-vote baseline when resources are constrained.
      </list-item>
     </list>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Snapshots of tasks
     </section-title>
    </section>
   </appendices>
  </root>
 </body>
</html>