<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Semantic representation, i.e., modeling the semantics of a linguistic item{sup:2} in a mathematical or machine interpretable form, is a fundamental problem in Natural Language Processing (NLP) and Artificial Intelligence (AI). Because they represent the lowest linguistic level, word senses play a vital role in natural language understanding. Effective representations of word senses can be directly useful to Word Sense Disambiguation [93], semantic similarity [13], [129], [106], coarsening sense inventories [92], [124], alignment of lexical resources [101], [98], [108], lexical substitution [75], and semantic priming [100]. Moreover, sense-level representation can be directly extended to applications requiring word representations, with the added benefit that it provides extra semantic information. Turney and Pantel [129] provide a review of some of the applications of word representation, including: automatic thesaurus generation [21], [22], word similarity [25], [128], [113] and clustering [103], query expansion [140], information extraction [61], semantic role labeling [29], [104], spelling correction [53], and Word Sense Disambiguation [93].
     </paragraph>
     <paragraph>
      The Vector Space Model (VSM) is a prominent approach for semantic representation. The model represents a linguistic item as a vector (or a point) in an n-dimensional semantic space, i.e., a mathematical space wherein each of the n dimensions (hence, axes of the space) denotes a single linguistic entity, such as a word. The popularity of the VSM representation is due to two main reasons. Firstly, it is straightforward to view vectors as sets of features and directly apply various machine learning techniques on them. Secondly, the model enjoys support from the field of Cognitive Science wherein several studies have empirically or theoretically suggested that various aspects of human cognition accord with VSMs [36], [64].
     </paragraph>
     <paragraph>
      However, most VSM-based techniques, whether in their conventional co-occurrence based form [119], [129], [63], or in their newer predictive branch [20], [81], [8], usually base their computation on the distributional statistics derived from text corpora. Hence, in order to be able to represent individual meanings of words (i.e., word senses), these techniques require large amounts of disambiguated text prior to modeling. Additionally, Word Sense Induction techniques [103], [11], [58], [27] require sense-annotated data, if their induced sense clusters are to be mapped to an existing sense inventory. However, providing sense-annotated data on a large scale is a time-consuming process which has to be carried out separately for each word sense and repeated for each new language of interest, i.e., the so-called knowledge acquisition bottleneck. Importantly, the largest manual effort for providing a wide-coverage sense-annotated dataset dates back to 1993, in the case of the SemCor corpus [85]. In fact, although cheap and fast annotations could be obtained by means of Amazon Mechanical Turk [123], [55], games with a purpose [133], [131], [56], or voluntary collaborative editing such as in Wikipedia [77], producing annotated resources manually is still an onerous task. On the other hand, the performance of Word Sense Disambiguation (WSD) techniques is still far from ideal [93], which in its turn prevents a reliable automatic sense-annotation of large text corpora that can be used for modeling individual word senses. This hinders the functionality of this group of vector space models in tasks such as WSD that require the representation of individual word senses.
     </paragraph>
     <paragraph>
      There have been several efforts to adapt and apply distributional approaches to the representation of word senses [103], [12], [114], [47], [68]. However, most of these techniques cannot provide representations that are already linked to a standard sense inventory, and consequently such mapping has to be carried out either manually, or with the help of sense-annotated data [48]. Recently, there have been attempts to address this issue and to obtain vectors for individual word senses by exploiting the WordNet semantic network [74], [106], [108], [116] and its glosses [19]. These approaches, however, are either restricted to the representation of concepts defined in WordNet and to the English language only, or are designed for specific tasks.
     </paragraph>
     <paragraph>
      In our recent work [16], we proposed a method that exploits the structural knowledge derived from semantic networks, together with distributional statistics from text corpora, to produce effective representations of individual word senses or concepts. Our approach provides two main advantages in comparison to previous VSM techniques. Firstly, it is multilingual, as it can be directly applied for the representation of concepts in dozens of languages. Secondly, each vector represents a concept, irrespective of its language, in a unified semantic space having concepts as its dimensions, permitting direct comparison of different representations across languages and hence enabling cross-lingual applications.
     </paragraph>
     <paragraph>
      In this article, we improve our approach, referred to as Nasari (Novel Approach to a Semantically-Aware Representation of Items) henceforth, and extend their application to a wider range of tasks in lexical semantics. Specifically, the novel contributions are as follow:
     </paragraph>
     <list>
      <list-item label="1.">
       We propose a new formulation for fast computation of lexical specificity (Section 3.1.1).
      </list-item>
      <list-item label="2.">
       We propose a new flexible way to get continuous embedded vector representations, with the added benefit of obtaining a semantic space shared by BabelNet synsets, words and texts (Section 3.3).
      </list-item>
      <list-item label="3.">
       We put forward a technique for improved computation of weights in the unified vectors and show how it can improve the accuracy and efficiency of the representations (Section 3.4).
      </list-item>
      <list-item label="4.">
       We compute and assign weights to individual edges in our semantic network (Section 4.1) and show by means of different experiments the advantage we gain when using this new weighted graph (Section 10).
      </list-item>
      <list-item label="5.">
       We release the lexical and unified vector representations for five different languages (English, French, German, Italian and Spanish) and the embedded vector representations for the English language at http://lcl.uniroma1.it/nasari/.
      </list-item>
     </list>
     <paragraph>
      In addition to these contributions, we also devised robust frameworks that enable direct application of our representations to four different tasks: Semantic Similarity (Section 6), Sense Clustering (Section 7), Domain Labeling (Section 8) and Word Sense Disambiguation (Section 9). For each of the tasks, we carried out a comprehensive set of evaluations on several datasets in order to verify the reliability and flexibility of Nasari different datasets and tasks. We provide a summary of the experiments in Section 5.
     </paragraph>
     <paragraph>
      The rest of this article is structured as follows. We first provide an introduction of some of the most widely used knowledge resources in lexical semantics, in Section 2. After which, in Section 3 we describe our methodology to convert text into lexical, embedded and unified vectors. The process to obtain vector representations for synset vectors by leveraging the knowledge resources described in Section 2, and the methodology to obtain vectors from text described in Section 3, is presented in Section 4. We present a summary of the experiments and the performance of Nasari across tasks in Section 5. Then, we describe some applications of the vectors with their respective frameworks and experiments in Sections 6 (Semantic Similarity), 7 (Sense Clustering), 8 (Domain Labeling), and 9 (Word Sense Disambiguation). We analyze the performance of different components of our model in Section 10. Finally, we discuss the related work in Section 11 and provide the concluding remarks in Section 12.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Knowledge resources
     </section-title>
     <paragraph>
      Knowledge resources can be divided into two general categories: expert made and collaboratively constructed. Each type has its own advantages and limits. Manually-annotated resources feature highly-accurate encoding of concepts and semantic relationships between them but, with a few exceptions, are usually limited in their lexical coverage, and are typically focused on a specific language only. A good example is WordNet[83], a semantic network whose basic units are synsets. A synset represents a concept which may be expressed through nouns, verbs, adjectives or adverbs and is composed of the different lexicalizations (i.e., synonyms that are used to express it). For example, the synset of the middle of the day concept comprises six lexicalizations: noon, twelve noon, high noon, midday, noonday, noontide. Synsets may also be seen as nodes in a semantic network. These nodes are connected to each other by means of lexical or semantic relations (hypernymy, meronymy, etc.). These relations are seen as the edges in the WordNet semantic network. Despite being one of the largest and most complete manually-made lexical resources, WordNet still lacks coverage of lemmas and senses from domain specific lexicons (e.g., law or medicine), named entities, creative slang usages, or those for technology that came into existence only recently.
     </paragraph>
     <paragraph>
      On the other hand, collaboratively-constructed resources, such as Wikipedia, provide features such as multilinguality, wide coverage and up-to-dateness. As of September 2015, Wikipedia provides more than 100K articles in over fifty languages. This coverage is steadily increasing. For instance, the English Wikipedia alone receives 750 new articles per day. Each of these articles provides, for its corresponding concept, a great deal of information in the form of textual information, tables, infoboxes, and various relations (such as redirections, disambiguations, and categories). These features have persuaded many researchers over the past few years to exploit the huge amounts of semi-structured knowledge available in such collaborative resources for different NLP applications [46], [125].
     </paragraph>
     <paragraph>
      The types of knowledge available in the expert-based and collaboratively-constructed resources make them complementary. This has motivated researchers to combine various lexical resources across the two categories [101], [108]. A prominent example is BabelNet[98], which provides a mapping of WordNet to a number of collaboratively-constructed resources, including Wikipedia. The structure of BabelNet{sup:3} is similar to that of WordNet. Synsets are the main linguistic units and are connected to other semantically related synsets, whose lexicalizations are multilingual in this case. For instance, the synset corresponding to United States is represented with a set of multilingual lexicalizations including United_StatesEN, United_States_of_AmericaEN, AmericaEN, U.S.EN, and U.S.A.EN in English, Estados_UnidosES, Estados_Unidos_de_AméricaES, EEUUES, E.E.U.U.ES, and EE. UU.ES in Spanish, and Stati_Uniti_d'AmericaIT, Stati_UnitiIT, AmericaIT, and U.S.A.IT in Italian. The relations between synsets are the ones coming from WordNet (hypernyms, hyponyms, etc.), plus new relations coming from other resources such as Wikipedia hyperlinks and WikiData{sup:4} relations (e.g. Madrid capital of Spain). BabelNet is the largest multilingual semantic network available, containing 13,789,332 synsets (6,418,418 concepts and 7,370,914 named entities) and 354,538,633 relations for 271 languages.{sup:5} For the English language, BabelNet contains 4,403,148 synsets with at least one Wikipedia page associated and 117,653 synsets with one WordNet synset associated, from which 99,705 synsets are composed of both a Wikipedia page and a WordNet synset.
     </paragraph>
     <paragraph>
      The gist of our approach lies in its combination of different types of knowledge from complementary resources. Specifically, our representation approach utilizes the following sources of knowledge: lexico-semantic relations in WordNet, BabelNet's mapping of WordNet synsets and Wikipedia articles, texts within Wikipedia articles and the inter-article links of Wikipedia. In our experiments we used WordNet 3.0 which covers more than 117K unique nouns in about 80K synsets, the Wikipedia dump of December 2014, and BabelNet 3.0, which covers 271 languages and contains over 13 million synsets.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Representing texts as vectors
     </section-title>
     <paragraph>
      One of the contributions of this article is the framework we are proposing for transforming texts into three different kinds of vector: lexical, embedded and unified. Our lexical vectors follow the conventional approach for representing a linguistic item in a semantic space with words as its dimensions [103] (multiword expressions are also considered). The weights in these vectors are usually computed on the basis of raw term frequencies (tf) or normalized frequencies, such as tf-idf[52]. Instead, we use lexical specificity for the computation of the weights in our lexical vectors. Having a solid statistical basis, lexical specificity provides several advantages over the previously mentioned measures [17] (see Section 10 for a comparison of lexical specificity and tf-idf). In what follows in this section we first explain lexical specificity and propose an efficient way for its fast computation (Section 3.1). We then provide more details of our three types of vector, i.e., lexical (Section 3.2), embedded (Section 3.3) and unified (Section 3.4).
     </paragraph>
     <section label="3.1">
      <section-title>
       Lexical specificity
      </section-title>
      <paragraph>
       Lexical specificity [62] is a statistical measure based on the hypergeometric distribution.{sup:6} The measure has been widely used in different NLP applications including term extraction [28], textual data analysis [66] and domain-based term disambiguation [14], [10], but it has rarely been used to measure weights in a vector space model. Lexical specificity essentially computes the set of most representative words for a given text based on the hypergeometric distribution. In our setting, we are interested in representing a given text, hereafter referred to as the sub-corpus {a mathematical formula}SC, through a vector comprising the weighted set of its most relevant words or concepts. In order to compute lexical specificity, we need a reference corpus {a mathematical formula}RC which should be a superset of {a mathematical formula}SC. Lexical specificity computes the weights for each word by contrasting the frequencies of that word across {a mathematical formula}SC and {a mathematical formula}RC.
      </paragraph>
      <paragraph>
       Following the notation of [16], let T and t be the respective total number of content words in {a mathematical formula}RC and {a mathematical formula}SC, while F and f denote the frequency of a given word w in {a mathematical formula}RC and {a mathematical formula}SC, respectively. Our goal is to compute a weight quantifying the association strength of w with our text {a mathematical formula}SC. We compute the probability of a word w having a frequency equal to or higher than f in our sub-corpus {a mathematical formula}SC using a hypergeometric distribution which takes as its parameters the frequency of w in the reference corpus {a mathematical formula}RC, i.e., F, and the sizes of {a mathematical formula}RC and {a mathematical formula}SC, i.e., T and t, respectively. A word w with a high probability is one with a high occurrence chance across arbitrary subsets of {a mathematical formula}RC of size t. Hence, the representative words of a given sub-corpus will be those with low probabilities since these specific words are the most suitable ones for distinguishing the sub-corpus from the reference corpus. As a result, the computed probability is inversely proportional to the relevance of the word w to {a mathematical formula}SC. In order to make the relation directly proportional, thus making the weights more interpretable, we apply the {a mathematical formula}−log10 operation to the computed probabilities as has been customary in the literature [28], [42]. This logarithmic operation also speeds up the calculations (more details in the following section). Moreover, using {a mathematical formula}log10, instead of for instance the natural logarithm, has the added benefit of leading to an easy calculation of the prior probability. For example, if an item has a lexical specificity of 5.0, it means that the probability of observing that item in {a mathematical formula}SC is {a mathematical formula}10−5=0.00005. Therefore, the lexical specificity of w in {a mathematical formula}SC is given by the following expression:{a mathematical formula} where X represents a random variable following a hypergeometric distribution with parameters F, t and T and {a mathematical formula}P(X≥f) is defined as follows:{a mathematical formula} where {a mathematical formula}P(X=i) represents the probability of a given word to appear exactly i times in the subcorpus {a mathematical formula}SC according to the hypergeometric distribution of parameters F, t and T. We propose an efficient implementation of Equation (2) in the following section.
      </paragraph>
      <section label="3.1.1">
       <section-title>
        Efficient implementation of lexical specificity
       </section-title>
       <paragraph>
        According to Equation (2), the computation of the hypergeometric distribution involves summing {a mathematical formula}(F−f)+1 addends, each of which is calculated as follows{sup:7}:{a mathematical formula}
       </paragraph>
       <paragraph>
        Given that the summation range of Equation (2) is generally directly proportional to the size of the corpus, the computation of lexical specificity can be quite expensive on large corpora wherein the value of F tends to be very high. Lafon [62] proposed a method to reduce the computation cost of Equation (2). According to this method, one can first calculate {a mathematical formula}P(X=i) only for the smallest i (i.e., f) and then calculate the rest of probabilities, i.e., {a mathematical formula}P(X=f+1), ..., {a mathematical formula}P(X=F), using the following property of the hypergeometric distribution:{a mathematical formula}
       </paragraph>
       <paragraph>
        Lafon [62] also suggested using the well-known Stirling formula for the computation of the factorial components in Equation (3). According to the Stirling formula, the logarithm of a factorial can be approximated as follows:{a mathematical formula}
       </paragraph>
       <paragraph>
        Thanks to the application of the Stirling formula we can transform Equation (3) into a summation. Despite these improvements in the calculation of lexical specificity, there remain issues when the above computation is to be applied to a large reference corpus. One of the main problems is the multiplication of potentially very small quantities. Specifically, a 64-bit binary floating-point number, which is the one typically used in current computers, has an approximate range from {a mathematical formula}10−308 through {a mathematical formula}10+308. During the computation of lexical specificity on large corpora, the lower bound can be reached several times. Our solution to solve this problem (which even optimizes the calculations) is obtained via the next two equations. Firstly, we rewrite Equation (4) by extracting the common factor {a mathematical formula}P(X=f):{a mathematical formula} where {a mathematical formula}af=1 and {a mathematical formula}ai=ai−1(F−i)(t−i)(i+1)(T−F−t+i+1),∀i=f+1,...,F.
       </paragraph>
       <paragraph>
        Now we only need to apply the logarithm to both sides of the equation in order to transform the previous multiplication into an addition and thus avoid small values. In this way we also avoid unnecessary exponentials in the calculations of {a mathematical formula}P(X=f):{a mathematical formula}
       </paragraph>
       <paragraph>
        Therefore, according to Equation (1) and by applying a change of logarithm base, we can compute lexical specificity given the four parameters T, t, F, and f as follows:{a mathematical formula} where k is the natural logarithm of 10 (i.e., {a mathematical formula}loge⁡10).
       </paragraph>
       <paragraph>
        For computational feasibility, the {a mathematical formula}∑i=fFai sum is usually not computed until F. Instead, a stopping criterion is introduced into the loop. Since the probability mass in the tail of the hypergeometric distribution is in most cases mathematically insignificant with respect the final cumulative probability distribution, the stopping criterion is usually satisfied well before reaching to the final F value, which considerably reduces the computation time.
       </paragraph>
       <paragraph>
        As an example we show in Fig. 1 the estimated probability distribution for the word mathematics in an arbitrary sub-corpus {a mathematical formula}SC of 100,000 content words from Wikipedia. If the word mathematics occurs more than twenty times in {a mathematical formula}SC, the word is considered to be very specific to the given subcorpus, since, as we can see from Fig. 1, most of the probability mass in the hypergeometric distribution is concentrated in the left part of the distribution range. The distribution range extends until 70,029, which is the number of occurrences of the word mathematics in the whole Wikipedia. However, the probability {a mathematical formula}P(X=45) is already as small as {a mathematical formula}10−20 and rapidly gets much smaller. This illustrates the point made above, in which the right tail of the probability mass is generally insignificant to values close to the expected value, and adding a stopping condition might make the calculations much faster, while not having any noticeable effect to the final specificity score.
       </paragraph>
       <paragraph>
        The next three sections provide more details on our three types of vector and on how we leverage lexical specificity for their construction.
       </paragraph>
      </section>
     </section>
     <section label="3.2">
      <section-title>
       Lexical vector representation
      </section-title>
      <paragraph>
       So far we have explained how lexical specificity can be used to determine the relevance of words for a given text. In this section we explain how we leverage lexical specificity in order to construct a lexical vector for a given text (i.e., {a mathematical formula}SC). Throughout the article the texts considered come from Wikipedia, thus we use the whole Wikipedia as our reference corpus ({a mathematical formula}RC). Our lexical vectors have individual words as their dimensions, therefore, in our lexical semantic space, a text is represented on the basis of its association with a set of lexical items, i.e., words. By contrasting the term frequencies across {a mathematical formula}SC and {a mathematical formula}RC, we compute the lexical specificity of each term for the given subcorpus.
      </paragraph>
      <paragraph>
       Specifically, in order to compute our lexical vector {a mathematical formula}v→lex(SC), we simply iterate over all the content words in our subcorpus {a mathematical formula}SC (only words with a total frequency greater than or equal to five in the whole Wikipedia are considered) and compute lexical specificity for each of them. We then prune the resulting vectors by keeping only those words that are relevant to the target text with a confidence of 99% or more according to the hypergeometric distribution ({a mathematical formula}P(X≥f)≤0.01), as also performed in earlier works [10], [17]. Words with weights below the aforementioned threshold are considered as zero dimensions. The vector truncation step helps reduce noise. Additionally, the truncation helps in speeding up the computation of the vectors, as they will be sparse and therefore computationally easier to work with.
      </paragraph>
      <paragraph>
       In our setting we also consider multiword expressions when they appear as lexicalizations of piped links.{sup:8} Note that we apply lexical specificity to content words (nouns, verbs and adjectives) after tokenization and lemmatization, but for notational simplicity we will keep using the term “word” to refer to them.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Embedded vector representation
      </section-title>
      <paragraph>
       In recent years, semantic representation has experienced a resurgence of interest in the use of neural network-based learning, a trend usually referred to as word embeddings. In addition to their fast processing of massive amounts of text, word embeddings have proved to be reliable techniques for modeling the semantics of words on the basis of their contexts. However, the application of these word-based techniques to the representation of word senses is not trivial and is bound to the availability of large amounts of sense-annotated data. There have been efforts aimed at learning sense-specific embeddings without needing to resort to sense-annotated data, often through clustering the contexts in which a word appears [138], [47], [99]. However, the resulting representations are usually not aligned to existing sense inventories.
      </paragraph>
      <paragraph>
       We put forward an approach that allows us to plug in an arbitrary word embedding representation with that of our lexical vector representations, providing three main advantages: (1) benefiting from the word-based knowledge derived as a result of learning from massive corpora for our sense-level representation; (2) reducing the dimensionality of our lexical space to a fixed-size continuous space; and (3) providing a shared semantic space between words and synsets (more details in Section 4), hence enabling a direct comparison of words and synsets.
      </paragraph>
      <paragraph>
       Our approach exploits the compositionality of word embeddings. According to this property, a compositional phrase representation can be obtained by combining, usually averaging, its constituents' representations [82]. For instance, the vector representation obtained by averaging the vectors of the words Vietnam and capital is very close to the vector representation of the word Hanoi in the semantic space of word embeddings. Our approach builds on this property and plugs a trained word embedding-based representation into our lexical vectors.
      </paragraph>
      <paragraph>
       Specifically, given an input text {a mathematical formula}T and a space of word embeddings E, we first calculate the lexical vector of {a mathematical formula}T (i.e., {a mathematical formula}v→lex(T)) as explained in Section 3.2 and then map our lexical vector to the semantic space E as follows:{a mathematical formula} where {a mathematical formula}E(w) is the embedding-based representation of the word w in E, and {a mathematical formula}rank(w,v→lex(T)) is the rank of the dimension corresponding to the word w in the lexical vector {a mathematical formula}v→lex(T), thus giving more importance to the higher weighted dimensions. In Section 10 we compare this harmonic average giving more importance to higher weighted words over a simple average. One of the main advantages of this representation combination technique is its flexibility, since any word embedding space can be given as input. As we show in our experiments in Sections 6.1 and 7.1, this combination enables us to benefit from word-specific knowledge and improve it by integrating it into our sense-specific representations.
      </paragraph>
     </section>
     <section label="3.4">
      <section-title>
       Unified vector representation
      </section-title>
      <paragraph>
       We also propose a third representation, which we call unified, that, in contrast to the lexical vector representation which has potentially ambiguous words as individual dimensions, has BabelNet synsets as its individual dimensions. Algorithm 1 shows the construction process of a unified vector given the sub-corpus {a mathematical formula}SC. The algorithm first clusters together those words in {a mathematical formula}SC that have a sense sharing the same hypernym (h in the algorithm) according to the WordNet taxonomy integrated in BabelNet (lines 4–6).
      </paragraph>
      <paragraph>
       On all hyponym clusters we impose the restriction that they should have at least one lexicalization of the hypernym above the standard lexical specificity threshold 2 (lines 16–18). The reason why we include this in the unified representation is to reduce some noise detected by applying the old unified algorithm [16]. Finally, if the cluster passes the threshold, the specificity is computed for the set of all the hyponyms of h, even those which do not occur in the sub-corpus {a mathematical formula}SC (lines 20–24). As in Section 3.1, F and f denote the frequencies in the reference corpus {a mathematical formula}RC (Wikipedia) and the sub-corpus {a mathematical formula}SC, respectively. In this case, the frequencies correspond to the aggregation of frequencies of h and all its hyponyms.
      </paragraph>
      <paragraph>
       Our clustering of sibling words into a single cluster represented by their common hypernym transforms a lexical space into a unified semantic space. This space has multilingual synsets as dimensions, enabling their direct comparability across languages. We evaluated this feature of the unified vectors on the task of cross-lingual word similarity in Section 6.1.3. The clustering may also be viewed as an implicit disambiguation of potentially ambiguous words, as they are disambiguated into their intended sense represented by their hypernym, resulting in a more accurate semantic representation.
      </paragraph>
     </section>
     <section label="3.5">
      <section-title>
       Vector comparison
      </section-title>
      <paragraph>
       As our vector comparison method for the lexical and unified vectors we use the square-rooted Absolute Weighted Overlap [17], [16], which is based on the Weighted Overlap measure [106]. For notational brevity, we will refer to the square-rooted Absolute Weighted Overlap as Weighted Overlap (WO). WO compares two vectors on the basis of their overlapping dimensions, which are harmonically weighted by their absolute rankings. For this measure the vectors are viewed as semantic sets or ranked lists[135], as the weights are only used to sort the elements within the vector and their actual values are not used in the calculation. Formally, Weighted Overlap between two vectors {a mathematical formula}v1→ and {a mathematical formula}v2→ is defined as follows:{a mathematical formula} where O is the set of overlapping dimensions (i.e., concepts or words) between the two vectors and {a mathematical formula}rank(d,vi→) is the rank of dimension d in the vector {a mathematical formula}vi→. Absolute WO differs from the original WO, which takes into account the relative ranks of the dimensions with respect to the overlapping dimensions, instead of considering all the dimensions of the vector. Owing to the use of absolute ranks this measure gives lower scores in comparison to the original WO. This is the reason behind the use of the square-root operator, which smooths the distribution of values over the [0,1] scale. This metric has been shown to suit specificity-based vectors more than the conventional cosine distance [17].
      </paragraph>
      <paragraph>
       In contrast, we use cosine for comparing our embedded vector representations. The dimensions of the embedded representations are not interpretable and the dimension values do not represent weights, thus rank-based WO is not applicable on this setting. Cosine is the usual measure used in the literature to measure similarity in an embedding space [81], [19], [68].
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      From a synset to its vector representations
     </section-title>
     <paragraph>
      In Section 3 we proposed three vector representations of an arbitrary text or subcorpus {a mathematical formula}SC belonging to a larger collection. We now see how we leverage these representations to obtain a semantic vector representation for concepts and named entities. As knowledge base we use BabelNet,{sup:9} a multilingual encyclopedic dictionary which merges WordNet with other lexical and encyclopedic resources such as Wikipedia and Wiktionary, thanks to its use of an automatic mapping algorithm [97], [98]. We chose BabelNet due to its large coverage of named entities and concepts in hundreds of languages. Moreover, concepts and named entities are organized into a full-fledged taxonomy which integrates the WordNet taxonomy, which is the one used in our experiments, and, from its latest versions, the Wikipedia Bitaxonomy [34], WikiData, and is-a relations coming from open information extraction techniques [26]. Our approach makes use of the full power of BabelNet, as it exploits the complementary information of the distributional statistics in Wikipedia articles that are tied to the taxonomical relations in BabelNet. In our experiments, we used version 3.0 of BabelNet (released in December 2014) which covers around 6.5M concepts and more than 7M named entities in 271 different languages. The rest of this section is divided into two parts. We first show how we collect contextual information for a given synset (Section 4.1) and then explain how this contextual information is processed in order to obtain our vector representations (Section 4.2).
     </paragraph>
     <section label="4.1">
      <section-title>
       Getting contextual information for a given synset
      </section-title>
      <paragraph>
       The goal of the first step is to create a subcorpus {a mathematical formula}SCs for a given BabelNet synset s. Let {a mathematical formula}Ws be the set containing the Wikipedia page corresponding to the concept s ({a mathematical formula}wps henceforth) and all the related Wikipedia pages that have an outgoing link to that page. Note that at this stage {a mathematical formula}Ws might be empty if there is no Wikipedia page corresponding to the BabelNet synset s. We further enrich {a mathematical formula}Ws by adding the corresponding Wikipedia pages of the hypernyms and hyponyms of s in the taxonomy of BabelNet. Fig. 2 illustrates our procedure for obtaining contextual information. Let {a mathematical formula}SCs be the set of content words occurring in the Wikipedia pages of {a mathematical formula}Ws after tokenization and lemmatization. The frequency of each content word w of {a mathematical formula}SCs is calculated as follows:{a mathematical formula} where n is the number of Wikipedia pages in {a mathematical formula}Ws, {a mathematical formula}fi(w) is the frequency of w in the Wikipedia page {a mathematical formula}pi∈Ws{a mathematical formula}(i=1,...,n), and {a mathematical formula}λi is the weight assigned to the page {a mathematical formula}pi to denote its importance. In the following subsection we explain how we calculate the weight {a mathematical formula}λi for a given page {a mathematical formula}pi.
      </paragraph>
      <section label="4.1.1">
       <section-title>
        Weighting semantic relations
       </section-title>
       <paragraph>
        In this section we explain how we weight the BabelNet semantic relations (i.e., {a mathematical formula}λi in Equation (11)) between the target synset s and the i-th page in {a mathematical formula}Ws. In previous versions of Nasari[17], [16] we were making an assumption that all the Wikipedia pages in {a mathematical formula}Ws were equally important (i.e., {a mathematical formula}λi=1,∀i≤n). In this article we set more meaningful weights for these pages on the basis of the source and type of semantic connection to the target synset s.
       </paragraph>
       <paragraph>
        A Wikipedia page in {a mathematical formula}Ws may come from three different sources (see Section 4.1): (1) the Wikipedia page corresponding to s ({a mathematical formula}wps), (2) the related Wikipedia pages that have an outgoing link to the page {a mathematical formula}wps, and (3) the Wikipedia pages that are connected to s through taxonomic relations in BabelNet. We compute and assign a weight in the {a mathematical formula}[0,1] range for the pages of each type as follows:
       </paragraph>
       <list>
        <list-item label="1.">
         The Wikipedia page corresponding to the BabelNet synsets (i.e., {a mathematical formula}wps) is assigned the highest possible weight of 1.
        </list-item>
        <list-item label="2.">
         The weights for the related Wikipedia pages that have an outgoing link to{a mathematical formula}wps are computed as follows. We first compute the lexical vectors of these Wikipedia pages, as well as for {a mathematical formula}wps. We then apply Weighted Overlap (see Section 3.5) to calculate the similarity between the lexical vectors of each of these pages and that of {a mathematical formula}wps. These similarity scores denote the weight of each related Wikipedia page. In order to reduce the high number of ingoing links in some cases, and to improve the quality of these links, we prune the ingoing links to include only the top 100 links on the basis of their similarity scores and those whose similarity score is higher than 0.25.
        </list-item>
        <list-item label="3.">
         Given there is a possibility that a particular synset does not have a Wikipedia page associated with it, the Wikipedia pages coming from taxonomic relations cannot be calculated as in the previous case. In this case, the Wikipedia pages coming from taxonomic relations are given a fixed score of 0.85, which was calculated as follows. We picked a set of 100 random taxonomic relations and calculated the average similarity score among the 100 pairs by using our previous Nasari system.
        </list-item>
       </list>
      </section>
     </section>
     <section label="4.2">
      <section-title>
       Transforming the contextual information into vector representations
      </section-title>
      <paragraph>
       Once we have gathered a corpus {a mathematical formula}SCs for a given BabelNet synset s and computed the associated frequencies {a mathematical formula}f(w) for each word w in {a mathematical formula}SCs, we proceed to calculate the lexical, embedded and unified vectors of s as explained in Sections 3.2, 3.3 and 3.4, respectively. In our experiments, we used the whole Wikipedia corpus as our reference corpus {a mathematical formula}RC (Wikipedia dump of December 2014).{sup:10} We computed Nasari lexical and unified vectors for English, German, French, Italian, and Spanish. The number of synset vectors for each of these languages is, respectively, 4.42M, 1.51M, 1.48M, 1.10M and 1.07M. On average, for the English language, the contextual information of a synset is composed of a subcorpus {a mathematical formula}SCs of 1561 words in total coming from 17 Wikipedia pages. For the embedded vectors, we took as word embeddings the pre-trained word and phrase vectors from Word2Vec.{sup:11} These vectors were trained on a 100-billion English corpus from Google News and have 300 dimensions.
      </paragraph>
      <paragraph>
       Lexical and unified synset vectors example  We show in Table 1, Table 2, respectively, the top-weighted dimensions of the lexical and unified vector representations for the financial and geographical senses of the noun bank in three different languages, i.e., English, French and Spanish. As can be seen, the two senses of bank are clearly identified and distinguished from each other according to the top dimensions of their vectors, irrespective of their language and type. Additionally, note that the unified vectors are comparable across languages. We mark in Table 2, across different languages, those word senses{sup:12} that correspond to the same BabelNet synset. It can be seen from the Table that the unified vectors in different languages share many of their top elements.
      </paragraph>
      <paragraph>
       Word and synset embeddings example  The dimensions are not interpretable in the embedded vectors. Therefore, a better way to distinguish different senses would be to show their closest elements in the space (using cosine as vector similarity measure). Table 3 shows the eight closest senses to the word bank, as well as those closest to two specific senses of this word, i.e., the financial and geographical senses (recall that in our embedded vector representation words and synsets share the same space). In this case, both senses of bank are again clearly distinguished by their closest BabelNet synsets in the space. Looking at the closest senses to the word bank we can see that most of these are rather somehow to the financial meaning of bank, with lower cosine values, though. This shows that the predominant sense of the word bank in the Google News corpus (on which the word embeddings are trained) is clearly its financial sense. We note that using our embedded vector representation one can easily compute the predominance of the senses of a word by directly comparing the representation of that word with those of its individual senses. Our shared space also provides a suitable framework for studying the ambiguity of words.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Summary of the experiments
     </section-title>
     <paragraph>
      In order to assess the reliability and flexibility of our technique across different datasets and tasks, we carried out a comprehensive set of evaluations. Specifically, we considered four different tasks: Semantic Similarity (Section 6), Sense Clustering (Section 7), Domain Labeling (Section 8) and Word Sense Disambiguation (Section 9). A brief overview of the evaluation benchmarks and the results across the four tasks follows:
     </paragraph>
     <list>
      <list-item label="1.">
       Semantic similarity.Nasari proved to be highly reliable in the task of semantic similarity measurement, as it provides state-of-the-art performance on several datasets across different evaluation benchmarks:
      </list-item>
      <list-item label="2.">
       Sense clustering. We constructed a highly competitive unsupervised system on the basis of the Nasari representations, outperforming state-of-the-art supervised systems on two manually-annotated Wikipedia sense clustering datasets [23].
      </list-item>
      <list-item label="3.">
       Domain labeling. We used our system for annotating synsets of a large lexical semantic resource (BabelNet), and benchmarked our system against three automatic baselines on two gold standard datasets: a dataset of domain-labeled WordNet synsets coming from WordNet 3.0, and a new manually-constructed dataset of domain-labeled BabelNet synsets. Nasari outperformed all automatic baselines, demonstrating that our approach is not only reliable, but is also flexible across different tasks.
      </list-item>
      <list-item label="4.">
       Word Sense Disambiguation. We proposed a simple framework for a knowledge-rich unsupervised disambiguation system. Our system obtained state-of-the-art results on multilingual All-Words Word Sense Disambiguation using Wikipedia as sense inventory, evaluated on the SemEval-2013 dataset [95], and on English All-Words Word Sense Disambiguation using WordNet as sense inventory, evaluated on the SemEval-2007 [111] and SemEval-2013 [95] datasets. Additionally, we performed an experiment to measure the reliability of our semantic representations for named entities, obtaining the best results among all unsupervised systems and near state-of-the-art performance on the SemEval-2015 WSD dataset [88].
      </list-item>
     </list>
    </section>
    <section label="6">
     <section-title>
      Semantic similarity
     </section-title>
     <paragraph>
      Semantic similarity is the most popular benchmark for the evaluation of different semantic representation techniques. The task here is to measure the semantic closeness of two linguistic items. The similarity of two items can be directly computed by comparing their corresponding vector representations. As we mentioned in Section 3.5, we opted for Weighted Overlap as our vector comparison method for lexical and unified representations, and cosine for the embedded representations. Note that by using our approach we obtain representations for individual BabelNet synsets. Moreover, because BabelNet merges different resources, our representations can be used to calculate the semantic similarity between any two semantic units within and across different resources, for instance between two Wikipedia pages, two WordNet synsets, or a Wikipedia page and a WordNet synset.
     </paragraph>
     <section label="6.1">
      <section-title>
       Evaluation
      </section-title>
      <paragraph>
       We benchmark our semantic similarity procedure on the word similarity task. Word similarity is a specific task from semantic similarity in which we measure how semantically close two words are. In order to be able to compute the similarity between words we first need to map the two words to their corresponding synsets. However, this mapping is a straightforward process, thanks to the multilingual sense inventory of BabelNet. As frequently done in this task, we measure the similarity between two words w and {a mathematical formula}w′ as the similarity between their closest senses [115], [13], [106], [17]:{a mathematical formula} where {a mathematical formula}Lw represents the set of synsets which contain w as one of its lexicalizations. As vector comparison VC we use WO (see Section 3.5) to compare lexical and unified representations, and cosine for the embedded representations.
      </paragraph>
      <paragraph>
       Note that, thanks to our unified representation, w and {a mathematical formula}w′ may belong to different languages. Throughout this section on the tasks based on semantic similarity, Nasarilexical and Nasariunified represent the systems based on the lexical and unified vectors, respectively. We refer to the combination of both lexical and unified vectors as Nasari. This combination is based on the average similarity scores given by lexical and unified vectors for each sense pair. We also report results of our Nasariembed vector representations which use the pre-trained Word2Vec vectors as input. We performed experiments on monolingual word similarity for English and other languages (presented in Sections 6.1.1 and 6.1.2, respectively) and cross-lingual similarity (presented in Section 6.1.3). Additionally, we evaluate our embedded representations in a cross-level semantic similarity task in Section 6.1.4.
      </paragraph>
      <section label="6.1.1">
       <section-title>
        Monolingual word similarity: English
       </section-title>
       <section>
        Datasets
        <paragraph>
         The majority of benchmarks for word similarity are available only for the English language. We compare our approach with other state-of-the-art word similarity systems on standard English word similarity datasets. We chose the standard MC-30 [84], WordSim-353 [33], and SimLex-999 [43] as evaluation benchmarks. MC-30 consists of a subset of RG-65 [117] which was re-annotated following new similarity guidelines. WordSim-353 consists of 353 word pairs, including both concepts and named entities. In the original WordSim-353 similarity conflated relatedness in the same dataset. In order to avoid this conflation, [1] cleverly divided the dataset into two subsets: the first one concerned relatedness while the second subset focused on similarity, the latter being the one used in our experiments. We will refer to this similarity subset of 203 word pairs as WS-Sim henceforth. Finally, we took the noun pairs from the SimLex-999 dataset as our last evaluation benchmark. The complete SimLex-999 dataset is composed of 999 word pairs, 666 of which are noun pairs.
        </paragraph>
       </section>
       <section>
        Comparison systems
        <paragraph>
         We selected state-of-the-art approaches which are available online as comparison systems. These systems can be split into two categories: knowledge-based and corpus-based. As knowledge-based, we selected two approaches based on the WordNet semantic graph: [106, ADW]{sup:13} and [69, Lin].{sup:14} Another knowledge-based approach is [35, ESA],{sup:15} which represents a word in a semantic space of Wikipedia articles. We also compared our systems with four corpus-based approaches.{sup:16} Firstly, we took the pre-trained word embeddings of Word2Vec[81],{sup:17} the same used for our Nasariembed system (see Section 4.2). Then, we took the best predictive and count-based models for semantic similarity released by [8].{sup:18} The best predictive model is based on Word2Vec (Best-Word2Vec henceforth), while the best count-based models (PMI-SVD) are traditional co-occurrence vectors based on Pointwise Mutual Information (PMI) combined with a Singular Value Decomposition (SVD) dimensionality reduction. Finally, we benchmarked our system against two embedding-based sense representation approaches. The first approach, Chen henceforth [19], leverages word embeddings, WordNet glosses and a WSD system for creating sense embeddings.{sup:19} The second one, called SensEmbed[48], uses BabelNet as the main knowledge source and also relies on pre-disambiguated text by using a WSD system. We report the results of these last two methods when using the same closest senses strategy used by our systems.
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 4 shows Pearson and Spearman correlation performance of our systems and all comparison systems on the three considered datasets.{sup:20} Both lexical and unified vectors, especially the lexical ones, prove to be quite robust across datasets. The combination of both lexical and unified vectors does not show any noticeable improvement over the lexical vectors single-handed. Our system gets the highest average Pearson correlation among all systems, outperforming even the embedding-based approaches which use one dataset (SensEmbed) or two datasets (Best-Word2Vec) in order to tune their hyperparameters.{sup:21} In terms of Spearman correlation, our system based on the lexical vectors also achieves the highest average performance among the systems which do not use any of the datasets for tuning with a single point advantage over Word2Vec. Nasariembed also proves to be quite competitive, outperforming all Word2Vec approaches in terms of Pearson correlation and obtaining the best overall result on MC-30.
        </paragraph>
        <paragraph>
         Lin, which does not perform particularly well on MC-30 and WS-Sim, surprisingly obtains the best overall performance on the SimLex-999 dataset, which is largest considered dataset, consisting of 666 noun pairs. Our system gets the second best overall performance on this dataset. A closer look at the output of the similarity scores given by our system compared to the gold standard shows noticeable errors when measuring the similarity between antonym pairs, which are heavily represented in this dataset. These antonym pairs were given consistently low values across the dataset, irrespective of the target words, whereas we argue that the similarity scores ought to vary according to the particular semantics of the antonym pairs. For instance, the pair day-night gets a score of 1.9 in the 0–10 scale, while our system gets a much higher 8.0 score.{sup:22} A similar phenomenon is found on the sunset–sunrise pair. Nevertheless, in both cases the words in the pair belong to coordinate synsets in WordNet. In fact, recent works [121], [105], [91] have shown how significant performance improvements can be obtained on this dataset by simply tweaking usual word embedding approaches to handle antonymy. This differs from the scores given in the WordSim-353 dataset, in which antonym pairs were considered as similar [43]. It is outside the scope of this work to change this feature of our system in order to resolve its judgment differences with respect to the human annotation of antonym pairs in the SimLex-999 dataset.
        </paragraph>
       </section>
      </section>
      <section label="6.1.2">
       <section-title>
        Multilingual word similarity
       </section-title>
       <section>
        Datasets
        <paragraph>
         We took the RG-65 dataset as evaluation benchmark. The language of this dataset was originally English [117]. It was later translated into French [54], German [39] and Spanish [15]. We used the four versions of the dataset for our experiments.
        </paragraph>
       </section>
       <section>
        Comparison systems
        <paragraph>
         We benchmark our system against other multilingual word similarity approaches. Wiki-wup[110] and LSA-Wiki[38] are systems which use Wikipedia as their main knowledge resource. We also provide results for co-occurrence-based methods such as PMI and SOC-PMI[54] and for the newer word embeddings [31]. For word embeddings we report results for the Word2Vec model{sup:23} and for an approach retrofitting these Word2Vec vectors into WordNet (Retrofitting) [31]. For the Spanish language no result was reported in [31] for Word2Vec, so we trained Word2Vec with the same hyperparameters of Best-Word2Vec[8] on the Spanish Billion Words Corpus{sup:24}[18]. We used these Spanish word embeddings as input for our Nasariembed system in this language. Additionally, we report results for pre-trained embeddings in all four languages [5, Polyglot-embed].{sup:25} These vectors have sixty-four dimensions and were trained on the Wikipedia corpus. We also compare this system with our embedded representations of synsets by using the polyglot word embeddings as input continuous representations (see Section 3.3). We will refer to this latter method as Nasaripoly-embed.
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 5 shows Pearson and Spearman correlation performance of our systems and all comparison systems on the RG-65 word similarity datasets for English, French, German and Spanish.{sup:26} Our system outperforms all multilingual comparison systems in English, French and German in terms of both Pearson and Spearman correlation. For the Spanish language our system surprisingly slightly outperforms the human inter-annotator agreement (which was calculated in terms of average pairwise Pearson correlation), hence demonstrating the competitiveness of our approach in this language too.
        </paragraph>
        <paragraph>
         The Polyglot-embed multilingual representations do not show a particular potential for the task. The reason behind these results may be due, apart from the inherent ambiguity of words, to their low dimensionality (64) and small vocabulary (100K words). However, our embedded representation using these word embeddings (Nasaripoly-embed) hugely improves the original vectors (obtaining an average twenty-three Pearson and twenty-eight Spearman correlation points improvement). Nasaripoly-embed, despite achieving lower results than our three representations, achieves competitive results with respect to other comparison systems, with the added benefit of being applicable to many languages (pre-trained polyglot embeddings are available for more than a hundred languages).
        </paragraph>
       </section>
      </section>
      <section label="6.1.3">
       <section-title>
        Cross-lingual word similarity
       </section-title>
       <section>
        Datasets
        <paragraph>
         We have chosen the RG-65 cross-lingual datasets released by [15] for English, French, German and Spanish. These datasets{sup:27} were automatically constructed by taking the manually-curated multilingual RG-65 datasets from the previous Section as input. In total, we evaluated on six datasets consisting of all the possible language pair combinations for the four languages.
        </paragraph>
       </section>
       <section>
        Comparison systems
        <paragraph>
         As cross-lingual comparison systems, we have included the best results provided by the CL-MSR-2.0 system [60]. This system applies PMI on an English–French parallel corpus obtained from WordNet. Additionally, we provide results for some of the best performing systems in English word similarity by using English as a pivot language.{sup:28} Baseline pivot systems include the WordNet-based system ADW[106], the pre-trained Word2Vec word embeddings [81] and the top performing Word2Vec model in similarity obtained by [8] (Best-Word2Vec), and the best count-based model obtained by [8] (PMI-SVD). See Section 6.1.1 for more details on these comparison systems. We also report results for our system using the combination of lexical and unified English Nasari vectors. We refer to all these systems using English as pivot language as pivot.
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 6 shows cross-lingual word similarity results according to Pearson and Spearman correlation performance. In this section we only report results for our unified vector representations, as their dimensions are BabelNet synsets, which are multilingual and therefore may be used for direct cross-lingual comparison. Our unified vector representations outperform all comparison systems (both types) in terms of Pearson correlation performance except for the French-German pair, in which our pivot system obtains the best result. It is interesting to note that our English monolingual similarity proves to be the most robust across language pairs among all pivot systems according to Pearson correlation measure, demonstrating the reliability of our system also on a purely monolingual scheme. Pivot systems prove to be competitive, outperforming the only cross-lingual baseline which does not use a pivot language. In fact, despite obtaining relatively modest Pearson results, ADW obtains the best results according to the Spearman correlation measure (our unified vector representations obtain the second best result overall). In terms of the harmonic mean of Pearson and Spearman, used as official measure in a previous semantic similarity SemEval task [57] and in previous works [41], our system outperforms ADW (second overall system) by three points (0.80 to 0.77), demonstrating the effectiveness of our direct cross-lingual word comparison with respect to the use of English as a pivot language.
        </paragraph>
       </section>
      </section>
      <section label="6.1.4">
       <section-title>
        Cross-level semantic similarity
       </section-title>
       <paragraph>
        Finally, we evaluated our embedded representations on the word to sense semantic similarity task. Recall from Section 4.2 that our embedded vector representations share the same space with word embeddings. Therefore, in order to calculate the similarity between a word and a sense, we only have to compute the cosine similarity between their respective vector representations. In this experiment, we take the BabelNet sense representation of a word sense if it is modeled by Nasari. Otherwise, in order to increase the coverage of our system, we simply take the word embedding of the lemma of the word sense as its representation.
       </paragraph>
       <section>
        Dataset
        <paragraph>
         As our benchmark we opted for the Word to Sense (word2sense) similarity subtask of the SemEval-2014 Cross-Level Semantic Similarity (CLSS) task [57]. The subtask provides 500 word–sense pairs for its test dataset. Each pair is associated with a score denoting the semantic overlap between the two items. From the dataset we took the subset in which the senses were nominal{sup:29} (277 pairs). This dataset includes many words that are not usually integrated in a knowledge source, such as slang words. Our embedded representation model is particularly suitable for this task as it provides a single semantic space for words and BabelNet senses, hence expanding the coverage far beyond the vocabulary of BabelNet.
        </paragraph>
       </section>
       <section>
        Comparison systems
        <paragraph>
         Thirty-eight systems participated in the word2sense subtask. We compare the performance of our embedded representations with the three best performing participating systems in this subtask. Meerkat Mafia[59] is a system that relies on Latent Semantic Analysis (LSA) and uses external dictionaries to handle OOV words. SemantiKLUE[112] combines a set of different unsupervised and supervised techniques to measure semantic similarity. The third system, the most similar to our system, is SimCompass[80], which relies on deep learning word embeddings and uses WordNet as its only knowledge source.
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 7 shows Pearson and Spearman correlation performance of the Nasari system with embedded representations together with the three comparison systems. Meerkat Mafia obtains the best overall performance on this dataset. Our system is the second best system, outperforming the remaining 37 participating systems of the SemEval task. Interestingly, Nasariembed provides a considerable improvement over SimCompass (0.09 and 0.07 in terms of Pearson and Spearman correlations, respectively), which is also based on word embeddings and uses WordNet as lexical resource.
        </paragraph>
       </section>
      </section>
     </section>
    </section>
    <section label="7">
     <section-title>
      Sense clustering
     </section-title>
     <paragraph>
      Our second application focuses on sense clustering. Some sense inventories suffer from a high granularity of their sense inventory. This high granularity could possibly affect the performance of applications based on their sense inventories [102] and, hence, clustering their senses could be beneficial.
     </paragraph>
     <paragraph>
      Given our setup, we could seamlessly perform sense clustering in BabelNet, WordNet or Wikipedia. We follow the same procedure as semantic similarity for sense clustering. Following [23], we view sense clustering as a binary classification task in which given a pair of senses the task is to decide if they have to be merged or not. In the usual setting of clustering, where senses which are semantically related are clustered together, we rely on our similarity scale and simply cluster a pair of items (synsets, senses or pages) together provided that their similarity exceeds the middle point in our similarity scale, i.e., 0.5 in the scale of [0, 1], and with a minimum overlap between vectors of five dimensions. In specific sense clustering settings, this middle-point threshold may be changed to another value, or determined using a tuning dataset.
     </paragraph>
     <section label="7.1">
      <section-title>
       Evaluation: Wikipedia sense clustering
      </section-title>
      <paragraph>
       Given the high granularity of the Wikipedia sense inventory, clustering related senses may improve systems which take Wikipedia as their knowledge source [46]. Wikipedia-based Word Sense Disambiguation [77], [24] is an example of an application which may benefit from this sense inventory clustering.
      </paragraph>
      <section label="7.1.1">
       <section-title>
        Datasets
       </section-title>
       <paragraph>
        Wikipedia can be considered as a sense inventory wherein the different meanings of a word are denoted by the articles listed in its disambiguation page [78]. Starting from these Wikipedia disambiguation pages and with the help of human annotation, [23] created two Wikipedia sense clustering datasets. In these datasets, clustering is viewed as a binary classification task in which all possible pairings of senses of a word are annotated whether they should be clustered or not. The first dataset, which we will refer to as 500-pair dataset, contains 500 pairs, 357 of which are set to belong to the same cluster or clustered, and the remaining 143 to not clustered. The second dataset, referred to as the SemEval dataset, is based on a set of highly ambiguous words taken from SemEval evaluations [77] and consists of 925 pairs, 162 of which are positively labeled (clustered). Parameter_(computer_programming)-Parameter and Fatigue(medical)-Fatigue(safety) are two sample pairs of Wikipedia pages that should be merged.
       </paragraph>
       <paragraph>
        As explained above, our system is based on Semantic Similarity (see Section 6) for the sense clustering tasks. Two senses (in this case two Wikipedia pages) are set to be clustered if their similarity is greater than or equal to the middle point of our similarity scale (i.e., 0.5).
       </paragraph>
      </section>
      <section label="7.1.2">
       <section-title>
        Results
       </section-title>
       <paragraph>
        Our experiments are carried out on the 500-pair and SemEval datasets. We set two naive baselines: one considering all the pairs as positive or clustered (Baselinecluster), and another one doing the opposite, i.e., not clustering any of the test pairs (Baselineno-cluster). We also compare our system to two systems proposed by [23]. Both systems exploit the structure and content of the Wikipedia pages by using a multi-feature Support Vector Machine classifier trained on an automatically-labeled dataset. This first system is totally monolingual (it only makes use of English Wikipedia pages), while the second system also exploits Wikipedia multilinguality.{sup:30} We will refer to the first system as SVM-monolingual and to the second system as SVM-multilingual.
       </paragraph>
       <paragraph>
        Table 8 shows the results obtained for the Wikipedia sense clustering task in the 500-pair and SemEval datasets. The results are shown in terms of accuracy (number of correctly labeled pairs divided by total number of instance pairs) and F-Measure (harmonic mean of precision and recall). As we can see from the Table, our system in its unsupervised setting achieves a very high accuracy, outperforming both systems of [23] on the SemEval dataset and SVM-monolingual on the 500-pair dataset. Only the supervised system of [23] using information of Wikipedia pages in different languages outperforms our main combined Nasari system in terms of accuracy (no F-Measure results were reported) by a narrow margin. Our system, in any of the three variants, comfortably outperforms the naive baselines in terms of both accuracy and F-Measure. When comparing our three systems, the combination of both lexical and unified vectors outperforms both single-handed components. However, both lexical- and unified- based systems (and embedding-based) also prove to be highly competitive single-handed, outperforming all baselines on the SemEval dataset, including the multilingual approach of [23].
       </paragraph>
      </section>
     </section>
    </section>
    <section label="8">
     <section-title>
      Domain labeling
     </section-title>
     <paragraph>
      Taking a BabelNet synset (or a Wikipedia page, or a WordNet synset) as input, the task in domain labeling consists of automatically tagging this synset or page with one of the domains in a given set. The domain labeling task has proven to be useful when integrated into a given lexical resource [127], [70] and has several direct applications, such as Word Sense Disambiguation [71], [3], [30] and Text Categorization [94]. WordNet version 3.0 has domains for some of its synsets. However, the number of domains used is quite large (357 domains) and they are not uniformly balanced. For instance, there is a domain named Ethiopia containing a single synset, but no other domains referring to different countries are to be found. There are other domains with single synsets, such as Molecular Biology or Cytology, whereas some domains are annotated with a relatively high number of instances, such as Law with 534 annotated instances. Moreover, the coverage of these domains is rather poor: only 4098 synsets have been annotated with at least one domain.
     </paragraph>
     <paragraph>
      In this section we present a Nasari-based approach to automatically tag a much larger lexical resource (BabelNet) using a different set of domains and achieving significantly higher coverage. Our creation of domain labels for BabelNet synsets relies on our lexical vectors.{sup:31} The first step consists of creating a lexical vector for each domain. To this end, we follow the procedure which was explained in Section 3.2 and learn a lexical vector for each given domain. We do this by using sets of seed Wikipedia pages which characterize a given domain. As context for learning the vectors of a given domain we use the concatenations of all the texts corresponding to the Wikipedia pages of the seeds.
     </paragraph>
     <paragraph>
      Then, in order to find the domain of a synset we computed Weighted Overlap between the corresponding English Nasari lexical vector and the lexical vector of each domain. For a given BabelNet synset s, we pick the domain with maximal similarity:{a mathematical formula} where {a mathematical formula}Nasari→lex(s) is the Nasari lexical vector of synset s and {a mathematical formula}v→lex(d) is the lexical vector of the domain d. Similarly to the sense clustering task, we tagged synsets with a domain provided that the minimum overlap between their respective lexical vectors exceeded five dimensions. For notational brevity, we will refer to the domain of synset s whose score is highest across all domains as its top domain.
     </paragraph>
     <paragraph>
      Wikipedia domains and seeds  To select our set of domains we used Wikipedia featured articles,{sup:32} in which a set of 33 domains (e.g. Animals, Meteorology or Music) is provided. For each domain, Wikipedians have selected several Wikipedia pages which best represent that domain. We will refer to these Wikipedia pages already tagged with a domain as seeds. Each domain has a different number of available pre-tagged Wikipedia pages, ranging from only 9 (Mathematics domain) to 189 (Media domain), totalling 4230 Wikipedia pages overall. From the set of domains we decided to remove the Companies domain, while retaining the more general Business, economics, and finance domain, as we thought Companies might conflate with other domains. For instance, Nestlé or Toyota may be tagged with the Companies domain, but also with Food and drink and Transport and travel, respectively. We also modified some domain names in order to make them more general by taking into account their given seeds. For example, the domain name Law was changed to Law and crime. The final set of labels includes 32 different domains. Table 9 shows all these domains in alphabetical order.
     </paragraph>
     <paragraph>
      By applying our pipeline on the Wikipedia seeds over 3.9 M BabelNet synsets (from a total of 4.4M English Nasari lexical vectors) were tagged with at least one domain. Over 90% of the 500K synsets that were not annotated with a domain label were isolated Wikipedia pages (i.e., pages that are not linked by any other Wikipedia page) composed of only a few sentences.
     </paragraph>
     <section label="8.1">
      <section-title>
       Experiments
      </section-title>
      <paragraph>
       In this section we report our experiments on the domain labeling task. First, in Section 8.1.1 we explain the construction of our gold standard domain-labeled datasets. Then, we describe our baseline systems in Section 8.1.2 and compare them against our system on the newly created gold standard datasets in Section 8.1.3.
      </paragraph>
      <section label="8.1.1">
       <section-title>
        Gold standard dataset construction
       </section-title>
       <paragraph>
        In order to evaluate the performance of our domain labeling approach we constructed two gold standard domain labeled datasets.
       </paragraph>
       <section>
        WordNet domain-labeled dataset
        <paragraph>
         For the construction of this dataset, we took the WordNet 3.0 synsets which were manually tagged with domains. The domain set of WordNet differs from our set of domains (see Table 9 for our final domain set). Therefore, we performed a manual mapping from the WordNet domains to our domain set in order to make them comparable. Domains in WordNet were mapped to one of our domains provided that the surface form of the WordNet domain matched the surface form of one of our domain labels. For instance, a WordNet synset whose domain was either Business, Economics or Finance was to be mapped to the domain Business, economics, and finance. There are WordNet synsets tagged with more than one domain in WordNet, but we considered only those with a single domain in WordNet for the gold standard construction. As a result, we obtained a gold standard dataset of 1540 WordNet synsets tagged with our domain set.{sup:33}
        </paragraph>
       </section>
       <section>
        BabelNet domain-labeled dataset
        <paragraph>
         In order to have a more realistic distribution of BabelNet synsets comprising not only synsets which belong to the WordNet sense inventory, we created a second gold-standard dataset based on BabelNet. For this, we randomly sampled 200 BabelNet synsets with at least one English lexicalization from the set of all 6.5M possible BabelNet synsets. Of these, 65% were integrated in Wikipedia and only 1.5% belonged to WordNet (the remaining synsets were mostly integrated in WikiData only). Two annotators manually labeled these 200 synsets. They were instructed to mark each synset with a single domain only. Any disagreements were adjudicated in a final phase by the two annotators. The inter-annotator agreement was computed to be 86%, which may be viewed as an upper-bound for the performance of automatic systems.
        </paragraph>
       </section>
      </section>
      <section label="8.1.2">
       <section-title>
        Comparison systems
       </section-title>
       <paragraph>
        As benchmark for our system, we developed three different baselines: two baselines based on Wikipedia and a third one propagating domains using a lexical resource taxonomy. Similarly to our approach, the first two baselines construct a lexical vector for each domain. As seeds, the Wikipedia-based baselines used the same set of Wikipedia pages as our system. Lexical vectors were also computed for each Wikipedia page and the similarity between a Wikipedia page and each domain was calculated. Finally, the Wikipedia page with the top domain similarity score was selected. Vectors were constructed following a classic vector space model scheme in which the resulting vectors are individual content words and the similarity between vectors is calculated by using the standard cosine similarity measure. The only difference between the two baselines lays in the calculation of weights for each dimension. Wikipedia-TF calculates weights on the basis of term frequencies (TF), whereas Wikipedia-TFidf combines term frequency with the conventional inverse document frequency weighting scheme [52, tf-idf]. For these two Wikipedia-based systems, we relied on the mapping provided in BabelNet 3.0 between WordNet synsets and Wikipedia pages.
       </paragraph>
       <paragraph>
        The third baseline system, Taxo-Prop henceforth, uses a taxonomy-based domain propagation. The system takes seeds from the respective domain-labeled gold standard datasets (Section 8.1.1). Algorithm 2 shows the process for obtaining a domain label for a non-tagged synset s. The system is based on a taxonomy and works iteratively. First, it goes over all the neighbors of s in the taxonomy and checks whether they are tagged with a domain (lines 7–16 in the Algorithm). In the case where a particular domain {a mathematical formula}dˆ is encountered more often than any other domain among the neighbors' domains, s is tagged with {a mathematical formula}dˆ (line 24 in the Algorithm). Otherwise, we repeat the process and move up and down in the taxonomy, thereby checking for the domain tags of the neighbors of the neighbors. We repeat this until a domain appearing more frequently than any other domain is found (lines 5–20 in the Algorithm). In order to test the algorithm on the datasets we used both WordNet Taxo-Prop (WN) and BabelNet Taxo-Prop (BN) taxonomies and carried out 10-fold cross validation on the test dataset.{sup:34}
       </paragraph>
       <paragraph>
        Finally, we also compared with WN-Domains-3.2[70], [9], which is the latest released version of WordNet Domains.{sup:35} The system is in essence very similar to the Taxo-Prop system described above, in the sense that it takes seeds for each domain (manually selected for synsets that are located high in the taxonomy) as input, and spreads them through the WordNet taxonomy. This system involves an undetermined amount of manual intervention in the selection of seeds (“a small number of high level synsets are manually annotated with their pertinent Subject Code Fields{sup:36}”), and manual curation (“the main problems are detected and the manual annotations are corrected”) [70]. WN-Domains-3.2 was released for WordNet 2.0. For testing it on the WordNet-based dataset we used the mapping between versions 2.0 and 3.0 of WordNet.{sup:37}
       </paragraph>
      </section>
      <section label="8.1.3">
       <section-title>
        Results
       </section-title>
       <paragraph>
        Results are shown in Table 10 in terms of standard precision, recall and F-Measure. When comparing the two Wikipedia-based systems, tf-idf proves to be more reliable than using term frequency only, but its performance is still significantly below our Nasari-based system. It is interesting to note that our system is robust across datasets, while Wikipedia-based approaches experience a drastically reduced performance on the BabelNet dataset. This is due to the fact that Wikipedia pages associated with WordNet synsets are, in general, richer and longer than an average Wikipedia page (in the BabelNet dataset, synsets were extracted randomly). In contrast, Taxo-Prop achieves more competitive results, obtaining a lower precision than our system, but the highest overall recall on the WordNet dataset. However, this may lead to wrong conclusions. Given that the coverage of our system is actually considerably larger than all the synsets covered in WordNet, the recall of our approach is in fact larger than any system relying on WordNet as its only knowledge resource (there are only 117K synsets in the whole of WordNet). Additionally, the WordNet-based approach has the advantage of annotating in exactly the same number of domains as occur in the gold standard dataset. Our system used 4230 Wikipedia pages of 32 different domains as seeds, in contrast to the 1386 domain-labeled WordNet synsets of 27 different domains comprising the gold standard dataset. As a measure to show how much supervision each system was using in each case, we calculated its seed density, which is the percentage of synsets used on average for each domain as seeds. Formally, it is calculated as the ratio of the average number of seeds per domain to the total number of synsets in the given resource. In fact, the seed density is significantly higher in the WordNet-based system (0.044% vs. 0.001% of our system).
       </paragraph>
       <paragraph>
        WN-Domains-3.2 outperforms our system in terms of F-Measure by 2.5 absolute percentage points in the WordNet dataset. Interestingly, despite using as benchmark a subset of WordNet, our system obtains a higher recall than WN-Domains-3.2. Additionally, as remarked above, our system annotates a significantly higher number of instances, including many more named entities and specialized concepts which are not covered by WordNet (over 3.9M domain-labeled synsets annotated by our system as opposed to the 74K synsets annotated by WN-Domains-3.2). In terms of precision, WN-Domains-3.2, which involves an undetermined amount of manual curation, outperforms our default system. However, by simply adding a confidence threshold, our system can considerably increase its precision. For instance, by only tagging synsets whose top domain score is higher than the middle point of our similarity scale (i.e., 0.5), we obtain comparable results in terms of precision percentage (92.5%) to the WN-Domains-3.2 system, while still obtaining a considerably higher coverage.
       </paragraph>
       <paragraph>
        Note that in both our system and WN-Domains-3.2, the range of domains considered in the original systems was larger than the number of domains found in the gold standard, which increases the error margin. For instance, in the original setting of our system we considered 32 domains (see Table 9), of which only 27 were present in the gold standard dataset. By analyzing the errors given by our system, we realized that there are synsets that might be tagged with more than one domain. If we take the top three domain tags into account, the precision of our system increases to 91.8% and 83.1%, with recall being 82.7% and 54% in the WordNet and the BabelNet datasets, respectively. For example, our system tags the WordNet synset corresponding to the concept {a mathematical formula}angular_velocityn1 with Mathematics as top domain by a narrow margin, but in this case it would also be tagged with the Physics and astronomy domain as second domain, which would be the right answer according to the gold dataset. As a second source of error, we realized that it is arguable whether many of the false positives given by our system are, in fact, entirely wrong. Indeed, in many cases the judgement made by our system could be considered as justifiable, and equally correct to the tagging found in the gold dataset. For instance, the synset represented by the data processing sense of operation is tagged with the Mathematics domain, while the gold domain is Computing. In this case, it is clear that the synset could be tagged with either of the two domains. Another example is the WordNet synset {a mathematical formula}aestheticsn1, defined in WordNet as The branch of philosophy dealing with beauty and taste (emphasizing the evaluative criteria that are applied to art), which is tagged with the Philosophy and psychology domain by our system instead of the Art, architecture, and archeology domain label found in the gold dataset.
       </paragraph>
      </section>
     </section>
    </section>
    <section label="9">
     <section-title>
      Word sense disambiguation
     </section-title>
     <paragraph>
      Word Sense Disambiguation (WSD) is a core task in natural language understanding. Given a target word in context, the task consists of associating it with an entry in a given sense repository [93]. WSD may eventually be applied to any Natural Language Processing task, enabling an understanding of the sentences by the machine which is not usually achieved by mainstream statistical approaches, and could benefit applications such as Machine Translation [134] and Information Retrieval [120], to name but a few.
     </paragraph>
     <paragraph>
      In Section 9.1 we present the different resources which may be used as knowledge repositories for WSD. A unified framework for WSD based on Nasari is presented in Section 9.2. Experiments are presented in Section 9.3.
     </paragraph>
     <section label="9.1">
      <section-title>
       Sense inventories
      </section-title>
      <paragraph>
       One of the main knowledge sense repositories used in this task was the manually constructed WordNet [95], [111], which usually leads to a fine-grained type of disambiguation given the nature of the senses in WordNet. Another resource more recently used for this task is Wikipedia [78], [24], [95], due to its wide coverage of named entities and multilinguality. A newer resource used as a knowledge repository that is gaining popularity thanks to its multilinguality and large coverage is BabelNet [95], [88], [136], which is our main resource. Given the nature of our vectors, and in contrast to other WSD systems, we can seamlessly disambiguate in any of these resources (BabelNet integrates, among other resources, WordNet and Wikipedia). In the following section, we propose a unified framework for disambiguating words in context irrespective of the resource.
      </paragraph>
     </section>
     <section label="9.2">
      <section-title>
       Framework for word sense disambiguation
      </section-title>
      <paragraph>
       In [16] we presented a WSD framework in which we used the lexical vectors and then calculated the overlap between the target word vector and its context, harmonically weighting the ranks of the overlapping words in the target word vector. This method considers each word in context to be equally important (same weight) in the disambiguation process. In this section we present a more suitable approach which keeps to the spirit of previous lexical semantics applications and gives each word its weight in context.
      </paragraph>
      <paragraph>
       Given a set of target words in a text {a mathematical formula}T, we build a lexical vector for the context, as explained in Section 3.2. Then, for each target word w in the text {a mathematical formula}T, we retrieve the set of all the possible BabelNet synsets which have this target word as one of its lexicalizations, a set we refer to as {a mathematical formula}Lw. Finally, we simply compute Weighted Overlap (see Section 3.5) between {a mathematical formula}v→lex(T) (the lexical vector of the text {a mathematical formula}T) and the Nasari vectors corresponding to the BabelNet synsets that contain senses of w. In our setting, the top BabelNet synset in terms of WO score ({a mathematical formula}sˆ) is selected as the best sense of the given target word:{a mathematical formula}
      </paragraph>
     </section>
     <section label="9.3">
      <section-title>
       Experiments
      </section-title>
      <paragraph>
       We perform Word Sense Disambiguation experiments using two sense inventories: Wikipedia and WordNet. Recall from Section 9.1 that, since our main knowledge sense inventory is BabelNet, we can seamlessly disambiguate instances using either of these two knowledge sources. The setting of the system is the same in both cases, with only one difference: we use only BabelNet synsets{sup:38} which are mapped to Wikipedia page or WordNet synset when disambiguating with either of these resources, respectively.
      </paragraph>
      <paragraph>
       As has often been done in the literature [132], [142], [89], we use a back-off strategy to the Most Frequent Sense (MFS) baseline in the cases when our system does not provide a confident answer. Hence, in our WSD framework, we only tagged those instances whose top similarity score (see Section 9.2 for more details on our WSD system) is higher than a given threshold θ. In order to compute θ, we use the English Wikipedia trial dataset provided within the SemEval-2013 WSD task [95]. The top performing value of θ was 0.20, value that is used across all WSD experiments.{sup:39}
      </paragraph>
      <paragraph>
       Section 9.3.1 presents multilingual WSD experiments using Wikipedia as main sense inventory (a task that is strongly related to the Wikification task [78]), Section 9.3.2 presents experiments for the Named Entity Disambiguation task using BabelNet as sense inventory, and finally Section 9.3.3 presents the WSD results for English using WordNet as sense inventory.
      </paragraph>
      <section label="9.3.1">
       <section-title>
        Multilingual word sense disambiguation using Wikipedia
       </section-title>
       <paragraph>
        We used the SemEval-2013 all-words WSD dataset [95] as benchmark for our multilingual evaluations.{sup:40}{sup:,}{sup:41} This dataset includes texts for five different languages (English, French, German, Italian and Spanish) with an average of 1303 disambiguated instances per language, including multiword expressions and named entities.
       </paragraph>
       <section>
        Comparison systems
        <paragraph>
         As comparison system we include Babelfy[89],{sup:42} a state-of-the-art graph-based system for multilingual joint WSD and Entity Linking. Babelfy relies on random walks in the BabelNet semantic network combined with various graph-based heuristics. We also report results for the best run on every language of the top SemEval-2013 system [40, UMCC-DLSI]. As baseline, although difficult to beat in some WSD tasks [93], we include the Most Frequent Sense (MFS{sup:43}) heuristic. Finally, we report results from Muffin[16], our previous WSD system based on the Nasari vectors that, in contrast, used a WSD framework in which words in context were considered equally important.
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 11 shows F-Measure percentage results for our system and all comparison systems on the SemEval-2013 dataset. As we can see from the table, although our system only achieves state-of-the-art results for French and German, it does achieve the best average performance among all languages, demonstrating its robustness across languages and outperforming the current state-of-the-art results of Babelfy. Our system outperforms our previous WSD approach Muffin by over a point on average, highlighting our improvements on this particular WSD task for which we proposed a new framework (see Section 9.2).
        </paragraph>
       </section>
      </section>
      <section label="9.3.2">
       <section-title>
        English Named Entity Disambiguation using BabelNet
       </section-title>
       <paragraph>
        In order to evaluate the quality of our named entity representations, we performed experiments on the Named Entity Disambiguation task. Given that Nasari provides semantic representations for both concepts and named entities, this task was analogous to Word Sense Disambiguation (see Section 9.2) with the only difference being that in this task we only considered entity synsets as candidates. To this end, we used the English named entity dataset from the SemEval-2015 Task on All-Words Sense Disambiguation and Entity Linking[88]. This dataset consists of 85 named entities to disambiguate.
       </paragraph>
       <section>
        Comparison systems
        <paragraph>
         We benchmarked our disambiguation system against the top three best performing systems in the task, which were also the only ones outperforming the MFS baseline: DFKI[137], SUDOKU[72], and el92[118]. DFKI is a multi-objective system based on both global unsupervised and local supervised objectives. SUDOKU uses the Personalized PageRank algorithm upon disambiguating monosemous instances within the text. Finally, el92 is based on a weighted voting of various disambiguation systems: Wikipedia Miner [87], TagME [32], DBpedia Spotlight [76], and Babelfy [89].
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 12 shows F-Measure percentage results on the Named Entity portion of the SemEval-2015 WSD dataset.{sup:44} Our system obtains the second overall position of all the 17 systems that participated in the SemEval-2015 Named Entity Disambiguation task. The combination of global unsupervised and local supervised objectives of DFKI obtains the best overall results. As we show in Section 9.3.3 and discuss in Section 9.4, our system, based solely on global semantic features, generally improves when including local supervised features.
        </paragraph>
       </section>
      </section>
      <section label="9.3.3">
       <section-title>
        English Word Sense Disambiguation using WordNet
       </section-title>
       <paragraph>
        For the task of English WSD using WordNet as main sense inventory, we used two recent SemEval WSD datasets: fine-grained all-words SemEval-2007[111] and all-words SemEval-2013[95]. We performed experiments on the 162 noun instances of the SemEval-2007 dataset. SemEval-2013's dataset contains 1644 instances.
       </paragraph>
       <section>
        Comparison systems
        <paragraph>
         We include the state-of-the-art IMS system [143] as a supervised system. As unsupervised systems, we report the performance of two graph-based approaches that are based on random walks over their respective semantic networks: BabelNet [89, Babelfy] and WordNet [4, UKB]. Another approach that uses BabelNet as reference knowledge base is Multi-Objective[136] which views WSD as a multi-objective optimization problem. We also report the results of the best configuration of the top-performing system in the SemEval-2013 dataset, namely UMCC-DLSI[40]. As in Section 9.3.1, we also include our earlier WSD system Muffin for comparison. Finally, we include a system called Nasari+IMS, which is based on our WSD framework with the only difference being that in this system we back-off to IMS instead of MFS.{sup:45}
        </paragraph>
       </section>
       <section>
        Results
        <paragraph>
         Table 13 shows the F-Measure percentage performance of all systems on the SemEval-2007 and SemEval-2013 WSD datasets. Similarly to the WSD results using Wikipedia as main sense inventory (Section 9.3.1), our system Nasari outperforms our previous Muffin system. Nasari in its default setting backing-off to MFS is only surpassed by Multi-Objective in SemEval-2013 and IMS in SemEval-2017, outperforming the remaining systems in both datasets.
        </paragraph>
        <paragraph>
         Our system backing-off to IMS (Nasari+IMS) improves our default Nasari system in both datasets, obtaining the best performance among all systems on the SemEval-2007 dataset. We remark that Nasari is an unsupervised system based on global contexts, while IMS is a supervised system based on local contexts. This combination of local and global contexts has already shown to be beneficial for WSD tasks [45], [107], [136].
        </paragraph>
       </section>
      </section>
     </section>
     <section label="9.4">
      <section-title>
       Discussion: global and local contexts
      </section-title>
      <paragraph>
       Our method functions by analyzing the whole context of a given target word which has to be disambiguated. Hence, it can fail to capture the correct intended meaning of the word in the case where the local context plays a key role in determining a specific meaning of the target word, especially in a fine-grained disambiguation setting. For instance, the following example from the SemEval-2013 Word Sense Disambiguation test set shows a case where our system committed a mistake for the fine-grained sense definition of the target word behaviour. However, we can see that the mistake can be solved by analyzing the local context which is the typical setting in supervised systems:
      </paragraph>
      <list>
       <list-item label="(1)">
        The expulsion presumably forged by two players of Real Madrid (Xabi Alonso and Sergio Ramos) in the game played on the 23rd of November against Ajax in European Champions League has caused rivers of ink to be written about if such behaviour is or is not unsportmanlike and if, both players should be sanctioned by UEFA.
       </list-item>
      </list>
      <paragraph>
       Our system was not able to make a confident selection among the sense {a mathematical formula}behaviourn3 (The aggregate of the responses or reactions or movements made by an organism in any situation) and {a mathematical formula}behaviourn4 (Manner of acting or controlling yourself), picking the latter by a narrow margin. In this case, we can leverage a system that exploits local contexts, such as IMS, for a more accurate disambiguation.
      </paragraph>
      <paragraph>
       On the other hand, there are cases in which it is not possible to fully distinguish the intended meaning by just looking at the local context of the target word. The following sentence from the SemEval 2013 dataset is an example for such a case:
      </paragraph>
      <list>
       <list-item label="(2)">
        This way, and since Real Madrid will finish as leader of its group, both players will fulfill the prescribed sanction during the next game of league.
       </list-item>
      </list>
      <paragraph>
       In this case, IMS picks {a mathematical formula}sanctionn1 (Formal and explicit approval), which is also the most frequent sense for the noun sanction. In fact the system is misled by just considering the local context and ignoring a more general picture that would be given by the global context. In this case, Nasari correctly captures the semantics within the text and chooses {a mathematical formula}sanctionn2 (A mechanism of social control for enforcing a society's standards).
      </paragraph>
      <paragraph>
       In both cases the combination of Nasari and IMS provides the correct answer. In general the combination of both methods shows a consistent improvement over the single system components. In fact, the results of the combination of a knowledge-based global-context disambiguation system (i.e., Nasari) with a state-of-the-art supervised local-context approach (i.e., IMS) proves to be quite robust across datasets, outperforming many strong baselines as we can see from Table 13.
      </paragraph>
     </section>
    </section>
    <section label="10">
     <section-title>
      Analysis
     </section-title>
     <paragraph>
      In order to gain a better insight into the role some of the key components of our system's pipeline play in the overall performance, we carried out an ablation test. In particular, we were interested in evaluating the impact and importance of the following three components:
     </paragraph>
     <list>
      <list-item label="1.">
       Lexical specificity. To check how lexical specificity (see Section 3.1) fares against the standard tf-idf measure [52], we generated Nasari lexical vectors in which weights were calculated using the conventional tf-idf. Given a word w, we calculate {a mathematical formula}TFidf(w) as follows:{a mathematical formula} where {a mathematical formula}f(w) is the frequency of w in the subcorpus {a mathematical formula}SCs representing the contextual information of the synset s (see Section 4.1) and D is the set of all pages in Wikipedia. We computed two sets of tf-idf-based lexical vectors. The first version, called Nasari-TFidf, keeps all the dimensions in the vector. For the second version, Nasari-TFidf-3000d, we follow [37] and prune the vector to its top 3000 non-zero dimensions. This pruning is similar to the one performed automatically by lexical specificity, which reduces the number of non-zero dimensions while retaining the interpretability of the vector dimensions.
      </list-item>
      <list-item label="2.">
       Weighted semantic relations. To assess the advantage we gain from introducing weights to semantic relations (see Section 4.1.1), we computed a version of our lexical vectors in which the semantic relations were uniformly weighted (i.e., {a mathematical formula}λi=1,∀i∈{1,…,n} in Equation (11)), as was the case in our earlier work [16]. We will refer to this version as Nasari-unif.weight.
      </list-item>
      <list-item label="3.">
       Combination strategy of embeddings. Finally, we carried out an analysis to compare the harmonic combination of word embeddings (see Section 3.3) against uniform combination (i.e., averaging). For this purpose, we computed the embedding vector for a given synset as the centroid of all the embeddings of the words present in its corresponding lexical vector. We will refer to this variant as Nasari-av.embed in our tables.
      </list-item>
     </list>
     <paragraph>
      We evaluated the first two components in an intrinsic task (word similarity) as well as a downstream application (Word Sense Disambiguation). For the third component, we compared our default Nasariembed and the embedding representations obtained through uniform weighting in the word similarity task. We performed the evaluations on the same datasets as those used in Section 6.1.1 for word similarity and in Section 9.3.3 for Word Sense Disambiguation with WordNet as sense inventory. The whole pipeline for both tasks was left unchanged for all variants, except for the components mentioned above.
     </paragraph>
     <paragraph>
      Table 14 shows the results of the ablation test on different word similarity and Word Sense Disambiguation datasets. Our default Nasarilexical system consistently outperforms all baselines in all datasets of both tasks, demonstrating the reliability of the proposed lexical specificity and the preweighting of the semantic relations. This result is especially meaningful taking into account that our default system is the one with the fewest non-zero dimensions on average among the four evaluated approaches. In fact, the average number of non-zero dimensions of our Nasarilexical vectors was 162, which is lower than the 280 non-zero dimensions of Nasari-unif.weight, 1033 of Nasari-TFidf-3000d,{sup:46} and 1561 of Nasari-TFidf. This low average number of non-zero dimensions enables a fast processing of the vectors, i.e., they are computationally faster to work with.
     </paragraph>
     <paragraph>
      As far as the Nasariembed vectors are concerned, our default system consistently obtained significantly better results when compared to the baseline (Nasari-av.embed). In general, Nasari-av.embed produces consistently high similarity values, even for non-similar pairs. This is due to the fact that words that are not very relevant to the input synset (i.e., relatively low lexical specificity values) are given the same weight as words that are clearly more relevant (i.e., high lexical specificity values). This, in turn, is why a weighted average of the word embeddings in the lexical vector leads to more accurate results than a simple average.
     </paragraph>
    </section>
    <section label="11">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      In addition to the semantic representation of word senses, which is the main topic of this article, we briefly review the recent literature on the two most popular applications on which we evaluated our representations: semantic similarity and Word Sense Disambiguation.
     </paragraph>
     <section label="11.1">
      <section-title>
       Representation of word senses
      </section-title>
      <paragraph>
       Most research studies in semantic representation have so far concentrated on the representation of words, as can be seen from the numerous available word similarity datasets and benchmarks, while relatively few studies have focused on the representation of word senses or concepts. This is partly due to the so-called knowledge acquisition bottleneck that arises because the application of distributional word modeling techniques (which are the prominent representation approach) at the sense level would require the availability of high-coverage sense-annotated data. However, word representations are known to suffer from some issues which dampen their suitability for tasks that require accurate representations of meaning. The most important drawback with word representations lies in their inability to model polysemy and homonymy, as they conflate different meanings that a word can have into a single representation [130], [114]. For instance, a word representation for the word bank does not distinguish between the financial institution and the river bank meanings of the word (the noun bank has ten senses according to WordNet 3.0). The approach of [31] which leverages semantic lexicons to improve word representations also suffers from the same drawback.
      </paragraph>
      <paragraph>
       Because they represent the lowest linguistic level, word senses and concepts play a crucial role in natural language understanding. Since at this level individual meanings of a word are identified and separately modeled, the resulting representations are ideal for accurate semantic representation. In addition, the fine-grained representation of word senses can be directly extended to higher linguistic levels [13], such as words, which makes them quite interesting. These features have recently attracted the attention of different research studies. Most of these techniques view sense representation as a specific type of word representation and try to adapt the existing distributional word modeling techniques to the sense level, usually through clustering the contexts in which a word appears [138], [47], [99]. The fundamental assumption here is that the intended meaning of a word mainly depends on its context and hence one can obtain sense-specific contexts for a given word sense by clustering the contexts in which the word appears in a given text corpus. Various clustering-based techniques usually differ in their clustering procedure and how this is combined with the representation technique. However, these models are often limited to representing only those senses that are covered in the underlying corpus. Moreover, the sense representations obtained using these methods are usually not linked to any sense inventory, and therefore such linking has to be carried out, either manually, or with the help of sense-annotated data if the representations are to be used for direct applications such as Word Sense Disambiguation.
      </paragraph>
      <paragraph>
       Most sense modeling techniques have based their representation on the knowledge derived from resources such as WordNet. Earlier techniques exploit the information provided in WordNet, such as the synonymous words in a synset, for the representation of word senses [79], [2]. More recent approaches usually adapt distributional models to the sense level on the basis of lexico-semantic knowledge derived from lexical resources such as Wikipedia [35], [77], WordNet [19], [50], [116] or other language-specific semantic networks [51]. WordNet is also viewed as a semantic network where its individual synsets are represented on the basis of graph-based algorithms [106]. Word Sense Disambiguation of large amounts of textual data has also been explored as a means of obtaining high-coverage annotated data for learning sense representations based on neural networks, a representation referred to as sense embeddings [48]. [19], which uses WordNet as main knowledge source, also relies on WSD for obtaining their sense representations. However, these two approaches are hampered by their inherently imperfect WSD systems.
      </paragraph>
      <paragraph>
       Additionally, these techniques are often limited to the reduced coverage of WordNet and to the English language only. In contrast, our method provides a multilingual representation of word senses on the basis of the complementary knowledge of two different resources, enabling a significantly higher coverage of specific domains and named entities. Our representations are not only multilingual, but can also be compared across languages through our unified representations.
      </paragraph>
     </section>
     <section label="11.2">
      <section-title>
       Semantic similarity
      </section-title>
      <paragraph>
       Semantic similarity between word senses is usually computed on the basis of the structural properties of lexical databases such as WordNet [6], [13], or thesauri such as Roget's [90], [49]. These measures often represent a lexical resource as a semantic network and then exploit the networks for the computation of semantic similarity between a pair of word senses. The conventional WordNet-based similarity techniques take as their source of information either only the structural properties of the WordNet semantic network, such as graph distance and the lowest common super-ordinate of two word senses [44], [65], [139], or combine the structural information with statistics obtained from text corpora [115], [69]. Collaboratively-constructed resources such as Wikipedia and Wiktionary have also been used as underlying lexical resources in different semantic similarity techniques [41], [126], [86]. More recent sense similarity methods first perform random walks on the semantic networks [106], [141], [109] in order to model individual word senses and then use these representations for the computation of sense similarity. All these techniques, however, are limited to the knowledge provided by their underlying semantic resource. In contrast, our approach combines expert-based and encyclopedic knowledge from two different types of resource, providing three advantages: (1) more effective measurement of similarity based on rich semantic representations, (2) the possibility of measuring cross-resource semantic similarity, i.e., between Wikipedia pages and WordNet synsets, and (3) the possibility of comparing the semantics of word senses across different languages.
      </paragraph>
     </section>
     <section label="11.3">
      <section-title>
       Word Sense Disambiguation
      </section-title>
      <paragraph>
       Word Sense Disambiguation is a task that can benefit significantly from the representation of word senses, mainly due to its sense-level application. Based on the type of resources they use, WSD techniques can be put into two main categories: knowledge-based and supervised [93]. Supervised systems receive sense-annotated data as their source of information, i.e., a set of contexts in which a specific sense of a word appears. These systems analyze the provided data and capture the context in which a specific word sense is more likely to appear. It Makes Sense [143, IMS] is an example of a supervised system which, despite using a small set of conventional features and a simple linear classifier, has been among the best performers on different WSD benchmarks. However, the performance of supervised systems very much depends on the availability of sense-annotated data for the target word sense [107]. Hence, the applicability of these systems is limited to those words and languages for which such data is available, practically restricting them to a small subset of word senses and mainly for the English language only. Knowledge-based approaches, on the other hand, do not suffer from the lack of sense-annotated data and therefore provide a relatively higher coverage. These systems usually exploit the structural or lexical-semantic information in lexical resources for disambiguation [122], [96], [4]. However, similarly to their supervised counterparts, knowledge-based techniques are mostly limited to the English language only. Recent years have seen a growing interest in multilingual WSD [95]. Multilinguality is usually offered by methods that exploit the structural information of large-scale multilingual lexical resources such as Wikipedia [40], [73], [46]. Babelfy [89] is such a WSD system which performs random walks on the BabelNet multilingual semantic network [98] and makes use of densest subgraph heuristics. However, the approach is limited to the WSD and Entity Linking tasks. In contrast, our approach is global, as it can be used in different NLP tasks, including WSD and Entity Linking.
      </paragraph>
     </section>
    </section>
    <section label="12">
     <section-title>
      Conclusions
     </section-title>
     <paragraph>
      In this article we presented Nasari, a novel technique for the representation of concepts and named entities in arbitrary languages. Our approach combines the structural knowledge from semantic networks with the statistical information derived from text corpora for effective representation of millions of BabelNet synsets, including WordNet nominal synsets and all Wikipedia pages. We evaluated our representations in a wide range of NLP tasks and applications: semantic similarity, sense clustering, Word Sense Disambiguation, and domain labeling. We reported state-of-the-art performance on several datasets across these tasks and in different languages.
     </paragraph>
     <paragraph>
      Three type of sense representation were put forward: two explicit vector representations (unified and lexical), in which vector dimensions are interpretable, and a latent embedding-based representation. Each representation has its own advantages and limitations. In general, a combination of lexical and unified vectors led to the most reliable results in the semantic similarity and sense clustering experiments (Sections 6 and 7). Among the three representations, the lexical representation (i.e., Nasarilexical) obtained the best performance in monolingual settings. However, although the lexical vectors are sparse and hence computationally faster to process, their dimensionality is high and equal to the size of the vocabulary. In contrast, our embedded representation (i.e., Nasariembed) has a fixed low number of latent dimensions. Additionally, embedded synset vectors share the same space with the word embeddings used as input. As regards our unified representation (i.e., Nasariunified), not only does it provide an effective way for representing word senses in different languages, but, thanks to its unified semantic space, it also enables a direct comparison of different representations across languages. In addition to being multilingual, Nasari improves over the existing techniques by providing a high coverage for millions of concepts and named entities defined in the BabelNet sense inventory.
     </paragraph>
     <paragraph>
      Release  We are releasing the complete set of representations obtained using our technique for five different languages (English, Spanish, French, German and Italian) at http://lcl.uniroma1.it/nasari, and we plan to generate representations for more languages in the near future. We also provide a Python script for the computation of lexical specificity. Finally, domain labels are included in the BabelNet 3.5 release version{sup:47} and the gold standard domain-labeled datasets used for our experiments (Section 8.1.1) are provided in the website.
     </paragraph>
     <paragraph>
      Future work  As future work we plan to pursue three main directions. Firstly, we aim to compute a global representation for each concept by exploiting the statistical information obtained from multiple languages. Secondly, we plan to develop a framework for a more meaningful combination of our representations in a supervised system for improved joint WSD and Entity Linking. Thirdly, we plan to integrate our multilingual semantic representations into different end-user applications, such as Machine Translation.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>