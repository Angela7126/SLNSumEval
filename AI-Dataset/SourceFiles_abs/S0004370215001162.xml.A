<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Constraint acquisition.
   </title>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Over the last forty years, considerable progress has been made in the field of Constraint Programming (CP), providing a powerful paradigm for solving combinatorial problems. Applications in many areas, such as resource allocation, scheduling, planning and design have been reported in the literature [2], [15], [32]. Informally, the basic idea underlying constraint programming is to model a combinatorial problem as a constraint network, i.e. using a set of variables, a set of domain values and a collection of constraints. Each constraint specifies a restriction on some set of variables. For example, a constraint such as {a mathematical formula}x1≤x2 states that the value on {a mathematical formula}x1 must be less or equal than the value on {a mathematical formula}x2. A solution of the constraint network is an assignment of variables to domain values that satisfies every constraint in the network. The Constraint Satisfaction Problem (CSP) is hence the problem of finding a solution for a given constraint network.
     </paragraph>
     <paragraph>
      However, the construction of constraint networks still remains limited to specialists in the field. Actually, it has long been recognized that modeling a combinatorial problem in the constraint formalism requires significant expertise in constraint programming [19], [20], [34]. Such a level of knowledge precludes novices from being able to use constraint networks without the help of an expert. Consequently, this has a negative effect on the uptake of constraint technology in the real-world by non-experts.
     </paragraph>
     <paragraph>
      To alleviate this issue, we propose to acquire a constraint network from a set of examples. This approach is exemplified by the following scenario. Suppose that a human user would like to build a constraint network in order to solve a series of repetitive tasks. Usually, the tasks are instances of the same combinatorial problem and only differ in the set of domains associated with variables. For example, in a time-tabling problem, it could be the problem of assigning a teacher and a time slot to every courses given to all classes, regardless of the actual time-slots at which teachers are available for the current semester. In practice, the user has already solved several instances of the problem without the help of a solver and knows how to classify an example as a solution or a non-solution to it. Based on these considerations, the overall aim of constraint acquisition is to induce from examples a general constraint network that adequately represents the target problem. This approach allows us to use the learned network with different initial domains in order to solve further tasks supplied by the user.
     </paragraph>
     <paragraph>
      In a nutshell, the constraint acquisition process can be regarded as an interplay between the user and the learner. The user has in mind a target problem but does not know how this problem can be modeled as an efficient constraint network. Yet, the user has at her disposal a set of solutions (positive examples) and non-solutions (negative examples) of the problem. For its part, the learner has at its disposal a set of variables on which the examples are defined and constraint language. The overall goal of the learner is to induce a constraint network that uses combinations of constraints defined from the language and that is consistent with the solutions and non-solutions provided by the user.
     </paragraph>
     <paragraph>
      There are two constraint acquisition approaches that naturally emerge from this vision. In passive constraint acquisition, the learner cannot ask queries of the user. In this setting, the key goal of the learner is to inform the user about the state of its “version space”, that is, the set of networks that are consistent with the pool of examples. For example, the learner can be required to determine whether its version space has converged in order to inform the user that no more examples are required to capture the target problem. By contrast, in active constraint acquisition, the learner can choose an example and ask whether it is a solution, or not, of the target problem. As a helpful teacher, the user supplies the correct response. In this setting, the goal of the learner is to find a short sequence of membership queries that rapidly converges towards the target problem.
     </paragraph>
     <paragraph>
      What we call user in this paper is not necessarily a human user. For instance, in [29], the learner tries to acquire a constraint network representing the sequences of elementary operations that constitute a valid action for a robot. The classification of actions as positive or negative depends on the success or failure of the action on the robot simulator. The simulator can be ran as much as we want to produce examples. However, in standard constraint acquisition, classifying examples requires an answer from a human user. Hence, we should seek to minimize the size of the training set required to acquire a target problem.
     </paragraph>
     <paragraph>
      In this paper we formally define the main constraint acquisition problems related to passive and active acquisition. Several important complexity results are provided. We show for instance that it is polynomial to decide if the version space still contains networks consistent with the examples. Testing convergence is, however, coNP-complete. We also show that in the context of active constraint acquisition, constraint networks are not learnable in general with a polynomial number of membership queries. In order to solve the different problems that arise from constraint acquisition, we develop a constraint acquisition architecture, named Conacq, which is based on a compact and efficient representation of version spaces into clausal formulas. This basic component is equipped with different methods, inspired from SAT techniques, used to handle consistency, convergence and identification problems in passive and active constraint acquisition. The complexity of our methods is analyzed.
     </paragraph>
     <paragraph>
      The necessary background in constraint programming and concept learning is introduced in Section 2. Section 3 gives a specification of the different constraint acquisition problems examined in this study and analyzes their complexity. In Section 4, we present a technique for representing version spaces and we describe Conacq.1, a passive acquisition algorithm using that representation. Section 5 shows how to use background knowledge to improve the learning process. Active acquisition is presented in Section 6 together with an algorithm to generate good queries, that is, queries that allow to learn the target problem with as few queries as possible. Experiments are reported in Section 7. Finally, we compare our framework with related work in Section 8, and conclude this work in Section 9.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background
     </section-title>
     <paragraph>
      In this section, we introduce some useful notions in constraint programming and concept learning.
     </paragraph>
     <section label="2.1">
      <section-title>
       Vocabulary and constraint networks
      </section-title>
      <paragraph>
       In the field of constraint programming, combinatorial problems are represented as constraint networks. We consider here a single domain for all the variables in our networks. This really is a notational convenience as the actual domain of a variable can be restricted by the constraint relations. It is possible to think of our unified domain as the union of all of the actual problem domains.
      </paragraph>
      <paragraph>
       We first define what we call a vocabulary. Intuitively, a vocabulary specifies the space of all complete assignments of the variables from their domain.
      </paragraph>
      <paragraph label="Definition 1">
       VocabularyA vocabulary is a pair {a mathematical formula}〈X,D〉 such that X is a finite set {a mathematical formula}{x1,⋯,xn} of variables, and D is a finite subset of {a mathematical formula}Z called the domain.
      </paragraph>
      <paragraph>
       We will assume that the learner starts from a prefixed vocabulary; the variables and the domain are known to the learner, and its goal is merely to acquire a set of constraints over this vocabulary. This justifies our slightly non-standard definition of constraint network.
      </paragraph>
      <paragraph label="Definition 2">
       Constraint networkA constraint network over a given vocabulary {a mathematical formula}〈X,D〉 is a finite set C of constraints. Each constraint c in C is a pair {a mathematical formula}〈var(c),rel(c)〉, where {a mathematical formula}var(c) is a sequence of variables of X, called the constraint scope of c, and {a mathematical formula}rel(c) is a relation over {a mathematical formula}D|var(c)|, called the constraint relation of c. For each constraint c, the tuples of {a mathematical formula}rel(c) indicate the allowed combinations of simultaneous value assignments for the variables in {a mathematical formula}var(c). The arity of a constraint c is given by the size {a mathematical formula}|var(c)| of its scope.
      </paragraph>
      <paragraph>
       For the sake of clarity, we will often take examples using binary constraints, that is, constraints with a scope involving two variables. With a slight abuse of notation, we use {a mathematical formula}cij to refer to the binary relation that specifies which pairs of values are allowed for the sequence {a mathematical formula}〈xi,xj〉. For example, {a mathematical formula}≤12 denotes the constraint specified on {a mathematical formula}〈x1,x2〉 with relation “less than or equal to”.
      </paragraph>
      <paragraph>
       The complement of a constraint c is the constraint denoted {a mathematical formula}c‾ such that {a mathematical formula}var(c‾)=var(c) and {a mathematical formula}rel(c‾)=D|var(c)|∖rel(c). In other words, {a mathematical formula}c‾ and c describe complementary relations defined on the same scope. For example, {a mathematical formula}&gt;12 is the complement of {a mathematical formula}≤12.
      </paragraph>
      <paragraph>
       Given a vocabulary {a mathematical formula}〈X,D〉, an assignment is a vector {a mathematical formula}x=〈x1,⋯,xn〉 in {a mathematical formula}D|X|. An assignment x maps to each variable {a mathematical formula}xi∈X a corresponding domain value {a mathematical formula}xi∈D. An assignment x satisfies a constraint c if the projection of x onto the scope {a mathematical formula}var(c) is a member of {a mathematical formula}rel(c). An assignment x violates a constraint c, or equivalently, the constraint c rejects x, if x does not satisfy c. An assignment x satisfies a constraint network C if x satisfies every constraint c in C. Such an assignment is called a solution of C. A non-solution of C is an assignment that violates at least one constraint from C.
      </paragraph>
      <paragraph>
       The set of all solutions of a constraint network C is denoted {a mathematical formula}sol(C). C is satisfiable if {a mathematical formula}sol(C)≠∅, and unsatisfiable otherwise. If {a mathematical formula}sol(C)⊆sol(C′) for two constraint networks C and {a mathematical formula}C′, then we say that C entails{a mathematical formula}C′. Finally, if {a mathematical formula}sol(C)=sol(C′), then we say that C and {a mathematical formula}C′ are equivalent.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Constraint language and bias
      </section-title>
      <paragraph>
       Borrowing from [14], a constraint language is a set of relations that restricts the type of constraints that are allowed when modeling a network.
      </paragraph>
      <paragraph label="Definition 3">
       Constraint languageA constraint language is a set {a mathematical formula}Γ={r1,⋯,rt} of t relations over some subset of {a mathematical formula}Z.
      </paragraph>
      <paragraph>
       In this study, we shall concentrate on constraint languages of fixed arity; for such languages, the arity of any relation occurring in a constraint language Γ is bounded by a constant k. Based on this assumption, global constraints will not be considered in this paper. Recall that global constraints, such as alldifferent, are relations defined for any (unbounded) arity. Though our framework does not prevent their use, the fact that a single global constraint leads to an exponential number of possible constraints, one for each subset of X for a vocabulary {a mathematical formula}〈X,D〉, makes most learning tasks analyzed in this paper non-polynomial.
      </paragraph>
      <paragraph label="Definition 4">
       ExtensionGiven a prefixed vocabulary {a mathematical formula}〈X,D〉, the extension of Γ on {a mathematical formula}〈X,D〉 is the set {a mathematical formula}BΓ of all constraints c for which {a mathematical formula}var(c) is a tuple of variables of X, and there exists a relation {a mathematical formula}ri in Γ such that {a mathematical formula}rel(c)=ri∩D|var(c)|.
      </paragraph>
      <paragraph>
       We are now ready to define the bias{sup:1} for the acquisition problem, that is, the set of all possible constraints that are candidate for being in the constraint network to be learned.
      </paragraph>
      <paragraph label="Definition 5">
       BiasGiven a prefixed vocabulary {a mathematical formula}〈X,D〉, the bias for the learning task is a set {a mathematical formula}B⊆BΓ.
      </paragraph>
      <paragraph>
       The bias B can be equal to the extension {a mathematical formula}BΓ of Γ on the vocabulary. But it can also be a strict subset if we have some initial information on the problem to be elicited. With these notions in hand, any constraint network C defined over Γ is simply a subset of {a mathematical formula}BΓ. Note that the extension of a constraint language of bounded arity k is always polynomial in the input dimension. Indeed, the size of {a mathematical formula}BΓ is bounded by {a mathematical formula}nkt in the general case, and by {a mathematical formula}n2t in the setting of binary constraint networks. By contrast, if Γ was containing global constraints, the size of {a mathematical formula}BΓ would no longer be polynomial. A single global constraint gives rise to {a mathematical formula}2n possible constraints in {a mathematical formula}BΓ.
      </paragraph>
      <paragraph label="Example 1">
       Consider the binary constraint language {a mathematical formula}Γ={≤,≠,≥} over {a mathematical formula}Z. Given the vocabulary defined by {a mathematical formula}X={x1,x2,x3} and {a mathematical formula}D={1,2,3,4}, we observe that the constraint network {a mathematical formula}C={≤12,≥12,≤23,≠23} is indeed a subset of the extension {a mathematical formula}BΓ={≤12,≠12,≥12,≤13,≠13,≥13,≤23,≠23,≥23}. ⋄
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       Concept learning
      </section-title>
      <paragraph>
       In inductive learning, it is generally assumed that learning algorithms operate over some concept class which captures the space of concepts that the learner can potentially generate over all possible sets of examples. In the setting of constraint acquisition, the concept class is defined according to a bias B, which specifies the constraints that are allowed for modeling the target constraint network.
      </paragraph>
      <paragraph>
       Given a prefixed vocabulary {a mathematical formula}〈X,D〉, a concept is a Boolean function over {a mathematical formula}D|X|, that is, a map that assigns to each assignment x a value in {a mathematical formula}{0,1}. Given two concepts f and g, and a set A of assignments, we shall write {a mathematical formula}f⊆g if {a mathematical formula}f−1(1)⊆g−1(1), and {a mathematical formula}f⊆A if {a mathematical formula}f−1(1)⊆A. A representation of a concept f is a constraint network C for which {a mathematical formula}f−1(1)=sol(C). A concept f is said to be representable by a bias B if there is a subset C of B such that C is a representation of f. We denote by {a mathematical formula}fC a concept represented by a set of constraints C. The concept class of B, denoted {a mathematical formula}CB, is the set of all concepts that are representable by B. For instance, the concept class {a mathematical formula}CB defined over the bias {a mathematical formula}B=B≤,≠,≥ is the set of all Boolean functions that are representable by inequality and disequality constraints.
      </paragraph>
      <paragraph>
       An example is a pair {a mathematical formula}e=〈x(e),y(e)〉 where {a mathematical formula}x(e) is an assignment and {a mathematical formula}y(e) is a value in {a mathematical formula}{0,1}. If {a mathematical formula}y(e)=1, e is called a positive example and, if {a mathematical formula}y(e)=0, e is called a negative example. A training set is a set {a mathematical formula}E={e1,⋯,em} of examples. A concept f is consistent with an example e if {a mathematical formula}f(x(e))=y(e). By extension, a concept f is consistent with a training set E if f is consistent with every example in E.
      </paragraph>
      <paragraph label="Example 2">
       Given again the vocabulary defined by {a mathematical formula}X={x1,x2,x3} and {a mathematical formula}D={1,2,3,4} and the bias {a mathematical formula}B=B≤,≠,≥, let us examine the concepts {a mathematical formula}f1 and {a mathematical formula}f2 represented by the networks {a mathematical formula}C1={=12,&lt;23} and {a mathematical formula}C2={=12,≤23}, respectively. Here, {a mathematical formula}=12 is an abbreviation of {a mathematical formula}{≤12,≥12} and {a mathematical formula}&lt;23 is an abbreviation of {a mathematical formula}{≤23,≠23}. Notice that {a mathematical formula}f1,f2∈CB. Suppose that we are given the training set E specified in the following table.{a mathematical formula}We can easily verify that {a mathematical formula}f1 is consistent with E whereas {a mathematical formula}f2 is inconsistent with E because {a mathematical formula}e3 is not consistent with {a mathematical formula}f2. ⋄
      </paragraph>
      <paragraph>
       Based on these notions, we are now in position to introduce the useful notion of version space, adapted from [27] and [24].
      </paragraph>
      <paragraph label="Definition 6">
       Version space/collapsing/convergenceGiven a bias B and a training set E, the version space of E with respect to B, denoted {a mathematical formula}CB(E), is the set of all concepts in {a mathematical formula}CB that are consistent with E. We say that the version space of E has collapsed if {a mathematical formula}CB(E)=∅. We say that the version space of E has converged if {a mathematical formula}|CB(E)|=1, i.e. {a mathematical formula}CB(E) is reduced to a singleton.
      </paragraph>
      <paragraph>
       Borrowing the terminology of [18], [22], any version space is a “convex poset”, which basically means that for any subset {a mathematical formula}F={f1,f2,⋯,fp} of concepts in {a mathematical formula}CB such that {a mathematical formula}f1⊆f2⊆⋯⊆fp, if the extreme elements {a mathematical formula}f1,fp are members of {a mathematical formula}CB(E), then the whole chain F must be included in {a mathematical formula}CB(E). This in turn means that any finite version space can be characterized by its minimal and maximal elements, with respect to set inclusion.
      </paragraph>
      <paragraph label="Definition 7">
       Maximally specific/generalGiven a bias B and a training set E, a concept {a mathematical formula}f∈CB(E) is maximally specific (resp. maximally general) if there is no {a mathematical formula}f′∈CB(E) such that {a mathematical formula}f′⊂f (resp. {a mathematical formula}f⊂f′). A constraint network {a mathematical formula}C⊆B is maximally specific (resp. maximally general) in {a mathematical formula}CB(E), if C is a representation of a maximally specific (resp. maximally general) concept {a mathematical formula}f∈CB(E).
      </paragraph>
      <paragraph label="Example 3">
       Consider again Example 2. The network {a mathematical formula}C1={=12,&lt;23} is maximally specific in {a mathematical formula}CB(E). Any other constraint from B either is implied by {a mathematical formula}C1 or rejects the positive example {a mathematical formula}e1 from E. The network {a mathematical formula}C2={≠23} is maximally general in {a mathematical formula}CB(E). We cannot remove any constraint from {a mathematical formula}C2 and still reject all negative examples. The network {a mathematical formula}C3={≥12,≠13} is also a maximally general constraint network of {a mathematical formula}CB(E). ⋄
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Constraint acquisition
     </section-title>
     <paragraph>
      In this section, we introduce several constraint acquisition problems that arise from the conjunction of concept learning and constraint programming. We provide the complexity of each of these problems.
     </paragraph>
     <section label="3.1">
      <section-title>
       Interaction between user and learner
      </section-title>
      <paragraph>
       The problem of acquiring a constraint network from examples can be viewed as an interactive process between two protagonists: the human user and a virtual learner. The user has in mind a target problem but does not know how to represent this problem into a constraint network. An interaction between the learner and the user is performed to acquire a network representing the target problem.
      </paragraph>
      <paragraph>
       There are two natural forms of interaction which emerge from the paradigm of constraint learning. In passive acquisition, the user provides classified examples and the learner is passive because it has no control on the series of supplied examples. By contrast, in active acquisition, the learner is active because it can guide the exploration of its version space by asking queries to the user.
      </paragraph>
      <paragraph>
       In machine learning, Angluin [1] defines several types of queries. An equivalence query requests the user to decide whether a given concept is equivalent to the target, and in case of negative answer, to provide an example showing the discrepancy between the given concept and the target. A membership query requests the user to classify a given example as positive or negative. In the setting of constraint acquisition, asking a user equivalence queries is unreasonable, especially because our starting assumption is that the user is not able to articulate the constraints of the target network directly. (Equivalence queries will be useful to characterize complexity of learning.) Membership queries are, however, a reasonable interaction process where the user is just asked to be able to recognize elements of her target concept.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       Problems related to constraint acquisition
      </section-title>
      <paragraph>
       The basic problem related to passive acquisition is to find whether the version space of the examples supplied by the user has collapsed, or not. If the version space has not collapsed, the learner is naturally required to provide a concept consistent with the examples.
      </paragraph>
      <paragraph label="Definition 8">
       Consistency problemGiven a bias B and a training set E, the consistency problem is to determine whether {a mathematical formula}CB(E)≠∅. If the answer is “yes”, then a representation C of some concept in {a mathematical formula}CB(E) must be returned.
      </paragraph>
      <paragraph>
       In the consistency problem, the learner only picks a concept in its version space and returns a representation of it to the user. A more informative task is to determine whether the version space has converged, or not. If this is indeed the case, the user knows that she has an exact characterization of her target problem, and hence, she does not need to supply more examples to the learner.
      </paragraph>
      <paragraph label="Definition 9">
       Convergence problemGiven a bias B and a training set E, the convergence problem is to determine whether {a mathematical formula}CB(E) has converged. If the answer is “yes”, then a representation C of the target concept in {a mathematical formula}CB(E) must be returned.
      </paragraph>
      <paragraph>
       Recall that in passive acquisition, the learner cannot control the dynamics of its version space. By contrast, in active acquisition, the learner is allowed to present queries to the user. Depending on the type TQ of query, the user is not requested the same information. With the equivalence query {a mathematical formula}EQ(C), the learner asks the user whether the network C is equivalent to its target problem or not. If not, the user must return a counter-example to the learner that is, a solution of C that is not solution of her target problem, or a solution of her target problem that is not solution of C. With the membership query {a mathematical formula}MQ(x), the learner asks the user what is the label of x. The user answers 1 if x is a solution of the target problem, and 0 otherwise. Starting from a bias, an initial training set, and a type TQ of query (among membership query and equivalence query), the goal is to find a polynomial sequence of queries leading to a network representing the target problem.
      </paragraph>
      <paragraph label="Definition 10">
       Identification problem/learnabilityGiven a bias B, a training set E on a vocabulary {a mathematical formula}〈X,D〉, and a type {a mathematical formula}TQ∈{membership,equivalence} of queries, the identification problem for a target concept f representable by Γ is to find a sequence {a mathematical formula}〈q1,⋯,qm〉 of queries of type TQ leading to the detection of a constraint network C representing f. If the length m of the sequence is polynomial in the number {a mathematical formula}|B| of constraints in the bias, we say that f is learnable by queries of type TQ.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Complexity of constraint acquisition
      </section-title>
      <paragraph>
       We now give the complexity of the constraint acquisition problems defined in the previous subsection.
      </paragraph>
      <paragraph label="Proof">
       The consistency problem can be solved in polynomial time.Given a bias B and a training set E, we compute the set C of constraints from B that are not violated by any positive example in E. This is done by traversing all positive examples from E and removing from B all constraints violated by such an example. We then check whether there exists a negative example in E satisfying C. This is done by traversing the negative examples and performing {a mathematical formula}|C| constraint checks for each of them. If a negative example satisfying C is found, we return “collapse”, otherwise we return “yes” and C as a witness.  □
      </paragraph>
      <paragraph>
       One might be tempted to believe that a version space has converged precisely when there is exactly one constraint network representing the version space. The convergence problem would then be tractable. It would be sufficient to compute the maximal subset C of B satisfying all positive examples, to check that it rejects all negative examples, and to prove that for every {a mathematical formula}c∈C, there exists a negative example solution of {a mathematical formula}C∖{c}. However, in most constraint languages, constraint relations are interdependent, as illustrated in the following example.
      </paragraph>
      <paragraph label="Example 4">
       As in Example 2, consider the vocabulary {a mathematical formula}X={x1,x2,x3} and {a mathematical formula}D={1,2,3,4} and the bias {a mathematical formula}B=B≤,≠,≥. The target network is {a mathematical formula}C={≤12,≤13,≤23}. Suppose we are given the training set E specified in the following table.{a mathematical formula}At the beginning {a mathematical formula}B={≤12,≠12,≥12,≤13,≠13,≥13,≤23,≠23,≥23}. After processing example {a mathematical formula}e1 we know that constraints {a mathematical formula}≥12,≥13, and {a mathematical formula}≥23 cannot belong to the target network C because they are violated by {a mathematical formula}e1, which is positive. After processing {a mathematical formula}e2 similarly the remaining possible constraints in the bias B are: {a mathematical formula}≤12,≤13, and {a mathematical formula}≤23. By processing {a mathematical formula}e3 we know that the constraint {a mathematical formula}≤23 is in the target network as it is the only one rejecting {a mathematical formula}e3. Similarly, {a mathematical formula}≤12 is in the target network as it is the only one rejecting {a mathematical formula}e4. Hence, there exists two constraint networks consistent with the training data E, namely {a mathematical formula}C={≤12,≤23} and {a mathematical formula}C′={≤12,≤13,≤23}. However, the version space {a mathematical formula}CB(E) contains a single concept and thus has converged because C and {a mathematical formula}C′ have the same solutions. ⋄
      </paragraph>
      <paragraph>
       On this example we see that any concept may have multiple representations because of the interdependency of constraints (i.e., the constraint {a mathematical formula}≤13 is redundant with {a mathematical formula}≤12 and {a mathematical formula}≤23). Consequently, even if the learner's version space has converged, it may still have more than one representative network. As shown by the following result, the convergence problem is much harder than the consistency problem.
      </paragraph>
      <paragraph label="Proof">
       The convergence problem is coNP-complete.First, observe that the problem is in coNP. A polynomial certificate is composed of the vocabulary {a mathematical formula}〈X,D〉, the bias B, the set E of examples, and optionally two networks {a mathematical formula}C1 and {a mathematical formula}C2 and an assignment x over {a mathematical formula}D|X|. The certificate is valid if and only if the version space {a mathematical formula}CB(E) is either empty or includes the two concepts {a mathematical formula}f1 and {a mathematical formula}f2, associated with {a mathematical formula}C1 and {a mathematical formula}C2, and the assignment x belongs to {a mathematical formula}f1∖f2. By Theorem 1, checking whether {a mathematical formula}CB(E)=∅ takes polynomial time. Checking whether {a mathematical formula}f1,f2∈CB(E) also takes polynomial time. For each positive example {a mathematical formula}e+ in E we just need to check whether {a mathematical formula}e+ belongs to {a mathematical formula}sol(C1) and to {a mathematical formula}sol(C2), which is done by checking whether {a mathematical formula}e+ satisfies all constraints in {a mathematical formula}C1 and all constraints in {a mathematical formula}C2. For each negative example {a mathematical formula}e− in E we need to check whether {a mathematical formula}e− belongs neither to {a mathematical formula}sol(C1) nor to {a mathematical formula}sol(C2), which is done by checking whether there exists a constraint in {a mathematical formula}C1 and a constraint in {a mathematical formula}C2 rejecting {a mathematical formula}e−. Finally, checking whether {a mathematical formula}f1(x)=1 and {a mathematical formula}f2(x)=0 can be done by testing whether x satisfies all constraints in {a mathematical formula}C1 and x violates at least one constraint in {a mathematical formula}C2.We now prove that the convergence problem is complete for coNP. Recall that 3-Col is the problem of deciding if a graph {a mathematical formula}(N,U) is 3-colorable, where {a mathematical formula}|N|=n. Let {a mathematical formula}〈X,D〉 be the vocabulary formed by the variables {a mathematical formula}X={x1,…,xn}, where {a mathematical formula}xi represents the node i in N, and the domain {a mathematical formula}D={0,1,r,g,b}, where {a mathematical formula}r,g and {a mathematical formula}b are the colors. The bias is {a mathematical formula}B=BΓ, with {a mathematical formula}Γ={r≠,r01,r4}, where {a mathematical formula}r≠ is the binary relation {a mathematical formula}D2∖{(r,r),(g,g),(b,b)}, {a mathematical formula}r01 is the binary relation {a mathematical formula}(D×{0})∪({0}×D)∪{(1,1)}, and {a mathematical formula}r4 is the quaternary relation formed by the set {a mathematical formula}{0,r,g,b}4∪{0,1}4 from which we remove all 4-tuples containing two 0s and two different colors, all 4-tuples containing no 1s and exactly one 0, and the tuple {a mathematical formula}(1,1,1,1).Any constraint network C over B is composed of binary constraints {a mathematical formula}c≠(xi,xj) such that {a mathematical formula}rel(c≠)=r≠, and/or binary constraints {a mathematical formula}c01(xi,xj) such that {a mathematical formula}rel(c01)=r01, and/or quaternary constraints {a mathematical formula}c4(xi,xj,xk,xl), such that {a mathematical formula}rel(c4)=r4.We now build the training set E as follows:
       <list>
        for every 4-tuple {a mathematical formula}i,j,k,l in {a mathematical formula}{1,⋯,n}, let {a mathematical formula}eijkl− be the negative example, where {a mathematical formula}x(eijkl−) is the n-tuple containing 1 in positions {a mathematical formula}i,j,k,l, and 0s everywhere else,for every pair {a mathematical formula}{i,j}∈U with {a mathematical formula}i&lt;j, let {a mathematical formula}eij− be the negative example where {a mathematical formula}x(eij−) is the n-tuple containing only 0s except in positions i and j where it contains {a mathematical formula}b,for every pair {a mathematical formula}{i,j}∉U with {a mathematical formula}i&lt;j, let {a mathematical formula}eij+ be the positive example where {a mathematical formula}x(eij+) is again the n-tuple containing only 0s except in positions i and j where it contains {a mathematical formula}b.Note that
       </list>
       <paragraph>
        {a mathematical formula}c4(xi,xj,xk,xl) is the only constraint that rejects the example {a mathematical formula}eijkl−. We are thus guaranteed that all {a mathematical formula}c4 for any quadruple of variables belong to all representations of consistent concepts. Moreover, for each pair {a mathematical formula}{i,j}∈U, {a mathematical formula}c01(xi,xj) and {a mathematical formula}c≠(xi,xj) are the only constraints that reject the example {a mathematical formula}eij−. So we know that any representation of a consistent concept contains either {a mathematical formula}c0(xi,xj) or {a mathematical formula}c≠(xi,xj). Finally, because of {a mathematical formula}eij+ we know that any representation of a concept in the version space excludes {a mathematical formula}c0(xi,xj) and {a mathematical formula}c≠(xi,xj) for each pair {a mathematical formula}{i,j}∉U. Hence, the constraint network S containing all constraints {a mathematical formula}c4 in B, and the constraints {a mathematical formula}c≠(xi,xj) and {a mathematical formula}c01(xi,xj) for all pairs {a mathematical formula}{i,j}∈U, is guaranteed to be the maximally specific network of E with respect to Γ.Based on these considerations, the version space has not converged if and only if there exists a constraint network C such that {a mathematical formula}fC is in the version space and {a mathematical formula}sol(S)⊂sol(C).Let x be a member of {a mathematical formula}sol(C)∖sol(S). Suppose that x contains a 1. We know that all {a mathematical formula}c4 constraints belong to C. So, x contains only 0s and 1s. Yet, because {a mathematical formula}r≠ and {a mathematical formula}r01 accept all tuples in {a mathematical formula}{0,1}2, x would necessarily be solution of S, a contradiction. So x contains only 0s and colors. Obviously, x must contain less that {a mathematical formula}n−1 0s because any tuple with a number of 0s greater than or equal to {a mathematical formula}n−1 is a solution of S. Suppose that x contains {a mathematical formula}n−2 0s, and colors at positions i and j. Then {a mathematical formula}xi and {a mathematical formula}xj must have the same color in x because otherwise any {a mathematical formula}c4 involving {a mathematical formula}xi and {a mathematical formula}xj would be violated. If the pair {a mathematical formula}{i,j} is not in U, then x is already a solution of S because it satisfies the same constraints as {a mathematical formula}eij+. So, {a mathematical formula}{i,j} must be an edge in U, but since C rejects any negative example {a mathematical formula}eij−, either {a mathematical formula}c01(xi,xj) or {a mathematical formula}c≠(xi,xj) belongs to C. In any case, this implies that {a mathematical formula}xi and {a mathematical formula}xj cannot take the same colors, a contradiction. So x must have at least three entries {a mathematical formula}i,j,k taking colors. But if there is a 0 at an arbitrary position l of x, the {a mathematical formula}c4 involving {a mathematical formula}xi,xj,xk and the variable {a mathematical formula}xl taking 0 rejects the tuple. As a result, x contains only colors. Since all {a mathematical formula}c01 are violated by x, it follows that all {a mathematical formula}c≠(xi,xj) with {a mathematical formula}{i,j}∈U are satisfied by x, and hence, x is a 3-coloring of {a mathematical formula}(N,U).Conversely, any 3-coloring x of {a mathematical formula}(N,U) is not a solution of S but is a solution of any network C containing all {a mathematical formula}c4 and all {a mathematical formula}c≠(xi,xj) for {a mathematical formula}{i,j}∈U. Clearly, the concept of C is consistent with E. So, if {a mathematical formula}(N,U) is 3-colorable, the version space has not converged.To summarize, the version space has converged if and only if the problem of coloring {a mathematical formula}(N,U) with three colors has no solution. Our construction contains {a mathematical formula}O(n4) constraints and {a mathematical formula}O(n4) examples, and hence, it is polynomial in the size of {a mathematical formula}(N,U). Therefore, the convergence problem is coNP-complete.  □
       </paragraph>
      </paragraph>
      <paragraph label="Theorem 3">
       Given any biasBand any target concept f representable byB, f is learnable by equivalence queries.
      </paragraph>
      <paragraph label="Proof">
       The following algorithm guarantees that we find a network representing the target concept with a polynomial number of equivalence queries. Remember that {a mathematical formula}EQ(C) returns an assignment in {a mathematical formula}(f∪sol(C))∖(f∩sol(C)).{a mathematical formula}Soundness. The algorithm is sound because it returns C only when {a mathematical formula}EQ(C) has returned true, which means that C is a representation of the target concept f.Termination in polynomial number of queries. Let {a mathematical formula}Cf be any network representing f over Γ, that is, {a mathematical formula}sol(Cf)=f−1(1). The property “{a mathematical formula}Cf⊆C” is true when we start the loop in Line 3 because {a mathematical formula}Cf⊆B=C. Hence, if the first call to {a mathematical formula}EQ(C) does not return true, it necessarily returns an example e such that {a mathematical formula}y(e)=1 and {a mathematical formula}x(e)∈f−1(1)∖sol(C). Constraints removed from C in Line 4 reject e and thus do not belong to {a mathematical formula}Cf, so “{a mathematical formula}Cf⊆C” remains true. The next example {a mathematical formula}e′ returned by {a mathematical formula}EQ(C) will again be such that {a mathematical formula}y(e′)=1 and {a mathematical formula}x(e′)∈f−1(1)∖sol(C), and “{a mathematical formula}Cf⊆C” is an invariant of the loop. In addition, there is at least one constraint in C rejecting e in Line 4 otherwise {a mathematical formula}EQ(C) would have returned true. Therefore C strictly decreases in size each time we go through the loop and the algorithm terminates in {a mathematical formula}O(|B|) number of equivalence queries.  □
      </paragraph>
      <paragraph label="Theorem 4">
       There exist biasesBwith fixed arity constraints, training sets E, and target concepts f representable byBthat are not learnable by membership queries, even if E contains a positive example.
      </paragraph>
      <paragraph label="Proof">
       Let Γ be the language containing only two binary constraints, given by {a mathematical formula}c1={(0,0),(1,1),(2,2)} and {a mathematical formula}c2={(0,1),(1,0),(2,2)}. Let B be the bias with 2n variables, domain {a mathematical formula}{0,1,2}, and language Γ, and let {a mathematical formula}CB be the class of concepts representable by this bias. Let G be the subset of {a mathematical formula}CB composed of all concepts representable by networks composed of n constraints from Γ, one constraint for each pair {a mathematical formula}(xi,xn+i) of variables. Every network representing a concept in G can be associated with a vector t of length n constructed by setting {a mathematical formula}t[i] to 0 if {a mathematical formula}c1(xi,xn+i) and {a mathematical formula}t[i] to 1 if {a mathematical formula}c2(xi,xn+i). Thus there is a bijection between G and {a mathematical formula}{0,1}n and G is isomorphic to {a mathematical formula}{0,1}n. As a result, the size of G is {a mathematical formula}2n. Suppose that the positive example defined over the assignment {a mathematical formula}(2,2,⋯,2) has already been given by the user. This does not eliminate any concept from G. Given any assignment {a mathematical formula}x∈{0,1,2}2n, a membership query answered 0 will eliminate at most one concept from G (when {a mathematical formula}x∈{0,1}2n). Therefore, whatever the strategy of generation of the queries, there exists a target concept f in G requiring at least {a mathematical formula}2n−1 queries for identifying it.  □
      </paragraph>
     </section>
    </section>
    <section label="4">
     The passive Conacq algorithm
     <paragraph>
      We are now in position to present the Conacq architecture. In this section, we concentrate on passive constraint acquisition, where the learner is presented a pool of examples and must determine the state of its version space. We provide a compact representation of version spaces, next we analyze the complexity of some common operations on versions spaces in this representation, and we finally describe the Conacq.1 algorithm.
     </paragraph>
     <section label="4.1">
      <section-title>
       Representing version spaces
      </section-title>
      <paragraph>
       The basic building block of our learning architecture is a clausal representation of version spaces for constraint biases. Given a prefixed vocabulary {a mathematical formula}〈X,D〉 and a bias B, the version space {a mathematical formula}CB(E) of a training set E is encoded into a clausal theory T, where each model of T is a representation of some concept in {a mathematical formula}CB(E). The size of T is linear in the number of examples, and its structure allows us to check satisfiability in polynomial time.
      </paragraph>
      <paragraph>
       Formally, any constraint {a mathematical formula}c∈B is associated with a Boolean atom, denoted {a mathematical formula}a(c). As usual, a positive literal is an atom {a mathematical formula}a(c), and a negative literal is the negation {a mathematical formula}¬a(c) of an atom. A clause α is a disjunction of literals; α is a Horn clause (resp. dual Horn clause) if it includes at most one positive (resp. negative) literal, and α is a unit clause if it includes exactly one (positive or negative) literal. A clausal theory T is a conjunction of clauses, and a Horn theory (resp. dual Horn theory) is a conjunction of Horn (resp. dual Horn) clauses. The size{a mathematical formula}|T| of a clausal theory T is given by the sum of the sizes of its clauses, where the size of a clause is the number of its literals. Given a clause α in T, {a mathematical formula}constraints(α) is the set {a mathematical formula}{ci|a(ci)∈α}.
      </paragraph>
      <paragraph>
       Any Boolean assignment I in {a mathematical formula}{0,1}|B| is called an interpretation, and we write {a mathematical formula}I[a(c)] to denote the (Boolean) value of the atom {a mathematical formula}a(c) under I. An interpretation I is a model of a clausal theory T if T is true in I according to the standard propositional semantics. The set of models of T is denoted {a mathematical formula}models(T). A clausal theory T is satisfiable if {a mathematical formula}models(T)≠∅, and unsatisfiable otherwise. Given two clausal theories T and {a mathematical formula}T′, we say that T entails{a mathematical formula}T′, and write {a mathematical formula}T⊨T′, if {a mathematical formula}models(T)⊆models(T′).
      </paragraph>
      <paragraph>
       The transformation φ assigns to each interpretation I over {a mathematical formula}{0,1}|B| a corresponding constraint network {a mathematical formula}φ(I) in {a mathematical formula}CB defined by the set of all constraints {a mathematical formula}c∈B such that {a mathematical formula}I[a(c)]=1. Clearly, φ is bijective. In the following {a mathematical formula}φ−1(C) denotes the characteristic model of C obtained by setting each atom {a mathematical formula}a(c) from B to 1 if {a mathematical formula}c∈C and to 0 otherwise. Given a set {a mathematical formula}C of constraint networks, {a mathematical formula}φ−1(C) denotes the set of characteristic models of the networks in {a mathematical formula}C.
      </paragraph>
      <paragraph>
       It is important to keep in mind that the negative literal {a mathematical formula}¬a(c) does not represent the complement {a mathematical formula}c‾ of the constraint c. Instead, {a mathematical formula}¬a(c) denotes the absence of c in the network. Thus, {a mathematical formula}¬a(c) captures a weaker form of negation than {a mathematical formula}c‾. For instance, given two variables {a mathematical formula}xi and {a mathematical formula}xj, the constraint {a mathematical formula}≤ij‾ is {a mathematical formula}&gt;12, but the literal {a mathematical formula}¬a(≤12) simply specifies the absence of {a mathematical formula}≤12 in the network.
      </paragraph>
      <paragraph label="Definition 11">
       Clausal representationGiven a bias B and a training set E, the clausal representation of {a mathematical formula}CB(E) is the dual Horn formula defined by:{a mathematical formula} where {a mathematical formula}κ(x) is the set of constraints c in B such that x violates c.
      </paragraph>
      <paragraph>
       Clearly, the clausal encoding of a version space can be performed incrementally: on each incoming example e, encode e as a set of clauses using {a mathematical formula}κ(x(e)). If e is positive, we must discard from the version space all concepts that reject {a mathematical formula}x(e). This is done by expanding the theory with a unit clause {a mathematical formula}¬a(c) for each constraint c in {a mathematical formula}κ(x(e)). Dually, if e is negative, we must discard from the version space all concepts that accept {a mathematical formula}x(e). This is done by expanding the theory with the clause consisting of all literals {a mathematical formula}a(c) in {a mathematical formula}κ(x(e)). The resulting theory T is indeed a dual Horn formula because each clause contains at most one negative literal.
      </paragraph>
      <paragraph label="Example 5">
       We wish to acquire a constraint network specified over the variables {a mathematical formula}{x1,x2,x3,x4}, the domain {a mathematical formula}{1,2,3,4}, and the bias {a mathematical formula}B=B≤,≠,≥. Suppose that the target network contains only one constraint, namely {a mathematical formula}x1≠x4. The following table illustrates how the clausal theory T is expanded after processing each example of some training set E.{a mathematical formula}The fourth clause can be reduced to {a mathematical formula}a(≠14)∨a(≠23) using unit propagation over T, and hence the third clause can be removed from T because it is subsumed by the reduced fourth clause. Thus, models of T are all interpretations that set {a mathematical formula}a(≠14) or {a mathematical formula}a(≠23) to true and that falsify all {a mathematical formula}a(≤ij) and all {a mathematical formula}a(≥ij). ⋄
      </paragraph>
      <paragraph>
       The next theorem establishes a one-to-one correspondence between the representations of concepts in the version space and the models of the clausal theory.
      </paragraph>
      <paragraph label="Proof">
       Let{a mathematical formula}〈X,D〉be a prefixed vocabulary,Ba bias, and E a training set. Let T be the clausal representation of{a mathematical formula}CB(E). Then,{a mathematical formula}Consider a representation {a mathematical formula}C=φ(I) such that {a mathematical formula}I∈models(T) but {a mathematical formula}fC∉CB(E). We show that this leads to a contradiction. Obviously, {a mathematical formula}fC must be inconsistent with at least one example e in the training set. If e is positive then {a mathematical formula}fC(x(e))=0, which implies that {a mathematical formula}x(e)∉sol(C). So, there is at least one constraint in C that is included in {a mathematical formula}κ(x(e)). It follows that I violates the clause set {a mathematical formula}⋀c∈κ(x(e))¬a(c), and hence, I cannot be a model of T. Dually, if e is negative then {a mathematical formula}fC(x(e))=1, which implies that {a mathematical formula}x(e)∈sol(C). So, there is no constraint in C that belongs to {a mathematical formula}κ(x(e)). It follows that I violates the clause {a mathematical formula}⋁c∈κ(x(e))a(c), and hence, I cannot be a model of T.Consider now a representation {a mathematical formula}C=φ(I) such that {a mathematical formula}fC∈CB(E) but {a mathematical formula}I∉models(T). Again, we show that this leads to a contradiction. If {a mathematical formula}I∉models(T), then there is at least one example e in E such that I falsifies the set of clauses generated from e. If e is positive then I must violate the conjunction {a mathematical formula}⋀c∈κ(x(e))¬a(c). So C must include at least one member of {a mathematical formula}κ(x(e)), which implies that at least one constraint in C is violated by {a mathematical formula}x(e). Therefore, {a mathematical formula}x(e)∉sol(C), and hence, {a mathematical formula}fC cannot be consistent with e. Dually, if e is negative then I must violate the disjunction {a mathematical formula}⋁c∈κ(x(e))a(c). So C must exclude all constraints c in {a mathematical formula}κ(x(e)), which implies that no constraint in C is violated by {a mathematical formula}x(e). Therefore, {a mathematical formula}x(e)∈sol(C), and hence, {a mathematical formula}fC cannot be consistent with e.  □
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Operations on the clausal representation of version spaces
      </section-title>
      <paragraph>
       The key interest of the clausal encoding of version spaces is to exploit the algorithmic properties of dual Horn theories.
      </paragraph>
      <paragraph>
       To this point, recall that unit propagation is the process of applying unit resolution (a resolution step in which at least one resolvant is a unit clause) on a clausal theory T until no further unit resolution steps can be carried out, or until the empty clause is obtained. If T is Horn or dual Horn, then unit propagation is enough for deciding the satisfiability of T in linear ({a mathematical formula}O(|T|)) time, by either returning a model I of T, or deriving the empty clause [17]. Interestingly, the “unit implicates” of Horn theories and dual Horn theories can also be computed efficiently. For a clausal formula T, let {a mathematical formula}unit+(T) (resp. {a mathematical formula}unit−(T)) be the set of positive (resp. negative) literals ℓ such that {a mathematical formula}T⊨ℓ, and let {a mathematical formula}unit(T)=unit+(T)∪unit−(T). A clausal theory T is reduced (under unit propagation) if there is no clause in T that properly contains a unit clause in {a mathematical formula}unit(T), or that contains the complementary literal of a unit clause in {a mathematical formula}unit(T).
      </paragraph>
      <paragraph>
       As shown in [13] (Chapter 5.3), if T is Horn or dual Horn, then it can be reduced in linear time, using unit propagation. Furthermore, the set {a mathematical formula}unit+(T) can be computed in linear time if T is a Horn theory, and the set {a mathematical formula}unit−(T) can be computed in linear time if T is a dual Horn theory.
      </paragraph>
      <paragraph>
       With these notions in hand, we shall consider in the following a prefixed vocabulary {a mathematical formula}〈X,D〉, a bias B, and a training set E. Based on Definition 11, the size of the dual Horn theory T encoding {a mathematical formula}CB(E) is given by {a mathematical formula}∑e|κ(x(e))|. Since {a mathematical formula}|κ(x(e))| contains at most {a mathematical formula}|B| constraints, it follows that {a mathematical formula}|T| is bounded by {a mathematical formula}|E|⋅|B|.
      </paragraph>
      <paragraph>
       We first show that the consistency problem is solvable in linear time.
      </paragraph>
      <paragraph label="Proof">
       The consistency problem can be solved in{a mathematical formula}O(|E|⋅|B|)time.By Theorem 5, we know that if T includes at least one model I then {a mathematical formula}CB(E) covers the concept {a mathematical formula}fφ(I). Conversely, if {a mathematical formula}CB(E) covers at least one concept f, then T includes all models I for which {a mathematical formula}f=sol(φ(I)). It follows that T is satisfiable if and only if {a mathematical formula}CB(E) is not empty. Since the satisfiability of T can be decided in {a mathematical formula}O(|T|) time, the result follows.  □
      </paragraph>
      <paragraph>
       In the update operation, we need to compute a new representation of the version space formed by the addition of a new example to the training set.
      </paragraph>
      <paragraph label="Proof">
       The update operation takes{a mathematical formula}O(|B|)time.Adding a new example e means expanding T with the encoding of e. The cost of this expansion is dominated by the construction of {a mathematical formula}κ(x(e)). Checking whether a constraint is satisfied or violated by an example e is in {a mathematical formula}O(1). Thus, the number of such checks is bounded by {a mathematical formula}|B|, which concludes.  □
      </paragraph>
      <paragraph>
       Consider a pair of training sets {a mathematical formula}E1 and {a mathematical formula}E2. The intersection operation requires computing a representation of the version space {a mathematical formula}CB(E1)∩CB(E2). This operation is interesting when we acquire a network from examples supplied by multiple users.
      </paragraph>
      <paragraph label="Proof">
       The intersection operation takes{a mathematical formula}O((|E1|+|E2|)⋅|B|)time.Let {a mathematical formula}T1 and {a mathematical formula}T2 be the representations of the version spaces {a mathematical formula}CB(E1) and {a mathematical formula}CB(E2), respectively. The representation of the version space {a mathematical formula}CB(E1)∩CB(E2) is simply obtained by {a mathematical formula}T1∧T2, which cost is in the sum of the sizes of the two theories, that is, {a mathematical formula}O((|E1|+|E2|)⋅|B|).  □
      </paragraph>
      <paragraph>
       Given a pair of training sets {a mathematical formula}E1 and {a mathematical formula}E2, we may wish to determine whether {a mathematical formula}CB(E1) is a subset of {a mathematical formula}CB(E2), or whether {a mathematical formula}CB(E1) is equal to {a mathematical formula}CB(E2).
      </paragraph>
      <paragraph label="Proposition 4">
       The subset and equality tests take{a mathematical formula}O(|E1|⋅|E2|⋅|B|2)time.
      </paragraph>
      <paragraph label="Proof">
       Let {a mathematical formula}T1 and {a mathematical formula}T2 be the clausal theories of {a mathematical formula}CB(E1) and {a mathematical formula}CB(E2), respectively. If {a mathematical formula}CB(E1)⊆CB(E2) then we must have {a mathematical formula}models(T1)⊆models(T2) because otherwise there would be a model I in {a mathematical formula}models(T1)∖models(T2), implying by Theorem 5 that {a mathematical formula}fφ(I)∈CB(E1) but {a mathematical formula}fφ(I)∉CB(E2), a contradiction. Conversely, if {a mathematical formula}models(T1)⊆models(T2) then we must have {a mathematical formula}CB(E1)⊆CB(E2) because otherwise there would exist a concept f in {a mathematical formula}CB(E1)∖CB(E2) implying, again by Theorem 5, that for every representation C of f, {a mathematical formula}φ−1(C) is a model of {a mathematical formula}T1, but not a model of {a mathematical formula}T2, a contradiction. Thus, deciding whether {a mathematical formula}CB(E1)⊆CB(E2) is equivalent to deciding whether {a mathematical formula}models(T1)⊆models(T2), which is equivalent to state whether {a mathematical formula}T1 entails {a mathematical formula}T2. By Lemma 5.6.1 from [13], the entailment problem of two dual Horn formulas {a mathematical formula}T1 and {a mathematical formula}T2 can be decided in {a mathematical formula}O(|T1|⋅|T2|) time. It follows that the subset operation takes {a mathematical formula}O(|B|2⋅|E1|⋅|E2|) time. For the equality operation, we simply need to check whether {a mathematical formula}T1 entails {a mathematical formula}T2 and {a mathematical formula}T2 entails {a mathematical formula}T1.  □
      </paragraph>
      <paragraph>
       The membership test is to determine whether a given constraint network is associated, or not, to a consistent concept in the version space.
      </paragraph>
      <paragraph label="Proof">
       The membership test takes{a mathematical formula}O(|E|⋅|B|)time.Let {a mathematical formula}C⊆B be a constraint network. By Theorem 5, determining whether {a mathematical formula}fC∈CB(E) is equivalent to check whether {a mathematical formula}φ−1(C)∈models(T), which can be done in {a mathematical formula}O(|T|) time.  □
      </paragraph>
      <paragraph>
       Apart from the membership test, we might also be interested to check whether a given assignment is classified in the same way by all concepts in the version space. Specifically, an assignment x is predictable by {a mathematical formula}CB(E), if {a mathematical formula}CB(E) is not empty, and {a mathematical formula}f(x)=1 for all {a mathematical formula}f∈CB(E), or {a mathematical formula}f(x)=0 for all {a mathematical formula}f∈CB(E). The prediction test suggested in [24], is to determine whether an assignment is predictable, or not.
      </paragraph>
      <paragraph label="Proof">
       The prediction test takes{a mathematical formula}O(|E|⋅|B|)time.Given an assignment x, let {a mathematical formula}T0 (resp. {a mathematical formula}T1) be the clausal theory obtained from T by updating {a mathematical formula}CB(E) with the example {a mathematical formula}〈x,0〉 (resp. {a mathematical formula}〈x,1〉). {a mathematical formula}T0 is unsatisfiable if and only if {a mathematical formula}CB(E∪{〈x,0〉})=∅. This condition holds if and only if {a mathematical formula}f(x)=1 for all {a mathematical formula}f∈CB(E). Analogously, {a mathematical formula}T1 is unsatisfiable if and only if {a mathematical formula}f(x)=0 for all {a mathematical formula}f∈CB(E). Note that {a mathematical formula}CB(E)=∅ if and only if both {a mathematical formula}T0 and {a mathematical formula}T1 are unsatisfiable. So, x is predictable if and only if exactly one of {a mathematical formula}T0 or {a mathematical formula}T1 is unsatisfiable. Since by Proposition 2 the update of {a mathematical formula}CB(E) with {a mathematical formula}〈x,0〉 (or {a mathematical formula}〈x,1〉) takes {a mathematical formula}O(|B|) time, the prediction test requires only two satisfiability tests over dual Horn formulas, which takes {a mathematical formula}O(|E|⋅|B|) time.  □
      </paragraph>
      <paragraph>
       We close the list of operations be examining the boundary elements of the version space, that is, the maximally specific concepts and the maximally general ones.
      </paragraph>
      <paragraph label="Proposition 7">
       If{a mathematical formula}CB(E)≠∅, then the maximally specific concept is unique and a representation of it can be computed in{a mathematical formula}O(|E|⋅|B|)time.
      </paragraph>
      <paragraph label="Proof">
       Suppose that {a mathematical formula}CB(E)≠∅, and consider the network {a mathematical formula}C={c∈B|¬a(c)∉unit−(T)}. Note that C can be constructed in {a mathematical formula}O(|E|⋅|B|) time by computing {a mathematical formula}unit−(T) using unit propagation, and taking the atoms in B which do not occur in {a mathematical formula}unit−(T). Now, consider any concept {a mathematical formula}f′∈CB such that {a mathematical formula}f′⊂fC. Any representation {a mathematical formula}C′ of {a mathematical formula}f′ must include at least one constraint c in {a mathematical formula}B∖C. This implies that {a mathematical formula}¬a(c)∈unit−(T), which in turn implies that {a mathematical formula}ϕ−1(C′) does not satisfy T. Therefore, {a mathematical formula}f′∉CB(E), and hence, {a mathematical formula}fC is the unique maximally specific concept in {a mathematical formula}CB(E).  □
      </paragraph>
      <paragraph>
       Unfortunately, a dual property for maximally general concepts cannot be derived, due to the fact that their number can grow exponentially with the size of B. Indeed, as observed in [23], even in the very restricted case where the domain is {a mathematical formula}{0,1} and B is a set of n unary constraints c with {a mathematical formula}rel(c)={1}, there are training sets E such that the number of maximally general concepts in {a mathematical formula}CB(E) is exponential in n. A tractable result can yet be derived when the clausal representation T is reduced to a monomial (i.e., conjunction of unit clauses).
      </paragraph>
      <paragraph label="Proposition 8">
       If{a mathematical formula}CB(E)≠∅and T is a monomial, then the maximally general concept is unique and a representation of it can be computed in{a mathematical formula}O(|E|⋅|B|)time.
      </paragraph>
      <paragraph label="Proof">
       Suppose again that {a mathematical formula}CB(E)≠∅, and consider the network {a mathematical formula}C={c∈B|a(c)∈unit+(T)}. Because T is a monomial, it is both Horn and dual Horn, and hence, C can be constructed in {a mathematical formula}O(|T|) time by deriving {a mathematical formula}unit+(T) via unit propagation. Now, consider any concept {a mathematical formula}f′∈CB such that {a mathematical formula}fC⊂f′. Here, any representation {a mathematical formula}C′ of {a mathematical formula}f′ must exclude at least one constraint {a mathematical formula}c∈C. This implies that {a mathematical formula}a(c)∉unit+(T), which in turn implies that {a mathematical formula}ϕ−1(C′) violates T. Therefore, {a mathematical formula}f′∉CB(E), and hence, {a mathematical formula}fC is the unique maximally general concept.  □
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       The algorithm
      </section-title>
      <paragraph>
       The Conacq.1 algorithm is presented in Algorithm 1. It takes as input a bias B and a training set E, and returns as output a clausal theory T that encodes the version space {a mathematical formula}CB(E). The algorithm starts from the empty theory (Line 1) and iteratively expands it by encoding each example in the training set (Line 2). If e is negative, we must discard from the version space all concepts that accept {a mathematical formula}x(e). This is done by expanding the theory with the clause consisting of all literals {a mathematical formula}a(c) in {a mathematical formula}κ(x(e)) (Line 4). Dually, if e is positive, we must discard from the version space all concepts that reject {a mathematical formula}x(e). This is done by expanding the theory with a unit clause {a mathematical formula}¬a(c) for each constraint c in {a mathematical formula}κ(x(e)) (Line 5). After encoding the example, a “collapse” message is returned if the theory is no longer satisfiable (Line 6). When all examples in the training set are processed, Conacq.1 calls a convergence procedure to determine whether {a mathematical formula}CB(E) is reduced to a singleton set, or not (Line 7). Finally Conacq.1 returns the resulting theory encoding {a mathematical formula}CB(E) together with the flag for convergence (Line 8).
      </paragraph>
      <paragraph>
       As stated by Theorem 2, the convergence problem is coNP-complete. A naive strategy for implementing the Convergence  procedure is to start from the interpretation I encoding the maximally specific network (as detailed in Proposition 7), and to explore the other models of T in order to find an interpretation {a mathematical formula}I′ for which the constraint networks {a mathematical formula}ϕ−1(I) and {a mathematical formula}ϕ−1(I′) are not equivalent. Yet, due to the exponential number of models of dual Horn theories, and the complexity of constraint network equivalence, such an implementation will be too expensive in most cases. The next section proposes two ways to improve the way convergence is handled in Conacq.1.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Improvements for convergence testing
     </section-title>
     <paragraph>
      A natural way to alleviate the computational barrier related to the convergence test is to use the notion of local consistency, which is ubiquitous in Constraint Programming. The idea of local consistency can be summarized as making explicit the inconsistent combinations of values or combinations of constraints among subsets of variables. Such information can be exploited as a background knowledge for improving the learning process and speeding up the convergence test. Taking a central part in Inductive Logic Programming [30], the background knowledge is a set of clauses that impose restrictions on the possible representations of learned concepts. In our constraint acquisition setting, the background knowledge is a set of rules, each encoding in a declarative way a form of local consistency. By combining the clausal representation of a version space with some background knowledge, the convergence problem can be solved using backbone tests, a powerful SAT technique.
     </paragraph>
     <paragraph>
      In this section, we first examine the concept of background knowledge adapted to constraint acquisition, and then we turn to the technique of backbone tests.
     </paragraph>
     <section label="5.1">
      <section-title>
       Background knowledge
      </section-title>
      <paragraph>
       Recall that a Horn clause is definite if it contains exactly one positive literal. Intuitively, a rule is a definite Horn clause that captures, in form of implication, a local consistency property between some constraints defined over the bias.
      </paragraph>
      <paragraph label="Definition 12">
       RuleGiven a vocabulary {a mathematical formula}〈X,D〉, and a bias B, a rule is a definite Horn clause {a mathematical formula}⋀i=1la(ci)→a(c) such that {a mathematical formula}{c1,⋯,cl,c}⊆B. The rule is correct for B if {a mathematical formula}{c1,⋯,cl,c‾} is unsatisfiable.
      </paragraph>
      <paragraph label="Definition 13">
       Background knowledgeGiven a bias B, a background knowledge K for B is a set of correct rules for B.
      </paragraph>
      <paragraph>
       Based on some background knowledge K, any candidate constraint network C can be “saturated” by K, in order to yield an equivalent network {a mathematical formula}C′ including all additional constraints forced by rules whose body is satisfied by C.
      </paragraph>
      <paragraph label="Definition 14">
       SaturationGiven a bias B, a background knowledge K for B, and a constraint network {a mathematical formula}C⊆B, the saturation of C with K, denoted {a mathematical formula}satK(C), is the network {a mathematical formula}C∪{c|a(c)∈unit+(K∪C)}. A network {a mathematical formula}C⊆B is saturated by K if {a mathematical formula}C=satK(C).
      </paragraph>
      <paragraph label="Example 6">
       Consider the vocabulary composed of variables {a mathematical formula}{x1,x2,x3} and domain {a mathematical formula}{1,2,3,4,5}, and the bias {a mathematical formula}B=B≤,≠,≥. The following set of rules is background knowledge for B.{a mathematical formula} Consider the constraint network {a mathematical formula}C={≤12,≤23}. The saturation of C with K is {a mathematical formula}C⁎={≤12,≤23,≤13}. ⋄
      </paragraph>
      <paragraph>
       Informally, the background knowledge K is “complete” if all local consistencies between constraints in the bias can be derived by K.
      </paragraph>
      <paragraph label="Definition 15">
       Subsumed ruleGiven a bias B, background knowledge K for B, and a correct rule R for B such that {a mathematical formula}R∉K, R is subsumed by K if and only if {a mathematical formula}satK∪{R}(C)=satK(C) for every constraint network {a mathematical formula}C⊆B.
      </paragraph>
      <paragraph label="Definition 16">
       Complete background knowledgeGiven a bias B, background knowledge K for B is complete if any correct rule for B is either in K or subsumed by K.
      </paragraph>
      <paragraph>
       In presence of complete background knowledge, the equivalence between constraint networks can be identified by saturation.
      </paragraph>
      <paragraph label="Proof">
       LetBbe a bias, and K be complete background knowledge for K. Then, for any{a mathematical formula}C,C′⊆B, C is equivalent to{a mathematical formula}C′if and only if{a mathematical formula}satK(C)=satK(C′).If {a mathematical formula}satK(C)=satK(C′) then, because K is a set of correct rules for B, it follows that C and {a mathematical formula}C′ are equivalent. Conversely, suppose that {a mathematical formula}C={c1,⋯,cl} is equivalent to {a mathematical formula}C′. For any constraint {a mathematical formula}c′∈C′, because {a mathematical formula}C∪{c′} is entailed by C, the rule R given by {a mathematical formula}a(c1)∧⋯∧a(cl)→a(c′) is correct. And, since K is complete, R is either in K or subsumed by K, which implies that {a mathematical formula}c′∈satK(C). By applying the same strategy to all constraints in the symmetric difference {a mathematical formula}(C′∖C)∪(C∖C′), we get that {a mathematical formula}C′∖C⊆satK(C) and {a mathematical formula}C∖C′⊆satK(C′). This, together with the fact that {a mathematical formula}C⊆satK(C) and {a mathematical formula}C′⊆satK(C′) implies that {a mathematical formula}satK(C)=satK(C∪C′)=satK(C′), as desired.  □
      </paragraph>
      <paragraph>
       Recall that a Boolean formula is uniquely satisfiable if it has a single model. Based on this notion and the above result, a useful property can be derived from complete forms of background knowledge.
      </paragraph>
      <paragraph label="Proof">
       Let K be complete background knowledge for some biasB. Then, for any training set E and its associated clausal representation T,{a mathematical formula}CB(E)has converged if and only if{a mathematical formula}K∧Tis uniquely satisfiable.Suppose that {a mathematical formula}CB(E) has converged to the unique concept {a mathematical formula}f⁎, and let {a mathematical formula}Cf⁎ be the equivalence class of all representations of {a mathematical formula}f⁎ in B. By Theorem 1, we know that {a mathematical formula}ϕ−1(C)⊨T for every {a mathematical formula}C∈Cf⁎. By Lemma 1, we also know that there exists exactly one representation {a mathematical formula}C⁎∈Cf⁎ such that {a mathematical formula}C⁎=satK(C) for all {a mathematical formula}C∈Cf⁎. Since {a mathematical formula}C⁎ is saturated, it follows that {a mathematical formula}ϕ−1(C⁎) satisfies every rule of K, and hence, {a mathematical formula}ϕ−1(C⁎)⊨T∧K. Now, consider any network {a mathematical formula}C∈Cf⁎ such that {a mathematical formula}C≠C⁎. Since C is a proper subset of {a mathematical formula}satK(C), {a mathematical formula}ϕ−1(C) violates every rule R with body {a mathematical formula}{a(c)|c∈C} and head in {a mathematical formula}{a(c′)|c′∈satK(C)∖C}. However, because K is complete, R is subsumed by K, and by contraposition, {a mathematical formula}ϕ−1(C) is not a model of K. Consequently, {a mathematical formula}ϕ−1(C⁎) is the unique model of {a mathematical formula}T∧K.Conversely, suppose that {a mathematical formula}K∧T is uniquely satisfiable, and let {a mathematical formula}I⁎ be the unique model of {a mathematical formula}K∧T, with associated network {a mathematical formula}C⁎=ϕ(I⁎). Since {a mathematical formula}I⁎⊨T, we know that {a mathematical formula}f⁎=fC⁎ is a member of {a mathematical formula}CB(E). Now consider any consistent concept {a mathematical formula}f∈CB(E). For any representation C of f, the interpretation {a mathematical formula}I=ϕ−1(C) must be a model of T. If I is distinct from {a mathematical formula}I⁎, it cannot be a model of {a mathematical formula}T∧K, and hence, C must be a proper subset of {a mathematical formula}C′=satK(C). For this saturated network {a mathematical formula}C′, we know that {a mathematical formula}I′=ϕ−1(C′) is a model of K. Since, in addition, K is a set of correct rules for B, {a mathematical formula}C′ is a representation of f, which implies that {a mathematical formula}I′ is also a model of T. Consequently, {a mathematical formula}I′⊨T∧K, implying that {a mathematical formula}I′=I⁎ by unique satisfiability of {a mathematical formula}T∧K, which in turn implies that {a mathematical formula}f=f⁎.  □
      </paragraph>
      <paragraph>
       Building complete background knowledge is often too expensive, both in time and space as it requires generating a set of rules potentially exponential in space (all combinations of constraints that imply another one). This is not surprising as it is closely related to the concept of relational consistency in constraint networks [16]. However, by analogy with “levels of consistency” used in Constraint Programming, it is possible to compute approximations by bounding the number of constraints in the body of a rule. For instance, in all the experiments we have performed with Conacq, we only generate the rules that contain two constraints in the body because we found that in practice many rules have a small length. Take for instance the rule {a mathematical formula}x≤y∧y≤z→x≤z. It can be detected by brute force generation of the {a mathematical formula}|D|3 assignments on {a mathematical formula}(x,y,z) that satisfy the body of the rule, and testing that the head is always satisfied. Once detected, such a rule must be put in K in the form of a clause {a mathematical formula}a(≤ij)∧a(≤jk)→a(≤ik) for all triplets of such constraints in B. In our example, there are {a mathematical formula}|n3| such rules. In general, given a rule of length l associated with a language Γ containing t relations of maximum arity k, and a vocabulary involving n variables, we can generate up to {a mathematical formula}nkl rules to be put in K.
      </paragraph>
      <paragraph>
       With these notions in hand, we can identify a case where checking convergence in Conacq.1 is polynomial. The method described in Algorithm 2 provides and implementation of the Convergence  procedure in situations where the theory T is reduced to a monomial after unit propagation. Recall that in such situations, the maximally specific and the maximally general concepts are both unique (Proposition 7, Proposition 8). We first take the maximally specific constraint network S formed by all constraints whose associated atom is not negated in T (Line 1). Next, we take the “saturated” maximally general network G formed by all constraints whose associated atom occurs positively in {a mathematical formula}T∧K (Line 2). The convergence is established by simply testing whether S and G are equal, or not (Line 3).
      </paragraph>
      <paragraph label="Proof">
       Restricted convergenceGiven a biasB, background knowledge K forB, and a training set E, if K is complete forBand the clausal representation T of{a mathematical formula}CB(E)is reduced to a monomial by unit propagation, then the convergence problem can be solved in{a mathematical formula}O(|B|+|K|)time.First, we examine the correctness of the restricted convergence procedure. Since T is a monomial, we know that the maximally specific concept {a mathematical formula}fS and the maximally general concept {a mathematical formula}fG of {a mathematical formula}CB(E) are unique (Proposition 7, Proposition 8). So, {a mathematical formula}CB(E) has converged if and only if {a mathematical formula}fS=fG. Let S (resp. G) be a representation of {a mathematical formula}fS (resp. {a mathematical formula}fG) over B. By Lemma 1, a sufficient condition to establish the equality {a mathematical formula}fS=fG is to show that {a mathematical formula}satK(S)=satK(G). From this perspective, consider the networks S and G constructed by Algorithm 2. By Proposition 7, S is a representation of {a mathematical formula}fS. Clearly {a mathematical formula}S=satK(S) because otherwise, there would be a constraint {a mathematical formula}c∈satK(S)∖S such that {a mathematical formula}¬a(c)∈unit−(T), which in turn would imply that {a mathematical formula}fS≠fsatK(S), contradicting the fact that K is a set of correct rules. By Proposition 8, the network {a mathematical formula}G′={c|a(c)∈unit+(T)} is a representation of {a mathematical formula}fG, and by construction, {a mathematical formula}G=satK(G′). Thus, {a mathematical formula}CB(E) has converged if and only if {a mathematical formula}S=G, which is precisely what Algorithm 2 returns.Now, let us turn to the complexity of the procedure. Since T is a monomial, {a mathematical formula}unit−(T) can be computed in {a mathematical formula}O(|T|) time, and since {a mathematical formula}T∧K is a Horn formula, {a mathematical formula}unit+(T∧K) can be computed in {a mathematical formula}O(|T+K|) time. The result follows using the fact that {a mathematical formula}|T| is bounded by {a mathematical formula}|B|.  □
      </paragraph>
      <paragraph>
       We note in passing that the above result can be derived from Proposition 9. Indeed, using the fact that {a mathematical formula}T∧K is a Horn theory when T is a monomial, the unique satisfiability test can be evaluated in {a mathematical formula}O(|T|+|K|) time, using a directed hypergraph representation of the Horn formula [13]. However, in our setting, the restricted convergence procedure is much simpler to implement, requiring only unit propagation for computing the sets S and G.
      </paragraph>
      <paragraph label="Example 7">
       Consider the theory T generated in Example 4. After unit propagation, T contains two positive literals {a mathematical formula}a(≤12) and {a mathematical formula}a(≤23). The maximally specific network S is precisely {a mathematical formula}{≤12,≤23,≤13} and the maximally general network G is {a mathematical formula}{≤12,≤23}. Using the background knowledge K presented in Example 6, we derive that S is equal to the saturation of G with K. Hence, we infer that the version space has converged. ⋄
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Backbone detection
      </section-title>
      <paragraph>
       In general, the theory T returned by Conacq.1 cannot be reduced to a simple monomial by unit propagation. Even if the theory includes conjunctions of disjunctions, the version space may have converged because each maximally general concept is equivalent to the maximally specific concept. This general case is illustrated in the following example.
      </paragraph>
      <paragraph label="Example 8">
       Consider the variables {a mathematical formula}{x1,x2,x3}, the domain {a mathematical formula}{1,2,3,4,5}, and the bias {a mathematical formula}B=B≤,≠,≥. Suppose that the target network is {a mathematical formula}C={=12,=13,=23}. To acquire this concept, the learner is given the set E of 7 examples illustrated in the following table.{a mathematical formula}This training set is sufficient to infer that the version space has converged. However, all positive clauses in T contain two positive literals. It follows that, even when using the complete background knowledge K given in Example 6, unit propagation on {a mathematical formula}T∧K is not sufficient to detect convergence. ⋄
      </paragraph>
      <paragraph>
       In the above example, unit propagation on {a mathematical formula}T∧K is not sufficient to infer that any concept in the version space is equivalent to the target constraint network {a mathematical formula}{=12,=13,=23}. The powerful notion of backbone of a propositional formula can be used here. A literal belongs to the backbone of a formula if it belongs to all models of the formula [28]. In the setting of our framework, we say that an atom {a mathematical formula}a(c) is in the backbone of a theory T with respect to background knowledge K if {a mathematical formula}a(c) is entailed by {a mathematical formula}T∧K, or equivalently, if {a mathematical formula}T∧K∧¬a(c) is unsatisfiable. Once the literals in the backbone are detected, they can be exploited to prove convergence.
      </paragraph>
      <paragraph>
       The general version of convergence testing is described in Algorithm 3. As in Algorithm 2, we start from the maximally specific concept S of the version space (Line 1). But this time we cannot generate a unique maximally general concept. So, we use the theory T and the background knowledge K and we determine whether each constraint in S lies in the backbone of {a mathematical formula}T∧K (Lines 2–3). If this is indeed the case, we have converged (Line 4).
      </paragraph>
      <paragraph label="Proof">
       General convergenceGiven a biasB, background knowledge K forB, and a training set E, if K is complete forB, then the convergence problem can be solved using{a mathematical formula}O(|B|)backbone tests.Suppose that K is complete, and let T be the clausal theory encoding {a mathematical formula}CB(E). By Proposition 9, we know that {a mathematical formula}CB(E) has converged if and only if {a mathematical formula}T∧K is uniquely satisfiable. If T is satisfiable, then testing whether {a mathematical formula}T∧K is uniquely satisfiable can be done by checking whether for each constraint {a mathematical formula}c∈B, the positive literal {a mathematical formula}a(c) or negative literal {a mathematical formula}¬a(c) over c is entailed by {a mathematical formula}T∧K. However, since we already know that all negative literals in {a mathematical formula}unit−(T) are entailed by {a mathematical formula}T∧K, we only need to check the constraints {a mathematical formula}c∈B for which {a mathematical formula}¬a(c)∉unit−(T). Furthermore, we also know that for the constraint network {a mathematical formula}S={c∈B|¬a(c)∉unit−(T)}, the corresponding interpretation {a mathematical formula}ϕ−1(S) is a model of {a mathematical formula}T∧K, because S is an encoding of the maximally specific concept {a mathematical formula}fS∈CB(E) (as stated in Proposition 7), and because S is saturated (as already shown in the proof of Theorem 6). Therefore, for each {a mathematical formula}c∈B such that {a mathematical formula}a(c)∉unit−(T), we know that {a mathematical formula}T∧K∧a(c) is satisfiable. So, for each of these constraints c, we only need to check the satisfiability of {a mathematical formula}T∧K∧¬a(c), which is precisely what the general convergence procedure performs, using at most {a mathematical formula}|B|−|unit−(T)| backbone tests.  □
      </paragraph>
      <paragraph>
       From a computational point of view, we must keep in mind that the theory T, coupled with the background knowledge K is neither a Horn (T is dual Horn) nor a dual Horn (K is Horn) formula. So, testing whether a literal is in the backbone of T with respect to K is coNP-complete. However, as shown in our experiments, backbone detection can be very fast under this representation scheme because both theories T and K efficiently propagate unit clauses.
      </paragraph>
      <paragraph label="Example 9">
       Consider again the scenario of Example 8. After processing the examples {a mathematical formula}e1, {a mathematical formula}e2 and {a mathematical formula}e4, we remark that {a mathematical formula}a(≥13) is in the backbone of {a mathematical formula}T∧K because K includes the rule {a mathematical formula}a(≥12)∧a(≥23)→a(≥13). Analogously, after processing the examples {a mathematical formula}e3 and {a mathematical formula}e5, {a mathematical formula}a(≤13) is in the backbone of T, because K includes the rule {a mathematical formula}a(≤12)∧a(≤23)→a(≤13). It follows that {a mathematical formula}=13 is in the target concept. After processing the example {a mathematical formula}e6, {a mathematical formula}a(≤23) is in the backbone of {a mathematical formula}T∧K because K includes the rules {a mathematical formula}a(≥12)∧a(=13)→a(≥32) and {a mathematical formula}a(≥32)→a(≤23). We similarly deduce that {a mathematical formula}a(≥12) is in the backbone. Dually, after processing the example {a mathematical formula}e7, {a mathematical formula}a(≤12) and {a mathematical formula}a(≥23) are in the backbone of {a mathematical formula}T∧K. We have converged on the concept {a mathematical formula}{=12,=13,=23}. ⋄
      </paragraph>
     </section>
    </section>
    <section label="6">
     The active Conacq.2 algorithm
     <paragraph>
      As specified in Section 3, constraint acquisition is the problem of identifying a representation of some target concept using a constraint language and user-supplied information taking the form of examples. In the passive acquisition setting examined in Section 4, the information given by the user is a training set E over which the learner has no control. By contrast, in the active acquisition setting, the learner is allowed to ask membership queries, that is, to select an assignment x and to ask the user what is the label of x. The user answers yes if x is a solution of the target problem, and no otherwise. For many concept classes, the use of membership queries in conjunction with equivalence queries is known to dramatically accelerate the learning process [11], [12]. Though equivalence queries are not considered in constraint acquisition, and membership queries alone are not powerful enough to guarantee convergence in a polynomial number of queries (Theorem 4), the use of membership queries in conjunction with a given training set can substantially improve the acquisition process.
     </paragraph>
     <paragraph>
      In this section, we will assume that the constraint acquisition problem is realizable, and membership queries are answered correctly. Namely, the target concept {a mathematical formula}f⁎ is representable by a satisfiable constraint network over B, and the answer y to any query {a mathematical formula}q=x is consistent with {a mathematical formula}f⁎, i.e. {a mathematical formula}y=f⁎(x). The more general, yet challenging, “agnostic” constraint acquisition setting, with possible omissions and errors in answers to membership queries, is deferred to future research.
     </paragraph>
     <paragraph>
      Intuitively, a membership query {a mathematical formula}q=x is “informative” or “irredundant” if, whatever being the user's answer y, the resulting example {a mathematical formula}〈x,y〉 added to the current training set E will ensure to reduce the learner's version space. The task of finding such queries is, however, far from easy. In this section, we begin by a formal characterization of the notion of informative membership query, and next, we show that the problem of finding such a query is NP-hard. We then present the Conacq.2 algorithm, an active version of Conacq, which relies on different strategies for efficiently generating informative queries.
     </paragraph>
     <section label="6.1">
      <section-title>
       Informative queries
      </section-title>
      <paragraph>
       Let B be a constraint bias, and E a set of examples. Formally, a membership query {a mathematical formula}q=x is informative (or irredundant) with respect to {a mathematical formula}CB(E) if and only if x is not predictable by {a mathematical formula}CB(E). In other words, q is informative if and only if x is not classified in the same way by all concepts in the version space.
      </paragraph>
      <paragraph label="Example 10">
       Redundant queryConsider the vocabulary defined over the variables {a mathematical formula}{x1,x2,x3}, the domain {a mathematical formula}{1,2,3,4}, and the bias {a mathematical formula}B=B≤,≠,≥. Given the positive example {a mathematical formula}e+=〈1,2,3〉, the associated clausal theory T will contain {a mathematical formula}¬a(≥12), {a mathematical formula}¬a(≥13), and {a mathematical formula}¬a(≥23). Asking the user to classify {a mathematical formula}x=〈1,2,4〉 is redundant because all constraints rejecting it are already forbidden by T. In other words, any concept in the version space classifies x as positive. ⋄
      </paragraph>
      <paragraph>
       The next property is a variant of Proposition 6 which will be useful in active constraint acquisition. Given an assignment x over {a mathematical formula}D|X|, we denote by {a mathematical formula}κ[T](x) the subset obtained by removing from {a mathematical formula}κ(x) all constraints that appear as negated literals in T, that is, {a mathematical formula}κ[T](x)=κ(x)∖{ci|¬a(ci)∈unit−(T)}. With a slight abuse of terminology, we say that {a mathematical formula}κ[T](x) is subsumed by a clause {a mathematical formula}α∈T if {a mathematical formula}constraints(α)⊆κ[T](x).
      </paragraph>
      <paragraph label="Proof">
       Given a constraint biasB, and a training set E, any membership query{a mathematical formula}q=xis informative if and only if{a mathematical formula}κ[T](x)is neither empty nor subsumed by any clause in T, where T is the reduced clausal theory of{a mathematical formula}CB(E).By Proposition 6, x is not predictable by {a mathematical formula}CB(E) if and only if both {a mathematical formula}T0 and {a mathematical formula}T1 are satisfiable, where {a mathematical formula}T0 and {a mathematical formula}T1 are the clausal encodings of {a mathematical formula}CB(E∪{〈x,0〉}) and {a mathematical formula}CB(E∪{〈x,1〉}), respectively. Let α be the positive clause {a mathematical formula}⋁{a(c)|c∈κ(x)}. Then, {a mathematical formula}T0 is satisfiable if and only if {a mathematical formula}T∧α is satisfiable, which is equivalent to state that {a mathematical formula}κ[T](x) is nonempty. Moreover, {a mathematical formula}T1 is satisfiable if and only if T does not entail α, which is equivalent to state that {a mathematical formula}κ[T](x) does not cover any clause in the reduced theory T.  □
      </paragraph>
      <paragraph label="Example 11">
       Irredundant queryConsider again Example 10 in which example {a mathematical formula}e+ has been processed, yielding {a mathematical formula}T={¬a(≥12),¬a(≥13),¬a(≥23)}. The query {a mathematical formula}q=〈1,2,2〉 is irredundant because {a mathematical formula}κ[T](q) is neither empty nor a superset of any clause in T, and {a mathematical formula}κ[T](q)=κ(q)∖{≥12,≥13,≥23}={≠23}. On the one hand, if the query q is classified as positive, the clauses {a mathematical formula}¬a(≥12), {a mathematical formula}¬a(≥13) and {a mathematical formula}¬a(≠23) are added to T because {a mathematical formula}κ(q)={≥12,≥13,≠23}, which yields the theory {a mathematical formula}T∧¬a(≠23). On the other hand, if the query q is classified as negative, the clause {a mathematical formula}a(≥12)∨a(≥13)∨a(≠23) is added to T, which by unit reduction yields the theory {a mathematical formula}T∧a(≠23). Regardless of the classification of q, something new was learned. ⋄
      </paragraph>
      <paragraph>
       Though irredundant queries ensure that a nonempty portion of the version space is eliminated, some queries are “more” informative than others, by reducing larger portions of the hypothesis space. Technically, our Conacq architecture does not have access to the size of version spaces, but instead to a logical representation of them, for which the number of models is an approximation of the number of consistent concepts. From this viewpoint, the quality of a membership query is assessed here by the number of models it eliminates from the clausal representation of the version space. By {a mathematical formula}‖T‖, we denote the number of distinct interpretations which satisfy a theory T, i.e. {a mathematical formula}‖T‖=|models(T)|.
      </paragraph>
      <paragraph label="Definition 17">
       Let B be a constraint bias, E a set of examples, and T be the clausal encoding of {a mathematical formula}CB(E). Given a real θ in {a mathematical formula}[0,1], a membership query {a mathematical formula}q=x is called weakly (resp. strongly) θ-informative with respect to {a mathematical formula}CB(E) if{a mathematical formula} for some (resp. all) {a mathematical formula}y∈{0,1}, where {a mathematical formula}Ty is the encoding of {a mathematical formula}CB(E∪{〈x,y〉}).
      </paragraph>
      <paragraph>
       In other words, a membership query {a mathematical formula}q=x is weakly θ-informative if, for at least one of the possible answers {a mathematical formula}y∈{0,1}, the resulting theory {a mathematical formula}Ty obtained by updating the version space with {a mathematical formula}〈x,y〉 eliminates at least {a mathematical formula}θ‖T‖ models from the original encoding T of {a mathematical formula}CB(E). By contrast, {a mathematical formula}q=x is strongly θ-informative if, whatever being the user's answer y, {a mathematical formula}‖T‖ will be reduced by a factor of at least θ. Since {a mathematical formula}‖Ty‖ must be positive, any (weakly or strongly) θ-informative query with {a mathematical formula}θ∈[0,1] is irredundant.
      </paragraph>
      <paragraph label="Proof">
       LetBbe a constraint bias, E be a set of examples, and T be the clausal encoding of{a mathematical formula}CB(E). Then, for any membership query{a mathematical formula}q=xsuch that{a mathematical formula}κ[T](x)does not cover any clause in T, if{a mathematical formula}|κ[T](x)|=tthen q is weakly{a mathematical formula}(1−12t)-informative, and if{a mathematical formula}|κ[T](x)|=1then q is strongly{a mathematical formula}12-informative.Let y be the answer to the query {a mathematical formula}q=x. If {a mathematical formula}y=1 then, by construction, {a mathematical formula}T1=T∧{¬a(c):c∈κ[T](x)}. Since {a mathematical formula}κ[T](x) and the set of constraints occurring in T are disjoint, it follows that {a mathematical formula}‖T1‖=12t‖T‖, and hence, q is weakly {a mathematical formula}(1−12t)-informative. Furthermore, if {a mathematical formula}t=1 then let c be the unique constraint occurring in {a mathematical formula}κ[T](x). Because {a mathematical formula}T0=T∧a(c), {a mathematical formula}T1=T∧¬a(c), and c does not occur in T, it follows that {a mathematical formula}‖T0‖=‖T1‖=12‖T‖, and hence, q is strongly {a mathematical formula}12-informative.  □
      </paragraph>
      <paragraph>
       In light of the above result, we shall examine two strategies for generating membership queries, namely, the optimistic strategy, and the optimal-in-expectation strategy.
      </paragraph>
      <section>
       <section>
        <section-title>
         Optimistic strategy
        </section-title>
        <paragraph>
         Intuitively, an optimistic query captures a large information gain when it is classified “in our favor”, but conveys very little information when it is classified otherwise. Given {a mathematical formula}x∈D|X|, the larger {a mathematical formula}|κ[T](x)|, the more optimistic the query {a mathematical formula}q=x. By denoting {a mathematical formula}t=|κ[T](x)|, we know by Proposition 11 that q is weakly {a mathematical formula}(1−12t)-informative. Specifically, if q is classified as positive, the resulting example prunes a large portion of {a mathematical formula}models(T) by assigning {a mathematical formula}|κ[T](x)| literals in T to 0; if q is classified as negative, it will only prune a tiny portion of {a mathematical formula}models(T) by just expanding T with the clause {a mathematical formula}⋁{a(c)|c∈κ[T](x)}. The next example illustrates the acquisition process with an optimistic strategy.
        </paragraph>
        <paragraph label="Example 12">
         Optimistic queriesSuppose we wish to acquire the constraint network specified in Example 5, namely, the target network involves four variables, {a mathematical formula}{x1,…,x4} defined over the domain {a mathematical formula}D={1,2,3,4}, a single constraint {a mathematical formula}x1≠x4. Using the context defined in Example 5, the bias is {a mathematical formula}B=B≤,≠,≥, and the training set is given by the three examples {a mathematical formula}e1+=〈1,2,3,4〉,e2+=〈4,3,2,1〉 and {a mathematical formula}e3−=〈1,1,1,1〉. The unique positive clause in T is {a mathematical formula}a(≠12)∨a(≠13)∨a(≠14)∨a(≠23)∨a(≠24)∨a(≠34). All other atoms in T are fixed to 0 because of {a mathematical formula}e1+ and {a mathematical formula}e2+. Here, {a mathematical formula}q4=〈1,1,1,3〉 and {a mathematical formula}q5=〈1,1,3,1〉 are two optimistic queries for T. {a mathematical formula}κ[T](q4)={≠12,≠13,≠23} and {a mathematical formula}κ[T](q5)={≠12,≠14,≠24} are both of size 3. According to the target network, {a mathematical formula}q4 will be classified positive by the user. Clauses {a mathematical formula}¬a(≠12), {a mathematical formula}¬a(≠13) and {a mathematical formula}¬a(≠23) are then added to T and its number of models is divided by 2{sup:3}. Consider {a mathematical formula}q5 instead of {a mathematical formula}q4: according to the target network, it is classified negative by the user. The clause {a mathematical formula}a(≠12)∨a(≠14)∨a(≠24) is added to T and its number of models only decreases of {a mathematical formula}1/23. This example reveals how unbalanced optimistic queries can be. ⋄
        </paragraph>
       </section>
       <section>
        <section-title>
         Optimal-in-expectation strategy
        </section-title>
        <paragraph>
         As observed in [12], a membership query is optimal if it reduces the size of the version space in half regardless of how the user classifies it. The aforementioned optimistic queries are clearly not optimal in this sense. A query {a mathematical formula}q=x is called optimal-in-expectation if {a mathematical formula}κ[T](x)=1. Since we are guaranteed that one literal will be fixed in T, whatever being the choice of y, q is strongly {a mathematical formula}12-informative. The next example illustrates a sequence of optimal-in-expectation queries that are sufficient for establishing the version space convergence.
        </paragraph>
        <paragraph label="Example 13">
         Optimal-in-expectation queriesConsider again the target network {a mathematical formula}x1≠x4, using a vocabulary involving four variables {a mathematical formula}x1,…,x4, the domain {a mathematical formula}D={1,2,3,4}, and the bias {a mathematical formula}B=B≤,≠,≥. Using the training set {a mathematical formula}E={e1+,e2+,e3−}, the unique positive clause in T is {a mathematical formula}α=a(≠12)∨a(≠13)∨a(≠14)∨a(≠23)∨a(≠24)∨a(≠34). All other atoms in T are set to 0. Using {a mathematical formula}T12 as an abbreviation of {a mathematical formula}¬a(≤12)∧…∧¬a(≤34)∧¬a(≥12)∧…¬a(≥34), we can write {a mathematical formula}T=T12∧α. Table 1 captures a sequence of queries which are optimal-in-expectation for {a mathematical formula}CB(E). The first column specifies q, the second column indicates the user response y, the third column defines {a mathematical formula}κ[T](q), and the final column captures the update of T. For the query {a mathematical formula}q4, we know that {a mathematical formula}a(≠12) will be fixed to a unit clause in the resulting theory, whatever being the user response. By repeating this invariant to all other queries, we are ensured that the version space will converge after processing {a mathematical formula}q8. ⋄
        </paragraph>
        <paragraph>
         In Example 13, we found a sequence of optimal-in-expectation queries for establishing convergence. However, it is not always possible to generate such queries, due to the interdependency between constraints in the bias. Example 14 illustrates the impossibility to generate an optimal-in-expectation query.
        </paragraph>
        <paragraph label="Example 14">
         No possible optimal-in-expectation queryConsider the acquisition problem, using {a mathematical formula}B=B≤,≠,≥, and with {a mathematical formula}x1=x2=x3 as a target network. After processing an initial positive example (for instance {a mathematical formula}e1+=〈2,2,2〉), the possible constraints in the version space are {a mathematical formula}≤12,≤13,≤23,≥12,≥13,≥23. Here, every membership query q has either a {a mathematical formula}κ[T](q) of size 3 (if no variables equal), or a {a mathematical formula}κ[T](q) of size 2 (if two variables equal), or a {a mathematical formula}κ[T](q) of size 0 (if all three variables equal). Therefore, no query with a {a mathematical formula}κ[T](q) of size 1 can be generated. Interdependency between constraints prevents us from generating such examples. ⋄
        </paragraph>
        <paragraph>
         Detecting whether a query strategy is feasible, or not, is intrinsically related to the “query generation” problem, examined below.
        </paragraph>
       </section>
      </section>
     </section>
     <section label="6.2">
      <section-title>
       Query generation problem
      </section-title>
      <paragraph>
       The membership queries q of interest in this study only differ by the number {a mathematical formula}t=|κ[T](q)| of constraints which are not rejected by the clausal theory T. For optimal-in-expectation queries, we search for an assignment such that {a mathematical formula}t=1, whereas for optimistic queries, we search for an assignment with a larger t. When T already contains a non-unary clause α, we may want to shrink α, thus searching for an assignment such that {a mathematical formula}κ[T](q)⊂constraints(α). The query generation problem is formulated as follows.
      </paragraph>
      <paragraph label="Definition 18">
       Query generation problemGiven a bias B, a training set E, an integer t, (and optionally a non-unary positive clause α in T), the query generation problem is to find a query q such that {a mathematical formula}κ[T](q) does not cover any clause in T (resp. {a mathematical formula}κ[T](q)⊂constraints(α)) and {a mathematical formula}|κ[T](q)|=t. Deciding whether such a query q exists is called the query existence problem.
      </paragraph>
      <paragraph label="Proposition 12">
       Intractability of query generationThe query existence problem is NP-complete for any value of t, and thus the generation problem is NP-hard for any value of t.
      </paragraph>
      <paragraph label="Proof">
       Membership. Given a bias B, a theory T, (optionally a non-unary positive clause α in T), and an expected size t, checking that a given query q is a solution to the query generation problem is polynomial. Building {a mathematical formula}κ(q) is linear in {a mathematical formula}|B|. From {a mathematical formula}κ(q), building {a mathematical formula}κ[T](q), checking that {a mathematical formula}κ[T](q) does not cover any clause in T (resp. checking that {a mathematical formula}κ[T](q)⊂constraints(α)), and checking that {a mathematical formula}κ[T](q) is of size t are all linear in {a mathematical formula}|κ(q)|⋅|T|.Completeness. We reduce the problem of coloring a graph with three colors (3Col) to the problem of the existence of a query q with {a mathematical formula}κ[T](q)=t. Let {a mathematical formula}(N,E) be a graph, where N is a set of n vertices and E a set of edges. {a mathematical formula}vi refers to the ith vertex and {a mathematical formula}eij to the edge between vertices {a mathematical formula}vi and {a mathematical formula}vj. We transform the instance of graph coloring problem into the following query generation problem. The vocabulary is composed of a set of variables {a mathematical formula}X={xi|vi∈N}∪{xn+1,…,xn+t+1}, and a domain D such that {a mathematical formula}D(xi)={1,2,3},∀i∈[1..n], and {a mathematical formula}D(xi)={1},∀i∈[n+1..n+t+1]. The constraint bias B is the set {a mathematical formula}{≠ij|eij∈E}∪{≠i,i+1|i∈[n+1..n+t]}. (The clausal theory T could optionally contain the clause {a mathematical formula}α=⋁≠ij∈Ba(≠ij). This could have happened after receiving the negative example {a mathematical formula}〈1,1,…,1,1〉.) Suppose we want to generate a query q with t (and optionally α) as input. Such a query will necessarily be an assignment on X that violates {a mathematical formula}≠i,i+1 for all {a mathematical formula}i∈{n+1,⋯,n+t} because {a mathematical formula}D(xi)=D(xi+1)={1}. Hence, the query will be any assignment that satisfies all other constraints in B because we want the query to violate exactly t constraints. Now, by construction, an assignment on {a mathematical formula}X∖{xn+1,…,xn+t+1} that satisfies all constraints in B corresponds to a 3-coloring of {a mathematical formula}(N,E). Thus, the query existence problem has a solution if and only if the graph is 3-colorable. Our transformation is polynomial in the size of {a mathematical formula}(N,E). Therefore, the query existence problem is NP-complete and the query generation problem is NP-hard.  □
      </paragraph>
     </section>
     <section label="6.3">
      Description of Conacq.2
      <paragraph>
       Based on a computational analysis of query generation, we are now ready to examine an active version of Conacq.
      </paragraph>
      <paragraph>
       As presented in Algorithm 4, the Conacq.2 algorithm takes as input a constraint bias B, background knowledge K for B, and a query generation strategy used by function QueryGeneration (Algorithm 5) for the generation of queries. Each time we discover an interdependency among constraints that is not captured by the background knowledge K, we encode the interdependency as a logical nogood that is stored in a set N to avoid repeatedly discovering it. The algorithm returns a clausal theory T that encodes a constraint network representing the target concept.
      </paragraph>
      <paragraph>
       Conacq.2 starts from an empty theory T (Line 1) and iteratively expands it by an example {a mathematical formula}〈x,y〉 formed by the query {a mathematical formula}q=x generated in Line 3, and the user response y supplied in Line 6. The query generation process is implemented by the function QueryGeneration, which takes as input the bias B, the current clausal theory T, the given background knowledge K, the current set of nogoods N, and the given strategy Strategy. If there exist irredundant queries, QueryGeneration returns an irredundant query q following Strategy as much as possible, that is, with {a mathematical formula}|κ[T](q)| as close as possible to the specified t. If there is no irredundant query, this means that we have reached convergence and we return the theory encoding the target network (Lines 4 and 8). Otherwise, the query q is supplied to the user, who answers by yes (1) or no (0). If q is classified as negative by the user, we must discard from the version space all concepts that accept q. This is done by expanding the theory with the clause consisting of all literals {a mathematical formula}a(c) with c in {a mathematical formula}κ(q) (Line 6). Dually, if q is classified as positive, we must discard from the version space all concepts that reject q. This is done by expanding the theory with a unit clause {a mathematical formula}¬a(c) for each constraint c in {a mathematical formula}κ(q) (Line 7).
      </paragraph>
     </section>
     <section label="6.4">
      <section-title>
       Implementing our strategies
      </section-title>
      <paragraph>
       The technique we propose to generate a query q is based on the following basic idea: define a constraint network expressing the strategy, that is, whose solutions are queries following the strategy. Based on Section 6.1, optimistic strategies and optimal-in-expectation strategies are both characterized by the number {a mathematical formula}t=|κ[T](q)| of constraints which are not inconsistent with (any concept of) the version space. To generate a query q that violates a number t of such constraints, we build a constraint network that forces t of those constraints to be violated. To be able to build this constraint network, we assume that for any constraint {a mathematical formula}c∈B, the complement {a mathematical formula}c‾ of c is available for building the constraint network. Thanks to the bounded arity assumption, the relation associated with {a mathematical formula}c‾ does not require an exponential space. As seen in Example 14 and Proposition 12, it may be the case that, due to interdependency between constraints, there does not exist any network in the version space that has a solution s with {a mathematical formula}|κ[T](s)|=t. We then must allow for some flexibility ϵ in the number of constraints rejecting a query.
      </paragraph>
      <paragraph>
       We implement the query generation problem in function QueryGeneration (Algorithm 5). The goal in function QueryGeneration is to find a good query following the given strategy. The algorithm starts by initializing the query q to nil and the clause α to the empty set (Line 1). Then, it enters the main loop in Line 2 if T is not a monomial. (Otherwise it immediately returns any irredundant query generated in Line 19.) The loop starts by testing whether a clause α is currently processed (i.e., α non-empty) or not. If α is empty and there still exist non-unary clauses in T that have not yet been marked, Line 4 reads such a (necessarily positive) clause in T. (The meaning of marked clauses is explained later.) We should bear in mind that a positive clause in T represents the set of constraints that reject a negative example already processed. So, we are sure that at least one of the constraints in {a mathematical formula}constraints(α) must belong to the target network. Line 5 initializes the flexibility parameter ϵ to 0. ϵ measures how much we accept to deviate from the strategy. That is, a generated query q has to be such that {a mathematical formula}|t−κ[T](q)|≤ϵ. The expected number t of constraints in the {a mathematical formula}κ[T](q) of the query q we will generate is set to the value corresponding to the strategy we use. For instance, in the optimal-in-expectation strategy, t is always set to 1.
      </paragraph>
      <paragraph>
       Once ϵ, α, and t are set, if α is non-empty the purpose of the loop is to find a query q that will allow us to reduce the size of α. In Line 6 the Boolean splittable is computed. splittable will tell us if ϵ is small enough to produce a query that will reduce the size of α. In Line 7, the function BuildFormula is called (see Algorithm 6). If splittable is true, this function returns a pseudo-Boolean formula F such that any solution s of any network corresponding to a model of F has a {a mathematical formula}κ[T](s) of size {a mathematical formula}t±ϵ subsuming α. If α is non-splittable, constraints outside α can belong to {a mathematical formula}κ[T](s). If α is empty, some constraints not yet decided as member or non-member of the target network must belong to {a mathematical formula}κ[T](s). The formula F is defined on the atoms of T plus all atoms {a mathematical formula}a(c‾) such that {a mathematical formula}c∈B and {a mathematical formula}c‾∉B. Once the formula F is generated, it is solved in Line 8. If {a mathematical formula}F∧K∧N is unsatisfiable, and α is still splittable, we must increase the flexibility ϵ (Line 9). If {a mathematical formula}F∧K∧N is unsatisfiable and α is non-splittable, this means that our current background knowledge K and nogoods N are sufficient to prove that α cannot be split because of the interdependency among its own constraints and independently of constraints outside α. Then, all literals corresponding to constraints of α are put in T, α is removed from T and is reset (Line 10) so that at the next iteration, a new clause will be selected.
      </paragraph>
      <paragraph>
       In Line 12 a model I of {a mathematical formula}F∧K∧N is selected. If the constraint network {a mathematical formula}φ(I) has no solution (Line 13), this means that an interdependency between constraints of {a mathematical formula}φ(I) was not caught by K and the current stored nogoods N. Lines 14–15 extract some conflicting sets of constraints (not necessarily all){sup:2} from {a mathematical formula}φ(I) and add the corresponding nogood clauses to N to avoid repeatedly generating models {a mathematical formula}I′ with this hidden inconsistency in {a mathematical formula}φ(I′). Finally, if {a mathematical formula}φ(I) has solutions, such a solution q is selected in Line 17 and returned as the query to be asked to the user (Line 20). If α was non-splittable (and non-empty), Line 18 marks it as non-splittable to avoid selecting it again in a later call to QueryGeneration.
      </paragraph>
      <paragraph>
       We now present function BuildFormula (Algorithm 6). As already said above, BuildFormula takes as input a Boolean splittable, a theory T, a clause α, an expected size t and an allowed flexibility ϵ. If α is not empty, the formula F is initialized to T minus the literals discarding the negation {a mathematical formula}c‾ of constraints c occurring in {a mathematical formula}constraints(α) (Line 2). The idea is to build a formula F that only allows for a subset of the constraints in α to be violated. To monitor this number, we must be able to force a constraint or the negation of a constraint in {a mathematical formula}constraints(α). This is why we remove the literals {a mathematical formula}¬a(c‾) for constraints c in {a mathematical formula}constraints(α). For each literal {a mathematical formula}a(c) not already negated in T (Line 3), if we are in the splittable mode and if c and {a mathematical formula}c‾ are both outside {a mathematical formula}constraints(α), we force {a mathematical formula}a(c) to be true (Line 5). Hence, we force the constraint c to belong to the network {a mathematical formula}φ(I) for any model I of F, so that any solution s of {a mathematical formula}φ(I) will be rejected only by constraints in {a mathematical formula}constraints(α) or by constraints already negated in T (so no longer in the version space). Thus, {a mathematical formula}κ[T](s)⊆constraints(α). This will ensure that the query generated cannot extend α. If non-splittable, we do not force constraints outside {a mathematical formula}constraints(α) to be satisfied.
      </paragraph>
      <paragraph>
       We now have to force the size of {a mathematical formula}κ[T](s) to be in the right interval. In the splittable mode, if {a mathematical formula}a(c) belongs to α we add the clause {a mathematical formula}(a(c)∨a(c‾)) to F to ensure that either c or its complementary constraint {a mathematical formula}c‾ is in the resulting network (Line 6). {a mathematical formula}c‾ is required because {a mathematical formula}¬a(c) only expresses the absence of the constraint c. {a mathematical formula}¬a(c) is not sufficient to force c to be violated. We now just add two pseudo-Boolean constraints that ensure that the number of constraints from {a mathematical formula}constraints(α) violated by solutions of {a mathematical formula}φ(I) will be in the interval {a mathematical formula}[t−ϵ..t+ϵ]. This is done by forcing at least {a mathematical formula}|α|−t−ϵ and at most {a mathematical formula}|α|−t+ϵ literals from α to be set to true (lines 8–9 and Line 11). The ‘min’ and ‘max’ ensure we avoid trivial cases (i.e., no literal or all literals from α set to true). In the non-splittable mode, we just set the lower and upper bounds to the smallest and greatest non-trivial values (Line 10).
      </paragraph>
      <paragraph>
       If α is empty, this means that we just want to generate a formula whose models will represent networks whose solutions are irredundant queries. The formula F is thus built in such a way that it satisfies T and violates at least one of the constraints not yet decided as member or not of the target network. For such constraints c we do not put {a mathematical formula}¬a(c‾) in F to be able to force violation of one such constraint c. Line 13 returns F.
      </paragraph>
      <paragraph>
       We finally present function IrredundantQuery (Algorithm 7). Function IrredundantQuery returns an irredundant query if one exists. This means that IrredundantQuery answers the convergence problem when it returns nil. As we want our technique to work whatever the background knowledge K is complete or not, we cannot apply Theorem 7 and thus cannot work at the level of the logical theory T to prove convergence. Fortunately, IrredundantQuery is called only when T is a monomial, which makes the test much easier. Line 1 builds the network {a mathematical formula}CL of constraints that have been learned as members of the target network. In lines 2 to 4, the function iteratively tries to randomly generate an irredundant query during a short amount of time that is sufficient in most cases to find one. If the time limit is reached, we enter a systematic way of generating such an irredundant query. Line 5 stores in the set RuledOut all constraints that have been ruled out of the version space. Then, for each constraint c that is neither ruled out nor set in {a mathematical formula}CL, Line 7 tries to find an assignment that satisfies {a mathematical formula}CL whereas violating c. If such an assignment s exists, it is returned as an irredundant query (Line 8). If no such assignment exists, this means that c is implied by {a mathematical formula}CL and {a mathematical formula}a(c) can be added to T without changing the version space (Line 9). If there does not exist any constraint that can be violated whereas satisfying {a mathematical formula}CL this means that T has converged. The function returns nil (Line 10).
      </paragraph>
      <paragraph label="Proof">
       QueryGenerationterminates and returns an irredundant query if there is one, nil otherwise.Termination. Termination is based on the fact that the loop of Line 2 cannot run forever. The loop starts by selecting an unmarked clause α if it exists (Line 4). Once a given α selected (or α still being the empty set), the loop necessarily executes one of the lines 9, 10, 15, or 17. We show that none of these instructions can be repeated forever for a given α.Let us first suppose {a mathematical formula}α≠∅. As α is of finite size and ϵ is bounded above by {a mathematical formula}|α| (Line 6), Line 9 cannot be executed an infinite number of times. Line 10 resets α to ∅ so it can be executed only once. In Line 15, a nogood is added to the set N of nogoods. This nogood is necessarily new, otherwise I would not have been a model of {a mathematical formula}F∧K∧N. As nogoods are defined on a finite number of atoms (atoms {a mathematical formula}a(c) and {a mathematical formula}a(c‾) for each c in B), we cannot add an infinite number of nogoods without making N unsatisfiable, and thus no longer entering Line 15. Line 17 is executed only once as it breaks the condition of the loop.Let us now consider the case where {a mathematical formula}α=∅. We show that lines 9 and 10 cannot be executed. If {a mathematical formula}α=∅, function BuildFormula has executed its Line 12. When entering BuildFormula, T was not a monomial. Hence, there exists at least one atom {a mathematical formula}a(c) which is unset (that is, {a mathematical formula}T∧K∧N has two models I and {a mathematical formula}I′ with {a mathematical formula}I[a(c)]=0 and {a mathematical formula}I′[a(c)]=1).{sup:3} Thus, the only clause in F which is not in T (added in line 12) is non-empty. Furthermore, the atoms of this clause are unset in F by construction. As a result, the extra clause added to F in Line 12 cannot lead to inconsistency of {a mathematical formula}F∧K∧N. We are thus guaranteed that {a mathematical formula}F∧K∧N is satisfiable and QueryGeneration goes directly to Line 12. Lines 15 and 17 cannot be executed an infinite number of times for the same reason as the case {a mathematical formula}α≠∅.Finally, we have to prove that a given α cannot be selected several times in Line 4. Once a given α selected, the only way to stop processing it is by executing Line 10 or Line 17. If Line 10 is executed, we know α was not empty. Adding its literals to T makes it subsumed by T, and thus it can no longer appear in T. If Line 17 is executed, we have two cases. The first case is that we are in the splittable mode. By construction of F, the query q is such that {a mathematical formula}κ[T](q)⊂constraints(α). This means that whatever the answer of the user, a clause will be generated by Conacq.2 that will subsume α, thus discarding α from T. The second case, non-splittable mode, means that Line 18 is executed and α is marked, so that it will never be selected again in Line 4.Correctness. QueryGeneration returns a query q generated in Line 17 or the result of IrredundantQuery in Line 19. If a query q has been generated in Line 17, by construction of formula F in function BuildFormula, q satisfies at least one constraint of each positive clause of T. In addition, it violates at least one constraint of α if α is not empty, or it violates at least one unset constraint. Thus, q is irredundant. If a query q has been generated in Line 19, it is irredundant by construction (see function IrredundantQuery). If no irredundant query has been found by IrredundantQuery, this means that none exists, and nil is returned.  □
      </paragraph>
      <paragraph label="Proof">
       Conacq.2 is correct and terminates.Correctness. At each non-terminal execution of the main loop, (Line 2), a new example {a mathematical formula}(q,y) is generated, where q is the query returned by BuildFormula, and y is the user's response given at Line 6. Since we took the assumption that the constraint acquisition setting is realizable, and membership queries are answered correctly, {a mathematical formula}(q,y) is consistent with the target concept, and hence, {a mathematical formula}CB(E) cannot collapse. By Definition 6, we also know that the clauses generated at Line 6 or at Line 7 yield a correct representation of {a mathematical formula}CB(E). So, by Theorem 5, the invariant of the main loop is that T is always satisfiable.The set N only contains nogoods that represent inconsistent sets of constraints. Hence, N cannot lead to the deletion of any concept from the version space, except the inconsistent concept (which, by assumption, is not the target concept). Thus, if K is a correct background knowledge, for any concept {a mathematical formula}fC consistent with the examples, {a mathematical formula}φ−1(C) is a model of {a mathematical formula}T∧K∧N. Now, by Lemma 2, Line 4 sets the Boolean converged to true if and only if there does not exist any irredundant query. So, the version space represented by T has converged.Termination. By Lemma 2 we know that the query q generated by BuildFormula is irredundant. So, by Proposition 10, the clauses added to T in lines 6 and 7 are not subsumed by T. As T involves a finite number of atoms, we cannot add new clauses forever without making T unsatisfiable. However, as an invariant of the main loop, we know that T is always satisfiable. This implies that the number of iterations of the main loop is finite.  □
      </paragraph>
      <paragraph>
       Now that we have presented the algorithm generating queries, we see that implementing our strategies is just a matter of setting t to the right value. In the optimal-in-expectation strategy, t must be set to 1 for every query. This corresponds to near-misses in machine learning [35], dividing by 2 the size of the version space whatever the answer from the user. In the optimistic strategy, we can set t to any value greater than 1 and smaller than the size of the clause α we want to shrink.
      </paragraph>
     </section>
     <section label="6.5">
      <section-title>
       Example of the query generation process
      </section-title>
      <paragraph>
       We now illustrate the query generation process on a small example.
      </paragraph>
      <paragraph label="Example 15">
       Query generationWe want to acquire the constraint network involving variables, {a mathematical formula}x1,…,x3, with domains {a mathematical formula}D(x1)=D(x2)=D(x3)={1,2,3,4} and constraints {a mathematical formula}CT={x1≤x2,x2≠x3}, with {a mathematical formula}B=B≤,≠,≥ and {a mathematical formula}K=∅. In the table below, the first column reports the queries generated by the query generation procedure, the second column reports the classification of the query, and the third column is the update of T.{a mathematical formula}At each execution of the main loop of Conacq.2, Line 3 calls the function QueryGeneration for producing a new query.
      </paragraph>
      <list>
       <list-item label="•">
        At the beginning T is empty. The query generation process directly goes to Line 19 of QueryGeneration and calls the function IrredundantQuery. As T is empty, {a mathematical formula}CL is also empty and an arbitrary assignment is returned (Line 4). Let {a mathematical formula}q1=〈2,2,4〉 be this first query. We have {a mathematical formula}κ(q1)={≠12,≥13,≥23}. As {a mathematical formula}q1 is classified positive by the user, we add the clauses {a mathematical formula}¬a(≠12), {a mathematical formula}¬a(≥13), and {a mathematical formula}¬a(≥23) to T in Line 7 of Conacq.2.
       </list-item>
       <list-item label="•">
        At the second call to QueryGeneration, T is still a monomial so we again directly call IrredundantQuery, which will return a query non-redundant with {a mathematical formula}q1. Let {a mathematical formula}q2=〈2,1,4〉 be that second query. We have {a mathematical formula}κ(q2)={≤12,≥13,≥23}, and {a mathematical formula}κ[T](q2)={≤12}. As {a mathematical formula}q2 is classified negative by the user, we add the clause {a mathematical formula}a(≤12) to T in Line 6 of Conacq.2.
       </list-item>
       <list-item label="•">
        Again T is a monomial and by the same process, the query {a mathematical formula}q3=〈1,1,1〉 is returned. We have {a mathematical formula}κ(q3)={≠12,≠13,≠23}, and {a mathematical formula}κ[T](q3)={≠13,≠23}. As {a mathematical formula}q3 is classified negative by the user, we add the clause {a mathematical formula}a(≠13)∨a(≠23) to T.
       </list-item>
       <list-item label="•">
        At this point T is no longer a monomial. QueryGeneration selects the non-unary clause {a mathematical formula}α=(a(≠13)∨a(≠23)) in Line 4 and calls function BuildFormula in Line 7 with the Boolean splittable set to true. In Line 2, function BuildFormula builds a formula F that contains all clauses of T. (Note that it is here irrelevant to remove literals {a mathematical formula}a(c‾) as our language does not contain any negation of constraints.) In addition, F forces all atoms which are neither already negated nor belonging to α to be true (Line 5). For each constraint c involved in α, F forces either {a mathematical formula}a(c) or {a mathematical formula}a(c‾). This will be used by the cardinality constraint in Line 11 to ensure that the number of constraints satisfying or rejecting a solution of a network built from a model of F follows the strategy. In our case, whatever the strategy is, {a mathematical formula}t=1 because {a mathematical formula}|α|=2. Thus, F is equal to {a mathematical formula}{a(≤12),a(≥12),a(≤13),a(≤23),a(≠13)∨a(=23),a(=13)∨a(≠23),exactly(1,α)}. A model of F is returned in Line 12 of QueryGeneration. But whatever this model is, {a mathematical formula}φ(I) is inconsistent because we cannot have {a mathematical formula}x1=x2, and {a mathematical formula}x3 equal to one of the variables {a mathematical formula}x1,x2 and different from the other. (Observe that if K had contained the rules {a mathematical formula}a(≤ij)∧a(≥ij)∧a(≠ik)→a(≠jk), {a mathematical formula}F∧K would have been unsatisfiable.) After two similar executions of the loop, Line 15 of QueryGeneration has added the conflict sets {a mathematical formula}{a(≤12),a(≥12),a(≠13),a(=23)} and {a mathematical formula}{a(≤12),a(≥12),a(=13),a(≠23)} to N. We loop to Line 2, selecting again α and entering BuildFormula. The same formula is produced, but this time Line 8 of QueryGeneration detects that {a mathematical formula}F∧N is inconsistent. ϵ is incremented and we loop again. splittable is flipped to false and BuildFormula is called. When splittable is false, we do not force all atoms outside α to be satisfied. Thus, {a mathematical formula}F∧N is satisfiable and Line 12 of QueryGeneration produces a model I that necessarily contains either {a mathematical formula}{a(≤12),a(≠13),a(=23)} or {a mathematical formula}{a(≤12),a(=13),a(≠23)}. Suppose I contains the former. {a mathematical formula}φ(I) has solutions, and one of them, {a mathematical formula}q4=〈1,2,2〉, is returned. Before returning {a mathematical formula}q4, α is marked in Line 18 of QueryGeneration, not to be selected again for splitting it. {a mathematical formula}q4 is classified as negative and a new clause {a mathematical formula}a(≥12)∨a(≠23) is added to T.
       </list-item>
       <list-item label="•">
        At the next loop, the clause {a mathematical formula}a(≠13)∨a(≠23) being marked, this is the clause {a mathematical formula}α=a(≥12)∨a(≠23) which is selected. BuildFormula works as usual, forcing all non-set literals to true except those in α, for which exactly one has to be satisfied. F is satisfiable, and its unique model leads to a satisfiable network. Let {a mathematical formula}q5=〈1,2,3〉 be the query generated. {a mathematical formula}κ[T](q5)={≥12}. {a mathematical formula}q5 is classified positive. As a result, {a mathematical formula}a(≥12) is set to false. By unit propagation, {a mathematical formula}a(≠23) is set to true and {a mathematical formula}CL is equal to the target network.
       </list-item>
       <list-item label="•">
        T is again a monomial. The two next loops of QueryGeneration call IrredundantQuery, which produces two, necessarily positive (because {a mathematical formula}CL=CT), queries, {a mathematical formula}q6=〈1,2,1〉 and {a mathematical formula}q7=〈2,4,1〉, leading to the unit literals {a mathematical formula}¬a(≠13), {a mathematical formula}¬a(≤23) and {a mathematical formula}¬a(≤13).
       </list-item>
       <list-item label="•">
        At this point IrredundantQuery is called a last time but cannot produce any irredundant query as all constraints in {a mathematical formula}B∖CL have been ruled out (Line 6). Conacq.2 returns convergence. ⋄
       </list-item>
      </list>
     </section>
    </section>
    <section label="7">
     <section-title>
      Experimental evaluation
     </section-title>
     <paragraph>
      We performed some experiments to evaluate and compare the algorithms presented in this paper. We implemented the passive Conacq.1 and the active Conacq.2 algorithms. We first present the benchmark problems we used for our experiments and we give a brief description on how we obtain a background knowledge for each problem instance. Then, we report the results of acquiring these problems with Conacq.1 and Conacq.2. We evaluate the impact of the background knowledge and of the different strategies proposed in Section 6. Our tests were conducted on an Intel Core i7 @ 2.9 GHz with 8 Gb of RAM.
     </paragraph>
     <section label="7.1">
      <section-title>
       Benchmark problems
      </section-title>
      <paragraph>
       Random. We generated binary random target networks {a mathematical formula}rand_n_d_m with n variables, domains of size d, and m binary constraints.
      </paragraph>
      <paragraph>
       Schur's lemma problem. (See prob015 in [21].) We considered the Schur's lemma problem. The problem is to put n balls labeled 1 to n into three boxes so that for any triple of balls {a mathematical formula}(x,y,z) with {a mathematical formula}x+y=z, not all are in the same box.
      </paragraph>
      <paragraph>
       Golomb rulers. (See prob006 in [21].) The problem is to find a ruler where the distance between any two marks is different from that between any other two marks. The target network is encoded with n variables corresponding to the n marks, and constraints of varying arity. For our experiments, we selected the instances from 4-marks to 8-marks rulers.
      </paragraph>
      <paragraph>
       Sudoku. We used the Sudoku logic puzzle with {a mathematical formula}4×4 and {a mathematical formula}9×9 grids. The grid must be filled with numbers from 1 to 4 (resp. 1 to 9) in such a way that all rows, all columns and the 4 (resp. 9) non-overlapping {a mathematical formula}2×2 (resp. {a mathematical formula}3×3) squares contain the numbers 1 to 4 (resp. 1 to 9). The target network of the Sudoku has 16 variables (resp. 81 variables) with domains of size 4 (resp. 9) and 56 (resp. 810) binary ≠ constraints on rows, columns and squares.
      </paragraph>
      <paragraph>
       For all these problems we used biases containing the basic arithmetic binary relations ≤, &lt;, ≠, =, ≥, and &gt;, plus, in some cases, quaternary constraints {a mathematical formula}|xi−xj|≠|xk−xl|, {a mathematical formula}|xi−xj|=|xk−xl|, {a mathematical formula}allEqual(xi,xj,xk,xl), {a mathematical formula}notAllEqual(xi,xj,xk,xl), {a mathematical formula}allDiff(xi,xj,xk,xl), {a mathematical formula}notAllDiff(xi,xj,xk,xl), and all ternary constraints obtained when two indices among {a mathematical formula}{i,j,k,l} are the same. This gives us biases of size 80 for the smallest problems to more than 6000 for the largest. For each problem, the bias used is able to express the problem.
      </paragraph>
     </section>
     <section label="7.2">
      <section-title>
       Background knowledge generation
      </section-title>
      <paragraph>
       We implemented a generator for the background knowledge to be used in Conacq. Generating complete background knowledge is generally too expensive in time and space as it requires generating a set of rules potentially exponential in space (see Section 5.1). Given a language Γ, our generator, called {a mathematical formula}gen_rules(#Vars,UB,maxSize), generates all first order rules involving a number #Vars of variables, with domains {a mathematical formula}[0,..,UB], and maxSize constraints. For instance, with {a mathematical formula}Γ={≤,≠,≥}, {a mathematical formula}gen_rules(3,UB,3) would produce the rule {a mathematical formula}a(≤ij)∧a(≤jk)→a(≤ik) whatever UB is. To ensure that generated rules are correct, the set {a mathematical formula}[0,..,UB] must be a superset of the domain D for all vocabularies {a mathematical formula}〈X,D〉 on which we want to apply the rule. For instance, the rule {a mathematical formula}a(≠ij)∧a(≠jk)→a(=ik) generated by {a mathematical formula}gen_rules(3,1,3) is no longer correct when applied on the domain {a mathematical formula}D={0,1,2}. Given a vocabulary {a mathematical formula}〈X,D〉, the background knowledge K is obtained by taking each rule in gen_rules and by generating all ground instances of the rule on all subsets of #Vars variables in X. Such a K is correct background knowledge.
      </paragraph>
      <paragraph>
       Table 2 reports the obtained background knowledge {a mathematical formula}Ki for each problem instance using our {a mathematical formula}gen_rules(#Vars,UB,maxSize) generator. The last column reports the CPU time consumed to generate {a mathematical formula}Ki. Take schurs_6 as an example. {a mathematical formula}gen_rules(4,10,2) takes 8.83 seconds to produce {a mathematical formula}K3, a background knowledge of 56 ground instances of rules.
      </paragraph>
     </section>
     <section label="7.3">
      Conacq.1: passive learning
      <paragraph>
       Table 3 reports the evaluation of passive learning using Conacq.1. For each problem instance, we ran 100 times the learning process. All number reported in the table are averages of these 100 runs. For each run, we randomly generated a training set E of 100 examples. The line denoted by {a mathematical formula}(1+100) represents the case where we process an extra positive example before processing E. The gray line represents the case where we use background knowledge ({a mathematical formula}Ki of Table 2). Each line of the table reports the size {a mathematical formula}|E| of the training set (smaller than 100 when convergence was reached before processing all examples), the number of positive examples {a mathematical formula}|E+| and negative examples {a mathematical formula}|E−|, and the size of the target network {a mathematical formula}CT. We also report the sizes of the learned network {a mathematical formula}CL and the most specific network S after all examples have been processed. S contains all constraints c from the bias such that {a mathematical formula}¬a(c) does not belong to T. The column {a mathematical formula}S/CT reports the ratio of solutions of S to {a mathematical formula}CT. {a mathematical formula}|EC| represents the number of examples needed to reach convergence. For obtaining {a mathematical formula}|EC| we have run Conacq.1 with 900 additional examples after the 100 first examples have been processed. The last column reports the average time in seconds needed to process an example (average over the 100 first examples).
      </paragraph>
      <paragraph>
       The first observation is the short time needed by Conacq.1 to process an example from the training set. If we exclude {a mathematical formula}schurs_10 which is more expensive, it goes from 0.1 seconds for the smallest problems to 2 seconds for the largest (the sudoku {a mathematical formula}9×9 and its 810 constraints). These CPU times include backbone detection in the theory T representing the version space each time an example is processed.
      </paragraph>
      <paragraph>
       The second observation from Table 3 is that without background knowledge, Conacq.1 is never able to converge to the target network. This is not surprising as we only work on the clausal theory T. If the target network contains an implied constraint c, taking any model of T and flipping {a mathematical formula}a(c) from 0 to 1 or 1 to 0 will remain a model, thus preventing convergence (see Algorithm 3). Nevertheless, we observe that the most specific network S is often quite close or equivalent to the target network ({a mathematical formula}S/CT≈100%). For instance, on {a mathematical formula}golomb_4, only two constraints were learned after 100 examples, but the most specific network S contains 24 constraints that are equivalent to the target network {a mathematical formula}CT ({a mathematical formula}S/CT=100%). Providing a positive example at the beginning increases even more this behavior, especially on problems which are critically constrained.
      </paragraph>
      <paragraph>
       The third observation we can make from Table 3 is that providing Conacq.1 with some background knowledge often helps a lot to reach convergence. Take for instance {a mathematical formula}schurs_6. Without background knowledge, Conacq.1 is able to learn a {a mathematical formula}CL of 6 constraints in {a mathematical formula}1+100 examples, and the most specific network S has 9 constraints. By adding background knowledge, we reach convergence after 70 examples. In this case the three missing constraints were implied by {a mathematical formula}CL. On {a mathematical formula}schurs_8 the scenario is almost the same. After {a mathematical formula}1+100 examples, Conacq.1 with some background knowledge has learned 8 more constraints than without. It has not yet converged but will do after 123 examples. Half of our problems have converged before 1000 examples.
      </paragraph>
     </section>
     <section label="7.4">
      Conacq.2: active learning
      <paragraph>
       We present some results on active learning using Conacq.2. In our implementation of Conacq.2 we made the following choices. In Line 4 of Algorithm 5, we always select the clause α of minimum size. In Line 14 of Algorithm 5, the function ConflictSets returns a single conflict set. In the optimistic strategy, the parameter t is set to {a mathematical formula}⌊|α|/2⌋. The following two subsections are respectively devoted to active learning without background knowledge and to active learning using some background knowledge.
      </paragraph>
      <section label="7.4.1">
       Conacq.2 without background knowledge
       <paragraph>
        Table 4 displays results on active learning using Conacq.2 without any background knowledge. We give a comparison between the two query generation strategies, optimal-in-expectation and optimistic, described in Section 6. For each strategy and for each problem instance, we report the size {a mathematical formula}|CT| of the target network, the total number of queries #q, and the numbers #yes and #no of positive and negative answers. We also report the time in seconds needed to generate and to process a query. (We obviously do not count the time needed by the user to reply. This effort is measured by #q.)
       </paragraph>
       <paragraph>
        The first observation that we can draw from Table 4 is that generating a query generally takes more time than processing an example in Conacq.1. Nevertheless, these times remain reasonable for a non-optimized implementation. They go from 0.05 to 11 seconds for the optimal-in-expectation strategy.
       </paragraph>
       <paragraph>
        The second observation is that the number of queries asked by Conacq.2 is dramatically reduced compared to Conacq.1 (regardless of the query generation strategy that is used). For all instances of Random, Schur's Lemma and Golomb rulers, Conacq.2 converges to the target network in less than 1000 queries whereas Conacq.1 without background knowledge could not converge. For instance, for {a mathematical formula}golomb_4, Conacq.1 has learned only 2 constraints with 100 examples, whereas Conacq.2 converges to the target network with only 95 queries. Unfortunately, we also observe that Conacq.2, like Conacq.1, is not able to learn Sudoku puzzles with 1000 queries.
       </paragraph>
       <paragraph>
        When we compare the two query generation approaches among themselves, we see that the optimistic one is the best on under-constrained networks (e.g., Schur's Lemma). This confirms that when the probability of a yes answer is high enough, choosing a query that violates several constraints is a good strategy. However, on critically constrained problems, where the density of solutions is low, the optimal-in-expectation strategy becomes the best.
       </paragraph>
       <paragraph>
        Fig. 1 shows the impact of Conacq.1 and Conacq.2 on the version space for {a mathematical formula}golomb_4. At the initial state, the learned network {a mathematical formula}CL is empty and the most specific S contains the whole bias (i.e., all possible constraints). Providing Conacq.1 and Conacq.2 with a positive example at the beginning divides the version space by half. Conacq.1 learns 4 constraints when processing the 526 first examples. Thereafter, the version space remains stable in size ({a mathematical formula}|S|=24,|CL|=4) until we have processed the 1000 examples in the training set. In Conacq.2, the learning process is much more efficient. S is reduced to the target network in 75 queries and convergence is proved in 95 queries.
       </paragraph>
      </section>
      <section label="7.4.2">
       Conacq.2 with background knowledge
       <paragraph>
        Table 5 reports our experiments on Conacq.2 using background knowledge. The strategy for query generation is optimal-in-expectation. In addition to the size of the learned network {a mathematical formula}|CL|, the number of queries (#q, #yes, #no) and the average time needed for generating and processing a query, we report the background knowledge {a mathematical formula}Ki of Table 2 we used.
       </paragraph>
       <paragraph>
        The first observation is that, in Conacq.2, the use of background knowledge does not reduce the number of queries required for convergence. This is explained by the test of convergence in Conacq.2, which is no longer done on the clausal theory T but on the actual constraint networks. Hence, implied constraints become irrelevant. (The small variations in number of queries between Conacq.2 with and without background knowledge are due to the difference in queries generated.)
       </paragraph>
       <paragraph>
        The second information we can draw from Table 5 is that the use of background knowledge speeds up significantly the generation of queries. It is, for instance, between 3 and 4 times faster on {a mathematical formula}schurs_10 or {a mathematical formula}golomb_8. The reason for the speed up in generating queries is that rules in K prevent some of the fails in Line 13 of function QueryGeneration. Instead of repeatedly generating unsatisfiable constraint networks, a rule allows to directly detect unsatisfiability in the clausal formula (Line 8), avoiding the generation of a potentially large number of nogoods.
       </paragraph>
       <paragraph>
        Finally, we observe that despite background knowledge, we are still unable to learn the sudoku puzzles. The reason for this bad behavior is probably that our background knowledges {a mathematical formula}K9 and {a mathematical formula}K10 do not contain rules useful for learning the cliques of disequalities of the sudoku.
       </paragraph>
      </section>
     </section>
     <section label="7.5">
      <section-title>
       Discussion
      </section-title>
      <paragraph>
       In this section we have provided basic experiments on a bench of toy problems. Not surprisingly, Conacq.1 often requires a huge number of examples to converge to the target network. The good news are that even when Conacq.1 has not converged, the most specific network in the version space is often quite close to the target network. Conacq.2, by its active behavior, reduces a lot the number of examples needed to converge to the target network. Once the user has produced the effort to learn her target network, we can wonder about the re-usability of this network. From the definition of our framework, the network will be usable as long as the user wants to solve instances of the problem on the same set of variables and with domains included in the domain of the vocabulary used to learn. For instance, once the sudoku network has been learned, any sudoku grid can be solved with this network, by simply specifying the singleton domains associated with the clues. However, one can object that even if re-usable, these numbers of queries remain too large if a human user is in the loop. This weakness of our basic constraint acquisition framework has led to several subsequent works based on or inspired by the Conacq approach. These works are briefly discussed in the next section. Their common denominator is to use as much background knowledge as possible or to allow for more complex queries asked to the user. Background knowledge is a well-known technique in machine learning consisting in using properties of the problem to learn to reduce the bias as much as possible so that the version space to explore becomes small enough. Complex queries is another way to speed up convergence by allowing a more informative communication between the user and the learner so as to capture more information.
      </paragraph>
     </section>
    </section>
    <section label="8">
     <section-title>
      Related work
     </section-title>
     <paragraph>
      It is instructive to compare our constraint acquisition approaches with the classical learning approaches advocated in the machine learning literature. Notably, in the Probably Approximately Correct (PAC) learning introduced by Valiant [36], the learner is merely passive and interacts with an oracle that supplies examples independently at random according to a fixed distribution. In this setting, an ϵ-good hypothesis is a concept for which the probability of misclassifying an example supplied by the oracle is at most ϵ. Very roughly, the goal of a PAC learning algorithm is to provide, with high probability {a mathematical formula}(1−δ), an ϵ-good hypothesis. In our passive acquisition, the user can be viewed as an example oracle. Yet, the essential difference is that she is not required to supply examples according to a fixed distribution. The distribution is allowed to change during the acquisition process.
     </paragraph>
     <paragraph>
      In [4], Beldiceanu and Simonis have proposed ModelSeeker, a passive constraint acquisition system. ModelSeeker is devoted to problems having a regular structure, such as matrix models. In ModelSeeker the bias contains global constraint from the global constraints catalog [3] whose scopes are the rows, the columns, or any other structural property ModelSeeker can capture. ModelSeeker also provides an efficient ranking technique that returns the best candidate constraints representing the pattern occurring on a particular set of variables (e.g., variables composing a row). As opposed to our passive learning approach, ModelSeeker handles positive examples only. Its very specific bias allows it to quickly find candidate models when the problem has a good structure. The counterpart is that it misses any constraint that does not belong to one of the structural patterns it is able to handle. Finally, its efficiency also comes from the fact that it does not prove convergence. It provides candidate constraints. It is the user who selects the constraints that fit the best the target problem.
     </paragraph>
     <paragraph>
      In [25], Lallouet et al. have proposed a passive constraint acquisition system based on inductive logic programming. Their system is able to learn constraints from a given language that classify correctly the examples. To overcome the problem of the huge space of possible candidate constraint networks, their system requires the structure of the constraint network to be put in the background knowledge. They illustrate their approach on graph coloring problems. The positive/negative examples (i.e., correct and wrong colorations) are provided with their logical description using a set of given predicates. The background knowledge already contains all edges of the graph. These assumptions on the provided background knowledge make the approach questionable.
     </paragraph>
     <paragraph>
      In [6], Bessiere et al. have proposed QuAcq, an active learner which, in addition to membership queries, is able to ask the user to classify partial queries. A partial query is an assignment that does not involve all variables. This is thus an extra condition on the capabilities of the user: Even if she is not able to articulate the constraints of her problem, she is able to decide if partial assignments of variables violate some requirements or not. The significant advantage of using partial queries is that our Theorem 4 about the impossibility to guarantee the existence of a polynomial sequence of queries to converge no longer holds. For instance, given a negative example, Conacq produces a non-unary positive clause on the candidate constraints for rejecting that example. Using partial queries, QuAcq is able, from the same negative example, to return exactly a constraint of the target network in a number of (partial) queries logarithmic in the number of variables. The overall number of queries to converge is thus polynomial.
     </paragraph>
     <paragraph>
      In [33], the approach used in Conacq.2 has been extended to allow the user to provide arguments. Given a target network C, an argument arg is a set of constraints and/or variable assignments that the user labels as positive (i.e., {a mathematical formula}C∪arg is satisfiable), negative (i.e., {a mathematical formula}C∪arg is unsatisfiable), sufficient (i.e., {a mathematical formula}arg⊨C), or necessary (i.e., {a mathematical formula}C⊨arg). Such arguments allow the constraint acquisition process to converge more rapidly (with less examples) than Conacq.2. However, it puts more of the effort of constraint acquisition on the shoulders of the user. For instance, deciding that an argument is positive is NP-complete, deciding that an argument is negative/sufficient/necessary is coNP-complete. In Conacq.2, classifying a query as positive/negative is linear in the number of constraints in the target network C.
     </paragraph>
     <paragraph>
      Finally, we would like to point out to the reader that what we call constraint acquisition in this paper is not comparable to works presented for instance in [10], [26]. These two papers propose techniques, which, given a model for a constraint problem, learn implied (global) constraints that can enhance the model (in [10]) or replace some simpler constraints of the model (in [26]). The goal is not to acquire a constraint model for the problem the user has in mind, but instead, to acquire a better model than the one the user proposed. In this case, better means a model with (global) constraints that are expected to propagate more or faster during search. This does not mean that Conacq cannot be viewed as a reformulation tool. We could indeed initialize Conacq with a bias containing only constraints for which we know efficient propagators, and let it interact with the user's model instead of the user directly. The learned network would then hopefully be more efficient to solve than the original one.
     </paragraph>
    </section>
    <section label="9">
     <section-title>
      Conclusion
     </section-title>
     <paragraph>
      In this paper we have presented the basic architecture for acquiring constraint networks from examples classified by the user. We have formally defined the main constraint acquisition problems related to passive and active acquisition. We have closed several central complexity results that were still open. For instance, we have shown that consistency of the version space is polynomial to decide whereas convergence is intractable. We have also shown that constraint networks are not learnable in general with a polynomial number of membership queries. We have then proposed Conacq, a system for acquiring constraint networks that uses a clausal representation of the version space. Conacq is presented in a passive version (Conacq.1), where the learner is only provided a pool of examples, and an active version (Conacq.2), where the learner asks membership queries to the user. The clausal representation of the version space allows Conacq to perform operations on the version space efficiently. For instance, the clausal representation is used to implement query generation strategies in active acquisition. Finally, we have compared experimentally the passive and active versions of Conacq on a set of toy problems.
     </paragraph>
    </section>
   </content>
  </root>
 </body>
</html>