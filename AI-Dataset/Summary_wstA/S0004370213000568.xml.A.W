<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:177">
</head>
<body bgcolor="white">
<a href="#0" id="0">Reinforcement learning (RL) and learning from demonstration (LfD) are two popular families of algorithms for learning policies for sequential decision problems, but they are often ineffective in high-dimensional domains unless provided with either a great deal of problem-specific domain information or a carefully crafted representation of the state and dynamics of the world.</a>
<a href="#1" id="1">We introduce new approaches inspired by these two techniques, which we broadly call abstraction from demonstration.</a>
<a href="#2" id="2">Our first algorithm, state abstraction from demonstration (AfD), uses a small set of human demonstrations of the task the agent must learn to determine a state-space abstraction.</a>
<a href="#3" id="3">Our second algorithm, abstraction and decomposition from demonstration (ADA), is additionally able to determine a task decomposition from the demonstrations.</a>
<a href="#4" id="4">These abstractions allow RL to scale up to higher-complexity domains, and offer much better performance than LfD with orders of magnitude fewer demonstrations.</a>
<a href="#5" id="5">Using a set of videogame-like domains, we demonstrate that using abstraction from demonstration can obtain up to exponential speed-ups in table-based representations, and polynomial speed-ups when compared with function approximation-based RL algorithms such as fitted Q-learning and LSPI.</a>
</body>
</html>