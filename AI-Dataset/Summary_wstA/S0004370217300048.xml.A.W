<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:260">
</head>
<body bgcolor="white">
<a href="#0" id="0">Dropout is a recently introduced algorithm for training neural networks by randomly dropping units during training to prevent their co-adaptation.</a>
<a href="#1" id="1">A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates.</a>
<a href="#2" id="2">The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case.</a>
<a href="#3" id="3">The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables.</a>
<a href="#4" id="4">The results are also extended to other classes of transfer functions, including rectified linear functions.</a>
<a href="#5" id="5">Approximation errors tend to cancel each other and do not accumulate.</a>
<a href="#6" id="6">Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network.</a>
<a href="#7" id="7">Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent.</a>
<a href="#8" id="8">Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.</a>
</body>
</html>