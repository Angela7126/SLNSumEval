<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    The complexity of Bayesian networks specified by propositional and relational languages.
   </title>
   <abstract>
    We examine the complexity of inference in Bayesian networks specified by logical languages. We consider representations that range from fragments of propositional logic to function-free first-order logic with equality; in doing so we cover a variety of plate models and of probabilistic relational models. We study the complexity of inferences when network, query and domain are the input (the inferential and the combined complexity), when the network is fixed and query and domain are the input (the query/data complexity), and when the network and query are fixed and the domain is the input (the domain complexity). We draw connections with probabilistic databases and liftability results, and obtain complexity classes that range from polynomial to exponential levels; we identify new languages with tractable inference, and we relate our results to languages based on plates and probabilistic relational models.
   </abstract>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      A Bayesian network can represent any distribution over a given set of random variables [33], [69], and this flexibility has been used to great effect in a variety of applications [107]. Many of these applications contain repetitive entities and relationships. Thus it is not surprising that practical concerns have led to modeling languages where Bayesian networks are specified using relations, logical variables, and quantifiers [46], [109]. Some of these languages enlarge Bayesian networks with plates [47], [83], while others resort to elements of database schema [44], [58]; some others mix probabilities with logic programming [104], [116] and even with functional programming [85], [89], [100]. The spectrum of tools that specify Bayesian networks by moving beyond propositional sentences is vast, and their applications are remarkable.
     </paragraph>
     <paragraph>
      Yet most of the existing complexity analysis of inference with Bayesian networks focuses on a simplified setting where nodes of a network are associated with categorial variables and distributions are specified by flat tables containing probability values [73], [113]. This is certainly unsatisfying: as a point of comparison, consider the topic of logical inference, where much is known about the impact of specific constructs on computational complexity—suffice to mention the beautiful and detailed study of inference complexity in description logics [3].
     </paragraph>
     <paragraph>
      In this paper we examine the complexity of inferences in Bayesian networks as dependent on the language that is used to specify the networks. We adopt a simple specification strategy inspired by probabilistic programming [105] and by structural equation models [99], where a Bayesian network over binary variables is specified by a set of logical formulas and a set of independent random variables. As we show in the paper, this abstract specification strategy captures a vast range of modeling languages in the literature. To illustrate the sort of specification we contemplate, consider a short example that will be elaborated later. Suppose we have a population of students, and denote by {a mathematical formula}fan(x) the fact that student {a mathematical formula}x is a fan of say a particular band. And write {a mathematical formula}friends(x,y) to indicate that {a mathematical formula}x is a friend of {a mathematical formula}y. Now consider a Bayesian network with a node {a mathematical formula}fan(x) per student, and a node {a mathematical formula}friends(x,y) per pair of students (see Fig. 1). Suppose each node {a mathematical formula}fan(x) is associated with the assessment {a mathematical formula}P(fan(x)=true)=0.2. And finally suppose that a person is always a friend of herself, and two people are friends if they are fans of the band; that is, for each pair of students, {a mathematical formula}friends(x,y) is associated with the formula{a mathematical formula} Now if we have data on some students, we may ask for the probability that some two students are friends, or the probability that a student is a fan. We may wish to consider more sophisticated formulas specifying friendship: how would the complexity of our inferences change, say, if we allowed quantifiers in our formula? Or if we allowed relations of arity higher than two? Such questions are the object of our discussion.
     </paragraph>
     <paragraph>
      We can thus parameterize computational complexity by the formal language that is allowed in the logical formulas; we can move from sub-Boolean languages to relational ones, in the way producing languages that are similar in power to plate models [47] and to probabilistic relational models [72]. Overall we follow a proven strategy adopted in logical formalisms: we focus on minimal sets of constructs (Boolean operators, quantifiers) that capture the essential connections between expressivity and complexity, and that can shed light on more sophisticated languages. Our broader goal is to help with the design of knowledge representation formalisms, and in that setting it is important to understand the complexity introduced by language features, however costly those may be.
     </paragraph>
     <paragraph>
      In this study, we distinguish a few concepts. Inferential complexity is the complexity when the network, the query and the domain are given as input. When the specification vocabulary is fixed, inference complexity is akin to combined complexity as employed in database theory. Query complexity is the complexity when the network is fixed and the input consists of query and domain. The same concept has been often referred to as data complexity in the context of probabilistic databases [121]; we use “query” here as typically done in the Bayesian network literature, and later comment on the connections between these concepts. Finally, domain complexity is the complexity when network and query are fixed, and only the domain is given as input. Query and domain complexity are directly related respectively to dqe-liftability and domain liftability, concepts that have been used in lifted inference [8], [64]. We make connections with lifted inference and probabilistic databases whenever possible, and benefit from deep results from those fields. One of the contributions of this paper is a framework that can unify these varied research efforts with respect to the analysis of Bayesian networks. We show that many non-trivial complexity classes characterize the cost of inference as induced by various languages, and we make an effort to relate our investigation to various knowledge representation formalisms, from probabilistic description logics to plates to probabilistic relational models.
     </paragraph>
     <paragraph>
      The paper is organized as follows. Section 2 reviews a few concepts concerning Bayesian networks and computational complexity. Our contributions start in Section 3, where we focus on propositional languages. In Section 4 we extend our framework to relational languages, and review relevant literature on probabilistic databases and lifted inference. In Sections 5 and 6 we study a variety of relational Bayesian network specifications. In Section 7 we connect these specifications to other schemes proposed in the literature. And in Section 8 we relate our results, mostly presented within Wagner's counting hierarchy and its extensions, to Valiant's counting hierarchy and its extensions. Section 9 summarizes our findings and proposes future work.
     </paragraph>
     <paragraph>
      All proofs are collected in AppendixA.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      A bit of notation and terminology
     </section-title>
     <paragraph>
      We denote by {a mathematical formula}P(A) the probability of event A. In this paper, every random variable X is a function from a finite sample space (usually a space with finitely many truth assignments or interpretations) to real numbers (usually to {a mathematical formula}{0,1}). We refer to an event {a mathematical formula}{X=x} as an assignment. Say that {a mathematical formula}{X=1} is a positive assignment, and {a mathematical formula}{X=0} is a negative assignment.
     </paragraph>
     <paragraph>
      A graph consists of a set of nodes and a set of edges (an edge is specified as a pair of nodes), and we focus on graphs that are directed and acyclic [69]. The parents of a node X, for a given graph, are denoted {a mathematical formula}pa(X). Suppose we have a directed acyclic graph {a mathematical formula}G such that each node is a random variable, and we also have a joint probability distribution {a mathematical formula}P over these random variables. Say that {a mathematical formula}G and {a mathematical formula}P satisfy the Markov condition iff each random variable X is independent with respect to {a mathematical formula}P of its nondescendants in {a mathematical formula}G given its parents in {a mathematical formula}G.
     </paragraph>
     <paragraph>
      A Bayesian network is a pair consisting of a directed acyclic graph {a mathematical formula}G whose nodes are random variables and a joint probability distribution {a mathematical formula}P over all variables in the graph, such that {a mathematical formula}G and {a mathematical formula}P satisfy the Markov condition [92]. For a collection of measurable sets {a mathematical formula}A1,…,An, we then have{a mathematical formula} whenever the conditional probabilities exist. If all random variables are discrete, then one can specify “local” conditional probabilities {a mathematical formula}P(Xi=xi|pa(Xi)=πi), and the joint probability mass function is the product of these local probabilities:{a mathematical formula} where {a mathematical formula}πi is the projection of {a mathematical formula}{x1,…,xn} on {a mathematical formula}pa(Xi), with the understanding that {a mathematical formula}P(Xi=xi|pa(Xi)=πi) stands for {a mathematical formula}P(Xi=xi) whenever {a mathematical formula}Xi has no parents.
     </paragraph>
     <paragraph>
      In this paper we only deal with finite objects, so we can assume that a Bayesian network is fully specified by a finite graph and a local conditional probability distribution per random variable: the local distribution associated with random variable X specifies the probability of X given the parents of X. Often probability values are given in tables (referred to as conditional probability tables). Depending on how these tables are encoded, the directed acyclic graph may be redundant; that is, all the information to reconstruct the graph and the joint distribution is already in the tables. Even though we rarely mention the graph {a mathematical formula}G in our results, graphs are visually useful and we often resort to drawing them in our examples.
     </paragraph>
     <paragraph>
      A basic computational problem for Bayesian networks is: Given a Bayesian network {a mathematical formula}B, a set of assignments Q and a set of assignments E, determine whether {a mathematical formula}P(Q|E)&gt;γ for some rational number γ. We assume that every probability value is specified as a rational number. Hence {a mathematical formula}P(Q|E)=P(Q,E)/P(E) is a rational number, as {a mathematical formula}P(Q,E) and {a mathematical formula}P(E) are computed by summing through products given by Expression (2).
     </paragraph>
     <paragraph>
      We adopt basic terminology and notation from computational complexity [97]. A language is a set of strings. A language defines a decision problem; that is, the problem of deciding whether an input string is in the language. A complexity class is a set of languages; we use well-known complexity classes {a mathematical formula}P, {a mathematical formula}NP, {a mathematical formula}PSPACE, and {a mathematical formula}EXP. We also use {a mathematical formula}ETIME, the class of languages that can be decided by a deterministic Turing machine in time {a mathematical formula}O(2cN) when the input is of size N, for some constant c; similarly, {a mathematical formula}NETIME is the class of languages that can be decided by a nondeterministic Turing machine in time {a mathematical formula}O(2cN). The complexity class {a mathematical formula}PP consists of those languages {a mathematical formula}L that satisfy the following property: there is a polynomial-time nondeterministic Turing machine M such that {a mathematical formula}ℓ∈L iff more than half of the computations of M on input ℓ end up accepting. Analogously, we have {a mathematical formula}PEXP, consisting of those languages {a mathematical formula}L with the following property: there is an exponential-time nondeterministic Turing machine M such that {a mathematical formula}ℓ∈L iff more than half of the computations of M on input ℓ end up accepting [14].
     </paragraph>
     <paragraph>
      To proceed, we need to define oracles and related complexity classes. An oracle Turing machine {a mathematical formula}ML, where {a mathematical formula}L is a language, is a Turing machine with additional tapes, such that it can write a string ℓ to a tape and obtain from the oracle, in unit time, the decision as to whether {a mathematical formula}ℓ∈L or not. If a class of languages/functions {a mathematical formula}A is defined by a set of Turing machines {a mathematical formula}M (that is, the languages/functions are decided/computed by these machines), then define {a mathematical formula}AL to be the set of languages/functions that are decided/computed by {a mathematical formula}{ML:M∈M}. For a function f, an oracle Turing machine {a mathematical formula}Mf can be similarly defined, and for any class {a mathematical formula}A we have {a mathematical formula}Af. If {a mathematical formula}A and {a mathematical formula}B are classes of languages/functions, {a mathematical formula}AB=∪x∈BAx. For instance, the polynomial hierarchy consists of classes {a mathematical formula}ΣiP=NPΣi−1P and {a mathematical formula}ΠiP=coΣiP, with {a mathematical formula}Σ0P=P (and {a mathematical formula}PH is the union {a mathematical formula}∪iΠiP=∪iΣiP).
     </paragraph>
     <paragraph>
      Wagner's polynomial counting hierarchy is the smallest set of classes containing {a mathematical formula}P and, recursively, for any class {a mathematical formula}C in the polynomial counting hierarchy, the classes {a mathematical formula}PPC, {a mathematical formula}NPC, and {a mathematical formula}coNPC (this characterization based on oracles comes from results by Wagner [136, Theorem 4] and Toran [127, Theorem 4.1]). The polynomial hierarchy is included in Wagner's counting polynomial hierarchy.
     </paragraph>
     <paragraph>
      A distinct counting hierarchy is Valiant's [129]. We examine this hierarchy further in Section 8; for now suffice to say that {a mathematical formula}#P is the class of functions such that {a mathematical formula}f∈#P iff {a mathematical formula}f(ℓ) is the number of computation paths that accept ℓ for some polynomial-time nondeterministic Turing machine. It is as if we had a special machine, called by Valiant a counting Turing machine, that on input ℓ prints on a special tape the number of computations that accept ℓ.
     </paragraph>
     <paragraph>
      We will also use the class {a mathematical formula}PP1, defined as the set of languages in {a mathematical formula}PP that have a single symbol as input vocabulary. We can take this symbol to be 1, so the input is just a sequence of 1s. One can interpret this input as a non-negative integer written in unary notation. This is the counterpart of Valiant's class {a mathematical formula}#P1 that consists of the functions in {a mathematical formula}#P that have a single symbol as input vocabulary [130].
     </paragraph>
     <paragraph>
      We focus on many-one reductions: such a reduction from {a mathematical formula}L to {a mathematical formula}L′ is a polynomial-time algorithm that takes the input to decision problem {a mathematical formula}L and transforms it into the input to decision problem {a mathematical formula}L′ such that {a mathematical formula}L′ has the same output as {a mathematical formula}L. A Turing reduction from {a mathematical formula}L to {a mathematical formula}L′ is a polynomial-time algorithm that decides {a mathematical formula}L using {a mathematical formula}L′ as an oracle. For a complexity class {a mathematical formula}C, a decision problem {a mathematical formula}L is {a mathematical formula}C-hard with respect to many-one reductions if each decision problem in {a mathematical formula}C can be reduced to {a mathematical formula}L with many-one reductions. A decision problem is then {a mathematical formula}C-complete with respect to many-one reductions if it is in {a mathematical formula}C and it is {a mathematical formula}C-hard with respect to many-one reductions. Similar definitions of hardness and completeness are obtained when “many-one reductions” are replaced by “Turing reductions”.
     </paragraph>
     <paragraph>
      An important {a mathematical formula}PP-complete (with respect to many-one reductions) decision problem is MAJSAT: the input is a propositional sentence ϕ and the decision is whether or not the majority of assignments to the propositions in ϕ make ϕ{a mathematical formula}true[48]. Another {a mathematical formula}PP-complete problem (with respect to many-one reductions) is deciding whether {a mathematical formula}#ϕ&gt;k[117]; we use #ϕ to denote the number of satisfying assignments for a formula ϕ. In fact this problem is still {a mathematical formula}PP-complete with respect to many-one reductions even if ϕ is monotone [51]. Recall that a sentence is monotone if it contains no negation.
     </paragraph>
     <paragraph>
      A formula is in kCNF iff it is in Conjunctive Normal Form with k literals per clause (if there is no restriction on k, we just write CNF). MAJSAT is {a mathematical formula}PP-complete with respect to many-one reductions even if the input is restricted to be in CNF; however, it is not known whether MAJSAT is still {a mathematical formula}PP-complete with respect to many-one reductions if the sentence ϕ is in 3CNF. Hence we will resort in proofs to a slightly different decision problem, following results by Bailey et al. [6]. The problem {a mathematical formula}#3SAT(&gt;) gets as input a propositional sentence ϕ in 3CNF and an integer k, and the decision is whether {a mathematical formula}#ϕ&gt;k. We will also use, in the proof of Theorem 2, the following decision problem. Say that a truth assignment to the propositions in a sentence in 3CNF respects the 1-in-3 rule if at most one literal per clause is assigned {a mathematical formula}true. Denote by {a mathematical formula}#(1-in-3)ϕ the number of satisfying assignments for ϕ that also respects the 1-in-3 rule. The decision problem {a mathematical formula}#(1-in-3)SAT(&gt;) gets as input a propositional sentence ϕ in 3CNF and an integer k, and decides whether {a mathematical formula}#(1-in-3)ϕ&gt;k. We have:
     </paragraph>
     <paragraph label="Proposition 1">
      Both{a mathematical formula}#3SAT(&gt;)and{a mathematical formula}#(1-in-3)SAT(&gt;)are{a mathematical formula}PP-complete with respect to many-one reductions.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Propositional languages: inferential and query complexity
     </section-title>
     <paragraph>
      In this section we focus on propositional languages, so as to present our proposed framework in the most accessible manner. Recall that we wish to parameterize the complexity of inferences by the language used in specifying local distributions.
     </paragraph>
     <section label="3.1">
      <section-title>
       A specification framework
      </section-title>
      <paragraph>
       We are interested in specifying Bayesian networks over binary variables {a mathematical formula}X1,…,Xn, where each random variable {a mathematical formula}Xi is the indicator function of a proposition {a mathematical formula}Ai. That is, consider the space Ω consisting of all truth assignments for these propositions (there are {a mathematical formula}2n such truth assignments); then {a mathematical formula}Xi yields 1 for a truth assignment that satisfies {a mathematical formula}Ai, and {a mathematical formula}Xi yields 0 for a truth assignment that does not satisfy {a mathematical formula}Ai. We will often use the same letter to refer to a proposition and the random variable that is the indicator function of the proposition.
      </paragraph>
      <paragraph>
       We adopt a specification strategy that moves away from tables of probability values, by borrowing from probabilistic programming [101], [116] and structural models [99]. A Bayesian network specification associates with each proposition {a mathematical formula}Xi either
      </paragraph>
      <list>
       <list-item label="•">
        a logical equivalence {a mathematical formula}Xi↔ℓi, or
       </list-item>
       <list-item label="•">
        a probabilistic assessment {a mathematical formula}P(Xi=1)=α,
       </list-item>
      </list>
      <paragraph>
       where {a mathematical formula}ℓi is a formula in a propositional language {a mathematical formula}L, such that the only extralogical symbols in {a mathematical formula}ℓi are propositions in {a mathematical formula}{X1,…,Xn}, and α is a rational number in the interval {a mathematical formula}[0,1].
      </paragraph>
      <paragraph>
       We refer to each logical equivalence {a mathematical formula}Xi↔ℓi as a definition axiom, borrowing terminology from description logics [3]. We refer to {a mathematical formula}ℓi as the body of the definition axiom. In order to avoid confusion between the leftmost symbol ↔ and possible logical equivalences within {a mathematical formula}ℓi, we write a definition axiom as in description logics:{a mathematical formula} and we emphasize that {an inline-figure} is just syntactic sugar for logical equivalence ↔.
      </paragraph>
      <paragraph>
       A Bayesian network specification induces a directed graph where the nodes are the random variables {a mathematical formula}X1,…,Xn, and {a mathematical formula}Xj is a parent of {a mathematical formula}Xi if and only if the definition axiom for {a mathematical formula}Xi contains {a mathematical formula}Xj. If this graph is acyclic, as we assume in this paper, then the Bayesian network specification does define a Bayesian network.
      </paragraph>
      <paragraph>
       Fig. 2 depicts a Bayesian network specified this way.
      </paragraph>
      <paragraph>
       Note that we avoid direct assessments of conditional probability, because one can essentially create negation through {a mathematical formula}P(X=1|Y=1)=P(X=0|Y=0)=0. In our framework, the use of negation is a decision about the language. We will see that negation does make a difference when complexity is analyzed.
      </paragraph>
      <paragraph>
       Any distribution over binary variables given by a Bayesian network can be equivalently specified using definition axioms, as long as definitions are allowed to contain negation and conjunction (and then disjunction is syntactic sugar). To see that, consider a conditional distribution for X given {a mathematical formula}Y1 and {a mathematical formula}Y2; we can specify this distribution using the definition axiom{a mathematical formula} where {a mathematical formula}Zab are fresh binary variables (that do not appear anywhere else), associated with assessments {a mathematical formula}P(Zab=1)=P(X=1|Y1=a,Y2=b). This sort of encoding can be extended to any set {a mathematical formula}Y1,…,Ym of parents, demanding the same space as the corresponding conditional probability table.
      </paragraph>
      <paragraph label="Example 1">
       Consider a simple Bayesian network with random variables X and Y, where Y is the sole parent of X, and where:{a mathematical formula} Then Fig. 2 presents an equivalent specification for this network, in the sense that both specifications have the same marginal distribution over {a mathematical formula}(X,Y). □
      </paragraph>
      <paragraph>
       Note that definition axioms can exploit structures that conditional probability tables cannot; for instance, to create a Noisy-Or gate [98], we simply write {an inline-figure}, where {a mathematical formula}W1 and {a mathematical formula}W2 are inhibitor variables.
      </paragraph>
     </section>
     <section label="3.2">
      <section-title>
       The complexity of propositional languages
      </section-title>
      <paragraph>
       Now consider a language {a mathematical formula}INF[L] that consists of the strings {a mathematical formula}(B,Q,E,γ) for which {a mathematical formula}P(Q|E)&gt;γ, where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}P is the distribution encoded by a Bayesian network specification {a mathematical formula}B with definition axioms whose bodies are formulas in {a mathematical formula}L,
       </list-item>
       <list-item label="•">
        Q and E are sets of assignments (the query),
       </list-item>
       <list-item label="•">
        and γ is a rational number in {a mathematical formula}[0,1].
       </list-item>
      </list>
      <paragraph>
       In all definitions and results in this paper it is assumed that if {a mathematical formula}P(E)=0 then the answer to the question “Is {a mathematical formula}P(Q|E)&gt;γ?” is “no”, meaning “there is no such probability that is larger than γ”.{sup:1}
      </paragraph>
      <paragraph>
       To start, denote by {a mathematical formula}Prop(∧,¬) the language of propositional formulas containing conjunction and negation. Then {a mathematical formula}INF[Prop(∧,¬)] is the language that decides the probability of a query for networks specified with definition axioms containing conjunction and negation. As every Bayesian network over binary variables can be specified with such definition axioms, {a mathematical formula}INF[Prop(∧,¬)] is in fact a {a mathematical formula}PP-complete language [33, Theorems 11.3 and 11.5].
      </paragraph>
      <paragraph>
       There is obvious interest in finding simple languages {a mathematical formula}L such that deciding {a mathematical formula}INF[L] is a tractable problem, so as to facilitate elicitation, decision-making and learning [31], [37], [62], [106], [114]. And there are indeed propositional languages that generate tractable Bayesian networks: for instance, it is well known that Noisy-Or networks display polynomial inference when the query consists of negative assignments [57]. Recall that a Noisy-Or network has a bipartite graph with edges pointing from nodes in one set to nodes in the other set, and the latter nodes are associated with Noisy-Or gates.
      </paragraph>
      <paragraph>
       One might think that tractability can only be attained by imposing some structural conditions on graphs, given results that connect complexity and graph properties [74]. However, it is possible to attain tractability without restrictions on graph topology. Consider the following result, where we use {a mathematical formula}Prop(∧) and {a mathematical formula}Prop(∨) to indicate propositional languages respectively restricted to conjunction and restricted to disjunction:
      </paragraph>
      <paragraph label="Theorem 1">
       {a mathematical formula}INF[Prop(∧)]is in{a mathematical formula}Pwhen the query{a mathematical formula}(Q,E)contains only positive assignments, and{a mathematical formula}INF[Prop(∨)]is in to{a mathematical formula}Pwhen the query contains only negative assignments.
      </paragraph>
      <paragraph>
       As the proof of this result shows (in Appendix A), only polynomial effort is needed to compute probabilities for positive queries in networks specified with {a mathematical formula}Prop(∧), even if one allows root nodes to be negated (that is, the variables that appear in probabilistic assessments can appear negated in the body of definition axioms).
      </paragraph>
      <paragraph>
       Alas, even small movements away from the conditions in Theorem 1 take us to {a mathematical formula}PP-completeness:
      </paragraph>
      <paragraph label="Theorem 2">
       {a mathematical formula}INF[Prop(∧)]and{a mathematical formula}INF[Prop(∨)]are{a mathematical formula}PP-complete with respect to many-one reductions.
      </paragraph>
      <paragraph>
       One might try to concoct additional propositional languages by using logical forms in the literature [34]. We leave this to future work; instead of pursuing various possible sub-Boolean languages, we wish to examine the query complexity of Bayesian networks, and then move to relational languages.
      </paragraph>
      <paragraph>
       A digression on reductions  The proof of Theorem 2 is somewhat long because it uses many-one reductions. In Appendix A we show that much simpler proofs for {a mathematical formula}PP-completeness of {a mathematical formula}INF[Prop(∧)] and {a mathematical formula}INF[Prop(∨)] are possible if one uses Turing reductions. A Turing reduction does give some valuable information: if a problem is {a mathematical formula}PP-complete with Turing reductions, then it is unlikely to be polynomial (for if it were polynomial, then {a mathematical formula}PPP would equal {a mathematical formula}P, a highly unlikely result given current assumptions in complexity theory [125]). However, Turing reductions tend to blur some significant distinctions. For instance, for Turing reductions it does not matter whether Q is a singleton or not: one can ask for {a mathematical formula}P(Q1|E1), {a mathematical formula}P(Q2|E2), and so on, and then obtain {a mathematical formula}P(Q1,Q2,…|E) as the product of the intermediate computations. Hence many-one reductions yield stronger results, so we emphasize them throughout this paper.
      </paragraph>
     </section>
     <section label="3.3">
      <section-title>
       Query complexity
      </section-title>
      <paragraph>
       We have so far considered that the input is a string encoding a Bayesian network specification {a mathematical formula}B, a query {a mathematical formula}(Q,E), and a rational number γ. However in practice one may face a situation where the Bayesian network is fixed, and the input is a string consisting of the pair {a mathematical formula}(Q,E) and a rational number γ; the goal is to determine whether {a mathematical formula}P(Q|E)&gt;γ with respect to the fixed Bayesian network.
      </paragraph>
      <paragraph>
       Denote by {a mathematical formula}QINF[B], where {a mathematical formula}B is a Bayesian network specification, the language consisting of each string {a mathematical formula}(Q,E,γ) for which {a mathematical formula}P(Q|E)&gt;γ with respect to {a mathematical formula}B. And denote by {a mathematical formula}QINF[L] the set of languages {a mathematical formula}QINF[B] where {a mathematical formula}B is a Bayesian network specification with definition axioms whose bodies are formulas in {a mathematical formula}L.
      </paragraph>
      <paragraph label="Definition 1">
       Let {a mathematical formula}L be a propositional language and {a mathematical formula}C be a complexity class. The query complexity of {a mathematical formula}L is in {a mathematical formula}C iff every language in {a mathematical formula}QINF[L] is in {a mathematical formula}C.
      </paragraph>
      <paragraph>
       The fact that query complexity may differ from inferential complexity was initially raised by Darwiche and Provan [31], and has led to a number of techniques emphasizing compilation of a fixed Bayesian network [22], [32]. Indeed the expression “query complexity” seems to have been coined by Darwiche [33, Section 6.9], without the formal definition presented here.
      </paragraph>
      <paragraph>
       The original work by Darwiche and Provan [31] shows how to transform a fixed Bayesian network into a Query-DAG such that {a mathematical formula}P(Q|E)&gt;γ can be decided in linear time. That is:
      </paragraph>
      <paragraph label="Theorem 3">
       Darwiche and Provan [31]{a mathematical formula}QINF[Prop(∧,¬)]is in{a mathematical formula}P.
      </paragraph>
      <paragraph>
       Results on query complexity become more interesting when we move to relational languages, as we do at once.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Relational languages: inferential, query, and domain complexity
     </section-title>
     <paragraph>
      In this section we extend our specification framework so as to deal with relational languages. Such languages have been used in a variety of applications with repetitive entities and relationships [46], [109], as we have alluded to in Section 1. They have roots in general probabilistic logics that mix deterministic knowledge and uncertain reasoning in very flexible, but sometimes too complicated, ways [56]. The more focused interest in extending Bayesian networks with relational constructs has produced an array of practical languages, as we summarize later in Section 7: plates, PRMs, probabilistic logic programs. It is not easy to extract a “common denominator” from these languages. However, with some reflection we see that they all parameterize random variables using relations; they all allow for logical definitions to be mixed with probabilistic assessments; they all resort to the semantics of first-order logic to interpret relations using sets (domains). We capture these common features in Section 4.1; to do so, we use “parvariables” and related techniques introduced by Poole [103], using as much syntax and semantics as possible from first-order logic and in particular from lifted inference techniques [90] (and borrowing some terminology from description logics and logic programming as appropriate). Throughout we resort to the same idea advocated in Section 3: that is, that a model can be specified by a set of probability assessments and a set of logical definitions that belong to a selected logical language.
     </paragraph>
     <paragraph>
      After introducing our modeling framework in Section 4.1, in Section 4.2 we describe the complexity questions we intend to answer, and we offer a few comments on lifted inference and probabilistic databases.
     </paragraph>
     <section label="4.1">
      <section-title>
       Relational Bayesian network specifications
      </section-title>
      <paragraph>
       A parameterized random variable, abbreviated parvariable, is a function that yields, for each combination of its input parameters, a random variable. For instance, parvariable X yields a random variable {a mathematical formula}X(a) for each selected a. In what follows, parvariables and their parameters will correspond to relations and their logical variables.
      </paragraph>
      <paragraph>
       We use a vocabulary consisting of names of relations. Every relation r is associated with a non-negative integer called its arity. A logical variable is referred to as a logvar. A vector of logvars {a mathematical formula}[x1,…,xk] is denoted {a mathematical formula}x→; then {a mathematical formula}r(x→) is an atom. A domain is a set; in this paper every domain is finite. When the logvars in an atom are replaced by elements of the domain, we obtain {a mathematical formula}r(a1,…,ak), a ground atom, often referred to as a grounding of relation X. An interpretation{a mathematical formula}I is a function that assigns to each relation X of arity k a relation on {a mathematical formula}Dk. An interpretation can be viewed as a function that assigns {a mathematical formula}true or {a mathematical formula}false to each grounding {a mathematical formula}r(a→), where {a mathematical formula}a→ is a tuple of elements of the domain. Typically in logical languages there is a distinction between constants and elements of a domain, but we avoid constants altogether in our discussion: as argued by Bacchus, if constants are used within a probabilistic logic, some sort of additional non-trivial rigidity assumption must be used [4]. It is possible that by adding constants to a language we increase its expressivity in ways that cannot be captured with standard probabilities [12], [13]; we leave an analysis of languages with constants to future work.
      </paragraph>
      <paragraph>
       Given a domain {a mathematical formula}D, we can associate with each grounding {a mathematical formula}r(a→) a random variable {a mathematical formula}X(a→) over the set of all possible interpretations, such that {a mathematical formula}X(a→)(I)=1 if interpretation {a mathematical formula}I assigns {a mathematical formula}true to {a mathematical formula}r(a→), and {a mathematical formula}X(a→)(I)=0 otherwise. Similarly, we can associate with a relation r a parvariable X that yields, once a domain is given, a random variable {a mathematical formula}X(a→) for each grounding {a mathematical formula}r(a→). To simplify matters, we use the same symbol for a grounding {a mathematical formula}r(a→) and its associated random variable {a mathematical formula}X(a→), much as we did with propositions and their associated random variables. Similarly, we use the same symbol for a relation r and its associated parvariable X. We can then write down logical formulas over relations/parvariables, and we can assess probabilities for relations/parvariables. The next example clarifies the dual use of symbols for relations/parvariables.
      </paragraph>
      <paragraph label="Example 2">
       Consider a model of friendship built on top of the example in Section 1. Two people are friends if they are both fans of the same band, or if they are linked in some other unmodeled way, and a person is always a friend of herself. Take relations {a mathematical formula}friends, {a mathematical formula}fan, and {a mathematical formula}linked. Given a domain, say {a mathematical formula}D={a,b}, we have the grounding {a mathematical formula}friends(a,b), whose intended interpretation is that a and b are friends; we take friendship to be asymmetric so {a mathematical formula}friends(a,b) may hold while {a mathematical formula}friends(b,a) may not hold. We also have groundings {a mathematical formula}fan(a), {a mathematical formula}linked(b,a), and so on. Each one of these groundings corresponds to a random variable that yields 1 or 0 when the grounding is respectively {a mathematical formula}true or {a mathematical formula}false is an interpretation.The stated facts about friendship might be encoded by a variant of Formula (1):{a mathematical formula}We can draw a directed graph indicating the dependence of {a mathematical formula}friends on the other relations, as in Fig. 3. Suppose we believe 0.2 is the probability that an element of the domain is a fan, and 0.1 is the probability that any two people are linked for some other reason. To express these assessments we might write{a mathematical formula} with implicit outer universal quantification. □
      </paragraph>
      <paragraph>
       Given a formula and a domain, we can produce all groundings of the formula by replacing its logvars by elements of the domain in every possible way (as usual when grounding first-order formulas). We can similarly ground probabilistic assessments by grounding the affected relations.
      </paragraph>
      <paragraph label="Example 3">
       In Example 2, we can produce the following groundings from domain {a mathematical formula}D={a,b} and Formula (3):{a mathematical formula} Similarly, we get:{a mathematical formula} by grounding assessments in Expression (4). □
      </paragraph>
      <paragraph>
       We wish to extend our propositional framework by specifying Bayesian networks using both parameterized probabilistic assessments and first-order definitions. So, suppose we have a finite set of parvariables, each one of them corresponding to a relation in a vocabulary. A relational Bayesian network specification associates, with each parvariable {a mathematical formula}Xi, either
      </paragraph>
      <list>
       <list-item label="•">
        a definition axiom{an inline-figure}, or
       </list-item>
       <list-item label="•">
        a probabilistic assessment{a mathematical formula}P(X(x→)=1)=α,
       </list-item>
      </list>
      <paragraph>
       where
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}ℓi is a well-formed formula in a language {a mathematical formula}L, containing relations {a mathematical formula}Y1,…,Ym and free logvars {a mathematical formula}x→ (and possibly additional logvars bound to quantifiers),
       </list-item>
       <list-item label="•">
        and α is a nonnegative rational in the interval {a mathematical formula}[0,1].
       </list-item>
      </list>
      <paragraph>
       The formula {a mathematical formula}ℓi is the body of the corresponding definition axiom. The parvariables that appear in {a mathematical formula}ℓi are the parents of parvariable {a mathematical formula}Xi, and are denoted by {a mathematical formula}pa(Xi). Clearly the definition axioms induce a directed graph where the nodes are the parvariables and the parents of a parvariable (in the graph) are exactly {a mathematical formula}pa(Xi). This is the parvariable graph of the relational Bayesian network specification (this sort of graph is called a template dependency graph by Koller and Friedman [69, Definition 6.13]). For instance, Fig. 3 depicts the parvariable graph for Example 2.
      </paragraph>
      <paragraph>
       When the parvariable graph of a relational Bayesian network specification is acyclic, we say the specification itself is acyclic. In this paper we assume that relational Bayesian network specifications are acyclic, and we do not even mention this anymore.
      </paragraph>
      <paragraph>
       The grounding of a relational Bayesian network specification {a mathematical formula}S on a domain {a mathematical formula}D is defined as follows. First, produce all groundings of all definition axioms. Then, for each parameterized probabilistic assessment {a mathematical formula}P(X(x→)=1)=α, produce its ground probabilistic assessments{a mathematical formula} for all appropriate tuples {a mathematical formula}aj→ built from the domain. The grounded relations, definitions and assessments specify a propositional Bayesian network that is then the semantics of {a mathematical formula}S with respect to domain {a mathematical formula}D.
      </paragraph>
      <paragraph label="Example 4">
       Consider Example 2. For a domain {a mathematical formula}{a,b}, the relational Bayesian network specification given by Expressions (3) and (4) is grounded into the sentences and assessments in Example 3. By repeating this process for a larger domain {a mathematical formula}{a,b,c}, we obtain a larger Bayesian network whose graph is depicted in Fig. 4. □
      </paragraph>
      <paragraph>
       Note that logical inference might be used to simplify grounded definitions; for instance, in the previous example, one might note that {a mathematical formula}friends(a,a) is simply {a mathematical formula}true. Note also that the grounding of a formula with quantifiers turns, as usual, an existential quantifier into a disjunction, and a universal quantifier into a conjunction.
      </paragraph>
      <paragraph label="Example 5">
       Take the following relational Bayesian network specification (with no particular meaning, just to illustrate a few possibilities):{a mathematical formula} Take a domain {a mathematical formula}D={1,2}; the grounded definition of {a mathematical formula}X5(1) is{a mathematical formula}Fig. 5 depicts the parvariable graph and the grounding of this relational Bayesian network specification. □
      </paragraph>
      <paragraph>
       In order to study complexity questions we must decide how to encode any given domain. Note that there is no need to find special names for the elements of the domain, so we take that the domain is always the set of numbers {a mathematical formula}{1,2,…,N}. Now if this list is explicitly given as input, then the size of the input is of order N. However, if only the number N is given as input, then the size of the input is either of order N when N is encoded in unary notation, or of order {a mathematical formula}log⁡N when N is encoded in binary notation. The distinction between unary and binary notation for input numbers is often used in description logics [3]. We will see later that the encoding of input N has significant impact on complexity.
      </paragraph>
      <paragraph>
       The conceptual difference between unary and binary encodings of domain size can be captured by the following analogy. Suppose we are interested in the inhabitants of a city: the probabilities that they study, that they marry, that they vote, and so on. Suppose the behavior of these inhabitants is modeled by a relational Bayesian network specification, and we observe evidence on a few people. If we then take our input N to be in unary notation, we are implicitly assuming that we have a directory, say a mailing list, with the names of all inhabitants; even if we do not care about their specific names, each one of them exists concretely in our modeled reality. But if we take our input N to be in binary notation, we are just focusing on the impact of city size on probabilities, without any regard for the actual inhabitants; we may say that N is a thousand, a million, or maybe even fifty million (and perhaps none of these numbers are close to actual city size).
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Inferential, combined, query and domain complexity
      </section-title>
      <paragraph>
       To repeat, we are interested in the relationship between the language {a mathematical formula}L that is employed in definition axioms and the complexity of inferences. While in the propositional setting we distinguished between inferential and query complexity, here we have an additional distinction to make. Consider the following definitions, where {a mathematical formula}S is a relational Bayesian network specification, N is the domain size, Q and E are sets of assignments for ground atoms, γ is a rational number in {a mathematical formula}[0,1], and {a mathematical formula}C is a complexity class. Recall that the inequality {a mathematical formula}P(Q|E)&gt;γ” is to be understood as an abbreviation for “{a mathematical formula}P(E)&gt;0 and {a mathematical formula}P(Q|E)&gt;γ”.
      </paragraph>
      <paragraph label="Definition 2">
       Denote by {a mathematical formula}INF[L] the language consisting of strings {a mathematical formula}(S,N,Q,E,γ) for which {a mathematical formula}P(Q|E)&gt;γ with respect to the grounding of {a mathematical formula}S on domain of size N, where {a mathematical formula}S contains definition axioms whose bodies are formulas in {a mathematical formula}L. The inferential complexity of {a mathematical formula}L is in {a mathematical formula}C iff {a mathematical formula}INF[L] is in {a mathematical formula}C; moreover, the inferential complexity is {a mathematical formula}C-hard with respect to a reduction iff {a mathematical formula}INF[L] is {a mathematical formula}C-hard with respect to the reduction, and it is {a mathematical formula}C-complete with respect to a reduction iff it is in {a mathematical formula}C and it is {a mathematical formula}C-hard with respect to the reduction.
      </paragraph>
      <paragraph label="Definition 3">
       Denote by {a mathematical formula}QINF[S] the language consisting of strings {a mathematical formula}(N,Q,E,γ) for which {a mathematical formula}P(Q|E)&gt;γ with respect to the grounding of {a mathematical formula}S on domain of size N. Denote by {a mathematical formula}QINF[L] the set of languages {a mathematical formula}QINF[S] for {a mathematical formula}S whose bodies of definition axioms are formulas in {a mathematical formula}L. The query complexity of {a mathematical formula}L is in {a mathematical formula}C iff every language in {a mathematical formula}QINF[L] is in {a mathematical formula}C; moreover, the query complexity is {a mathematical formula}C-hard with respect to a reduction iff some language in {a mathematical formula}QINF[L] is {a mathematical formula}C-hard with respect to the reduction, and it is {a mathematical formula}C-complete with respect to a reduction iff it is in {a mathematical formula}C and it is {a mathematical formula}C-hard with respect to the reduction.
      </paragraph>
      <paragraph label="Definition 4">
       Denote by {a mathematical formula}DINF[S,Q,E] the language consisting of strings {a mathematical formula}(N,γ) for which {a mathematical formula}P(Q|E)&gt;γ with respect to the grounding of {a mathematical formula}S on domain of size N. Denote by {a mathematical formula}DINF[L] the set of languages {a mathematical formula}DINF[S,Q,E] for {a mathematical formula}S whose bodies of definition axioms are formulas in {a mathematical formula}L, and where Q and E are sets of assignments. The domain complexity of {a mathematical formula}L is in {a mathematical formula}C iff every language in {a mathematical formula}DINF[L] is in {a mathematical formula}C; moreover, the domain complexity is {a mathematical formula}C-hard with respect to a reduction iff some language in {a mathematical formula}DINF[L] is {a mathematical formula}C-hard with respect to the reduction, and it is {a mathematical formula}C-complete with respect to a reduction iff it is in {a mathematical formula}C and it is {a mathematical formula}C-hard with respect to the reduction.
      </paragraph>
      <paragraph>
       We conclude this section with a number of observations.
      </paragraph>
      <paragraph>
       Combined complexity  The definition of inferential complexity imposes no restriction on the vocabulary; later we will impose bounds on relation arity. We might instead assume that the vocabulary is fixed; in this case we might use the term combined complexity, as this is the term used in finite model theory and database theory to refer to the complexity of model checking when both the formula and the model are given as input, but the vocabulary is fixed [78].
      </paragraph>
      <paragraph>
       Lifted inference  We note that query and domain complexities are related respectively to dqe-liftability and domain-liftability, as defined in the study of lifted inference [64], [63].
      </paragraph>
      <paragraph>
       The term “lifted inference” is usually attached to algorithms that try to compute inferences involving parvariables without actually producing groundings [66], [90], [103]. A formal definition of lifted inference has been proposed by Van den Broeck [132]: an algorithm is domain lifted iff inference runs in polynomial-time with respect to N, for fixed model and query. This definition assumes that N is given in unary notation; if N is given in binary notation, the input is of size {a mathematical formula}log⁡N, and a domain lifted algorithm may take exponential time. Domain liftability has been extended to dqe-liftability, where the inference must run in polynomial-time with respect to N and the query, for fixed model [64].
      </paragraph>
      <paragraph>
       In short, dqe-liftability means that query complexity is polynomial, while domain-liftability means that domain complexity is polynomial. Deep results have been obtained both on the limits of liftability [64], [63], and on algorithms that attain liftability [8], [65], [94], [122], [134]. We will use several of these results in our later proofs; in particular, the inferential/query/domain complexity of relational Bayesian network specifications based on function-free first-order logic with equality, and its bounded variable fragments, can be extracted from previous results.
      </paragraph>
      <paragraph>
       We feel that dqe-liftability and domain-liftability are important concepts but they focus only on a binary choice (polynomial versus non-polynomial); our goal here is to map languages and complexities in more detail. As we have mentioned in Section 1, our main goal is to grasp the complexity, however high, of language features.
      </paragraph>
      <paragraph>
       Probabilistic databases  Highly relevant material has been produced in the study of probabilistic databases; that is, databases where data may be associated with probabilities [28], [68], [118], [137], [139]. As an example of probabilistic database, the Trio system lets the user indicate that {a mathematical formula}Amy drives an {a mathematical formula}Acura with probability 0.8 [9]. As another example, the NELL system scans text from the web and builds a database of facts, each associated with a number between zero and one [91].
      </paragraph>
      <paragraph>
       Here we just summarize the framework described by Suciu et al. [121]. Thus consider a set of relations, each implemented as a table. Each tuple in a table may be associated with a probability, and these probabilistic tuples are assumed independent (as dependent tuples can be modeled from independent ones [121, Section 2.7.1]). A probabilistic database management system receives a logical formula {a mathematical formula}ϕ(x→) and must determine, using data and probabilities in the tables, the probability {a mathematical formula}P(ϕ(a→)) for tuple {a mathematical formula}a→. The logical formula {a mathematical formula}ϕ(x→) is referred to as the query; for example, ϕ may be a Union of Conjunctive Queries (a first-order formula with equality, conjunction, disjunction and existential quantification). Note that the word “query” is not used with the meaning usually adopted in the context of Bayesian networks; in probabilistic databases, a query is a formula whose probability is to be computed.
      </paragraph>
      <paragraph>
       Suppose that all tuples in the table for relation {a mathematical formula}X(x→) receive identical probability value α. This table can be viewed as the grounding of a parvariable {a mathematical formula}X(x→) that is associated with a single assessment {a mathematical formula}P(X(x→)=1)=α. Beame et al. say that the probabilistic database is symmetric iff each table can be associated with a parvariable and a single probabilistic assessment [8]. It should be clear that a symmetric probabilistic database and a query expressed as a logical formula ϕ map directly to our relational Bayesian network specification; hence results on both topics can be transferred with little effort. This is pleasant because several deep results have been derived on probabilistic databases; as we discuss later, the inferential and query complexity of first-order logic and of some bounded variable fragments have been derived, together with deep results on domain complexity for those logical languages. Research on probabilistic databases has also identified classes of queries (referred to as safe queries) that are associated with dichotomy theorems: that is, either a query belongs to such a class and is tractable, or it is not tractable. We return to these results at the end of Section 5.
      </paragraph>
      <paragraph>
       A distinguishing characteristic of research on probabilistic databases is the intricate search for languages that lead to tractable inferences. As we have already indicated in our previous discussion of lifted inference, our main goal here is to understand the connection between features of a knowledge representation formalism and the complexity of conditional probability calculations. We are not so focused on finding tractable cases that can take upon large volumes of data, even though we are obviously looking for them; indeed we later present tractability results for the {a mathematical formula}DLLitenf language, results that we take to be are one of the main contributions of this paper.
      </paragraph>
      <paragraph>
       Query or data complexity?  The definition of query complexity (Definition 3) reminds one of data complexity as adopted in finite model theory and in database theory [78]. It is thus not surprising that research on probabilistic databases has used the term “data complexity” to mean the complexity when the database is the only input, leaving “query” to its meaning in database theory [121].
      </paragraph>
      <paragraph>
       In the context of Bayesian networks, usually a “query” is a pair {a mathematical formula}(Q,E) of assignments. If Q and E contain all assignments that are present in the input, then there is no real difference between “query” and “data”. Indeed, we might have adopted the term “data complexity” throughout this paper as we only discuss queries that contain all available data.{sup:2}
      </paragraph>
      <paragraph>
       However we feel that there are situations where the “query” is not equal to the “data”. For instance, in probabilistic relational models one often uses auxiliary grounded relations to indicate which groundings are parents of a given grounding (we return to this in Section 7). And in probabilistic logic programming one can use probabilistic facts to associate probabilities with specific groundings [41], [101], [116]. In these cases one deals with a “query” {a mathematical formula}(Q,E) and separate “data” that regulate parts of the grounded Bayesian network.
      </paragraph>
      <paragraph>
       Another possible distinction between “query” and “data” complexities can be found in probabilistic databases. Suppose we have a relational Bayesian network specification, and a formula ϕ whose probability {a mathematical formula}P(ϕ) is to be computed, and data given by tables where each cell is associated with a probability. One might then either fix the specification and vary the formula and the data (we might call this the “query” complexity), or fix the specification and the formula ϕ and vary only the data (this would clearly be the “data” complexity). This sort of distinction actually parallels distinctions found in description logics [19].
      </paragraph>
      <paragraph>
       It is possible that differences between “query” and “data” are not found to be of practical value in future work. For now we prefer to keep open the possibility of a fine-grained analysis of complexity, so we use the term “query complexity” even though our queries are simply sets of assignments containing all available data.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      The complexity of relational Bayesian network specifications
     </section-title>
     <paragraph label="Theorem 4">
      We start with function-free first-order logic with equality, a language we denote by {a mathematical formula}FFFO. One might guess that such a powerful language leads to exponentially hard inference problems. Indeed: {a mathematical formula}INF[FFFO]is{a mathematical formula}PEXP-complete with respect to many-one reductions, regardless of whether the domain is specified in unary or binary notation.
     </paragraph>
     <paragraph>
      We note that Grove, Halpern and Koller have already argued that counting the number of suitably defined distinct interpretations of monadic first-order logic is hard for the class of languages decided by exponential-time counting Turing machines [54, Theorem 4.14]. As they do not present a proof of their counting result (and no similar proof seems to be available in the literature), and as we need some of the reasoning to address query complexity later, we present a detailed proof of Theorem 4 in Appendix A.
     </paragraph>
     <paragraph>
      We emphasize that when the domain is specified in binary notation the proof of Theorem 4 only requires relations of arity one. One might hope to find lower complexity classes for fragments of {a mathematical formula}FFFO that go beyond monadic logic but restrict quantification. For instance, in description logics one often restricts relations to have arity two, and existential quantification to follow the pattern {a mathematical formula}∃y:X(x,y)∧Y(y), in many cases obtaining interesting complexity classes [3]. Such a restriction on quantification will not do in our setting. To understand why, note that if {a mathematical formula}P(X(x,y)=1)=1, then{a mathematical formula} with probability one. Similarly, we can impose {a mathematical formula}∃y:X(x,y) with probability one. Now these quantification patterns, together with negation, are sufficient to build the entire proof of Theorem 4. Thus {a mathematical formula}PEXP-completeness is reached again.
     </paragraph>
     <paragraph>
      We might try an even more stringent restriction. So, consider the following language, inspired by the popular {a mathematical formula}ALC description logic (to be discussed in more detail in Section 6). Here we have restricted quantification and added the restriction that only two logvars can be used and reused.
     </paragraph>
     <paragraph label="Definition 5">
      The language {a mathematical formula}ALC consists of all formulas recursively defined so that {a mathematical formula}X(x) and {a mathematical formula}X(y) are formulas where X is a unary relation, ¬ϕ is a formula when ϕ is a formula, {a mathematical formula}ϕ∧φ is a formula when both ϕ and φ are formulas, and {a mathematical formula}∃y:X(x,y)∧Y(y) and {a mathematical formula}∃x:X(y,x)∧Y(x) are formulas when X is a binary relation and Y is a unary relation, with the restriction that only {a mathematical formula}x and {a mathematical formula}y can appear as logvars in formulas.
     </paragraph>
     <paragraph>
      Because logical inference with {a mathematical formula}ALC is a {a mathematical formula}PSPACE-complete problem [3], one might hope that probabilistic inference might be solved within polynomial space. Alas, {a mathematical formula}ALC does not move us below {a mathematical formula}PEXP when domain size is given in binary notation (later we show that unary notation leads to better behavior):
     </paragraph>
     <paragraph label="Theorem 5">
      {a mathematical formula}INF[ALC]is{a mathematical formula}PEXP-complete with respect to many-one reductions when domain size is given in binary notation.
     </paragraph>
     <paragraph>
      Now returning to full {a mathematical formula}FFFO, consider its query complexity. We divide the analysis in two parts, as the related proofs are quite different{sup:3}:
     </paragraph>
     <paragraph label="Theorem 6">
      {a mathematical formula}QINF[FFFO]is{a mathematical formula}PEXP-complete with respect to many-one reductions when domain size is specified in binary notation.
     </paragraph>
     <paragraph label="Theorem 7">
      {a mathematical formula}QINF[FFFO]is{a mathematical formula}PP-complete with respect to many-one reductions when domain size is specified in unary notation.
     </paragraph>
     <paragraph>
      As far as domain complexity is concerned, it seems very hard to establish a completeness result for {a mathematical formula}FFFO when domain size is given in binary notation.{sup:4} We simply rephrase an ingenious argument by Jaeger [63] to establish:
     </paragraph>
     <paragraph label="Theorem 8">
      Suppose{a mathematical formula}NETIME≠ETIME. Then{a mathematical formula}DINF[FFFO]is not solved in deterministic exponential time when domain size is given in binary notation.
     </paragraph>
     <paragraph>
      And for domain size in unary notation:
     </paragraph>
     <paragraph label="Theorem 9">
      {a mathematical formula}DINF[FFFO]is{a mathematical formula}PP1-complete with respect to many-one reductions when domain size is given in unary notation.
     </paragraph>
     <paragraph>
      Theorem 9 is in essence implied by arguments in a major result by Beame et al. [8, Lemma 3.9]: they show that counting the number of interpretations for formulas in the three-variable fragment {a mathematical formula}FFFO3 is {a mathematical formula}#P1-complete. The fragment {a mathematical formula}FFFOk consists of the formulas in {a mathematical formula}FFFO that employ at most k logvars; note that logvar symbols may be reused within a formula, but there is a bounded supply of such symbols [78, Chapter 111]. For instance, {a mathematical formula}ALC belongs to {a mathematical formula}FFFO2. The proof by Beame et al. is rather involved as they restrict themselves to three logvars; in Appendix A we show that Theorem 9 admits a much simpler proof, a small contribution that may be useful to researchers.
     </paragraph>
     <paragraph>
      It is apparent from Theorem 4, Theorem 5, Theorem 6, Theorem 8 that we are bound to obtain exponential complexity when domain size is given in binary notation. Hence, from now on we work with domain sizes in unary notation, unless explicitly indicated.
     </paragraph>
     <paragraph>
      Of course, a domain size in unary notation cannot by itself avoid exponential behavior (consider for instance Theorem 4). In particular, an exponentially large number of groundings can be generated by increasing arity: even a domain with two individuals leads to {a mathematical formula}2k groundings for a relation with arity k. Hence, we often assume that our relations have bounded arity. We might instead assume that the vocabulary is fixed, as done in finite model theory when studying combined complexity. We prefer the more general strategy where we bound arity; clearly a fixed vocabulary implies a fixed maximum arity.
     </paragraph>
     <paragraph>
      With such additional assumptions, we obtain {a mathematical formula}PSPACE-completeness of inferential complexity:
     </paragraph>
     <paragraph label="Theorem 10">
      {a mathematical formula}INF[FFFO]is{a mathematical formula}PSPACE-complete with respect to many-one reductions when relations have bounded arity and domain size is given in unary notation.
     </paragraph>
     <paragraph>
      With a few differences, this result is implied by results by Beame et al. in their important paper [8, Theorem 4.1]: they show that counting interpretations with a fixed vocabulary is {a mathematical formula}PSPACE-complete (that is, they focus on combined complexity and avoid conditioning assignments). We present a short proof of Theorem 10 within our framework in Appendix A.
     </paragraph>
     <paragraph>
      Note that the proof of Theorem 7 is already restricted to arity 2, hence {a mathematical formula}QINF[FFFO] is {a mathematical formula}PP-complete with respect to many-one reductions when relations have bounded arity (larger than one) and the domain is given in unary notation.
     </paragraph>
     <paragraph>
      We now turn to {a mathematical formula}FFFOk. As we have already noted, this sort of language has been studied already, again by Beame et al., who have derived their domain and combined complexity [8]. In Appendix A we present a short proof of the next result, to emphasize that it follows by a simple adaptation of the proof of Theorem 7:
     </paragraph>
     <paragraph label="Theorem 11">
      {a mathematical formula}INF[FFFOk]is{a mathematical formula}PP-complete with respect to many-one reductions for all{a mathematical formula}k≥0when domain size is given in unary notation.
     </paragraph>
     <paragraph>
      Query complexity also follows directly from arguments in the proofs of previous results, as is clear from the proof of the next theorem in Appendix A{sup:5}:
     </paragraph>
     <paragraph label="Theorem 12">
      {a mathematical formula}QINF[FFFOk]is{a mathematical formula}PP-complete with respect to many-one reductions for all{a mathematical formula}k≥2when domain size is given in unary notation.
     </paragraph>
     <paragraph>
      Now consider domain complexity for the bounded variable fragment; previous results in the literature establish this complexity [8], [132], [134]. In fact, the case {a mathematical formula}k&gt;2 is based on a result by Beame et al. that we have already alluded to; in Appendix A we present a simplified argument for this result.
     </paragraph>
     <paragraph label="Theorem 13">
      {a mathematical formula}DINF[FFFOk]is{a mathematical formula}PP1-complete with respect to many-one reductions for{a mathematical formula}k&gt;2and polynomial for{a mathematical formula}k≤2when domain size is given in unary notation.
     </paragraph>
     <paragraph>
      There are important knowledge representation formalisms within bounded-variable fragments of {a mathematical formula}FFFO. For instance, many description logics belong to {a mathematical formula}FFFO2[3]; one example is the logic {a mathematical formula}ALC that we have discussed before. In the next section we examine description logics in more detail.
     </paragraph>
     <paragraph>
      As a different exercise, we now consider the quantifier-free fragment of {a mathematical formula}FFFO. In such a language, every logvar in the body of a definition axiom must appear in the defined relation, as no logvar is bound to any quantifier. Denote this language by {a mathematical formula}QF; in Section 7 we show the close connection between {a mathematical formula}QF and plate models. We have:
     </paragraph>
     <paragraph label="Theorem 14">
      Suppose relations have bounded arity.{a mathematical formula}INF[QF]and{a mathematical formula}QINF[QF]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[QF]requires constant computational effort. These results hold even if domain size is given in binary notation.
     </paragraph>
     <paragraph>
      As we have discussed at the end of Section 4, the literature on lifted inference and on probabilistic databases has produced deep results on query and domain complexity. One example is the definition of safe queries, a large class of formulas with tractable query complexity [29]; similar classes of formulas have been studied for symmetric probabilistic databases [53]. Note that even though a formula can be decided in polynomial time to be safe or not, computational support is needed for such a decision. As we have here focused on languages whose complexity can be determined directly from their syntactic features, we leave to future work the study of relational Bayesian network specifications that are based on safe queries and related languages.
     </paragraph>
    </section>
    <section label="6">
     <section-title>
      Specifications based on description logics
     </section-title>
     <paragraph>
      The term “description logic” encompasses a large family of formal languages that can encode terminologies and assertions about individuals. Those languages are now fundamental knowledge representation tools, as they have solid semantics and computational guarantees concerning reasoning tasks [3]. Given the favorable properties of description logics, much effort has been spent in mixing them with probabilities [81].
     </paragraph>
     <paragraph>
      Relational Bayesian network specifications based on description logics can benefit from well tested tools and results, and offer a natural path to encode probabilistic ontologies. We have already examined the description logic {a mathematical formula}ALC in the previous section, and we continue that study here.
     </paragraph>
     <paragraph>
      Typically a description logic deals with individuals, concepts, and roles. An individual like {a mathematical formula}John corresponds to a constant in first-order logic; a concept like {a mathematical formula}researcher corresponds to a unary relation in first-order logic; and a role like {a mathematical formula}buysFrom corresponds to a binary relation in first-order logic. A vocabulary contains a set of individuals plus some primitive concepts and some primitive roles. From these primitive concepts and roles one can define other concepts and roles using a set of operators. For instance, one may allow for concept intersection: then {a mathematical formula}C⊓D denotes the intersection of concepts C and D. Likewise, {a mathematical formula}C⊔D denotes the union of C and D, and ¬C denotes the complement of C. For a role r and a concept C, a common construct is {a mathematical formula}∀r.C, called a value restriction. Another common construct is {a mathematical formula}∃r.C, an existential restriction. Description logics often define additional constructs such as intersection/union/complement of roles, composition of roles, and inverse of roles. For instance, usually {a mathematical formula}r− denotes the inverse of role r.
     </paragraph>
     <paragraph>
      The semantics of description logics typically resorts to domains and interpretations. A domain{a mathematical formula}D is a set. An interpretation{a mathematical formula}I maps each individual to an element of the domain, each primitive concept to a subset of the domain, and each role to a set of pairs of elements of the domain. And then the semantics of {a mathematical formula}C⊓D is fixed by {a mathematical formula}I(C⊓D)=I(C)∩I(D). Similarly, {a mathematical formula}I(C⊔D)=I(C)∪I(D) and {a mathematical formula}I(¬C)=D\I(C). And for the restricted quantifiers, we have {a mathematical formula}I(∀r.C)={x∈D:∀y:(x,y)∈I(r)→y∈I(C)} and {a mathematical formula}I(∃r.C)={x∈D:∃y:(x,y)∈I(r)∧y∈I(C)}. The semantics of the inverse role {a mathematical formula}r− is, unsurprisingly, given by {a mathematical formula}I(r−)={(x,y)∈D×D:(y,x)∈I(r)}.
     </paragraph>
     <paragraph>
      We can translate this syntax and semantics to their counterparts in first-order logic. Thus {a mathematical formula}C⊓D can be read as {a mathematical formula}C(x)∧D(x), {a mathematical formula}C⊔D as {a mathematical formula}C(x)∨D(x), and ¬C as {a mathematical formula}¬C(x). Moreover, {a mathematical formula}∀r.C translates to {a mathematical formula}∀y:r(x,y)→C(y) and {a mathematical formula}∃r.C translates to {a mathematical formula}∃y:r(x,y)∧C(y).
     </paragraph>
     <paragraph>
      The description logic {a mathematical formula}ALC adopts intersection, complement, and existential restriction (union and value restrictions are obtained from those constructs). Definition 5 introduced the language {a mathematical formula}ALC that corresponds to {a mathematical formula}ALC, and Theorem 5 examined its inferential complexity when domain size is given in binary notation. For domain size in unary notation, we have:
     </paragraph>
     <paragraph label="Theorem 15">
      Suppose the domain size is specified in unary notation. Then{a mathematical formula}INF[ALC]and{a mathematical formula}QINF[ALC]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[ALC]is in{a mathematical formula}P.
     </paragraph>
     <paragraph>
      Of course, we might go much further than {a mathematical formula}ALC in expressivity and still be within the two-variable fragment of {a mathematical formula}FFFO; for instance, we might allow for Boolean operations on roles, role composition and role inverses.
     </paragraph>
     <paragraph>
      Conversely, we can also contemplate description logics that are less expressive than {a mathematical formula}ALC in an attempt to obtain tractability. Indeed, some description logics combine selected Boolean operators with restricted quantification to obtain polynomial complexity of logical inferences. Two notable such description logics are {a mathematical formula}EL and DL-Lite.{sup:6}
     </paragraph>
     <section label="6.1">
      Specifications based on the description logic {a mathematical formula}EL
      <paragraph>
       Consider the description logic {a mathematical formula}EL, where the only allowed operators are intersection and existential restrictions [2]. Define:
      </paragraph>
      <paragraph label="Definition 6">
       The language {a mathematical formula}EL consists of all formulas recursively defined so that {a mathematical formula}X(x) and {a mathematical formula}X(y) are formulas when X is a unary relation, {a mathematical formula}ϕ∧φ is a formula when both ϕ and φ are formulas, and {a mathematical formula}∃y:X(x,y)∧Y(y) and {a mathematical formula}∃x:X(y,x)∧Y(x) are formulas when X is a binary relation and Y is a unary relation, with the restriction that only the symbols {a mathematical formula}x and {a mathematical formula}y can appear as logvars in formulas.
      </paragraph>
      <paragraph>
       That is, {a mathematical formula}EL is the negation-free fragment of {a mathematical formula}ALC. We note that {a mathematical formula}EL also includes the top concept ⊤ that is interpreted as the whole domain; in our setting we essentially have it by introducing {a mathematical formula}P(⊤=1)=1.
      </paragraph>
      <paragraph>
       Because {a mathematical formula}EL contains conjunction, we easily have that {a mathematical formula}INF[EL] is {a mathematical formula}PP-hard by Theorem 2. And domain complexity is polynomial as implied by {a mathematical formula}DINF[ALC]. Query complexity requires significant additional work as discussed in the proof of the next theorem:
      </paragraph>
      <paragraph label="Theorem 16">
       Suppose the domain size is specified in unary notation. Then{a mathematical formula}INF[EL]and{a mathematical formula}QINF[EL]are{a mathematical formula}PP-complete with respect to many-one reductions, even if the query contains only positive assignments, and{a mathematical formula}DINF[EL]is in{a mathematical formula}P.
      </paragraph>
     </section>
     <section label="6.2">
      <section-title>
       Specifications based on the description logic DL-Lite
      </section-title>
      <paragraph>
       The description logic DL-Lite is particularly interesting because it captures central features of ER or UML diagrams, and yet common inference services have polynomial complexity [1], [18].
      </paragraph>
      <paragraph>
       The simplicity and computational efficiency of the DL-Lite language have led many researchers to mix them with probabilities. For instance, D'Amato et al. [30] propose a variant of DL-Lite where the interpretation of each sentence is conditional on a context that is specified by a Bayesian network. A similar approach was taken by Ceylan and Peñalosa [21], with minor semantic differences. A different approach is to extend the syntax of DL-Lite sentences with probabilistic subsumption connectives, as in the Probabilistic DL-Lite[111]. Differently from our focus here, none of those proposals employ DL-Lite to specify Bayesian networks.
      </paragraph>
      <paragraph>
       In DL-Lite one has primitive concepts as before, and also basic concepts: a basic concept is either a primitive concept, or ∃r for a role r, or {a mathematical formula}∃r− for a role r. Again, {a mathematical formula}r− denotes the inverse of r. And then a concept in DL-Lite is either a basic concept, or ¬C when C is a basic concept, or {a mathematical formula}C⊓D when C and D are concepts. The semantics of {a mathematical formula}r−, ¬C and {a mathematical formula}C⊓D are as before, and the semantics of ∃r is, unsurprisingly, given by {a mathematical formula}I(∃r)={x∈D:∃y:(x,y)∈I(r)}.
      </paragraph>
      <paragraph>
       We focus on the negation-free fragment of DL-Lite; that is, we consider:
      </paragraph>
      <paragraph label="Definition 7">
       The language {a mathematical formula}DLLitenf consists of all formulas recursively defined so that {a mathematical formula}X(x) and {a mathematical formula}X(y) are formulas when X is a unary relation, {a mathematical formula}ϕ∧φ is a formula when both ϕ and φ are formulas, and {a mathematical formula}∃y:X(x,y), {a mathematical formula}∃y:X(y,x), {a mathematical formula}∃x:X(y,x), and {a mathematical formula}∃x:X(x,y) are formulas when X is a binary relation, with the restriction that only {a mathematical formula}x and {a mathematical formula}y can appear as logvars in formulas.
      </paragraph>
      <paragraph label="Example 6">
       Consider a short relational specification:{a mathematical formula} For domain {a mathematical formula}D={1,2}, this relational Bayesian network specification is grounded into the Bayesian network in Fig. 6. □
      </paragraph>
      <paragraph>
       Note that {a mathematical formula}INF[DLLitenf] is {a mathematical formula}PP-hard by Theorem 2; moreover, as {a mathematical formula}DLLitenf belongs to {a mathematical formula}FFFO2, {a mathematical formula}INF[DLLitenf] is in {a mathematical formula}PP by Theorem 11 and {a mathematical formula}DINF[DLLitenf] is in {a mathematical formula}P[132], [134].{sup:7} More interestingly, we obtain tractability if we restrict queries to positive assignments:
      </paragraph>
      <paragraph label="Theorem 17">
       Suppose the domain size is specified in unary notation. If in addition the query{a mathematical formula}(Q,E)contains only positive assignments, then{a mathematical formula}INF[DLLitenf],{a mathematical formula}QINF[DLLitenf], and{a mathematical formula}DINF[DLLitenf]are in{a mathematical formula}P.
      </paragraph>
      <paragraph>
       In proving this result (in Appendix A) we show that an inference with a positive query can be reduced to a particular weighted edge cover counting problem of independent interest. And using these edge cover counting techniques we can also show that a related problem, namely finding the most probable explanation, is polynomial for relational Bayesian network specifications based on {a mathematical formula}DLLitenf. To state this result precisely, consider a relational Bayesian network specification {a mathematical formula}S based on {a mathematical formula}DLLitenf, a set of assignments E for ground atoms, and a domain size N. Denote by X the set of random variables that correspond to groundings of relations in {a mathematical formula}S. Now there is at least one set of assignments M such that: (i) M contains assignments to all random variables in X that are not mentioned in E; and (ii) {a mathematical formula}P(M,E) is maximum over all such sets of assignments. Denote by {a mathematical formula}MLE(S,E,N) the problem that consists of finding such a set of assignments M.
      </paragraph>
      <paragraph label="Theorem 18">
       Given a relational Bayesian network{a mathematical formula}Sbased on{a mathematical formula}DLLitenf, a set of positive assignments to grounded relationsE, and a domain size N in unary notation,{a mathematical formula}MLE(S,E,N)can be solved in polynomial time.
      </paragraph>
      <paragraph>
       These results on {a mathematical formula}DLLitenf can be directly extended in some important ways. For example, suppose we allow negative groundings of binary relations in the query. Then most of the proof of Theorem 17 follows through, but we must resort to approximations for weighted edge cover counting [80] so as to develop a fully polynomial-time approximation scheme (FPTAS) for inference. Moreover, the {a mathematical formula}MLE(S,E,N) problem remains polynomial. Similarly, we could allow for different groundings of the same relation to be associated with different probabilities; the proofs given in Appendix A can be modified to develop a FPTAS for inference.
      </paragraph>
      <paragraph>
       We have so far presented results for a number of languages. Table 1 (both table and caption) summarizes most of our findings; as noted previously, several results on {a mathematical formula}FFFO with bounded relation arity and on {a mathematical formula}FFFOk have been in essence derived by Beame et al. [8].
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Plates, probabilistic relational models, and related specification languages
     </section-title>
     <paragraph>
      In this paper we have followed a strategy that has long been cherished in the study of formal languages; that is, we have focused on languages that are based on minimal sets of constructs borrowed from logic. Clearly this plan succeeds only to the extent that results can be transfered to practical specification languages. In this section we examine some important cases where our strategy pays off.
     </paragraph>
     <paragraph>
      Consider, for instance, plate models, a rather popular specification formalism. Plate models have been extensively used in statistical practice [82] since they were introduced in the BUGS project [47], [83]. In machine learning, they have been often used since their first appearance [16].
     </paragraph>
     <paragraph>
      There seems to be no standard formalization for plate models, so we adapt some of our previous concepts as needed. A plate model consists of a set of parvariables, a directed acyclic graph where each node is a parvariable, and a set of template conditional probability distributions. Parvariables are typed: each parameter of a parvariable is associated with a set, the domain of the parameter. All parvariables that share a domain are said to belong to a plate. The central constraint on “standard” plate models is that the domains that appear in the parents of a parvariable must appear in the parvariable. For a given parvariable X, its corresponding template conditional probability distribution associates a probability value to each value of X given each configuration of parents of X.
     </paragraph>
     <paragraph>
      Even though plate models often specify discrete and continuous random variables [82], [120], here we focus on parvariables with values 0 and 1, that can thus be put to correspondence with relations. Dealing with binary random variables is enough to prove {a mathematical formula}PP-hardness in Theorem 19; the proof of this theorem shows that membership to {a mathematical formula}PP is obtained even if parvariables have a bounded number of values that is larger than two.
     </paragraph>
     <paragraph>
      We can use the same semantics as before to interpret plate models, with a small change: now the groundings of a relation are produced by running only over the domains of its associated logvars.
     </paragraph>
     <paragraph label="Example 7">
      Suppose we are interested in a “University World” containing a population of students and a population of courses [44]. Parvariable {a mathematical formula}Failed?(x,y) yields the final status of student {a mathematical formula}y in course {a mathematical formula}x; {a mathematical formula}Difficult?(x) is a parvariable indicating the difficulty of a course {a mathematical formula}x, and {a mathematical formula}Committed?(y) is a parvariable indicating the commitment of student {a mathematical formula}y.A plate model is drawn in Fig. 7, where plates are rectangles. Each parvariable is associated with a template conditional probability distribution:{a mathematical formula}{a mathematical formula}
     </paragraph>
     <paragraph>
      Note that plate models can always be specified using definition axioms in the quantifier-free fragment of {a mathematical formula}FFFO, given that the logvars of a relation appear in its parent relations. For instance, the table in Example 7 can be encoded as follows:{a mathematical formula} where we introduced four auxiliary parvariables with associated assessments {a mathematical formula}P(A1(x,y)=1)=0.6, {a mathematical formula}P(A2(x,y)=1)=0.2, {a mathematical formula}P(A3(x,y)=1)=0.9, and {a mathematical formula}P(A4(x,y)=1)=0.5.
     </paragraph>
     <paragraph>
      Denote by {a mathematical formula}INF[PLATE] the language consisting of inference problems as in Definition 2, where relational Bayesian network specifications are restricted to satisfy the constraints of plate models. Adopt {a mathematical formula}QINF[PLATE] and {a mathematical formula}DINF[PLATE] similarly. We can reuse arguments in the proof of Theorem 14 to show that:
     </paragraph>
     <paragraph label="Theorem 19">
      {a mathematical formula}INF[PLATE]and{a mathematical formula}QINF[PLATE]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[PLATE]requires constant computational effort. These results hold even if the domain size is given in binary notation.
     </paragraph>
     <paragraph>
      One can find extended versions of plate models in the literature, where a node can have children in other plates. See for instance the smoothed Latent Dirichlet Allocation (sLDA) model [10] depicted in Fig. 8: here {a mathematical formula}ϕ(z) has a child {a mathematical formula}W(x,y). In such extended plates a template conditional probability distribution can refer to logvars from plates that are not enclosing the parvariable. If definition axioms are used to specify such template distributions, one gets as before {a mathematical formula}INF[FFFO], {a mathematical formula}QINF[FFFO], etc; that is, results obtained in previous sections apply.
     </paragraph>
     <paragraph>
      Besides plates, several other languages can encode repetitive Bayesian networks. Early proposals resorted to object orientation [71], [84], to frames [72], and to rule-based statements [5], [49], [55], all inspired by knowledge-based model construction [50], [59], [138]. Some of these proposals coalesced into a family of models loosely grouped under the name of Probabilistic Relational Models (PRMs) [43]. We here adopt the definition of PRMs by Getoor et al. [44]; again, to simplify matters, we focus on parvariables that correspond to relations.
     </paragraph>
     <paragraph>
      Similarly to a plate model, a PRM contains typed parvariables and domains. A domain is now called a class; each class appears as a box containing parvariables. For instance, Fig. 9 depicts a PRM for the University World: edges between parvariables indicate probabilistic dependence, and dashed edges between classes indicate associations between elements of the classes. In Fig. 9 we have classes Course, Student, and Registration, with associations between them. Consider association studentOf: the idea is that {a mathematical formula}studentOf(x,z) holds when {a mathematical formula}x is the student in registration {a mathematical formula}z. Following terminology by Koller and Friedman [69], we say that relations that encode classes and associations, such as Course and studentOf, are guard parvariables.
     </paragraph>
     <paragraph>
      A relational skeleton for a PRM is an explicit specification of elements in each class, plus the explicit specification of pairs of objects that are associated. That is, the relational skeleton specifies the groundings of the guard parvariables.
     </paragraph>
     <paragraph>
      Each parvariable X in a PRM is then associated with a template probability distribution that specifies probabilities for the parvariable X given a selected set of other parvariables. The latter are the parents of X, again denoted by {a mathematical formula}pa(X). In the University World of Fig. 9, we must associate with {a mathematical formula}Failed? the template probabilities for {a mathematical formula}P(Failed?(z)|Difficult?(x),Committed?(y)). But differently from plate models, here the parents of a particular grounding are determined by going through associations: for instance, to find the parents of {a mathematical formula}Failed?(r), we must find the course c and the student s such that {a mathematical formula}courseOf(c,r) and {a mathematical formula}studentOf(s,r) hold, and then we have parents {a mathematical formula}Difficult?(c) and {a mathematical formula}Committed?(s).
     </paragraph>
     <paragraph>
      All of these types and associations can, of course, be encoded using first-order logic, as long as all parvariables correspond to relations. For instance, here is a definition axiom that captures the PRM for the University World:{a mathematical formula} using the same auxiliary parvariables employed in Expression (6). The parvariable graph for the resulting specification is depicted in Fig. 10.
     </paragraph>
     <paragraph>
      Thus we can take a PRM and translate it into a relational Bayesian network specification {a mathematical formula}S. As long as the parvariable graph is acyclic, results in the previous sections apply. To see this, note that a skeleton is simply an assignment for all groundings of the guard parvariables. Thus a skeleton can be encoded into a set of assignments S, and our inferences should focus on deciding {a mathematical formula}P(Q|E,S)&gt;γ with respect to {a mathematical formula}S and a domain that is the union of all classes of the PRM.
     </paragraph>
     <paragraph>
      Now suppose we have a fixed PRM and we receive as input a skeleton and a query {a mathematical formula}(Q,E), and we wish to decide whether {a mathematical formula}P(Q|E,S)&gt;γ: if the template probability distributions are specified with {a mathematical formula}FFFO, and the parvariable graph is acyclic, we have that query complexity is {a mathematical formula}PP-complete. We can replay our previous results on inferential and query complexity this way. The concept of domain complexity seems less meaningful when PRMs are considered: the larger the domain, the more data on guard parvariables are needed, so we cannot really fix the domain in isolation.
     </paragraph>
     <paragraph>
      We conclude this section with a few observations.
     </paragraph>
     <paragraph>
      Cyclic parvariable graphs  Our results assume acyclicity of parvariable graphs, but this is not a condition that is typically imposed on PRMs. A cyclic parvariable graph may still produce an acyclic grounding, depending on the given skeleton. For instance, one might want to model blood-type inheritance, where a Person inherits a genetic predisposition from another Person. This creates a loop around the class Person, even though we do not expect a cycle in any valid grounding of the PRM. The literature has proposed languages that allow cycles [44], [58]; one example is shown in Fig. 11. The challenge then is to guarantee that a given skeleton will lead to an acyclic grounded Bayesian network; future work on cyclic parvariable graphs must deal with such a consistency problem [35].
     </paragraph>
     <paragraph>
      Other specification languages  There are several other languages that specify PRMs and related formalisms; such languages can be subjected to the same analysis we have explored in this paper. A notable formalism is the Probabilistic Relational Language (PRL) [45]. In PRL a logic program is used to specify a PRM; the specification is divided into logical background (that is, guard parvariables), probabilistic background, and probabilistic dependencies. Two other examples of textual formalisms that can be used to encode PRMs are Logical Bayesian Networks (LBNs) [40], [39] and Bayesian Logic Programs (BLPs) [67], [110]. Both distinguish between logical predicates that constrain the grounding into graphs (that is, guard parvariables), and probabilistic or Bayesian predicates that encode probabilistic assessments [93].
     </paragraph>
     <paragraph>
      A more visual language, based on Entity-Relationship Diagrams, is DAPER [58]. Fig. 12 shows a DAPER diagram for the University World and a DAPER diagram for the blood-type model. Another diagrammatic language is given by Multi-Entity Bayesian Networks (MEBNs), a graphical representation for arbitrary first-order sentences [76]. Several other graphical languages mix probabilities with description logics [20], [23], [36], [70], as mentioned in Section 6.
     </paragraph>
     <paragraph>
      There are other formalisms in the literature that focus on textual specifications. For instance, Jaeger's Relational Bayesian Networks[60], [61] offer a solid representation language where the user can directly specify and manipulate probability values. As a short example, one can specify a Relational Bayesian Network as follows{a mathematical formula} The first sentence specifies, in our framework, {a mathematical formula}P(burglary(x)=1)=0.005. The second sentence yields the probability of {a mathematical formula}alarm(x) as a combination of probabilities and values of {a mathematical formula}burglary(x)—this sentence actually yields a conditional probability. Finally the third sentence uses some special syntax to build a Noisy-Or gate by essentially quantifying over all possible values of logvar {a mathematical formula}x. We have examined the complexity of Relational Bayesian Networks elsewhere [86]; some results and proofs, but not all of them, are similar to the ones presented here.
     </paragraph>
     <paragraph>
      There are also languages that encode repetitive Bayesian networks using functional programming [85], [89], [100], [123] or logic programming [24], [41], [101], [102], [112], [115], As an example, the functional language Venture allows statements such as {a mathematical formula}flip(0.4) that assigns probability 0.4 to a particular computation step [85]. Probabilistic logic languages are particularly close to our framework as they have a declarative style, where rules convey the deterministic operations that are mixed with probabilities. Consider for instance ProbLog [41], a popular language that adopts syntactic and semantic conventions usually referred to as “Sato's distribution semantics” [115]. In ProbLog one can write{a mathematical formula} to mean, in our framework, {a mathematical formula}P(burglary(x)=1)=0.005. And one can write a rule such as{a mathematical formula} to mean{a mathematical formula} We have examined the complexity of probabilistic logic programming elsewhere [26]; again, some results and proofs, but not all of them, are similar to the ones presented here. Finally, we note that there are other languages that offer relational counterparts of Markov networks, such as relational Markov networks or Markov logic networks [46]. These formalisms are not directly connected to our efforts here, but one can usually translate complexity results on models that employ directed models into results on models that employ undirected models [69]. We leave the pursuit of this path to future work.
     </paragraph>
    </section>
    <section label="8">
     <section-title>
      A detour into Valiant's counting hierarchy
     </section-title>
     <paragraph>
      We have so far focused on inferences that compare a conditional probability with a given rational number. However one might argue that the real purpose of a probabilistic inference is to compute a probability value. While most previous work on probabilistic databases and lifted inference has looked at the computation of probability values and has produced results using Valiant's counting hierarchy, in this paper we have so far focused on Wagner's counting hierarchy. In this section we justify our strategy and adapt our results to Valiant's hierarchy.
     </paragraph>
     <paragraph>
      Valiant defines, for complexity class {a mathematical formula}A, the class {a mathematical formula}#A to be {a mathematical formula}∪L∈A(#P)L, where {a mathematical formula}(#P)L is the class of functions counting the accepting paths of nondeterministic polynomial-time Turing machines with {a mathematical formula}L as oracle [129]. Valiant declares function f to be {a mathematical formula}#P-hard when {a mathematical formula}#P⊆FPf; that is, f is {a mathematical formula}#P-hard if any function {a mathematical formula}f′ in {a mathematical formula}#P can be reduced to f by the analogue of a polynomial-time Turing reduction (recall that {a mathematical formula}FP is the class of functions that can be computed in polynomial-time by a deterministic Turing machine). Valiant's is a very loose notion of hardness; as shown by Toda and Watanabe [126], any function in {a mathematical formula}#PH can be reduced to a function in {a mathematical formula}#P via a Turing reduction (where {a mathematical formula}#PH is a counting class with the whole polynomial hierarchy as oracle). In fact their result is stronger as the reduction can be made to compute the function in {a mathematical formula}#P a single time (that is, a one-Turing reduction). Thus a Turing reduction is too weak to distinguish classes of functions within {a mathematical formula}#PH. For this reason, other reductions have been considered for counting problems [7], [38].
     </paragraph>
     <paragraph>
      A somewhat stringent strategy is to say that f is {a mathematical formula}#P-hard if any function {a mathematical formula}f′ in {a mathematical formula}#P can be produced from f by a parsimonious reduction; that is, {a mathematical formula}f′(ℓ) is computed by applying a polynomial-time function g to ℓ and then computing {a mathematical formula}f(g(ℓ))[117]. However, such a strategy is inadequate for our purposes: counting classes such as {a mathematical formula}#P produce integers, and we cannot produce integers by computing probabilities.
     </paragraph>
     <paragraph>
      A sensible strategy is to adopt a reduction that allows for multiplication by a polynomial function. This has been done both in the context of probabilistic inference with “reductions modulo normalization” [73] and in the context of probabilistic databases [121]. We adopt the reductions proposed by Bulatov et al. in their study of weighted constraint satisfaction problems [15]. They define weighted reductions as follows (Bulatov et al. consider functions into the algebraic numbers, but for our purposes we can restrict the weighted reductions to rational numbers):
     </paragraph>
     <paragraph label="Definition 8">
      Consider functions {a mathematical formula}f1 and {a mathematical formula}f2 from an input language {a mathematical formula}L to rational numbers {a mathematical formula}Q. A weighted reduction from {a mathematical formula}f1 to {a mathematical formula}f2 is a pair of polynomial-time functions {a mathematical formula}g1:L→Q and {a mathematical formula}g2:L→L such that {a mathematical formula}f1(ℓ)=g1(ℓ)f2(g2(ℓ)) for all ℓ.
     </paragraph>
     <paragraph>
      We say a function f is {a mathematical formula}#P-hard with respect to weighted reductions if any function in {a mathematical formula}#P can be reduced to f via a weighted reduction.
     </paragraph>
     <paragraph>
      Having decided how to define hardness, we must look at membership. As counting problems generate integers, we cannot really say that probabilistic inference problems belong to any class in Valiant's counting hierarchy. In fact, in his seminal work on the complexity of Bayesian networks, Roth notes that “strictly speaking the problem of computing the degree of belief is not in {a mathematical formula}#P, but easily seem equivalent to a problem in this class” [113]. The challenge is to formalize such an equivalence.
     </paragraph>
     <paragraph>
      Grove, Halpern, and Koller quantify the complexity of probabilistic inference by allowing polynomial computations to occur after counting [54, Definition 4.12]. Their strategy is to say that f is {a mathematical formula}#P-easy if there exists {a mathematical formula}f′∈#P and {a mathematical formula}f″∈FP such that for all ℓ we have {a mathematical formula}f(ℓ)=f″(f′(ℓ)). Similarly, Campos, Stamoulis and Weyland take f to be {a mathematical formula}#P[1]-equivalent if f is {a mathematical formula}#P-hard (in Valiant's sense) and belongs to {a mathematical formula}FP#P[1]. Here the superscript {a mathematical formula}#P[1] means that the oracle {a mathematical formula}#P can be called only once. It is certainly a good idea to resort to a new term (“equivalence”) in this context; however one must feel that membership to {a mathematical formula}FP#P[1] is too weak a requirement given Toda and Watanabe's theorem [126]: any function in {a mathematical formula}#PH can be produced within {a mathematical formula}FP#P[1].
     </paragraph>
     <paragraph>
      We adopt a stronger notion of equivalence: a function f is {a mathematical formula}#P-equivalent if it is {a mathematical formula}#P-hard with respect to weighted reductions and {a mathematical formula}g⋅f is in {a mathematical formula}#P for some polynomial-time function g.
     </paragraph>
     <paragraph>
      Also, we need to define a class of functions that corresponds to the complexity class {a mathematical formula}PEXP. We might extend Valiant's definitions and take {a mathematical formula}#EXP to be {a mathematical formula}∪L∈EXP(#P)L. However functions in such a class produce numbers whose size is at most polynomial on the size of the input, as the number of accepting paths of a polynomial-time nondeterministic Turing machine on input ℓ is bounded by {a mathematical formula}2p(|ℓ|) where p is polynomial and {a mathematical formula}|ℓ| is the length of ℓ. This is not appropriate for our purposes, as even simple specifications may lead to exponentially long output (for instance, take {a mathematical formula}P(X(x)=1)=1/2 and compute {a mathematical formula}P(∃x:X(x)): we must be able to write the answer {a mathematical formula}1−1/2N using N bits, and this number of bits is exponential on the input if the domain size is given in binary notation). For this reason, we take {a mathematical formula}#EXP to be the class of functions that can be computed by counting Turing machines of exponential-time complexity [96]. We say that a function f is {a mathematical formula}#EXP-equivalent if f is {a mathematical formula}#EXP-hard with respect to reductions that follow exactly Definition 8, except for the fact that {a mathematical formula}g1 may be an exponential-time function, and {a mathematical formula}g⋅f is in {a mathematical formula}#EXP for some exponential-time function g.
     </paragraph>
     <paragraph>
      Now consider polynomial bounds on space. We will use the class {a mathematical formula}♮PSPACE defined by Ladner [75], consisting of those functions that can be computed by counting Turing machines with a polynomial-space bound and a polynomial bound on the number of nondeterministic moves. This class is actually equal to {a mathematical formula}FPSPACE[poly], the class of functions computable in polynomial space whose outputs are strings encoding numbers in binary notation, and bounded in length by a polynomial [75, Theorem 2]. We say that a function is {a mathematical formula}♮PSPACE-equivalent if f is {a mathematical formula}♮PSPACE-hard with respect to weighted reductions (as in Definition 8), and {a mathematical formula}g⋅f is in {a mathematical formula}♮PSPACE for some polynomial-space function g from the input language to the rational numbers. Of course we might have used “{a mathematical formula}FPSPACE[poly]-equivalent” instead, but we have decided to follow Ladner's original notation.
     </paragraph>
     <paragraph>
      There is one more word of caution when it comes to adopting Valiant's counting Turing machines. It is actually likely that conditional probabilities {a mathematical formula}P(Q|E) cannot be produced by counting Turing machines, as classes in Valiant's counting hierarchy are not likely to be closed under division even by polynomial-time computable functions [95]. Thus we must focus on inferences of the form {a mathematical formula}P(Q); indeed this is the sort of computation that is analyzed in probabilistic databases [121].
     </paragraph>
     <paragraph label="Theorem 20">
      Thus the drawback of Valiant's hierarchy is that a significant amount of adaptation is needed before that hierarchy can be applied to probabilistic inference; hence our preference for Wagner's hierarchy. But after all this preliminary work, we can easily convert our previous results accordingly. For instance, we have the following inferential complexity results: Consider the class of functions that get as input a relational Bayesian network specification based on{a mathematical formula}FFFO, a domain size N (in binary or unary notation), and a set of assignmentsQ, and return{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#EXP-equivalent.
     </paragraph>
     <paragraph label="Theorem 21">
      Consider the class of functions that get as input a relational Bayesian network specification based on{a mathematical formula}FFFOwith relations of bounded arity, a domain size N in unary notation, and a set of assignmentsQ, and return{a mathematical formula}P(Q). This class of functions is{a mathematical formula}♮PSPACE-equivalent.
     </paragraph>
     <paragraph label="Theorem 22">
      Consider the class of functions that get as input a relational Bayesian network specification based on{a mathematical formula}FFFOkfor{a mathematical formula}k≥2, a domain size N in unary notation, and a set of assignmentsQ, and return{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#P-equivalent.
     </paragraph>
     <paragraph label="Theorem 23">
      Consider the class of functions that get as input a plate model based on{a mathematical formula}FFFO, a domain size N (either in binary or unary notation), and a set of assignmentsQ, and return{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#P-equivalent.
     </paragraph>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Proofs
     </section-title>
     <section label="A.1">
      Propositional languages (Section 3)
      <paragraph label="Proof">
       Both{a mathematical formula}#3SAT(&gt;)and{a mathematical formula}#(1-in-3)SAT(&gt;)are{a mathematical formula}PP-complete with respect to many-one reductions.Consider first {a mathematical formula}#3SAT(&gt;). It belongs to {a mathematical formula}PP because deciding {a mathematical formula}#ϕ&gt;k, for propositional sentence ϕ, is in {a mathematical formula}PP[117, Theorem 4.4]. And it is {a mathematical formula}PP-hard because it is already {a mathematical formula}PP-complete when the input is {a mathematical formula}k=2n/2−1[6, Proposition 1].Now consider {a mathematical formula}#(1-in-3)SAT(&gt;). Suppose the input is a propositional sentence ϕ in 3CNF with propositions {a mathematical formula}A1,…,An and m clauses. Turn ϕ into another sentence φ in 3CNF by turning each clause {a mathematical formula}L1∨L2∨L3 in ϕ into a set of four clauses:{a mathematical formula} where the {a mathematical formula}Bj are fresh propositions not in ϕ. We claim that {a mathematical formula}#ϕ=#(1-in-3)φ; that is, {a mathematical formula}#(1-in-3)φ&gt;k is equivalent to {a mathematical formula}#ϕ&gt;k, proving the desired hardness. To prove this claim, note that for each clause {a mathematical formula}L1∨L2∨L3 in ϕ, and for each assignment to {a mathematical formula}(L1,L2,L3) that satisfies the clause, there is only one assignment to {a mathematical formula}(B1,B2,B3,B4,B5) that satisfies the clauses in Expression (A.1). To prove this, Table A.2 presents the set of all assignments that satisfy the latter four clauses.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}INF[Prop(∧)]is in{a mathematical formula}Pwhen the query{a mathematical formula}(Q,E)contains only positive assignments, and{a mathematical formula}INF[Prop(∨)]is in{a mathematical formula}Pwhen the query contains only negative assignments.Consider first {a mathematical formula}INF[Prop(∧)]. To run inference with positive assignments {a mathematical formula}(Q,E), just run d-separation to collect the set of root variables that must be {a mathematical formula}true given the assignments (note that as soon as a node is set to true, its parents must be true, and so on recursively). Then the probability of the conjunction of assignments in Q and in E is just the product of probabilities for these latter atomic propositions to be {a mathematical formula}true, and these probabilities are given in the network specification. Thus we obtain {a mathematical formula}P(Q,E). Now repeat the same polynomial computation only using assignments in E, to obtain {a mathematical formula}P(E), and determine whether {a mathematical formula}P(Q,E)/P(E)&gt;γ or not.Now consider {a mathematical formula}INF[Prop(∨)]. For any input network specification, we can easily build a network specification in {a mathematical formula}INF[Prop(∧)] by turning every variable X into a new variable {a mathematical formula}X′ such that {a mathematical formula}X=¬X′. Then the root node associated with assessment {a mathematical formula}P(X=1)=α is turned into a root node associated with {a mathematical formula}P(X′=1)=1−α, and a definition axiom {an inline-figure} is turned into a definition axiom {an inline-figure}. Any negative evidence is then turned into positive evidence, and the reasoning in the previous paragraph applies.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}INF[Prop(∧)]and{a mathematical formula}INF[Prop(∨)]are{a mathematical formula}PP-complete with respect to many-one reductions.Membership follows from the fact that {a mathematical formula}INF[Prop(∧,¬)]∈PP. We therefore focus on hardness.Consider first {a mathematical formula}INF[Prop(∧)]. We present a parsimonious reduction from {a mathematical formula}#(1-in-3)SAT(&gt;) to {a mathematical formula}INF[Prop(∧)], following a strategy by Mauá et al. [87].Take a sentence ϕ in 3CNF with propositions {a mathematical formula}A1,…,An and m clauses. If there is a clause containing three times the same literal (for instance, {a mathematical formula}(A1∨A1∨A1)), then the 1-in-3 rule cannot be respected; we assume that such problems are detected and solved at once, hence we deal with formulas where each clause does not have three identical literals.For each literal in ϕ, introduce a random variable {a mathematical formula}Xij, where i refers to the ith clause, and j refers to the jth literal (note: {a mathematical formula}j∈{1,2,3}). The set of all such random variables is L.For instance, suppose we have the sentence {a mathematical formula}(A1∨A2∨A3)∧(A4∨¬A1∨A3). We then make the correspondences: {a mathematical formula}X11↝A1, {a mathematical formula}X12↝A2, {a mathematical formula}X13↝A3, {a mathematical formula}X21↝A4, {a mathematical formula}X22↝¬A1, {a mathematical formula}X23↝A3.Note that {a mathematical formula}{Xij=1} indicates an assignment of {a mathematical formula}true to the corresponding literal. Say that a configuration of L is gratifying if {a mathematical formula}Xi1+Xi2+Xi3≥1 for every clause (without necessarily respecting the 1-in-3 rule). Say that a configuration is respectful if is respects the 1-in-3 rule; that is, if {a mathematical formula}Xi1+Xi2+Xi3≤1 for every clause. And say that a configuration is sensible if two variables that correspond to the same literal have the same value, and two variables that correspond to a literal and its negation have distinct values (in the example in the last paragraph, both {a mathematical formula}{X11=1,X22=1} and {a mathematical formula}{X13=1,X23=0} fail to produce a sensible configuration).For each random variable {a mathematical formula}Xij, introduce the assessment {a mathematical formula}P(Xij=1)=1−ε, where ε is a rational number to be determined later. Our strategy is to introduce definition axioms so that only the gratifying-respectful-sensible configurations of L get high probability, while the remaining configurations have low probability. The main challenge is to do so without negation.Let Q be an initially empty set of assignments. We first eliminate the configurations that do not respect the 1-in-3 rule. To do so, for {a mathematical formula}i=1,…,m include definition axioms{a mathematical formula} and add {a mathematical formula}{Yi1i2=0,Yi1i3=0,Yi2i3=0} to Q. This guarantees that configurations of L that fail to be respectful are incompatible with Q.We now eliminate gratifying-respectful configurations that are not sensible. We focus on gratifying and respectful configurations because, as we show later, ungratifying configurations compatible with Q are assigned low probability.
       <list>
        Suppose first that we have a clause where the same proposition appears more than once. There are three possibilities (recall that clauses with three identical literals are not possible at this point). If the ith clause can be written as {a mathematical formula}(A∨A∨L) for some proposition A and some literal L (that may even be ¬A), then add {a mathematical formula}{Xi3=1} to Q. If instead the ith clause can be written as {a mathematical formula}(¬A∨¬A∨L) for some proposition A and some literal L (that may even be A), then add {a mathematical formula}{Xi3=1} to Q. Finally, if the ith clause can be written as {a mathematical formula}(A∨¬A∨L) for some proposition A and some literal L that is unrelated to A, then add {a mathematical formula}{Xi3=0} to Q.Suppose we have two distinct clauses {a mathematical formula}(A∨Li2∨Li3) and {a mathematical formula}(¬A∨Lj2∨Lj3), where A is a proposition and the {a mathematical formula}Luv are literals (possibly referring more than once to the same propositions). Suppose the six literals in these two clauses correspond to variables {a mathematical formula}(Xi1,Xi2,Xi3) and {a mathematical formula}(Xj1,Xj2,Xj3), in this order. We must have {a mathematical formula}Xi1=1−Xj1. To encode this relationship, we take two steps. First, introduce the definition axiom{a mathematical formula} and add {a mathematical formula}{Yi1j1=0} to Q: at most one of {a mathematical formula}Xi1 and {a mathematical formula}Xj1 is equal to 1, but there may still be gratifying-respectful configurations where {a mathematical formula}Xi1=Xj1=0. Thus the second step is to enforce the sentence {a mathematical formula}θ=¬(Li2∨Li3)∨¬(Lj2∨Lj3), as this forbids {a mathematical formula}Xi1=Xj1=0. Note that θ is equivalent to {a mathematical formula}¬(Li2∧Lj2)∧¬(Li2∧Lj3)∧¬(Li3∧Lj2)∧¬(Li3∧Lj3), so introduce the definition axiom{a mathematical formula} and add {a mathematical formula}{Yiujv=0} to Q, for each {a mathematical formula}u∈{2,3} and {a mathematical formula}v∈{2,3}. Proceed similarly if the literals of interest appear in other positions in the clauses.Now we focus on any two distinct clauses that share a literal. Say we have {a mathematical formula}(A∨Li2∨Li3) and {a mathematical formula}(A∨Lj2∨Lj3) where the symbols are as in the previous bullet, and where the literals are again paired with variables {a mathematical formula}(Xi1,Xi2,Xi3) and {a mathematical formula}(Xj1,Xj2,Xj3). If {a mathematical formula}Xi1=1, then we must have {a mathematical formula}Xj1=1, and to guarantee this in a gratifying-respectful configuration we introduce{a mathematical formula} and add {a mathematical formula}{Yi1j2=0,Yi1j3=0} to Q. Similarly, if {a mathematical formula}Xj1=1, we must have {a mathematical formula}Xi1=1, so introduce{a mathematical formula} and add {a mathematical formula}{Yi2j1=0,Yi3j1=0} to Q. Again, proceed similarly if the literals of interest appear in other positions in the clauses.Consider a configuration
       </list>
       <paragraph>
        All other arguments carry, so we obtain the desired hardness.  □
       </paragraph>
      </paragraph>
      <paragraph>
       It is instructive to look at a proof of Theorem 2 that uses Turing reductions, as it is much shorter than the previous proof:
      </paragraph>
      <paragraph label="Proof">
       To prove hardness of {a mathematical formula}INF[Prop(∨)], we use the fact that the function {a mathematical formula}#MON2SAT is {a mathematical formula}#P-complete with respect to Turing reductions [130, Theorem 1]. Recall that {a mathematical formula}#MON2SAT is the function that counts the number of satisfying assignments of a monotone sentence in 2CNF. So, we can take any MAJSAT problem where the input is sentence ϕ and produce (with polynomial effort) another sentence {a mathematical formula}ϕ′ in 2CNF such that #ϕ is obtained from {a mathematical formula}#ϕ′ (again with polynomial effort). And we can compute {a mathematical formula}#ϕ′ using {a mathematical formula}INF[Prop(∨)], as follows. Write {a mathematical formula}ϕ′ as {a mathematical formula}⋀i=1m(Ai1∨Ai2), where each {a mathematical formula}Aij is a proposition in {a mathematical formula}A1,…,An. Introduce fresh propositions/variables {a mathematical formula}Ci and definition axioms {an inline-figure}. Also introduce {a mathematical formula}P(Ai=1)=1/2 for each {a mathematical formula}Ai, and consider the query {a mathematical formula}Q={C1,…,Cm}. Note that {a mathematical formula}P(Q)&gt;γ if and only if {a mathematical formula}#ϕ′=2nP(Q)&gt;2nγ, so we can bracket {a mathematical formula}#ϕ′. From {a mathematical formula}#ϕ′ we obtain #ϕ and we can decide whether {a mathematical formula}#ϕ&gt;2n−1, thus solving the original MAJSAT problem.To prove hardness of {a mathematical formula}INF[Prop(∧)], note that the number of satisfying assignments of {a mathematical formula}ϕ′ in 2CNF is equal to the number of satisfying assignments of {a mathematical formula}⋀i=1m(¬Ai1∨¬Ai2), because one can take each satisfying assignment for the latter sentence and create a satisfying assignment for {a mathematical formula}ϕ′ by interchanging {a mathematical formula}true and {a mathematical formula}false, and likewise for the unsatisfying assignments. Introduce fresh propositions/variables {a mathematical formula}Ci and definition axioms {an inline-figure}. Also introduce {a mathematical formula}P(Ai=1)=1/2 for each {a mathematical formula}Ai, and consider the query where {a mathematical formula}Q={¬C1,…,¬Cm}. Again we can bracket the number of assignments that satisfy {a mathematical formula}ϕ′, and thus we can solve any MAJSAT problem by using {a mathematical formula}INF[Prop(∧)] and appropriate auxiliary polynomial computations.  □
      </paragraph>
     </section>
     <section label="A.2">
      Relational languages (Section 5)
      <paragraph label="Proof">
       {a mathematical formula}INF[FFFO]is{a mathematical formula}PEXP-complete with respect to many-one reductions, regardless of whether the domain is specified in unary or binary notation.To prove membership, note that a relational Bayesian network specification based on {a mathematical formula}FFFO can be grounded into an exponentially large Bayesian network, and inference can be carried out in that network using a counting Turing machine with an exponential bound on time. This is true even if we have unbounded arity of relations: even if we have domain size {a mathematical formula}2N and maximum arity k, grounding each relation generates up to {a mathematical formula}2kN nodes, still an exponential quantity in the input.To prove hardness, we focus on binary domain size N as this simplifies the notation. Clearly if N is given in unary, then an exponential number of groundings can be produced by increasing the arity of relations (even if the domain is of size 2, an arity k leads to {a mathematical formula}2k groundings).Given the scarcity of {a mathematical formula}PEXP-complete problems in the literature, we have to work directly from Turing machines. Start by taking any language {a mathematical formula}L such that {a mathematical formula}ℓ∈L if and only if ℓ is accepted by more than half of the computation paths of a nondeterministic Turing machine {a mathematical formula}M within time {a mathematical formula}2p(|ℓ|) where p is a polynomial and {a mathematical formula}|ℓ| denotes the size of ℓ. To simplify matters, denote {a mathematical formula}p(|ℓ|) by n. The Turing machine is defined by its alphabet, its states, and its transition function.Denote by σ a symbol in {a mathematical formula}M's alphabet, and by q a state of {a mathematical formula}M. A configuration of {a mathematical formula}M can be described by a string {a mathematical formula}σ0σ1…σi−1(qσi)σi+1…σ2n−1, where each {a mathematical formula}σj is a symbol in the tape, {a mathematical formula}(qσi) indicates both the state q and the position of the head at cell i with symbol {a mathematical formula}σi. The initial configuration is {a mathematical formula}(q0σ⁎0)σ⁎1…σ⁎m−1 followed by {a mathematical formula}2n−m blanks, where {a mathematical formula}q0 is the initial state. There are also states {a mathematical formula}qa and {a mathematical formula}qr that respectively indicate acceptance or rejection of the input string {a mathematical formula}σ⁎0…σ⁎m−1. We assume that if {a mathematical formula}qa or {a mathematical formula}qr appear in some configuration, then the configuration is not modified anymore (that is, the transition moves from this configuration to itself). This is necessary to guarantee that the number of accepting computations is equal to the number of ways in which we can fill in a matrix of computations. For instance, a particular accepting computation could be depicted as a {a mathematical formula}2n×2n matrix as in Fig. A.14, where ⌞⌟ denotes the blank, and where we complete the rows of the matrix after the acceptance by repeating the accepting row.The transition function δ of {a mathematical formula}M takes a pair {a mathematical formula}(q,σ) consisting of a state and a symbol in the machine's tape, and returns a triple {a mathematical formula}(q′,σ′,m): the next state {a mathematical formula}q′, the symbol {a mathematical formula}σ′ to be written in the tape (we assume that a blank is never written by the machine), and an integer m in {a mathematical formula}{−1,0,1}. Here −1 means that the head is to move left, 0 means that the head is to stay in the current cell, and 1 means that the head is to move right.We now encode this Turing machine using monadic logic, mixing some ideas by Lewis [77] and by Tobies [124].Take a domain of size {a mathematical formula}22n. The idea is that each {a mathematical formula}x is a cell in the computation matrix. From now on, a “point” is a cell in that matrix. Introduce parvariables {a mathematical formula}X0(x),…,Xn−1(x) and {a mathematical formula}Y0(x),…,Yn−1(x) to encode the index of the column and the row of point {a mathematical formula}x. Impose, for {a mathematical formula}0≤i≤n−1, the assessments {a mathematical formula}P(Xi(x)=1)=P(Yi(x)=1)=1/2.We need to specify the concept of adjacent points in the computation matrix. To this end we introduce two macros, {a mathematical formula}EAST(x,y) and {a mathematical formula}NORTH(x,y) (note that we do not actually need binary relations here; these expressions are just syntactic sugar). The meaning of {a mathematical formula}EAST(x,y) is that for point {a mathematical formula}x there is a point {a mathematical formula}y that is immediately to the right of {a mathematical formula}x; the meaning of {a mathematical formula}NORTH(x,y) is that for point {a mathematical formula}x there is a point {a mathematical formula}y that is immediately on top of {a mathematical formula}x[124].{a mathematical formula}{a mathematical formula} In these expressions an empty conjunction means {a mathematical formula}true and an empty disjunction means {a mathematical formula}false.Now introduce{a mathematical formula}{a mathematical formula} Now if {a mathematical formula}Z1∧Z2 is {a mathematical formula}true, we “build” a square “board” of size {a mathematical formula}2n×2n (in fact this is a torus as the top row is followed by the bottom row, and the rightmost column is followed by the leftmost column). The intuition is that {a mathematical formula}Z2 guarantees the existence of an element in the “origin” and then {a mathematical formula}Z1 guarantees that neighbors exist to the “east” and to the “north”.Introduce a relation {a mathematical formula}Cj for each triplet {a mathematical formula}(α,β,γ) where each element of the triplet is either a symbol σ or a symbol of the form {a mathematical formula}(qσ) for our machine {a mathematical formula}M, and with an additional condition: if {a mathematical formula}(α,β,γ) has β equal to a blank, then γ is a blank. Furthermore, introduce a relation {a mathematical formula}Cj for each triple {a mathematical formula}(⋄,β,γ), where β and γ are as before, and ⋄ is a new special symbol (these relations are needed later to encode the “left border” of the board). We refer to each {a mathematical formula}Cj as a tile, as we are in effect encoding a domino system [77]. For each tile, impose {a mathematical formula}P(Cj(x)=1)=1/2.Now each point must have one and only one tile:{a mathematical formula}Having defined the tiles, we now define a pair of relations encoding the “horizontal” and “vertical” constraints on tiles, so as to encode the transition function of the Turing machine. Denote by H the relation consisting of pairs of tiles that satisfy the horizontal constraints and by V the relation consisting of pairs of tiles that satisfy the vertical constraints.The horizontal constraints must enforce the fact that, in a fixed row, a tile {a mathematical formula}(α,β,γ) at column i for {a mathematical formula}0≤i≤2n−1 overlaps the tile {a mathematical formula}(α′,β′,γ′) at column {a mathematical formula}i+1 by satisfying{a mathematical formula}The vertical constraints must encode the possible computations. To do so, consider a tile {a mathematical formula}t=(α,β,γ) at row j, for {a mathematical formula}0≤j≤2n−1, and tile {a mathematical formula}t′=(α′,β′,γ′) at row {a mathematical formula}j+1, both at the same column. The pair {a mathematical formula}(t,t′) is in V if and only if (a) {a mathematical formula}t′ can be reached from t given the states in the Turing machine; and (b) if {a mathematical formula}t=(⋄,β,γ), then {a mathematical formula}t′=(⋄,β′,γ′) for {a mathematical formula}β′ and {a mathematical formula}γ′ that follow from the behavior of {a mathematical formula}M.We distinguish the last row and the last column, as the transition function does not apply to them:{a mathematical formula} We can now encode the transition function:{a mathematical formula}{a mathematical formula}We create a parvariable that signals the accepting state:{a mathematical formula}Finally, we must also impose the initial conditions. Take the tiles in the first row so that symbols in the input of {a mathematical formula}M are encoded as m tiles, with the first tile {a mathematical formula}t0=(⋄,(q0σ⁎0),σ⁎1) and the following ones {a mathematical formula}tj=(σ⁎j−1,σ⁎j,σ⁎j+1) up to {a mathematical formula}tm−1=(σ⁎m−2,σ⁎m−1,⌞⌟). So the next tile will be {a mathematical formula}(σ⁎m−1,⌞⌟,⌞⌟), and after that all tiles in the first row will contain only blanks. Now take individuals {a mathematical formula}ai for {a mathematical formula}i∈{0,…,m−1} and create an assignment {a mathematical formula}{Ci0(ai)=1} for each {a mathematical formula}ai, where {a mathematical formula}Ci0 is the ith tile encoding the initial conditions. Denote by E the set of such assignments.Now {a mathematical formula}P(Z6=1|E∧⋀i=15{Zi=1})&gt;1/2 if and only if the number of correct arrangements of tiles that contain the accepting state is larger than the total number of possible valid arrangements. Hence an inference with the constructed relational Bayesian network specification decides the language {a mathematical formula}L we started with, as desired.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}INF[ALC]is{a mathematical formula}PEXP-complete with respect to many-one reductions when domain size is given in binary notation.Membership follows from Theorem 4.To prove hardness, we simplify a previous proof by Cozman and Polastro [27], using as much as possible the construction in the proof of Theorem 4. So assume we have the input ℓ and the Turing machine {a mathematical formula}M as described in that proof, as well as the domain of size {a mathematical formula}22n. Assume also that we have the parvariables {a mathematical formula}X0(x),…,Xn−1(x),Y0(x),…,Yn−1(x), together with their associated assessments {a mathematical formula}P(Xi(x)=1)=P(Yi(x)=1)=1/2, and the parvariables {a mathematical formula}Cj(x) with their associated assessments {a mathematical formula}P(Cj(x)=1)=1/2. Then note that, using the arguments around Expression (5), we can “create” quantification patterns such as {a mathematical formula}∃y:X(x,y) and {a mathematical formula}∃y:Y(y) with probability one. We can use these patterns to write all expressions defining {a mathematical formula}Z1,Z2,…,Z6 in the proof of Theorem 4, except for the fact that macros {a mathematical formula}EAST and {a mathematical formula}NORTH (respectively Expressions (A.3) and (A.4)) cannot be written directly with {a mathematical formula}ALC. Thus we could, except for the macros {a mathematical formula}EAST and {a mathematical formula}NORTH, repeat here the proof of Theorem 4, at the end making our desired decision {a mathematical formula}P(Z6=1|E∧⋀i=15{Zi=1})&gt;1/2.To reproduce the behavior of the macros {a mathematical formula}EAST and {a mathematical formula}NORTH, we resort to techniques developed by Tobies [124]. First we introduce two binary relations, {a mathematical formula}east(x,y) and {a mathematical formula}north(x,y); these relations will be forced to behave as the missing macros. The groundings of {a mathematical formula}east and {a mathematical formula}north will be set to specific values given the evidence on {a mathematical formula}Z1,…,Z5, but to complete the specification we just introduce {a mathematical formula}P(east(x,y)=1)=1/2 and {a mathematical formula}P(north(x,y)=1)=1/2. Then introduce:{a mathematical formula}{a mathematical formula}Now replace {a mathematical formula}EAST by {a mathematical formula}east and {a mathematical formula}NORTH by {a mathematical formula}north in the definitions of {a mathematical formula}Z1, {a mathematical formula}Z4 and {a mathematical formula}Z5, and introduce{a mathematical formula} If {a mathematical formula}Z1∧Z2∧Z9 is {a mathematical formula}true, we ‘build” a torus as in the proof of Theorem 4; this is a consequence of the fact that each one of the {a mathematical formula}22n cells of the torus must contain at least an element of the domain [124, Lemma 3.5], and the fact that we have exactly {a mathematical formula}22n elements in the domain. Hence we produce the desired decision by {a mathematical formula}P(Z6=1|{Z9=1}∧E∧⋀i=15{Zi=1})&gt;1/2.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}QINF[FFFO]is{a mathematical formula}PEXP-complete with respect to many-one reductions when domain size is specified in binary notation.Membership is obvious as the inferential complexity is already in {a mathematical formula}PEXP. To show hardness, take a Turing machine {a mathematical formula}M that solves some {a mathematical formula}PEXP-complete problem within {a mathematical formula}2n steps, where n is polynomial on the size of the input ℓ (as in the proof of Theorem 4). That is, there is a {a mathematical formula}PEXP-complete language {a mathematical formula}L such that {a mathematical formula}ℓ∈L if and only if the input string ℓ is accepted by more than half of the computation paths of {a mathematical formula}M within time {a mathematical formula}2n.Such a Turing machine {a mathematical formula}M has alphabet, states and transitions as in the proof of Theorem 4. Assume that {a mathematical formula}M repeats its configuration as soon as it enters into the accepting or the rejecting state, as in the proof of Theorem 4.To encode {a mathematical formula}M we resort to a construction by Grädel [52] where relations of arity two are used. We use: (a) for each state q of {a mathematical formula}M, a unary relation {a mathematical formula}Xq; (b) for each symbol σ in the alphabet of {a mathematical formula}M, a binary relation {a mathematical formula}Yσ; (c) a binary relation Z. The idea is that {a mathematical formula}Xq(x) means that {a mathematical formula}M is in state q at computation step {a mathematical formula}x, while {a mathematical formula}Yσ(x,y) means that σ is the symbol at the {a mathematical formula}yth position in the tape at computation step {a mathematical formula}x, and {a mathematical formula}Z(x,y) means that the machine head is at the {a mathematical formula}yth position in the tape at computation step {a mathematical formula}x. Impose {a mathematical formula}P(Xq(x)=1)=P(Yσ(x,y)=1)=P(Z(x,y)=1)=1/2.We use a distinguished relation &lt;, assumed not to be in the vocabulary. This relation is to be a linear order on the domain; to obtain this behavior, just introduce {a mathematical formula}P(&lt;(x,y)=1)=1/2 and{a mathematical formula} We will later set evidence on {a mathematical formula}Z1 to force &lt; to be a linear order. The important point is that we can assume that a domain of size {a mathematical formula}2n is given and all elements of this domain are ordered according to &lt;.Clearly we can define a successor relation using &lt;:{a mathematical formula} Also, we can define a relation that signals the “first” individual:{a mathematical formula}We must guarantee that at any given step the machine is in a single state, each cell of the tape has a single symbol, and the head is at a single position of the tape:{a mathematical formula}{a mathematical formula}{a mathematical formula}We also have to guarantee that computations do not change the content of a cell that is not visited by the head:{a mathematical formula} We must encode the changes made by the transition function:{a mathematical formula}We must also guarantee that all cells to the right of a blank cell are also blank:{a mathematical formula}Finally, we must signal the accepting state:{a mathematical formula}We have thus created a set of formulas that encode the behavior of the Turing machine. Now take the input string ℓ, equal to {a mathematical formula}σ⁎0,σ⁎1,…,σ⁎m−1, and encode it as a query as follows. Start by “creating” the first individual in the ordering by taking the assignment {a mathematical formula}{first(a0)=1}. Then introduce {a mathematical formula}{Z(a0,a0)=1} to initialize the head. Introduce {a mathematical formula}{Yσ⁎0(a0,a0)=1} to impose the initial condition on the first cell, and for each subsequent initial condition {a mathematical formula}σ⁎i we set {a mathematical formula}{Yσ⁎i(a0,ai)=1} and {a mathematical formula}{successor(ai−1,ai)=1} where {a mathematical formula}ai is a fresh individual. Finally, set {a mathematical formula}{Y⌞⌟(a0,am)=1} and {a mathematical formula}{successor(am−1,am)=1} and {a mathematical formula}{Xq0(a0)=1}. These assignments are denoted by E.Now {a mathematical formula}P(Z8=1|E∧⋀i=17{Zi=1})&gt;1/2 for a domain of size {a mathematical formula}2n if and only if the number of interpretations reaching the accepting state is larger than the total number of possible interpretations encoding computation paths.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}QINF[FFFO]is{a mathematical formula}PP-complete with respect to many-one reductions when domain size is specified in unary notation.To prove hardness, take a MAJSAT problem where ϕ is in CNF with m clauses and propositions {a mathematical formula}A1,…,An. Make sure {a mathematical formula}m=n: if {a mathematical formula}m&lt;n, then add trivial clauses such as {a mathematical formula}A1∨¬A1; if instead {a mathematical formula}n&lt;m, then add fresh propositions {a mathematical formula}An+1,…,Am. These changes do not change the output of MAJSAT. Introduce unary relations {a mathematical formula}sat(x) and impose {a mathematical formula}P(sat(x))=1/2. Take a domain {a mathematical formula}{1,…,n}; the elements of the domain serve a dual purpose, indexing both propositions and clauses. Introduce relations {a mathematical formula}sat(x), {a mathematical formula}positiveLit(x,y) and {a mathematical formula}negativeLit(x,y), assessments {a mathematical formula}P(sat(x)=1)=P(positiveLit(x,y)=1)=P(negativeLit(x,y)=1)=1/2, and definition axioms:{a mathematical formula}{a mathematical formula} Take evidence E as follows. For each clause, run over the literals. Consider the ith clause, and its non-negated literal {a mathematical formula}Aj: set {a mathematical formula}positiveLit(i,j) to {a mathematical formula}true. And consider negated literal {a mathematical formula}¬Aj: set {a mathematical formula}negativeLit(i,j) to {a mathematical formula}true. Set all other groundings of {a mathematical formula}positiveLit and {a mathematical formula}negativeLit to {a mathematical formula}false. Note that {a mathematical formula}P(E)=2−2n2&gt;0. Now decide whether {a mathematical formula}P(query=1|E)&gt;1/2. If YES, the MAJSAT problem is accepted, if NO, it is not accepted. Hence we have the desired polynomial reduction (the query is quadratic on domain size; all other elements are linear on domain size).To prove membership in {a mathematical formula}PP, we describe a Turing machine {a mathematical formula}M that decides whether {a mathematical formula}P(Q|E)&gt;γ. The machine guesses a truth assignment for each one of the polynomially-many grounded root nodes (and writes the guess in the working tape). Note that each grounded root node X is associated with an assessment {a mathematical formula}P(X=1)=c/d, where c and d are the smallest such integers. Then the machine replicates its computation paths out of the guess on X: there are c paths with identical behavior for guess {a mathematical formula}{X=1}, and {a mathematical formula}d−c paths with identical behavior for guess {a mathematical formula}{X=0}.Now the machine verifies whether the set of guessed truth assignment satisfies E; if not, move to state {a mathematical formula}q1. If yes, then verify whether the guessed truth assignment fails to satisfy Q; if not, move to state {a mathematical formula}q2. And if yes, then move to state {a mathematical formula}q3. The key point is that there is a logarithmic space, hence polynomial-time, algorithm that can verify whether a set of assignments holds once the root nodes are set [78, Section 6.2].Suppose that out of N computation paths that {a mathematical formula}M can take, {a mathematical formula}N1 of them reach {a mathematical formula}q1, {a mathematical formula}N2 reach {a mathematical formula}q2, and {a mathematical formula}N3 reach {a mathematical formula}q3. By construction,{a mathematical formula} where we abuse notation by taking {a mathematical formula}¬Q to mean that some assignment in Q is not {a mathematical formula}true. Note that up to this point we do not have any rejecting nor accepting path, so the specification of {a mathematical formula}M is not complete.The remainder of this proof just reproduces a construction by Park in his proof of {a mathematical formula}PP-completeness for propositional Bayesian networks [33, Theorem 11.5]. Park's construction adds rejecting/accepting computation paths emanating from {a mathematical formula}q1, {a mathematical formula}q2 and {a mathematical formula}q3. It uses numbers{a mathematical formula} and the smallest integers {a mathematical formula}a1, {a mathematical formula}a2, {a mathematical formula}b1, {a mathematical formula}b2 such that {a mathematical formula}a=a1/a2 and {a mathematical formula}b=b1/b2. Now, out of {a mathematical formula}q1 branch into {a mathematical formula}a2b2 computation paths that immediately stop at the accepting state, and {a mathematical formula}a2b2 computation paths that immediately stop at the rejecting state.{sup:8} Out of {a mathematical formula}q2 branch into {a mathematical formula}2a2b1 paths that immediately stop at the accepting state, and {a mathematical formula}2(b2−b1)a2 paths that immediately stop at the rejecting state. Out of {a mathematical formula}q3 branch into {a mathematical formula}2a1b2 paths that immediately stop at the accepting state, and {a mathematical formula}2(a2−a1)b2 paths that immediately stop at the rejecting state. For the whole machine {a mathematical formula}M, the number of computation paths that end up at the accepting state is {a mathematical formula}a2b2N1+2a2b1N2+2a1b2N3, and the total number of computation paths is {a mathematical formula}a2b2N1+a2b2N1+2b1a2N2+2(b2−b1)a2N2+2a1b2N3+2(a2−a1)b2N3=2a2b2N. Hence the number of accepting paths divided by the total number of paths is {a mathematical formula}(N1(1/2)+(b1/b2)N2+(a1/a2)N3)/N. By combining this construction with Expression (A.7), we obtain{a mathematical formula}{a mathematical formula} as we can assume that {a mathematical formula}P(E)&gt;0 (otherwise the number of accepting paths is equal to the number of rejecting paths), and then{a mathematical formula} Hence the number of accepting computation paths of {a mathematical formula}M is larger than half the total number of computation paths if and only if {a mathematical formula}P(Q|E)&gt;γ. This completes the proof of membership.  □
      </paragraph>
      <paragraph label="Proof">
       Suppose{a mathematical formula}NETIME≠ETIME. Then{a mathematical formula}DINF[FFFO]is not solved in deterministic exponential time when domain size is given in binary notation.Jaeger describes results implying, in case {a mathematical formula}NETIME≠ETIME, that there is a sentence {a mathematical formula}ϕ∈FFFO such that the spectrum of ϕ cannot be recognized in deterministic exponential time [63]. Recall: the spectrum of a sentence is a set containing each integer N, in binary notation, such that ϕ has a model whose domain size is N[52]. So take N in binary notation, the relational Bayesian network specification {an inline-figure}, and decide whether {a mathematical formula}P(A)&gt;0 for domain size N; if yes, then N is in the spectrum of ϕ.  □
      </paragraph>
      <paragraph label="Theorem 9">
       {a mathematical formula}DINF[FFFO]is{a mathematical formula}PP1-complete with respect to many-one reductions when domain size is given in unary notation.
      </paragraph>
      <paragraph label="Proof">
       To prove membership, just consider the Turing machine used in the proof of Theorem 7, now with a fixed query. This is a polynomial-time nondeterministic Turing machine that gets the domain size in unary (that is, as a sequence of 1s) and produces the desired output.To prove hardness, take a Turing machine with input alphabet consisting of symbol 1, and that solves a {a mathematical formula}PP1-complete problem in {a mathematical formula}Nm steps for input consisting of N symbols 1. Take the probabilistic assessment and the definition axioms for {a mathematical formula}successor, {a mathematical formula}first, and {a mathematical formula}Z1 as in the proof of Theorem 6. Now introduce relations {a mathematical formula}Xq, {a mathematical formula}Yσ and Z as in that proof, with the difference that {a mathematical formula}x is substituted for m logvars {a mathematical formula}xi, and likewise {a mathematical formula}y is substituted for m logvars {a mathematical formula}xj. For instance, we now have {a mathematical formula}Z(x1,…,xm,y1,…,ym). Repeat definition axioms for {a mathematical formula}Z2,…,Z8 as presented in the proof of Theorem 6, with appropriate changes in the arity of relations. In doing so we have an encoding for the Turing machine where the computation steps are indexed by a vector {a mathematical formula}[x1,…,xm], and the tape is indexed by a vector {a mathematical formula}[y1,…,ym]. The remaining problem is to insert the input. To do so, introduce:{a mathematical formula} Now {a mathematical formula}P(Z8=1|{Z′=1}∧⋀i=17{Zi=1})&gt;1/2 for a domain of size N if and only if the number of interpretations that set an accepting state to {a mathematical formula}true is larger than half the total number of interpretations encoding computation paths.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}INF[FFFO]is{a mathematical formula}PSPACE-complete with respect to many-one reductions when relations have bounded arity and domain size is given in unary notation.To prove membership, construct a Turing machine that goes over the truth assignments for all of the polynomially-many grounded root nodes. The machine generates an assignment, writes it using polynomial space, and verifies whether E can be satisfied: there is a polynomial-space algorithm to do this, as we basically need to do model checking in first-order logic [52, Section 3.1.4]. While cycling through truth assignments, keep adding the probabilities of the truth assignments that satisfy E. If the resulting probability for E is zero, reject; otherwise, again go through every truth assignment of the root nodes, now keeping track of how many of them satisfy {a mathematical formula}{Q,E}, and adding the probabilities for these assignments. Then divide the probability of {a mathematical formula}{Q,E} by the probability of E, and compare the result with the rational number γ.To show hardness, consider the definition axiom {an inline-figure}, where each {a mathematical formula}Qi is a quantifier (either ∀ or ∃) and ϕ is a quantifier-free formula containing only Boolean operators, a unary relation X, and logvars {a mathematical formula}x1,…,xn. The relation X is associated with assessment {a mathematical formula}P(X(x)=1)=1/2. Take domain {a mathematical formula}D={0,1} and evidence {a mathematical formula}E={X(0)=0,X(1)=1}. Then {a mathematical formula}P(Y=1|E)&gt;1/2 if and only if {a mathematical formula}Q1x1:…Qnxn:ϕ(x1,…,xn) is satisfiable. Deciding the latter satisfiability question is in fact equivalent to deciding the satisfiability of a Quantified Boolean Formula, a {a mathematical formula}PSPACE-complete problem [78, Section 6.5].  □
      </paragraph>
      <paragraph>
       Now consider the bounded variable fragment {a mathematical formula}FFFOk. It is important to notice that if the body of every definition axiom belongs to {a mathematical formula}FFFOk for an integer k, then all definition axioms together are equivalent to a single formula in {a mathematical formula}FFFOk. Hence results on logical inference for {a mathematical formula}FFFOk can be used to derive inferential, query and domain complexities.
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}INF[FFFOk]is{a mathematical formula}PP-complete with respect to many-one reductions for all{a mathematical formula}k≥0when domain size is given in unary notation.Hardness is trivial: even {a mathematical formula}Prop(∧,¬) is {a mathematical formula}PP-hard. To prove membership, use the Turing machine described in the proof of membership in Theorem 7, with a small difference: when it is necessary to check whether E (or {a mathematical formula}Q∪E) holds given a guessed assignment for root nodes, use the appropriate model checking algorithm [135], as this verification can be done in polynomial time.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}QINF[FFFOk]is{a mathematical formula}PP-complete with respect to many-one reductions for all{a mathematical formula}k≥2when domain size is given in unary notation.To prove membership, note that {a mathematical formula}QINF[FFFO] is in {a mathematical formula}PP by Theorem 7. To prove hardness, note that the proof of hardness in Theorem 7 uses only {a mathematical formula}FFFO2.  □
      </paragraph>
      <paragraph label="Proof">
       {a mathematical formula}DINF[FFFOk]is{a mathematical formula}PP1-complete with respect to many-one reductions for{a mathematical formula}k&gt;2and polynomial for{a mathematical formula}k≤2when domain size is given in unary notation.For {a mathematical formula}k≤2, results in the literature show how to count the number of satisfying models of a formula in polynomial time [132], [134].For {a mathematical formula}k&gt;2, membership is shown as the proof of Theorem 9. Hardness has been in essence proved by Beame et al. [8, Lemmas 3.8, 3.9]; our only contribution is to simplify their arguments by removing the need to enumerate the counting Turing machines. Take a Turing machine {a mathematical formula}M that solves a {a mathematical formula}#P1-complete problem in {a mathematical formula}Nm steps for an input consisting of N ones. By padding the input, we can always guarantee that {a mathematical formula}M runs in time linear in the input. To show this, consider that for the input sequence with N ones, we can generate another sequence {a mathematical formula}S(N) consisting of {a mathematical formula}f(N)=(2N+1)2m⌈log2⁡N⌉ ones. Because {a mathematical formula}(21+log2⁡N)m≥2m⌈log2⁡N⌉, we have {a mathematical formula}(2N+1)2mNm&gt;f(N), and consequently {a mathematical formula}S(N) can be generated in polynomial time. Modify {a mathematical formula}M so that the new machine proceeds as follows:
       <list>
        it receives {a mathematical formula}S(N);in linear time it produces the binary representation of {a mathematical formula}S(N), using an auxiliary tape{sup:9};it then discards the trailing zeroes to obtain {a mathematical formula}2N+1;it computes N;it writes N ones in its tape;and then it runs the original computation in {a mathematical formula}M.We use the Turing machine encoding described in the proof of
       </list>
       <paragraph>
        Theorem 8, but instead of using a single relation {a mathematical formula}Z(x,y) to indicate the head position at step {a mathematical formula}x, we use{a mathematical formula} with the understanding that for a fixed {a mathematical formula}x we have that {a mathematical formula}Zi,j(x,y) yields the position {a mathematical formula}y of the head in step {a mathematical formula}x and sub-step i, either in the main tape (tape 1) or in the auxiliary tape (tape 2). So, {a mathematical formula}Z1,j is followed by {a mathematical formula}Z2,j and so on until {a mathematical formula}ZM,j for a fixed step {a mathematical formula}x. Similarly, we use {a mathematical formula}Xqt(x), {a mathematical formula}Yσt,1(x,y) and {a mathematical formula}Yσt,2(x,y) for {a mathematical formula}t∈{1,…,M}. Definition axioms must be changed accordingly; for instance, we have{a mathematical formula} and{a mathematical formula} As another example, we can change {a mathematical formula}Z4 as follows. First, introduce auxiliary definition axioms:{a mathematical formula}{a mathematical formula} and then write:{a mathematical formula} Similar changes must be made to {a mathematical formula}Z7:{a mathematical formula} The changes to {a mathematical formula}Z5 and {a mathematical formula}Z6 are similar, but require more tedious repetition; we omit the complete expressions but explain the procedure. Basically, {a mathematical formula}Z5 and {a mathematical formula}Z6 encode the transitions of the Turing machine. So, instead of just taking the successor of a computation step {a mathematical formula}x, we must operate in substeps: the successor of step {a mathematical formula}x substep t is {a mathematical formula}x substep {a mathematical formula}t+1, unless {a mathematical formula}t=M (in which case we must move to the successor of {a mathematical formula}x, substep 1). We can also capture the behavior of the Turing machine with two transition functions, one per tape, and it is necessary to encode each one of them appropriately. It is enough to have M different versions of {a mathematical formula}Z5 and 2M different versions of {a mathematical formula}Z6, each one of them responsible for one particular substep transition.We must then encode the initial conditions. Introduce:{a mathematical formula} and{a mathematical formula} Finally, we must detect acceptance:{a mathematical formula}Now {a mathematical formula}P(Z9=1|⋀i=18{Zi=1})&gt;1/2 for a domain of size {a mathematical formula}f(N)+1 if and only if the number of interpretations that set an accepting state to {a mathematical formula}true is larger than half the total number of interpretations encoding computation paths.  □
       </paragraph>
      </paragraph>
      <paragraph label="Proof">
       Suppose relations have bounded arity.{a mathematical formula}INF[QF]and{a mathematical formula}QINF[QF]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[QF]requires constant computational effort. These results hold even if domain size is given in binary notation.Consider first {a mathematical formula}INF[QF]. To prove membership, take a relational Bayesian network specification {a mathematical formula}S with relations {a mathematical formula}X1,…,Xn, all with arity no larger than k. Suppose we ground this specification on a domain of size N. To compute {a mathematical formula}P(Q|E), the only relevant groundings are the ones that are ancestors of each of the ground atoms in {a mathematical formula}Q∪E. Our strategy will be to bound the number of such relevant groundings. To do that, take a grounding {a mathematical formula}Xi(a1,…,aki) in {a mathematical formula}Q∪E, and suppose that {a mathematical formula}Xi is not a root node in the parvariable graph. Each parent {a mathematical formula}Xj of {a mathematical formula}Xi in the parvariable graph may appear in several different forms in the definition axiom related to {a mathematical formula}Xi; that is, we may have {a mathematical formula}Xj(x2,x3),Xj(x9,x1),…, and each one of these combinations leads to a distinct grounding. There are in fact at most {a mathematical formula}kiki ways to select individuals from the grounding {a mathematical formula}Xi(a1,…,aki) so as to form groundings of {a mathematical formula}Xj. So for each parent of {a mathematical formula}Xi in the parvariable graph there will be at most {a mathematical formula}kk relevant groundings. And each parent of these parents will again have at most {a mathematical formula}kk relevant groundings; hence there are at most {a mathematical formula}(n−1)kk relevant groundings that are ancestors of {a mathematical formula}Xi(a1,…,aki). We can take the union of all groundings that are ancestors of groundings of {a mathematical formula}Q∪E, and the number of such groundings is still polynomial in the size of the input. Thus in polynomial time we can build a polynomially-large Bayesian network that is a fragment of the grounded Bayesian network. Then we can run a Bayesian network inference in this smaller network (an effort within {a mathematical formula}PP); note that domain size is actually not important so it can be specified either in unary or binary notation. To prove hardness, note that {a mathematical formula}INF[Prop(∧,¬)] is {a mathematical formula}PP-hard, and a propositional specification can be reproduced within {a mathematical formula}QF.Now consider {a mathematical formula}QINF[QF]. To prove membership, note that even {a mathematical formula}INF[QF] is in {a mathematical formula}PP. To prove hardness, take an instance of {a mathematical formula}#3SAT(&gt;) consisting of a sentence ϕ in 3CNF, with propositions {a mathematical formula}A1,…,An, and an integer k. Consider the relational Bayesian network specification consisting of eight definition axioms:{a mathematical formula} and {a mathematical formula}P(sat(x)=1)=1/2. Now the query is just a set of assignments Q (E is empty) containing an assignment per clause. If a clause is {a mathematical formula}¬A2∨A3∨¬A1, then take the corresponding assignment {a mathematical formula}{clause2(a2,a3,a1)=1}, and so on. The {a mathematical formula}#3SAT(&gt;) problem is solved by deciding whether {a mathematical formula}P(Q)&gt;k/2n with domain of size n; hence the desired hardness is proved.And {a mathematical formula}DINF[QF] requires constant effort: in fact, domain size is not relevant to a fixed inference, as can be seen from the proof of inferential complexity above.  □
      </paragraph>
     </section>
     <section label="A.3">
      Description logics (Section 6)
      <paragraph label="Proof">
       Suppose the domain size is specified in unary notation. Then{a mathematical formula}INF[ALC]and{a mathematical formula}QINF[ALC]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[ALC]is in{a mathematical formula}P.Because {a mathematical formula}ALC is in {a mathematical formula}FFFO2, both {a mathematical formula}INF[ALC] and {a mathematical formula}QINF[ALC] are in {a mathematical formula}PP by Theorem 11, Theorem 12, and {a mathematical formula}DINF[ALC] is in {a mathematical formula}P. A simple proof for {a mathematical formula}PP-hardness of {a mathematical formula}INF[ALC] and {a mathematical formula}QINF[ALC] can be extracted from the proof of Theorem 7: reproduce that proof, except that Expression (A.6) must be discarded and the inference now asks whether {a mathematical formula}P(clause(1)=1,…,clause(n)=1|E)&gt;1/2. This shows the desired hardness as Expression (A.5) belongs to {a mathematical formula}ALC.  □
      </paragraph>
      <paragraph label="Proof">
       Suppose the domain size is specified in unary notation. Then{a mathematical formula}INF[EL]and{a mathematical formula}QINF[EL]are{a mathematical formula}PP-complete with respect to many-one reductions, even if the query contains only positive assignments, and{a mathematical formula}DINF[EL]is in{a mathematical formula}P.{a mathematical formula}INF[EL] belongs to {a mathematical formula}PP by Theorem 11 as {a mathematical formula}EL belongs to {a mathematical formula}FFFO2. Hardness is obtained from hardness of query complexity.So, consider {a mathematical formula}QINF[EL]. Membership follows from membership of {a mathematical formula}INF[EL], so we focus on hardness. Our strategy is to reduce {a mathematical formula}#(1-in-3)SAT(&gt;) to {a mathematical formula}QINF[EL], using most of the construction in the proof of Theorem 2. So take a sentence ϕ in 3CNF with propositions {a mathematical formula}A1,…,An and m clauses, and an integer k. The goal is to decide whether {a mathematical formula}#(1-in-3)ϕ&gt;k. We can assume that no clause contains a repeated literal.We start by adapting several steps in the proof of Theorem 2. First, associate each literal with a random variable {a mathematical formula}Xij (where {a mathematical formula}Xij stands for a negated literal). In the present proof we use a parvariable {a mathematical formula}X(x); the idea is that {a mathematical formula}x is the integer {a mathematical formula}3(i−1)+j for some {a mathematical formula}i∈{1,…,n} and {a mathematical formula}j∈{1,2,3} (clearly we can obtain {a mathematical formula}(i,j) from {a mathematical formula}x and vice-versa). Then associate X with the assessment{a mathematical formula} where ε is exactly as in the proof for {a mathematical formula}INF[Prop(∨)].The next step in the proof of Theorem 2 is to introduce a number of definition axioms of the form {an inline-figure}, together with assignments {a mathematical formula}{Yiuv=1}. There are 3m such axioms. Then additional axioms are added to guarantee that configurations are sensible. Note that we can compute in polynomial time the total number of definition axioms that are to be created. We denote this number by N, as we will use it as the size of the domain. In any case, we can easily bound N: first, each clause produces 3 definition axioms as in Expression (A.2); second, to guarantee that configurations are sensible, every time a literal is identical to another literal, or identical to the negation of another literal, four definition axioms are inserted (there are 3m literals, and for each one there may be 2 identical/negated literals in the other {a mathematical formula}m−1 clauses). Thus we have that {a mathematical formula}N≤3m+4×3m×2(m−1)=24m2−21m. Suppose we order these definition axioms from 1 to N by some appropriate scheme.To encode these N definition axioms, we introduce two other parvariables {a mathematical formula}Y(x) and {a mathematical formula}Z(x,y), with definition axiom{a mathematical formula} and assessment{a mathematical formula} for some η to be determined later. The idea is this. We take a domain with size N, and for each {a mathematical formula}x from 1 to N, we set {a mathematical formula}Z(x,y) to 0 if {a mathematical formula}X(y) does not appear in the definition axiom indexed by {a mathematical formula}x, and we set {a mathematical formula}Z(x,y) to 1 if {a mathematical formula}X(y) appears in the definition axiom indexed by {a mathematical formula}x. We collect all these assignments in a set E. Note that E in effect “creates” all the desired definition axioms by selecting two instances of X per instance of Y.Note that if we enforce {a mathematical formula}{Y(x)=1} for all {a mathematical formula}x, we obtain the same construction used in the proof of Theorem 2, we one difference: in that proof we had 3m variables {a mathematical formula}Xij, while here we have N variables {a mathematical formula}X(x) (note that {a mathematical formula}N≥3m, and in fact {a mathematical formula}N&gt;3m for {a mathematical formula}m&gt;1).Consider grounding this relational Bayesian network specification and computing{a mathematical formula} This distribution is encoded by a Bayesian network with nodes {a mathematical formula}X(1),…,X(N) and nodes {a mathematical formula}Y(1),…,Y(N), where all nodes {a mathematical formula}Z(x,y) are removed as they are set by E; also, each node {a mathematical formula}Y(x) has two parents, and all nodes {a mathematical formula}X(3m+1),…,X(N) have no children. Denote by L a generic configuration of {a mathematical formula}X(1),…,X(3m), and by Q a configuration of {a mathematical formula}Y(1),…,Y(N) where all variables are assigned value 1. As in the proof of Theorem 2, we have {a mathematical formula}P(L)=α if L is gratifying-sensible-respectful, and {a mathematical formula}P(L)≤β if L is respectful but not gratifying. If {a mathematical formula}#(1-in-3)ϕ&gt;k, then {a mathematical formula}P(Q|E)=∑LP(L,Q)≥(k+1)α. And if {a mathematical formula}#(1-in-3)ϕ≤k, then {a mathematical formula}P(Q|E)≤kα+4mβ. Define {a mathematical formula}δ1=(k+1)α and {a mathematical formula}δ2=kα+4mβ and choose {a mathematical formula}ε&lt;1/(1+4m) to guarantee that {a mathematical formula}δ1&gt;δ2, so that we can differentiate between the two cases with an inference.We have thus solved our original problem using a fixed Bayesian network specification plus a query {a mathematical formula}(Q,E). Hence {a mathematical formula}PP-hardness of {a mathematical formula}QINF[EL] is obtained. However, note that Q contains only positive assignments, but E contains both positive and negative assignments. We now constrain ourselves to positive assignments.Denote by {a mathematical formula}E1 the assignments of the form {a mathematical formula}{Z(x,y)=1} in E, and denote by {a mathematical formula}E0 the assignments of the form {a mathematical formula}{Z(x,y)=0} in E. Consider:{a mathematical formula} where {a mathematical formula}E0c is the event consisting of configurations of those variables that appear in {a mathematical formula}E0 such that at least one of these variables is assigned 1 (of course, such variables are assigned 0 in {a mathematical formula}E0).We have that {a mathematical formula}P(Q|E0,E1)=P(Q|E) by definition. And variables in {a mathematical formula}E0 and {a mathematical formula}E1 are independent, hence {a mathematical formula}P(E0|E1)=P(E0)=(1−η)M where M is the number of variables in {a mathematical formula}E0 (so {a mathematical formula}M≤N2). Consequently, {a mathematical formula}P(E0c|E1)=1−(1−η)M. Thus we reach:{a mathematical formula}Now reason as follows. If {a mathematical formula}#(1-in-3)ϕ&gt;k, then {a mathematical formula}P(Q|E1)≥(1−η)Mδ1. And if {a mathematical formula}#(1-in-3)ϕ≤k, then {a mathematical formula}P(Q|E1)≤(1−(1−η)M)+(1−η)Mδ2. To guarantee that {a mathematical formula}(1−η)Mδ1&gt;(1−(1−η)M)+(1−η)Mδ2, we must have {a mathematical formula}(1−η)M&gt;1/(1+δ1−δ2). We do so by selecting η appropriately. Denote {a mathematical formula}1/(1+δ1−δ2) by {a mathematical formula}δ3, and note that {a mathematical formula}δ3∈(0,1) by our choice of ε. We must select η so that {a mathematical formula}1−η&gt;δ31/M; to guarantee this, we find a quantity {a mathematical formula}δ4 that is guaranteed to be larger than {a mathematical formula}δ31/M, and impose {a mathematical formula}1−η&gt;δ4. Note that {a mathematical formula}1+(x−1)/M&gt;x1/M for any {a mathematical formula}x∈(0,1), so take {a mathematical formula}δ4=1+(δ3−1)/M and impose{a mathematical formula} that is,{a mathematical formula} By doing so, we can differentiate between the two cases with an inference, so the desired hardness is proved.Domain complexity is polynomial because {a mathematical formula}EL is in {a mathematical formula}FFFO2[132], [134].  □
      </paragraph>
      <paragraph label="Theorem 17">
       Suppose the domain size is specified in unary notation. If in addition the query{a mathematical formula}(Q,E)contains only positive assignments, then{a mathematical formula}INF[DLLitenf]and{a mathematical formula}QINF[DLLitenf]and{a mathematical formula}DINF[DLLitenf]are in{a mathematical formula}P.
      </paragraph>
      <paragraph label="Proof">
       We prove the polynomial complexity of {a mathematical formula}INF[DLLitenf] with positive queries by a quadratic-time reduction to multiple problems of counting weighted edge covers with uniform weights in a particular class of graphs. Each one of these counting problems can be in turn solved in quadratic time; thus the whole effort is polynomial.We describe techniques to compute the probability of a set of positive assignments. One must first compute the probability of all assignments in {a mathematical formula}(Q,E), then the probability of assignments in E, and finally divide both numbers. From now on Q refers to a set of positive assignments whose probability is of interest; this set may include or be restricted to E as needed.We first simplify a bit the notation, as follows. Introduce {a mathematical formula}er(x)≡∃y:r(x,y) and {a mathematical formula}er−(x)≡∃y:r(y,x). Then replace each appearance of the formula {a mathematical formula}∃y:r(x,y) by {a mathematical formula}er(x) (or {a mathematical formula}∃x:r(y,x) by {a mathematical formula}er(y)), and each appearance of {a mathematical formula}∃y:r(y,x) by {a mathematical formula}er−(x) (or {a mathematical formula}∃x:r(x,y) by {a mathematical formula}er−(y)). This transformation allows us to easily refer to groundings of existentially quantified formulas as groundings of {a mathematical formula}er and {a mathematical formula}er−, respectively.Observe that only the nodes with assignments in Q and their ancestors are relevant for the computation of {a mathematical formula}P(Q), as every other node in the Bayesian network is barren [33]. Hence, we can assume without loss of generality that Q contains only leaves of the network. If Q contains only root nodes, then {a mathematical formula}P(Q) can be computed trivially as the product of marginal probabilities which are readily available from the specification. Thus assume that Q assigns a positive value to at least one non-root leaf grounding {a mathematical formula}s(a), where a is some individual in the domain. Suppose {a mathematical formula}s(a) is associated with a logical sentence {a mathematical formula}X1∧⋯∧Xk, where each {a mathematical formula}Xi is a grounding of a unary relation in a. It follows that {a mathematical formula}P(Q)=P(s(a)=1|X1=1,…,Xk=1)P(Q′)=P(Q′), where {a mathematical formula}Q′ is Q after removing the assignment {a mathematical formula}{s(a)=1} and adding the assignments {a mathematical formula}{X1=1,…,Xk=1}. The problem of computing {a mathematical formula}P(Q) boils down to computing {a mathematical formula}P(Q′). By repeating this procedure for all non-root nodes which are not groundings of {a mathematical formula}er or {a mathematical formula}er−, we end up with a set A containing positive assignments of groundings of relations, including of auxiliary relations {a mathematical formula}er and {a mathematical formula}er−. Each root node is marginally independent of all other groundings in A; hence {a mathematical formula}P(A)=P(B|C)∏iP(Ai), where each {a mathematical formula}Ai is an assignment to a root node, B are (positive) assignments to groundings of relations {a mathematical formula}er and {a mathematical formula}er− for relations {a mathematical formula}r, and {a mathematical formula}C⊆{A1,A2,…} are groundings of binary relations (if C is empty then assume it expresses a tautology). Because the marginal probabilities {a mathematical formula}P(Ai) are available from the specification, {a mathematical formula}∏iP(Ai) can be computed in linear time from the input. We thus focus on computing {a mathematical formula}P(B|C). To recap, B is a set of assignments {a mathematical formula}er(a)=1 and {a mathematical formula}er−(b)=1 and C is a set of assignments {a mathematical formula}r(c,d)=1 for arbitrary binary relations {a mathematical formula}r and individuals {a mathematical formula}a,b,c and d. Note that if B is empty, we are done.For a binary relation {a mathematical formula}r, let {a mathematical formula}Dr be the set of individuals {a mathematical formula}a∈D such that {a mathematical formula}er(a)=1 is in B, and let {a mathematical formula}Dr− be the set of individuals {a mathematical formula}a∈D such that B contains {a mathematical formula}er−(a)=1. Let {a mathematical formula}gr(r) be the set of all groundings of relation {a mathematical formula}r, and let {a mathematical formula}r1,…,rk be the binary relations in the (relational) network. By the factorization property of Bayesian networks it follows that{a mathematical formula} noting that we only write down the groundings from {a mathematical formula}gr(ri) that affect groundings {a mathematical formula}eri and {a mathematical formula}eri− (using the Markov condition on the grounded Bayesian network).By distributing the products over sums in the last expression, we obtain:{a mathematical formula}Now consider an assignment {a mathematical formula}r(a,b)=1 in C. By construction, the children of the grounding {a mathematical formula}r(a,b) are {a mathematical formula}er(a) and {a mathematical formula}er−(b). Moreover, the assignment {a mathematical formula}r(a,b)=1 implies that {a mathematical formula}P(er(a)=1|pa(er(a)),C)=1 (for any assignment to the other parents) and {a mathematical formula}P(er−(b)=1|pa(er(a)),C)=1 (for any assignment to the other parents). This is equivalent in the factorization above to removing {a mathematical formula}r(a,b) from C (as it is independent of all other groundings), and removing individuals a from {a mathematical formula}Dr and b from {a mathematical formula}Dr−. So repeat this procedure for every grounding in C until this set is empty (this can be done in polynomial time). The inference problem becomes one of computing{a mathematical formula} for every relation {a mathematical formula}ri, {a mathematical formula}i=1,…,k, with properly adapted sets {a mathematical formula}Dri and {a mathematical formula}Dri−. We will show that this problem can be reduced to a tractable instance of counting weighted edge covers. To this end, we need to introduce some notation and terminology that we also use in the next section.A black-and-white graph (bw-graph) is a triple {a mathematical formula}G=(V,E,χ) where {a mathematical formula}(V,E) is a simple undirected graph and {a mathematical formula}χ:V→{0,1} is a binary-valued function on the node set (assume 0 means white and 1 means black). Denote by {a mathematical formula}EG(u) the set of edges incident on a node {a mathematical formula}u∈V, and by {a mathematical formula}NG(u) the open neighborhood of u (i.e., not including u). An edge cover of a bw-graph is a subset of the edges {a mathematical formula}C⊆E such that for each black node u we have that {a mathematical formula}EG(u)∩C≠∅ (i.e., there is at least one edge incident in each black node). Denote by {a mathematical formula}EC(G) the set of edge covers of G. For any {a mathematical formula}C∈EC(G) and real {a mathematical formula}λ&gt;0, we say that {a mathematical formula}λ|C| is the weight of cover C. The partition function of a bw-graph G is the total sum of cover weights (a.k.a. the weighted edge cover counting): {a mathematical formula}Z(G,λ)=∑C∈EC(G)λ|C|. Note that {a mathematical formula}Z(G,1)=|EC(G)| counts the number of edge covers.We recast marginal inference in DL-Lite Bayesian networks as the computation of the partition function of the bw-graph {a mathematical formula}Gr=(V1∪V2∪V3∪V4,E,χ) such that:
       <list label="Lemma 1">
        The following result connects marginal inference and weighted edge cover counting:Let{a mathematical formula}Gr=(V1,V2,V3,V4,E)be the intersection graph ofBwith respect to a relation{a mathematical formula}rand domain{a mathematical formula}D. Then{a mathematical formula}γ(r)=Z(Gr,α/(1−α))/(1−α)|E|, where{a mathematical formula}α=P(r(x,y)).Consider an edge cover C of the graph. The assignment that sets to {a mathematical formula}true all groundings {a mathematical formula}r(a,b) corresponding to edges in C, and sets to {a mathematical formula}false the remaining groundings of {a mathematical formula}r makes{a mathematical formula} for every {a mathematical formula}a∈Dr and {a mathematical formula}b∈Dr−; it makes {a mathematical formula}P(gr(r))=P(r)|C|(1−P(r))|E|−|C|=(1−α)|E|α|C|/(1−α)|C|, which is the weight of the cover C scaled by {a mathematical formula}(1−α)|E|. Now consider a set of edges C which is not an edge cover and that yields an assignment to groundings {a mathematical formula}gr(r) as before. There is at least one node in {a mathematical formula}V2∪V3 that does not contain any incident edges in C. Assume that node is {a mathematical formula}e(a); then all parents of {a mathematical formula}e(a) are assigned false, which implies that {a mathematical formula}P(er(a)=1|pa(er(a)))=0. The same is true if the node not covered is a grounding {a mathematical formula}e−(a). Hence, for each edge cover C the probability of the corresponding assignment equals its weight up to the factor {a mathematical formula}(1−α)|E|. And for each edge set C which is not an edge cover its corresponding assignment has probability zero.  □We have thus established that, if a particular class of weighted edge cover counting problems is polynomial, then marginal inference in DL-Lite Bayesian networks is also polynomial for positive assignments in the input. Because the problem of weighted edge cover counting is of independent interest, we deal with it in the next subsection; by describing there a polynomial counting algorithm, we finish the proof of
       </list>
       <paragraph>
        Theorem 17.  □
       </paragraph>
      </paragraph>
      <paragraph label="Proof">
       Given a relational Bayesian network{a mathematical formula}Sbased on{a mathematical formula}DLLitenf, a set of positive assignments to grounded relationsE, and a domain size N in unary notation,{a mathematical formula}MLE(S,E,N)can be solved in polynomial time.In this theorem we are interested in finding an assignment X to all groundings that maximizes {a mathematical formula}P(X∧E), where E is a set of positive assignments. Perform the substitution of formulas {a mathematical formula}∃y:r(x,y) and {a mathematical formula}∃y:r(y,x) by logically equivalent concepts {a mathematical formula}er and {a mathematical formula}er− as in the proof of Theorem 17. Consider a non-root grounding {a mathematical formula}s(a) in E which is not the grounding of {a mathematical formula}er or {a mathematical formula}er−; by construction, {a mathematical formula}s(a) is logically equivalent to a conjunction {a mathematical formula}X1∧⋯∧Xk, where {a mathematical formula}X1,…,Xk are unary groundings. Because {a mathematical formula}s(a) is assigned to {a mathematical formula}true, any assignment X with nonzero probability assigns {a mathematical formula}X1,…,Xk to {a mathematical formula}true. Moreover, since {a mathematical formula}s(a) is a non-root node, its corresponding probability is one. Hence, if we include all the assignments {a mathematical formula}{Xi=1} to its parents in E, the MPE value does not change. Assume we repeat this procedure until E contains all ancestors of the original groundings that are groundings of unary relations. Note that at this point we only need to assign values to nodes that are either not ancestors of any node in the original set E, and to groundings of (collapsed) binary relations {a mathematical formula}r.Consider the groundings of primitive unary relations {a mathematical formula}r that are not ancestors of any grounding in E. Setting their value to maximize the marginal probability does not introduce any inconsistency with respect to E. Moreover, for any assignment to these groundings, we can find a consistent assignment to the remaining groundings (which are internal nodes and not ancestors of E), that is, an assignment which assigns positive probability. Because this is the maximum probability we can obtain for these groundings, this is a partial optimum assignment.We are thus only left with the problem of assigning values to the groundings of relations {a mathematical formula}r that are ancestors of E. Consider a relation {a mathematical formula}r such that {a mathematical formula}P(r)≥1/2. Then assigning all groundings of {a mathematical formula}r to {a mathematical formula}true maximizes their marginal probability and satisfies the logical equivalences of all groundings in E. Hence, this is a maximum assignment (and its value can be computed efficiently). So assume there is a relation {a mathematical formula}r with {a mathematical formula}P(r)&lt;1/2 such that a grounding of {a mathematical formula}er or {a mathematical formula}er− appear in E. In this case, the greedy assignment sets every grounding of {a mathematical formula}r; however, such an assignment is inconsistent with the logical equivalence of {a mathematical formula}er and {a mathematical formula}er−, hence it gets probability zero. Now consider an assignment that assigns exactly one grounding {a mathematical formula}r(a,b) to {a mathematical formula}true and all the other to {a mathematical formula}false. This assignment is consistent with {a mathematical formula}er(a) and {a mathematical formula}er(b), and maximizes the probability; any assignment that sets more groundings to {a mathematical formula}true has a lower probability since it replaces a term {a mathematical formula}1−P(r)≥1/2 with a term {a mathematical formula}P(r)&lt;1/2 in the joint probability. More generally, to maximize the joint probability we need to assign to {a mathematical formula}true as few groundings {a mathematical formula}r(a,b) which are ancestors of E as possible. This is equivalent to a minimum cardinality edge covering problem as follows.For every relation {a mathematical formula}r in the relational network, construct the bipartite complete graph {a mathematical formula}Gr=(V1,V2,E) such that {a mathematical formula}V1 is the set of groundings {a mathematical formula}er(a) that appears and has no parent {a mathematical formula}r(a,b) in E, and {a mathematical formula}V2 is the set of groundings {a mathematical formula}er−(a) that appears and has no parents in E. We identify an edge connecting {a mathematical formula}er(a) and {a mathematical formula}er−(b) with the grounding {a mathematical formula}r(a,b). For any set {a mathematical formula}C⊆E, construct an assignment by attaching {a mathematical formula}true to the groundings {a mathematical formula}r(a,b) in C and {a mathematical formula}false to every other grounding {a mathematical formula}r(a,b). This assignment is consistent with E if and only if C is an edge cover; hence the minimum cardinality edge cover maximizes the joint probability (it is consistent with E and attaches {a mathematical formula}true to the least number of groundings of r). This concludes the proof of Theorem 18.  □
      </paragraph>
     </section>
     <section label="A.4">
      <section-title>
       Counting edge covers in polynomial time
      </section-title>
      <paragraph>
       This section focuses on weighted edge cover counting as needed in the proof of Theorem 17. Even though there has been significant work in connecting model counting with graph-theoretical representations of formulas [11], [108], [128], [131], [140], the class of problems discussed here seems to have escaped these previous efforts.
      </paragraph>
      <paragraph>
       Recall the notation and terminology introduced right before Lemma 1: a black-and-white graph (bw-graph, for short) is a triple {a mathematical formula}G=(V,E,χ), where {a mathematical formula}G=(V,E) is a simple undirected graph and χ is a {a mathematical formula}{0,1}-valued function partitioning the node set into white ({a mathematical formula}χ(v)=0) and black nodes ({a mathematical formula}χ(v)=1); {a mathematical formula}EG(u) denotes the set of edges incident in a node u, and {a mathematical formula}NG(u) the open neighborhood of u. An edge {a mathematical formula}e=(u,v)∈E can be classified into one of three categories{sup:10}:
      </paragraph>
      <list>
       <list-item label="•">
        free edge: if {a mathematical formula}χ(u)=χ(v)=0;
       </list-item>
       <list-item label="•">
        dangling edge: if {a mathematical formula}χ(u)≠χ(v); or
       </list-item>
       <list-item label="•">
        regular edge: if {a mathematical formula}χ(u)=χ(v)=1.
       </list-item>
      </list>
      <paragraph>
       In the graph in Fig. A.16(b), the edge {a mathematical formula}(f,g) is a dangling edge while the edge {a mathematical formula}(g,j) is a free edge. The edge {a mathematical formula}(f,g) in the graph in Fig. A.16(a) is a regular edge.
      </paragraph>
      <paragraph>
       The set {a mathematical formula}EC(G) contains the edge covers of a bw-graph G, that is, the subsets {a mathematical formula}C⊆V such that for each black node {a mathematical formula}v∈V, {a mathematical formula}χ(v)=1, it follows that {a mathematical formula}C∩EG(v)≠∅. An edge cover for the graph in Fig. A.16(a) is {a mathematical formula}C={(a,d),(d,g),(e,g),(f,g),(h,j)}.
      </paragraph>
      <paragraph>
       We are interested in computing the partition function {a mathematical formula}Z(G,λ)=∑C∈EC(G)λ|C| of a certain class of bw-graphs G, given a fixed real {a mathematical formula}λ&gt;0. This problem is {a mathematical formula}#P-complete in general [17], and admits an FPTAS [79], [80]. To simplify notation, we assume in the rest of this section that λ is fixed, and we write Z as function of the graph alone.
      </paragraph>
      <paragraph>
       So consider the following class of graphs, collectively denoted by {a mathematical formula}B, that are clearly inspired by the contents of the previous proof. A bw-graph {a mathematical formula}G=(V,E,χ) in {a mathematical formula}B has a set of nodes that can be partitioned into four disjoint sets {a mathematical formula}V1, {a mathematical formula}V2, {a mathematical formula}V3, {a mathematical formula}V4, such that {a mathematical formula}V1∪V4 contain only white nodes, {a mathematical formula}V2∪V3 contain only black nodes, and for {a mathematical formula}i=1,2,3 the subgraph over nodes {a mathematical formula}Vi∪Vi+1 is bipartite complete.
      </paragraph>
      <paragraph>
       We now present a dynamic programming approach to computing the partition function of graphs in {a mathematical formula}B. As we have discussed elsewhere [88], a much larger class of graphs can be tackled by the following results or by various extensions or approximation techniques.
      </paragraph>
      <paragraph>
       Let e be an edge and u be a node in bw-graph G. We define the following operations and notation:
      </paragraph>
      <list>
       <list-item label="•">
        edge removal:{a mathematical formula}G−e=(V,E∖{e},χ).
       </list-item>
       <list-item label="•">
        node whitening:{a mathematical formula}G−u=(V,E,χ′), where {a mathematical formula}χ′(u)=0 and {a mathematical formula}χ′(v)=χ(v) for {a mathematical formula}v≠u.
       </list-item>
      </list>
      <paragraph>
       Note that these operations do not alter the node set, and that they are associative (e.g., {a mathematical formula}G−e−f=G−f−e, {a mathematical formula}G−u−v=G−v−u, and {a mathematical formula}G−e−u=G−u−e). Hence, if {a mathematical formula}E={e1,…,ed} is a set of edges, we can write {a mathematical formula}G−E to denote {a mathematical formula}G−e1−⋯−ed applied in any order. The same is true for node whitening and for any combination of node whitening and edge removal. These operations are illustrated in the examples in Fig. A.16.
      </paragraph>
      <paragraph label="Proposition 2">
       The following result shows that the partition function can be decomposed in terms of two simpler problems: Let{a mathematical formula}e=(u,v)be a dangling edge with u colored black. Then:{a mathematical formula}
      </paragraph>
      <paragraph label="Proof">
       Consider the graph {a mathematical formula}G′=G−e−u. Let A denote the set of edge covers of G that contain e and B denote the set of edge covers that do not contain e. Since A is exactly the set of edge covers of {a mathematical formula}G′ (after removing e) and B is exactly the set of edge covers of {a mathematical formula}G−e, we have that {a mathematical formula}Z(G)=∑C∈Aλ|C|+∑C∈Bλ|C|=λZ(G′)+Z(G−e). Similarly, let {a mathematical formula}A′ be the set of edge covers of {a mathematical formula}G′ that contain at least one edge of {a mathematical formula}EG′(u), and {a mathematical formula}B′ be the set of edge covers that contain no edge of {a mathematical formula}EG′(u). Then {a mathematical formula}Z(G′)=∑C∈A′λ|C|+∑C∈B′λ|C|=Z(G−e)+Z(G−EG(u)−u). Isolating the term {a mathematical formula}Z(G−e) and substituting for it in the first identity gives us the desired result.  □
      </paragraph>
      <paragraph label="Proposition 3">
       Free edges can be removed by adjusting the partition function accordingly: Let{a mathematical formula}e=(u,v)be a free edge of G. Then{a mathematical formula}Z(G)=(1+λ)Z(G−e).
      </paragraph>
      <paragraph label="Proof">
       If C is an edge cover of {a mathematical formula}G−e then both C and {a mathematical formula}C∪{e} are edge covers of G. Hence, the sum of the weights of edge covers containing e equals the sum of weights {a mathematical formula}Z(G−e) of edge covers not containing e up to a factor λ.  □
      </paragraph>
      <paragraph>
       We can use the formulas in Proposition 2, Proposition 3 to compute the partition function of a bw-graph recursively. Each recursion computes {a mathematical formula}Z(G) as a function of the partition function of two graphs obtained by the removal of edges and whitening of a node. Such a naive approach however requires an exponential number of recursions (in the number of edges or nodes of the initial graph) and finishes after exponential time. We can transform such an approach into a polynomial-time algorithm by exploiting the symmetries of the graphs produced during the recursions. In particular, we take advantage of the invariance of the partition function to isomorphisms of a graph, as we discuss next.
      </paragraph>
      <paragraph>
       We say that two bw-graphs {a mathematical formula}G=(V,E,χ) and {a mathematical formula}G′=(V′,E′,χ′) are isomorphic if there is a bijection γ from V to {a mathematical formula}V′ (or vice-versa) such that (i) {a mathematical formula}χ(v)=χ′(γ(v)) for all {a mathematical formula}v∈V, and (ii) {a mathematical formula}(u,v)∈E if and only if {a mathematical formula}(γ(u),γ(v))∈E′. In other words, two bw-graphs are isomorphic if there is a color-preserving renaming of nodes that preserves the binary relation induced by E. The function γ is called an isomorphism from V to {a mathematical formula}V′. The graphs in Figs. A.16(b) and A.16(c) are isomorphic by an isomorphism that maps g to h and maps any other node to itself. If C is an edge cover of G and γ is an isomorphism between G and {a mathematical formula}G′, then {a mathematical formula}C′={(γ(u),γ(v)):(u,v)∈C} is an edge cover for {a mathematical formula}G′ with the same weight. Hence, {a mathematical formula}Z(G)=Z(G′). The following result shows how to obtain isomorphic graphs with a combination of node whitenings and edge removals.
      </paragraph>
      <paragraph label="Proposition 4">
       Consider a bw-graph G with nodes{a mathematical formula}v1,…,vn, where{a mathematical formula}NG(v1)=⋯=NG(vn)≠∅and{a mathematical formula}χG(v1)=⋯=χG(vn). For any node{a mathematical formula}w∈NG(v1), bijection{a mathematical formula}γ:{v1,…,vn}→{v1,…,vn}, and nonnegative integers{a mathematical formula}k1and{a mathematical formula}k2such that{a mathematical formula}k1+k2≤nthe graphs{a mathematical formula}G′=G−EG(v1)−⋯−EG(vk1)−(w,vk1+1)−⋯−(w,vk1+k2)−v1−⋯−vk1+k2and{a mathematical formula}G″=G−EG(γ(v1))−⋯−EG(γ(vk1))−(w,γ(vk1+1))−⋯−(w,γ(vk1+k2))−γ(v1)−⋯−γ(vk1+k2)are isomorphic.
      </paragraph>
      <paragraph label="Proof">
       Let {a mathematical formula}γ′ be the bijection on the nodes of G that extends γ, that is, {a mathematical formula}γ′(u)=u for {a mathematical formula}u∉{v1,…,vn} and {a mathematical formula}γ′(u)=γ(vi), for {a mathematical formula}i=1,…,n. We will show that {a mathematical formula}γ′ is an isomorphism from {a mathematical formula}G′ to {a mathematical formula}G″. First note that {a mathematical formula}χG(u)=χG(γ(u)) for every node u. The only nodes that have their color (possibly) changed in {a mathematical formula}G′ with respect to G are the nodes {a mathematical formula}v1,…,vk1+k+2, and these are white nodes in {a mathematical formula}G′. Likewise, the only nodes that would (possibly) changed color in {a mathematical formula}G″ were {a mathematical formula}γ(v1),…,γ(vk1+k2) and these are white in {a mathematical formula}G″. Hence, {a mathematical formula}χG′(u)=χG″(γ(u)) for every node u.Now let us look at the edges. First note that since {a mathematical formula}NG(vi) is constant through {a mathematical formula}i=1,…,n, {a mathematical formula}G′ and {a mathematical formula}G″ have the same number of edges. Hence, it suffices to show that for each edge {a mathematical formula}(u,v) in {a mathematical formula}G′ the edge {a mathematical formula}(γ′(u),γ′(v)) is in {a mathematical formula}G″. The only edges modified in obtaining {a mathematical formula}G′ and {a mathematical formula}G″ are, respectively, those incident in {a mathematical formula}v1,…,vk1+k2 and in {a mathematical formula}γ(v1),…,γ(vk1+k2). Consider an edge {a mathematical formula}(u,v) where {a mathematical formula}u,v∉{v1,…,vn} (hence not in {a mathematical formula}EG(vi) for any i). If {a mathematical formula}(u,v)=(γ′(u),γ′(v)) is in {a mathematical formula}G′ then it is also in {a mathematical formula}G″. Now consider an edge {a mathematical formula}(u,vi) in G where {a mathematical formula}u∉{w,vk1+1,…,vn} and {a mathematical formula}k1&lt;i≤k1+k2. Then {a mathematical formula}(u,vi) is in {a mathematical formula}G′ and {a mathematical formula}(γ′(u),γ′(vi)) is in {a mathematical formula}G″. Note that u could be in {a mathematical formula}NG(vi) for {a mathematical formula}k1+k2&lt;i≤n.  □
      </paragraph>
      <paragraph>
       According to the proposition above, the graphs in Figs. A.16(b) and A.16(c) are isomorphic by a bijection between g and h (and with {a mathematical formula}w=i). Hence, the partition function of either graph is the same.
      </paragraph>
      <paragraph>
       The algorithms {a mathematical formula}RightRecursion and {a mathematical formula}LeftRecursion described in Fig. A.17, Fig. A.18, respectively, exploit the isomorphisms described in Proposition 4 in order to achieve polynomial-time behavior when using the recursions in Proposition 2, Proposition 3. Either algorithm requires a base white node w and integers {a mathematical formula}k1 and {a mathematical formula}k2 specifying the recursion level (with the same meaning as in Proposition 4). Unless {a mathematical formula}k1+k2 equals the number of neighbors of w in the original graph, a call to either algorithm generates two more calls to the same algorithm: one with the graph obtained by removing edge {a mathematical formula}(w,vh) and whitening {a mathematical formula}vh, and another by removing edges {a mathematical formula}E(vh) and whitening {a mathematical formula}vh. Assume that {a mathematical formula}|V2|≥|V3| (if {a mathematical formula}|V3|&gt;|V2| we can simply manipulate node sets to obtain an isomorphic graph satisfying the assumption). The {a mathematical formula}RightRecursion algorithm first checks whether the value for the current recursion level has been already computed; if yes, then it simply returns the cached value; otherwise it uses the formula in Proposition 2 (and possibly the isomorphism in Proposition 4) and generates two calls of the same algorithm on smaller graphs (i.e. with fewer edges) to compute the partition function for the current graph and stores the result in memory. The recursion continues until the recursion levels equates with the number of nodes in {a mathematical formula}V3, in which case it checks for free edges, removes them and computes the correction factor {a mathematical formula}(1+λ)k, where k is the number of free edges, and calls the algorithm {a mathematical formula}LeftRecursion to start a new recursion. At this point the graph in the input is bipartite complete and contains only nodes in {a mathematical formula}V1 and {a mathematical formula}V2. The latter algorithm behaves very similarly to the former except at the termination step. When all neighbors {a mathematical formula}vh of w have been whitened the graph no longer contains black nodes, and the corresponding partition function can be directly computed using the formulas in Proposition 3. Note that a different cache function must be used when we call {a mathematical formula}LeftRecursion from {a mathematical formula}RightRecursion (this can be done by instantiating an object at that point and passing it as argument; we avoid stating the algorithm is this way to avoid cluttering).
      </paragraph>
      <paragraph>
       Fig. A.19 shows the recursion diagram of a run of {a mathematical formula}RightRecursion. Each box in the figure represents a call of the algorithm with the corresponding graph as input. The left child of each box is the call {a mathematical formula}RightRecursion(G−(vh,w)−vh,w,k1,k2+1), and the right child is the call {a mathematical formula}RightRecursion(G−EG(vh)−vh,w,k1+1,k2). The number of the graph in each box corresponds to the order in which each call was generated. Solid arcs represent non-cached calls, while dotted arcs indicate cached calls. The recursion diagram for {a mathematical formula}LeftRecursion is shown in Fig. A.20 (the semantics is analogous). Note that the recursion of {a mathematical formula}LeftRecursion eventually reaches a graph with no black nodes, for which the partition function can be computed efficiently in closed-form.
      </paragraph>
      <paragraph>
       Without the caching of computations, the algorithm would perform exponentially many recursive calls (and its corresponding diagram would be a binary tree with exponentially many nodes). The use of caching allows us to compute only one call of {a mathematical formula}RightRecursion for each configuration of {a mathematical formula}k1,k2 such that {a mathematical formula}k1+k2≤n, resulting in at most {a mathematical formula}∑i=0n(i+1)=(n+1)(n+2)/2=O(n2) calls for {a mathematical formula}RightRecursion, where {a mathematical formula}n=|V3|. Similarly, each call of {a mathematical formula}LeftRecursion requires at most {a mathematical formula}∑i=0m(i+1)=(m+1)(m+2)/2=O(m2) recursive calls for {a mathematical formula}LeftRecursion, where {a mathematical formula}m=|V2|. Each call to {a mathematical formula}RightRecursion with {a mathematical formula}k1+k2=n generates a call to {a mathematical formula}LeftRecursion (there are {a mathematical formula}n+1 such configurations). Hence, the overall number of recursions (i.e., call to either function) is{a mathematical formula} This leads us to the following result.
      </paragraph>
      <paragraph label="Proof">
       Let G be a graph in{a mathematical formula}Bwith{a mathematical formula}w∈V4≠∅. Then the call{a mathematical formula}RightRecursion(G,w,0,0)outputs{a mathematical formula}Z(G)in time and memory at most cubic in the number of nodes of G.Except when {a mathematical formula}k1+k2=n, {a mathematical formula}RightRecursion calls the recursion given in Proposition 2 with the isomorphisms in Proposition 4 (any graph obtained from G by {a mathematical formula}k1 operations {a mathematical formula}−EG(vi) and {a mathematical formula}k2 operations {a mathematical formula}−(w,vi) are isomorphic). For {a mathematical formula}k1+k2, any edge left connecting a node in {a mathematical formula}V3 and a node in {a mathematical formula}V4 must be a free edge (since all nodes in {a mathematical formula}V4 have been whitened), hence they can be removed according to Proposition 3 with the appropriate correction of the partition function. By the same result, any isolated node can be removed. When the remaining nodes in {a mathematical formula}V3 are transfered to {a mathematical formula}V1, the resulting graph is bipartite complete (with white nodes in one part and black nodes in the other). Hence, we can call {a mathematical formula}LeftRecursion, which is guaranteed to compute the correct value by the same arguments.The cubic time and space behavior is due to {a mathematical formula}RightRecursion and {a mathematical formula}LeftRecursion being called at most {a mathematical formula}O(n2) and {a mathematical formula}O(nm2), respectively, and by the fact that each call consists of local operations (edge removals and node whitenings) which take at most linear time in the number of nodes and edges of the graph.  □
      </paragraph>
      <paragraph>
       The algorithm {a mathematical formula}RightRecursion requires the existence of a dangling edge. Now it might be that the graph contains no dangling edges; this happens when {a mathematical formula}V1 and {a mathematical formula}V4 are empty. The next result shows how to decompose the computation of the partition function into smaller graphs that either contain dangling edges (so that we can apply the previous algorithm), or are also bipartite complete.
      </paragraph>
      <paragraph label="Proposition 6">
       Let G be a bipartite complete bw-graph with all nodes colored black and{a mathematical formula}e=(u,v)be some edge. Then{a mathematical formula}Z(G)=(1+λ)Z(G−e−u−v)−Z(G−EG(v)−v)−Z(G−EG(u)−u)−Z(G−EG(u)−EG(v)−u−v).
      </paragraph>
      <paragraph label="Proof">
       The edge covers of G can be partitioned according to whether they contain the edge e: the weight of edge covers that contain e equal the weight of edge covers that do not contain e up to the factor λ. Thus, {a mathematical formula}Z(G)=λZ(G−e−u−v)+Z(G−e). Consider an edge cover C of {a mathematical formula}G−e−u−v, and let {a mathematical formula}A=EG−e(u)∩C and {a mathematical formula}B=EG−e(v)∩C. We have four cases: if {a mathematical formula}A⊃∅ and {a mathematical formula}B⊂∅, then C is also an edge cover for {a mathematical formula}G−e with the same weight in either graph; conversely, if C is an edge cover for {a mathematical formula}G−e, then it is also an edge cover for {a mathematical formula}G−e−u−v (with equal weight). If {a mathematical formula}A⊃∅ and {a mathematical formula}B=∅ then C is an edge cover for {a mathematical formula}G−EG(v)−v with same weight; and any edge cover for {a mathematical formula}G−EG(v)−v is an edge cover for {a mathematical formula}G−e−u−v with equal weight. If {a mathematical formula}A=∅ and {a mathematical formula}B⊃∅ then C is an edge cover for {a mathematical formula}G−EG(u)−u with equal weight; and any edge cover for {a mathematical formula}G−EG(u)−u is an edge cover for {a mathematical formula}G−e−u−v (same weight). Finally, if {a mathematical formula}A=B=∅, then C is an edge cover for {a mathematical formula}G−EG(u)−EG(v)−u−v, and any cover for this latter graph is an edge cover for {a mathematical formula}G−e−u−v. We thus have that {a mathematical formula}Z(G−e−u−v)=Z(G−e)+Z(G−EG(v)−v)+Z(G−EG(u)−u)+Z(G−EG(u)−EG(v)−u−v). Substituting {a mathematical formula}Z(G−e) into the first equation leads to the desired result.  □
      </paragraph>
      <paragraph>
       In the result above, the graph {a mathematical formula}G−e−u−v contains dangling edges, while the graphs {a mathematical formula}G−EG(v)−v, {a mathematical formula}G−EG(u)−u and {a mathematical formula}G−EG(u)−EG(v)−u−v are bipartite complete. Proposition 4 can be applied to show that altering the edges on which the operations are applied lead to isomorphic graphs. Therefore, a very similar algorithm to {a mathematical formula}LeftRecursion, implementing the recursion in the result above in polynomial time can be easily derived.
      </paragraph>
      <paragraph label="Proposition 7">
       Let G be a graph in{a mathematical formula}B(possibly with{a mathematical formula}V1=V4=∅. Then{a mathematical formula}Z(G)can be computed in time and memory at most polynomial in the number of nodes of G.
      </paragraph>
     </section>
     <section label="A.5">
      Plates (Section 7)
      <paragraph label="Proof">
       {a mathematical formula}INF[PLATE]and{a mathematical formula}QINF[PLATE]are{a mathematical formula}PP-complete with respect to many-one reductions, and{a mathematical formula}DINF[PLATE]requires constant computational effort. These results hold even if the domain size is given in binary notation.Consider first {a mathematical formula}INF[PLATES]. To prove membership, take a plate model with relations {a mathematical formula}X1,…,Xn. Suppose we ground this specification on a domain of size N. To compute {a mathematical formula}P(Q|E), the only relevant groundings are the ones that are ancestors of each of the ground atoms in {a mathematical formula}Q∪E. Our strategy will be to bound the number of such relevant groundings. To do that, take a grounding {a mathematical formula}Xi(a1,…,aki) in {a mathematical formula}Q∪E, and suppose that {a mathematical formula}Xi is not a root node. Each parent {a mathematical formula}Xj of {a mathematical formula}Xi may appear once in the definition axiom related to {a mathematical formula}Xi. And each parent of these parents will again have a limited number of parent groundings; in the end there are at most {a mathematical formula}(n−1) relevant groundings that are ancestors of {a mathematical formula}Xi(a1,…,aki). We can take the union of all groundings that are ancestors of groundings of {a mathematical formula}Q∪E, and the number of such groundings is still polynomial in the size of the input. Thus in polynomial time we can build a polynomially-large Bayesian network that is a fragment of the grounded Bayesian network. Then we can run a Bayesian network inference in this smaller network, an effort within {a mathematical formula}PP; note that each random variable may actually have more than two values and membership to {a mathematical formula}PP is still obtained (that is, if we were to consider “relations” with values other than {a mathematical formula}true and {a mathematical formula}false, the grounding into a Bayesian network would still yield an inference problem within {a mathematical formula}PP). Note also that domain size is actually not important so it can be specified either in unary or binary notation. To prove hardness, note that {a mathematical formula}INF[Prop(∧,¬)] is {a mathematical formula}PP-hard, and a propositional specification can be reproduced within {a mathematical formula}PLATES.Now consider {a mathematical formula}QINF[PLATES]. First, to prove membership, note that even {a mathematical formula}INF[PLATES] is in {a mathematical formula}PP. To prove hardness, reproduce the proof of Theorem 14 by encoding a {a mathematical formula}#3SAT(&gt;) problem, specified by sentence ϕ and integer k, with the definition axioms:{a mathematical formula} and {a mathematical formula}P(left(x)=1)=P(middle(x)=1)=P(right(x)=1)=1/2. The resulting plate model is depicted in Fig. A.21. The query is again just a set of assignments Q (E is empty) containing an assignment per clause. If a clause is {a mathematical formula}¬A2∨A3∨¬A1, then take the corresponding assignment {a mathematical formula}{clause2(a2,a3,a1)=1}, and so on. Moreover, add the assignments {a mathematical formula}{equal(ai,ai,ai)=1} for each {a mathematical formula}i∈{1,…,n}, to guarantee that {a mathematical formula}left, {a mathematical formula}middle and {a mathematical formula}right have identical truth assignments for all elements of the domain. The {a mathematical formula}#3SAT(&gt;) is solved by deciding whether {a mathematical formula}P(Q)&gt;k/2n with domain of size n; hence the desired hardness is proved.And {a mathematical formula}DINF[PLATES] requires constant effort: in fact, domain size is not relevant to a fixed inference, as can be seen from the proof of inferential complexity above.  □
      </paragraph>
     </section>
     <section label="A.6">
      Valiant's counting hierarchy (Section 8)
      <paragraph label="Proof">
       Consider the class of functions that gets as input a relational Bayesian network specification based on{a mathematical formula}FFFO, a domain size N (in binary or unary notation), and a set of assignmentsQ, and returns{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#EXP-equivalent.Build a relational Bayesian network specification as in the proof of Theorem 4. Note that the {a mathematical formula}p=P(E∧⋀i=16Zi) is the probability that a tiling of the torus is built satisfying all horizontal and vertical restrictions and the initial condition, and moreover containing the accepting state {a mathematical formula}qa.If we can recover the number of tilings of the torus from this probability, we obtain the number of accepting computations of the exponential-time Turing machine we started with. Assume we have p. There are {a mathematical formula}22n elements in our domain; if the plate model is grounded, there are {a mathematical formula}22n(2n+c) grounded root random variables, hence there are {a mathematical formula}222n(2n+c) interpretations. Hence {a mathematical formula}p×222n(2n+c) is the number of truth assignments that build the board satisfying all horizontal and vertical constraints and the initial conditions. However, this number is not equal to the number of tilings of the board. To see this, consider the grounded Bayesian network where each a in the domain is associated with a “slice” containing groundings {a mathematical formula}Xi(a), {a mathematical formula}Yi(a), {a mathematical formula}Cj(a) and so on. If a particular configuration of these indicator variables corresponds to a tiling, then we can produce the same tiling by permuting all elements of the domain with respect to the slices of the network. Intuitively, we can fix a tiling and imagine that we are labeling each point of the torus with an element of the domain; clearly every permutation of these labels produces the same tiling (this intuition is appropriate because each a corresponds to a different point in the torus). So, in order to produce the number of tilings of the torus, we must compute {a mathematical formula}p×222n(2n+c)/(22n!), where we divide the number of satisfying truth assignments by the number of repeated tilings.  □
      </paragraph>
      <paragraph label="Proof">
       Consider the class of functions that gets as input a relational Bayesian network specification based on{a mathematical formula}FFFOwith relations with bounded arity, a domain size N in unary notation, and a set of assignmentsQ, and returns{a mathematical formula}P(Q). This class of functions is{a mathematical formula}♮PSPACE-equivalent.First we describe a counting Turing machine that produces a count proportional to {a mathematical formula}P(Q) using a polynomial number of nondeterministic guesses. This nondeterministic machine guesses a truth assignment for each one of the polynomially-many grounded root nodes (and writes the guess in the working tape). Note that each grounded root node X is associated with an assessment {a mathematical formula}P(X=1)=c/d, where c and d are integers. The machine must replicate its computation paths to handle such rational assessments exactly as in the proof of Theorem 7. The machine then verifies, in each computation path, whether the guessed truth assignment satisfies Q; if it does, then accept; if not, then reject. Denote by R the number of grounded root nodes and by #A the number of accepting paths of this machine; then {a mathematical formula}P(Q)=#A/2R.Now we show that Q is {a mathematical formula}♮PSPACE-hard with respect to weighted reductions. Define {a mathematical formula}φ(x1,…,xm) to be a quantified Boolean formula with free logvars {a mathematical formula}x1,…,xm:{a mathematical formula} where each logvar can only be {a mathematical formula}true or {a mathematical formula}false, each {a mathematical formula}Qj is a quantifier (either ∀ or ∃). And define #φ to be the number of instances of {a mathematical formula}x1,…,xm such that {a mathematical formula}φ(x1,…,xm) is {a mathematical formula}true. Denote by {a mathematical formula}♮QBF the function that gets a formula {a mathematical formula}φ(x1,…,xm) and returns #φ; Ladner shows that {a mathematical formula}♮QBF is {a mathematical formula}♮PSPACE-complete [75, Theorem 5(2)]. So, adapt the hardness proof of Theorem 10: introduce the definition axiom{a mathematical formula} where {a mathematical formula}ϕ′ has the same structure of ϕ but logvars are replaced as follows. First, each {a mathematical formula}xj is replaced by a relation {a mathematical formula}Xj of arity zero (that is, a proposition). Second, each logvar {a mathematical formula}yj is replaced by the atom {a mathematical formula}X(yj) where X is a fresh unary relation. These relations are associated with assessments {a mathematical formula}P(Xj=1)=1/2 and {a mathematical formula}P(X(x)=1)=1/2. This completes the relational Bayesian network specification. Now for domain {a mathematical formula}{0,1}, first compute {a mathematical formula}P(Q) for {a mathematical formula}Q={Y=1,X(0)=0,X(1)=1} and then compute {a mathematical formula}2m(P(Q)/(1/4)). The latter number is the desired value of {a mathematical formula}♮QBF; note that {a mathematical formula}P(Q)/(1/4)=P(Y=1|X(0)=0,X(1)=1).  □
      </paragraph>
      <paragraph label="Proof">
       Consider the class of functions that gets as input a relational Bayesian network specification based on{a mathematical formula}FFFOkfor{a mathematical formula}k≥2, a domain size N in unary notation, and a set of assignmentsQ, and returns{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#P-equivalent.Hardness is trivial: even {a mathematical formula}Prop(∧,¬) is {a mathematical formula}#P-equivalent, as {a mathematical formula}Prop(∧,¬) suffices to specify any propositional Bayesian network, and equivalence is then obtained [113]. To prove membership, use the Turing machine described in the proof of membership in Theorem 11 without assignments E (that is, the machine only processes Q), without the final computations in Park's construction. This machine produces the number #A of computation paths that satisfy Q; then return {a mathematical formula}#A/2R, where R is the number of grounded root nodes.  □
      </paragraph>
      <paragraph label="Proof">
       Consider the class of functions that get as input a plate model based on{a mathematical formula}FFFO, a domain size N (either in binary or unary notation), and a set of assignmentsQ, and returns{a mathematical formula}P(Q). This class of functions is{a mathematical formula}#P-equivalent.Hardness is trivial: a propositional Bayesian network can be encoded with a plate model. To prove membership, build the same fragment of the grounded Bayesian network as described in the proof of Theorem 19: inference with the plate model is then reduced to inference with this polynomially large Bayesian network.  □
      </paragraph>
     </section>
    </section>
   </appendices>
  </root>
 </body>
</html>