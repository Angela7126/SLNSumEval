<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Learning in the machine: Random backpropagation and the deep learning channel.
   </title>
   <abstract>
    Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric weights in a physical neural system. To better understand random backpropagation, we first connect it to the notions of local learning and learning channels. Through this connection, we derive several alternatives to RBP, including skipped RBP (SRBP), adaptive RBP (ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their computational complexity. We then study their behavior through simulations using the MNIST and CIFAR-10 benchmark datasets. These simulations show that most of these variants work robustly, almost as well as backpropagation, and that multiplication by the derivatives of the activation functions is important. As a follow-up, we study also the low-end of the number of bits required to communicate error information over the learning channel. We then provide partial intuitive explanations for some of the remarkable properties of RBP and its variations. Finally, we prove several mathematical results for RBP and its variants including: (1) the convergence to optimal fixed points for linear chains of arbitrary length; (2) convergence to fixed points for linear autoencoders with decorrelated data; (3) long-term existence of solutions for linear systems with a single hidden layer, and their convergence in special cases; and (4) convergence to fixed points of non-linear chains, when the derivative of the activation functions is included.
   </abstract>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      Over the years, the question of the biological plausibility of the backpropagation algorithm, which implements stochastic gradient descent in neural networks, has been raised several times. The question has gained further relevance due to the numerous successes achieved by backpropagation in a variety of problems ranging from computer vision [21], [31], [30], [14] to speech recognition [12] in engineering, and from high energy physics [7], [26] to biology [8], [32], [1] in the natural sciences, as well to recent results on the optimality of backpropagation [6]. There are however, several well known issues facing biological neural networks in relation to backpropagation, these include: (1) the continuous real-valued nature of the gradient information and its ability to change sign, violating Dale's Law; (2) the need for some kind of teacher's signal to provide targets; (3) the need for implementing all the linear operations involved in backpropagation; (4) the need for multiplying the backpropagated signal by the derivatives of the forward activations each time a layer is traversed; (5) the need for precise alternation between forward and backward passes; and (6) the complicated geometry of biological neurons and the problem of transmitting error signals with precision down to individual synapses. However, perhaps the most formidable obstacle is that the standard backpropagation algorithm requires propagating error signals backwards using synaptic weights that are identical to the corresponding forward weights. Furthermore, a related problem that has not been sufficiently recognized, is that this weight symmetry must be maintained at all times during learning, and not just during early neural development. It is hard to imagine mechanisms by which biological neurons could both create and maintain such perfect symmetry. However, recent simulations [24] surprisingly indicate that such symmetry may not be required after all, and that in fact backpropagation works more or less as well when random weights are used to backpropagate the errors. Our general goal here is to investigate backpropagation with random weights and better understand why it works.
     </paragraph>
     <paragraph>
      The foundation for better understanding random backpropagation (RBP) is provided by the concepts of local learning and deep learning channels introduced in [6]. Thus we begin by introducing the notations and connecting RBP to these concepts. In turn, this leads to the derivation of several alternatives to RBP, which we study through simulations on well known benchmark datasets before proceeding with more formal analyses.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Setting, notations, and the learning channel
     </section-title>
     <paragraph>
      Throughout this paper, we consider layered feedforward neural networks and supervised learning tasks. We will denote such an architecture by{a mathematical formula} where {a mathematical formula}N0 is the size of the input layer, {a mathematical formula}Nh is the size of hidden layer h, and {a mathematical formula}NL is the size of the output layer. We assume that the layers are fully connected and let {a mathematical formula}wijh denote the weight connecting neuron j in layer {a mathematical formula}h−1 to neuron i in layer h. The output {a mathematical formula}Oih of neuron i in layer h is computed by:{a mathematical formula} The transfer functions {a mathematical formula}fih are usually the same for most neurons, with typical exceptions for the output layer, and usually are monotonic increasing functions. The most typical functions used in artificial neural networks are the: identity, logistic, hyperbolic tangent, rectified linear, and softmax.
     </paragraph>
     <paragraph>
      We assume that there is a training set of M examples consisting of input and output-target pairs {a mathematical formula}(I(t),T(t)), with {a mathematical formula}t=1,…,M. {a mathematical formula}Ii(t) refers to the i-th component of the t-th input training example, and similarly for the target {a mathematical formula}Ti(t). In addition, there is an error function {a mathematical formula}E to be minimized by the learning process. In general we will assume standard error functions such as the squared error in the case of regression and identity transfer functions in the output layer, or relative entropy in the case of classification with logistic (single class) or softmax (multi-class) units in the output layer, although this is not an essential point.
     </paragraph>
     <paragraph>
      While we focus on supervised learning, it is worth noting that several “unsupervised” learning algorithms for neural networks (e.g. autoencoders, neural autoregressive distribution estimators, generative adversarial networks) come with output targets and thus fall into the framework used here.
     </paragraph>
     <section label="2.1">
      <section-title>
       Standard backpropagation (BP)
      </section-title>
      <paragraph>
       Standard backpropagation implements gradient descent on {a mathematical formula}E, and can be applied in a stochastic fashion on-line (or in mini batches) or in batch form, by summing or averaging over all training examples. For a single example, omitting the t index for simplicity, the standard backpropagation learning rule is easily obtained by applying the chain rule and given by:{a mathematical formula} where η is the learning rate, {a mathematical formula}Ojh−1 is the presynaptic activity, and {a mathematical formula}Bih is the backpropagated error. Using the chain rule, it is easy to see that the backpropagated error satisfies the recurrence relation:{a mathematical formula} with the boundary condition:{a mathematical formula} Thus in short the errors are propagated backwards in an essentially linear fashion using the transpose of the forward matrices, hence the symmetry of the weights, with a multiplication by the derivative of the corresponding forward activations every time a layer is traversed.
      </paragraph>
     </section>
     <section label="2.2">
      <section-title>
       Standard random backpropagation (RBP)
      </section-title>
      <paragraph>
       Standard random backpropagation operates exactly like backpropagation except that the weights used in the backward pass are completely random and fixed. Thus the learning rule becomes:{a mathematical formula} where the randomly backpropagated error satisfies the recurrence relation:{a mathematical formula} and the weights {a mathematical formula}ckih+1 are random and fixed. The boundary condition at the top remains the same:{a mathematical formula} Thus in RBP the weights in the top layer of the architecture are updated by gradient descent, identically to the BP case.
      </paragraph>
     </section>
     <section label="2.3">
      <section-title>
       The critical equations
      </section-title>
      <paragraph>
       Within the supervised learning framework considered here, the goal is to find an optimal set of weights {a mathematical formula}wijh. The equations that the weights must satisfy at any critical point are simply:{a mathematical formula} Thus in general the optimal weights must depend on both the input and the targets, as well as the other weights in the network. And learning can be viewed as a lossy storage procedure for transferring the information contained in the training set into the weights of the architecture.
      </paragraph>
      <paragraph>
       The critical Equation (9) shows that all the necessary forward information about the inputs and the lower weights leading up to layer {a mathematical formula}h−1 is subsumed by the term {a mathematical formula}Ojh−1(t). Thus in this framework a separate channel for communicating information about the inputs to the deep weights is not necessary. Thus here we focus on the feedback information about the targets, contained in the term {a mathematical formula}Bih(t) which, in a physical neural system, must be transmitted through a dedicated channel.
      </paragraph>
      <paragraph>
       Note that {a mathematical formula}Bih(t) depends on the output {a mathematical formula}OL(t), the target {a mathematical formula}T(t), as well as all the weights in the layers above h in the fully connected case (otherwise just those weight which are on a path from unit i in layer h to the output units), and in two ways: through {a mathematical formula}OL(t) and through the backpropagation process. In addition, {a mathematical formula}Bih(t) depends also on all the upper derivatives, i.e. the derivatives of the activations functions for all the neurons above unit i in layer h in the fully connected case (otherwise just those derivatives which are on a path from unit i in layer h to the output units). Thus in general, in a solution of the critical equations, the weights {a mathematical formula}wijhmust depend on {a mathematical formula}Ojh−1, the outputs, the targets, the upper weights, and the upper derivatives. Backpropagation shows that it is sufficient for the weights to depend on {a mathematical formula}Ojh−1, {a mathematical formula}T−O, the upper weights, and the upper derivatives.
      </paragraph>
     </section>
     <section label="2.4">
      <section-title>
       Local learning
      </section-title>
      <paragraph>
       Ultimately, for optimal learning, all the information required to reach a critical point of {a mathematical formula}E must appear in the learning rule of the deep weights. In a physical neural system, learning rules must also be local [6], in the sense that they can only involve variables that are available locally in both space and time, although for simplicity here we will focus only on locality in space. Thus typically, in the present formalism, a local learning rule for a deep layer must be of the form{a mathematical formula} and{a mathematical formula} assuming that the targets are local variables for the top layer. Among other things, this allows one to organize and stratify learning rules, for instance by considering polynomial learning rules of degree one, two, and so forth.
      </paragraph>
      <paragraph>
       Deep local learning is the term we use to describe the use of local learning in all the adaptive layers of a feedforward architecture. Note that Hebbian learning [15] is a form of local learning, and deep local learning has been proposed for instance by Fukushima [10] to train the neocognitron architecture, essentially a feed forward convolutional neural network inspired by the earlier neurophysiological work of Hubel and Wiesel [18]. However, in deep local learning, information about the targets is not propagated to the deep layers and therefore in general deep local learning cannot find solutions of the critical equations, and thus cannot succeed at learning complex functions [6].
      </paragraph>
     </section>
     <section label="2.5">
      <section-title>
       The deep learning channel
      </section-title>
      <paragraph>
       From the critical equations, any optimal neural network learning algorithm must be capable of communicating some information about the outputs, the targets, and the upper weights to the deep weights and, in a physical neural system, a communication channel [28], [27] must exist to communicate this information. This is the deep learning channel, or learning channel in short [6], which can be studied using tools from information and complexity theory. In physical systems the learning channel must correspond to a physical channel and this leads to important considerations regarding its nature, for instance whether it uses the forward connections in the reverse direction or a different set of connections. Here, we focus primarily on how information is coded and sent over this channel.
      </paragraph>
      <paragraph>
       In general, the information about the outputs and the targets communicated through this channel to {a mathematical formula}wijh is denoted by {a mathematical formula}Iijh(T,OL). Although backpropagation propagates this information from the top layer to the deep layers in a staged way, this is not necessary and {a mathematical formula}Iijh(T,OL) could be sent directly to the deep layer h somehow skipping all the layers above. This observation leads immediately to the skipped variant of RBP described in the next section. It is also important to note that in principle this information should have the form {a mathematical formula}Iijh(T,OL,wrslforl&gt;h,f′(Srl)forl≥h). However standard backpropagation shows that it is possible to send the same information to all the synapses impinging onto the same neuron, and thus it is possible to learn with a simpler type of information of the form {a mathematical formula}Iih(T,OL,wrslforl&gt;h,f′(Srl)forl≥h) targeting the postsynaptic neuron i. This class of algorithms or channels is what we call deep targets algorithms, as they are equivalent to providing a target for each deep neuron. Furthermore, backpropagation shows that all the necessary information about the outputs and the targets is contained in the term {a mathematical formula}T−OL so that we only need {a mathematical formula}Iih(T−OL,wrslforl≥h,f′(Srl)forl&gt;h). Standard backpropagation uses information about the upper weights in two ways: (1) through the output {a mathematical formula}OL which appears in the error terms {a mathematical formula}T−OL; and (2) through the backpropagation process itself. Random backpropagation crucially shows that the information about the upper weights contained in the backpropagation process is not necessary. Thus ultimately we can focus exclusively on information which has the simple form: {a mathematical formula}Iih(T−OL,rrslforl≥h,f′(Srl)forl≥h), where r denotes a set of fixed random weights.
      </paragraph>
      <paragraph>
       Thus, using the learning channel, we are interested in local learning rules of the form:{a mathematical formula} In fact, here we shall focus exclusively on learning rules with the multiplicative form:{a mathematical formula} corresponding to a product of the presynaptic activity with some kind of backpropagated error information, with standard BP and RBP as a special cases. Obvious important questions, for which we will seek full or partial answers, include: (1) what kinds of forms can {a mathematical formula}Iih(T−OL,rrslforl≥h,f′(Srl)forl≥h) take (as we shall see there are multiple possibilities)? (2) what are the corresponding tradeoffs among these forms, for instance in terms of computational complexity or information transmission? and (3) are the upper derivatives necessary and why?
      </paragraph>
     </section>
    </section>
    <section label="3">
     <section-title>
      Random backpropagation algorithms and their computational complexity
     </section-title>
     <paragraph>
      We are going to focus on algorithms where the information required for the deep weight updates {a mathematical formula}Iih(T−OL,f′(Srl) forl≥h) is produced essentially through a linear process whereby the vector {a mathematical formula}T(t)−O(t), computed in the output layer, is processed through linear operations, i.e. additions and multiplications by constants (which can include multiplication by the upper derivatives). Standard backpropagation is such an algorithm, but there are many other possible ones. We are interested in the case where the matrices are random. However, even within this restricted setting, there are several possibilities, depending for instance on: (1) whether the information is progressively propagated through the layers (as in the case of BP), or broadcasted directly to the deep layers; (2) whether multiplication by the derivatives of the forward activations is included or not; and (3) the properties of the matrices in the learning channel (e.g. sparse vs dense). This leads to several new algorithms. Here we will use the following notations:
     </paragraph>
     <list>
      <list-item label="•">
       BP = (standard) backpropagation.
      </list-item>
      <list-item label="•">
       RBP = random backpropagation, where the transpose of the feedforward matrices are replaced by random matrices.
      </list-item>
      <list-item label="•">
       SRBP = skipped random backpropagation, where the backpropagated signal arriving onto layer h is given by {a mathematical formula}Ch(T−O) with a random matrix {a mathematical formula}Ch directly connecting the output layer L to layer h, and this for each layer h.
      </list-item>
      <list-item label="•">
       ARBP = adaptive random backpropagation, where the matrices in the learning channel are initialized randomly, and then progressively adapted during learning using the product of the corresponding forward and backward signals, so that {a mathematical formula}Δcrsl=ηRsl+1Orl, where R denotes the randomly backpropagated error. In this case, the forward channel becomes the learning channel for the backward weights.
      </list-item>
      <list-item label="•">
       ASRBP = adaptive skipped random backpropagation, which combines adaptation with skipped random backpropagation.
      </list-item>
      <list-item label="•">
       The default for each algorithm involves the multiplication at each layer by the derivative of the forward activation functions. The variants where this multiplication is omitted will be denoted by: “(no {a mathematical formula}f′)”.
      </list-item>
      <list-item label="•">
       The default for each algorithm involves dense random matrices, generated for instance by sampling from a normalized Gaussian for each weight. But one can consider also the case of random ±1 (or ({a mathematical formula}0,1)) binary matrices, or other distributions, including sparse versions of the above.
      </list-item>
      <list-item label="•">
       As we shall see, using random weights that have the same sign as the forward weights is not essential, but can lead to improvements in speed and stability. Thus we will use the word “congruent weights” to describe this case. Note that with fixed random matrices in the learning channel initialized congruently, congruence can be lost during learning when the sign of a forward weight changes.
      </list-item>
     </list>
     <paragraph>
      SRBP is introduced both for information theoretic reasons – what happens if the error information is communicated directly? – and because it may facilitate the mathematical analyses since it avoids the backpropagation process. However, in one of the next sections, we will also show empirically that SRBP is a viable learning algorithm, which in practice can work even better than RBP. Importantly, these simulation results suggest that when learning the synaptic weight{a mathematical formula}wijhthe information about all the upper derivatives ({a mathematical formula}f′(Srl)forl≥h) is not needed. However the immediate{a mathematical formula}(l=h)derivative{a mathematical formula}f′(Sih)is needed.
     </paragraph>
     <paragraph>
      Note this suggests yet another possible algorithm, skipped backpropagation (SBP). In this case, for each training example and at each epoch, the matrix used in the feedback channel is the product of the corresponding transposed forward matrices, ignoring multiplication by the derivative of the forward transfer functions in all the layers above the layer under consideration. Multiplication by the derivative of the forward transfer functions is applied to the layer under consideration. Another possibility is to have a combination of RBP and SRBP in the learning channel, implemented by a combination of long-ranged connections carrying SRBP signals with short-range connections carrying a backpropagation procedure, when no long-range signals are available. This may be relevant for biology since combinations of long-ranged and short-ranged feedback connections is common in biological neural systems.
     </paragraph>
     <paragraph>
      In general, in the case of linear networks, {a mathematical formula}f′=1 and therefore including or excluding derivative terms makes no difference. Furthermore, for any linear architecture {a mathematical formula}A[N,…,N,…,N] where all the layers have the same size, then RBP is equivalent to SRBP. However, if the layers do not have the same size, then the layer sizes introduce rank constraints on the information that is backpropagated through RBP that may differ from the information propagated through SRBP. In both the linear and non-linear cases, for any network of depth 3 ({a mathematical formula}L=3), RBP is equivalent to SRBP, since there is only one random matrix.
     </paragraph>
     <paragraph>
      Additional variations can be obtained by using dropout, or multiple sets of random matrices, in the learning channel, for instance for averaging purposes. Another variation in the skipped case is cascading, i.e. allowing backward matrices in the learning channel between all pairs of layers. Note that the notion of cascading increases the number of weights and computations, yet it is still interesting from an exploratory and robustness point of view.
     </paragraph>
     <section label="3.1">
      <section-title>
       Computational complexity considerations
      </section-title>
      <paragraph>
       The number of computations required to send error information over the learning channel is a fundamental quantity which, however, depends on the computational model used and the cost associated with various operations. Obviously, everything else being equal, the computational cost of BP and RBP are basically the same since they differ only by the value of the weights being used. However more subtle differences can appear with some of the other algorithms, such as SRBP.
      </paragraph>
      <paragraph>
       To illustrate this, consider an architecture {a mathematical formula}A[N0,…,Nh,…,NL], fully connected, and let W be the total number of weights. In general, the primary cost of BP is the multiplication of each synaptic weight by the corresponding signal in the backward pass. Thus it is easy to see that the bulk of the operations required for BP to compute the backpropagated signals scale like {a mathematical formula}O(W) (in fact {a mathematical formula}Θ(W)) with:{a mathematical formula} Note that whether biases are added separately or, equivalently, implemented by adding a unit clamped to one to each layer, does not change the scaling. Likewise, adding the costs associated with the sums computed by each neuron and the multiplications by the derivatives of the activation functions does not change the scaling, as long as these operations have costs that are within a constant multiplicative factor of the cost for multiplications of signals by synaptic weights.
      </paragraph>
      <paragraph>
       As already mentioned, the scaling for RBP is obviously the same, just using different matrices. However the corresponding term for SRBP is given by{a mathematical formula} In this sense, the computational complexity of BP and SRBP is identical if all the layers have the same size, but it can be significantly different otherwise, especially taking into consideration the tapering off associated with most architectures used in practice. In a classification problem, for instance, {a mathematical formula}NL=1 and all the random matrices in SRBP have rank 1, and {a mathematical formula}W′ scales like the total number of neurons, rather than the total number of forward connections. Thus, provided it leads to effective learning, SRBP could lead to computational savings in a digital computer. However, in a physical neural system, in spite of these savings, the scaling complexity of BP and SRBP could end up being the same. This is because in a physical neural system, once the backpropagated signal has reached neuron i in layer h it still has to be communicated to the synapse. A physical model would have to specify the cost of such communication. Assuming one unit cost, both BP and SRBP would require {a mathematical formula}Θ(W) operations across the entire architecture. Finally, a full analysis in a physical system would have to take into account also costs associated with wiring, and possibly differential costs between long and short wires as, for instance, SRBP requires longer wires than standard BP or RBP.
      </paragraph>
     </section>
    </section>
    <section label="4">
     <section-title>
      Algorithm simulations
     </section-title>
     <paragraph>
      In this section, we simulate the various algorithms using standard benchmark datasets. The primary focus is not on achieving state-of-the-art results, but rather on better understanding these new algorithms and where they break down. The results are summarized in Table 1 at the end.
     </paragraph>
     <section label="4.1">
      <section-title>
       MNIST
      </section-title>
      <paragraph>
       Several learning algorithms were first compared on the MNIST [22] classification task. The neural network architecture consisted of 784 inputs, four fully-connected hidden layers of 100 tanh units, followed by 10 softmax output units. Weights were initialized by sampling from a scaled normal distribution [11]. Training was performed for 100 epochs using mini-batches of size 100 with an initial learning rate of 0.1, decaying by a factor of {a mathematical formula}10−6 after each update, and no momentum. In Fig. 1, the performance of each algorithm is shown on both the training set (60,000 examples) and test set (10,000 examples). Results for the adaptive versions of the random propagation algorithms are shown in Fig. 2, and results for the sparse versions are shown in Fig. 3, Fig. 4.
      </paragraph>
      <paragraph>
       The main conclusion is that the general concept of RBP is very robust and works almost as well as BP. Performance is unaffected or degrades gracefully when the random backwards weights are initialized from different distributions or even change during training. The skipped versions of the algorithms seem to work slightly better than the non-skipped versions. Very deep networks can be trained with SRBP without any problem (not shown). Finally, RBP and its variants can be used with different neuron activation functions. Multiplication by the derivatives of the activation functions associated with the layer being updated, which are locally available, seems to play an important role.
      </paragraph>
     </section>
     <section label="4.2">
      <section-title>
       Additional MNIST experiments
      </section-title>
      <paragraph>
       In addition to the experiments presented above, the following observations were made by training on MNIST with other variations of these algorithms:
      </paragraph>
      <list>
       <list-item label="1.">
        If the matrices of the learning channel in RBP are randomly changed at each stochastic mini-batch update, sampled from a distribution with mean 0, performance is poor and similar to training only the top layer.
       </list-item>
       <list-item label="2.">
        If the matrices of the learning channel in RBP are randomly changed at each stochastic mini-batch update, but each backwards weight is constrained to have the same sign as the corresponding forward weight, then training error goes to 0%. This is the sign-concordance algorithm explored by Liao et al. [23].
       </list-item>
       <list-item label="3.">
        If the elements of the matrices of the learning channel in RBP or SRBP are sampled from a uniform or normal distribution with non-zero mean, performance is unchanged. This is also consistent with the sparsity experiments above, where the means of the sampling distributions are not zero.
       </list-item>
       <list-item label="4.">
        Updates to a deep layer with RBP or SRBP appear to require updates in the precedent layers in the learning channel. If we fix the weights in layer h, while updating the rest of the layers with SRBP, performance is often worse than if we fix layers {a mathematical formula}l≤h.
       </list-item>
       <list-item label="5.">
        If we remove the magnitude information from the SRBP updates, keeping only the sign, performance is better than the Top Layer Only algorithm, but not as good as SRBP. This is further explored in the next section.
       </list-item>
       <list-item label="6.">
        If we remove the sign information from the SRBP updates, keeping only the absolute value, things do not work at all.
       </list-item>
       <list-item label="7.">
        If a different random backward weight is used to send an error signal to each individual weight, rather than to a hidden neuron which then updates all its incoming weights, things do not work at all.
       </list-item>
       <list-item label="8.">
        The RBP learning rules work with different transfer functions as well, including linear, logistic, and ReLU (rectified linear) units.
       </list-item>
      </list>
     </section>
     <section label="4.3">
      <section-title>
       CIFAR-10
      </section-title>
      <paragraph>
       To further test the validity of these results, we performed similar simulations with a convolutional architecture on the CIFAR-10 dataset [20]. The specific architecture was based on previous work [16], and consisted of 3 sets of convolution and max-pooling layers, followed by a densely-connected layer of 1024 tanh units, then a softmax output layer. The input consists of 32-by-32 pixel 3-channel images; each convolution layer consists of 64 tanh channels with {a mathematical formula}5×5 kernel shape and {a mathematical formula}1×1 strides; max-pooling layers have {a mathematical formula}3×3 receptive fields and {a mathematical formula}2×2 strides. All weights were initialized by sampling from a scaled normal distribution [11], and updated using stochastic gradient descent on mini-batches of size 128 and a momentum of 0.9. The learning rate started at 0.01 and decreased by a factor of {a mathematical formula}10−5 after each update. During training, the training images are randomly translated up to 10% in either direction, horizontally and vertically, and flipped horizontally with probability {a mathematical formula}p=0.5.
      </paragraph>
      <paragraph>
       Examples of results obtained with these 2D convolutional architectures are shown in Fig. 5, Fig. 6. Overall they are very similar to those obtained on the MNIST dataset.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Bit precision in the learning channel
     </section-title>
     <section label="5.1">
      <section-title>
       Low-precision error signals
      </section-title>
      <paragraph>
       In the following experiment, we investigate the nature of the learning channel by quantizing the error signals in the BP, RBP, and SRBP algorithms. This is distinct from other work that uses quantization to reduce computation [17] or memory [13] costs. Quantization is not applied to the forward activations or weights; quantization is only applied to the backpropagated signal received by each hidden neuron, {a mathematical formula}Jih(T−OL), where each weight update after quantization is given by{a mathematical formula}{a mathematical formula} where {a mathematical formula}(fih)′ is the derivative of the activation function and{a mathematical formula} in the non-quantized update. We define the quantization formula used here as{a mathematical formula} where {a mathematical formula}bits is the number of bits needed to represent {a mathematical formula}2bits possible values and α is a scale factor such that the quantized values fall in the range {a mathematical formula}[−α,α]. Note that this definition is identical to the quantization function defined in Hubara et al. [17], except that this definition is more general in that α is not constrained to be a power of 2.
      </paragraph>
      <paragraph>
       In BP and RBP, the quantization occurs before the error signal is backpropagated to previous layers, so the quantization errors accumulate. In experiments, we used a fixed scale parameter {a mathematical formula}α=2−3 and varied the bit width {a mathematical formula}bits. Fig. 7 shows that the performance degrades gracefully as the precision of the error signal decreases to small values; for larger values, e.g. {a mathematical formula}bits=10, the performance is indistinguishable from the unquantized updates with 32-bit floats.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       Low-precision weight updates
      </section-title>
      <paragraph>
       The idea of using low-precision weight updates is not new [25], and Liao et al. [23] recently explored the use of low-precision updates with RBP. In the following experiment, we investigate the robustness of both RBP and SRBP to low-precision weight updates by controlling the degree of quantization. Equation (19) is again used for quantization, with the scale factor reduced to {a mathematical formula}α=2−6 since weight updates need to be small. The quantization is applied after the error signals have been backpropagated to all the hidden layers, but before summing over the minibatch; as in the previous experiments, we use minibatch updates of size 100, a non-decaying learning rate of 0.1, and no momentum term (Fig. 8). The main conclusion is that even very low-precision updates to the weights can be used to train an MNIST classifier to 90% accuracy, and that low-precision weight updates appear to degrade the performance of BP, RBP, and SRBP in roughly the same way.
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Observations
     </section-title>
     <paragraph>
      In this section, we provide a number of simple observations that provide some intuition for some of the previous simulation results and why RBP and some of its variations may work. Some of these observations are focused on SRBP which in general is easier to study than standard RBP.
     </paragraph>
     <paragraph label="Fact 1">
      In all these RBPs algorithms, the L-layer at the top with parameters {a mathematical formula}wijL follows the gradient, as it is trained just like BP, since there are no random feedback weights used for learning in the top layer. In other words, BP = RBP = SRBP for the top layer.
     </paragraph>
     <paragraph label="Fact 2">
      For a given input, if the sign of {a mathematical formula}T−O is changed, all the weights updates are changed in the opposite direction. This is true of all the algorithms considered here – BP, RBP, and their variants – even when the derivatives of the activations are included.
     </paragraph>
     <paragraph label="Fact 3">
      In all RBP algorithms, if {a mathematical formula}T−O=0 (online or in batch mode) then for all the weights {a mathematical formula}Δwijh=0 (on line or in batch mode).
     </paragraph>
     <paragraph label="Fact 4">
      Congruence of weights is not necessary. However it can be helpful sometimes and speed up learning. This can easily be seen in simple cases. For instance, consider a linear or non-linear {a mathematical formula}A[N0,N1,1] architecture with coherent weights, and denote by a the weights in the bottom layer, by b the weights in the top layer, and by c the weights in the learning channel. Then, for all variants of RBP, all the weights updates are in the same direction as the gradient. This is obvious for the top layer (Fact 1 above). For the first layer of weights, the changes are given by {a mathematical formula}Δwij1=η(T−O)ciIj, which is very similar to the change produced by gradient descent {a mathematical formula}Δij1=η(T−O)biIj since {a mathematical formula}ci and {a mathematical formula}bi are assumed to be coherent. So while the dynamics of the lower layer is not exactly in the gradient direction, it is always in the same orthant as the gradient and thus downhill with respect to the error function. Additional examples showing the positive but not necessary effect of coherence are given in Section 7.
     </paragraph>
     <paragraph label="Fact 5">
      SRBP seems to perform well showing that the upper derivatives are not needed. However the derivative of the corresponding layer seem to matter. In general, for the activation functions considered here, these derivatives tend to be between 0 and 1. Thus learning is attenuated for neurons that are saturated. So an ingredient that seems to matter is to let the synapses of neurons that are not saturated change more than the synapses of neurons that are saturated ({a mathematical formula}f′ close to 0).
     </paragraph>
     <paragraph label="Fact 6">
      Thus, in conclusion, an intuitive picture of why RBP may work is that: (1) the random weights introduce a fixed coupling between the learning dynamics of the forward weights (see also mathematical analyses below); (2) the top layer of weights always follows gradient descent and stirs the learning dynamic in the right direction; and (3) the learning dynamic tends to cluster inputs associated with the same response and move them away from other similar clusters. Next we discuss a possible connection to dropout.
     </paragraph>
     <section label="6.1">
      <section-title>
       Connections to dropout
      </section-title>
      <paragraph>
       Dropout [16], [5] is a very different training algorithm which, however, is also based on using some form of randomness. Here we explore some possible connections to RBP.
      </paragraph>
      <paragraph>
       First observe that the BP equations can be viewed as a form of dropout averaging equations, in the sense that, for a fixed example, they compute the ensemble average activity of all the units in the learning channel. The ensemble average is taken over all the possible backpropagation networks where each unit is dropped stochastically, unit i in layer h being dropped with probability {a mathematical formula}1−f′(Soh) [assuming the derivatives of the transfer functions are always between 0 and 1 inclusively, which is the case for the standard transfer functions, such as the logistic or the rectified linear transfer functions – otherwise some rescaling is necessary]. Note that in this way the dropout probabilities change with each example and units that are more saturated are more likely to be dropped, consistently with the remark above that saturated units should learn less.
      </paragraph>
      <paragraph>
       In this view there are two kinds of noise: (1) choice of the dropout probabilities which vary with each example; (2) the actual dropout procedure. Consider now adding a third type of noise on all the symmetric weights in the backward pass in the form{a mathematical formula} and assume for now that {a mathematical formula}E(ξijh)=0. The distribution of the noise could be Gaussian for instance, but this is not essential. The important point is that the noise on a weight is independent of the noise on the other weights, as well as independent of the dropout noise on the units. Under these assumptions, as shown in [5], the expected value of the activity of each unit in the backward pass is exactly given by the standard BP equations and equal to {a mathematical formula}Bih for unit i in layer h. In other words, standard backpropagation can be viewed as computing the exact average over all backpropagation processes implemented on all the stochastic realizations of the backward network under the three forms of noise described above. Thus we can reverse this argument and consider that RBP approximates this average or BP by averaging over the first two kinds of noise, but not the third one where, instead of averaging, a random realization of the weights is selected and then fixed at all epochs. This connection suggests other intermediate RBP variants where several samples of the weights are used, rather than a single one.
      </paragraph>
      <paragraph>
       Finally, it is possible to use dropout in the backward pass. The forward pass is robust to dropping out neurons, and in fact the dropout procedure can be beneficial [16], [5]. Here we apply the dropout procedure to neurons in the learning channel during the backward pass. The results of simulations are reported in Fig. 9 and confirm that BP, RBP, SRBP, are robust with respect to dropout.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Mathematical analysis
     </section-title>
     <section label="7.1">
      <section-title>
       General considerations
      </section-title>
      <paragraph>
       The general strategy to try to derive more precise mathematical results is to proceed from simple architectures to more complex architectures, and from the linear case to the non-linear case. The linear case is more amenable to analysis and, in this case, RBP and SRBP are equivalent when there is only one hidden layer, or when all the layers have the same size. Thus we study the convergence of RBP to optimal solutions in linear architectures of increasing complexity: {a mathematical formula}A[1,1,1], {a mathematical formula}A[1,1,1,1], {a mathematical formula}A[1,1,…,1], {a mathematical formula}A[1,N,1]{a mathematical formula}A[N,1,N], and then the general {a mathematical formula}A[N0,N1,N2] case with a single hidden layer. This is followed by the study of a non-linear {a mathematical formula}A[1,1,1] case.
      </paragraph>
      <paragraph>
       For each kind of linear network, under a set of standard assumptions, one can derive a set of non-linear – in fact polynomial – autonomous, ordinary differential equations (ODEs) for the average (or batch) time evolution of the synaptic weights under the RBP or SRBP algorithm. As soon as there is more than one variable and the system is non-linear, there is no general theory to understand the corresponding behavior. In fact, even in two dimensions, the problem of understanding the upper bound on the number and relative position of the limit cycles of a system of the form {a mathematical formula}dx/dt=P(x,y) and {a mathematical formula}dy/dt=Q(x,y), where P and Q are polynomials of degree n is open – in fact this is Hilbert's 16-th problem in the field of dynamical systems [29], [19].
      </paragraph>
      <paragraph>
       When considering the specific systems arising from the RBP/SRBP learning equations, one must first prove that these systems have a long-term solution. Note that polynomial ODEs may not have long-term solutions (e.g. {a mathematical formula}dx/dt=xα, with {a mathematical formula}x(0)≠0, does not have long-term solutions for {a mathematical formula}α&gt;1). If the trajectories are bounded, then long-term solutions exist. We are particularly interested in long-term solutions that converge to a fixed point, as opposed to limit cycles or other behaviors.
      </paragraph>
      <paragraph>
       A number of interesting cases can be reduced to polynomial differential equations in one dimension. These can be understood using the following theorem.
      </paragraph>
      <paragraph label="Theorem 1">
       Let{a mathematical formula}dx/dt=Q(x)=k0+k1x+…+knxnbe a first order polynomial differential equation in one dimension of degree{a mathematical formula}n&gt;1, and let{a mathematical formula}r1&lt;r2…&lt;rk({a mathematical formula}k≤n) be the ordered list of distinct real roots of Q (the fixed points). If{a mathematical formula}x(0)=rithen{a mathematical formula}x(t)=riand the solution is constant If{a mathematical formula}ri&lt;x(0)&lt;ri+1then{a mathematical formula}x(t)→riif{a mathematical formula}Q&lt;0in{a mathematical formula}(ri,ri+1), and{a mathematical formula}x(t)→ri+1if{a mathematical formula}Q&gt;0in{a mathematical formula}(ri,ri+1). If{a mathematical formula}x(0)&lt;r1and{a mathematical formula}Q&gt;0in the corresponding interval, then{a mathematical formula}x(t)→r1. Otherwise, if{a mathematical formula}Q&lt;0in the corresponding interval, there is no long time solution and{a mathematical formula}x(t)diverges to −∞ within a finite horizon. If{a mathematical formula}x(0)&gt;rkand{a mathematical formula}Q&lt;0in the corresponding interval, then{a mathematical formula}x(t)→rk. Otherwise, if{a mathematical formula}Q&gt;0in the corresponding interval, there is no long time solution and{a mathematical formula}x(t)diverges to +∞ within a finite horizon. A necessary and sufficient condition for the dynamics to always converge to a fixed point is that the degree n be odd, and the leading coefficient be negative.
      </paragraph>
      <paragraph label="Proof">
       The proof of this theorem is straightforward and can be visualized by plotting the function Q.Finally, in general the matrices in the forward channel are denoted by {a mathematical formula}A1,A2,…, and the matrices in the learning channel are denoted by {a mathematical formula}C1,C2,… Theorems are stated in concise form and additional important facts are contained in the proofs.
      </paragraph>
     </section>
     <section label="7.2">
      The simplest linear chain: {a mathematical formula}A[1,1,1]
      <paragraph label="Derivation of the system">
       The simplest case correspond to a linear {a mathematical formula}A[1,1,1] architecture (Fig. 10). Let us denote by {a mathematical formula}a1 and {a mathematical formula}a2 the weights in the first and second layer, and by {a mathematical formula}c1 the random weight of the learning channel. In this case, we have {a mathematical formula}O(t)=a1a2I(t) and the learning equations are given by:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}α=E(IT) and {a mathematical formula}β=E(I2). With the proper scaling of the learning rate ({a mathematical formula}η=Δt) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1 and {a mathematical formula}a2 during learning:{a mathematical formula} Note that the dynamic of {a mathematical formula}P=a1a2 is given by:{a mathematical formula} The error is given by:{a mathematical formula} and:{a mathematical formula} the last equality requires {a mathematical formula}ai≠0.
      </paragraph>
      <paragraph label="Theorem 2">
       The system in Equation(23)always converges to a fixed point. Furthermore, except for trivial cases associated with{a mathematical formula}c1=0, starting from any initial conditions the system converges to a fixed point corresponding to a global minimum of the quadratic error function. All the fixed points are located on the hyperbolas given by{a mathematical formula}α−βP=0and are global minima of the error function. All the fixed points are attractors except those that are interior to a certain parabola. For any starting point, the final fixed point can be calculated by solving a cubic equation.
      </paragraph>
      <paragraph label="Proof">
       As this is the first example, we first deal with the trivial cases in detail. For subsequent systems, we will skip the trivial cases entirely.Trivial cases: 1) If {a mathematical formula}β=0 then we must have {a mathematical formula}I=0 and thus {a mathematical formula}α=0. As a result the activity of the input, hidden, and output, neuron will always be 0. Therefore the weights {a mathematical formula}a1 and {a mathematical formula}a2 will remain constant ({a mathematical formula}da1/dt=da2/dt=0) and equal to their initial values {a mathematical formula}a1(0) and {a mathematical formula}a2(0). The error will also remain constant, and equal to 0 if and only if {a mathematical formula}T=0. Thus from now on we can assume that {a mathematical formula}β&gt;0.2) If {a mathematical formula}c1=0 then the lower weight {a mathematical formula}a1 never changes and remains equal to its initial value. If this initial value satisfies {a mathematical formula}a1(0)=0, then the activity of the hidden and output unit remains equal to 0 at all times, and thus {a mathematical formula}a2 remains constant and equal to its initial value {a mathematical formula}a2=a2(0). The error remains constant and equal to 0 if only if T is always 0. If {a mathematical formula}a1(0)≠0, then the error is a simple quadratic convex function of {a mathematical formula}a2 and since the rule for adjusting {a mathematical formula}a2 is simply gradient descent, the value of {a mathematical formula}a2 will converge to its optimal value given by: {a mathematical formula}a2=α/βa1(0).General case: Thus from now on, we can assume that {a mathematical formula}β&gt;0 and {a mathematical formula}c1≠0. Furthermore, it is easy to check that changing the sign of α corresponds to a reflection about the {a mathematical formula}a2-axis. Likewise, changing the sign of {a mathematical formula}c1 corresponds to a reflection about the origin (i.e. across both the {a mathematical formula}a1 and {a mathematical formula}a2 axis). Thus in short, it is sufficient to focus on the case where: {a mathematical formula}α&gt;0, {a mathematical formula}β&gt;0, and {a mathematical formula}c1&gt;0.In this case, the critical points for {a mathematical formula}a1 and {a mathematical formula}a2 are given by:{a mathematical formula} which corresponds to two hyperbolas in the two-dimensional {a mathematical formula}(a1,a2) plane, in the first and third quadrant for {a mathematical formula}α=E(IT)&gt;0. Note that these critical points do not depend on the feedback weight{a mathematical formula}c1. All these critical points correspond to global minima of the error function {a mathematical formula}E=12E[(T−O)2]. Furthermore, the critical points of P include also the parabola:{a mathematical formula} (Fig. 11). These critical points are dependent on the weights in the learning channel. This parabola intersects the hyperbola {a mathematical formula}a1a2=P=α/β at one point with coordinates: {a mathematical formula}a1=(−c1α/β)1/3 and {a mathematical formula}a2=(−α2/3/(c11/3β2/3).In the upper half plane, where {a mathematical formula}a2 and {a mathematical formula}c1 are congruent and both positive, the dynamics is simple to understand. For instance in the first quadrant where {a mathematical formula}a1,a2,c1&gt;0, if {a mathematical formula}α−βP&gt;0 then {a mathematical formula}da1/dt&gt;0, {a mathematical formula}da2/dt&gt;0, and {a mathematical formula}dP/dt&gt;0 everywhere and therefore the gradient vector flow is directed towards the hyperbola of critical points. If started in this region, {a mathematical formula}a1, {a mathematical formula}a2, and P will grow monotonically until a critical point is reached and the error will decrease monotonically towards a global minimum. If {a mathematical formula}α−βP&lt;0 then {a mathematical formula}da1/dt&lt;0, {a mathematical formula}da2/dt&lt;0, and {a mathematical formula}dP/dt&lt;0 everywhere and again the vector flow is directed towards the hyperbola of critical points. If started in this region, {a mathematical formula}a1, {a mathematical formula}a2, and P will decrease monotonically until a critical point is reached and the error will decrease monotonically towards a global minimum. A similar situation is observed in the fourth quadrant where {a mathematical formula}a1&lt;0 and {a mathematical formula}a2&gt;0.More generally, if {a mathematical formula}a2 and {a mathematical formula}c1 have the same sign, i.e. are congruent as in BP, then {a mathematical formula}a12+a2c1≥0 and P will increase if {a mathematical formula}α−βP&gt;0, and decrease if {a mathematical formula}α−βP&lt;0. Note however that this is also true in general when {a mathematical formula}c1 is small regardless of its sign, relative to {a mathematical formula}a1 and {a mathematical formula}a2, since in this case it is still true that {a mathematical formula}a12+a2c1 is positive. This remains true even if {a mathematical formula}c1 varies, as long as it is small. When {a mathematical formula}c1 is small, the dynamics is dominated by the top layer. The lower layer changes slowly and the top layer adapts rapidly so that the system again converges to a global minimum. When {a mathematical formula}a2=c1 one recovers the convergent dynamic of BP, as {a mathematical formula}dP/dt always has the same sign as {a mathematical formula}α−βP. However, in the lower half plane, the situation is slightly more complicated (Fig. 11).To solve the dynamics in the general case, from Equation (23) we get:{a mathematical formula} which gives {a mathematical formula}a2=12c1a12+C so that finally:{a mathematical formula} Given a starting point {a mathematical formula}a1(0) and {a mathematical formula}a2(0), the system will follow a trajectory given by the parabola in Equation (30) until it converges to a critical point (global optimum) where {a mathematical formula}da1/dt=da2/dt=0. To find the specific critical point to which it converges to, Equations (30) and (27) must be satisfied simultaneously which leads to the depressed cubic equation:{a mathematical formula} which can be solved using the standard formula for the roots of cubic equations. Note that the parabolic trajectories contained in the upper half plane intersect the critical hyperbola in only one point and therefore the equation has a single real root. In the lower half plane, the parabolas associated with the trajectories can intersect the hyperbolas in 1, 2, or 3 distinct points corresponding to 1 real root, 2 real roots (1 being double), and 3 real roots. The double root corresponds to the point {a mathematical formula}−(c1α/β)1/3 associated with the intersection of the parabola of Equation (30) with both the hyperbola of critical points {a mathematical formula}a1a2=α/β and the parabola of additional critical points for P given by Equation (28).When there are multiple roots, the convergence point of each trajectory is easily identified by looking at the derivative vector flow (Fig. 11). Note on the figure that all the points on the critical hyperbolas are stable attractors, except for those in the lower half-plane that satisfy both {a mathematical formula}a1a2=α/β and {a mathematical formula}a2c1+a12&lt;0. This can be shown by linearizing the system around its critical points.
      </paragraph>
      <paragraph label="Linearization around critical points">
       If we consider a small deviation {a mathematical formula}a1+u and {a mathematical formula}a2+v around a critical point {a mathematical formula}a1,a2 (satisfying {a mathematical formula}α−βa1a2=0) and linearize the corresponding system, we get:{a mathematical formula} with {a mathematical formula}a1a2=α/β. If we let {a mathematical formula}w=a2u+a1v we have:{a mathematical formula} Thus if {a mathematical formula}β(c1a2+a12)&gt;0, w converges to zero and {a mathematical formula}a1,a2 is an attractor. In particular, this is always the case when {a mathematical formula}c1 is very small, or {a mathematical formula}c1 has the same sign as {a mathematical formula}a2. If {a mathematical formula}β(c1a2+a12)&lt;0, w diverges to +∞, and corresponds to unstable critical points as described above. If {a mathematical formula}β(c1a2+a12)=0, w is constant.Finally, note that in many cases, for instance for trajectories in the upper half plane, the value of P along the trajectories increases or decreases monotonically towards the global optimum value. However this is not always the case and there are trajectories where {a mathematical formula}dP/dt changes sign, but this can happen only once.
      </paragraph>
     </section>
     <section label="7.3">
      Adding depth: the linear chain {a mathematical formula}A[1,1,1,1]
      <paragraph label="Derivation of the system">
       In the case of a linear {a mathematical formula}A[1,1,1,1] architecture, for notational simplicity, let us denote by {a mathematical formula}a1,a2 and {a mathematical formula}a3 the forward weights, and by {a mathematical formula}c1 and {a mathematical formula}c2 the random weights of the learning channel (note the index is equal to the target layer). In this case, we have {a mathematical formula}O(t)=a1a2a3I(t)=PI(t). The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}P=a1a2a3. With the proper scaling of the learning rate ({a mathematical formula}η=Δt) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}a1,a2 and {a mathematical formula}a3 during learning:{a mathematical formula} The dynamic of {a mathematical formula}P=a1a2a3 is given by:{a mathematical formula}
      </paragraph>
      <paragraph label="Theorem 3">
       Except for trivial cases (associated with{a mathematical formula}c1=0or{a mathematical formula}c2=0), starting from any initial conditions the system in Equation(36)converges to a fixed point, corresponding to a global minimum of the quadratic error function. All the fixed points are located on the hypersurface given by{a mathematical formula}α−βP=0and are global minima of the error function. Along any trajectory, and for each i,{a mathematical formula}ai+1is a quadratic function of{a mathematical formula}ai. For any starting point, the final fixed point can be calculated by solving a polynomial equation of degree seven.
      </paragraph>
      <paragraph label="Proof">
       If {a mathematical formula}c1=0, {a mathematical formula}a1 remains constant and thus we are back to the linear case of a {a mathematical formula}A[1,1,1] architecture where the inputs I are replaced by {a mathematical formula}a1I. Likewise, if {a mathematical formula}c2=0{a mathematical formula}a2 remains constant and the problem can again be reduced to the {a mathematical formula}A[1,1] case with the proper adjustments. Thus for the rest of this section we can assume {a mathematical formula}c1≠0 and {a mathematical formula}c2≠0.The critical points of the system correspond to {a mathematical formula}α−βP=0 and do not depend on the weights in the learning channel. These critical points correspond to global minima of the error function. These critical points are also critical points for the product P. Additional critical points for P are provided by the hypersurface: {a mathematical formula}a12a22+c1a2a3+c2a12a3=0 with {a mathematical formula}(a1,a2,a3) in {a mathematical formula}R3.The dynamics of the system can be solved by noting that Equation (36) yields:{a mathematical formula} As a result:{a mathematical formula} and:{a mathematical formula} Substituting these results in the first equation of the system gives:{a mathematical formula} and hence:{a mathematical formula} In short {a mathematical formula}da1/dt=Q(a1) where Q is a polynomial of degree 7 in {a mathematical formula}a1. By expanding and simplifying Equation (42), it is easy to see that the leading term of Q is negative and given by {a mathematical formula}βc22/(16c12). Therefore, using Theorem 1, for any initial conditions {a mathematical formula}a1(0), {a mathematical formula}a1(t) converges to a finite fixed point. Since {a mathematical formula}a2 is a quadratic function of {a mathematical formula}a1 it also converges to a finite fixed point, and similarly for {a mathematical formula}a3. Thus in the general case the system always converges to a global minimum of the error function satisfying {a mathematical formula}α−βP=0. The hypersurface {a mathematical formula}a12a22+c1a2a3+c2a12a3=0 depends on {a mathematical formula}c1,c2 and provides additional critical points for the product P. It can be shown again by linearization that this hypersurface separates stable from unstable fixed points.As in the previous case, small weights and congruent weights can help learning but are not necessary. In particular, if {a mathematical formula}c1 and {a mathematical formula}c2 are small, or if {a mathematical formula}c1 is small and {a mathematical formula}c2 is congruent (with {a mathematical formula}a3), then {a mathematical formula}a12a22+c1a2a3+c2a12a3&gt;0 and {a mathematical formula}dP/dt has the same sign as {a mathematical formula}α−βP.
      </paragraph>
     </section>
     <section label="7.4">
      The general linear chain: {a mathematical formula}A[1,…,1]
      <paragraph label="Derivation of the system">
       The analysis can be extended immediately to a linear chain architecture {a mathematical formula}A[1,…,1] of arbitrary length (Fig. 12). In this case, let {a mathematical formula}a1,a2,…,aL denote the forward weights and {a mathematical formula}c1,…,cL−1 denote the feedback weights. Using the same derivation as in the previous cases and letting {a mathematical formula}O=PI=a1a2…aLI gives the system:{a mathematical formula} for {a mathematical formula}i=1,…,L. Taking expectations as usual leads to the set of differential equations:{a mathematical formula} or, in more compact form:{a mathematical formula} with {a mathematical formula}cL=1. As usual, {a mathematical formula}P=∏i=1Lai, {a mathematical formula}α=E(TI), and {a mathematical formula}β=E(I2). A simple calculation yields:{a mathematical formula} the last equality requiring {a mathematical formula}ai≠0 for every i.
      </paragraph>
      <paragraph label="Theorem 4">
       Except for trivial cases, starting from any initial conditions the system in Equation(44)converges to a fixed point, corresponding to a global minimum of the quadratic error function. All the fixed points are located on the hypersurface given by{a mathematical formula}α−βP=0and are global minima of the error function. Along any trajectory, and for each i,{a mathematical formula}ai+1is a quadratic function of{a mathematical formula}ai. For any starting point, the final fixed point can be calculated by solving a polynomial equation of degree{a mathematical formula}2L−1.
      </paragraph>
      <paragraph label="Proof">
       Again, when all the weights in the learning channel are non-zero, the critical points correspond to the curve {a mathematical formula}α−βP=0. These critical points are independent of the weights in the learning channel and correspond to global minima of the error function. Additional critical points for the product {a mathematical formula}P=a1…aL are given by the surface {a mathematical formula}∑i=1LPciai∏k=1i−1ak=0. These critical points are dependent on the weights in the learning channel. If the {a mathematical formula}ci are small or congruent with the respective feedforward weights, then {a mathematical formula}∑k=1L[∏i≠kai][cL−k∏j=1j=k−1aj]&gt;0 and {a mathematical formula}dP/dt has the same sign as {a mathematical formula}α−βP. Thus small or congruent weights can help the learning but they are not necessary.To see the convergence, from Equation (45), we have:{a mathematical formula} Note that if one the derivatives {a mathematical formula}dai/dt is zero, then they are all zero and thus there cannot be any limit cycles. Since in the general case all the {a mathematical formula}ci are non-zero, we have:{a mathematical formula} showing that there is a quadratic relationship between {a mathematical formula}ai+1 and {a mathematical formula}ai, with no linear term, for every i. Thus every {a mathematical formula}ai can be expressed as a polynomial function of {a mathematical formula}a1 of degree {a mathematical formula}2i−1, containing only even terms:{a mathematical formula} and:{a mathematical formula} By substituting these relationships in the equation for the derivative of {a mathematical formula}a1, we get {a mathematical formula}da1/dt=Q(a1) where Q is a polynomial with an odd degree n given by:{a mathematical formula} Furthermore, from Equation (50) it can be seen that leading coefficient is negative therefore, using Theorem 1, for any set of initial conditions the system must converge to a finite fixed point. For a given initial condition, the point of convergence can be solved by looking at the nearby roots of the polynomial Q of degree n.
      </paragraph>
      <paragraph label="Gradient descent equations">
       For comparison, the gradient descent equations are:{a mathematical formula} (the equality in the middle requires that {a mathematical formula}ai≠0). In this case, the coupling between neighboring terms is given by:{a mathematical formula} Solving this equation yields:{a mathematical formula}
      </paragraph>
     </section>
     <section label="7.5">
      Adding width (expansive): {a mathematical formula}A[1,N,1]
      <paragraph label="Derivation of the system">
       Consider a linear {a mathematical formula}A[1,N,1] architecture (Fig. 13). For notational simplicity, we let {a mathematical formula}a1,…,aN be the weights in the lower layer, {a mathematical formula}b1,…,bN be the weights in the upper layer, and {a mathematical formula}c1,…,cN the random weights of the learning channel. In this case, we have {a mathematical formula}O(t)=∑iaibiI(t). We let {a mathematical formula}P=∑iaibi. The learning equations are:{a mathematical formula} When averaged over the training set:{a mathematical formula} where {a mathematical formula}α=E(IT) and {a mathematical formula}β=E(I2). With the proper scaling of the learning rate ({a mathematical formula}η=Δt) this leads to the non-linear system of coupled differential equations for the temporal evolution of {a mathematical formula}ai and {a mathematical formula}bi during learning:{a mathematical formula} The dynamic of {a mathematical formula}P=∑iaibi is given by:{a mathematical formula}
      </paragraph>
      <paragraph label="Theorem 5">
       Except for trivial cases, starting from any initial conditions the system in Equation(57)converges to a fixed point, corresponding to a global minimum of the quadratic error function. All the fixed points are located on the hypersurface given by{a mathematical formula}α−βP=0and are global minima of the error function. Along any trajectory, each{a mathematical formula}biis a quadratic polynomial function of{a mathematical formula}ai. Each{a mathematical formula}aiis an affine function of any other{a mathematical formula}aj. For any starting point, the final fixed point can be calculated by solving a polynomial differential equation of degree 3.
      </paragraph>
      <paragraph label="Proof">
       Many of the features found in the linear chain are found again in this system using similar analyses. In the general case where the weights in the learning channel are non-zero, the critical points are given by the surface {a mathematical formula}α−βP=0 and correspond to global optima. These critical points are independent of the weights in the learning channel. Additional critical points for the product {a mathematical formula}P=∑iaibi are given by the surface {a mathematical formula}∑iai2+bici=0 which depends on the weights in the learning channel. If the {a mathematical formula}ci's are small, or congruent with the respective {a mathematical formula}bi's, then {a mathematical formula}∑iai2+bici&gt;0 and {a mathematical formula}dP/dt has the same sign as {a mathematical formula}α−βP.To address the convergence, Equation (57) leads to the vertical coupling between {a mathematical formula}ai and {a mathematical formula}bi:{a mathematical formula} for each {a mathematical formula}i=1,…,N. Thus the dynamics of the {a mathematical formula}ai variables completely determines the dynamics of the {a mathematical formula}bi variables, and one only needs to understand the behavior of the {a mathematical formula}ai variables. In addition to the vertical coupling between {a mathematical formula}ai and {a mathematical formula}bi, there is an horizontal coupling between the {a mathematical formula}ai variables given again by Equation (57) resulting in:{a mathematical formula} Thus, iterating, all the variables {a mathematical formula}ai can be expressed as affine functions of {a mathematical formula}a1 in the form:{a mathematical formula} Thus solving the entire system can be reduced to solving for {a mathematical formula}a1. The differential equation for {a mathematical formula}a1 is of the form {a mathematical formula}da1/dt=Q(a1) where Q is a polynomial of degree 3. Its leading term, is the leading term of {a mathematical formula}−c1βP. To find its leading term we have:{a mathematical formula} and thus the leading term of Q is given by {a mathematical formula}Ka13 where:{a mathematical formula} Thus the leading term of Q has a negative coefficient, and therefore {a mathematical formula}a1 always converges to a finite fixed point, and so do all the other variables.
      </paragraph>
     </section>
     <section label="7.6">
      Adding width (compressive): {a mathematical formula}A[N,1,N]
      <paragraph label="Derivation of the system">
       Consider a linear {a mathematical formula}A[N,1,N] architecture (Fig. 13). The on-line learning equations are given by:{a mathematical formula} for {a mathematical formula}i=1,…,N. As usual taking expectations, using matrix notation and a small learning rate, leads to the system of differential equations:{a mathematical formula} Here A is an {a mathematical formula}1×N matrix, B is an {a mathematical formula}N×1 matrix, and C is an {a mathematical formula}1×N matrix, and {a mathematical formula}Mt denotes the transpose of the matrix M. {a mathematical formula}ΣII=E(IIt) and {a mathematical formula}ΣTI=E(TIt) are {a mathematical formula}N×N matrices associated with the data.
      </paragraph>
      <paragraph label="Lemma 1">
       Along the flow of the system defined by Equation(65), the solution satisfies:{a mathematical formula}where K is a constant depending only on the initial values.
      </paragraph>
      <paragraph label="Proof">
       The proof is immediate since:{a mathematical formula} where {a mathematical formula}||A||2=a12+…+aN2. The theorem is obtained by integration.
      </paragraph>
      <paragraph label="Theorem 6">
       In the case of an autoencoder with uncorrelated normalized data (Equation(68)), the system converges to a fixed point satisfying{a mathematical formula}A=βC, where β is a positive root of a particular cubic equation. At the fixed point,{a mathematical formula}B=Ct/(β||C||2)and the product{a mathematical formula}P=BAconverges to{a mathematical formula}CtC/||C||2.
      </paragraph>
      <paragraph label="Proof">
       For an autoencoder with uncorrelated and normalized data ({a mathematical formula}ΣTI=ΣII=Id). In this case the system can be written as:{a mathematical formula} We define{a mathematical formula} and let {a mathematical formula}A0=A(0). Note that {a mathematical formula}σ(t)≥K. We assume that C and {a mathematical formula}A0 are linearly independent, otherwise the proof is easier. Then we have:{a mathematical formula} Therefore the solution {a mathematical formula}A(t) must have the form:{a mathematical formula} which yields:{a mathematical formula} or:{a mathematical formula} From the above expressions, we know that both f and g are nonnegative. We also have{a mathematical formula} Since {a mathematical formula}σ(t)≥K, {a mathematical formula}g(t) is bounded, and thus{a mathematical formula} By a more general theorem shown in the next section, we know also that {a mathematical formula}‖A‖ is bounded and therefore f is also bounded. Using Equation (74), this implies that {a mathematical formula}g(t)→0 as {a mathematical formula}t→∞. Now we consider again the equation:{a mathematical formula} Now consider the cubic equation:{a mathematical formula} For t large enough, since {a mathematical formula}g(t)→0, we have:{a mathematical formula} Thus Equation (76) is close to the polynomial differential equation:{a mathematical formula} By Theorem 1, this system is always convergent to a positive root of Equation (77), and by comparison the system in Equation (76) must converge as well. This proves that {a mathematical formula}f(t)→β as {a mathematical formula}t→∞, and in combination with {a mathematical formula}g(t)→0 as {a mathematical formula}t→∞, shows that A converges to βC. As A converges to a fixed point, the error function converges to a convex function and B performs gradient descent on this convex function and thus must also approach a fixed point. By the results in [2], [3], the solution must satisfy {a mathematical formula}BAAt=At. When {a mathematical formula}A=βC this gives: {a mathematical formula}B=Ct/(βCCt)=Ct/(β‖C‖2). In this case, the product {a mathematical formula}P=BA converges to the fixed point: {a mathematical formula}CtC/‖C‖2. The proof can easily be adapted to the slightly more general case where {a mathematical formula}ΣII is a diagonal matrix.
      </paragraph>
     </section>
     <section label="7.7">
      The general linear case: {a mathematical formula}A[N0,N1,…,NL]
      <paragraph label="Derivation of the system">
       Although we cannot yet provide a solution for this case, it is still useful to derive its equations. We assume a general feedforward linear architecture (Fig. 14) {a mathematical formula}A[N0,N1,…,NL] with adjustable forward matrices {a mathematical formula}A1,…,AL and fixed feedback matrices {a mathematical formula}C1,…,CL−1 (and {a mathematical formula}CL=Id). Each matrix {a mathematical formula}Ai is of size {a mathematical formula}Ni×Ni−1 and, in SRBP, each matrix {a mathematical formula}Ci is of size {a mathematical formula}Ni×NL. As usual, {a mathematical formula}O(t)=PI(t)=(∏i=1LAi)I(t).Assuming the same learning rate everywhere, using matrix notation we have:{a mathematical formula} which, after taking averages, leads to the system of differential equations{a mathematical formula} with {a mathematical formula}P=ALAL−1…A1, {a mathematical formula}ΣTI=E(TIt), and {a mathematical formula}ΣII=E(IIt). {a mathematical formula}ΣTI is a {a mathematical formula}NL×N0 matrix and {a mathematical formula}ΣII is a {a mathematical formula}N0×N0 matrix. In the case of an autoencoder, {a mathematical formula}T=I and therefore {a mathematical formula}ΣTI=ΣII. Equation (81) is true also for {a mathematical formula}i=1 and {a mathematical formula}i=L with {a mathematical formula}CL=Id where Id is the identity matrix. These equations establish a coupling between the layers so that:{a mathematical formula} When the layers have the same sizes, the coupling can be written as:{a mathematical formula} where we can assume that the random matrices {a mathematical formula}Ci are invertible square matrices.
      </paragraph>
      <paragraph label="Gradient descent equations">
       For comparison, the gradient descent equations are given by:{a mathematical formula} resulting in the coupling:{a mathematical formula} and, by definition:{a mathematical formula} where {a mathematical formula}E=E(T−PI)2/2.
      </paragraph>
      <paragraph label="RBP equations">
       Note that in the case of RBP with backward matrices {a mathematical formula}C1,…,CL−1, as opposed to SRBP, one has the system of differential equations:{a mathematical formula} By letting {a mathematical formula}Bi=Ci…CL−1 one obtains the SRBP equations however the size of the layers may impose constraints on the rank of the matrices {a mathematical formula}Bi.
      </paragraph>
     </section>
     <section label="7.8">
      The general three-layer linear case {a mathematical formula}A[N0,N1,N2]
      <paragraph label="Derivation of the system">
       We begin with the following theorem.
      </paragraph>
      <paragraph label="Theorem 7">
       The general three layer linear system (Equation(88)) always has long-term solutions. Moreover{a mathematical formula}‖A1‖is bounded.
      </paragraph>
      <paragraph label="Proof">
       As in Lemma 1, we have:{a mathematical formula} Thus we have:{a mathematical formula} It follows that:{a mathematical formula} where {a mathematical formula}C0 is a constant matrix. Let:{a mathematical formula} Using Lemma 2 below, we have:{a mathematical formula} Since:{a mathematical formula} or:{a mathematical formula} using Equation (92), we have:{a mathematical formula} Using the second inequality in Lemma 2 below, we have:{a mathematical formula} for positive constants {a mathematical formula}c1,…,c5. Since {a mathematical formula}A1 has long-term existence, so does f. Note that it is not possible for f to be increasing as {a mathematical formula}t→∞ because if we had {a mathematical formula}f′(t)≥0, then we would have {a mathematical formula}c5−12c1f2≥0 and thus f would be bounded ({a mathematical formula}f≤2c5/c1). But if f is not always increasing, at each local maximum point of f we have {a mathematical formula}f≤2c5/c1, which implies {a mathematical formula}f≤2c5/c1 everywhere.
      </paragraph>
      <paragraph label="Lemma 2">
       There is a constant{a mathematical formula}c1&gt;0such that
      </paragraph>
      <list>
       <list-item label="1.">
        {a mathematical formula}f≥c1‖A1‖2,
       </list-item>
       <list-item label="2.">
        {a mathematical formula}Tr(A1A1tA1ΣIIA1t)≥c1f2.
       </list-item>
      </list>
      <paragraph label="Theorem 8">
       The main result of this section is as follows.Partial Convergence TheoremAlong the flow of the system in Equation(88),{a mathematical formula}A1and{a mathematical formula}C1A2are uniformly bounded. Moreover,{a mathematical formula}C1A2A1→C1ΣTIΣII−1as{a mathematical formula}t→∞and:{a mathematical formula}
      </paragraph>
      <paragraph label="Proof">
       Let:{a mathematical formula} Then:{a mathematical formula} It follows that:{a mathematical formula} Thus we have:{a mathematical formula} Here, for two matrices X and Y, we write {a mathematical formula}X≤Y if and only if {a mathematical formula}Y−X is a semi-positive matrix. Let:{a mathematical formula} Then:{a mathematical formula} By Theorem 7, there is a lower bound on the matrix V{a mathematical formula} for a constant matrix C. Thus as {a mathematical formula}t→∞, {a mathematical formula}V=V(t) is convergent. Using the inequality above, the expression{a mathematical formula} is monotonically decreasing. Since {a mathematical formula}A1 is bounded by Theorem 7, and {a mathematical formula}A2A2T is nonnegative, the expression is convergent. In particular, {a mathematical formula}C1A2 is also bounded along the flow. By the (108), both {a mathematical formula}A1 and {a mathematical formula}C1A2 are {a mathematical formula}L2 integrable. Thus in fact we have pointwise convergence of {a mathematical formula}C1A2A1. Since {a mathematical formula}C1 may not be full rank, we call it partial convergence. If {a mathematical formula}C1 has full rank (which in general is the case of interest), then as {a mathematical formula}C1A2A1 is convergent, so is {a mathematical formula}A2A1.When does partial convergence imply the convergences of the solution {a mathematical formula}(A1(t),A2(t))? The following result gives a sufficient condition.
      </paragraph>
      <paragraph label="Theorem 9">
       If the set of matrices{a mathematical formula}A1,A2satisfying:{a mathematical formula}is discrete, then{a mathematical formula}A1(t)and{a mathematical formula}C1A2(t)are convergent.
      </paragraph>
      <paragraph label="Proof">
       By the proof of Theorem 8, we know that {a mathematical formula}A1(t), {a mathematical formula}C1A2(t) are bounded, and the limiting points of the pair {a mathematical formula}(A1(t),C1A2(t)) satisfy the relationships in Equation (110). If the set is discrete, then the limit must be unique and {a mathematical formula}A1(t) and {a mathematical formula}C1A2(t) converge.If {a mathematical formula}C1 has full rank, then the system in Equation (88) is convergent, if the assumptions in Theorem 9 are satisfied. Applying this result to the {a mathematical formula}A[1,N,1] and {a mathematical formula}A[N,1,N] cases, provides alternative proofs for Theorem 3 and Theorem 6. The details are omitted. Beyond these two cases, the algebraic set defined by Equation (110) is quite complicated to study. The first non-trivial case that can be analyzed corresponds to the {a mathematical formula}A[2,2,2] architecture. In this special case, we can solve the convergence problem entirely as follows.For the sake of simplicity, we assume that {a mathematical formula}ΣII=ΣTI=C1=I. Then the system associated with Equation (88) can be simplified to:{a mathematical formula} where {a mathematical formula}A(t),B(t) are {a mathematical formula}2×2 matrix functions. By Theorem 7, we know that {a mathematical formula}B(t)A(t) is convergent. In order to prove that {a mathematical formula}B(t) and {a mathematical formula}A(t) are individually convergent, we prove the following result.
      </paragraph>
      <paragraph label="Theorem 10">
       Let{a mathematical formula}Fbe the set of{a mathematical formula}2×2matrices{a mathematical formula}A,Bsatisfying the equations:{a mathematical formula}where{a mathematical formula}K,Lare fixed matrices. Then{a mathematical formula}Fis a discrete set and the system defined by Equation(111)is convergent.
      </paragraph>
      <paragraph label="Proof">
       The proof is somewhat long and technical and thus is given in the Appendix. It uses basic tools from algebraic geometry.
      </paragraph>
      <paragraph>
       Theorem 10 provides evidence that in general the algebraic set defined by Equation (110) might be discrete. Although at this moment we are not able to prove discreteness in the general case, this is a question of separate interest in mathematics (real algebraic geometry). The system defined by Equation (110) is an over-determined system of algebraic equations. For example, if {a mathematical formula}A(t),B(t) are {a mathematical formula}n×n matrices, and if C is non-singular, then the system contains {a mathematical formula}n(n+1) equations with {a mathematical formula}n2 unknowns. One can define the Koszul complex [9] associated with these equations Using the complex, given specific matrices {a mathematical formula}C,ΣTI,ΣII,K,L, there is a constructive algorithmic way to determine whether the set is discrete. If it is, then the corresponding system of ODE is convergent.{sup:1}
      </paragraph>
     </section>
     <section label="7.9">
      <section-title>
       A non-linear case
      </section-title>
      <paragraph>
       As can be expected, the case of non-linear networks is challenging to analyze mathematically. In the linear case, the transfer functions are the identity and thus all the derivatives of the transfer functions are equal to 1 and thus play no role. The simulations reported above provide evidence that in the non-linear case the derivatives of the activation functions play a role in both RBP and SRBP. Here we study a very simple non-linear case which provides some further evidence.
      </paragraph>
      <paragraph>
       We consider a simple {a mathematical formula}A[1,1,1] architecture, with a single power function non-linearity with power {a mathematical formula}μ≠1 in the hidden layer, so that {a mathematical formula}O1(S1)=(S1)μ. The final output neuron is linear {a mathematical formula}O2(S2)=S2 and thus the overall input-output relationship is: {a mathematical formula}O=a2(a1I)μ. Setting μ to 1/3, for instance, provides an S-shaped transfer function for the hidden layer, and setting {a mathematical formula}μ=1 corresponds to the linear case analyzed in a previous section. The weights are {a mathematical formula}a1 and {a mathematical formula}a2 in the forward network, and {a mathematical formula}c1 in the learning channel.
      </paragraph>
      <paragraph label="Derivation of the system without derivatives">
       When no derivatives are included, one obtains:{a mathematical formula} where here {a mathematical formula}α=E(TIμ), {a mathematical formula}β=E(I2μ), {a mathematical formula}γ=E(TI), and {a mathematical formula}δ=E(Iμ+1). Except for trivial cases, such a system cannot have fixed points since in general one cannot have {a mathematical formula}a2a1μ=α/β and {a mathematical formula}a2a1μ=γ/δ at the same time.
      </paragraph>
      <paragraph label="Derivation of the system with derivatives">
       In contrast, when the derivative of the forward activation is included the system becomes:{a mathematical formula} This leads to the coupling:{a mathematical formula} excluding as usual the trivial cases where {a mathematical formula}c1=0 or {a mathematical formula}μ=0. Here K is a constant depending only on {a mathematical formula}a1(0) and {a mathematical formula}a2(0). The coupling shows that if {a mathematical formula}da1/dt=0 then {a mathematical formula}da2/dt=0 and therefore in general limit cycles are not possible. The critical points are given by the equation:{a mathematical formula} and do not depend on the weight in the learning channel. Thus, in the non-trivial cases, {a mathematical formula}a2 is an hyperbolic function of {a mathematical formula}a1μ. It is easy to see, at least in some cases, that the system converges to a fixed point. For instance, when {a mathematical formula}α&gt;0, {a mathematical formula}c1&gt;0, {a mathematical formula}μ&gt;1, and {a mathematical formula}a1(0) and {a mathematical formula}a2(0) are small and positive, then {a mathematical formula}da1/dt&gt;0 and {a mathematical formula}da2/dt&gt;0 and both derivatives are monotonically increasing and {a mathematical formula}α−βa2a1μ decreases monotonically until convergence to a critical point. Thus in general the system including the derivatives of the forward activations is simpler and better behaved. In fact, we have a more general theorem.
      </paragraph>
      <paragraph label="Theorem 12">
       Assume that{a mathematical formula}α&gt;0,{a mathematical formula}β&gt;0{a mathematical formula}c1&gt;0, and{a mathematical formula}μ≥1. Then for any positive initial values{a mathematical formula}a1(0)≥0and{a mathematical formula}a2(0)≥0, the system described by Equation(114)is convergent to one of the positive roots of the equation for t:{a mathematical formula}
      </paragraph>
      <paragraph label="Proof">
       Using Equation (115), the differential equation for {a mathematical formula}a1 can be rewritten as:{a mathematical formula} When μ is an integer, {a mathematical formula}Q(a1) is a polynomial of odd degree with a leading coefficient that is negative and therefore, using Theorem 1, the system is convergent. If μ is not an integer, let {a mathematical formula}r1&lt;…&lt;rk be the positive roots of the function Q. The proof then proceeds similarly to the proof of Theorem 1. That is this differential equation (Equation (118)) is convergent to one of the (non-negative) roots of {a mathematical formula}Q(t). However, since {a mathematical formula}a1(0)&gt;0, a more careful analysis shows that it is not for {a mathematical formula}a1 to converge to zero. Thus {a mathematical formula}a1 must converge to a positive root of Equation (117).
      </paragraph>
      <paragraph label="Gradient descent equations">
       Finally, for comparison, in the case of gradient descent, the system is given by:{a mathematical formula} Except for trivial cases, the critical points are again given by Equation (116), and the system always converges to a critical point.
      </paragraph>
     </section>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     Proof of Theorem 10
     <paragraph>
      Assume that {a mathematical formula}(A,B)∈F. If near {a mathematical formula}(A,B), {a mathematical formula}F is not discrete, then there are real analytic matrix-valued functions {a mathematical formula}(A(t),B(t))∈F for small {a mathematical formula}t&gt;0 such that {a mathematical formula}(A(0),B(0))=(A,B). Moreover, if we write:{a mathematical formula} then {a mathematical formula}E≠0. We use {a mathematical formula}A′,A″,A‴,B′,B″,B‴ to denote {a mathematical formula}A′(0),A″(0),A‴(0),B′(0), {a mathematical formula}B″(0),B‴(0), respectively. The general strategy is to prove that {a mathematical formula}E=0 or, in the case {a mathematical formula}E≠0, to take higher order derivatives to reach a contradiction.
     </paragraph>
     <paragraph>
      It is easy to compute:{a mathematical formula} By taking the derivative of the first two relations in Equation (112), we have:{a mathematical formula} Let:{a mathematical formula} Then by the above equations, both {a mathematical formula}X,Y are skew symmetric, and we have {a mathematical formula}YAT=X. If {a mathematical formula}Y≠0, using an orthogonal transformation and scaling, we may assume that:{a mathematical formula} Write:{a mathematical formula} Then:{a mathematical formula} Since X skew-symmetric also, we must have {a mathematical formula}b=c=0, and {a mathematical formula}a=d. Thus {a mathematical formula}A=aI for a real number {a mathematical formula}a≠0. As a result, we have:{a mathematical formula} and {a mathematical formula}(A,B)=(aI,a−1I). Let {a mathematical formula}(A˜(t),B˜(t)) be the upper triangular matrices obtained by orthogonal transformation from {a mathematical formula}(A(t),B(t)). Since both {a mathematical formula}K,L are proportional to the identity, {a mathematical formula}(A˜(t),B˜(t))∈F. Now let us write:{a mathematical formula} Then the equation {a mathematical formula}B+BT−AAT=K is equivalent to the following system:{a mathematical formula} Since t is small, {a mathematical formula}A˜(t) should be sufficiently close to aI. From the second equation of the system above, we have {a mathematical formula}d˜=a. If {a mathematical formula}b˜=0, then we conclude from the first equation of the same system that {a mathematical formula}a˜=a, and hence {a mathematical formula}A˜(t)=aI. This implies that {a mathematical formula}(A(t),B(t))=(A,B). So in this case {a mathematical formula}E=0.
     </paragraph>
     <paragraph>
      Things are more complicated when {a mathematical formula}b˜≠0. We first assume that {a mathematical formula}a≠−1. In this case, from the third equation of the system above, we have {a mathematical formula}a˜−1d˜−1+d˜=0. Since we already have {a mathematical formula}d˜=a≠1, for sufficiently small t, {a mathematical formula}a˜=−d˜−2=−a−2, which is distinct from a. Thus in this case {a mathematical formula}b˜ must be zero. If {a mathematical formula}a=−1, then we have {a mathematical formula}d˜=−1 and {a mathematical formula}a˜=−1. Using the first equation of the system above, we have {a mathematical formula}b˜=0 and the again {a mathematical formula}(A(t),B(t))=(A˜(t),B˜(t))=(A,B), and we conclude that {a mathematical formula}E=0.
     </paragraph>
     <paragraph>
      From the results above, we know that if {a mathematical formula}Y≠0 or if A is proportional to the identity, near {a mathematical formula}(A,B)∈F, there are no other elements in {a mathematical formula}F and thus {a mathematical formula}F is discrete. When {a mathematical formula}X=Y=0, it is possible to have {a mathematical formula}E≠0. However, we have the following Lemma:
     </paragraph>
     <paragraph label="Lemma 3">
      If{a mathematical formula}X=Y=0, and if{a mathematical formula}A≠−I, then E is not an invertible matrix.
     </paragraph>
     <paragraph label="Proof">
      By contradiction, assume that E is invertible. Then from {a mathematical formula}X=0, we have:{a mathematical formula} By taking determinant on both sides, we get:{a mathematical formula} Thus we have:{a mathematical formula} Since A is similar to a negative definite matrix {a mathematical formula}−BBT, the eigenvalues {a mathematical formula}λ1,λ2 of A are all negative. Since {a mathematical formula}λ1λ2=det⁡A=1, we have:{a mathematical formula} Using the same matrix representation as in Equation (125), we have:{a mathematical formula} However:{a mathematical formula} and the equality is true if and only if {a mathematical formula}b=c and {a mathematical formula}a+d=−2. Since {a mathematical formula}−λ1−λ2=2 and {a mathematical formula}λ1λ2=1, the eigenvalues of A must be {a mathematical formula}−1,−1, which implies {a mathematical formula}b=c=0. Thus {a mathematical formula}A=−I which is impossible by our assumption.Next we consider the remaining case: {a mathematical formula}X=Y=0, and E is not invertible (but not equal to zero), and A is not proportional to the identity. In this case, we have to take up to third order derivatives to reach the conclusion. By taking derivatives of the first two relations in Equation (112), we get:{a mathematical formula} where:{a mathematical formula} Similar to the relations between the matrices {a mathematical formula}X,Y, we have:{a mathematical formula} Since {a mathematical formula}AB=I, we have:{a mathematical formula} Thus:{a mathematical formula} because {a mathematical formula}X=0. Since A is not proportional to the identity, then we must have {a mathematical formula}P=Q=0 as in the case for X and Y.The relationship between {a mathematical formula}R,S is more complicated, but can be computed using the same idea. We first have:{a mathematical formula} Using Equation (139) and the fact that {a mathematical formula}P=0, we have:{a mathematical formula} Since E is not invertible and we assume that {a mathematical formula}E≠0, we must have:{a mathematical formula} for some column vectors {a mathematical formula}ξ,η. From the fact that {a mathematical formula}Y=0, we conclude that:{a mathematical formula} and:{a mathematical formula} Thus we compute:{a mathematical formula} and:{a mathematical formula} If {a mathematical formula}〈ξ,η〉≠0, then {a mathematical formula}S≠0. Thus:{a mathematical formula} For the matrix {a mathematical formula}S−1ξξT, both the trace and determinant are zero. So the eigenvalues are zero. On the other hand, since both {a mathematical formula}S,R are skew-symmetric matrices, {a mathematical formula}S−1R is proportional to the identity. As a result, the matrix {a mathematical formula}AT, hence A, has two identical eigenvalues. Let λ be an eigenvalue of A, then:{a mathematical formula} Taking the trace in the first two relations of Equation (112), we get:{a mathematical formula} Thus for fixed {a mathematical formula}K,L, λ and {a mathematical formula}‖A‖ can only assume discrete values. Since t is small, {a mathematical formula}A(t)=Q(t)AQ(t)T for some orthogonal matrix {a mathematical formula}Q(t). Let us write:{a mathematical formula} Then {a mathematical formula}E=A′(0) is equal to:{a mathematical formula} By Lemma 3, E is not invertible. Thus {a mathematical formula}b=0. But if {a mathematical formula}b=0, then A is proportional to the identity and this case has been discussed above.We must still deal with the case {a mathematical formula}〈ξ,η〉=0. Without loss of generality, we may assume that:{a mathematical formula} By checking the equation {a mathematical formula}AE=−EBBT, we can conclude that:{a mathematical formula} In fact, when t is small, the eigenvalues of {a mathematical formula}A(t) must be {a mathematical formula}−d−2 and d for some {a mathematical formula}d≠0. Again, by taking the trace of the first two relations in Equation (112), we get:{a mathematical formula} Therefore, d is locally uniquely determined by {a mathematical formula}K,L. Finally, if we write {a mathematical formula}A(t)=Q(t)AQ(t)T and assume that:{a mathematical formula} we have:{a mathematical formula} Since E must be singular, we have {a mathematical formula}d=−1 and hence {a mathematical formula}A=−I. This case has been covered above and thus the proof of Theorem 10 is complete.
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>