<?xml version="1.0" encoding="utf-8"?>
<html>
 <body>
  <root>
   <title>
    Red–black planning: A new systematic approach to partial delete relaxation.
   </title>
   <abstract>
    To date, delete relaxation underlies some of the most effective heuristics for deterministic planning. Despite its success, however, delete relaxation has significant pitfalls in many important classes of planning domains, and it has been a challenge from the outset to devise heuristics that take some deletes into account. We herein devise an elegant and simple method for doing just that. In the context of finite-domain state variables, we define red variables to take the relaxed semantics, in which they accumulate their values rather than switching between them, as opposed to black variables that take the regular semantics. Red–black planning then interpolates between relaxed planning and regular planning simply by allowing a subset of variables to be painted red. We investigate the tractability region of red–black planning, extending Chen and Giménez' characterization theorems for regular planning to the more general red–black setting. In particular, we identify significant islands of tractable red–black planning, use them to design practical heuristic functions, and experiment with a range of “painting strategies” for automatically choosing the red variables. Our experiments show that these new heuristic functions can improve significantly on the state of the art in satisficing planning.1
   </abstract>
   <content>
    <section label="1">
     <section-title>
      Introduction
     </section-title>
     <paragraph>
      In deterministic, also know as “classical”, planning for action selection, the world states are represented by complete assignments to a set of variables, the actions allow for deterministic modifications of these assignments, and the objective is to find a sequence of actions that modifies a given initial assignment to an assignment that satisfies a goal property. In the last two decades, solvers for classical planning have made spectacular advances in their empirical efficiency. A key role in this progress, especially in the context of satisficing planning where no optimality guarantee is required, is played by the monotonic, or “delete-free”, relaxation[4], [5], [6].
     </paragraph>
     <paragraph>
      At a high level, state variables in the monotonic relaxation accumulate their values, rather than switching between them. The key property of such a relaxation is that applying actions under value accumulating semantics does not reduce the applicability of actions in the future. As a result, while regular satisficing planning is {a mathematical formula}PSPACE-complete even for simple formalisms, monotonic satisficing planning is polynomial-time [7]. Despite this, plans for the monotonic relaxation – commonly referred to as relaxed plans – often yield very useful heuristic functions [8], [9], and have been a key ingredient of state-of-the-art satisficing planners (e.g., [5], [6], [10], [11]) since more than a decade.
     </paragraph>
     <paragraph>
      While some of the most effective heuristics to date are obtained in this manner, the delete relaxation has significant pitfalls. A striking example (see, e.g., [8], [12]) is the inability to account for “having to move back” on a road map: Under the relaxation, once we traversed the map once, we are in all locations simultaneously so there never is a need to move back. In effect, if, say, a truck needs to move across a line of road segments to pick up a cargo and then move back to deliver it, then the heuristic value remains constant (equal to the length of the line) until the truck reaches the cargo. In many domains that involve transportation or other types of movement, this leads to huge plateaus, i.e., regions of states with identical heuristic value. Another prominent issue (see, e.g., [13], [14], [15]) is “resource persistence”, that is, inability to account for the consumption of non-replenishable resources. The monotonic relaxation furthermore ignores any detrimental side effects an action may have on other parts of the plan, trivializing domains with a puzzle nature. For example, monotonic relaxation of Rubic's Cube tasks loses the dependencies across the subcubes.
     </paragraph>
     <paragraph>
      Given these weaknesses of monotonic relaxation, it has been an actively pursued research challenge from the outset to design heuristics that take some deletes into account. This resulted in a wealth of approaches, taking into account conflicts in the relaxed plan [16], [10], [17], [13], [18], [15], keeping track of (some) side effects [12], [19], [20], and incorporating TSP solvers responsible for the movements of particular variables in the relaxed plan [21], [22], [23]. It has proved daunting, however, to devise frameworks that fully interpolate between regular planning and monotonic planning, by providing a choice of which, and the ability to scale freely with how many, deletes are taken into account. The first such interpolation framework was put forward in 2012, enriching the monotonic relaxation with an explicitly represented set of fact conjunctions, forcing the heuristic to eventually become perfect as that set gets larger [24], [25], [26]. We herein propose a much simpler interpolation framework: we relax only some of the state variables.
     </paragraph>
     <paragraph>
      In this framework, which we baptize red–black planning, some state variables, called red, take the relaxed semantics and accumulate their values, while all other variables, called black, keep the regular semantics and thus switch between their values.{sup:2} Consider again our previous example where a truck needs to move across a line of road segments. Say we paint the truck-position variable black, and we paint the cargo-position variable red. A red–black plan then needs to include the backward truck moves, and the length of an optimal red–black plan is equal to that of an optimal real plan. The same applies to VisitAll when painting the robot position black. A heuristic function generated this way would be perfect.
     </paragraph>
     <paragraph>
      The problematic word here of course is the “would”. Apart from the quality of the heuristic, we also need to consider the computational effort required to compute it. As red–black planning generalizes monotonic planning – the special case where all variables are painted red – and optimal monotonic planning is {a mathematical formula}NP-hard [7], optimal red–black planning also is (at least) {a mathematical formula}NP-hard. Therefore, in analogy to commonly used relaxed plan heuristics, we will generate an (inadmissible) heuristic function by generating some, not necessarily optimal, red–black plan. Still, the question is whether there are significant tractable fragments of satisficing red–black planning.
     </paragraph>
     <paragraph>
      Fortunately, the answer is “yes”. Analyzing the complexity of satisficing red–black planning, we in particular show tractability for planning tasks whose black causal graph – the projection of the standard causal graph [27], [28], [29] onto the black variables – is acyclic, and whose black variables satisfy a certain invertibility condition. Specifically, we show that any fully relaxed (aka monotonic) plan for a problem in this fragment can be “repaired” into a valid red–black plan with only a polynomial runtime overhead.
     </paragraph>
     <paragraph>
      Investigating the corresponding heuristic function from a practical perspective, we find that its bias to follow decisions made in the “basis” monotonic plans can be harmful, leading to dramatic over-estimation even in very simple toy benchmarks. Targeting this pitfall, we devise an improved red–black planning algorithm that relies less on monotonic plans, getting rid of much of this over-estimation phenomenon. We fill in important realization details, pertaining in particular to planning with acyclic causal graphs and invertible variables (a sub-procedure of our heuristic function). We devise optimizations enhancing red–black plan applicability, short-cutting the search if the red–black plan is applicable in the original planning task.
     </paragraph>
     <paragraph>
      To obtain an automatic planning methodology, we finally require painting strategies for automatically deciding which variables should adopt each color. We devise a family of such strategies, and analyze their performance. We finally run comparative experiments against the state of the art on the IPC benchmarks, showing that our new heuristic functions bring significant advantages over standard delete-relaxation heuristics, as well as over alternative partial delete-relaxation heuristics, in several domains and overall.
     </paragraph>
     <paragraph>
      The rest of the paper is structured as follows. In Section 2 we provide the necessary background, and in Section 3 we formally introduce red–black planning as a framework for relaxation. We analyze the complexity of satisficing red–black planning (Section 4), discuss the practical aspects of generating heuristic functions in this context (Section 5), investigate painting strategies (Section 6), and run experiments against the state of the art (Section 7). We conclude with a brief summary and discussion of future work (Section 8). Some proofs are replaced by proof sketches in the main text. The full proofs are in Appendix A.
     </paragraph>
    </section>
    <section label="2">
     <section-title>
      Background
     </section-title>
     <paragraph>
      A planning task in finite-domain representation ({a mathematical formula}FDR) is given by a quadruple {a mathematical formula}Π=〈V,A,I,G〉, where:
     </paragraph>
     <list>
      <list-item label="•">
       V is a set of state variables, with each {a mathematical formula}v∈V being associated with a finite domain {a mathematical formula}D(v).
      </list-item>
      <list-item label="•">
       I is an initial state. The goal G is a partial assignment to V.
      </list-item>
      <list-item label="•">
       A is a finite set of actions. Each action a is a pair {a mathematical formula}〈pre(a),eff(a)〉 of partial assignments to V called precondition and effect, respectively.
      </list-item>
     </list>
     <paragraph>
      The semantics of {a mathematical formula}FDR tasks is as follows. An action a is applicable in a state s iff {a mathematical formula}s[V(pre(a))]=pre(a), i.e., iff {a mathematical formula}s[v]=pre(a)[v] for all {a mathematical formula}v∈V(pre(a)). Applying a in state s changes the value of every {a mathematical formula}v∈V(eff(a)) to {a mathematical formula}eff(a)[v]; the resulting state is denoted by {a mathematical formula}s〚a〛. If a has a precondition on v but does not change it, we say that a is prevailed by v; the set of all such preconditions is denoted {a mathematical formula}prevail(a).
     </paragraph>
     <paragraph>
      By {a mathematical formula}s〚〈a1,…,ak〉〛 we denote the state obtained from sequential application of the (respectively applicable) actions {a mathematical formula}a1,…,ak starting at state s. Such an action sequence is an s-plan if {a mathematical formula}s〚〈a1,…,ak〉〛[V(G)]=G, and it is an optimal s-plan if its length is minimal among all s-plans. The computational task of (optimal) planning is finding an (optimal) I-plan.
     </paragraph>
     <paragraph label="Example 1">
      Fig. 1 illustrates an example that we use throughout the paper.The example is akin to the Grid benchmark, and is encoded in {a mathematical formula}FDR using the following state variables: R, the robot position in {a mathematical formula}{1,…,7}; A, the key A position in {a mathematical formula}{R,1,…,7}; B, the key B position in {a mathematical formula}{R,1,…,7}; F in {a mathematical formula}{0,1} encoding whether the robot hand is free; O in {a mathematical formula}{0,1} encoding whether the lock is open. The robot can move from i to {a mathematical formula}i+1, or vice versa, provided that either the lock is open or {a mathematical formula}{i,i+1}∩{4}=∅.{sup:3} The robot can take a key if its hand is free, drop a key it is holding, or open the lock if the robot is at 3 or 5 and holds key A. The goal requires key B to be at 1. An optimal plan for the robot is to move to 2, take key A, move to 3, open the lock, move to 7, drop key A and take key B, move back to 1, and drop key B.
     </paragraph>
     <paragraph>
      A monotonic finite-domain representation ({a mathematical formula}MFDR) planning task is given by a quadruple {a mathematical formula}Π=〈V,A,I,G〉 exactly as for {a mathematical formula}FDR tasks, but the semantics is different.{sup:4} Informally, the state variables in {a mathematical formula}MFDR tasks accumulate their values, rather than switching between them. That is, if an {a mathematical formula}FDR action switches the value of a variable v from x to y, then the monotonic version of that action “extends” the value of v from {a mathematical formula}{x} to {a mathematical formula}{x,y}. More specifically,
     </paragraph>
     <list>
      <list-item label="•">
       An {a mathematical formula}MFDR state s is a function that assigns each {a mathematical formula}v∈V a non-empty subset{a mathematical formula}s[v]⊆D(v) of its domain.
      </list-item>
      <list-item label="•">
       An {a mathematical formula}MFDR action a is applicable in state s iff {a mathematical formula}pre(a)[v]∈s[v] for all {a mathematical formula}v∈V(pre(a)).
      </list-item>
      <list-item label="•">
       Applying an {a mathematical formula}MFDR action a in state s changes the value of {a mathematical formula}v∈V(eff(a)) from {a mathematical formula}s[v] to {a mathematical formula}s[v]∪{eff(a)[v]}.
      </list-item>
     </list>
     <paragraph>
      Respectively, an {a mathematical formula}MFDR action sequence {a mathematical formula}〈a1,…,ak〉 applicable in state s is an s-plan if {a mathematical formula}G[v]∈s〚〈a1,…,ak〉〛[v] for all {a mathematical formula}v∈V(G). In all other respects, {a mathematical formula}MFDR and {a mathematical formula}FDR semantics are identical.
     </paragraph>
     <paragraph>
      While {a mathematical formula}FDR planning is {a mathematical formula}PSPACE-complete even for binary state variables, satisficing plan generation for {a mathematical formula}MFDR tasks is polynomial time (this follows directly from Bylander's [7] results). Exploiting the latter for deriving heuristic estimates is a key ingredient of many competitive satisficing planning systems, via the notion of monotonic, or delete, relaxation. Given an {a mathematical formula}FDR planning task {a mathematical formula}Π=〈V,A,I,G〉, the monotonic relaxation of Π is the {a mathematical formula}MFDR task {a mathematical formula}Π+=Π. The optimal delete relaxation heuristic{a mathematical formula}h+(Π) is defined as the length of an optimal plan for {a mathematical formula}Π+. For arbitrary states s, {a mathematical formula}h+(s) is defined via the {a mathematical formula}MFDR task {a mathematical formula}〈V,A,s,G〉.
     </paragraph>
     <paragraph>
      For a state s and applicable action sequence π in Π, we sometimes use {a mathematical formula}s〚π+〛 to denote the outcome of executing π in the same state of {a mathematical formula}Π+. In general, if {a mathematical formula}π+ is a plan for {a mathematical formula}Π+, then {a mathematical formula}π+ is referred to as a relaxed plan for Π. For example, a relaxed plan for our SimpleGrid task in Example 1 is for the robot to move to 2, take key A, move to 3, open the lock, move over to position 7, take key B (without having to first drop key A), and then immediately drop key B at position 1 (without having to first move back there).
     </paragraph>
     <paragraph>
      To characterize fragments of planning, we use two standard graphical structures induced by the description of {a mathematical formula}FDR tasks.
     </paragraph>
     <list>
      <list-item label="•">
       The causal graph{a mathematical formula}CGΠ of Π is a digraph with vertices V. An arc {a mathematical formula}(v,v′) is in {a mathematical formula}CGΠ iff {a mathematical formula}v≠v′ and there exists an action {a mathematical formula}a∈A such that {a mathematical formula}(v,v′)∈V(eff(a))∪V(pre(a))×V(eff(a)), that is, a affects {a mathematical formula}v′ while either affecting v or being preconditioned by the value of v.Special cases of interest will be those where the causal graph is acyclic (a DAG), or where the causal graph is arcless, i.e., does not contain any arcs at all.
      </list-item>
      <list-item label="•">
       The domain transition graph{a mathematical formula}DTGΠ(v) of a variable {a mathematical formula}v∈V is an arc-labeled multi-digraph with vertices {a mathematical formula}D(v). {a mathematical formula}DTGΠ(v) has an arc from d to {a mathematical formula}d′ induced by a iff {a mathematical formula}eff(a)[v]=d′, and either {a mathematical formula}pre(a)[v]=d or {a mathematical formula}v∉V(pre(a)). We denote such arcs by {a mathematical formula}(d,a,d′), and by {a mathematical formula}(d,d′) for convenience where the action referred to is clear from context, or where there is no need for referring to a particular action. Each arc is labeled with its outside condition{a mathematical formula}ocon(d,a,d′)=pre(a)[V∖{v}] and its outside effect{a mathematical formula}oeff(d,a,d′)=eff(a)[V∖{v}].
      </list-item>
     </list>
     <paragraph label="Example 2">
      Fig. 2 (a) illustrates the notion of causal graphs in the SimpleGrid task from Example 1. R is a prerequisite for changing every other variable. Each key is interdependent with F because taking/dropping them affects both. Key A influences O, and O influences R.Figs. 2 (b) and 2 (c) illustrate the definition of domain transition graphs on the variables F and R in the SimpleGrid task. Note that the transitions of F in (b) are each induced by several actions—all {a mathematical formula}drop(x,y) actions for the transition {a mathematical formula}0→1 and all {a mathematical formula}take(x,y) actions for the transition {a mathematical formula}1→0), whereas each of the transitions of R in (c) is induced by a single {a mathematical formula}move(i,j) action. The label {a mathematical formula}〈O/1〉 in {a mathematical formula}DTGΠ(R) is the same for the transitions {a mathematical formula}3→4 and {a mathematical formula}4→3 (as well as for the transitions {a mathematical formula}5→4 and {a mathematical formula}4→5) because moving away from the lock also requires it to be open.
     </paragraph>
     <paragraph>
      Adopting the notation of Chen and Giménez [32], for a digraph G, let {a mathematical formula}#vertices(G), {a mathematical formula}cc-size(G), and {a mathematical formula}scc-size(G) denote the number of vertices, the size of the largest connected component in (the undirected graph induced by) G, and the size of the largest strongly connected component in G, respectively. For a set of digraphs {a mathematical formula}G, we say that {a mathematical formula}#vertices(G) is bounded if there exists a constant k such that {a mathematical formula}#vertices(G)≤k for all {a mathematical formula}G∈G. Bounded {a mathematical formula}cc-size(G) and bounded {a mathematical formula}scc-size(G) are defined similarly. {a mathematical formula}PlanExist(G) and {a mathematical formula}PlanGen(G) are the plan existence and plan generation problems restricted to {a mathematical formula}FDR planning tasks whose causal graphs are elements of {a mathematical formula}G.
     </paragraph>
    </section>
    <section label="3">
     <section-title>
      Red–black relaxation
     </section-title>
     <paragraph>
      In {a mathematical formula}FDR respectively {a mathematical formula}MFDR, all state variables adopt the value-switching respectively the value-accumulating semantics. Formulated that way, it is obvious that {a mathematical formula}FDR and {a mathematical formula}MFDR can be viewed as the two extreme ends of an entire spectrum of relaxations, choosing the semantics on a variable-by-variable basis. We baptize this approach red–black planning.
     </paragraph>
     <paragraph>
      A red–black finite-domain representation ({a mathematical formula}RB) planning task is given by a quintuple {a mathematical formula}Π=〈VB,VR,A,I,G〉, where {a mathematical formula}VB and {a mathematical formula}VR are sets of finite-domain state variables, called black variables and red variables, respectively, and A, I, and G are exactly as in {a mathematical formula}FDR and {a mathematical formula}MFDR tasks, specified with respect to the union of the black and red variables. The semantics of {a mathematical formula}RB is as follows:
     </paragraph>
     <list>
      <list-item label="(i)">
       A state s assigns each {a mathematical formula}v∈VB∪VR a non-empty subset {a mathematical formula}s[v]⊆D(v), where {a mathematical formula}|s[v]|=1 for all {a mathematical formula}v∈VB.
      </list-item>
      <list-item label="(ii)">
       An action a is applicable in state s iff {a mathematical formula}pre(a)[v]∈s[v] for all {a mathematical formula}v∈V(pre(a)).
      </list-item>
      <list-item label="(iii)">
       Applying an action a in state s changes the value of each black variable {a mathematical formula}v∈VB(eff(a)) to {a mathematical formula}{eff(a)[v]}, and changes the value of each red variable {a mathematical formula}v∈VR(eff(a)) to {a mathematical formula}s[v]∪eff(a)[v].
      </list-item>
      <list-item label="(iv)">
       An action sequence {a mathematical formula}〈a1,…,ak〉 applicable in s is an s-plan if {a mathematical formula}G[v]∈s〚〈a1,…,ak〉〛[v] for all {a mathematical formula}v∈V(G).
      </list-item>
     </list>
     <paragraph label="Example 3">
      Consider again the SimpleGrid task from Example 1. Recall that the optimal relaxed plan for this task takes key A, opens the lock, and moves over to position 7. It then takes key B (without having to first drop key A), and drops key B at position 1 (without having to first move back there).Now, say we paint variables {a mathematical formula}R,A,B,O red, but paint F black. The robot in the resulting {a mathematical formula}RB task needs to drop key A before taking key B. If R is also painted black, then the robot also needs to move back to position 1 before dropping key B.
     </paragraph>
     <paragraph>
      Obviously, if {a mathematical formula}〈V,A,I,G〉 is an {a mathematical formula}FDR task, then we obtain an equivalent {a mathematical formula}RB task as {a mathematical formula}〈V,∅,A,I,G〉, and if {a mathematical formula}〈V,A,I,G〉 is an {a mathematical formula}MFDR task, then we obtain an equivalent {a mathematical formula}RB task as {a mathematical formula}〈∅,V,A,I,G〉. In other words, {a mathematical formula}RB generalizes both {a mathematical formula}FDR and {a mathematical formula}MFDR.
     </paragraph>
     <paragraph>
      In what follows, we use {a mathematical formula}RB tasks primarily as relaxations of {a mathematical formula}FDR tasks. Given an {a mathematical formula}FDR planning task {a mathematical formula}Π=〈V,A,I,G〉 and a subset {a mathematical formula}VR⊆V of its variables, the red–black relaxation of Π relative to {a mathematical formula}VR is the {a mathematical formula}RB task {a mathematical formula}ΠVRRB=〈V∖VR,VR,A,I,G〉. The optimal red–black relaxation heuristic{a mathematical formula}hVRRB⁎(I) of Π relative to {a mathematical formula}VR is defined as the length of an optimal plan for {a mathematical formula}ΠVRRB. For arbitrary states s, {a mathematical formula}hVRRB⁎(s) is defined via the {a mathematical formula}RB task {a mathematical formula}〈V∖VR,VR,A,s,G〉. When the set of red variables is clear from context, and when our discussion pertains to arbitrary such sets, we drop the subscript “{a mathematical formula}VR”. If {a mathematical formula}πRB is a plan for {a mathematical formula}ΠRB, then we refer to {a mathematical formula}πRB as a red–black relaxed plan for Π. It is easy to see that:
     </paragraph>
     <paragraph label="Proposition 1">
      {a mathematical formula}hVRRB⁎is consistent and dominates{a mathematical formula}h+. If{a mathematical formula}VR′⊆VR, then{a mathematical formula}hVR′RB⁎dominates{a mathematical formula}hVRRB⁎. If{a mathematical formula}VR=∅, then{a mathematical formula}hVRRB⁎is perfect.
     </paragraph>
     <paragraph label="Proof">
      Regarding the last part of the claim, if {a mathematical formula}VR=∅ then {a mathematical formula}ΠVRRB is obviously equivalent to Π and in particular {a mathematical formula}hVRRB⁎ is perfect. Domination of {a mathematical formula}h+ holds because, as follows directly from definition, if {a mathematical formula}πRB is a red–black relaxed plan for any state s, then {a mathematical formula}πRB also is a relaxed plan for s, i.e., is a valid plan in {a mathematical formula}Π+. The same argument shows that, if {a mathematical formula}VR′⊆VR, then {a mathematical formula}hVR′RB⁎ dominates {a mathematical formula}hVRRB⁎.Regarding consistency, we need to show that, for any state s and applicable action a in Π, denoting {a mathematical formula}s′:=s〚a〛 we have that {a mathematical formula}hVRRB⁎(s)≤1+hVRRB⁎(s′). Let {a mathematical formula}πRB(s′) be an optimal {a mathematical formula}s′-plan in {a mathematical formula}ΠVRRB. Then {a mathematical formula}〈a〉⋅πRB(s′) is necessarily an s-plan in {a mathematical formula}ΠVRRB, because, for every variable {a mathematical formula}v∈VB∪VR, {a mathematical formula}s′[v]⊆s〚a〛[v].  □
     </paragraph>
     <paragraph>
      Some words are in order regarding the previous works of Fox and Long [21], [22] and Keyder and Geffner [23], which can also be viewed as “un-relaxing” some of the state variables. Keyder and Geffner allow to un-relax any single variable v moving which does not have any side effects on other variables. Fox and Long automatically recognize transportation sub-problems, and un-relax all vehicle position variables v. Note that the un-relaxed variables v have no direct influence on each other. This is in contrast to some of the tractable fragments of red–black planning that we will identify. In both approaches, the actions moving v in a relaxed plan {a mathematical formula}π+ are replaced by an approximate TSP solution visiting the subset of values from {a mathematical formula}D(v) required in {a mathematical formula}π+; denote that set by {a mathematical formula}D(v,π+). This is different from painting v black because it disregards repetitions and ordering of these values. For example, say v is a worker that must perform a sequence {a mathematical formula}j1,…,jn of jobs, each at a location {a mathematical formula}di, where {a mathematical formula}di and {a mathematical formula}dj may be the same for {a mathematical formula}i≠j. Then the TSP approximation follows some path visiting each member of the set {a mathematical formula}D(v,π+)={d1,…,dn} once. In a red–black plan, v follows a path visiting the sequence {a mathematical formula}d1,…,dn.{sup:5} Indeed, in the relaxed plan repair algorithm introduced in Section 4.3, moves for the black variables are inserted following the sequence of values required in the relaxed plan. The outcome of that procedure, given the same relaxed plan as input, can only be larger than the outcome of the TSP treatment.
     </paragraph>
     <paragraph>
      Computing {a mathematical formula}hVRRB⁎ is {a mathematical formula}NP-hard even in the simplest case, where there are no black variables, simply because that special case corresponds to {a mathematical formula}h+. We therefore adopt the popular strategy of upper-approximation by satisficing relaxed planning. That, in turn, requires to investigate the tractability boundary of satisficing planning for {a mathematical formula}RB tasks, and we proceed with this task in the next section.
     </paragraph>
    </section>
    <section label="4">
     <section-title>
      Tractability
     </section-title>
     <paragraph>
      Central to our investigation of tractable fragments of {a mathematical formula}RB is the notion of causal graph. While the causal graph {a mathematical formula}CGΠ of an {a mathematical formula}RB task Π is defined exactly as for {a mathematical formula}FDR, it is essential to exploit the red/black coloring of its vertices. In particular, by the black causal graph{a mathematical formula}CGΠB of Π, we refer to the sub-graph of {a mathematical formula}CGΠ induced by only the black variables {a mathematical formula}VB of Π. Another notion that we use in some parts of our analysis is the application of monotonic relaxation to {a mathematical formula}RB instead of {a mathematical formula}FDR. This just means to paint all black variables red, i.e., the monotonic relaxation of an {a mathematical formula}RB task {a mathematical formula}Π=〈VB,VR,A,I,G〉 is the {a mathematical formula}MFDR task {a mathematical formula}Π+=〈VB∪VR,A,I,G〉.
     </paragraph>
     <paragraph>
      Our investigation is sub-divided into three parts, which we present in different sub-sections. The first two parts are oriented at Chen and Giménez' complexity characterization theorems for {a mathematical formula}FDR, analyzing planning complexity as a function of the causal graph structure, with and without imposing the additional requirement of reversibility[32]. Throughout, we consider the structure of the black causal graph, hence imposing no restrictions on the red variables. In Section 4.1 we find that causal graph structure on its own is insufficient for tractability. In fact, we show that, even if the black causal graph does not contain any arcs at all, both the number and domain size of the black variables must be bounded to obtain a tractable problem. In Section 4.2 we examine the prospects of the reversibility requirement, adapted to the specifics of {a mathematical formula}RB. In analogy to the results of Chen and Giménez', we find that it suffices to bound the size of the strongly connected components in the causal graph. While that result is positive – it suggests, e.g., that it suffices to paint at least one variable on each cycle red – (a) reversibility cannot be easily tested, and (b) the result pertains to plan existence only, not to plan generation, while it is the latter that is needed to devise a goal distance estimate. We fix these issues in Section 4.3 where we replace reversibility with a stronger and easily testable notion of invertibility, under which satisficing red–black plan generation is tractable.
     </paragraph>
     <paragraph>
      Our practical heuristic functions, which will be the concern of the entire rest of the paper after this section, rely on the results of Section 4.3 exclusively. Therefore, to not distract from the flow of the paper, we do not give full details in Sections 4.1 and 4.2, instead providing only the core of the proofs and delegating technical details to Appendix A.
     </paragraph>
     <section label="4.1">
      <section-title>
       Bounding variable number and size
      </section-title>
      <paragraph>
       A first question one might ask is, what about restricting the number of black variables? For example, is the red–black planning problem still easy when taking into account the deletes on a single binary-valued variable? It is this question that initiated our investigation, and it turns out the answer is “yes”. We now prove the following more general result{sup:6}:
      </paragraph>
      <paragraph label="Theorem 1">
       Plan generation for{a mathematical formula}RBtasks with a fixed number of black variables, each with a fixed size domain, is polynomial-time.
      </paragraph>
      <paragraph label="Proof">
       A set {a mathematical formula}{v1,…,vn} of black variables can be compiled into a single black variable v with domain {a mathematical formula}⨂i=1nD(vi)[33]. This compilation is exponential in n, but incurs polynomial overhead for fixed n. Thus, it suffices to prove the claim for {a mathematical formula}RB tasks {a mathematical formula}Π=〈{vB},VR,A,I,G〉 with a single black variable {a mathematical formula}vB, {a mathematical formula}|D(vB)|=k=O(1). For ease of presentation, in what follows we assume that actions {a mathematical formula}AvB affecting {a mathematical formula}vB are also all preconditioned by the value of {a mathematical formula}vB; this assumption is without loss of generality because any action {a mathematical formula}a∈AvB with no precondition on {a mathematical formula}vB can be replaced with k actions that are preconditioned on different values of {a mathematical formula}vB, and otherwise are identical to a.A straightforward property of {a mathematical formula}RB tasks with a single black variable that plays an important role in the proof is that the outside conditions of all the actions {a mathematical formula}AvB are all red. Thus, in particular, once an action from {a mathematical formula}AvB is applied along a plan for Π, it can then be re-applied any time, as long as its inside condition on the value of {a mathematical formula}vB is satisfied.Now, let {a mathematical formula}π+=〈a1,…,an〉 be a relaxed plan for Π, i.e., a plan for the monotonic relaxation {a mathematical formula}Π+ of our {a mathematical formula}RB task Π, and, for {a mathematical formula}0≤i≤n, let {a mathematical formula}π+i=〈a1,…,ai〉 denote the corresponding prefix of {a mathematical formula}π+.
       <list>
        For {a mathematical formula}0≤i≤n, let {a mathematical formula}Γi be the subgraph of the domain transition graph {a mathematical formula}DTGΠ(vB) induced by the actions {a mathematical formula}AvB∩{a1,…,ai}. That is, the subgraph {a mathematical formula}Γ0 comprises only the vertex {a mathematical formula}I[vB], and, in general, each subgraph {a mathematical formula}Γi has the value set {a mathematical formula}I〚π+i〛[vB] as its vertices, and its arcs constitute the {a mathematical formula}π+i-induced subset of the arcs between these vertices in {a mathematical formula}DTGΠ(vB). From that, it is immediate that each {a mathematical formula}Γi is a subgraph of {a mathematical formula}Γi+1, with the latter extending the former with at most one vertex and at most one arc.Let m be the number of times the number of strongly connected components (SCCs) changes from {a mathematical formula}Γi−1 to {a mathematical formula}Γi along {a mathematical formula}π+, and σ be the (increasing) function that captures the indexes of the corresponding SCC-changing actions {a mathematical formula}{aσ(1),…,aσ(m)} along {a mathematical formula}π+. In what follows, we also use two dummy indexes: {a mathematical formula}σ(0), with {a mathematical formula}aσ(0) for “establishing” the initial state I, and {a mathematical formula}σ(m+1)≡n, with {a mathematical formula}σ(m+1) denoting “the last action of {a mathematical formula}π+”.Suppose that,
       </list>
       <list>
        <list-item label="•">
         for all {a mathematical formula}0≤i≤m and all {a mathematical formula}σ(i)&lt;j≤σ(i+1), the action {a mathematical formula}aj either has no precondition on {a mathematical formula}vB, or {a mathematical formula}pre(aj)[vB] and {a mathematical formula}eff(aσ(i))[vB] belong to the same SCC of {a mathematical formula}Γσ(i); and
        </list-item>
        <list-item label="•">
         if {a mathematical formula}vB∈V(G), then {a mathematical formula}G[vB] and {a mathematical formula}eff(aσ(m))[vB] belong to the same SCC of {a mathematical formula}Γσ(m).
        </list-item>
       </list>
       <paragraph>
        Referring to such relaxed plans {a mathematical formula}π+ as SCC-aligned, in Lemma 2, p. 107, we show that (a) any SCC-aligned relaxed plan can be extended, with only a polynomial overhead, into a proper plan for Π, and (b) the monotonic relaxation {a mathematical formula}π+ of any plan π for Π is SCC-aligned. Therefore, in particular, any sound and complete procedure for SCC-aligned relaxed planning for {a mathematical formula}RB tasks with singleton {a mathematical formula}VB extends with only a polynomial overhead to a sound and complete red–black planning procedure for that fragment of {a mathematical formula}RB.Now, let {a mathematical formula}π+ be an SCC-aligned relaxed plan for Π. Given the sequence of the SCC-changing actions {a mathematical formula}〈aσ(1),…,aσ(m)〉 along {a mathematical formula}π+, consider a different relaxed plan for Π,{a mathematical formula} where, inductively, {a mathematical formula}ρi+ is a relaxed plan from {a mathematical formula}I〚ρ0+⋅aσ(1)⋅ρ1+⋅…⋅aσ(i)〛 to the relaxed planning fixpoint, using only those actions from A that are neither preconditioned by values of{a mathematical formula}vBoutside of the SCC of{a mathematical formula}eff(aσ(i))[vB]in{a mathematical formula}Γinor have such values among their effects.It is not hard to verify (and we anyway show this in Lemma 3, p. 108), that {a mathematical formula}ρ+ is also an SCC-aligned relaxed plan for Π and that the SCC-changing actions of {a mathematical formula}ρ+ are precisely the SCC-changing actions of {a mathematical formula}π+. Thus, {a mathematical formula}ρ+ can be seen as a canonical representative of the set of all the relaxed plans for Π that have {a mathematical formula}〈aσ(1),…,aσ(m)〉 as their sequence of SCC-changing actions. Given that, instead of searching for a general SCC-aligned relaxed plan for Π, we can restrict our search to only the canonical SCC-aligned relaxed plans as in Eq. (1) by, e.g.,
       </paragraph>
       <list>
        <list-item label="1.">
         enumerating all possible candidate sequences {a mathematical formula}〈aσ(1),…,aσ(l)〉 of SCC-changing actions, and
        </list-item>
        <list-item label="2.">
         checking whether the corresponding canonical action sequence {a mathematical formula}ρ+ is a plan for {a mathematical formula}Π+.
        </list-item>
       </list>
       <paragraph>
        Note that each action {a mathematical formula}aσ(i) along a valid candidate {a mathematical formula}〈aσ(1),…,aσ(l)〉 either extends the set of SCCs of {a mathematical formula}Γσ(i−1) with a new SCC in {a mathematical formula}Γσ(i), or combines several SCCs in {a mathematical formula}Γσ(i−1) into a single SCC in {a mathematical formula}Γσ(i). The former type of changes can happen at most {a mathematical formula}k−1 times because it corresponds to extending {a mathematical formula}Γσ(i−1) with a new vertex, and {a mathematical formula}Γ0 already contains the vertex {a mathematical formula}I[vB]. In turn, the latter type of changes can also happen at most {a mathematical formula}k−1 times because it decreases the number of SCCs from {a mathematical formula}Γσ(i−1) to {a mathematical formula}Γσ(i) by at least one. In sum, we have {a mathematical formula}l≤2(k−1), and thus the number of candidate action sequences is {a mathematical formula}O(|AvB|2k−2). Given that {a mathematical formula}k=O(1), and that test (2) corresponds to {a mathematical formula}l+1 calls to (polynomial-time) monotonic planning, putting things together results in a polynomial-time procedure for solving {a mathematical formula}RB tasks with a single fixed-domain black variable.  □
       </paragraph>
      </paragraph>
      <paragraph>
       We remark that the algorithm we just described differs from that in our previous conference paper [1], and provides a stronger bound, {a mathematical formula}2k−2 instead of {a mathematical formula}(k+1)(k−1), on “the amount of search” needed. The bound {a mathematical formula}2k−2 is tight in the sense that there are cases where {a mathematical formula}2k−2 SCC-changing actions are required to solve Π.
      </paragraph>
      <paragraph>
       While our algorithm runs in polynomial time for fixed k, it is exponential in that parameter, which itself is exponential in the number of black variables in case we have to pre-compile a fixed number of these. This makes it doubtful whether the algorithm is of practical value for computing heuristic functions.{sup:7} Having this in mind, a natural next question is whether the tractability result of Theorem 1 can be extended either to a fixed number of black variables with unrestricted domains, or to an unrestricted number of black variables with fixed-size domains. Theorem 2, Theorem 3 below show that the answer to this question is “no”, even under some additional restrictions on the black variables.
      </paragraph>
      <paragraph label="Theorem 2">
       Plan existence for{a mathematical formula}RBtasks with a fixed number of black variables is{a mathematical formula}NP-complete.
      </paragraph>
      <paragraph>
       The {a mathematical formula}NP-hardness part of the proof of Theorem 2 is by a reduction from CNF satisfiability testing; the proof appears in Appendix A, p. 108.
      </paragraph>
      <paragraph label="Theorem 3">
       Plan existence for{a mathematical formula}RBtasks where all black variables have fixed-size domains is{a mathematical formula}PSPACE-complete, and it is{a mathematical formula}NP-complete even if{a mathematical formula}CGΠBis arcless.
      </paragraph>
      <paragraph>
       The first part of the proof for Theorem 3 is straightforward: If we fix the domain size of the black variables, but not their number, then we obtain a problem that is as hard as {a mathematical formula}FDR with fixed-size domains, which is {a mathematical formula}PSPACE-hard [7]. In turn, {a mathematical formula}PSPACE membership is straightforward because the addition of red variables still allows for a proof similar to that for {a mathematical formula}FDR. The proof of {a mathematical formula}NP-hardness for the second part of the claim is, again, by a reduction from CNF satisfiability testing. The complete proof is given in Appendix A, p. 109.
      </paragraph>
      <paragraph>
       We now relate the statement of Theorem 2, Theorem 3 to the main characterization theorem of Chen and Giménez [32] on the relation between the {a mathematical formula}FDR planning complexity and the structure of the causal graph:
      </paragraph>
      <paragraph label="Theorem 4">
       (See Chen and Giménez[32].) Let{a mathematical formula}Gbe a set of directed graphs. If{a mathematical formula}cc-size(G)is bounded, then{a mathematical formula}PlanGen(G)is polynomial-time solvable. Otherwise,{a mathematical formula}PlanExist(G)is not polynomial-time decidable (unless{a mathematical formula}W[1]⊆nu-FPT).
      </paragraph>
      <paragraph>
       {a mathematical formula}W[1]⊈nu-FPT is a standard assumption on parametric complexity hierarchy [34]. Note that Theorem 4 is not a dichotomy result: unless {a mathematical formula}P=NP, there are digraph sets {a mathematical formula}G for which {a mathematical formula}PlanExist(G) is neither in {a mathematical formula}P nor {a mathematical formula}NP-hard. As the tractability result in Theorem 4 covers only trivial planning problems, the theorem shows that valuable islands of tractability within {a mathematical formula}FDR must be characterized in terms that go beyond the structure of the causal graph.
      </paragraph>
      <paragraph>
       Focusing on the structure of {a mathematical formula}CGΠB, let {a mathematical formula}RB-PlanExist(G) and {a mathematical formula}RB-PlanGen(G) be respectively the plan existence and plan generation problems restricted to {a mathematical formula}RB planning tasks whose black causal graphs are elements of{a mathematical formula}G. Note that these problems put no restriction on the structure of the causal graph itself beyond being a super-graph of some element of {a mathematical formula}G. Theorem 5 below refines the complexity characterization for {a mathematical formula}RB with respect to the structure of the black causal graph, providing a valuable fragment of tractability via Theorem 1, and establishing {a mathematical formula}P/NPdichotomies for both general {a mathematical formula}RB, as well as for {a mathematical formula}RB restricted to fixed-size domain variables. The first part of Theorem 5 is by the polynomial-time plan generation for {a mathematical formula}MFDR and Theorem 2, and the second part is by Theorem 1, Theorem 3.
      </paragraph>
      <paragraph label="Theorem 5">
       Let{a mathematical formula}Gbe a set of directed graphs.
      </paragraph>
      <list>
       <list-item label="•">
        If{a mathematical formula}#vertices(G)=0, then{a mathematical formula}RB-PlanGen(G)is polynomial-time solvable. Otherwise,{a mathematical formula}RB-PlanExist(G)is{a mathematical formula}NP-hard.
       </list-item>
       <list-item label="•">
        If{a mathematical formula}#vertices(G)is bounded, then{a mathematical formula}RB-PlanGen(G)restricted to{a mathematical formula}RBtasks with bounded black variable domains is polynomial-time solvable. Otherwise,{a mathematical formula}RB-PlanExist(G)for{a mathematical formula}RBwith bounded black variable domains is{a mathematical formula}NP-hard.
       </list-item>
      </list>
     </section>
     <section label="4.2">
      <section-title>
       Causal graph structure and reversibility
      </section-title>
      <paragraph>
       Departing from the conclusive yet pessimistic statement of Theorem 4, Chen and Giménez [32] considered so-called reversible{a mathematical formula}FDR tasks: An {a mathematical formula}FDR task {a mathematical formula}Π=〈V,A,I,G〉 is reversible if, for every state s reachable from I, there exists an action sequence π that “reverts” s back to I, i.e., {a mathematical formula}s〚π〛=I. The characterization theorem of Chen and Giménez for this fragment of {a mathematical formula}FDR, Theorem 6 below, establishes a valuable tractability result. In fact, this fragment has already been successfully exploited for devising heuristic functions [29].
      </paragraph>
      <paragraph label="Theorem 6">
       (See Chen and Giménez[32].) Let{a mathematical formula}Gbe a set of directed graphs. If{a mathematical formula}scc-size(G)is bounded, then{a mathematical formula}PlanGen(G)restricted to reversible{a mathematical formula}FDRis polynomial-time solvable (under a succinct plan representation). Otherwise,{a mathematical formula}PlanExist(G)for reversible{a mathematical formula}FDRis not polynomial-time decidable (unless{a mathematical formula}W[1]⊆nu-FPT).
      </paragraph>
      <paragraph>
       Adopting the notion of reversibility in red–black planning requires a slight, natural adaptation: Since the value sets of the red variables in states reachable from I will always include their initial values anyway, reversibility should be requested only on the task's black variables. That is, we say that an {a mathematical formula}RB task {a mathematical formula}Π=〈VB,VR,A,I,G〉 is reversible if, for every state s reachable from I, there exists an action sequence π so that {a mathematical formula}s〚π〛[VB]=I[VB].
      </paragraph>
      <paragraph>
       While this extension of reversibility to {a mathematical formula}RB may at first sight appear minor and insignificant, at closer inspection it turns out to be quite substantial. In particular, reversibility of {a mathematical formula}FDR tasks with acyclic causal graph can be tested in linear time at the level of the individual domain transition graphs of the variables: such a task is reversible if and only if the reachable part of each domain transition graph is strongly connected. In contrast, even if {a mathematical formula}RB is restricted to tasks with arcless black causal graphs, testing reversibility is not (likely to be) polynomial-time:
      </paragraph>
      <paragraph label="Theorem 7">
       It is{a mathematical formula}co-NP-hard to test whether an{a mathematical formula}RBtask is reversible, even when restricting the input to{a mathematical formula}RBtasks whose black causal graph is arcless.
      </paragraph>
      <paragraph>
       The proof of Theorem 7 is by a reduction from DNF tautology testing; see Appendix A, p. 109. We now show that, somewhat surprisingly given Theorem 7, plan existence for reversible {a mathematical formula}RB tasks with acyclic black causal graphs can be decided in polynomial time.
      </paragraph>
      <paragraph label="Theorem 8">
       Let{a mathematical formula}Gbe a set of directed graphs. If{a mathematical formula}scc-size(G)is bounded, then{a mathematical formula}RB-PlanExist(G)restricted to reversible{a mathematical formula}RBis polynomial-time solvable. Otherwise, the problem{a mathematical formula}RB-PlanExist(G)for reversible{a mathematical formula}RBis not polynomial-time decidable (unless{a mathematical formula}W[1]⊆nu-FPT).
      </paragraph>
      <paragraph>
       Note that Theorem 8 substantially extends Chen and Giménez' tractability result for {a mathematical formula}PlanExist(G) (Theorem 6 here) to the red–black setting, because Theorem 8 puts no constraints on the overall structure of the causal graph. At the same time, note also the subtle difference between the claims of Theorem 6 and Theorem 8 regarding solving plan generation vs. deciding plan existence. Both the negative and positive parts of Theorem 8 consider plan existence, whereas the positive part of Theorem 6 makes a stronger claim of tractability of plan generation. It is an open question whether plan generation is tractable in the setting of Theorem 8; we conjecture that it is not. We will get back to this at the end of this section, and for now consider plan existence only.
      </paragraph>
      <paragraph>
       The negative part of Theorem 8 follows immediately from the negative part of Theorem 6. As for the positive part, given bounded {a mathematical formula}scc-size(G) we can with polynomial overhead compile each strongly connected component into a single black variable. Since the compilation leaves the semantics of the task intact, if the original task was reversible, so is the compiled task. Thus, it suffices to consider acyclic black causal graphs. In turn, for this setting, we now show that red–black plan existence is equivalent to relaxed plan existence{sup:8}:
      </paragraph>
      <paragraph label="Theorem 9">
       Any reversible{a mathematical formula}RBtask with acyclic black causal graph is solvable if and only if its monotonic relaxation is.
      </paragraph>
      <paragraph label="Proof">
       The “only if” direction is trivial. For the “if” direction, we start with a simple observation that, in any reversible {a mathematical formula}RB task Π, we can achieve all reachable red facts up front. Let {a mathematical formula}R⊆⋃v∈VRD(v) be the minimal set of red facts such that, for every state s reachable in Π, we have {a mathematical formula}s[VR]⊆R. If s is reachable in Π, then we can “complete” it into {a mathematical formula}R(s):=s∪R by reverting the black variables to {a mathematical formula}I[VB], then reaching {a mathematical formula}R(I) by going to every reachable state in turn, reverting the black variables to {a mathematical formula}I[VB] in between every two states, and then re-achieving the black facts {a mathematical formula}s[VB], arriving into {a mathematical formula}R(s) as desired. In sum, if s is a reachable state in a reversible {a mathematical formula}RB task, then {a mathematical formula}R(s) is reachable in that task as well. Hence, the existence of a plan for {a mathematical formula}R(s) implies the existence of a plan for s. So, without loss of generality, we can consider only {a mathematical formula}R-completed states, and, in particular, assume that {a mathematical formula}I=R(I).Let {a mathematical formula}Π=〈VB,VR,A,I,G〉 be a reversible {a mathematical formula}RB task with {a mathematical formula}I=R(I) and acyclic black causal graph, and {a mathematical formula}{v1,…,vn} be a topological ordering of {a mathematical formula}VB with respect to {a mathematical formula}CGΠB. If {a mathematical formula}π+=〈a1,…,am〉 is a relaxed plan for Π, consider a red–black action sequence{a mathematical formula} constructed as follows.For {a mathematical formula}1≤i≤n, assume inductively that {a mathematical formula}πn⋅…⋅πi−1 is applicable in I. If {a mathematical formula}vi∉V(G), then we set {a mathematical formula}πi:=〈〉. Otherwise, by the acyclicity of the black causal graph and the {a mathematical formula}R-completeness of I, we can revert any top subset of the black variables while leaving the remaining ones untouched. In particular, by Lemma 4 (Appendix A, p. 109), there exists an action sequence {a mathematical formula}ρi that reverts the black variables {a mathematical formula}{v1,…,vi} to their initial values, and neither is preconditioned by nor affects the topologically lower black variables {a mathematical formula}{vi+1,…,vn}. That is,{a mathematical formula} Given that, if {a mathematical formula}G[vi]=I[vi], then we set {a mathematical formula}πi:=ρi. Otherwise, by the virtue of {a mathematical formula}π+ being a relaxed plan for Π, we must have an action {a mathematical formula}ak in {a mathematical formula}π+ achieving {a mathematical formula}G[vi] for some {a mathematical formula}1≤k≤m, and, by the acyclicity of {a mathematical formula}CGΠB, the black preconditions of {a mathematical formula}ak may involve only variables {a mathematical formula}{v1,…,vi}. In turn, by Lemma 5 (Appendix A, p. 110), there exists an action sequence {a mathematical formula}πak that is applicable in I and achieves {a mathematical formula}pre(ak) while neither being preconditioned by nor affecting the topologically lower black variables {a mathematical formula}{vi+1,…,vn}. By Eq. (2) we then have {a mathematical formula}πak being applicable in {a mathematical formula}I〚πn⋅…⋅πi+1⋅ρi〛, and, for each black variable {a mathematical formula}v∈V(pre(ak))∩VB,{a mathematical formula} Given that, we set {a mathematical formula}πi:=ρi⋅πak⋅〈ak〉. It is easy to verify that the action sequence π constructed via this iterative bottom-up achievement of the black sub-goals is applicable in I and is a plan for Π.  □
      </paragraph>
      <paragraph label="Example 4">
       To illustrate with our SimpleGrid example the construction in the proof of Theorem 9, say we paint all variables except F and O black. This yields a reversible {a mathematical formula}RB task with acyclic black causal graph as in Fig. 3. Assume that I is {a mathematical formula}R-complete, {a mathematical formula}I=R(I), i.e. {a mathematical formula}I[F]={0,1} and {a mathematical formula}I[O]={0,1}. (If this is not the case, we can complete I as described in the proof, or more effectively by moving to position 2, taking key A, moving to position 3, opening the lock, and then reverting R and A to their initial values.)Consider a goal for the robot to reach location 7 with both keys being picked up. Let {a mathematical formula}π+ be the relaxed plan reaching this goal by moving to position 2, taking key A, moving to position 7, and taking key B.The construction of π for this goal {a mathematical formula}G={〈R/7〉,〈A/R〉,〈B/R〉} proceeds as follows. Presume that we process the black goals in the order {a mathematical formula}B,A,R. (The only other choice would be {a mathematical formula}A,B,R.) Starting with the goal fact {a mathematical formula}〈B/R〉, we commit to using {a mathematical formula}π+'s action a achieving that fact, namely taking key B at position 7. This action's precondition generates the new sub-goal {a mathematical formula}〈R/7〉. To achieve that sub-goal, we first revert R to its initial value (for which the empty action sequence suffices, in this case), and then following {a mathematical formula}π+ we achieve {a mathematical formula}〈R/7〉 by moving R from 1 to 7. This move sequence has no black outside conditions (if there were such conditions, they would be handled recursively), and all red outside conditions (on O, in this case) are true because {a mathematical formula}I=R(I). Once the move sequence has executed, we can apply a, finishing with our treatment of variable B.We next process the goal fact {a mathematical formula}〈A/R〉, committing to {a mathematical formula}π+'s action {a mathematical formula}a′ taking key A at position 2. The new sub-goal {a mathematical formula}〈R/2〉 is achieved by reverting R to its initial value (moving back from 7 to 1), then following {a mathematical formula}π+ to move R from 1 to 2. We then apply {a mathematical formula}a′ and are done with A. Finally, processing the goal fact {a mathematical formula}〈R/7〉, we revert R to its initial value by moving it from 2 to 1, then establish the precondition of the {a mathematical formula}π+'s action {a mathematical formula}a″ that moves R from 6 to 7 by moving R from 1 to 6. We finalize the red–black plan by applying {a mathematical formula}a″.
      </paragraph>
      <paragraph>
       As the example illustrates, while the constructed π is a red–black plan, it certainly is not a good red–black plan, which it should be in order to avoid over-estimation when used inside a heuristic function. Much worse, while the proof of Theorem 9 is constructive, executing that construction involves enumerating all reachable red–black states, so the overall construction of π is not polynomial time.
      </paragraph>
      <paragraph>
       As pointed out, it is an open question whether plan generation for reversible {a mathematical formula}RB with acyclic black causal graphs is tractable, and we conjecture that it is not. To understand this pessimism, recall that the overall causal graph – over black and red variables – may contain cycles (e.g., Fig. 2 (a)). Thus, it is unclear how to tackle red and black variables in combination, rather than separating the red variables out using {a mathematical formula}R-completeness. In particular, we can not in general follow the ordering of goal and sub-goal values achieved in the relaxed plan, because achieving these might require reverting black variables, which in turn might require red values not achieved by the relaxed plan yet.
      </paragraph>
     </section>
     <section label="4.3">
      <section-title>
       Causal graph structure and invertibility
      </section-title>
      <paragraph>
       To resolve the issues just observed regarding reversibility, we now replace reversibility with a stronger restriction, i.e., a sufficient criterion for reversibility. This criterion is easily testable, and we show that, given the criterion applies, satisficing red–black plan generation is tractable for acyclic black causal graphs.
      </paragraph>
      <paragraph>
       Our criterion is based on the idea of invertibility, where every action application can be directly “undone”. Invertibility criteria have been devised previously, e.g. by Koehler and Hoffmann [35]. Straightforwardly translated to {a mathematical formula}FDR, Koehler and Hoffmann's criterion postulates, for every action a, the existence of a corresponding inverse action {a mathematical formula}a′. That action must be always applicable behind a, ensured in {a mathematical formula}FDR by {a mathematical formula}pre(a′)⊆prevail(a)∪eff(a); and it must undo a exactly, ensured in {a mathematical formula}FDR by {a mathematical formula}V(eff(a′))=V(eff(a))⊆V(pre(a)) and {a mathematical formula}eff(a′)=pre(a)[V(eff(a))]. For any reachable state s, we can then revert to the initial state I simply by inverting the path that lead from I to s.
      </paragraph>
      <paragraph>
       It turns out that our setting admits a much less restrictive definition. What's more, the definition is per-variable, identifying a subset {a mathematical formula}VI⊆V of {a mathematical formula}FDR variables (the invertible ones) that can be painted black in principle. This enables efficient red–black relaxation design: Identify the set {a mathematical formula}VI, paint all other variables red, keep painting more variables red until there are no more cycles in the black causal graph.
      </paragraph>
      <paragraph>
       Note that, once the selection of red variables is completed, every action will affect at most one black variable, or otherwise there would be cycles between the black effect variables. Since red variables accumulate their values anyway, there is no need to “undo” any effects on them. Therefore, inverting an action a now comes down to undoing its single effect (if any) on a black variable. Denoting that variable by {a mathematical formula}vB, inverting a corresponds to inverting an arc {a mathematical formula}(d,a,d′) in the domain transition graph of {a mathematical formula}vB. Furthermore, both the outside condition and the outside effect of {a mathematical formula}(d,a,d′) will remain true and can be used as conditions for the inverse arc: For {a mathematical formula}〈u/e〉∈oeff(d,a,d′), this is simply because u is affected by a but {a mathematical formula}u≠vB, and so u must be red. For {a mathematical formula}〈u/e〉∈ocon(d,a,d′), if u is red there is nothing to show, and if u is black then {a mathematical formula}〈u/e〉 must be a prevail condition because all the outside effects are red. Putting these observations together leads us to the following definition.
      </paragraph>
      <paragraph>
       An arc {a mathematical formula}(d,a,d′) is relaxed side effects invertible, or RSE-invertible for short, if there exists an arc {a mathematical formula}(d′,a′,d) with outside condition{a mathematical formula} A variable v is RSE-invertible if all arcs in {a mathematical formula}DTGΠ(v) are RSE-invertible, and an {a mathematical formula}RB task is RSE-invertible if all its black variables are.
      </paragraph>
      <paragraph label="Theorem 10">
       Any RSE-invertible {a mathematical formula}RB task with acyclic black causal graph is reversible.
      </paragraph>
      <paragraph label="Proof sketch">
       The proof in Appendix A, p. 110 shows that, for any state s and action a applicable in s, from {a mathematical formula}s〚〈a〉〛 one can reach a state {a mathematical formula}s′ so that {a mathematical formula}s′[VB]=s[VB] and, for every {a mathematical formula}v∈VR, {a mathematical formula}s′[v]⊇s[v]. This is trivial if all variables in {a mathematical formula}V(eff(a)) are red. Otherwise, exactly one variable {a mathematical formula}vB affected by a is black. Considering the arc {a mathematical formula}(d,a,d′) in {a mathematical formula}DTGΠ(vB) taken by a in s, there exists an inverse arc {a mathematical formula}(d′,a′,d) by prerequisite, and the claim follows easily from the arguments outlined above.  □
      </paragraph>
      <paragraph>
       Obviously, RSE-invertibility can be tested in polynomial time. Note that it generalizes the earlier definition for actions, given above: {a mathematical formula}vB being the black effect variable of a, and assuming as above that {a mathematical formula}V(eff(a))⊆V(pre(a)), it corresponds to the requirement that {a mathematical formula}pre(a′)⊆pre(a)∪eff(a) (compared to {a mathematical formula}pre(a′)⊆prevail(a)∪eff(a)) and {a mathematical formula}eff(a′)[vB]=pre(a)[vB] (compared to {a mathematical formula}eff(a′)=pre(a)[V(eff(a))]).
      </paragraph>
      <paragraph label="Example 5">
       Consider the SimpleGrid example from Fig. 1, and the domain transition graphs of F and R as illustrated in Fig. 2 (b) and (c). These variables are RSE-invertible. For R, arcs {a mathematical formula}(i,i+1) and {a mathematical formula}(i+1,i) where {a mathematical formula}{i,i+1}∩{4}=∅ have empty outside conditions, and thus are trivially RSE-invertible. The other arcs all have outside condition {a mathematical formula}{O=1}, and so they are RSE-invertible, too.For F, arcs {a mathematical formula}(1,0) induced by {a mathematical formula}take(x,y) are inverted by arcs {a mathematical formula}(0,1) induced by {a mathematical formula}drop(x,y): {a mathematical formula}ocon(0,1)={〈R/x〉,〈y/R〉} is contained in {a mathematical formula}ocon(1,0)={〈R/x〉,〈y/x〉}∪oeff(1,0)={〈y/R〉}. Similarly vice versa, i.e., arcs {a mathematical formula}(0,1) are inverted by the corresponding arcs {a mathematical formula}(1,0). Note here that {a mathematical formula}ocon(0,1)⊈ocon(1,0), i.e., it is important to allow the inverse arc to make use of conditions established by the original arc's outside effect.{sup:9}
      </paragraph>
      <paragraph>
       The outside condition of {a mathematical formula}DTGΠ(R) arcs {a mathematical formula}(4,3) and {a mathematical formula}(4,5) in our example is non-standard in the sense that it is not explicitly specified in the IPC version of the Grid domain. There, instead, that condition is an invariant based on the implicit assumption that the robot is initially in an open position. We have chosen this example to illustrate that, like previous similar notions, RSE-invertibility can be subject to modeling details. It would be an option to use invariance analysis (e.g., [37], [38], [39], [40]) to detect implicit preconditions, but we have not done so for the moment.
      </paragraph>
      <paragraph>
       Together with Theorem 9, Theorem 10 immediately implies that:
      </paragraph>
      <paragraph label="Corollary 1">
       Any RSE-invertible {a mathematical formula}RB task with acyclic black causal graph is solvable if and only if its monotonic relaxation is.
      </paragraph>
      <paragraph>
       In other words, existence of a relaxed plan implies existence of a red–black plan. These results, however, do not tell us anything about how to actually find such a plan efficiently. Indeed, recall our conjecture that there is no polynomial-time “how to” when relying on reversibility only. The stronger notion of RSE-invertibility solves that issue: If all black variables are RSE-invertible, then we can efficiently repair a relaxed plan to form a valid red–black plan. We will spell this algorithm out in detail below. In a nutshell, we simply execute the relaxed plan action-by-action. Whenever the black precondition of the next action (or, at the end, the goal) is not satisfied, we project the planning task onto the black variables, and then project these variables' domains onto the values already visited along our red–black plan prefix. We then solve this projected task to move the black variables into place. The projected tasks have black variables only, so are in {a mathematical formula}FDR. Their causal graph is acyclic because the black causal graph of the original task Π is. Thanks to RSE-invertibility, all their domain transition graphs are strongly connected. These two properties together ensure that we can solve each projected task efficiently:
      </paragraph>
      <paragraph label="Lemma 1">
       Let{a mathematical formula}Gbe a set of directed graphs. If all graphs in{a mathematical formula}Gare acyclic, then{a mathematical formula}PlanGen(G)restricted to{a mathematical formula}FDR with strongly connected domain transition graphs is polynomial-time (under a succinct plan representation).
      </paragraph>
      <paragraph label="Proof">
       By Chen and Giménez [32], as stated in Theorem 6, {a mathematical formula}PlanGen(G) restricted to reversible {a mathematical formula}FDR is polynomial-time, under a succinct plan representation. So it suffices to show that any {a mathematical formula}FDR task Π with acyclic causal graph and strongly connected domain transition graphs is reversible. Say that s is any reachable state in {a mathematical formula}Π=〈V,A,I,G〉. We construct a modified task as {a mathematical formula}Π′=〈V,A,s,I〉, i.e., we take s as the initial state and we take the original initial state as the goal. Obviously, it now suffices to show that {a mathematical formula}Π′ is solvable. Since {a mathematical formula}Π′, like Π itself, has an acyclic causal graph and strongly connected domain transition graphs, that follows directly from Observation 7 of Helmert [29], which shows that any {a mathematical formula}FDR task with these properties is solvable.  □
      </paragraph>
      <paragraph>
       Results closely related to Lemma 1 have been mentioned at various places in the literature (e.g., [27], [41], [28], [29], [32]), but to our knowledge the lemma has never been stated in this precise form, as is needed in our context.
      </paragraph>
      <paragraph>
       The succinct plan representation in Chen and Giménez' proof exploits recursive macros for value pairs within domain transition graphs. This representation trick is required as plans in this setup may be exponentially long (e.g., [42], [36]).{sup:10} We remark that, in our actual implementation (detailed below in Section 5.3), we use an explicit plan representation, not relying on macros, as long plans do not tend to occur in our context (inside the heuristic function), and building the macros would incur way too much overhead. In that sense, the importance of Lemma 1 is mainly theoretical. It enables us to prove the main result of this section. We include the full proof here as it essentially consists of our relaxed plan repair algorithm, and thus directly underlies our basic heuristic function{sup:11}:
      </paragraph>
      <paragraph label="Theorem 11">
       Let{a mathematical formula}Gbe a set of directed graphs. If all graphs in{a mathematical formula}Gare acyclic, then{a mathematical formula}RB-PlanGen(G)restricted to RSE-invertible {a mathematical formula}RB is polynomial-time (under a succinct plan representation).
      </paragraph>
      <paragraph label="Proof">
       Fig. 4 provides pseudo-code for an algorithm that turns, with polynomial overhead, a relaxed plan into a red–black plan. We use the notations from the figure in what follows.Consider an iteration i of the main loop. Any red preconditions of {a mathematical formula}ai are true in the current state {a mathematical formula}I〚π〛 because the red–black plan prefix π includes the relaxed plan actions {a mathematical formula}a1,…,ai−1 processed so far. Unsatisfied black preconditions g are tackled by {a mathematical formula}Achieve(π,g), solving an {a mathematical formula}FDR task {a mathematical formula}ΠB with goal g. The returned action sequence {a mathematical formula}πB is attached to π.We next prove that (i) {a mathematical formula}ΠB is well-defined, that (ii) all its domain transition graphs are strongly connected, and that (iii) any plan {a mathematical formula}πB for {a mathematical formula}ΠB is, in our {a mathematical formula}RB task Π, applicable in the current state {a mathematical formula}I〚π〛. This suffices to prove the claim: As the causal graph of {a mathematical formula}ΠB is (obviously) acyclic, with (ii) and Lemma 1 we know that we can solve {a mathematical formula}ΠB in polynomial time (under a succinct plan representation), and so the overall algorithm runs in polynomial time. As {a mathematical formula}ΠB ignores the red variables, but effects on these cannot hurt anyway, with (iii) {a mathematical formula}ai is applicable in {a mathematical formula}I〚π⋅πB〛. Iterating that argument shows that the algorithm returns a red–black plan.For (i), we need to show that all variable values {a mathematical formula}〈v/d〉 occurring in {a mathematical formula}ΠB are indeed members of the respective variable domains, i.e., {a mathematical formula}d∈DB(v). This is obvious for {a mathematical formula}IB and {a mathematical formula}AB. It holds for {a mathematical formula}GB because by construction these are facts made true by the relaxed plan actions {a mathematical formula}a1,…,ai−1 already processed.For (ii), observe that all values in {a mathematical formula}DTGΠB(v) are, by construction, reached from {a mathematical formula}I[v] by a sequence of arcs {a mathematical formula}(d,d′) induced by actions in π. So it suffices to prove that every such arc has a corresponding arc {a mathematical formula}(d′,d) in {a mathematical formula}DTGΠB(v). Say {a mathematical formula}v∈VB, and {a mathematical formula}(d,aB,d′) is an arc in {a mathematical formula}DTGΠB(v) induced by {a mathematical formula}aB where {a mathematical formula}a∈π. Because {a mathematical formula}(d,a,d′) is RSE-invertible in Π, there exists an action {a mathematical formula}a′∈A inducing an arc {a mathematical formula}(d′,a′,d) in {a mathematical formula}DTGΠ(v) whose outside condition is contained in {a mathematical formula}pre(a)∪eff(a). Since, obviously, {a mathematical formula}pre(a)∪eff(a)⊆F, we get {a mathematical formula}pre(a′)⊆F. Since {a mathematical formula}a′ can have only one black effect, {a mathematical formula}eff(a′)[VB]={〈v/d〉} which is contained in F. Thus {a mathematical formula}a′B∈AB, and {a mathematical formula}(d′,d) is an arc in {a mathematical formula}DTGΠB(v) as desired.Finally, (iii) holds because, with {a mathematical formula}pre(a)⊆F for all actions where {a mathematical formula}aB∈AB, the red preconditions of all these actions a are true in the current state {a mathematical formula}I〚π〛. So applicability of {a mathematical formula}πB in {a mathematical formula}I〚π〛, in Π, depends on the black variables only, all of which are contained in {a mathematical formula}ΠB. This concludes the proof.  □
      </paragraph>
      <paragraph label="Example 6">
       Consider again the SimpleGrid example from Fig. 1. For {a mathematical formula}VB={R,F}, the example is in our tractable fragment: the black causal graph is acyclic (compare Fig. 2 (a)), and R and F are RSE-invertible (Fig. 2 (b) and (c), cf. Example 5).Say the relaxed plan {a mathematical formula}π+ is: {a mathematical formula}〈move(1,2),take(2,A),move(2,3),open(3,4,A),move(3,4),move(4,5),move(5,6),move(6,7),take(7,B),drop(1,B)〉.In {a mathematical formula}RelaxedPlanRepair(Π,π+), there will be no calls to {a mathematical formula}Achieve(π,g) until {a mathematical formula}ai=take(7,B). To achieve the precondition of that action, {a mathematical formula}Achieve(π,g) constructs {a mathematical formula}ΠB with initial state {a mathematical formula}{〈R/7〉,〈F/0〉} and goal {a mathematical formula}{〈R/7〉,〈F/1〉}, and an action set all of whose “drop” actions have the form “{a mathematical formula}drop(x,A)” (because the red fact {a mathematical formula}B=R has not been reached yet). We get the plan {a mathematical formula}πB=〈drop(7,A)〉. Thus {a mathematical formula}drop(7,A) and {a mathematical formula}take(7,B) are added to π. The next iteration calls {a mathematical formula}Achieve(π,g) for the precondition of {a mathematical formula}drop(1,B), constructing {a mathematical formula}ΠB with initial state {a mathematical formula}{〈R/7〉,〈F/0〉} and goal {a mathematical formula}{〈R/1〉}, yielding {a mathematical formula}πB moving R back to 1. Subsequently, {a mathematical formula}πB and {a mathematical formula}drop(1,B) are added to π, and the algorithm stops. The returned action sequence is a real plan and its length corresponds to the perfect heuristic value 17.
      </paragraph>
      <paragraph>
       Theorem 11 does not hold for the weaker requirement of bounded-size strongly connected components. This is because multiplying fixed-size sets of variables into single variables may lose RSE-invertibility. Indeed:
      </paragraph>
      <paragraph label="Theorem 12">
       There exists a set{a mathematical formula}Gof directed graphs where{a mathematical formula}scc-size(G)is bounded by 2, and{a mathematical formula}RB-PlanExist(G)restricted to RSE-invertible {a mathematical formula}RB is{a mathematical formula}NP-hard.
      </paragraph>
      <paragraph label="Proof sketch">
       Given a CNF formula ϕ, for each Boolean variable p occurring in ϕ we include two binary state variables {a mathematical formula}xp and {a mathematical formula}yp with initial value 0, an action {a mathematical formula}apx1y1 with precondition {a mathematical formula}{〈xp/0〉,〈yp/0〉} and effect {a mathematical formula}{〈xp/1〉,〈yp/1〉}, an action {a mathematical formula}apx0 with precondition {a mathematical formula}{〈xp/1〉,〈yp/0〉} and effect {a mathematical formula}{〈xp/0〉}, and an action {a mathematical formula}apy0 with precondition {a mathematical formula}{〈xp/0〉,〈yp/1〉} and effect {a mathematical formula}{〈yp/0〉}. Each individual variable {a mathematical formula}xp and {a mathematical formula}yp is RSE-invertible, but their product is not: we can transition to {a mathematical formula}{〈xp/1〉,〈yp/1〉} but we cannot go back. Thus, for each p in ϕ, we can encode the decision whether to set p to true ({a mathematical formula}〈xp/1〉,〈yp/1〉}) or false ({a mathematical formula}{〈xp/0〉,〈yp/0〉}). Adding, in a straightforward manner, variables and actions that allow to test satisfaction of ϕ for given decisions on each p concludes the argument. The detailed proof is given in Appendix A, p. 111.  □
      </paragraph>
      <paragraph>
       We remark that the proof does not actually require any red variables, so the same result holds for the special case of {a mathematical formula}FDR planning. Note that the issue here lies in the assumption underlying RSE-invertibility, that all outside-effect variables will be painted red: The transition {a mathematical formula}(0,1) of {a mathematical formula}xp (and likewise of {a mathematical formula}yp) is RSE-invertible because the outside condition {a mathematical formula}〈yp/0〉 of the inverse transition {a mathematical formula}(1,0) is contained in {a mathematical formula}ocon(0,1), and the over-writing effect {a mathematical formula}〈yp/1〉∈oeff(0,1) is, under our assumption, red and thus harmless. When merging {a mathematical formula}xp with {a mathematical formula}yp into a single (black) variable, this assumption breaks because the “side effect” is now compiled into that single variable, with black semantics. It remains an open question whether stronger notions of invertibility can yield the ability to naturally handle bounded-size strongly connected components, and whether that yields any advantage in practice.
      </paragraph>
     </section>
    </section>
    <section label="5">
     <section-title>
      Heuristic functions
     </section-title>
     <paragraph>
      With tractability of (satisficing) red–black plan generation, Theorem 11 establishes an essential prerequisite for generating heuristic functions in analogy to relaxed plan heuristics, and it already provides a red–black planning algorithm for doing so. The resulting basic heuristic function, which we denote by {a mathematical formula}hrepairRB, is obtained for any state s by
     </paragraph>
     <list>
      <list-item label="•">
       generating a relaxed plan {a mathematical formula}π+ for s,
      </list-item>
      <list-item label="•">
       returning {a mathematical formula}hrepairRB(s):=∞ if no relaxed plan exists and hence (by Corollary 1) no red–black plan exists either,
      </list-item>
      <list-item label="•">
       else using relaxed plan repair to transform {a mathematical formula}π+ into a red–black plan {a mathematical formula}πRB for s, and
      </list-item>
      <list-item label="•">
       returning the length of {a mathematical formula}πRB as the heuristic value {a mathematical formula}hrepairRB(s).
      </list-item>
     </list>
     <paragraph>
      We herein tackle the obstacles involved in making this practical. Foremost, we observe in Section 5.1 that {a mathematical formula}hrepairRB is prone to dramatic over-estimation even in trivial examples. This is due to bad decisions inherited from the relaxed plan {a mathematical formula}π+, which we fix in Section 5.2 by devising a refined algorithm, red facts following, that relies less on {a mathematical formula}π+; the refined heuristic is denoted {a mathematical formula}hfollowRB. In Section 5.3, we fill in some details, pertaining in particular to our handling of acyclic black causal graphs. Section 5.4 concludes our treatment by empirically showing the merits of {a mathematical formula}hfollowRB vs. {a mathematical formula}hrepairRB, and of using acyclic black causal graphs as opposed to arcless ones (which were used in our prior works on red–black plan heuristics). We will keep the “winning techniques” ({a mathematical formula}hfollowRB, acyclic black causal graphs) fixed for the rest of the paper, simplifying and focusing the subsequent empirical evaluations.
     </paragraph>
     <section label="5.1">
      <section-title>
       Over-estimation in relaxed plan repair
      </section-title>
      <paragraph>
       As a first illustration, consider that in Example 6 we used the relaxed plan {a mathematical formula}〈move(1,2),take(2,A),move(2,3),open(3,4,A),move(3,4),move(4,5),move(5,6),move(6,7),take(7,B),drop(1,B)〉. We showed that relaxed plan repair (with R and F painted black) ends up returning an optimal real plan. What we did not say is that this relies on a particular sequencing of the relaxed plan. Under the delete relaxation, the relaxed plan may just as well start with {a mathematical formula}〈move(1,2),move(2,3),take(2,A),open(3,4,A)〉 instead. If that happens, then a call to {a mathematical formula}Achieve(π,g) before {a mathematical formula}take(2,A) will insert {a mathematical formula}move(3,2), and another call before {a mathematical formula}open(3,4,A) will insert {a mathematical formula}move(2,3), needlessly undoing and re-doing the work involved in getting from position 2 to position 3. While this may seem benign, similar forms of over-estimation are triggered massively by the typical behavior of standard relaxed plan extraction schemes. Consider the following illustrative example:
      </paragraph>
      <paragraph label="Example 7">
       In Fig. 5, truck T needs to transport each package {a mathematical formula}X∈{A,B,C,D} to its respective goal location {a mathematical formula}x∈{a,b,c,d}. The truck can only carry one package at a time, encoded by a binary variable F (“free”). A real plan has length 15 (8 load/unload actions, 7 drive actions). A relaxed plan has length 12 (4 drive actions suffice as there is no need to drive back).Standard relaxed plan extraction schemes (e.g. [6], [23]) can be characterized as starting at the goal facts, selecting a “best supporter” action a for each (minimizing precondition achievement cost, estimated using {a mathematical formula}hmax or {a mathematical formula}hadd[5]), marking a's preconditions as new sub-goals, and iterating until the sub-goals are true in the initial state. Doing this in a breadth-first fashion, and scheduling the actions a in reverse order of selection, we obtain a sequential relaxed plan {a mathematical formula}π+ as needed as input for relaxed plan repair. In our example here, {a mathematical formula}π+ will start with the 4 {a mathematical formula}drive(init,x) actions, followed by the 8 {a mathematical formula}load(X,init) and {a mathematical formula}unload(X,x) actions.Say we paint T and F black and paint the packages red, obtaining an RSE-invertible task with acyclic black causal graph as desired. Example issues are:
      </paragraph>
      <list>
       <list-item label="(A)">
        Say that {a mathematical formula}π+=〈drive(init,a),…,drive(init,d),load(A,init),unload(A,a),…,load(D,init),unload(D,d)〉. Handing this over to relaxed plan repair, processing the {a mathematical formula}drive(init,x) actions will result in a valid path of drives navigating the truck across the entire map – without performing any actual loads or unloads! Subsequently, when {a mathematical formula}load(A,init) comes up, we drive back to init, and drive on to A for {a mathematical formula}unload(A,a); and similarly for {a mathematical formula}B,C,D. The resulting red–black plan duplicates the effort for driving across the entire map.
       </list-item>
       <list-item label="(B)">
        Matters are even worse if {a mathematical formula}π+ schedules {a mathematical formula}load(A,init),…,load(D,init) in front of {a mathematical formula}unload(A,a),…,unload(D,d): When {a mathematical formula}load(B,init) comes up, we need to achieve the black precondition {a mathematical formula}〈F/1〉, the shortest plan for which is to apply {a mathematical formula}unload(A,init). Note that, after the latter action, A is “still in the truck” because the position of A is painted red. The resulting red–black plan duplicates drives and contains superfluous unloading actions (which furthermore cause the red–black plan to be inapplicable in the original task).
       </list-item>
       <list-item label="(C)">
        Finally, say we extend the example by including N endpoint locations, N packages that need to be transported to these, and N trucks, where all packages and trucks are initially in the middle. Then all optimal plans use one truck per package. An optimal relaxed plan, however, can use a single truck. Starting from this, relaxed plan repair will use a single truck as well.{sup:12}
       </list-item>
      </list>
      <paragraph>
       To mention a concrete IPC benchmark in which such things happen, consider, e.g., IPC'11 Elevators. Assume we paint the lift positions black and paint everything else red. Like in Example 7 (A), board/leave actions (“loading/unloading” passengers) will tend to be scheduled behind the elevator moves, resulting in a red–black plan that first moves the elevators all over the place without actually transporting anybody. Similarly as in Example 7 (B), since the relaxed plan is free to choose any board/leave actions, it may decide to use the same lift capacity precondition (typically, the one initially true one for that lift) for all boarding actions. This forces the red–black plan to achieve the desired capacity by applying useless instances of board/leave.
      </paragraph>
      <paragraph>
       An issue not represented in Example 7 are cumbersome solutions enforced by the need to incorporate the relaxed plan actions, whereas not incorporating them allows much simpler (and shorter!) solutions. A prime example for this is IPC'11 VisitAll, painting the robot position black. If, for example, in the current state the robot is located in the right bottom corner of a grid, then the relaxed plan is likely to visit the grid in a breadth-first fashion, going outwards in all directions from that corner. Given this, during relaxed plan repair, when the robot reaches, say, the top right corner, instead of just moving one step to the left (to the nearest yet un-visited grid cell), we move it all the way back to the bottom before moving out from there again.
      </paragraph>
     </section>
     <section label="5.2">
      <section-title>
       A refined algorithm: red facts following
      </section-title>
      <paragraph>
       The major source of the observed difficulties is that relaxed plan repair commits to the particular actions chosen by the relaxed plan, as well as to their ordering. We now define an algorithm, red facts following, that makes do with a much weaker commitment, namely to the set of red facts employed by the relaxed plan. Namely, we consider the set{a mathematical formula} of red facts that are required either by the goal, or by an action precondition in {a mathematical formula}π+.{sup:13} Pseudo-code for our algorithm is shown in Fig. 6.
      </paragraph>
      <paragraph>
       The algorithm maintains two monotonically increasing sets of variable values: R is the set of all currently already achieved red variable values, and B is the set of all black variable values currently achievable under R, i.e., that we can achieve based on using only (red) outside conditions from R. Both R and B are maintained by the Update procedure. Consider that procedure first. For {a mathematical formula}v∈VB, {a mathematical formula}DTGΠ(v)|R∪B is obtained as follows. Let G be the subgraph of {a mathematical formula}DTGΠ(v) obtained by removing all arcs whose outside condition is not contained in {a mathematical formula}R∪B. The graph {a mathematical formula}DTGΠ(v)|R∪B is obtained from G by removing all vertices (and incident arcs) that are not reachable from {a mathematical formula}I[v]. Abusing notation, we use {a mathematical formula}DTGΠ(v)|R∪B to denote both the DTG subgraph and the set of vertices (variable values) of that graph. The updating is done in a topological order, i.e., from the roots of the black causal graph to its leaves, because each variable depends on its parent variables so we need to determine the reachable values for the parent variables first.
      </paragraph>
      <paragraph>
       Getting back to the main procedure, consider the while loop. Our candidate actions for inclusion in the red–black plan prefix, in every iteration of the loop, are given by the set {a mathematical formula}A0 of actions whose preconditions are contained in {a mathematical formula}B∪R (the red precondition is true and the black precondition is achievable), and that achieve at least one fact from {a mathematical formula}R+∖R (the action makes progress on {a mathematical formula}R+). Once an action {a mathematical formula}a∈A0 is selected, the call to {a mathematical formula}Achieve(pre(a)[VB]) serves to find a plan fragment {a mathematical formula}πB establishing its black precondition, if needed. We append {a mathematical formula}πB to the red–black plan prefix π, and we append a itself, then we iterate. In other words, the red facts {a mathematical formula}R+ employed in the relaxed plan serve as targets which we are free to achieve in any order, and using any actions we like, as long as we know we will be able to achieve their black preconditions. Once all of {a mathematical formula}R+ has been achieved, we know that we will be able to achieve the black goal.
      </paragraph>
      <paragraph>
       It remains to explain the {a mathematical formula}Achieve(g) procedure, responsible for generating the plan fragments {a mathematical formula}πB establishing black sub-goals g corresponding to either the precondition of an action in {a mathematical formula}A0, or to the black part of the original goal. Similarly as in relaxed plan repair, this is accomplished via a projected planning task {a mathematical formula}ΠB designed for that purpose. However, the design is more complicated than before. By {a mathematical formula}DTGΠ(v)|R∪B←, we denote the set of “complementary inverse transitions” {a mathematical formula}(d′,a′,d) for {a mathematical formula}DTGΠ(v)|R∪B: For every arc {a mathematical formula}(d,a,d′) in {a mathematical formula}DTGΠ(v)|R∪B, we include into {a mathematical formula}DTGΠ(v)|R∪B← all inverse arcs {a mathematical formula}(d′,a′,d) (i.e., arcs with {a mathematical formula}ocon(d′,a′,d)⊆ocon(d,a,d′)∪oeff(d,a,d′)) that are not already contained in {a mathematical formula}DTGΠ(v)|R∪B itself. Such inverse arcs must make use of at least one yet non-established red outside condition {a mathematical formula}〈v/d〉∈ocon(d′,a′,d)∖R, where by construction we must have {a mathematical formula}〈v/d〉∈oeff(d,a,d′).{sup:14} By {a mathematical formula}VR(ocon(DTGΠ(v)|R∪B←)), we denote the set of red variables appearing in the outside conditions of the arcs {a mathematical formula}DTGΠ(v)|R∪B←. By {a mathematical formula}A(DTGΠ(v)|R∪B←), we denote the set of actions inducing the arcs in {a mathematical formula}DTGΠ(v)|R∪B←.
      </paragraph>
      <paragraph>
       Let us explain why the more complicated construction of {a mathematical formula}ΠB is required. In relaxed plan repair, the sub-goals g to be achieved always consist of facts (variable values) already visited on the plan prefix π. However, in the present algorithm g may contain facts that we can reach given the current R, but that we haven't actually visited yet. This leads to complications with our generous definition of invertibility, where the inverse transition {a mathematical formula}(d′,d) may make use of red outside conditions that will be established only through the red outside effect when executing the original transition{a mathematical formula}(d,d′). To capture this behavior, we need to keep track of which potential red outside conditions of inverse transitions – values of variables {a mathematical formula}VR(ocon(DTGΠ(v)|R∪B←)) – are currently true, and we need to include the actions {a mathematical formula}A(DTGΠ(v)|R∪B←) which will then be usable to execute the activated inverse transitions. This is best understood through an example:
      </paragraph>
      <paragraph label="Example 8">
       Consider an {a mathematical formula}RB task with two black variables {a mathematical formula}b1,b2 and one red variable r. All variables are binary-valued and initialized to 0. The goal is {a mathematical formula}〈b1/0〉,〈b2/1〉. The actions are {a mathematical formula}a1fwd with precondition {a mathematical formula}〈b1/0〉 and effect {a mathematical formula}〈b1/1〉,〈r/1〉; {a mathematical formula}a1bwd with precondition {a mathematical formula}〈b1/1〉,〈r/1〉 and effect {a mathematical formula}〈b1/0〉; {a mathematical formula}a2fwd with precondition {a mathematical formula}〈b2/0〉,〈b1/1〉 and effect {a mathematical formula}〈b2/1〉; and {a mathematical formula}a2bwd with precondition {a mathematical formula}〈b2/1〉,〈b1/1〉 and effect {a mathematical formula}〈b2/0〉. The black causal graph is acyclic and both black variables are RSE-invertible. Say the relaxed plan is {a mathematical formula}〈a1fwd,a2fwd〉, hence {a mathematical formula}R+=∅, and hence the while loop in Fig. 6 terminates immediately and we get a single call of {a mathematical formula}Achieve(g), on the original goal.Assume for the moment that, in {a mathematical formula}Achieve(g), we would be using a simpler construction {a mathematical formula}ΠB with only the black variables, and not including the actions {a mathematical formula}A(DTGΠ(v)|R∪B←). Then {a mathematical formula}ΠB would have variables {a mathematical formula}b1 and {a mathematical formula}b2, including their values {a mathematical formula}〈b1/1〉 and {a mathematical formula}〈b2/1〉 because {a mathematical formula}〈b1/1〉 can be reached given {a mathematical formula}R={〈r/0〉} and {a mathematical formula}〈b2/1〉 can be reached given {a mathematical formula}〈b1/1〉, but not including {a mathematical formula}a1bwd because that requires the red outside condition {a mathematical formula}〈r/1〉∉R. This {a mathematical formula}ΠB is unsolvable because, while we can bring {a mathematical formula}b1 to value 1 as required for moving {a mathematical formula}b2, we cannot bring {a mathematical formula}b1 back into value 0 as required for its own goal.To ensure solvability of {a mathematical formula}ΠB, we must ascertain that we can always “go back”. RSE-invertibility does ensure that, but subject to red outside conditions that will be established “on the way out”. Using {a mathematical formula}Achieve(g) as stated in Fig. 6, {a mathematical formula}DTGΠ(b1)|R∪B← consists of the single arc {a mathematical formula}(1,0) with outside condition {a mathematical formula}r=1, and {a mathematical formula}DTGΠ(b2)|R∪B← is empty. Thus {a mathematical formula}ΠB has all three variables because {a mathematical formula}r∈V←R=VR(ocon(DTGΠ(b1)|R∪B←))∪VR(ocon(DTGΠ(b2)|R∪B←)), and includes all actions because {a mathematical formula}a1bwd∈A(DTGΠ(b1)|R∪B←).
      </paragraph>
      <paragraph>
       Note that {a mathematical formula}ΠB here is not a standard {a mathematical formula}FDR task, but is a red–black planning task itself. It is a benign kind of red–black planning task though, because its black causal graph is acyclic, there aren't any goals on the red variables, and for each black DTG we know that (a) from our initial position we can reach all values, and (b) once we traversed any arc {a mathematical formula}(d,d′) we will have the red outside conditions required for at least one inverse arc {a mathematical formula}(d′,d). Like acyclic {a mathematical formula}FDR tasks with strongly connected DTGs, such {a mathematical formula}RB tasks are always solvable (and, as we shall describe further below, can be solved very similarly). Hence our algorithm works as desired:
      </paragraph>
      <paragraph label="Theorem 13">
       Let{a mathematical formula}Π=〈VB,VR,A,I,G〉be an RSE-invertible {a mathematical formula}RB planning task with acyclic black causal graph,{a mathematical formula}π+be a relaxed plan for Π, and{a mathematical formula}R+=G[VR]∪⋃a∈π+pre(a)[VR]. Then, assuming a complete solver for the sub-tasks{a mathematical formula}ΠBgenerated,{a mathematical formula}RedFactsFollowing(Π,R+)terminates, and the action sequence π it returns is a plan for Π.
      </paragraph>
      <paragraph label="Proof sketch">
       The proof is very similar to that of Theorem 11, and it is given in Appendix A, p. 111. The major differences lie in the structure of the main loop (following {a mathematical formula}R+ instead of the actions in the relaxed plan) and in the details regarding the sub-tasks {a mathematical formula}ΠB.The while loop terminates because, as long as {a mathematical formula}R⊉R+, we always have {a mathematical formula}A0≠∅. This is simply because there always exists an action in {a mathematical formula}π+ which is a member of {a mathematical formula}A0: As {a mathematical formula}π+ achieves all of {a mathematical formula}R+, there must be at least one action {a mathematical formula}ai∈π+ with {a mathematical formula}eff(ai)∩(R+∖R)≠∅; for the smallest such index i, it is easy to see that the {a mathematical formula}π+ prefix up to i cannot make use of any preconditions outside {a mathematical formula}R∪B. Similarly, once the while loop has terminated, we must have {a mathematical formula}G[VR]⊆R simply because {a mathematical formula}G[VR]⊆R+, and we must have {a mathematical formula}G[VB]⊆B because {a mathematical formula}π+ cannot make use of any preconditions outside {a mathematical formula}R∪B.The correctness of precondition and goal achievement, i.e., the correct functioning of the calls to {a mathematical formula}Achieve(g), follows, similarly as in the proof of Theorem 11, from three properties of the sub-tasks {a mathematical formula}ΠB: (i) {a mathematical formula}ΠB is well-defined; (ii) {a mathematical formula}ΠB is solvable; and (iii) any plan {a mathematical formula}πB for {a mathematical formula}ΠB is, in the {a mathematical formula}RB task Π, applicable in {a mathematical formula}I〚π〛. Thanks to (i) and (ii), we will obtain a plan {a mathematical formula}πB for {a mathematical formula}ΠB. Thanks to (iii) it is valid to append {a mathematical formula}πB to π. Furthermore, while {a mathematical formula}ΠB ignores some of the red variables, effects on these cannot hurt anyway, so (in the while loop) the action a is applicable in {a mathematical formula}I〚π⋅πB〛.The proofs for (i) and (iii) are minor extensions to those in the proof of Theorem 11. The proof of (ii) is by an extension of Helmert's Observation 7. Recall that, by this observation, any {a mathematical formula}FDR task with acyclic causal graph and strongly connected domain transition graphs is solvable. This follows from:
       <list>
        Acyclicity of the causal graph implies that we can solve the planning task “top-down”, from causal graph leaves to roots, fixing a DTG path for each variable v and propagating the required preconditions as sub-goals to v's parents.As every DTG is strongly connected, every required path is available, i.e., every variable can always move from its current value to any other value it is required to achieve as a sub-goal (or its own goal).It remains to prove (a) and (b). Both arguments are very similar to how we showed in
       </list>
       <paragraph>
        Theorem 11 that DTGs are strongly connected: Once we actually executed an action in {a mathematical formula}ΠB, the red outside conditions needed for the inverse action are true. This shows (a) because, by construction, all values in {a mathematical formula}DB(v)=DTGΠ(v)|R∪B are reachable from {a mathematical formula}I[v], and as π induces a path from {a mathematical formula}I[v] to {a mathematical formula}I〚π〛[v] in {a mathematical formula}DTGΠ(v)|R∪B we can invert that path to go back from {a mathematical formula}I〚π〛[v] to {a mathematical formula}I[v]. For (b), once we applied {a mathematical formula}aB in {a mathematical formula}ΠB where {a mathematical formula}a′ is the corresponding inverse action, if {a mathematical formula}pre(a′)⊆R∪B then the claim is trivial. If {a mathematical formula}pre(a′)⊈R∪B then {a mathematical formula}a′∈A(DTGΠ(v)|R∪B←), so we have {a mathematical formula}a′B∈AB and {a mathematical formula}V(pre(a′))∩VR⊆V←R. This concludes the proof as, by construction, {a mathematical formula}pre(a′)[VR]⊆pre(a)∪eff(a).  □
       </paragraph>
      </paragraph>
      <paragraph>
       Does our refined algorithm actually yield a benefit? Does {a mathematical formula}hfollowRB reduce over-estimation, compared to {a mathematical formula}hrepairRB? Further below, we provide empirical data strongly supporting that the answer is “yes”. For now, let's reconsider the examples from Section 5.1.
      </paragraph>
      <paragraph label="Example 9">
       In the StarShapeLogistics example from Fig. 5, no matter how we order the relaxed plan, the set {a mathematical formula}R+ will consist of the facts {a mathematical formula}{〈A/init〉,〈A/T〉,〈A/a〉,…,〈D/init〉,〈D/T〉,〈D/d〉}. The truck moves are then completely up to the calls of the {a mathematical formula}Achieve(g) procedure, and the while loop is free to choose the order in which to tackle the sub-goals (loading a package, bringing it to its goal position). Hence, assuming that {a mathematical formula}Achieve(g) finds short plans, and the choice of {a mathematical formula}a∈A0 is done intelligently enough to prefer delivering a package once it is loaded – which we do accomplish in our implementation, see Section 5.3.2 – the issues pointed out in Example 7 (A) and (B) are solved.The issue pointed out in Example 7 (C), however, persists. If the relaxed plan uses a single truck T only, then {a mathematical formula}R+ will have the same form as above, fixing usage of T through the red facts {a mathematical formula}〈A/T〉,…,〈D/T〉. It remains an open question how to solve this, and related issues of resource assignment. Perhaps low-conflict relaxed plans [18] could deliver input better suited to that purpose.
      </paragraph>
      <paragraph>
       As far as the two IPC benchmarks we pointed out are concerned, matters are fine. In Elevators, painting the lift positions black and painting everything else red, {a mathematical formula}R+ consists of passenger initial, intermediate (in a lift) and goal locations, as well as of lift capacities required along {a mathematical formula}π+. The actions achieving these facts are board/leave; these are the actions selected by the main loop, and moving the lifts is entirely up to {a mathematical formula}Achieve(g), solving issue (A). Regarding issue (B), assume that all board actions in {a mathematical formula}π+ are preconditioned by the initially true capacity of that lift ({a mathematical formula}cl). Then all leave actions in {a mathematical formula}π+ will be preconditioned by the {a mathematical formula}cl−1 capacity, and {a mathematical formula}cl and {a mathematical formula}cl−1 are the only capacity-related facts in {a mathematical formula}R+. As {a mathematical formula}cl is true in the initial state, and {a mathematical formula}cl−1 is achieved by the first board action into the respective lift, after that action there are no more capacity-related facts in {a mathematical formula}R+∖R. Thus action selection in the main loop will be based exclusively on following red facts related to the passenger locations.
      </paragraph>
      <paragraph>
       In VisitAll, painting the robot variable black and everything else red, {a mathematical formula}R+ consists of “visited(location)” facts exclusively, and the robot moves are mainly determined by how the next such fact is selected for achievement, i.e., how we select {a mathematical formula}a∈A0. Based on the selection criteria we use in our implementation (Section 5.3.2), the red–black planner will always move to a yet non-visited location nearby, and the issue is solved.
      </paragraph>
     </section>
     <section label="5.3">
      <section-title>
       Realization details
      </section-title>
      <paragraph>
       We first fill in the details how we handle the sub-tasks {a mathematical formula}ΠB that need to be solved for black precondition (and goal) achievement within both relaxed plan repair and red facts following. We then describe important optimizations regarding the choice points in red facts following.
      </paragraph>
      <section label="5.3.1">
       Solving the sub-tasks {a mathematical formula}ΠB
       <paragraph>
        Consider first the sub-tasks {a mathematical formula}ΠB in relaxed plan repair, which are {a mathematical formula}FDR tasks with acyclic causal graph and strongly connected DTGs (namely, {a mathematical formula}DTGΠB(v) is the subgraph of {a mathematical formula}DTGΠ(v) induced by the values {a mathematical formula}D(v)∩F, cf. Fig. 4). As discussed in the context of Lemma 1, plan generation for such tasks is polynomial time, using a succinct plan representation (required as plans may be exponentially long). The succinct plan representation, devised by Chen and Giménez [32], consists of recursive macro actions for pairs of initial-value/other-value within each variable's DTG. That approach, while superior in theory, has several disadvantages that make its practicality in our setting more than doubtful. First, generating the macros involves the exhaustive enumeration of shortest paths for initial-value/other-value pairs in all DTGs, which must be done anew for every call of {a mathematical formula}Achieve(g) (i.e., several times inside each invocation of the heuristic!), as each task {a mathematical formula}ΠB has its own individual initial state and DTGs. Second, the macros yield highly redundant plans moving parent variables back to their initial value in between every two sub-goal requests. For example, if a truck unloads two packages at the same location, then it is moved back to its start location in between the two unload actions. Finally, the tasks {a mathematical formula}ΠB will typically have small plans anyhow – after all, we merely wish to achieve the next action's black preconditions – so why should we bother to represent these plans compactly?
       </paragraph>
       <paragraph>
        Given these considerations, we decided to use an explicit plan representation instead, trading the theoretical worst-case efficiency of Chen and Giménez' macros against the practical advantages of less overhead and (potentially) shorter plans. After exploring a few options, we settled on the simple algorithm in Fig. 7. Starting at the leaf variables and working up to the roots, the partial plan {a mathematical formula}πB is augmented with plan fragments (DTG paths) bringing the supporting variables into place. This is essentially the same algorithm as described by Helmert [29] as a proof for his Observation 7 (Helmert did not actually implement and use that algorithm, though). It is easy to see that:
       </paragraph>
       <paragraph label="Proposition 2">
        The algorithm{a mathematical formula}AcyclicPlanning(ΠB)is sound and complete, and its runtime is polynomial in the size of{a mathematical formula}ΠBand the length of the plan{a mathematical formula}πBreturned.
       </paragraph>
       <paragraph label="Proof sketch">
        In Appendix A, p. 112, we show by induction that, at the end of each iteration i of the for-loop, {a mathematical formula}πB is a plan for {a mathematical formula}ΠB projected on variables {a mathematical formula}vi,…,vn. This is trivial for {a mathematical formula}i=n. Given it holds for {a mathematical formula}i+1,…,n, it also holds for i because, by acyclicity of the causal graph, the actions inserted to move {a mathematical formula}vi do not affect any other variables, and do not rely on any preconditions on the variables {a mathematical formula}vi+1,…,vn.  □
       </paragraph>
       <paragraph>
        As indicated, the length of {a mathematical formula}πB here is worst-case exponential in the size of {a mathematical formula}ΠB, and so is the runtime of {a mathematical formula}AcyclicPlanning(ΠB).{sup:15} Unlike the macro-based algorithm of Chen and Giménez, our algorithm does not superfluously keep switching supporting variables back to their initial values. But it is not especially clever, either: If variable {a mathematical formula}v0 supports two otherwise independent leaf variables {a mathematical formula}v1 and {a mathematical formula}v2, then the sub-plans for {a mathematical formula}v1 and {a mathematical formula}v2 will be inserted sequentially into {a mathematical formula}πB, losing any potential for synergies in the values of {a mathematical formula}v0 required. We performed a limited investigation into more flexible algorithms addressing that weakness through using a partially-ordered {a mathematical formula}πB, but these algorithms required non-trivial book-keeping, and initial implementations did not yield any apparent benefits. It remains an open question whether something can be gained by a more complex machinery here.
       </paragraph>
       <paragraph>
        In red facts following, the sub-tasks {a mathematical formula}ΠB are red–black planning tasks, with the weaker properties discussed above: the black causal graph is acyclic, there aren't any goals on the red variables, and for each black DTG we know that (a) from our initial position we can reach all values, and (b) once we traversed any arc {a mathematical formula}(d,d′) we will have the red outside conditions required for at least one inverse arc {a mathematical formula}(d′,d). As argued in the proof of Theorem 13, these tasks are still guaranteed to be solvable. We can use the same decomposition method (solving {a mathematical formula}ΠB from leaves to roots of the black causal graph), and whenever we need a DTG path from the current value to a sub-goal value {a mathematical formula}dg of a black variable v, by (b) we know that v can “go back” to its start value, and by (a) v can reach {a mathematical formula}dg from there. Therefore, to appropriately extend the algorithm as depicted in Fig. 7, there is no need to consider the red–black planning tasks {a mathematical formula}ΠB as defined in Fig. 6. Instead, we focus on the black variables in these {a mathematical formula}ΠB tasks, and initialize their DTGs as {a mathematical formula}DTGΠB(v):=DTGΠ(v)|R∪B. We collect, for each variable {a mathematical formula}vi individually (i.e., within each iteration of the for loop in Fig. 7), the red side effects {a mathematical formula}Rvi of the current {a mathematical formula}πB prefix affecting {a mathematical formula}vi, extending {a mathematical formula}DTGΠB(v) with the transitions whose red outside conditions are contained in {a mathematical formula}R∪Rvi. In fact, we do so only in case there is no path in {a mathematical formula}DTGΠB(v) to the current sub-goal value {a mathematical formula}dg. This is to avoid unnecessary computational overhead: In all IPC benchmarks, and in all search states that were encountered in these benchmarks during our experiments, the necessary DTG paths during {a mathematical formula}AcyclicPlanning(ΠB) were present in {a mathematical formula}DTGΠ(v)|R∪B already, without enabling any new transitions based on red side effects. (In other words, situations as in Example 8 appear to be extremely rare in practice.)
       </paragraph>
       <paragraph>
        In previous works on red–black plan heuristics [2], [3], we made use of a simpler tractable fragment of red–black plan generation, namely that where the black causal graph is arcless. In this special case, the planning tasks solved inside {a mathematical formula}Achieve(g) are trivial: All black variables v are completely independent, and it suffices to find a DTG path from v's current value to {a mathematical formula}g[v] (if defined) for each v individually. This reduces the computational effort required to compute the heuristic function, at the price of a potential loss in accuracy. As we shall see below in Section 5.4, the more complex heuristics based on acyclic black causal graphs tend to pay off in domains where non-trivial acyclic black causal graphs occur. (In case the black causal graph is arcless, our more complex heuristics simplify to the heuristics defined for that special case.)
       </paragraph>
      </section>
      <section label="5.3.2">
       <section-title>
        Instantiating the choice points
       </section-title>
       <paragraph>
        The main choice points are (i) selecting actions {a mathematical formula}a∈A0 in the while loop (pertains to red facts following), and (ii) selecting DTG paths for black variables in the solution to {a mathematical formula}ΠB, i.e., inside {a mathematical formula}Achieve(g) (pertains to both, red facts following and relaxed plan repair). We would like to make both choices in a way such that the returned red–black plan π is (a) short, and (b) executable as much as possible in the original {a mathematical formula}FDR  planning task we are trying to solve.
       </paragraph>
       <paragraph>
        The importance of (a) should be self-evident (avoiding over-estimation). (b) is important because red–black plans, compared to fully-delete relaxed plans, have a much higher chance of actually working in reality. We exploit this property by a simple method we refer to as stop search: If the red–black plan π generated for a search state s is a plan for s in the original {a mathematical formula}FDR task, then we stop and output π pre-fixed by the action sequences that lead to s. For illustration, in StarShapeLogistics (Fig. 5), a fully relaxed plan will not work unless we have already transported all but one package. If we paint just T black, then the red–black plan for the initial state might work (in case it happens to make the right choices where to place the load and unload actions). If we paint both T and F black, then every optimal red–black plan for the initial state definitely works, and we can stop the search before we have even started it. (The restriction to optimal red–black plans is needed here because, package variables being red, non-optimal red–black plans may contain superfluous load and unload actions.)
       </paragraph>
       <paragraph>
        Towards finding (a) short red–black plans, our simple measure in choice point (ii) is to find shortest DTG paths (cf. Fig. 7). Choice point (i) is more important, and more difficult to handle. The set of actions {a mathematical formula}A0 achieving some fact from {a mathematical formula}R+∖R often is quite large (for example, in VisitAll when painting just the robot position black, {a mathematical formula}A0 contains every action moving into any yet non-visited position in the grid). The straightforward criterion is to select an action achieving whose precondition takes the smallest number of steps. We approximate this number, for any action {a mathematical formula}a∈A0, by pre-computing all-pairs shortest path distances for each black variable v, and summing up the distance from {a mathematical formula}I〚π〛[v] to {a mathematical formula}pre(a)[v] over all of a's black precondition variables {a mathematical formula}v∈VB∩V(pre(a)). (Note that these estimates are exact if, and only if, the paths underlying these distances do not rely on any outside conditions that we would need to establish.)
       </paragraph>
       <paragraph>
        Towards enhancing (b) {a mathematical formula}FDR-executability of the red–black plan, we designed a simple criterion for each of the choice points (i) and (ii). To illustrate our criterion for (i), say that, in StarShapeLogistics, T and F are painted black, {a mathematical formula}R+={〈A/init〉,〈A/T〉,〈A/a〉,…,〈D/init〉,〈D/T〉,〈D/d〉}, and red facts following started by selecting {a mathematical formula}load(A,init). Then {a mathematical formula}unload(A,a)might be selected next, but the algorithm might just as well select {a mathematical formula}load(B,init) because the (estimated and real) number of steps for achieving the precondition is 1 for each of these actions: Exactly one black precondition needs to be achieved, namely {a mathematical formula}〈T/a〉 for {a mathematical formula}unload(A,a) and {a mathematical formula}〈F/1〉 for {a mathematical formula}load(B,init), and each of these takes a single transition in the respective DTG, induced by {a mathematical formula}drive(init,a) respectively {a mathematical formula}unload(A,init). Regarding {a mathematical formula}unload(A,init), note that variable A is red, so the detrimental side effect is ignored, and later on the red–black plan will apply {a mathematical formula}unload(A,a) without re-loading A in between, losing {a mathematical formula}FDR-executability. In other words, unless we manage to distinguish between {a mathematical formula}drive(init,a) respectively {a mathematical formula}unload(A,init) here, we suffer from a similar issue as pointed out in Example 7 (B). The same phenomenon may occur in any domain with renewable resources. We tackle it by giving a preference to actions {a mathematical formula}a∈A0 getting whose black preconditions does not involve deleting {a mathematical formula}R+ facts already achieved beforehand. To avoid excessive overhead, we approximate this by recording, in a pre-process, which red facts may be deleted by moving each black variable, and prefer an action if none of its black preconditions may incur any such side effects. In our example, moving F may incur such side effects, but moving T may not.
       </paragraph>
       <paragraph>
        In choice point (ii), we enhance {a mathematical formula}FDR-executability simply by preferring executable DTG paths. This pertains exclusively to the red outside conditions on the paths. We say that such a condition is “active” if it is true when executing the current red–black plan prefix under the {a mathematical formula}FDR (fully un-relaxed) semantics. As long as there exists at least one DTG path all of whose red outside conditions are active, we use a shortest such path. (E.g., if a storage-capacity variable is red, then this will prefer loads/unloads that use the actual capacity instead of an arbitrary one.)
       </paragraph>
      </section>
     </section>
     <section label="5.4">
      <section-title>
       Evaluation
      </section-title>
      <paragraph>
       As indicated, we include experiments at this point already in order to evaluate specific aspects of our algorithm design so far, and to simplify the experiments in the remainder of the paper by fixing the “winning techniques”. We evaluate, in this order, (i) the advantage of red facts following over relaxed plan repair, (ii) the impact of the stop search method and the associated FDR applicability enhancements, and (iii) the advantage of using acyclic black causal graphs over using arcless black causal graphs.
      </paragraph>
      <paragraph>
       All our techniques are implemented on top of Fast Downward (FD). All experiments in this paper are run on a cluster of Intel E5-2660 machines running at 2.20 GHz, with runtime (memory) limits of 30 minutes (2 GB). We run all satisficing-track STRIPS benchmarks from the FD benchmark collection, i.e., benchmarks from the IPC satisficing/deterministic/sequential tracks (where distinguished from other forms of planning). We consider uniform costs throughout, ignoring action costs where specified. Since we cannot paint any variable black if there are no RSE-invertible variables, we omit instances in which that is the case, and we omit domains where it is the case for all instances. These domains are (all variants of) Airport, Freecell, Openstacks, Parking, and Pathways.
      </paragraph>
      <paragraph>
       To keep things simple, in all of (i)–(iii) here we fix a canonical search algorithm, namely FD's greedy best-first search with lazy evaluation and a second open list using preferred operators [29], [46]. We also fix a simple painting strategy, i.e., a method for deciding which variables to paint red or black. The strategy is based on Fast Downward's level ordering of the variables [12], [29]. It prefers to paint red the variables with higher level, i.e., the variables “close to the causal graph leaves”. We will describe the strategy in detail, along with all other painting strategies we use, in Section 6. The qualitative picture of the following results is similar for our other painting strategies.
      </paragraph>
      <paragraph>
       Even though each of our experiments will focus on particular algorithm parameters and/or performance aspects only, for easier reference across experiments, we employ a system of acronyms to identify configurations. Again for easier cross-reference, we choose these acronyms to be consistent with those in our earlier works [2], [3]. Red facts following is denoted by R and relaxed plan repair is denoted by F (this seemingly unintuitive notation was used by Katz et al. [2] in reference to a minor optimization of re-ordering the relaxed plan by Forwarding actions with no black effects). The use of acyclic black causal graphs is denoted D (for “DAG”) and that of arcless black causal graphs is denoted E. The painting strategy described above is denoted L. If stop search is in use, we denote that by S. For example, “FEL” is the configuration that uses relaxed plan repair with arcless black causal graphs and painting strategy L and stop search switched off, whereas “RDLS” is the configuration that uses red facts following with DAG black causal graphs and painting strategy L and stop search switched on. (Other configuration parameters are less important so we don't define acronyms for them.) Table 1 shows our data regarding (i) and (ii).
      </paragraph>
      <paragraph>
       Consider first the left half of the table. In the “No PO” configurations, we measure the quality of the respective heuristic functions {a mathematical formula}hrepairRB (F) vs.  {a mathematical formula}hfollowRB (R) in the most basic setting possible, plugging them into a plain best-first search without the two search enhancements (preferred operators and stop search). The coverage data resulting from this – overall, 778 for {a mathematical formula}hrepairRB vs. 903 for {a mathematical formula}hfollowRB – impressively confirms the advantage of red facts following over relaxed plan repair. This drastic advantage gets watered down when turning on the search enhancements, as these can yield substantial improvements even where the underlying heuristic has low quality (e.g., in VisitAll, see also next).
      </paragraph>
      <paragraph>
       The “Average {a mathematical formula}h(I)” columns illuminate the difference between {a mathematical formula}hrepairRB and {a mathematical formula}hfollowRB in terms of their plan length estimation in the initial state. As is evident, {a mathematical formula}hfollowRBdoes yield much shorter red–black plans than {a mathematical formula}hrepairRB in many domains. Consistently across all domains, {a mathematical formula}hfollowRB never yields larger average red–black plan length. Consider VisitAll, where the behavior is most extreme. {a mathematical formula}hrepairRB overestimates dramatically so is not useful in search, in contrast to {a mathematical formula}hfollowRB, cf. our discussion of over-estimation in this domain (Sections 5.1 and 5.2) and the coverage data for FDL and RDL. Stop search fixes this issue in FDLS, but the plans correspond to the “Average {a mathematical formula}h(I)” column so are of extremely poor quality. Note that shorter red–black plans (even if they are not {a mathematical formula}FDR-executable) are always better in the sense that, ideally, we would like our heuristic to return {a mathematical formula}hRB⁎: any value larger than this, even if it happens to be closer to {a mathematical formula}h⁎ than {a mathematical formula}hRB⁎, is not justified by the red–black relaxation and must be attributed to arbitrary phenomena in the practical heuristic, as opposed to systematic estimates of goal distance.
      </paragraph>
      <paragraph>
       Consider now the right half of Table 1. For coverage, the RDLS column is identical to that in the left half of the table, we repeat it here just for ease of reading. As can be seen, the impact of stop search in terms of coverage is small. Without our {a mathematical formula}FDR-executability enhancements, i.e., for RDLS-, coverage is identical to RDL except in Driverlog, Sokoban11, and VisitAll, in each of which RDLS- solves a single instance more. With the executability enhancements, coverage gets worse in Sokoban but a bit better in Floortile and a lot better in Transport, resulting in an overall “net win” for RDLS by a small margin. In other words, when using {a mathematical formula}hfollowRB with DAG causal graphs, coverage is already too high to allow to discriminate between stop search vs. no stop search (we show data for some larger, non-IPC, test instances below in Table 2). The more fine-grained data regarding the number of evaluated states goes to show that, actually, search space size decreases substantially across a range of IPC domains and IPC test instances: 9 of our 29 domains here, not counting IPC'08/IPC'11 duplicates. In 3 of these 9 domains (NoMystery, Transport, and Zenotravel), RDLS has a significant advantage over RDLS-. As the “Solved in I” data shows, in most (though not all) of these 9 domains, stop search always fires on the initial state. Consistently across all the domains where it sometimes fires on the initial state, stop search returns plans of very similar quality as would be returned by the search itself.
      </paragraph>
      <paragraph>
       Consider finally the issue (iii) of DAG black causal graphs (D) vs. arcless black causal graphs (E), Table 2. The table is much smaller because, for this comparison, we need to consider only those IPC instances whose causal graph has at least one directed arc{a mathematical formula}(v,v′)between RSE-invertible variables v and{a mathematical formula}v′, with no backwards arc{a mathematical formula}(v′,v). These are exactly the tasks for which there exists a choice of black variables so that (a) the resulting red–black planning task is inside our tractable fragment, and (b) the black causal graph is a non-arcless  DAG. Table 2 contains only these tasks. In other tasks, each of our D configurations simplifies exactly to the corresponding E configuration.
      </paragraph>
      <paragraph>
       The “main” data for this part of our evaluation, considering IPC benchmark instances, is in the left half of Table 2. In terms of coverage, the richer structure underlying the DAG heuristic pays off mainly in Transport, and a little bit in Elevators; it leads to somewhat worse performance in Trucks. The evaluations data illuminates the advantage in terms of search space size, which extends also to the Gripper domain. This advantage is due to stop search, which in the three domains in question – Elevators, Gripper, Transport – always fires on the initial state (cf. our discussion of Table 1 above). To shed some more light on this, the right half of Table 2 considers extreme cases in these domains, with instances much larger than those used in the IPC. Stop search still fires immediately, scaling to almost arbitrary instance sizes which are completely unfeasible for search, i.e., when either using the weaker E relaxation (RELS) where stop search does not fire, or when switching stop search off (RDL). As the number of evaluations for RDLS is constant 1, the evaluations data here also shows us that, even without stop search, the DAG heuristic has an advantage over the arcless one.
      </paragraph>
      <paragraph>
       We remark that Table 2 is the only point here where the particular painting strategy L we chose for this presentation does make a substantial difference. Stop search for the DAG heuristic fires on the initial states of Elevators and Transport for about half of our painting strategies (including L). Using one of the painting strategies from the “bad half” does not affect coverage in the IPC instances, but it does affect coverage on our extended instances, and it of course affects the number of state evaluations in those two domains (see also Table 4 in the next section).
      </paragraph>
     </section>
    </section>
    <section label="6">
     <section-title>
      Painting strategies
     </section-title>
     <paragraph>
      As yet, we have not specified how to automatically choose which variables to paint red, and which variables to paint black. We refer to strategies for making that choice as painting strategies. We now introduce a family of such strategies and examine their behavior. We start in Section 6.1 by making a few basic observations and describing our strategies. We evaluate the strategies in Section 6.2, drawing the high-level conclusion that “the performance differences between different painting strategies are typically minor, and sometimes brittle with respect to small changes”. To shed additional light on this, we also examine the behavior of random painting strategies. Similarly as we did for the red–black planning variants in Section 5, we make a selection of “winning strategies” and keep these fixed for the rest of the paper.
     </paragraph>
     <section label="6.1">
      <section-title>
       Painting strategy design
      </section-title>
      <paragraph>
       The first observation to be made when designing painting strategies is that there actually are two kinds of variables that can be immediately excluded from those that might end up being painted black. The first condition is obvious and was already discussed beforehand: As our tractable fragment requires all black variables to be RSE-invertible, variables that are not RSE-invertible can immediately be painted red. The second condition is slightly less obvious:
      </paragraph>
      <paragraph label="Theorem 14">
       Let Π be an{a mathematical formula}FDRplanning task and let{a mathematical formula}VRbe a subset of its variables. If all variables in{a mathematical formula}VRhave no outgoing arcs in{a mathematical formula}CGΠ, then{a mathematical formula}hVRRB⁎is perfect.
      </paragraph>
      <paragraph label="Proof">
       It clearly suffices to show that, in this setting, all non-redundant red–black relaxed plans, i.e., red–black relaxed plans that do not contain any superfluous actions, are valid plans for the original {a mathematical formula}FDR task Π. Which is true because the leaf variables v in {a mathematical formula}CGΠ are neither (1) used to support value changes of any other variables, nor (2) affected as a side effect of changing any other variables. Due to (1), any non-redundant red–black relaxed plan either changes v along a simple (acyclic) path from v's initial value to its goal value, or leaves v untouched in case it has no goal. That same path will be executable within Π. Due to (2), such execution is not interfered with by any other actions in the red–black relaxed plan. This concludes the argument.  □
      </paragraph>
      <paragraph>
       In other words, causal graph leaves can be painted red without affecting the (ideal) red–black plan heuristic.
      </paragraph>
      <paragraph>
       Our second observation regards paintings that are maximal in the sense that, if we paint one more variable black, then the black causal graph is no longer acyclic. Such paintings are always preferable over non-maximal ones, at least in theory, because {a mathematical formula}hRB⁎ grows monotonically with the set of black variables, cf. Proposition 1. We therefore design our painting strategies in a way guaranteeing maximality. We get back to the practical implications of this below, after explaining our painting algorithm.
      </paragraph>
      <paragraph>
       All our painting strategies proceed by iteratively painting variables red (starting from the set of variables identified above), until all cycles in the causal graph are broken. We then employ a simple post-processing step to ensure maximality. See Fig. 8.
      </paragraph>
      <paragraph>
       Note that this pseudo-code is used for configurations D, i.e., DAG black causal graphs, which we keep fixed for the rest of the paper. For configurations E, we simply test whether the black causal graph is arcless, as opposed to acyclic. The post-processing step checks, for each variable that was selected to be painted red, whether that variable can now be removed from the set of red variables (i.e., be painted black) without breaking acyclicity. The ordering of these checks is not important as far as the maximality guarantee is concerned. We order them from back to front because, in all our painting strategies, the “least important” variables are painted red first. Given this, the ordering from back to front gives a preference to removing the more important variables from {a mathematical formula}VR.
      </paragraph>
      <paragraph>
       The post-process is required for maximality because a new variable v added into {a mathematical formula}VR might break all cycles concerning a previously-added variable {a mathematical formula}v′. For example, in StarShapeLogistics (Fig. 5), there is a cycle between every package {a mathematical formula}X∈{A,B,C,D} and the capacity variable F. If the algorithm selects, say, A, B, and C first, and only thereafter selects F, then painting A, B, and C red has become redundant. This will be recognized by the post-process.
      </paragraph>
      <paragraph>
       We remark that, in our prior works on red–black plan heuristics [2], [3], we did not use the post-processing step because we overlooked the lack of maximality in this setup. Thus, we effectively employed painting strategies geared at finding maximal paintings, but not giving a guarantee. Now, while in theory the heuristic function can only get better with more black variables, in practice of course this is not as clear. As far as the IPC benchmarks are concerned, our modified (maximality-guaranteeing) painting strategies here give performance very similar to the previous ones, better in some domains, and never worse except for a minor performance loss in Trucks. We also performed limited experimentation with painting strategies geared at finding non-maximal paintings: these keep painting variables red even when the black causal graph is already acyclic, stopping at a randomly selected time point. It appears that maximal paintings tend to work better. But exploring this comprehensively is an open topic.
      </paragraph>
      <paragraph>
       It remains to instantiate the choice point in Fig. 8. We have devised a variety of methods for doing so. The common rationale behind all of these is the attempt to paint black as many variables as possible, and/or for the black variables to be as “important” as possible. The strategies differ in how they attempt to achieve these objectives:
      </paragraph>
      <list>
       <list-item label="•">
        L: Selects v with highest level in the causal graph heuristic [12], i.e., “closest to the causal graph leaves”. This aims at painting red the “client” variables, which do not tend to move back and forth to suit the needs of other variables.
       </list-item>
       <list-item label="•">
        A: Select v with the maximal number {a mathematical formula}A(v) of incident arcs to black variables, i.e., to variables in {a mathematical formula}V∖VR. The intuition behind this is to remove many arcs from the black causal graph quickly so that we minimize the number of red variables. We break ties by smaller variable domain size, and if ties still remain then, break them by the L strategy.
       </list-item>
       <list-item label="•">
        C: Select v with the minimal number {a mathematical formula}C(v) of conflicts, i.e., relaxed plan actions with a precondition on v that will be violated when executing the relaxed plan with black v. The intuition is for the “least critical” variables to be red. We break ties by the L strategy.
       </list-item>
       <list-item label="•">
        C[N]: Extends C by sampling N random states, then selecting v with the minimal average number of conflicts in the relaxed plans for the sample states. The motivation is that C depends on the relaxed plan for the initial state, in which conflicts (e.g., on resources) that will necessarily occur later on may not be visible. We run {a mathematical formula}N∈{5,25,100} in our experiments.
       </list-item>
       <list-item label="•">
        CA[p]: Interpolation between C (with {a mathematical formula}p=0) and A (with {a mathematical formula}p=1). Selects v maximizing {a mathematical formula}P(v):=p⁎Aˆ(v)+(1−p)⁎(1−Cˆ(v)), where {a mathematical formula}Aˆ(v) and {a mathematical formula}Cˆ(v) are normalized counters of the number of incident edges and the number of conflicts, respectively: {a mathematical formula}Aˆ(v) is the number of incident edges of v divided by the maximal number of incident edges among all variables in {a mathematical formula}V∖VR, and {a mathematical formula}Cˆ(v) is the number of conflicts of v divided by the maximal number of conflicts among all variables in {a mathematical formula}V∖VR.A subtlety in CA[p] regards the tie breaking. Ties are broken differently in A and C, cf. above. In CA[p], we adopt the tie breaking of A. Thus CA[1] is equivalent to A, but CA[0] is a variant of C using the tie breaking from A. We run {a mathematical formula}p∈{0,0.25,0.5,0.75} in our experiments.
       </list-item>
      </list>
      <paragraph>
       We also experiment with corresponding inverse strategies, as well as with random strategies, intended as sanity checks:
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}L¯: Like L but selecting v with lowest level, i.e., “closest to the causal graph roots”: What happens if we paint the “servant” variables red?
       </list-item>
       <list-item label="•">
        {a mathematical formula}A¯: Like A but selecting v with the minimal number {a mathematical formula}A(v) of incident arcs to black variables: What happens if we try to maximize the number of red variables?
       </list-item>
       <list-item label="•">
        {a mathematical formula}C¯: Like C but selecting v with the maximal number of conflicts: What happens if we paint the “most critical” variables red?
       </list-item>
       <list-item label="•">
        RND: Selects v at random, uniformly from {a mathematical formula}V∖VR.
       </list-item>
      </list>
     </section>
     <section label="6.2">
      <section-title>
       Evaluation
      </section-title>
      <paragraph>
       Table 3 shows coverage data for all our painting strategies, with the canonical search algorithm as well as the fixed best-performing configuration parameters RDS as per Section 5. The message of this table is easiest to appreciate by observing the sparsity of boldface numbers, which indicate best per-domain coverage performance where there are differences: In all except Barman, Scanalyzer, Sokoban, and Tidybot, coverage is constant across all painting strategies, including the average of 10 runs of our random painting strategy RND. Indeed, as the “RND Std dev” column shows, the per-domain standard deviation in the distribution of random-painting coverage is 0 except in Barman, Scanalyzer, and Sokoban (IPC'11 only). In the next column to the right, “{a mathematical formula}|Vinv|”, we see that this is not for lack of RSE-invertible variables (i.e., candidates for being painted black): in most domains, there is quite a few of these. We elucidate this further in the rightmost column, where we indicate the number of maximal paintings per domain. We approximate that number through running RND 10 000 times (without running the actual planner), and counting how many different paintings were generated. In most domains, this count is very small. Indeed, in the majority of the domains, the average is 1.0 or 2.0, and in all but one of these (Pipesworld-NoTankage) the count is constant 1 respectively 2 across all instances. This gives a very strong indication that the number of maximal paintings tends to be small, at least in the IPC benchmarks. As an example of how this happens, consider StarShapeLogistics (Fig. 5). We can paint T and F black, or paint T and the packages black. All other paintings either do not yield a DAG black causal graph, or are not set-inclusion maximal among such paintings.
      </paragraph>
      <paragraph>
       That said, as we see in Barman, already a very small number of paintings to choose from (2, in this case) can make a huge performance difference (20 vs. 8 instances solved). Furthermore, Table 3 gives too simple a picture, due to the abstraction level implied by looking at coverage only. Do the 1000s of choices in Satellite really have no effect on performance? Or are we only observing the lack of instances at the borderline of feasibility for the group of planners and computational resources considered? Table 4 shows average search space size, to address these questions.
      </paragraph>
      <paragraph>
       There can of course not be any performance differences between painting strategies in those domains where there is no actual choice ({a mathematical formula}|P|=1). We keep these in the table merely to point out that our painting strategies do not discover anything not discovered by the 1000 random runs underlying {a mathematical formula}|P|, and to give explicit search space size data (as opposed to relative data such as for RDL/RDLS in Table 1) for these domains as well.
      </paragraph>
      <paragraph>
       For the domains where different painting choices do exist, an interesting observation is that a small choice {a mathematical formula}|P| of different paintings available does not imply a small scale of performance differences in the domain, as a function of that choice. For a quick look, just compare column “{a mathematical formula}|P|” with column “RND Std dev”, and consider the extreme cases Barman and Driverlog ({a mathematical formula}|P|=2, standard deviation {a mathematical formula}&gt;100000). There is of course also a long list of domains with both small {a mathematical formula}|P| and little performance variation (e.g., Mystery, Mprime, Woodworking), but then again there are cases like Satellite and Tidybot ({a mathematical formula}|P|&gt;1000, standard deviation &lt;500) where a huge amount of choice results in comparatively little performance variation. Altogether, the best conclusion we can draw from this is that the performance of different painting strategies is rather unpredictable, and in some domains is extremely brittle even when there is little choice to be made.{sup:16}
      </paragraph>
      <paragraph>
       Considering our sanity test strategies {a mathematical formula}A¯, {a mathematical formula}C¯, and {a mathematical formula}L¯, the only “expected” observation is that, in terms of coverage, {a mathematical formula}A¯ and {a mathematical formula}L¯ are indeed consistently dominated by their more intuitive counterparts A and L. However, in coverage, {a mathematical formula}C¯ almost consistently dominates (namely in all domains except Tidybot) its counterpart C, and in terms of search space size almost all of the strategies are pairwise complementary, with better or worse performance depending on the domain. Ironically, {a mathematical formula}L¯ ends up doing best when averaging the per-domain average search space size across all domains (see next).
      </paragraph>
      <paragraph>
       In terms of competitive performance evaluation, our primary intention for including the “global average” row at the bottom of Table 4 is to point out that, overall, the differences are minor. The more interesting parameter to look at is best performance per domain, where dramatic differences do exist. With this in mind, our selection of strategies to focus on in the remainder of the paper is L, {a mathematical formula}L¯, and C[5]: In 18 out of the 20 domains where performance differences do exist, these three configurations represent the best performance observed.{sup:17} Furthermore, L and C[5] together represent all best performances in terms of coverage, L is best in terms of overall coverage, and {a mathematical formula}L¯ is best in terms of the global average for evaluations.
      </paragraph>
      <paragraph>
       Note that we restrict to a selection of just three strategies here to make the remaining experiments more feasible and accessible. Given the observations of unpredictable cross-strategy variance in Table 4, it could make sense to design methods trying different painting strategies on a domain, e.g., employing re-starting or auto-tuning. We leave this as an open topic for future work.
      </paragraph>
     </section>
    </section>
    <section label="7">
     <section-title>
      Comparison to the state of the art
     </section-title>
     <paragraph>
      To conclude the empirical evaluation of our work, we now compare its most competitive configurations against the state of the art. Section 7.1 examines the main performance parameters against related heuristic functions and a representation of the state of the art in satisficing planning. Section 7.2 analyzes simple methods for combining our new methods with existing ones, highlighting the mutual benefits. Both of these experiments maintain the benchmarks from the previous experiments, i.e., the FD collection of satisficing-planning STRIPS benchmarks up to IPC 2011. In Section 7.3 we provide an additional brief evaluation on the IPC 2014 benchmarks, in which one of our techniques, implemented in the “Mercury” planner, participated successfully.
     </paragraph>
     <section label="7.1">
      <section-title>
       Competitive performance
      </section-title>
      <paragraph>
       As before, we exclude benchmarks without RSE-invertible variables, and we use FD's greedy best-first search with lazy evaluation and a second open list using preferred operators [29], [46] as our base search algorithm. Into that search algorithm, we plug:
      </paragraph>
      <list>
       <list-item label="•">
        {a mathematical formula}hFF, as a baseline;
       </list-item>
       <list-item label="•">
        {a mathematical formula}hcea, as a competitive heuristic function from earlier work on (not-interpolating) partial delete relaxation heuristics;
       </list-item>
       <list-item label="•">
        two versions of {a mathematical formula}hFF(ΠceC), namely the configurations performing best (overall) in the original experiments [25] (referred to as “{a mathematical formula}ΠceC1”) respectively the more recent experiments [26] (referred to as “{a mathematical formula}ΠceC2”), representing the only competing interpolation framework for partial delete relaxation heuristics.
       </list-item>
      </list>
      <paragraph>
       We run the first search iteration of LAMA [11] which we refer to as “LAMA-1st” (short: “LA-1”), as a representation of the state of the art in “agile” satisficing planning, i.e., focusing on runtime performance without investing any extra effort into optimizing plan quality. Exclusively for a comparison regarding plan quality, i.e., the length of the plans returned (recall that we ignore action costs), we also run LAMA in full (referred to simply as “LAMA”). Finally, to represent the recent competitive approach to satisficing planning via SAT, we run Rintanen's Mp solver [47]. Table 5, Table 6, Table 7 show the data.
      </paragraph>
      <paragraph>
       Let us start with a quick look at coverage in Table 5. Mp is competitive with the heuristic search planners in most domains, and stronger than these in Depots and NoMystery. Overall, however, Mp has substantially smaller coverage. We focus in what follows on the heuristic search planners, which are more comparable with each other and have a larger basis of commonly solved instances.
      </paragraph>
      <paragraph>
       {a mathematical formula}hFF(ΠceC) leads to an overall win over {a mathematical formula}hFF, but only a slight one, excelling mainly in Floortile. Our red–black plan heuristics substantially improve over {a mathematical formula}hFF. They almost never have worse coverage (the only exceptions are Sokoban, Barman for {a mathematical formula}RDC[5]S, and Tidybot for RDLS and {a mathematical formula}RDL¯S). They have much better coverage in VisitAll, Transport, and NoMystery, as well as smaller improvements in a few other domains. LAMA is still best overall, profiting from its use of two different heuristics as opposed to a single one like for all other heuristic search planners here. Our new heuristics beat LAMA's coverage (to small extents) in Floortile, Transport, and Trucks.
      </paragraph>
      <paragraph>
       We consider runtime and plan length in terms of ratios vs. a representation of the state of the art, namely LAMA-1st respectively LAMA. We consider search space size, i.e. the number of state evaluations (calls to the heuristic function), in terms of ratios vs. the baseline {a mathematical formula}hFF. All these ratios, see Table 5, Table 6, are taken over the set of instances commonly solved by all heuristic search planners in our experiments. So the ratios are directly comparable across columns. In some domains, due to the smaller coverage of {a mathematical formula}hFF, {a mathematical formula}hcea, and {a mathematical formula}hFF(ΠceC), the set of common instances is small. For these domains, Table 7 shows data over the larger set of instances commonly solved by the most competitive planners, i.e., LAMA and the three variants of our new heuristics (we do not show evaluations data here as the ratio to {a mathematical formula}hFF is not defined on this instance set).
      </paragraph>
      <paragraph>
       Regarding search time, at a high level the picture is somewhat similar to what we observed for coverage. Considering Table 5, we see advantages of {a mathematical formula}hcea over LAMA in some domains, and we see the enormous advantage of {a mathematical formula}hFF(ΠceC) in Floortile. Our new heuristics yield significantly better runtime than all other planners in Elevators, Satellite, Transport, and VisitAll.
      </paragraph>
      <paragraph>
       To discuss this in some more detail, consider RDLS, the most competitive variant of our heuristics in terms of coverage. In the median runtime ratio, RDLS does worse than LAMA-1st in 14 domains (i.e., instance suites), and better in 18. Table 7 complies with these observations, RDLS being worse than LAMA-1st in 8 of the shown instance suites, and better in 12. To shed some light on the distribution of ratios, we also include the per-domain minimum and maximum in Table 5, Table 7. This data mainly goes to show that there is a lot of per-domain variance, with very good and very bad behavior mixed in many domains. In Table 5, there are only 5 instance suites (Table 7: 1 suit) where RDLS is consistently worse, maximum &lt;1; and 6 instance suites (Table 7: 4 suites) where RDLS is consistently better, minimum &gt;1. The “consistently better” cases arise in Elevators, Miconic, NoMystery, ParcPrinter, Transport, and VisitAll. In the remaining domains – with ratios on both sides of 1 – we see that the median typically is indicative of whether or not large improvements occur: For median ratios close to 1, the maximum typically is below 3. There are some exceptions to this, most notably Driverlog in Table 5 as well as NoMystery, Pipesworld-Tankage, and Sokoban in Table 7. And there are domains like PegSol where the intra-domain variance is huge (the ratios &gt;3800 are extreme outliers, the next highest ratio is &lt;20 in each of these suites). Overall, it is not clear to us what causes this variance, nor how one could get rid of it. A simple promising approach would be to run several different heuristics in parallel. We leave this open for future research.
      </paragraph>
      <paragraph>
       The data regarding the number of evaluations in Table 6 shows that the domains basically fall into two classes: a large class of domains where the informativity gain over {a mathematical formula}hFF is smallish and roughly similar for all the partial delete relaxation heuristics considered here; and a smaller class of domains where at least one of these heuristics yields dramatic gains. The latter is the case for {a mathematical formula}hFF(ΠceC) in Floortile and Woodworking, and for our new heuristics in Elevators, Gripper, Logistics, Miconic, NoMystery, Satellite, Transport, VisitAll, and Zenotravel (which almost coincides with the domains with a consistent runtime advantage, cf. above). Note that these latter advantages are mostly due to stop search (compare Table 1).
      </paragraph>
      <paragraph>
       Regarding plan length, the right half of Table 6, as well as that of Table 7, show that LAMA has a very consistent advantage over all other planners, which is expected, as none of these planners make any effort to minimize plan length after the first solution is found. But LAMA's advantage tends to be small. There are few cases of median ratios worse than 0.8; the largest advantage by far arises in VisitAll for {a mathematical formula}hFF and {a mathematical formula}hcea. For our new heuristics, median ratios worse than 0.8 occur only in Sokoban. In a few cases (Elevators, Scanalyzer, Transport, and VisitAll) the plans found using our heuristics are somewhat shorter than those found by LAMA. Note that, in most of these latter cases, the plans are actually found by stop search, showing that this technique can be useful not only for runtime but also for plan length.
      </paragraph>
     </section>
     <section label="7.2">
      <section-title>
       Simple combinations
      </section-title>
      <paragraph>
       We so far ran our new heuristics against competing state-of-the-art techniques, but of course one can instead combine all these techniques to exploit their complementary strengths. We explore two straightforward forms of such combination, showing the potential benefits. For simplicity, we consider only the overall strongest variant of our heuristic function, RDLS, and we consider only its combinations with LAMA.
      </paragraph>
      <paragraph>
       The simplest form of combination is to replace {a mathematical formula}hFF in LAMA by RDLS. We refer to this approach by the name “Mercury”, as used for a corresponding planner that participated in IPC 2014 (see also the next section). We remark that, while LAMA implements an interaction (“synergy”) between {a mathematical formula}hFF and its landmark heuristic, and while that interaction could presumably be adapted to our red–black plan heuristics, we did not perform this adaptation yet. That is, in Mercury, the two heuristics are strictly separate.
      </paragraph>
      <paragraph>
       Our second combination method is the classical, and highly successful, sequential portfolio idea, where the component planners are run in a fixed sequence, each with a fixed time share of the total time (1800 seconds, in our case). We take the two portfolio components to be RDLS and LAMA. We denote the time share of RDLS (in seconds) by T; the time share of LAMA is {a mathematical formula}1800−T. We do implement a basic form of interaction between these components: If RDLS succeeded in solving the task at hand (within its time share), but we are trying to minimize plan length, then the length of the plan found by RDLS is provided as an initial upper bound on plan length to LAMA.
      </paragraph>
      <paragraph>
       To clarify the influence of the parameter T, Fig. 9 shows portfolio coverage as a function of that parameter. We obtained this data by running each of RDLS and LAMA once, reconstructing, for each of the 1801 values of T, the coverage the portfolio would have had for that setting. Notice first of all that the y-axis shows only the part of the coverage scale above 1000 instances – the differences in overall coverage are small (as expected, given Table 5). The sharp drops at each extreme end of the T scale are due to losing coverage in domains where either RDLS (T close to 0) or LAMA (T close to 1800) excels, solving tasks within a few seconds that are out of reach for the respective other component planner. In between the extremes, the coverage curve is rather flat, with a performance peak of coverage 1022 for RDLS time shares {a mathematical formula}663≤T≤714. The difference between the two extreme values of T in this peak is small; in what follows, we selected {a mathematical formula}T=714 as the “best” time share setting.
      </paragraph>
      <paragraph>
       Consider Table 8. We do not show data for runtime because, on those instances commonly solved by LAMA and either of the two portfolios in the table, the vast majority of instances is solved by RDLS already. Hence the runtime ratio over these instances would essentially come down to the ratios already shown in Table 5, Table 7. Regarding plan length, as before we employ LAMA as a representation of the state of the art. The picture is clear: most of the time the median performance is identical to that of LAMA. There are only five cases (Barman for the uniform portfolio, Floortile for all our planners, Woodworking11 for Mercury) where plans become worse. In Elevators, Scanalyzer, Transport, and VisitAll, plans become shorter (similarly as in Table 6). Note also that the disadvantage on Sokoban when running RDLS alone (cf. Table 6, Table 7) disappears when using the portfolio instead, showing that the “plan-improvement post-process” by LAMA in that portfolio is effective.
      </paragraph>
      <paragraph>
       Regarding coverage, the basic message is that replacing {a mathematical formula}hFF with RDLS in LAMA is marginally effective at best, while sequential portfolios of RDLS and LAMA do yield substantial improvements. In a little more detail, Mercury improves coverage over LAMA in NoMystery (1 instance), Transport (3), and Trucks (3), but loses in PegSol (2), Pipesworld-Tankage (1), Sokoban (6), and Tidybot (2), for an overall loss of 4. As for the portfolios, with {a mathematical formula}T=714 the portfolio is at least as good as LAMA in all except Pipesworld-Tankage where it loses a single instance, while it performs substantially better in 4 domains. In detail, coverage goes down in Pipesworld-Tankage (by 2 instances for {a mathematical formula}T=900 and by 1 instance for {a mathematical formula}T=714) and Sokoban (by 4 instances for {a mathematical formula}T=900), but goes up in Floortile (2 for each value of T), NoMystery (2 for each), Transport (3 for each), and Trucks (3 for each). Note that, in NoMystery, the portfolio solves more instances than each of its components, showing that complementary strengths can sometimes be exploited even within a domain.
      </paragraph>
     </section>
     <section label="7.3">
      <section-title>
       IPC 2014
      </section-title>
      <paragraph>
       Mercury obtained the 2nd prize in the IPC 2014 sequential satisficing track, being outperformed (in terms of IPC quality score which was used to rank the planners in this track) only by the IBaCoP2 portfolio planner [48]. As stated by the organizers in their results presentation, LAMA would have ranked much worse, on place 12th out of 21 in that track. Given our observations above – Mercury having only a marginal performance advantage over LAMA, the portfolio approach being substantially more effective – this raises the questions (1) why Mercury was superior to LAMA in IPC 2014 (due to the different benchmarks, or due to the different ranking criterion?), and (2) how the portfolio approach would have fared. Consider Table 9.{sup:18}
      </paragraph>
      <paragraph>
       Considering question (1) first, the answer clearly is that Mercury's superiority over LAMA is due to coverage. Mercury never decreases coverage relative to LAMA. It dramatically improves coverage in Tetris and Transport, and yields smaller improvements in Barman, Citycar, and Hiking. In domains with equal coverage, the IPC quality score picture is mixed; Mercury has the edge in VisitAll and GED, but loses in Parking, and even in Barman despite its slight coverage advantage. In the light of our results above, the conclusion is that the IPC 2014 benchmarks, most specifically Tetris, are qualitatively different from the earlier IPC benchmarks, and that this qualitative difference leads to Mercury's advantage in coverage and hence its advantage in the official competition outcome.
      </paragraph>
      <paragraph>
       Considering now question (2), a similar conclusion applies: In difference to the earlier IPC benchmarks used above, Mercury ends up having the edge in coverage over the portfolio. Substantial differences occur in three new domains, Childsnack, Hiking, and Tetris. The portfolio is much more effective in Childsnack, but is much more ineffective in Hiking and Tetris. The portfolio also tends to do a bit worse than Mercury in the quality score, which makes sense as Mercury will plausibly have a tendency to spend more time trying to improve the initial solution (and IPC quality score is more sensitive to that than the median plan length ratios reported above). Overall, the portfolio still beats LAMA on these different benchmarks and ranking criterion, but not as convincingly as Mercury does.
      </paragraph>
     </section>
    </section>
   </content>
   <appendices>
    <section label="Appendix A">
     <section-title>
      Proofs
     </section-title>
     <paragraph label="Lemma 2">
      For any{a mathematical formula}RBtask{a mathematical formula}Π=〈{vB},VR,A,I,G〉it holds that
     </paragraph>
     <list>
      <list-item label="(1)">
       the monotonic relaxation{a mathematical formula}π+of any plan π for Π is SCC-aligned, and
      </list-item>
      <list-item label="(2)">
       any SCC-aligned relaxed plan{a mathematical formula}π+for Π can be extended, with only a polynomial overhead, into a plan for Π.
      </list-item>
     </list>
     <paragraph label="Proof">
      SCC-aligned relaxed plans, as well as all the auxiliary notions and notation, are defined in the proof of Theorem 1, p. 78.(1) The proof of this part is straightforward. If {a mathematical formula}π=〈a1,…,an〉 is a plan for Π, then {a mathematical formula}π+=〈a1,…,an〉 is a relaxed plan for Π. Let {a mathematical formula}〈aσ(1),…,aσ(m)〉 be the sequence of all the SCC-changing actions along {a mathematical formula}π+.For all {a mathematical formula}0≤i≤m, and each {a mathematical formula}σ(i)&lt;j≤σ(i+1), the action {a mathematical formula}aj either has no precondition on {a mathematical formula}vB or {a mathematical formula}pre(aj)=eff(al) for some {a mathematical formula}σ(i)≤l&lt;j. If {a mathematical formula}l=σ(i), then {a mathematical formula}pre(aj)[vB]=eff(aσ(i))[vB], and thus the two trivially belong to the same SCC of {a mathematical formula}Γσ(i). Otherwise, if {a mathematical formula}l&gt;σ(i), since {a mathematical formula}l&lt;σ(i+1), {a mathematical formula}pre(aj)[vB]=eff(al)[vB] and {a mathematical formula}eff(aσ(i))[vB] belong to the same SCC of {a mathematical formula}Γσ(i), or otherwise {a mathematical formula}al would be an SCC-changing action.Finally, let {a mathematical formula}aj be the last {a mathematical formula}vB-changing action along π (and thus along {a mathematical formula}π+). In particular, it means that {a mathematical formula}j≥σ(m). Since π is a plan for Π, if {a mathematical formula}vB∈V(G), then {a mathematical formula}eff(aj)[vB]=G[vB]. Following precisely the same argument as above: If {a mathematical formula}j=σ(m), then trivially {a mathematical formula}eff(aσ(m))[vB]=G[vB]. Otherwise, if {a mathematical formula}j&gt;σ(m), then {a mathematical formula}eff(aσ(m))[vB] and {a mathematical formula}eff(aj)[vB]=G[vB] belong to the same SCC of {a mathematical formula}Γσ(m), or otherwise {a mathematical formula}aσ(m) would not be the last SCC-changing action along {a mathematical formula}π+.(2) Let {a mathematical formula}π+=〈a1,…,an〉 be a SCC-aligned relaxed plan for Π, and {a mathematical formula}〈aσ(1),…,aσ(m)〉 be the sequence of all the SCC-changing actions along {a mathematical formula}π+. These m SCC-changing actions divide {a mathematical formula}π+ into m consecutive action subsequences {a mathematical formula}π+i=〈aσ(i−1)+1,…,aσ(i)〉, each ending with the corresponding SCC-changing action, plus, possibly an empty, action subsequence {a mathematical formula}π+m+1=〈aσ(m)+1,an〉.Given that, we construct a plan{a mathematical formula} for Π such that, for all {a mathematical formula}v∈VR,{a mathematical formula} and{a mathematical formula} The plan π is constructed, and the satisfaction of Eqs. (A.1)–(A.2) is proven, inductively, as follows. For {a mathematical formula}i=1, we use {a mathematical formula}π1:=〈a1〉. This choice trivially satisfies Eq. (A.1), and, since {a mathematical formula}1≤σ(1), Eq. (A.2) is satisfied as well (via either its first or second cases). This provides us with a basis for our induction. Assuming now that Eqs. (A.1)–(A.2) hold for {a mathematical formula}π1⋅…⋅πi−1, {a mathematical formula}i&gt;1, we construct {a mathematical formula}πi so that {a mathematical formula}π1⋅…⋅πi also satisfy Eqs. (A.1)–(A.2). We do that as follows.Considering {a mathematical formula}i≤n, let {a mathematical formula}σ(j)&lt;i≤σ(j+1) for some {a mathematical formula}0≤j≤m. Given that, we set{a mathematical formula} where, if {a mathematical formula}ai is not preconditioned by {a mathematical formula}vB (and thus, in particular, does not affect {a mathematical formula}vB), then {a mathematical formula}πi′=πi″=〈〉, and otherwise, {a mathematical formula}πi′ and {a mathematical formula}πi″ are a pair of sequences of actions from {a mathematical formula}{aσ(1),…,aσ(j)} that change {a mathematical formula}vB from {a mathematical formula}eff(aσ(j))[vB] to {a mathematical formula}pre(ai)[vB] and back, from {a mathematical formula}eff(ai)[vB] to {a mathematical formula}eff(aσ(j))[vB], respectively.First, by the structure of {a mathematical formula}π+ and our inductive assumption on {a mathematical formula}π1⋅…⋅πi−1 with respect to Eq. (A.1), all the (red) preconditions of {a mathematical formula}ai hold in {a mathematical formula}I〚π1⋅…⋅πi−1〛. Second, if {a mathematical formula}aiis preconditioned by {a mathematical formula}vB, then, by the SCC-alignment of {a mathematical formula}π+, {a mathematical formula}eff(aσ(j)), {a mathematical formula}pre(ai)[vB], and, if {a mathematical formula}i&lt;σ(j+1), {a mathematical formula}eff(ai)[vB], belong to the same SCC of {a mathematical formula}Γσ(j). Therefore, an action sequence {a mathematical formula}πi′, and, if needed, an action sequence {a mathematical formula}πi″ as in Eq. (A.3) are guaranteed to exit. Third, by our inductive assumption on Eq. (A.2), {a mathematical formula}I〚π1⋅…⋅πi−1〛[vB]=eff(aσ(j))[vB]. Forth, since our inductive construction ensures that all the actions {a mathematical formula}{aσ(1),…,aσ(j)} are already executed along π prior to executing {a mathematical formula}πi, all the red preconditions of actions in {a mathematical formula}πi′ and {a mathematical formula}πi″ hold from {a mathematical formula}I〚π1⋅…⋅πi−1〛 onwards.Together, these four arguments imply that {a mathematical formula}πi is applicable in {a mathematical formula}I〚π1⋅…⋅πi−1〛, and, since the only action in {a mathematical formula}πi that is novel to {a mathematical formula}π1⋅…⋅πi−1 is {a mathematical formula}ai, {a mathematical formula}π1⋅…⋅πi satisfies Eq. (A.1). In turn, if {a mathematical formula}πi′=πi″=〈〉, then {a mathematical formula}I〚π1⋅…⋅πi〛[vB]=I〚π1⋅…⋅πi−1〛[vB], and thus our inductive assumption on Eq. (A.2) directly implies that Eq. (A.2) is satisfied by {a mathematical formula}π1⋅…⋅πi. Otherwise, if {a mathematical formula}i=σ(j+1), then, by Eq. (A.3), {a mathematical formula}πi ends with {a mathematical formula}ai, and thus {a mathematical formula}I〚π1⋅…⋅πi〛[vB]=eff(aσ(j+1)) satisfies Eq. (A.2). Finally, if {a mathematical formula}i&lt;σ(j+1), then {a mathematical formula}I〚π1⋅…⋅πi〛[vB]=eff(aσ(j)), satisfying Eq. (A.2) as well.This finalizes both the construction step and the proof of its correctness for {a mathematical formula}i≤n. Considering now the end-case of {a mathematical formula}i=n+1, if {a mathematical formula}vB∉V(G), then we set {a mathematical formula}πn+1=〈〉. Otherwise, if {a mathematical formula}vB∈V(G), then {a mathematical formula}πn+1 is set to an action sequence from {a mathematical formula}{aσ(1),…,aσ(m)} that change {a mathematical formula}vB from {a mathematical formula}eff(aσ(m)) to {a mathematical formula}G[vB]. Such a sequence of actions from {a mathematical formula}{aσ(1),…,aσ(m)} exists by the SCC-alignment of {a mathematical formula}π+, and it is applicable in {a mathematical formula}I〚π1⋅…⋅πn〛 because (1) by Eq. (A.2), {a mathematical formula}I〚π1⋅…⋅πn〛[vB]=eff(aσ(m))[vB], and (2) all the red preconditions of the actions in {a mathematical formula}πn+1 hold in {a mathematical formula}I〚π1⋅…⋅πn〛 since {a mathematical formula}π1⋅…⋅πn already contains an instance of each action in {a mathematical formula}{aσ(1),…,aσ(m)}. The latter argument also implies that {a mathematical formula}I〚π1⋅…⋅πn+1〛 satisfies Eq. (A.1), and the satisfaction of Eq. (A.2) is immediate by the construction of {a mathematical formula}πn+1.
     </paragraph>
     <paragraph label="Lemma 3">
      Let{a mathematical formula}Π=〈{vB},VR,A,I,G〉be an{a mathematical formula}RBtask,{a mathematical formula}π+=〈a1,…,an〉be an SCC-aligned relaxed plan for Π, and{a mathematical formula}〈aσ(1),…,aσ(m)〉be the sequence of all the SCC-changing actions along{a mathematical formula}π+. Let{a mathematical formula}be a sequence of actions from A in which, inductively,{a mathematical formula}ρi+is a relaxed plan from{a mathematical formula}I〚ρ0+⋅aσ(1)⋅ρ1+⋅…⋅aσ(i)〛to the relaxed planning fixpoint, using only those actions from A that are neither preconditioned by the values of{a mathematical formula}vBoutside of the SCC of{a mathematical formula}eff(aσ(i))[vB]in{a mathematical formula}Γσ(i), nor have such values among their effects.Then, similarly to{a mathematical formula}π+,
     </paragraph>
     <list>
      <list-item label="1.">
       {a mathematical formula}ρ+is an SCC-aligned relaxed plan for Π, and
      </list-item>
      <list-item label="2.">
       {a mathematical formula}〈aσ(1),…,aσ(m)〉is the sequence of all the SCC-changing actions along{a mathematical formula}ρ+.
      </list-item>
     </list>
     <paragraph label="Proof">
      First, if the action sequence {a mathematical formula}ρ+ constructed as above is a relaxed plan for Π, then (2) is immediate by the very restrictions put on the segments {a mathematical formula}ρi+. Now, let {a mathematical formula}π+=π+0⋅〈aσ(1)〉⋅π+1⋅…⋅π+n−1⋅〈aσ(m)〉⋅π+m. Showing by induction that, for {a mathematical formula}0≤i≤m,{a mathematical formula} for all {a mathematical formula}v∈{vB}∪VR, we prove that {a mathematical formula}ρ+ is a relaxed plan for Π.For {a mathematical formula}i=0, by the definition of SCC-alignment, {a mathematical formula}π+0 comprises a subset of actions that (a) appear on action sequences applicable in {a mathematical formula}I[π+0], and (b) neither are preconditioned by, nor have among their effects, values of {a mathematical formula}vB except for {a mathematical formula}I[vB]. In turn, by the construction, {a mathematical formula}ρ0+ contains all the actions satisfying (a)–(b). Thus, we have {a mathematical formula}I〚π+0〛[v]⊆I〚ρ0+〛[v] for all {a mathematical formula}v∈VR, and {a mathematical formula}I〚π+0〛[vB]=I〚ρ0+〛[vB]={I[vB]}.Assuming now that Eq. (A.5) holds for {a mathematical formula}i−1≥0, we show that it holds for i, using the arguments very close to these used for the induction basis. Since, by the induction hypothesis,{a mathematical formula} for all {a mathematical formula}v∈{vB}∪VR, we have {a mathematical formula}aσ(i) applicable in {a mathematical formula}I〚ρ0+⋅…⋅〈aσ(i−1)〉⋅ρi−1+〛 and{a mathematical formula} for all {a mathematical formula}v∈{vB}∪VR. In turn, by the definition of SCC-alignment, {a mathematical formula}π+i comprises a subset of actions that (a) appear on action sequences applicable in {a mathematical formula}I〚π+0⋅…⋅π+i−1⋅〈aσ(i)〉〛, and (b) neither are preconditioned by, nor have among their effects, values of {a mathematical formula}vB outside of the SCC of {a mathematical formula}eff(aσ(i))[vB] in {a mathematical formula}Γσ(i). By Eq. (A.6), (a) implies that {a mathematical formula}π+i can be seen as comprising a subset of actions that (a') appear on action sequences applicable in {a mathematical formula}I〚ρ0+⋅…⋅ρi−1+⋅〈aσ(i)〉〛, while, by the construction of {a mathematical formula}ρi+, {a mathematical formula}ρi+ contains all the actions satisfying (a') and (b). This finalizes the proof of the induction step, and thus, of the lemma.  □
     </paragraph>
     <paragraph label="Theorem 2">
      Plan existence for{a mathematical formula}RBtasks with a fixed number of black variables is{a mathematical formula}NP-complete.
     </paragraph>
     <paragraph label="Proof">
      For {a mathematical formula}NP-hardness, we construct an {a mathematical formula}RB task, with a single black variable, that is solvable iff an input CNF formula φ is satisfiable. Consider a planning encoding {a mathematical formula}Π(φ) with: (1) ternary-valued variables {a mathematical formula}x1,…,xn which encode the propositional variables in φ, the domain of each {a mathematical formula}xi comprising true, false, and “unassigned”; (2) Boolean variables {a mathematical formula}c1,…,cm which encode satisfaction of the clauses in φ; and (3) a variable {a mathematical formula}vB with domain {a mathematical formula}{1,…,n+1}. Initially, {a mathematical formula}vB=1, all clause variables are false, and all the proposition variables are “unassigned”, and the goal is to make all clause variables true. There are actions changing a clause variable {a mathematical formula}cj to true provided an {a mathematical formula}xi variable corresponding to one of {a mathematical formula}cj's literals is assigned the correct truth value. Other actions change the value of {a mathematical formula}xi from “unassigned” to either false or true provided {a mathematical formula}vB=i, and applying such an action also changes the value of {a mathematical formula}vB to {a mathematical formula}i+1. This encoding does not work in the delete relaxation – that is, if we paint all variables of {a mathematical formula}Π(φ) red – because {a mathematical formula}xi may be set to both truth values. However, if only {a mathematical formula}vB is painted black, then plans are forced to assign each {a mathematical formula}xi exactly one truth value, yielding the desired equivalence.For membership in {a mathematical formula}NP, just observe that there always exists a polynomial-length plan. We can pre-compile the fixed number of black variables into a single black variable, whose domain size is polynomial. Since all causal graph neighbors of that variable are red, the number of times it needs to change its value (i.e., traverse its DTG from some node to another one) is bounded by the sum of the domain sizes of these neighbors.  □
     </paragraph>
     <paragraph label="Theorem 3">
      Plan existence for{a mathematical formula}RBtasks where all black variables have fixed-size domains is{a mathematical formula}PSPACE-complete, and it is{a mathematical formula}NP-complete even if{a mathematical formula}CGΠBis arcless.
     </paragraph>
     <paragraph label="Proof">
      If we fix the domain size of the black variables, but not their number, then we obtain a problem that is as hard as {a mathematical formula}FDR with fixed-size domains, which is {a mathematical formula}PSPACE-hard [7], and {a mathematical formula}PSPACE membership is straightforward because the addition of red variables still allows for a proof similar to that for {a mathematical formula}FDR. For the second part of the claim, consider the CNF encoding from the proof of Theorem 2, but now without the special variable {a mathematical formula}vB. If all the clause variables {a mathematical formula}cj, but none of the variables {a mathematical formula}xi, are painted red, then the black causal graph of the resulting {a mathematical formula}RB task contains no arcs – each arc in the causal graph involves a red clause variable. At the same time, since the {a mathematical formula}xi variables are all black, the plans are forced to assign each {a mathematical formula}xi exactly one truth value, and thus our {a mathematical formula}RB task is solvable iff φ is satisfiable. Membership in {a mathematical formula}NP follows from basically the same argument as in the proof of Theorem 2: for each black variable, all causal graph neighbors are red so we get a polynomial bound on the number of moves required.  □
     </paragraph>
     <paragraph label="Theorem 7">
      It is{a mathematical formula}co-NP-hard to test whether an{a mathematical formula}RBtask is reversible, even when restricting the input to{a mathematical formula}RBtasks whose black causal graph is arcless.
     </paragraph>
     <paragraph label="Proof">
      The proof is by reduction from DNF tautology testing. Given a propositional DNF formula φ over l clauses {a mathematical formula}c1,…,cl, consider an {a mathematical formula}RB planning encoding {a mathematical formula}Π(φ) with: black variables {a mathematical formula}x1,…,xn with {a mathematical formula}D(xi)={unassigned,true,false}, encoding the propositional variables in φ; and a Boolean red variable r, encoding whether or not φ is satisfied under a given assignment to {a mathematical formula}x1,…,xn. All {a mathematical formula}xi variables are initially unassigned, and r is initially false; the goal does not matter here. The value of {a mathematical formula}xi can be changed from unassigned to either false or true with no preconditions, and back from false or true to unassigned with precondition r. We can set r to true using actions {a mathematical formula}{a1,…,al}, where the precondition of {a mathematical formula}aj requires that all {a mathematical formula}xi participating in {a mathematical formula}cj are assigned to their values required by {a mathematical formula}cj.Let {a mathematical formula}M be the set of all {a mathematical formula}2n valuations of φ's propositions. For every {a mathematical formula}m∈M, let {a mathematical formula}Sm⊆S be the set of reachable states in which all variables {a mathematical formula}xi are assigned as in m. We observe:
      <list>
       For every {a mathematical formula}m∈M that does satisfy φ, the states in {a mathematical formula}Sm are reversible in the red–black sense.For every {a mathematical formula}m∈M that does not satisfy φ, none of the states in {a mathematical formula}Sm is reversible in the red–black sense.For every state s reachable in {a mathematical formula}Π(φ), and every {a mathematical formula}m∈M that complies with the (partial) valuation to {a mathematical formula}x1,…,xn defined by s, there exists a state {a mathematical formula}sm∈Sm such that {a mathematical formula}sm is reachable from s.If
      </list>
      <paragraph>
       φ is a tautology, then by (3) and (1) every reachable state in {a mathematical formula}Π(φ) is reversible. If φ is not a tautology, then there exists a valuation m that does not satisfy φ. Applying (3) to the initial state, we can reach a state {a mathematical formula}sm∈Sm, which by (2) is not reversible.  □
      </paragraph>
     </paragraph>
     <paragraph label="Lemma 4">
      Let{a mathematical formula}Π=〈VB,VR,A,I,G〉be a reversible{a mathematical formula}RBtask with acyclic black causal graph,{a mathematical formula}{v1,…,vn}be a topological ordering of{a mathematical formula}VB, and s be a reachable state of Π with{a mathematical formula}R(s)=s. Then, for any{a mathematical formula}vi∈VB, there exists a sequence of actions π applicable in s such that
     </paragraph>
     <list>
      <list-item label="(i)">
       {a mathematical formula}s〚π〛[v1,…,vi]=I[v1,…,vi], and
      </list-item>
      <list-item label="(ii)">
       actions along π neither are preconditioned by nor affect variables{a mathematical formula}vi+1,…,vn.
      </list-item>
     </list>
     <paragraph label="Proof">
      By reversibility, there exists a reverting sequence {a mathematical formula}π′ for s, that is, a plan for {a mathematical formula}〈VB,VR,A,s,I[VB]〉. Let π be an action sequence obtained from {a mathematical formula}π′ by removing all actions that have no effect on any of the variables {a mathematical formula}v1,…,vi. We show that π has the desired properties (i) and (ii). Provided π is applicable in s, its outcome is {a mathematical formula}I[vj] for all vars {a mathematical formula}vj with {a mathematical formula}j≤i because the responsible actions from {a mathematical formula}π′ are contained in π, yielding property (i). To see applicability, first note that any red preconditions along π are true simply because {a mathematical formula}s=R(s). By acyclicity of {a mathematical formula}CGΠB every action in A affects at most one black variable, and so, by the construction of π, every action a in π affects at most one {a mathematical formula}vj where {a mathematical formula}j≤i. But then, since {a mathematical formula}{v1,…,vn} is a topological ordering of {a mathematical formula}VB, any black preconditions of a are on variables {a mathematical formula}vl for {a mathematical formula}l≤j≤i, and thus the actions supporting these preconditions in {a mathematical formula}π′, if any, are contained in π. Therefore, π is applicable in s. These arguments also show that neither the effects nor the preconditions of the actions in {a mathematical formula}ρ′ touch the variables {a mathematical formula}vi+1,…,vn. Thus we have (ii), concluding the argument.  □
     </paragraph>
     <paragraph label="Lemma 5">
      Let{a mathematical formula}Π=〈VB,VR,A,I,G〉be a reversible{a mathematical formula}RBtask with acyclic black causal graph and{a mathematical formula}I=R(I),{a mathematical formula}π+be an action sequence applicable in the monotonic relaxation{a mathematical formula}Π+, and{a mathematical formula}gBbe an assignment to{a mathematical formula}VBsuch that{a mathematical formula}gB⊆I〚π+〛. Then, there exists an action sequence π applicable in I such that
     </paragraph>
     <list>
      <list-item label="(i)">
       {a mathematical formula}I〚π〛[VB]=gB, and
      </list-item>
      <list-item label="(ii)">
       {a mathematical formula}I〚π〛[VR]⊇I〚π+〛[VR].
      </list-item>
     </list>
     <paragraph label="Proof">
      Let {a mathematical formula}{v1,…,vn} be a topological ordering of {a mathematical formula}VB. We prove the stronger claim that there exists π with the claimed properties (i) and (ii), as well as with the third property that (iii) actions in π neither are preconditioned nor affect black variables {a mathematical formula}vj with {a mathematical formula}j&gt;m(gB), where, for a partial assignment g,{a mathematical formula} denotes the index of the topologically lowest black variable assigned by g.The proof is by induction over the length of {a mathematical formula}π+=〈a1,…,ak〉. For the base case, if {a mathematical formula}π+ is empty, then the claim is trivially satisfied by the empty sequence π. Assuming now that the claim holds for all relaxed plans of length {a mathematical formula}k−1≥0, we prove it for {a mathematical formula}π+ of length k.First, consider the sequence {a mathematical formula}π+k−1 containing the first {a mathematical formula}k−1 actions of {a mathematical formula}π+. Since {a mathematical formula}π+ is applicable in I, we have {a mathematical formula}pre(ak)⊆I[π+k−1]. Thus, by the induction assumption applied to {a mathematical formula}π+k−1, there is an action sequence {a mathematical formula}ρk−1 for Π applicable in I such that {a mathematical formula}pre(ak+1)⊆I〚ρk−1〛 and {a mathematical formula}I〚π+k−1〛[VR]⊆I〚ρk−1〛[VR]. In turn, action sequence {a mathematical formula}ρk−1⋅〈ak〉 is thus applicable in I, and obviously, {a mathematical formula}I〚π+〛[VR]⊆I〚ρk−1⋅〈ak〉〛[VR]. In other words, the red variable values achieved by {a mathematical formula}π+ are all reachable in Π. But then, {a mathematical formula}I〚π+〛[VR]⊆I because, by prerequisite, the initial state is {a mathematical formula}R-completed, i.e., {a mathematical formula}I=R(I). Thus, part (ii) of the claim is trivially satisfied, for any red–black action sequence π applicable in Π.Given that, the action sequence {a mathematical formula}π=πn⋅πn−1⋅…⋅π1 achieving {a mathematical formula}gB is constructed in a way similar to the construction in the proof of Theorem 9. There, however, the construction relied on Lemma 5 we prove here, while here, the corresponding reliance will be on the induction assumption.For {a mathematical formula}1≤i≤n, assume inductively that {a mathematical formula}πn⋅…⋅πi−1 is applicable in I; the basis case of the empty action sequence for {a mathematical formula}i=n trivially holds. If {a mathematical formula}vi∉V(gB), then we set {a mathematical formula}πi:=〈〉. Otherwise, by Lemma 4 above, there exists an action sequence {a mathematical formula}ρi that (1) reverts the black variables {a mathematical formula}{v1,…,vi} to their initial values, and (2) neither is preconditioned by nor affects the topologically lower black variables {a mathematical formula}{vi+1,…,vn}. That is,{a mathematical formula} Given that, if {a mathematical formula}gB[vi]=I[vi], then we set {a mathematical formula}πi:=ρi. Otherwise, by the virtue of {a mathematical formula}π+ being a relaxed plan for Π, we must have {a mathematical formula}eff(aj)[vi]=gB[vi] for some {a mathematical formula}π+'s action {a mathematical formula}aj, {a mathematical formula}1≤j≤k. Furthermore, by the acyclicity of {a mathematical formula}CGΠB, the black preconditions of {a mathematical formula}aj may involve only variables {a mathematical formula}{v1,…,vi}, and thus {a mathematical formula}m(pre(aj))≤i. In turn, by our induction assumption on the prefix {a mathematical formula}π+aj of {a mathematical formula}π+ that precedes {a mathematical formula}aj, there exists an action sequence {a mathematical formula}πaj that is applicable in I and achieves {a mathematical formula}pre(aj) while neither being preconditioned by nor affecting the black variables {a mathematical formula}{vi+1,…,vn}. By Eq. (A.7) we then have {a mathematical formula}πaj being applicable in {a mathematical formula}I〚πn⋅…⋅πi+1⋅ρi〛, and, for each black variable {a mathematical formula}v∈V(pre(aj))∩VB,{a mathematical formula} Given that, we set {a mathematical formula}πi:=ρi⋅πaj⋅〈aj〉. Based on the applicability of each {a mathematical formula}πi in {a mathematical formula}I〚πn⋅…⋅πi+1〛, the overall action sequence π constructed this way is applicable in I. Finally, since {a mathematical formula}πi does not touch (neither in preconditions nor in effects) the black variables {a mathematical formula}{vi+1,…,vn}, it achieves {a mathematical formula}gB[vi] while satisfying (iii), and thus, in particular, invalidates no sub-goal already achieved by {a mathematical formula}πn⋅…⋅πi+1 on the topologically lower variables {a mathematical formula}vi+1,…,vn. Therefore, the action sequence π satisfies (i), concluding the argument.  □
     </paragraph>
     <paragraph label="Theorem 10">
      Any RSE-invertible {a mathematical formula}RB task with acyclic black causal graph is reversible.
     </paragraph>
     <paragraph label="Proof">
      Say {a mathematical formula}Π=〈VB,VR,A,I,G〉. We prove that we can “undo” all action applications. For any state s and action a applicable in s, we show that from {a mathematical formula}s〚〈a〉〛 we can reach a state {a mathematical formula}s′ so that {a mathematical formula}s′[VB]=s[VB] and, for every {a mathematical formula}v∈VR, {a mathematical formula}s′[v]⊇s[v]. If all variables {a mathematical formula}V(eff(a)) affected by a are red, there is nothing to do. Otherwise, because the black causal graph is acyclic, exactly one variable {a mathematical formula}vB affected by a is black. Let {a mathematical formula}(d,a,d′) be the corresponding arc in {a mathematical formula}DTGΠ(vB), and {a mathematical formula}(d′,a′,d) be the inverse arc that exists by prerequisite. We show that we can set {a mathematical formula}s′:=s〚〈a,a′〉〛. First, {a mathematical formula}a′ is applicable in {a mathematical formula}s〚〈a〉〛: The precondition of {a mathematical formula}a′ is {a mathematical formula}pre(a′)=ocon(d′,a′,d)∪{〈vB/d′〉}. Obviously, {a mathematical formula}〈vB/d′〉∈s〚〈a〉〛. Further, we have {a mathematical formula}ocon(d′,a′,d)⊆ocon(d,a,d′)∪oeff(d,a,d′). If {a mathematical formula}〈v/d″〉∈oeff(d,a,d′), then it is made true by a. If {a mathematical formula}〈v/d″〉∈ocon(d,a,d′), then it is true in s, and remains true if v is black because all variables affected by {a mathematical formula}oeff(d,a,d′) must be red. Applying {a mathematical formula}a′ to {a mathematical formula}s〚〈a〉〛, the black variable {a mathematical formula}vB is reverted to its value d in s; all other effects of {a mathematical formula}a′ are red. This concludes the proof of Theorem 10.  □
     </paragraph>
     <paragraph label="Theorem 12">
      There exists a set{a mathematical formula}Gof directed graphs where{a mathematical formula}scc-size(G)is bounded by 2, and{a mathematical formula}RB-PlanExist(G)restricted to RSE-invertible {a mathematical formula}RB is{a mathematical formula}NP-hard.
     </paragraph>
     <paragraph label="Proof">
      The proof is by a polynomial reduction from CNF satisfiability testing: Given a CNF formula ϕ, we construct an RSE-invertible {a mathematical formula}RB  task with strongly connected black causal graph components of size 2 that has a plan iff ϕ is satisfiable.For each Boolean variable p occurring in ϕ, we include two binary state variables {a mathematical formula}xp and {a mathematical formula}yp with initial value 0. We furthermore include an action {a mathematical formula}apx1y1 with precondition {a mathematical formula}{〈xp/0〉,〈yp/0〉} and effect {a mathematical formula}{〈xp/1〉,〈yp/1〉}, an action {a mathematical formula}apx0 with precondition {a mathematical formula}{〈xp/1〉,〈yp/0〉} and effect {a mathematical formula}{〈xp/0〉}, and an action {a mathematical formula}apy0 with precondition {a mathematical formula}{〈xp/0〉,〈yp/1〉} and effect {a mathematical formula}{〈yp/0〉}. Each individual variable {a mathematical formula}xp and {a mathematical formula}yp is RSE-invertible, but their product is not; we can transition to {a mathematical formula}{〈xp/1〉,〈yp/1〉} but we cannot go back. Thus, for each p in ϕ, we can encode the decision whether to set p to true ({a mathematical formula}{〈xp/1〉,〈yp/1〉}) or false ({a mathematical formula}{〈xp/0〉,〈yp/0〉}).We conclude our construction by adding, in a straightforward manner, variables and actions that allow to test satisfaction of ϕ for given decisions on each p. Namely, for each clause {a mathematical formula}c={l1,…,lk} in ϕ, we include a binary state variable {a mathematical formula}xc with initial value 0, and we include an action {a mathematical formula}aci for every literal {a mathematical formula}li in c. If {a mathematical formula}li=p then {a mathematical formula}aci has precondition {a mathematical formula}{〈xp/1〉,〈yp/1〉} and effect {a mathematical formula}{〈xc/1〉}. If {a mathematical formula}li=¬p then {a mathematical formula}aci has precondition {a mathematical formula}{〈xp/0〉,〈yp/0〉} and effect {a mathematical formula}{〈xc/1〉}.We paint all variables black. The resulting {a mathematical formula}RB  task obviously has the desired properties, concluding the proof.  □
     </paragraph>
     <paragraph label="Theorem 13">
      Let{a mathematical formula}Π=〈VB,VR,A,I,G〉be an RSE-invertible {a mathematical formula}RB planning task with acyclic black causal graph,{a mathematical formula}π+be a relaxed plan for Π, and{a mathematical formula}R+=G[VR]∪⋃a∈π+pre(a)[VR]. Then, assuming a complete solver for the sub-tasks{a mathematical formula}ΠBgenerated,{a mathematical formula}RedFactsFollowing(Π,R+)terminates, and the action sequence π it returns is a plan for Π.
     </paragraph>
     <paragraph label="Proof">
      We start by showing that, as long as {a mathematical formula}R⊉R+, we always have {a mathematical formula}A0≠∅. This is done with the help of the relaxed plan {a mathematical formula}π+. Let {a mathematical formula}ai∈π+ be the action with the smallest index i such that {a mathematical formula}eff(ai)∩(R+∖R)≠∅ (at least one such action must exist as not all of {a mathematical formula}R+ has been established yet). We show that {a mathematical formula}ai∈A0, i.e., that {a mathematical formula}pre(ai)⊆R∪B. First, regarding the red variables, assume to the contrary that there exists {a mathematical formula}v∈VR(pre(ai)) such that {a mathematical formula}pre(ai)[v]∉R. Then {a mathematical formula}pre(ai)[v]≠I[v] and thus there exists {a mathematical formula}1≤j≤i−1 such that {a mathematical formula}eff(aj)[v]=pre(ai)[v]∈R+. But then, {a mathematical formula}eff(aj)∩(R+∖R)≠∅, in contradiction to the assumption that i is the smallest index i with {a mathematical formula}eff(ai)∩(R+∖R)≠∅. Second, regarding the black variables, because {a mathematical formula}π+ is a relaxed plan {a mathematical formula}a1⋅…⋅ai−1 correspond, for each black variable {a mathematical formula}v∈VB(pre(ai)), to a path in {a mathematical formula}DTGΠ(v) that visits the value {a mathematical formula}pre(ai)[v]. Because {a mathematical formula}R∪B is a superset of the facts achieved in the relaxed plan using {a mathematical formula}a1⋅…⋅ai−1, the path {a mathematical formula}DTGΠ(v) uses only actions with outside conditions in {a mathematical formula}R∪B. Therefore, {a mathematical formula}pre(ai)⊆R∪B and we have {a mathematical formula}ai∈A0 as desired.Consider an iteration of the while loop. By definition of {a mathematical formula}A0, any red preconditions of the selected action {a mathematical formula}a∈A0 are true in the current state {a mathematical formula}I〚π〛. For the unsatisfied black preconditions {a mathematical formula}g=pre(a)[VB], we have {a mathematical formula}g⊆B, and they are tackled by {a mathematical formula}Achieve(g), solving an {a mathematical formula}FDR task {a mathematical formula}ΠB with goal g. We show below that:
      <list>
       {a mathematical formula}ΠB is well-defined;{a mathematical formula}ΠB is solvable; andany plan {a mathematical formula}πB for {a mathematical formula}ΠB is, in our {a mathematical formula}RB task Π, applicable in {a mathematical formula}I〚π〛.Since
      </list>
      <paragraph>
       {a mathematical formula}eff(a)∩(R+∖R)≠∅, {a mathematical formula}|R+∖R| decreases by at least 1 in each iteration, the while loop terminates after at most {a mathematical formula}|R+| iterations. Upon termination, we have {a mathematical formula}R+⊆I〚π〛[VR]=R. The latter implies that {a mathematical formula}G[VB]⊆B because the relaxed plan achieves the goal without using any other red variable values: for each black variable {a mathematical formula}v∈VB∩V(G), {a mathematical formula}π+ corresponds to a path in {a mathematical formula}DTGΠ(v), visiting {a mathematical formula}G[v] and using red outside conditions from {a mathematical formula}R+ only. Calling {a mathematical formula}Achieve(G[VB]) (if needed), as we will show the {a mathematical formula}FDR task {a mathematical formula}ΠB constructed will again have properties (i)–(iii), so it is solvable and appending its solution will turn π into a plan for our {a mathematical formula}RB task Π.Regarding (i), we need to show that all variable values occurring in {a mathematical formula}ΠB are indeed members of the respective variable domains, i.e., the reduced domains {a mathematical formula}DB(v) for black variables v. This is obvious for {a mathematical formula}IB. It holds for {a mathematical formula}GB=pre(a)[VB] as {a mathematical formula}a∈A0, and it holds for {a mathematical formula}GB=G[VB] because, then, {a mathematical formula}G[VB]⊆B. Finally, for actions, for the red variables there is nothing to show as they keep their original domains. For the black variables, if {a mathematical formula}pre(a)⊆R∪B then the (single) black effect must be contained in B as well. If {a mathematical formula}a∈A(DTGΠ(v)|R∪B←) then by construction of {a mathematical formula}DTGΠ(v)|R∪B← its black precondition and effect on v are contained in {a mathematical formula}DTGΠ(v)|R∪B, and any black (outside) preconditions on variables other than v are contained in {a mathematical formula}ocon(d,d′)∪oeff(d,d′) for some arc {a mathematical formula}(d,d′) in {a mathematical formula}DTGΠ(v)|R∪B and are thus contained in B; by construction, a cannot have any black outside effects.Regarding (iii), for all actions {a mathematical formula}aB∈AB where we have {a mathematical formula}pre(a)⊆R∪B, the red preconditions are true in the current state {a mathematical formula}I〚π〛. For all other actions {a mathematical formula}aB∈AB we have {a mathematical formula}a∈A(DTGΠ(v)|R∪B←) for some black variable v, which implies that all red (outside) precondition variables are contained in {a mathematical formula}V←R. So applicability, in Π, of {a mathematical formula}πB in {a mathematical formula}I〚π〛 depends only on variables that are contained in {a mathematical formula}ΠB, which immediately implies (iii).We finally show (ii), i.e., that {a mathematical formula}ΠB is solvable. By Observation 7 of Helmert [29], any {a mathematical formula}FDR task with acyclic causal graph and strongly connected domain transition graphs is solvable. By contrast to relaxed plan repair (cf. the proof of Theorem 11), {a mathematical formula}ΠB as constructed here does not fit that profile; however, the argument can be easily adapted. Helmert's observation relies on the facts that:
      </paragraph>
      <list>
       <list-item label="(1)">
        Acyclicity of the causal graph implies that we can solve the planning task “top-down”, from causal graph leaves to roots, fixing a DTG path for each variable v and propagating the required preconditions as sub-goals to v's parents.
       </list-item>
       <list-item label="(2)">
        As every DTG is strongly connected, every required path is available, i.e., every variable can always move from its current value to any other value it is required to achieve as a sub-goal (or its own goal).
       </list-item>
      </list>
      <paragraph>
       (1) is preserved in our setting, for the black variables; the red variables are handled exclusively as part of our adaptation of (2) below, which is possible as they have no own goals ({a mathematical formula}GB does not mention the red variables). So how do we ascertain (2)? We employ the following two observations, valid for every black variable {a mathematical formula}v∈VB:
      </paragraph>
      <list>
       <list-item label="(a)">
        From v's start value, {a mathematical formula}IB[v]=I〚π〛[v], all values {a mathematical formula}d∈DB(v) of v's reduced domain are reachable in {a mathematical formula}DTGΠ(v)|R∪B.
       </list-item>
       <list-item label="(b)">
        Assume that {a mathematical formula}πB is any applicable action sequence in {a mathematical formula}ΠB which has, at some point, executed action {a mathematical formula}aB traversing {a mathematical formula}DTGΠ(v)|R∪B arc {a mathematical formula}(d,d′). Then there exists an action {a mathematical formula}a′B∈AB inducing an inverse arc {a mathematical formula}(d′,d) whose red outside conditions are contained in the outcome {a mathematical formula}IB〚πB〛 of applying {a mathematical formula}πB in {a mathematical formula}ΠB.
       </list-item>
      </list>
      <paragraph label="Proof">
       To see that (a) and (b) together show (2), observe first that the paths whose existence are postulated in (2) may make use of arbitrary black outside preconditions (“arbitrary” subject to causal graph acyclicity, of course). To achieve this in our setting, all we need to show is that we can reach any other value of v, from its current value, without having to rely on any red outside conditions that are not already true. Now, when v makes its first move, by (a) any path that may be required is available and relies only on the red facts R which are already true at the start. In any subsequent move of v, for all {a mathematical formula}DTGΠ(v)|R∪B arcs we have traversed so far, by (b) there exists a suitable inverse action {a mathematical formula}a′ (relying only on red outside conditions that are already true) inducing the respective inverse arc. So, say we need to reach any value {a mathematical formula}dg that may be required as a sub-goal. We can go back to the start value {a mathematical formula}IB[v], exploiting the fact that by (b) we can invert any {a mathematical formula}DTGΠ(v)|R∪B arc we traversed, and exploiting the fact that any non-{a mathematical formula}DTGΠ(v)|R∪B arc {a mathematical formula}(d′,d) we traversed must be the inverse of a {a mathematical formula}DTGΠ(v)|R∪B arc {a mathematical formula}(d,d′), so we can take {a mathematical formula}(d,d′) for our path back to {a mathematical formula}IB[v]. From {a mathematical formula}IB[v], by (a) we can move to {a mathematical formula}dg, concluding this argument.It remains to prove (a) and (b). Regarding (a), by construction {a mathematical formula}DB(v) consists exactly of the values in {a mathematical formula}DTGΠ(v)|R∪B, which by construction are all reachable from {a mathematical formula}I[v]. So it suffices to prove that {a mathematical formula}DTGΠ(v)|R∪B contains a path from {a mathematical formula}IB[v]=I〚π〛[v] to {a mathematical formula}I[v]. Clearly, π induces a path from {a mathematical formula}I[v] to {a mathematical formula}I〚π〛[v] in {a mathematical formula}DTGΠ(v)|R∪B. Let {a mathematical formula}(d,d′) induced by {a mathematical formula}aB (where {a mathematical formula}a∈π) be any arc on that path. We show that there exists an inverse arc {a mathematical formula}(d′,d) in {a mathematical formula}DTGΠ(v)|R∪B. Because {a mathematical formula}(d,d′) is RSE-invertible in Π, there exists an action {a mathematical formula}a′∈A inducing an arc {a mathematical formula}(d′,d) in {a mathematical formula}DTGΠ(v) whose outside condition is contained in {a mathematical formula}pre(a)∪eff(a). Since, obviously, {a mathematical formula}{〈v/d′〉}∪pre(a)∪eff(a)⊆R∪B, we get {a mathematical formula}pre(a′)⊆R∪B. Thus {a mathematical formula}a′B∈AB, and {a mathematical formula}(d′,d) is an arc in {a mathematical formula}DTGΠB(v) as desired.To show (b), a similar argument can be applied. If {a mathematical formula}πB is an applicable action sequence in {a mathematical formula}ΠB, containing {a mathematical formula}aB traversing the {a mathematical formula}DTGΠ(v)|R∪B arc {a mathematical formula}(d,d′), then by RSE-invertibility there exists an inverse arc whose new red outside conditions (if any) have been established by {a mathematical formula}aB. In more detail, because {a mathematical formula}(d,d′) is RSE-invertible in Π, there exists an action {a mathematical formula}a′∈A inducing an arc {a mathematical formula}(d′,d) in {a mathematical formula}DTGΠ(v) whose outside condition is contained in {a mathematical formula}pre(a)∪eff(a). If {a mathematical formula}ocon(d′,d)[VR]⊆R, then the claim is trivial as {a mathematical formula}R⊆IB. If {a mathematical formula}ocon(d′,d)[VR]⊈R, then {a mathematical formula}(d′,d) is contained in {a mathematical formula}DTGΠ(v)|R∪B←: It is an inverse arc to an arc in {a mathematical formula}DTGΠ(v)|R∪B, and is not itself contained in {a mathematical formula}DTGΠ(v)|R∪B. Thus its red outside condition variables are contained in {a mathematical formula}V(ocon(DTGΠ(v)|R∪B←))⊆V←R, and {a mathematical formula}a′B∈AB as {a mathematical formula}a′∈A(DTGΠ(v)|R∪B←). By construction, {a mathematical formula}ocon(d′,d)[VR]⊆pre(a)∪eff(a). Obviously, all red facts in {a mathematical formula}pre(a)∪eff(a) are true in {a mathematical formula}IB〚πB〛. This concludes the proof.  □The algorithm{a mathematical formula}AcyclicPlanning(ΠB)is sound and complete, and its runtime is polynomial in the size of{a mathematical formula}ΠBand the length of the plan{a mathematical formula}πBreturned.Note that all DTGs are strongly connected, and thus the required sequences of actions {a mathematical formula}πv(d,d′) exist for every pair of values {a mathematical formula}d,d″∈DTGΠ(v). The algorithm starts with an empty sequence, and stops after n iterations. At each iteration the current sequence {a mathematical formula}π=〈a1⋅…⋅ak〉 is extended with sequences {a mathematical formula}πj, {a mathematical formula}1≤j≤k+1, resulting in a new sequence {a mathematical formula}π1⋅〈a1〉⋅…⋅πk⋅〈ak〉⋅πk+1. We need to show that the final sequence is a plan.We show by induction that the sequence obtained at the end of each iteration is a plan for {a mathematical formula}ΠB projected on variables {a mathematical formula}vi,…,vn. For the first iteration, with {a mathematical formula}i=n we have the final sequence being empty if {a mathematical formula}G[vn] is not defined, and {a mathematical formula}πvn(I[vn],G[vn]) otherwise. Assume now that {a mathematical formula}π=〈a1⋅…⋅ak〉 is a plan for {a mathematical formula}ΠB projected on variables {a mathematical formula}vi+1,…,vn. Let {a mathematical formula}πj, {a mathematical formula}1≤j≤k+1 be the sequences as defined in iteration i of the algorithm. Since the causal graph is acyclic, we know that (a) all actions in {a mathematical formula}πj effect only the variable {a mathematical formula}vi, and (b) none of the actions in {a mathematical formula}πj are preconditioned on any of the variables {a mathematical formula}vi+1,…,vn. Thus, focusing on the projection {a mathematical formula}ΠiB of {a mathematical formula}ΠB on variables {a mathematical formula}vi,…,vn, the actions in {a mathematical formula}πj, {a mathematical formula}1≤j≤k+1 are preconditioned only on the variable {a mathematical formula}vi, and effect only that variable. In addition, the sequence {a mathematical formula}π1⋅…⋅πk+1 induces a path in {a mathematical formula}DTGΠi(vi) from {a mathematical formula}I[vi] to {a mathematical formula}G[vi].Furthermore, if {a mathematical formula}pre(aj)[vi] is defined, then the sequence {a mathematical formula}π1⋅…⋅πj induces a path in {a mathematical formula}DTGΠi(vi) from {a mathematical formula}I[vi] to {a mathematical formula}pre(aj)[vi].Thus, {a mathematical formula}π1⋅〈a1〉⋅…⋅πk⋅〈ak〉⋅πk+1 is an applicable sequence of actions that leads to the goal of {a mathematical formula}ΠiB, hence is a plan for {a mathematical formula}ΠiB.  □
      </paragraph>
     </paragraph>
    </section>
   </appendices>
  </root>
 </body>
</html>