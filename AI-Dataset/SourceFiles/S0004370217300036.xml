<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370217300036</url><title>Introduction to the special issue on Combining Constraint Solving with Mining and Learning</title><authors>Andrea Passerini,Guido Tack,Tias Guns</authors><content><section label="1"><section-title>Introduction</section-title><paragraph>Data mining, machine learning and constraint solving are major themes in artificial intelligence research. They have evolved quite independently, though in recent years, there is a growing interest in the potential of integrating these fields.</paragraph><paragraph>Data mining and machine learning are methods for extracting regularities out of data, for example which instances cluster together, what patterns appear frequently in the data or what function can discriminate positive from negative examples. On the other hand, constraint solving investigates generic methods for solving constraint satisfaction and optimization problems.</paragraph><paragraph>Until now, these fields have evolved quite independently. Nevertheless, complex mining and learning tasks can often be formalized in terms of constraints that need to be satisfied, ranging from logical constraints to (non-linear) numeric ones. This methodology is becoming increasingly compelling, as the community is progressively moving from addressing relatively simple tasks on tabular data to complex problems on structured data. The use of machine learning and data mining techniques to enhance constraint solving is equally promising, and only starting to be explored. Consequently, there is a mutual benefit in the study of these approaches, which can lead to advances in both fields.</paragraph><paragraph>For this reason, a number of workshops have been organized on the topic, as well as this special issue. A combination, or integration, of Machine Learning (ML) and Data Mining (DM) with Constraint Solving (CS) can work in two directions: CS techniques can be used to (partly) solve tasks in DM/ML; Alternatively, one can use DM/ML techniques to augment CS techniques. Both directions are increasingly being explored. Yet, a proper approach in either direction requires good knowledge of both research fields, and hence feedback from both communities. The aim of this special issue is to bundle recent advances on combinations of the two fields, and to foster interaction between researchers of both communities.</paragraph><paragraph>The integration of machine learning and constraint solving is rooted in recent trends in these research areas. Fields like statistical relational learning [1], probabilistic programming [2] and structured-output prediction [3] naturally integrate learning algorithms with reasoning engines for addressing the underlying sets of constraints. SAT solvers [4], constraint solvers [5], mixed integer programming [6] as well as high-level modeling languages such as OPL [7] and MiniZinc [8] are extending both the scale and diversity of problems that can be addressed, as well as making the technology more accessible to other researchers.</paragraph><paragraph>A number of developments in recent years have extended the reach and applicability of integrations between the two fields. In particular: an increasing number of discrete and symbolic data mining and learning problems, for which other mathematical programming approaches are less suited; the rise of interest in ‘constraint-based’ mining and learning and the applicability of generic constraint solvers for this purpose; the interest in learning to predict structured outputs, namely complex entities such as trees and graphs, rather than class labels. In general, the increased presence of hard constraints in machine learning and data mining creates a need for generic and flexible methods that deal with such hard constraints. This need, combined with the advancement of both the speed of computer hardware as well as the scalability and maturity of constraint solving systems, is creating new opportunities for interaction between these methods.</paragraph><paragraph>There is also an increasing interest in using machine learning to improve the solving of constraint problems, as well as improving the modeling and the embedding of learned constraints and objectives into the model. This is driven by advances in computer hardware and the availability of large storage capacities and multiple processing units, combined with the increased availability of data regarding all aspects of the solving process. These include internal data regarding the solving and the effect of solver parameters that can be used to learn and automatically adapt the solving behavior; external data regarding the problem domain that can help in learning the parameters of constraints or even entire parts of the constraint model; and external data useful to infer objective functions or other functions that could not otherwise be formalized in terms of simple constraints. These integrations are non-trivial, and many research questions remain regarding how to best approach them.</paragraph><paragraph>In the following we briefly introduce the papers in this special issue, grouping them according to the aspects of integration they focus on.</paragraph></section><section label="2"><section-title>Machine learning and data mining with constraints</section-title><paragraph>In today's data-rich world, one wishes to guide the learning and mining process by information on the type of knowledge we are seeking. A powerful way to express this is through constraints.</paragraph><paragraph>In recent years, constraint solving has been shown to offer a generic methodology that fits many mining and learning settings. This special issue contains recent advancements in the following broad research domains: constraint solving in pattern mining, clustering and learning.</paragraph><section label="2.1"><section-title>Pattern mining using constraint solving</section-title><paragraph>Pattern mining and constraint-based mining seeks to extract patterns from a large set of observations that satisfy the provided constraints, for example that a pattern must appear in a given number of observations. Constraint solving techniques provide a general framework for addressing mining tasks subject to a wide range of constraints. The following papers investigate the possibilities for different classes of tasks.</paragraph><paragraph>MiningZinc: A declarative framework for constraint-based miningby Tias Guns, Anton Dries, Siegfried Nijssen, Guido Tack and Luc De Raedt</paragraph><paragraph>The authors aim at bridging the gap between problem-specific pattern mining procedures and declarative problem solving methods. They present MiningZinc, a modeling language allowing to specify data mining tasks as constraint solving problems. The language is coupled with an execution mechanism which automatically extracts different execution strategies combining generic and specialized solvers. Tasks ranging from closed itemset mining to discriminative pattern mining and mining pattern sets can be modeled and solved in this framework.</paragraph><paragraph>Mining Top-k motifs with a SAT-based frameworkby Said Jabbour, Lakhdar Sais and Yakoub Salhi</paragraph><paragraph>This work introduces a generalization of partial max-SAT and X-minimal model computation called Top-k SAT. Given a preference relation, it finds those solutions that have fewer than k other solutions preferred to it. This framework and the proposed algorithm is applied to constrained itemset mining and sequence mining. The authors show how the approach can be used to mine top-k closed itemsets under size constraints, as well as top-k sequences of items and sequences of itemsets.</paragraph><paragraph>Skypattern mining: From pattern condensed representations to dynamic constraint satisfaction problemsby Willy Ugarte, Patrice Boizumault, Bruno Cremilleux, Alban Lepailleur, Samir Loudni, Marc Plantevit, Chedy Raissi and Arnaud Soulet</paragraph><paragraph>The use of a preference relation is studied in this work from a different angle: the paper studies the task of finding all Pareto-optimal solutions given a set of measures. It investigates under what conditions the number of measures can be safely reduced, through skylineability. It then compares and unifies the traditional data mining approach of post-processing the output of a dedicated algorithm with the novel use of a generic constraint solver, showing the strengths and weaknesses of each.</paragraph></section><section label="2.2"><section-title>Clustering using constraint solving</section-title><paragraph>In clustering the goal is to group instances based on similarity. The following works demonstrate that constraint solving is applicable to many different forms of clustering.</paragraph><paragraph>Constrained clustering by constraint programmingby Thi-Bich-Hanh Dao, Khanh-Chuong Duong and Christel Vrain</paragraph><paragraph>A main body of work is on clustering points into a partition. This work presents a generic partition-based clustering framework using constraint programming, which finds exact, optimal clusters. Multiple clustering measures as well as clustering constraints can be expressed and combined in this framework. It is also shown how it can be used for multi-objective clustering.</paragraph><paragraph>A flexible ILP formulation for hierarchical clusteringby Sean Gilpin and Ian Davidson</paragraph><paragraph>A widely used clustering algorithm in practice is hierarchical clustering, which returns a dendogram or hierarchy instead of a partitioning. As the authors show this problem can be expressed as an Integer Linear Programming problem. This also allows for expressing additional constraints on the instances and the structure, as well as relaxing some constraints to find overlapping groups.</paragraph><paragraph>Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiabilityby Jeremias Berg and Matti Jarvisalo</paragraph><paragraph>Correlation clustering assumes that each pair of instances can be classified as being similar or not similar, and the goal is to find a clustering that correlates well with this classification. The authors show how this problem can be transformed into a (weighted) partial MaxSAT problem, and hence such solvers can be used to find the optimal solution efficiently. They show how a number of other constraints can be incorporated as well.</paragraph></section><section label="2.3"><section-title>Learning using constraint solving</section-title><paragraph>In the following we summarize the contributions which incorporate constraint solving approaches into inference and learning algorithms.</paragraph><paragraph>Semantic-based regularization for learning and inferenceby Michelangelo Diligenti, Marco Gori and Claudio Saccà</paragraph><paragraph>Semantic-based regularization is a framework for statistical relational learning combining first order logic and kernel machines. The idea is that of using kernel machines as base statistical learners for individual target predicates, and first order logic to introduce constraints enforcing known relationships between predicates. The authors provide a broad overview of the framework, its relationship with existing paradigms, and extend it in a number of directions.</paragraph><paragraph>Structured learning modulo theoriesby Stefano Teso, Roberto Sebastiani and Andrea Passerini</paragraph><paragraph>The paper introduces a new way for both learning from structured input as well as predicting structured output. The approach is centered around the definition of hard constraints and soft constraints over Boolean and real variables. Structured SVM learning can then be used to learn the weights of the soft constraints, where SAT module optimization is used for both the SVM learning its separation problem as well as for inferring the best prediction for a given input.</paragraph><paragraph>Relational linear programmingby Kristian Kersting, Martin Mladenov and Pavel Tokmakov</paragraph><paragraph>The authors introduce a framework combining linear and logic programs, two forms of constraint solving. The framework allows to express linear programs involving varying numbers of variables and constraints in a relational fashion, as templates to be compiled into ground logic programs when applied to a knowledge base. The authors additionally developed a lifted linear programming approach based on fractional automorphisms to speed up the optimization process. They show the potential of the proposed approach on a range of machine learning and artificial intelligence tasks.</paragraph><paragraph>Learning an efficient constructive sampler for graphsby Fabrizio Costa</paragraph><paragraph>Learning to generate novel structures from a set of examples is an extremely challenging task, with a wide range of potential applications. The author presents a Metropolis–Hastings sampler for graph generation combining local moves in the space of graphs with probability estimation of the quality of the resulting candidate. Local moves are generated by upgrading the substitutability principle from strings to graphs, for which validity of a change is based on compatibility of its context. These local constraints allow to generate highly structured and constrained graphs of good predictive quality.</paragraph><paragraph>Learning Bayesian networks under equivalence constraintsby Tiansheng Yao, Arthur Choi and Adnan Darwiche</paragraph><paragraph>Given a partially observed dataset, an equivalence constraint for a variable is a set of instances sharing the same (unknown) value for that variable. The resulting constrained log-likelihood is intractable even for networks where standard inference is tractable. The authors propose an efficient approximate algorithm and prove its effectiveness on a number of experimental scenarios. They also show how the framework allows to naturally address tasks like semi-supervised clustering and topic model refinement.</paragraph><paragraph>Integer Linear Programming for the Bayesian network structure learning problemby Mark Bartlett and James Cussens</paragraph><paragraph>This paper discusses ILP techniques for learning the structure of Bayesian Networks. The problem is addressed by a branch-and-cut procedure, where branching is conducted on variable values and linear relaxations of the resulting ILP subproblems are iteratively solved. Cutting planes are added during the search to prune away as much of the search space as possible without removing relevant candidate solutions. The authors present an in-depth analysis of various types of cuts and their influence on the speed of convergence of the algorithm.</paragraph></section></section><section label="3"><section-title>Constraint solving using machine learning</section-title><paragraph>In this special issue three areas where machine learning techniques have been successfully applied to improve constraint solving are discussed: algorithm selection, constraint acquisition, and optimization over learned models.</paragraph><section label="3.1"><section-title>Learning for constraint solver and parameter selection</section-title><paragraph>The goal here is to collect data about the empirical performance of different algorithms and their configurations for different problem instances, and use them to learn how to select and tune the most appropriate solver(s) for the task.</paragraph><paragraph>Automatic construction of parallel portfolios via algorithm configurationby Marius Lindauer, Holger Hoos, Kevin Leyton-Brown and Torsten Schaub</paragraph><paragraph>This article applies automatic algorithm selection and configuration techniques to the problem of automatically constructing parallel SAT solver portfolios from one or multiple, sequential or parallel, basic SAT algorithms. The approach has been evaluated on standard SAT benchmarks, and the results show that this method can produce portfolios that outperform any sequential portfolio solver, without requiring complex parallel code to be developed.</paragraph><paragraph>Alors: An algorithm recommender systemby Mustafa Misir and Michele Sebag</paragraph><paragraph>The authors describe Alors, a novel recommender system for algorithm selection. The system combines collaborative filtering approaches for sparse matrix completion with learning of latent representations for cold-start recommendation on novel problem instances. A deep and extensive evaluation highlights the advantages of the proposed approach in learning from sparse evidence, recommending algorithms for completely novel problem instances and identifying potential reasons in case of suboptimal recommendations.</paragraph></section><section label="3.2"><section-title>Learning constraints</section-title><paragraph>Constraint acquisitionby Christian Bessiere, Frédéric Koriche, Nadjib Lazaar and Barry O'Sullivan</paragraph><paragraph>In this article, the authors tackle the difficult problem of helping non-expert users of constraint programming tools to model their problems. The approach presented here is to learn constraint networks from positive and negative examples, as classified by the user. This article introduces the basic definitions of the constraint acquisition problem, and then presents several variants of the Conacq system, which uses version space learning to acquire constraint networks based on user feedback.</paragraph></section><section label="3.3"><section-title>Optimization over learned models</section-title><paragraph>When dealing with real-world systems, it is not always possible to model the problem or system explicitly in terms of constraints. This can be due to the inherent complexity of the system, e.g. physical systems, or it can be due to inherent dynamics, where the system changes over time.</paragraph><paragraph>An alternative approach is to observe the system and learn the underlying relations using machine learning techniques. By integrating this learned model into the optimization model, one can use it as part of the constraint specification.</paragraph><paragraph>Empirical decision model learningby Michele Lombardi, Michela Milano and Andrea Bartolini</paragraph><paragraph>This paper investigates this approach of integrating a learned model into an optimization model for a thermal-aware dispatching problem on a multi-core CPU, and dubs it Empirical Model Learning (EML). On the machine learning side, neural networks and decision trees are used and it is shown how the resulting models can be expressed in multiple constraint solving formalisms, namely Local Search, Constraint Programming, Mixed Integer Non-Linear Programming and SAT Modulo Theories.</paragraph><paragraph>Auction optimization using regression trees and linear models as integer programsby Sicco Verwer, Yingqian Zhang and Qing Chuan Ye</paragraph><paragraph>The order in which items are auctioned can influence the profit of the auction. This paper proposes to use machine learning to predict the expected profit of an auction order. Furthermore, it is shown how learned linear regression models and regression trees can be modeled in Integer Linear Programming. This combination of regression approaches and Integer Linear Programming yields an effective procedure for optimizing auction orderings.</paragraph></section><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>We would like to thank the co-organizers of the different CoCoMile workshops Barry O'Sullivan, Remi Coletta and Lars Kotthoff. We would also like to thank Luc De Raedt and Siegfried Nijssen for their support.</paragraph></acknowledgements><references><reference label="[1]"><authors>L. Getoor,B. Taskar</authors><title>Introduction to Statistical Relational Learning</title><host>(2007)The MIT Press</host></reference><reference label="[2]"><authors>A.D. Gordon,T.A. Henzinger,A.V. Nori,S.K. Rajamani</authors><title>Probabilistic programming</title><host>International Conference on Software Engineering (ICSE Future of Software Engineering)(2014)IEEE</host></reference><reference label="[3]"><authors>G.H. Bakir,T. Hofmann,B. Schölkopf,A.J. Smola,B. Taskar,S.V.N. Vishwanathan</authors><title>Predicting Structured Data</title><host>(2007)The MIT Press</host></reference><reference label="[4]"><host>A. BiereM.J.H. HeuleH. van MaarenT. WalshHandbook of SatisfiabilityFrontiers in Artificial Intelligence and Applicationsvol. 185 (2009)IOS Press</host></reference><reference label="[5]"><authors>F. Rossi,P. van Beek,T. Walsh</authors><title>Handbook of Constraint Programming</title><host>Foundations of Artificial Intelligence (2006)Elsevier Science Inc.</host></reference><reference label="[6]"><authors>G.L. Nemhauser,L.A. Wolsey</authors><title>Integer Programming and Combinatorial Optimization</title><host>(1988)WileyChichester</host><authors>G.L. Nemhauser,M.W.P. Savelsbergh,G.S. Sigismondi</authors><title>Constraint classification for mixed integer programming formulations</title><host>COAL Bull.20 (1992) pp.8-12</host></reference><reference label="[7]"><authors>P. Van Hentenryck</authors><title>The OPL Optimization Programming Language</title><host>(1999)The MIT Press</host></reference><reference label="[8]"><authors>N. Nethercote,P.J. Stuckey,R. Becket,S. Brand,G.J. Duck,G. Tack</authors><title>MiniZinc: towards a standard CP modelling language</title><host>Principles and Practice of Constraint ProgrammingLecture Notes in Computer Sciencevol. 4741 (2007)Springer pp.529-543</host></reference></references><footnote/></root>