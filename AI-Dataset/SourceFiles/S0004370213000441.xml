<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370213000441</url><title>How much does it help to know what she knows you know? An agent-based simulation study</title><authors>Harmen de Weerd,Rineke Verbrugge,Bart Verheij</authors><abstract>In everyday life, people make use of theory of mind by explicitly attributing unobservable mental content such as beliefs, desires, and intentions to others. Humans are known to be able to use this ability recursively. That is, they engage in higher-order theory of mind, and consider what others believe about their own beliefs. In this paper, we use agent-based computational models to investigate the evolution of higher-order theory of mind. We consider higher-order theory of mind across four different competitive games, including repeated single-shot and repeated extensive form games, and determine the advantage of higher-order theory of mind agents over their lower-order theory of mind opponents. Across these four games, we find a common pattern in which first-order and second-order theory of mind agents clearly outperform opponents that are more limited in their ability to make use of theory of mind, while the advantage for deeper recursion to third-order theory of mind is limited in comparison.</abstract><keywords>Agent-based models;Evolution of theory of mind</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>In everyday life, we regularly make use of theory of mind, by reasoning about what other people know and believe. For example, we identify with characters in literature and movies, and accept that they may have beliefs and intentions different from our own. When telling a joke, a speaker engages in higher-order theory of mind, by believing that the hearer knows that the speaker does not intend to convey an actual fact or opinion. In this paper, we make use of agent-based computational models to explain the evolution of our ability to reason about mental content of others.{sup:1}</paragraph><paragraph>In settings where humans and computational agents perform actions that influence each otherʼs decision-making process, for example in automated negotiation [3], [4], it is necessary to accurately predict the behaviour of others in order to respond appropriately. In artificial intelligence, modeling an opponent explicitly can be achieved through formal approaches such as for example dynamic epistemic logic [5], [6], recursive opponent modeling [7], interactive POMDPs [8], networks of influence diagrams [9], game theory of mind [10], or iterated best-response models such as cognitive hierarchy models [11] and level-n theory [12], [13]. These models allow for recursive modeling of an opponent, by modeling the opponent as an opponent-modeling agent itself, creating increasingly complicated models to predict the actions of increasingly sophisticated opponents. For cognitive agents that are meant to interact with humans, it is important to know whether these formal models of cognition allow for accurate modeling of human reasoning, or whether other models better capture the type of bounded rationality exhibited by humans [14], [15].</paragraph><section label="1.1"><section-title>Theory of mind abilities in humans and animals</section-title><paragraph>In humans, the ability to predict the actions of others by explicitly attributing to them unobservable mental content, such as beliefs, desires, and intentions, is known in psychology as theory of mind[16]. Experiments in which humans play games show evidence that humans use theory of mind recursively in their decision-making process [17], [18], [19], [20]. They take this ability to a second-order theory of mind, in which they reason about the way others reason about mental content. For example, when asked to search for a hidden object in one of four boxes, participants tend to ignore the most salient box, using their nested belief that a hider would believe that a seeker would consider the most obvious place to search for a hidden object to be a box that stands out [21].</paragraph><paragraph>The use of higher-order (i.e. at least second-order) theory of mind allows individuals to make a second-order attribution such as “Alice doesnʼt know that Bob knows that she is throwing him a surprise party”. The human ability for higher-order theory of mind is well-established, both through false belief tasks [17], [22], [23] and strategic games [18], [19], [20], [24]. However, the use of theory of mind of any kind by non-human species is a controversial matter. Primates [25], [26], monkeys [27], but also goats [28], dogs [29] and corvids [30], [31] have been proposed to be able to take the mental content of others into account. However, experiments in which animals behave in a way that is consistent with them having a theory of mind are criticized for not being able to distinguish between theory of mind and strategies that do not rely on mental state attribution [32], [33]. Opponents of attributing theory of mind to animals posit that the animal could have learned the behaviour through previous experiences, combined with simple mechanisms such as stress [34], [35]. Likewise, experiments in which animals fail to show an ability to attribute mental states to others are criticized as well, either for being too complex or ecologically not meaningful [36].</paragraph></section><section label="1.2"><section-title>Evolution of theory of mind</section-title><paragraph>The differences in the ability to make use of theory of mind between humans and other animals raise the issue of the reason for the evolution of a system that allows humans to make use of theory of mind recursively, and use higher-order theory of mind to reason about what other people understand about mental content, while other animals, including chimpanzees and other primates, do not appear to have this ability. Furthermore, whereas recursive opponent modeling could continue indefinitely, humans appear to use higher-order theory of mind only up to a certain point [18], [19], [37]. In an evolutionary sense, the costs of using higher orders of theory of mind may therefore outweigh the benefits.</paragraph><paragraph>One of the hypotheses that explain the emergence of social cognition is the Machiavellian intelligence hypothesis{sup:2}[38]. According to the Machiavellian intelligence hypothesis, social cognition allows individuals to make use of deception and social manipulation to obtain an evolutionary advantage over others. If a parallel can be drawn to higher-order theory of mind, the evolution of a higher-order theory of mind would then be favored by giving individuals a competitive advantage over others. This way, the ability to make use of higher-order theory of mind would both be beneficial to individuals that have this trait, as well as detrimental to individuals without such abilities.</paragraph><paragraph>In this paper, we aim to test the Machiavellian intelligence hypothesis by making use of agent-based modeling in an attempt to show that there are reasonably natural competitive settings in which higher-order theory of mind is advantageous for agents.</paragraph></section><section label="1.3"><section-title>Agent-based modeling</section-title><paragraph>Agent-based modeling is a simulation technique in which individual agents act and interact based on their own perception of their local situation. By explicitly modeling heterogeneity among individual agents, agent-based models can represent systems that are too complex to capture through equation-based modeling approaches. This technique has proven its usefulness as a research tool to investigate how behavioral patterns may emerge from the interactions between individuals (cf. [39], [40]). Among others, agent-based models have been used to explain fighting in crowds [41], trust in negotiations [42], the evolution of agriculture [43], the evolution of cooperation and punishment [44], [45], [46], and the evolution of language [37], [47], [48], [49]. In this paper, we consider agent-based computational models to investigate the advantages of making use of higher-order theory of mind. The use of agent-based models allows us to precisely control and monitor the mental content, including application of theory of mind, of our test subjects. This allows us to simulate computational agents in game settings, and determine the extent to which higher-order theory of mind provides individuals with an advantage over competitors that are more restricted in their use of theory of mind. By varying game settings, this allows us to determine scenarios in which the ability to make use of theory of mind is beneficial to an agent, as well as whether increasingly higher orders of theory of mind provide individuals with increasing advantages over competitors.</paragraph><paragraph>To test the Machiavellian intelligence hypothesis, we consider a number of competitive zero-sum games in which we let our computational agents compete to determine whether the ability to make use of higher-order theory of mind is advantageous in a competitive setting. We consider four different games. First, we consider three variations on repeated single-shot rock–paper–scissors (RPS) games. The transparent setup of RPS allows us to relate differences in the effectiveness of higher-order theory of mind more easily to the structure of the game. The fourth game is Limited Bidding, which involves planning over multiple rounds of play. We also consider a more complex, extensive form game to judge how well the evolutionary advantage of making use of higher-order theory of mind generalizes across games. The four games are described in detail in Section 2.</paragraph><paragraph>Agents may benefit from theory of mind in these games by considering the position of their opponent, and determining what mental content they would have if the roles were reversed. This process is first described intuitively in Section 3. A formal description of the model we use is presented in Section 4. To determine whether the use of theory of mind presents agents with an advantage over opponents without such abilities, we placed agents of different orders of theory of mind in competition with one another. The results of these can be found in Section 5. Finally, Section 6 provides discussion and gives directions for future research.</paragraph><paragraph>Throughout this paper, we will be considering agents engaged in a competitive two-player game. To avoid confusion, we will refer to the focal agent or player as if he were male, and his opponent as if she were female.</paragraph></section></section><section label="2"><section-title>Game settings</section-title><paragraph>We investigate theory of mind in four game settings. The games we describe are strictly competitive games in the sense that they are zero-sum games; there is no possibility for a win–win situation in these games. In each of the games we present, each player can guarantee an expected outcome of zero, irrespective of how his opponent plays. That is, the value of each of these games is zero. By playing a mixed strategy, in which the player randomly selects one of the actions he can perform, a player can prevent his opponent from structurally winning the game. However, through repeated games, a player may learn regularities in his opponentʼs strategy over time, which he might be able to use to his advantage.</paragraph><section label="2.1"><section-title>Rock–paper–scissors variations</section-title><paragraph>In the following subsections, we describe the well-known game rock–paper–scissors (RPS), as well as two variations. The rock–paper–scissors game, also known as RoShamBo, is a game settings in which the ability to model an opponent has informally demonstrated its relevance and applicability [50], [51]. Although no strategy can consistently defeat an agent that plays RPS randomly, an agent that repeatedly encounters the same opponent in the setting of an RPS game may use regularities in the opponentʼs strategy to its advantage. In programming competitions [50], the random strategy only results in an average score. The existence of agents that play according to a non-randomizing strategy allows stronger players to increase their score at the expense of weaker players. The champion of the programming competition in 2000 made use of strategies that detect regularities in the opponentʼs behaviour, but also considered the possibility that the opponent was using similar strategies to model the championʼs behaviour [51].</paragraph><section label="2.1.1"><section-title>Rock–paper–scissors</section-title><paragraph>The game of rock–paper–scissors (RPS) [52] is a two-player symmetric zero-sum game in which both players simultaneously choose one of the three possible actions ‘rock’, ‘paper’, or ‘scissors’. If both choose the same action, the game ends in a tie. Otherwise, the player that chooses ‘rock’ wins from the one that chooses ‘scissors’, ‘scissors’ wins from ‘paper’, and ‘paper’ wins from ‘rock’. The game can be represented as shown in Table 1, which shows the payoff table and a graph representation for the RPS game. The matrix shows the payoff for the player choosing the row action for every possible choice of the player choosing the column action. In the graph, an arrow from action A to action B denotes the relation ‘A defeats B’.</paragraph><paragraph>RPS is known to have a unique mixed-strategy Nash equilibrium (see e.g. [53]) in which the player chooses each of the options with equal probability. That is, when player strategies are known, there is always a player that can improve his expected outcome unless both players play by randomly choosing one of the possible actions. When agents repeatedly play RPS against the same opponent, an agent that randomizes his actions prevents his opponent from taking advantages of regularities in his strategy. However, randomizing also prevents the agent from exploiting regularities in his opponentʼs behaviour that may show up over repeated games. By correctly modeling regularities in an opponentʼs behaviour, an agent can increase his score at the expense of his opponent. Experimental evidence suggests that human participants are poor at generating random sequences [54], [55], and play RPS in a non-random way [56], [57].</paragraph><paragraph>We expect that the ability to make use of theory of mind will present an agent with an advantage over opponents without such abilities. The champion of the first international RPS programming competition in 2000 made use of a strategy that detected regularities in the opponentʼs behaviour, but also considered the possibility that the opponent was using a similar strategy to model the behaviour of the championʼs [51]. That is, this program engaged in theory of mind, by attributing the intention to win the game to his opponent. However, due to the limited action space, there may be a limit to the effectiveness of theory of mind that is specific to this particular game.</paragraph></section><section label="2.1.2"><section-title>Elemental rock–paper–scissors</section-title><paragraph>Although the simple structure of RPS is appealing, the limitation to three actions may influence the effectiveness of higher-order theory of mind. To address this issue, we also consider elemental rock–paper–scissors (ERPS). ERPS extends RPS such that it includes the five actions ‘wood’, ‘metal’, ‘fire’, ‘water’, and ‘earth’, as shown in Table 2. The ERPS game preserves the property of RPS that each action is defeated by exactly one response. That is, for each action that an opponent may play, there exists a unique best response that guarantees a positive outcome for the agent.{sup:3}</paragraph><paragraph>As in the case of RPS, the unique mixed-strategy Nash equilibrium for ERPS is to randomize over all possible actions. However, due to the increased action space, ERPS may have an increased support for theory of mind. That is, we expect theory of mind agents to perform at least as well on ERPS as they would in RPS. Moreover, any differences in the performance of theory of mind agents playing ERPS, compared to those playing RPS, can be attributed to the differences in the structure of the games. In particular, increased performance of higher-order theory of mind agents in ERPS indicates that a limited action space influences the effectiveness of theory of mind.</paragraph></section><section label="2.1.3"><section-title>Rock–paper–scissors–lizard–Spock</section-title><paragraph>Rock–paper–scissors–lizard–Spock (RPSLS) [58] is an extension of RPS, which adds the actions ‘lizard’ and ‘Spock’ to the actions ‘rock’, ‘paper’, and ‘scissors’ from RPS. Like ERPS, RPSLS has five actions, but in RPSLS each action wins from exactly two other actions, while being defeated by the remaining two other actions. Table 3 shows the payoff matrix and a graph representation of the RPSLS game.</paragraph><paragraph>Unlike the previous two games, the best response to an action in RPSLS is not unique. This means that when an agent attributes mental content to his opponent, this does not result in a clear prediction of opponent behaviour. An agent that predicts his opponent to play ‘paper’ has no preference for playing either ‘scissors’ or ‘lizard’, since either will defeat ‘paper’ equally well. As a result, an agent that believes his opponent to believe that the agent will play ‘paper’, will predict that she is going to play either ‘scissors’ or ‘lizard’. Similarly, the opponentʼs behaviour in RPSLS is less informative than in RPS and ERPS. After all, if an agent plays ‘paper’, it may have believed that his opponent would play ‘rock’, but also that she would play ‘Spock’. As a result, playing RPSLS repeatedly against the same opponent provides less information about the opponentʼs behaviour than in RPS and ERPS. Due to this increased difficulty in modeling the opponent, we expect that theory of mind agents perform more poorly in RPSLS than in RPS and ERPS.</paragraph></section></section><section label="2.2"><section-title>Limited Bidding</section-title><paragraph>Unlike rock–paper–scissors and the two variations on it we presented in the previous subsections, Limited Bidding (LB){sup:4} is a game that plays across several rounds. When the game starts, each player is handed an identical set of 5 tokens each, valued 1 to 5. Over the course of 5 rounds, players simultaneously choose one of their own tokens to use as a ‘bid’ for the round. Once both players have made their choice, the tokens selected by the players are revealed and compared, and the round is won by the player that selected the highest value token. In case of a draw, there are no winners. The object of the game is to win as many rounds as possible while losing as few rounds as possible. However, each token may be used only once per game. This forces players to plan ahead and strategically choose which of the tokens that are still available to them they should place as the bid. For example, a player that selects the token with value 5 in the first round will make sure that the first round will not result in a win for his opponent. However, this also means that for the remaining 4 rounds, the token with value 5 will not be available to this player. Players therefore have to weigh the additional probability that they will win the current round against the loss of competitive strength in later rounds that results from using a higher valued token. Fig. 1 shows an example of the way LB is played. In this case, the game is won by the light blue player on the right.</paragraph><paragraph>Note that in LB, it is not possible to win all the rounds. Instead, any player can win a maximum of four rounds, in which case the last round is won by his opponent. As a result, a player can achieve a maximum score of 3 in LB. As for the variations on RPS described earlier, a player can prevent his opponent from winning the game. He can do so by randomly choosing to play one of the tokens still available to him at each round of the game. Averaged over repeated games, this mixed strategy of randomizing over all available choices will result in a score of zero for both the player and his opponent.</paragraph></section><section label="2.3"><section-title>Rational players</section-title><paragraph>In game theory, it is common to make the assumption that every player is rational, and that this fact is known by all players. Moreover, players are assumed to know that everyone knows that every player is rational, continuing in this fashion ad infinitum. In terms of theory of mind, this common knowledge of rationality[60], [61] means that players possess the ability to make use of theory of mind of any depth or order. In this section, we will explain how rational players play the Limited Bidding game under the assumption of common knowledge of rationality.</paragraph><paragraph>For simplicity, we consider a limited bidding game of three tokens. In such a game, players decide what token to play at two moments: once at the start of the game, and again once the result of the first round has been announced. Although new information also becomes available after the second round, the choice of which token to play in the third round is a degenerate one; at the start of the third round both players only have one token left. Since both players have the choice of three tokens to play in the first round, there are nine variations of the subgame the agents play at the second round of the game. We first consider what a rational agent will choose to do at the start of the second round.</paragraph><paragraph>Since every player tries to maximize the number of rounds won and minimize the numbers of rounds lost, at the end of each game, each player receives a payoff equal to the difference between the two. Table 4 lists the payoffs for both players for each possible outcome of the game, where each outcome is represented as the concatenation of the tokens in the order in which the player has played them. Each payoff structure is presented as a tuple {a mathematical formula}(x,y), such that player 1 receives payoff x and player 2 receives payoff y. The subgames that are played at the beginning of the second round are represented as 2-by-2 submatrices, highlighted by alternating background color in Table 4.</paragraph><paragraph>Note that whenever the first round of the game ends in a draw, the resulting subgame is a degenerate one. In this case, both players receive zero payoff irrespective of the final outcome. When the first round does not end in a draw, the resulting subgame is a variation on the matching pennies game [62]. This game is known to have no pure-strategy Nash equilibrium. That is, there is no combination of pure strategies such that each player maximizes his payoff given the strategy of his opponent. However, there is a unique mixed-strategy Nash equilibrium in which each player plays each possible strategy with equal probability. If both players play either one of their remaining tokens with 50% probability, neither one of them has an incentive to switch strategies: given that his opponent is playing randomly, a rational agent has no strategy available that will yield a better expected payoff than playing randomly as well.</paragraph><paragraph>Due to the common knowledge of rationality, each player knows that both of them have reached the conclusion that after the first round, they will both play randomly. This means we can rewrite the payoff matrix to reflect the results of each of the subgames, as shown in Table 5. Note that this is a variation of the rock–paper–scissors game. As before, there is no pure-strategy Nash equilibrium, but the unique mixed-strategy Nash equilibrium is reached when both players play each strategy with equal probability. That is, rational agents, under the assumption of common knowledge of rationality, solve the limited bidding game by playing randomly at each round.</paragraph><paragraph>This result also holds when the game is played using more than three tokens. That is, to prevent their opponent from taking advantage of any regularity in their strategy, rational agents play the limited bidding game randomly.</paragraph></section><section label="2.4"><section-title>Hypotheses about the effectiveness of theory of mind</section-title><paragraph>In this section, we described four different games: rock–paper–scissors, elemental rock–paper–scissors, rock–paper–scissors–lizard–Spock and Limited Bidding. In the game of rock–paper–scissors, agents choose from the three possible actions, each of which is defeated by exactly one of the other actions. The game of elemental rock–paper–scissors resembles RPS in that each action is defeated by exactly one other action, but agents playing ERPS have five different actions to choose from. Rock–paper–scissors–lizard–Spock allows for five different actions as well, but unlike ERPS, each action is defeated by exactly two other actions. Finally, Limited Bidding is a game that spans several rounds, in which agents decide the order in which they play tokens from an initial set of five.</paragraph><paragraph>The game of RPS serves as a transparent base scenario to determine whether theory of mind benefits agents in competitive settings. We expect that the ability to make use of theory of mind is advantageous in competitive settings. Specifically, we expect that the ability to make use of higher orders of theory of mind allows an agent to outperform an opponent that is of a lower order of theory of mind in the game of rock–paper–scissors. In the remainder, we will refer to this expectation as hypothesis {a mathematical formula}HRPS.</paragraph><paragraph>The small number of actions that agents choose from in RPS may limit the effectiveness of higher-order theory of mind. The ERPS game, in which agents have a larger action space, addresses this issue. We expect that the larger action space in the ERPS game allows higher-order theory of mind agents to outperform opponents of a lower order of theory of mind at least as well as in the RPS game, which we will refer to as hypothesis {a mathematical formula}HERPS.</paragraph><paragraph>Agents make use of theory of mind to model the opponent in an attempt to predict her behaviour. As a result, theory of mind is likely to be more effective when the opponent is more predictable. We therefore selected the RPSLS game, in which there is no unique best-response to each action, which should make opponent behaviour harder to predict. Hypothesis {a mathematical formula}HRPSLS states that we expect theory of mind agents to have difficulty predicting their opponent in RPSLS, and to perform more poorly compared to performance in RPS and ERPS.</paragraph><paragraph>Limited Bidding is a multi-stage game, and represents a more complex situation than the single-shot games RPS, ERPS, and RPSLS. This game has been selected to determine whether theory of mind is advantageous, and whether the results from simple single-shot games translate towards a more complex setting. We expect that the results in Limited Bidding may be quantitatively different to those of rock–paper–scissors, but that the results will be qualitatively similar; we will call this hypothesis {a mathematical formula}HLB.</paragraph></section></section><section label="3"><section-title>Playing the games using simulation-theory of mind</section-title><paragraph>In the games described in Section 2, players can prevent their opponent from winning the game by playing randomly. However, this strategy not only prevents an agent from losing the game from his opponent, but also prevents the agent from winning the game for himself. As a result, the randomizing strategy only results in an average score in the RoShamBo programming competitions [50] discussed in Section 2.1. An agent that believes its opponent to play in a non-random way may try to predict the opponentʼs behaviour to take advantage of regularities in its opponentʼs strategy, and win the game.</paragraph><paragraph>For humans, one way of generating predictions of opponent behaviour is by using simulation-theory of mind [63], [64], [65]. In simulation-theory of mind, a player takes the perspective of its opponent, and determines what its own decision would be if the player had been in the position faced by its opponent. Using the implicit assumption that the opponentʼs thought process can be accurately modeled by its own thought process, the player then predicts that the opponent will make the same decision the player would have made if the roles were reversed.</paragraph><paragraph>In this section, we describe the intuition behind the process of perspective-taking for agents that differ in their abilities to explicitly model mental states, and illustrate how this affects their choices in playing RPS. In Section 4, this intuition is described in a computational model. In the remainder, we will speak of a {a mathematical formula}ToMk agent to indicate an agent that has the ability to use theory of mind up to and including the k-th order, but not beyond.</paragraph><section label="3.1"><section-title>Zero-order theory of mind</section-title><paragraph>A zero-order theory of mind ({a mathematical formula}ToM0) agent is unable to model the mental content such as beliefs, desires and intentions of his opponent. In particular, a {a mathematical formula}ToM0 agent is unable to represent that his opponent has goals that are different from his own goals. When predicting his opponentʼs behaviour, the agent is limited to his memory of previous events. The {a mathematical formula}ToM0 agent is intended to model an inexperienced or frustrated player, who only consider his opponentʼs behaviour rather than thinking about the way she reacts to his actions.</paragraph><paragraph>A {a mathematical formula}ToM0 agent believes that what happened in the past is a good predictor for what is going to happen in the future. This reflects human playersʼ tendency to interpret repetition as indicative for a pattern [66]. Fig. 2 illustrates a possible thought process of a {a mathematical formula}ToM0 agent. If a {a mathematical formula}ToM0 agent remembers that his opponent mostly played ‘paper’ in previous RPS games, he concludes that his opponent is most likely to play ‘paper’ in the next game. Given this belief, the {a mathematical formula}ToM0 agent would therefore adjust his behaviour to play ‘scissors’.</paragraph></section><section label="3.2"><section-title>First-order theory of mind</section-title><paragraph>In contrast to a {a mathematical formula}ToM0 agent, a first-order theory of mind ({a mathematical formula}ToM1) agent considers the possibility that his opponent is trying to win the game for herself, and that she reacts to the choices made by the {a mathematical formula}ToM1 agent. To predict his opponentʼs behaviour, the {a mathematical formula}ToM1 agent puts himself in the position of his opponent, and considers the information available to him from her perspective. Fig. 3 shows an example of such a thought process. Suppose that the {a mathematical formula}ToM1 agent remembers that he mostly played ‘paper’ in previous RPS games against the same opponent. He realizes that if the roles were reversed, and he would remember that his opponent mostly played ‘paper’, he would conclude that his opponent would most likely be playing ‘paper’ again, and that he should play ‘scissors’. The {a mathematical formula}ToM1 agent has the ability to attribute this thought process to his opponent, and predict that she is likely to play ‘scissors’. Given this prediction, the {a mathematical formula}ToM1 agent should play ‘rock’.</paragraph><paragraph>Although the {a mathematical formula}ToM1 agent models his opponent as being able use zero-order theory of mind, agents in our setup do not know the extent of the abilities of their opponents with certainty. Through repeated interaction, a {a mathematical formula}ToM1 agent may come to believe that his opponent is not a {a mathematical formula}ToM0 agent, and that she has no beliefs at all. Such an opponent without beliefs could, for example, play ‘rock’ irrespective of what the {a mathematical formula}ToM1 agent has previously played. Based on this belief, a {a mathematical formula}ToM1 agent can choose to play as if he were a {a mathematical formula}ToM0 agent, and follow a thought process such as the one presented in Fig. 2.</paragraph></section><section label="3.3"><section-title>Second-order theory of mind</section-title><paragraph>Just as a {a mathematical formula}ToM1 agent models his opponent as having a zero-order theory of mind, a second-order theory of mind ({a mathematical formula}ToM2) agent models a {a mathematical formula}ToM1 opponent. That is, a {a mathematical formula}ToM2 agent considers the possibility that his opponent is putting herself in his position, and is modeling him as a {a mathematical formula}ToM0 agent. Fig. 4 depicts a possible though process of a {a mathematical formula}ToM2 agent. If the {a mathematical formula}ToM2 agent remembers his opponent to have mostly played ‘paper’ in previous encounters, he would believe his opponent to predict that he will be playing ‘scissors’ more often. As a result, the {a mathematical formula}ToM2 agent would predict his opponent to play ‘rock’ more often, in which case the agent should play ‘paper’ more often himself.</paragraph><paragraph>Each additional order of theory of mind allows for deeper recursion of mental state attribution. Note that each order of theory of mind represents an additional model for opponent behaviour, with a corresponding prediction. A {a mathematical formula}ToM2 agent therefore considers three predictions of his opponentʼs behaviour, based on the application of zero-order, first-order, and second-order theory of mind.</paragraph></section></section><section label="4"><section-title>Model</section-title><paragraph>We implemented computational agents that make use of simulation-theory of mind, and play similarly to intuitive description of Section 3. In this section, we discuss the implementation of these agents, which play the competitive games described in Section 2. The agents presented here differ in their ability to explicitly represent beliefs, and therefore in their ability to make use of theory of mind.</paragraph><section label="4.1"><section-title>Representation of the games</section-title><paragraph>In the model we discuss, a game is a tuple {a mathematical formula}G=〈N,S,A,T,π〉, where:</paragraph><list><list-item label="•">{a mathematical formula}N={i,j} is the set of agents, where i denotes the focal agent, and j denotes his opponent;</list-item><list-item label="•">{a mathematical formula}S is the set of possible states of the game;</list-item><list-item label="•">{a mathematical formula}A=Ai×Aj is the set of possible action pairs, where {a mathematical formula}Ai is the set of actions that can be performed by the agent i, and {a mathematical formula}Aj is the set of actions that can be performed by his opponent j;</list-item><list-item label="•">T is a partial transition function {a mathematical formula}T:S×A→S, which describes the results of the pair of actions of the focal agent and his opponent on the game state; and</list-item><list-item label="•">{a mathematical formula}π=(πi,πj) is the pair of payoff functions {a mathematical formula}πi,πj:S×A→R.</list-item></list><paragraph> An instance of a game as played by two players then consists of an initial game state and a sequence of action pairs. For example, in Limited Bidding, each game state {a mathematical formula}s∈S encodes the tokens that are still available to the agent, as well as those still available to his opponent. This allows agents that are playing LB to distinguish between individual rounds, and make their beliefs concerning the opponentʼs gameplay conditional on the tokens that can still be played.</paragraph><paragraph>Fig. 5 shows a representation of an instance of an LB game, which corresponds to the example game shown in Fig. 1. In Fig. 5, states are represented by boxes, while arrows show the state transitions. Each state transition shows the action pair that caused the transition, as well as the payoff {a mathematical formula}πi for the focal agent and the payoff {a mathematical formula}πj for his opponent. The game starts in the initial state in which both the agent and his opponent each have an identical set of five tokens. The actions that the agent can perform correspond to selecting one of the tokens that is still available to him. Once the agent and his opponent have selected an action, the game transitions to a new state, and both players receive a payoff. This process is repeated until a final state is reached in which no combination of actions leads to a change in game state. For Limited Bidding, this final state is the situation after five rounds, when there are no more tokens to play.</paragraph><paragraph>The game states {a mathematical formula}S are intended to model the different stages of a multi-stage game such as Limited Bidding. Single-shot games, such as the variations on rock–paper–scissors described in Section 2, can be represented as a game that contains two states {a mathematical formula}S={s0,s1}, such that the game transitions from the start state {a mathematical formula}s0 to the end state {a mathematical formula}s1 through any possible action pair {a mathematical formula}(ai,aj)∈A. That is, {a mathematical formula}T(s0,(ai,aj))=s1 for all {a mathematical formula}(ai,aj)∈A.</paragraph><paragraph>Additional to transitioning to a new game state, agents receive a payoff based on their actions. The payoff functions {a mathematical formula}πi,πj:S×A→R determine the payoff {a mathematical formula}πi(s,(ai,aj)) for the focal agent i, and payoff {a mathematical formula}πj(s,(ai,aj)) for his opponent for each combination of game state {a mathematical formula}s∈S and action pair {a mathematical formula}(ai,aj)∈A. Note that since we consider only zero-sum games here, {a mathematical formula}πi(s,(ai,aj))=−πj(s,(ai,aj)).</paragraph></section><section label="4.2"><section-title>Zero-order theory of mind agents</section-title><paragraph>In the present setup, we assume that agents understand the game. Furthermore, we assume no agent considers the possibility that his opponent does not understand the game, or that his opponent believes that he does not understand the game, continuing in this fashion ad infinitum. This means that, for example, none of the agents considers it possible that his opponent will perform an action that is not in her action space {a mathematical formula}Aj. Similarly, no agent believes that his opponent considers it a possibility that he himself will play an action that is not in his action space {a mathematical formula}Ai.</paragraph><paragraph>Note that this assumption is similar to the assumption that the rules and dynamics of the game are common knowledge[5], [67], [68], which requires that each agent understands the game, and knows that his opponent understands the game, and so forth. However, the simulated agents described here are limited in their ability to make use of theory of mind, and may not be able to represent their opponentʼs mental content. That is, in this case it is not possible to assume that the rules and dynamics of the game are common knowledge. Instead, we assume that no agent has beliefs that conflict with common knowledge of the rules and dynamics of the game.</paragraph><paragraph>Agents form beliefs {a mathematical formula}b(0) in the form of a probability distribution over the opponentʼs actions {a mathematical formula}Aj for every game state, such that {a mathematical formula}b(0)(aj;s) represents what the agent believes to be the probability that his opponent will play action {a mathematical formula}aj∈Aj given that the game is in situation {a mathematical formula}s∈S. We assume:{a mathematical formula}{a mathematical formula} That is, (1) agents assign non-negative probability to their opponent playing a certain action in a certain game state, and (2) the probabilities assigned to each possible opponent action sum up to 1 for each possible game state.</paragraph><paragraph>For a {a mathematical formula}ToM0 agent, the belief structure {a mathematical formula}b(0) represents the extent of his beliefs concerning his opponentʼs behaviour. Given the gameʼs payoff function and his beliefs about the way his opponent plays the game, a {a mathematical formula}ToM0 agent is able to assign a subjective value {a mathematical formula}Φi(ai;b(0),s) to playing a certain action {a mathematical formula}ai∈Ai in game state {a mathematical formula}s∈S, given his beliefs {a mathematical formula}b(0) concerning his opponentʼs behaviour. To determine this value, the agent considers how likely he considers it to be that his opponent is going to play some action {a mathematical formula}aj∈Aj. If the opponent would play {a mathematical formula}aj, playing {a mathematical formula}ai would yield the agent an immediate payoff {a mathematical formula}πi(s,(ai,aj)), but it would also cause the game to move forward, and end up in a new state {a mathematical formula}s′=T(s,(ai,aj)). The agent takes this into account by planning ahead, and determining the maximum value he can achieve when the game reaches state {a mathematical formula}s′. The combination of immediate payoff {a mathematical formula}πi(s,(ai,aj)) and the maximum value that can be achieved in the state {a mathematical formula}T(s,(ai,aj)) are weighted by what the agent believes to be the probability {a mathematical formula}b(0)(aj;s) that his opponent is actually going to play action {a mathematical formula}aj. The value {a mathematical formula}Φi(ai;b(0),s) that the focal agent i assigns to playing action {a mathematical formula}ai in game state s, based on his belief {a mathematical formula}b(0) concerning his opponentʼs behaviour, is given by{a mathematical formula} We assume that agents choose rationally given their beliefs. That is, agents choose to play the action {a mathematical formula}ai that maximizes the value function. This is represented by the decision function {a mathematical formula}ti⁎, given by{a mathematical formula}</paragraph><paragraph label="Example 1">Consider an agent that plays rock–paper–scissors against his opponent. The RPS game consists of two states {a mathematical formula}S={s0,s1}, where the first state {a mathematical formula}s0 represents the start of the game, and the second state {a mathematical formula}s1 is the end of the game. The action spaces of the agent and his opponent are the same, {a mathematical formula}Ai=Aj={R,P,S}. The transition function T is defined such that {a mathematical formula}T(s0,ai,aj)=s1 for all {a mathematical formula}ai∈Ai, and {a mathematical formula}aj∈Aj. The payoffs in state {a mathematical formula}s0 are given by Table 1, while payoffs are zero in state {a mathematical formula}s1.We consider a {a mathematical formula}ToM0 agent, whose mental content is listed in Table 6a. The agentʼs zero-order beliefs {a mathematical formula}b(0) indicate that the agent believes that there is a 50% probability that his opponent is going to play R, a 30% probability that his opponent is going to play P, and a 20% probability that his opponent is going to play S. Based on these zero-order beliefs, the agent can determine the value for each of the actions R, P, and S, based on the expected payoff. For example, the agent believes that if he plays R, there is a 30% probability that he will lose because his opponent played P, and a 20% probability that he will win because his opponent played S. This results in the following values:{a mathematical formula}{a mathematical formula}{a mathematical formula} The agent then chooses to play the action that has maximum value. In this case:{a mathematical formula} That is, the {a mathematical formula}ToM0 agent described in Table 6a chooses to play P.</paragraph></section><section label="4.3"><section-title>First-order theory of mind agents</section-title><paragraph>A {a mathematical formula}ToM1 agent attributes beliefs to his opponent in the form of an additional probability distribution {a mathematical formula}b(1). Here, {a mathematical formula}b(1)(ai;s) represents what the agent believes his opponent to judge what the probability is that he will play action {a mathematical formula}ai∈Ai in game state {a mathematical formula}s∈S. However, a {a mathematical formula}ToM1 agent also has zero-order beliefs {a mathematical formula}b(0) about what his opponent will do. The decision process of the {a mathematical formula}ToM1 agent consists of roughly three steps:</paragraph><list><list-item label="1.">making a prediction {a mathematical formula}aˆj(1) of opponent behaviour, based on the agentʼs first-order beliefs {a mathematical formula}b(1);</list-item><list-item label="2.">integrating the first-order prediction {a mathematical formula}aˆj(1) of opponent behaviour and the zero-order belief {a mathematical formula}b(0); and</list-item><list-item label="3.">selecting the action that maximizes the agentʼs expected payoff, given his integrated beliefs about opponent behaviour.</list-item></list><paragraph> Let us describe each step more precisely.</paragraph><paragraph>(1) First, the {a mathematical formula}ToM1 agent makes a prediction of opponent behaviour based on his first-order beliefs {a mathematical formula}b(1). Using simulation-theory of mind, the agent uses his own decision function {a mathematical formula}t⁎ to make a prediction of the action his opponent will play. To do so, the agent determines the action {a mathematical formula}aˆj(1)∈Aj that maximizes the value function from the perspective of the opponent, given that the agent believes his opponent to have zero-order beliefs {a mathematical formula}b(1). That is,{a mathematical formula} Note that Eq. (5) is similar to Eq. (4). That is, the {a mathematical formula}ToM1 agent determines his prediction of opponent behaviour similar to the way a {a mathematical formula}ToM0 agent determines his own behaviour. In calculating the prediction {a mathematical formula}aˆj(1), the agent makes use of his own value function and his beliefs {a mathematical formula}b(1). Note that by specifying {a mathematical formula}aˆj(1), the agent makes a single prediction of the opponentʼs behaviour rather than assigning probabilities to each possible opponent action. This allows the agent to check the validity of his prediction more easily, by comparing the prediction {a mathematical formula}aˆj(1) with the opponentʼs actual behaviour. However, this also means that slight differences between the agentʼs value function and that of his opponent may render the prediction incorrect.</paragraph><paragraph>(2) A {a mathematical formula}ToM1 agentʼs first-order theory of mind provides the agent with a prediction {a mathematical formula}aˆj(1) of opponent behaviour. This prediction may conflict with his zero-order beliefs {a mathematical formula}b(0). The extent to which first-order theory of mind governs the decisions of the agentʼs actions is determined by his confidence {a mathematical formula}0⩽c1⩽1 that first-order theory of mind accurately predicts his opponentʼs behaviour. The value of his confidence {a mathematical formula}c1 allows the agent to distinguish between different types of opponents, and he weights his zero-order beliefs against the prediction of first-order theory of mind accordingly. This weighting process is captured by a belief integration function U. This function integrates the agentʼs first-order prediction {a mathematical formula}aˆj with his zero-order beliefs {a mathematical formula}b(0) of opponent behaviour. Compared to his zero-order beliefs {a mathematical formula}b(0), the agentʼs integrated belief that his opponent will be playing action {a mathematical formula}aˆj(1) is increased, while his integrated belief that his opponent will be playing any other action is decreased. Specifically,{a mathematical formula}</paragraph><paragraph>(3) After integrating his zero-order beliefs {a mathematical formula}b(0) and his prediction of opponent behaviour {a mathematical formula}aˆj(1) based on first-order theory of mind, the agent chooses what action to play. This decision is made analogously to the way a {a mathematical formula}ToM0 agent decides (Eq. (4)). However, the {a mathematical formula}ToM1 agent decides based on his integrated beliefs {a mathematical formula}U(b(0),aˆj(1),c1) of opponent behaviour, instead of his zero-order beliefs {a mathematical formula}b(0) directly. That is, a {a mathematical formula}ToM1 agent chooses to play the action given by{a mathematical formula} In the special case where the agent has no confidence in first-order theory of mind, {a mathematical formula}c1=0, the {a mathematical formula}ToM1 agentʼs decision is only influenced by his zero-order beliefs. In this case, the agent chooses as if he were a {a mathematical formula}ToM0 agent.</paragraph><paragraph label="Example 2">Consider a {a mathematical formula}ToM1 agent that plays rock–paper–scissors, similar to the agent in Example 1, whose mental content is given in Table 6b. The table shows that the {a mathematical formula}ToM1 agent has zero-order beliefs {a mathematical formula}b(0), which indicate the agentʼs beliefs concerning his opponentʼs actions, as well as first-order beliefs {a mathematical formula}b(1). For example, since {a mathematical formula}b(1)(R,s0)=0.4, the agent believes that his opponent believes that there is a 40% probability that he is going to play R. Taking the perspective of his opponent, the agent determines what he would do in her place. That is, the agent first calculates the value that he would assign to each of the actions available to his opponent, if his first-order beliefs {a mathematical formula}b(1) were actually his zero-order beliefs, and his opponentʼs payoffs were actually his payoffs.{a mathematical formula}{a mathematical formula}{a mathematical formula} The agentʼs first-order theory of mind predicts that his opponent will select the action that will yield her the highest payoff.{a mathematical formula} Using his first-order theory of mind, the agent predicts that his opponent is going to play P.Note that the agentʼs prediction {a mathematical formula}aˆj(1) conflicts with his zero-order beliefs {a mathematical formula}b(0). According to his first-order theory of mind, his opponent is going to play P, while the agentʼs zero-order beliefs assign a 50% probability that his opponent is going to play R. To be able to make a decision, the agent integrates his first-order prediction with his zero-order beliefs {a mathematical formula}b(0). In this case, the agentʼs confidence {a mathematical formula}c1 in first-order theory of mind is 0.9. This means that the agentʼs integrated beliefs are determined for 90% by his prediction based on first-order theory of mind, and for 10% by his zero-order beliefs.{a mathematical formula}{a mathematical formula}{a mathematical formula} After integrating his zero-order beliefs and first-order prediction, the agent believes there is a 5% probability that his opponent is going to play R, a 93% probability that his opponent is going to play P and a 2% probability that his opponent is going to play S.Based on his integrated beliefs, the agent determines the value for playing each of the actions.{a mathematical formula}{a mathematical formula}{a mathematical formula} The agent then chooses to play the action that has maximum value. In this case:{a mathematical formula} That is, the {a mathematical formula}ToM1 agent described in Table 6b chooses to play S.</paragraph></section><section label="4.4"><section-title>Second-order theory of mind agents</section-title><paragraph>Similar to the way a {a mathematical formula}ToM1 agent models his opponent as a {a mathematical formula}ToM0 agent, a {a mathematical formula}ToM2 agent considers the possibility that his opponent may be a {a mathematical formula}ToM1 agent. As such, the {a mathematical formula}ToM2 agent has an explicit model of what beliefs he believes his opponent to be attributing to him. In our model, these beliefs are represented by an additional belief structure {a mathematical formula}b(2). Using simulation-theory of mind, the agent attributes the decision-making process described by Eq. (7) to his opponent. That is, the agent considers the game from the perspective of his opponent, and determines what he would do in her position, if he were a {a mathematical formula}ToM1 agent.</paragraph><paragraph>To determine his opponentʼs actions, the {a mathematical formula}ToM2 agent needs to know her confidence {a mathematical formula}c1 in first-order theory of mind. In our experiments, we have assumed that all {a mathematical formula}ToM2 agents use a value of 0.8 to determine their opponentʼs behaviour playing as a {a mathematical formula}ToM1 agent.{sup:5} Based on second-order theory of mind, the {a mathematical formula}ToM2 agent therefore predicts that his opponent will be playing (square brackets are used for readability){a mathematical formula} This prediction {a mathematical formula}aˆj(2) based on second-order theory of mind is integrated with the {a mathematical formula}ToM2 agentʼs zero-order beliefs {a mathematical formula}b(0) and his prediction {a mathematical formula}aˆj(1) based on first-order theory of mind, before he makes his choice of what action to play. As for the {a mathematical formula}ToM1 agent, a {a mathematical formula}ToM2 agent does not know at which order of theory of mind his opponent is playing. Instead, the extent to which second-order theory of mind governs the decisions of the {a mathematical formula}ToM2 agentʼs actions is determined by his confidence {a mathematical formula}0⩽c2⩽1 that second-order theory of mind accurately predicts his opponentʼs behaviour. The {a mathematical formula}ToM2 agent weights the integrated beliefs in Eq. (7) against his prediction of opponent behaviour {a mathematical formula}aˆj(2) based on second-order theory of mind. As a result, the {a mathematical formula}ToM2 agentʼs integrated beliefs about his opponent behaviour are given by{a mathematical formula} The {a mathematical formula}ToM2 agent therefore performs two belief integration steps. First, the agent integrates his zero-order beliefs {a mathematical formula}b(0) concerning his opponentʼs behaviour with his prediction {a mathematical formula}aˆj(1) based on application of first-order theory of mind. In the second step, his prediction {a mathematical formula}aˆj(2) based on second-order theory of mind is integrated into these beliefs as well. The {a mathematical formula}ToM2 agent then makes his final choice of what action to select based on these beliefs:{a mathematical formula}</paragraph><paragraph label="Example 3">Consider a {a mathematical formula}ToM2 agent that plays rock–paper–scissors, similar to Example 2, whose mental content is given in Table 7. When a {a mathematical formula}ToM2 agent considers his opponentʼs first-order beliefs about his own actions, the agent performs the decision process of a {a mathematical formula}ToM1 agent from the viewpoint of his opponent. That is, he calculates what he believes that she predicts that he will do based on her first-order beliefs. The agentʼs model of his opponentʼs first-order beliefs are captured by {a mathematical formula}b(2). This is what the agent believes his opponent to believe his first-order beliefs to be. Firstly, the agent determines what he would do if his second-order beliefs {a mathematical formula}b(2) were actually his zero-order beliefs.{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula} That is, the {a mathematical formula}ToM2 agent believes his opponent to predict that he will be playing R.Secondly, the agent determines how his opponentʼs prediction that he will be playing R influences her zero-order beliefs. The agent does not explicitly model the opponentʼs confidence in first-order theory of mind. Rather, he assumes a value of 0.8 for this confidence. The agent then integrates his first-order beliefs {a mathematical formula}b(1), which he believes to correspond to his opponentʼs zero-order beliefs, with the prediction that he will play R.{a mathematical formula}{a mathematical formula}{a mathematical formula} These integrated beliefs specify what the agent believes what his opponentʼs beliefs are concerning his actions. For example, based on application of his second-order theory of mind, the {a mathematical formula}ToM2 agent believes that his opponent believes that there is an 88% probability that he himself will play R. From the viewpoint of his opponent, the agent then determines what the value would be for playing each of the possible actions, given the integrated beliefs of opponent action.{a mathematical formula}{a mathematical formula}{a mathematical formula} The action that maximizes this value represents the agentʼs prediction of the action his opponent is going to play according to his second-order theory of mind.{a mathematical formula} Based on second-order theory of mind, the agent therefore believes his opponent will play P.To make a decision, the agent integrates his zero-order beliefs {a mathematical formula}b(0), his first-order prediction {a mathematical formula}aˆj(1)=P (see Example 2), and his second-order prediction {a mathematical formula}aˆj(2)=P. Example 2 shows how the agentʼs zero-order beliefs and his first-order prediction of opponent behaviour are integrated. Using this confidence {a mathematical formula}c2, the agent also integrates his belief that his opponent is going to play P. In this example, the agent has confidence {a mathematical formula}c2=0.1 in second-order theory of mind. This results in the following integrated beliefs:{a mathematical formula}{a mathematical formula}{a mathematical formula} Based on these integrated beliefs, the agent determines the value for playing each of the actions.{a mathematical formula}{a mathematical formula}{a mathematical formula} The agent then chooses to play the action that maximizes the value. In this case:{a mathematical formula} Based on his integrated beliefs of what the opponent is going to do, the {a mathematical formula}ToM2 agentʼs choice is to play S.</paragraph></section><section label="4.5"><section-title>Higher orders of theory of mind agents</section-title><paragraph>For every order of theory of mind available to the agent beyond the second-order, say order k, the agent maintains an additional belief structure {a mathematical formula}b(k). These beliefs are used to expand his decision process by modeling the decision process of a {a mathematical formula}(k−1)st-order theory of mind agent from his opponentʼs point of view. The resulting prediction is weighted against the decision process of {a mathematical formula}(k−1)st-order of theory of mind from his own point of view. For example, a {a mathematical formula}ToM3 agent expands the decision process of a {a mathematical formula}ToM2 agent, represented by Eq. (10). He does so by modeling the decision process of a {a mathematical formula}ToM2 agent from his opponentʼs point of view. That is, the {a mathematical formula}ToM3 agent calculates his prediction of opponent behaviour {a mathematical formula}aˆj(3) based on third-order theory of mind:{a mathematical formula} Once the {a mathematical formula}ToM3 agent has determined his prediction based on third-order theory of mind, he weights this prediction against the decision process of a {a mathematical formula}ToM2 agent, represented by Eq. (10). The extent to which the {a mathematical formula}ToM3 agentʼs prediction {a mathematical formula}aˆj(3) of a {a mathematical formula}ToM2 opponentʼs behaviour is reflected in his own behaviour is determined by his confidence {a mathematical formula}0⩽c3⩽1 that third-order theory of mind yields accurate predictions of his opponentʼs behaviour. That is, the choice of a {a mathematical formula}ToM3 agent is given by{a mathematical formula}</paragraph></section><section label="4.6"><section-title>Belief adjustment and learning speed</section-title><paragraph>In the previous subsections, we discussed how agents of different orders of theory of mind decide what action to play, based on their current beliefs {a mathematical formula}b(k) and confidence levels {a mathematical formula}ck. By placing himself in the position of his opponent, and viewing the game from her perspective, an agent makes predictions for the action his opponents is going to perform. Each order of theory of mind available to the agent generates such a prediction. The agent can use the accuracy of these predictions to gain information about the opponentʼs abilities over repeated games, and adjust his beliefs and confidence levels accordingly.</paragraph><paragraph>For example, a {a mathematical formula}ToM2 agent may learn that his opponent is not playing as predicted by his second-order theory of mind, but that his first-order theory of mind consistently makes accurate predictions of her actions. In such a case, the {a mathematical formula}ToM2 agent may start to play as if he were a {a mathematical formula}ToM1 opponent, and ignore predictions from his second-order theory of mind altogether. However, it is important to note that while the {a mathematical formula}ToM2 agent may adjust his behaviour to take advantage of predictable behaviour of his opponent, his opponent is trying to do the same. In this section, we describe how agents update their beliefs {a mathematical formula}b(k) and confidence levels {a mathematical formula}ck when they observe the outcome of a game.</paragraph><paragraph>When an agent plays against an unfamiliar opponent for the first time, his beliefs {a mathematical formula}b(k) are initialized randomly, while his confidence levels {a mathematical formula}ck are initialized at zero. After each round, the actual choice {a mathematical formula}a˜i of the agent and {a mathematical formula}a˜j of his opponent are revealed. At this moment, an agent updates his confidence in theory of mind based on the accuracy of his predictions. A {a mathematical formula}ToM1 agent increases his confidence {a mathematical formula}c1 in first-order theory of mind when his first-order prediction {a mathematical formula}aˆj(1) calculated through Eq. (5) was correct. In other cases, his confidence in first-order theory of mind decreases. This process is represented by the update{a mathematical formula} where {a mathematical formula}0⩽λ⩽1 is an agent-specific learning speed. An agentʼs learning speed indicates the relative weight of new information in determining beliefs. An agent with a high learning speed determines whether his opponent is a {a mathematical formula}ToM0 agent based on his most recent observations. The {a mathematical formula}ToM1 agentʼs confidence {a mathematical formula}c1 in first-order theory of mind reflects the accuracy of first-order theory of mind in the most recent games in this case. An agent with a low learning speed depends on experience built up over a longer period of time.</paragraph><paragraph>For higher orders of theory of mind, an agent additionally adjusts each of his confidences {a mathematical formula}ck in kth-order theory of mind for each order k of theory of mind available to him. Similar to the update of his confidence {a mathematical formula}c1 in first-order theory of mind, an agent reduces his confidence {a mathematical formula}ck in kth-order theory of mind when the corresponding prediction {a mathematical formula}aˆj(k) of opponent behaviour based on application of kth-order theory of mind was incorrect, that is {a mathematical formula}aˆj(k)≠a˜j. However, an agent only increases his confidence in kth-order theory of mind when it yields correct predictions, and the predictions made by each order of theory of mind lower than k were incorrect. If there is some lower order {a mathematical formula}n&lt;k of theory of mind for which {a mathematical formula}aˆj(n)=a˜j, the agent does not increase his confidence in kth-order theory of mind. That is, theory of mind agents only grow more confident in the use of higher-order theory of mind when this results in accurate predictions that could not have been made with a lower order of theory of mind. This feature makes agents less likely to overestimate the theory of mind abilities of their opponent.{a mathematical formula} When the actual choice of the agent {a mathematical formula}a˜i and his opponent {a mathematical formula}a˜j are revealed, the agent also updates his beliefs {a mathematical formula}b(k). Since the zero-order beliefs {a mathematical formula}b(0) represent the agentʼs beliefs concerning his opponentʼs behaviour, these beliefs are updated using his opponentʼs choice {a mathematical formula}a˜j. This is done by increasing the belief the opponent will perform action {a mathematical formula}a˜j in the same game state {a mathematical formula}s∈S, while decreasing the belief that she will perform any other action. Second-order beliefs {a mathematical formula}b(2) specify what the agent believes his opponent to believe about what he believes that she is going to do. That is, an agentʼs second-order beliefs {a mathematical formula}b(2) describe beliefs concerning the actions of his opponent, and are therefore updated using her choice {a mathematical formula}a˜j as well. After this update, the agent believes that his opponent believes that he believes more strongly that she will perform the action {a mathematical formula}a˜j in the same game state {a mathematical formula}s∈S. This is true for each of the even-numbered orders of theory of mind available to the agent. The belief structure {a mathematical formula}b(k) of all even-numbered orders of theory of mind are updated using the opponents choice {a mathematical formula}a˜j.</paragraph><paragraph>On the other hand, the odd-numbered orders of theory of mind describe the actions of the agent himself. These beliefs are therefore updated using the agentʼs choice {a mathematical formula}a˜i. For example, after the belief adjustment, the agent believes that his opponent believes more strongly that he will perform action {a mathematical formula}a˜i when the same game state {a mathematical formula}s∈S is encountered again. Using the belief updating function U, the beliefs are adjusted using the agentʼs learning speed λ, such that{a mathematical formula}{a mathematical formula} That is, the agent adjusts his beliefs based on the forecasting technique of exponential smoothing [69]. Note that these adjustments only apply to the game state s in which the actions were taken.</paragraph><paragraph>The agentʼs learning speed λ determines how quickly the agent learns. That is, a higher value of λ shows that the agent changes his beliefs more radically based on new information. At the maximum of {a mathematical formula}λ=1, an agent effectively believes that the last action his opponent performed determines future behaviour. At the other extreme of {a mathematical formula}λ=0, the agent does not learn, and does not change his beliefs when new information becomes available. This also means that an agent with learning speed {a mathematical formula}λ=0 does not change his behaviour.</paragraph><paragraph>The agents we describe do not actively try to model the learning speed λ of their opponent. Instead, an agent assumes that his opponent updates her beliefs using the same learning speed as he does himself. That is, our computational agents do not consider the possibility that their opponent reacts differently to new information than they do themselves. This means that in general, the beliefs that an agent attributes to his opponent are structurally different from her actual beliefs.</paragraph><paragraph>An agent makes use of theory of mind by considering the position of his opponent from his own viewpoint. When an agent makes use of second-order theory of mind, he also considers what his opponent knows about his own viewpoint. Since the games we consider have symmetric information, this causes the agentʼs second-order beliefs {a mathematical formula}b(2) to resemble his zero-order beliefs {a mathematical formula}b(0) more closely with each update. That is, the agent eventually believes that a first-order theory of mind opponent knows what his zero-order beliefs are.</paragraph><paragraph>Due to the restrictions on the learning speed λ, Eqs. (14), (15) preserve the normalization and non-negativity of beliefs. Similarly, the confidences {a mathematical formula}ci in the application of ith-order theory of mind remain limited to the range {a mathematical formula}[0,1].</paragraph><paragraph label="Example 4">Consider the {a mathematical formula}ToM2 agent from Example 3, whose mental content is given in Table 7. Once both the agent and his opponent have decided on an action to play, the actions are revealed to both players, and each receives the payoff based on those actions. Once the outcome of the game is revealed, each agent updates his beliefs based on what is observed. Our calculations showed that the {a mathematical formula}ToM2 agent we discussed in our example has played action {a mathematical formula}a˜i=S. We assume that his opponent played {a mathematical formula}a˜j=P, and that the agentʼs learning speed {a mathematical formula}λ=0.6.Table 8 lists the agentʼs predictions, confidences in theory of mind and beliefs before and after the belief update. Depending on the accuracy of the prediction of application of ith-order theory of mind, the confidence {a mathematical formula}ci in that order of theory of mind increases or decreases. In our example, first-order theory of mind accurately predicted that the opponent would play P, since {a mathematical formula}a˜j=aˆj(1). As a result, the new confidence {a mathematical formula}c1, as calculated by Eq. (12), becomes{a mathematical formula}Table 8 shows that {a mathematical formula}aˆj(2)=P=a˜j, and thus that second-order theory of mind also correctly predicted that the opponent would play P. However, since first-order theory of mind is of a lower order than second-order theory of mind, and since first-order theory of mind also correctly predicted the action of the opponent, the confidence {a mathematical formula}c2 in second-order theory of mind remains unchanged.The actions that were actually played by the agent and his opponent also change the agentʼs beliefs. Each even-numbered order of theory of mind refers to beliefs concerning the opponentʼs actions. These beliefs are therefore updated to reflect the action that the opponent has taken most recently. This is done by increasing the belief that the opponent will perform the same action, in our case P, while decreasing the other beliefs. That is, the agentʼs zero-order beliefs {a mathematical formula}b(0) are updated, such that after the update{a mathematical formula}{a mathematical formula}{a mathematical formula} This means that after the belief update, the agent believes that there is a 72% probability that his opponent is going to repeat the action P in the next round.The agentʼs second-order beliefs {a mathematical formula}b(2) also concern the actions of the opponent. Specifically, the agentʼs second-order beliefs {a mathematical formula}b(2) determine what the agent believes his opponent to believe what he believes about her actions. The agent uses the action {a mathematical formula}a˜j=P actually performed by his opponent to update his second-order beliefs as well.The odd-numbered orders of theory of mind represent beliefs concerning the agentʼs own actions. These beliefs are therefore updated to reflect that the agent chose action {a mathematical formula}a˜i=S. For the agentʼs first-order beliefs {a mathematical formula}b(1), this results in{a mathematical formula}{a mathematical formula}{a mathematical formula} This means that after the belief update, the agent believes that his opponent believes that there is a 64% probability that he will repeat the action S in the next round.</paragraph></section></section><section label="5"><section-title>Results</section-title><paragraph>The agent model described in Section 4 has been implemented in Java and its performance has been tested in each of the settings described in Section 2. For the rock–paper–scissors game, as well as the variations on this game, each trial consisted of an agent that plays 20 consecutive games against the same opponent.{sup:6} An agentʼs trial score is the average of the agentʼs game scores over all games in the trial. The graphs in this section depict the average trial score, averaged over 500 trials. Since Limited Bidding is a more complex game, a longer sequence is needed to learn to model the opponent. Each trial in this game consisted of an agent that plays 50 consecutive games against the same opponent. Our results were qualitatively similar if longer trials of 100 games were used instead.</paragraph><paragraph>In this section, performance is measured as the average trial score of the focal agent, as a function of his learning speed {a mathematical formula}λi, as well as the learning speed {a mathematical formula}λj of his opponent. The figures in this section show simulation results for every 0.02 step in learning speeds over the range {a mathematical formula}λi,λj∈[0,1]. We report the results of simulations in which a focal agent is exactly one order of theory of mind higher than his opponent. In simulations in which the difference in theory of mind ability of the focal agent and his opponent was larger than one order, performance of the focal agent turned out to be similar.</paragraph><section label="5.1"><section-title>Rock–paper–scissors</section-title><paragraph>Fig. 6 shows how the ability to represent mental content of others affects the performance of agents in the RPS game as a function of the learning speed {a mathematical formula}λi of the focal agent and of the learning speed {a mathematical formula}λj of his opponent. Higher and lighter areas indicate that the focal agent won more games than he lost, while lower and darker areas show that his opponent had the upper hand. To emphasize the shape of the surface, the grid that appears on the bottom plane has been projected onto the surface, and the plane of zero performance appears as a semi-transparent surface in the figure. Red areas indicate where performance was not significantly different from zero, at a significance level {a mathematical formula}α=0.01.</paragraph><paragraph>Fig. 6a shows that a {a mathematical formula}ToM1 agent that has a learning speed {a mathematical formula}λi=0 cannot compete with his opponent. When the agent does not learn from his opponentʼs behaviour, he loses nearly all rounds. Similarly, his opponent loses nearly all rounds when she does not learn at all ({a mathematical formula}λj=0). This shows that both zero-order theory of mind agents and first-order theory of mind agents can successfully model an opponent that always performs the same action.</paragraph><paragraph>The figure also shows that a {a mathematical formula}ToM1 agent mostly outperforms a {a mathematical formula}ToM0 opponent. Whenever the {a mathematical formula}ToM1 agentʼs learning speed is at least {a mathematical formula}λi&gt;0.1, he will on average win more rounds than he loses, and obtain a positive score. The {a mathematical formula}ToM1 agentʼs score is particularly high when both he and his opponent learn at a high rate, in which case the agent wins almost all rounds. When his {a mathematical formula}ToM0 opponent learns at a low rate, the average score of the {a mathematical formula}ToM1 agent is reduced.</paragraph><paragraph>The relatively low performance of the {a mathematical formula}ToM1 agent against slow learning opponents is due to the fact that learning speed determines an agentʼs memory. A {a mathematical formula}ToM0 agent with high learning speed adapts to new situations quickly, but also quickly forgets information from previous rounds. When faced with an unpredictable opponent, a {a mathematical formula}ToM0 agent with high learning speed will therefore choose erratically, but with confidence. In this case, the {a mathematical formula}ToM0 agent believes that his opponent will repeat the same action she has performed the last time they met. For a {a mathematical formula}ToM1 opponent, this represents a predictable situation that she can use to her advantage.</paragraph><paragraph>Conversely, a {a mathematical formula}ToM0 agent with low learning speed retains his former beliefs for a longer time. When encountering an unpredictable opponent, such a {a mathematical formula}ToM0 agent will therefore start playing with little confidence. That is, the probability distribution modeled by {a mathematical formula}b(0) may gradually come to resemble a uniform distribution. This causes a {a mathematical formula}ToM0 agent with low learning speed to play the action that he weakly believes to be a slightly better choice than the rest. This also makes it more difficult for a {a mathematical formula}ToM1 opponent to predict which token the {a mathematical formula}ToM0 agent with low learning speed will play, since his choice is not robust against small deviations in his beliefs.</paragraph><paragraph>Although a {a mathematical formula}ToM1 agent performs better against an opponent that learns quickly than against an opponent that learns slowly, performance of the {a mathematical formula}ToM1 agent is largely independent of the quality of his model. When a {a mathematical formula}ToM1 agent makes use of his theory of mind, he assumes that his opponent reacts to new information the same way he does. That is, the agent assumes that he and his opponent share the same learning speed. However, the figure does not show an increase in performance along the line of equal learning speeds. That is, the cost of assuming equal learning speeds is low in RPS.</paragraph><paragraph>Fig. 6b shows the performance of a {a mathematical formula}ToM2 agent playing RPS against a {a mathematical formula}ToM1 opponent. Note that Fig. 6b is similar to Fig. 6a. As for the {a mathematical formula}ToM1 agent, a {a mathematical formula}ToM2 agent performs best when playing RPS against a {a mathematical formula}ToM1 opponent when both he and his opponent learn at a high speed, while the {a mathematical formula}ToM2 agent has more difficulty modeling a {a mathematical formula}ToM1 agent that learns slowly. This shows that application of higher-order theory of mind can benefit an agent when playing RPS. Performance of the {a mathematical formula}ToM2 agent playing RPS against a {a mathematical formula}ToM1 opponent is nonetheless slightly lower than that of a {a mathematical formula}ToM1 agent playing RPS against a {a mathematical formula}ToM0 opponent.</paragraph><paragraph>Figs. 6a and 6b suggest that application of higher orders of theory of mind benefits an agent. However, performance of a {a mathematical formula}ToM3 agent playing RPS against a {a mathematical formula}ToM2 agent, as shown in Fig. 6c, is poor in comparison. Although the {a mathematical formula}ToM3 agent still outperforms a {a mathematical formula}ToM2 opponent, he does so at a lower margin. The average score of the {a mathematical formula}ToM3 agent only exceeds 0.5 when his opponent has learning speed zero. When facing a {a mathematical formula}ToM2 opponent that has a low learning speed, the average score of a {a mathematical formula}ToM3 agent that learns quickly even becomes negative.</paragraph><paragraph>Although the {a mathematical formula}ToM3 agent can still outperform a {a mathematical formula}ToM2 opponent at a small margin, Fig. 6d shows that a {a mathematical formula}ToM4 agent no longer outperforms a {a mathematical formula}ToM3 opponent in RPS. In this scenario, the outcome of the game is mostly dependent on which of the agents has the highest learning speed, and no longer on theory of mind abilities. In Fig. 6d, this can be seen by the fact that the {a mathematical formula}ToM4 agent obtains a positive outcome on average only if his learning speed {a mathematical formula}λi is higher than the learning speed {a mathematical formula}λj of his opponent.</paragraph><paragraph>In summary, the ability to make use of theory of mind can benefit an agent in the game of RPS. As we hypothesized (cf. hypothesis {a mathematical formula}HRPS, Section 2.4), both the {a mathematical formula}ToM1 agent and the {a mathematical formula}ToM2 agent outperform opponents of a lower order of theory of mind. The performance of the {a mathematical formula}ToM3 agent and the {a mathematical formula}ToM4 agent suggests that there may be a limit to the effectiveness of application of higher orders of theory of mind. However, since rock–paper–scissors involves three possible opponent actions, the game leaves room for only three unique predictions of the opponentʼs next action. The low performance of the {a mathematical formula}ToM3 and {a mathematical formula}ToM4 agents may therefore be caused by specific characteristics of the RPS game, rather than a limit to the effectiveness of application of higher orders of theory of mind. The next section describes a game with more than three actions in order to differentiate between these alternative explanations.</paragraph></section><section label="5.2"><section-title>Elemental rock–paper–scissors</section-title><paragraph>In Section 2.1.2, we introduced elemental rock–paper–scissors, a variation on the classical RPS game in which agents choose from an action set of five actions. ERPS preserves the feature of RPS that each action is defeated by exactly one other action. Differences in performance of theory of mind agents that play RPS and those that play ERPS allow us to determine whether features of the game structure affect the effectiveness of higher orders of theory of mind in competitive games.</paragraph><paragraph>The results for ERPS are shown in Fig. 7. Our expectation that performance of theory of mind agents in playing ERPS would be at least as good as performance in RPS is only partially correct. Similar to our results of theory of mind agents playing RPS, Fig. 7a shows that a {a mathematical formula}ToM1 agent outperforms a {a mathematical formula}ToM0 opponent, while Fig. 7b shows that a {a mathematical formula}ToM2 agent outperforms a {a mathematical formula}ToM1 opponent as well. However, performance in the game of ERPS is slightly reduced compared to the situation in which they were playing RPS. Especially when either the agent or his opponent learns at a low speed, it is more difficult for a theory of mind agent to model his opponent in a game of ERPS than it is in RPS.</paragraph><paragraph>The main qualitative difference between RPS and ERSP is shown by the performance of the {a mathematical formula}ToM3 agent and performance of the {a mathematical formula}ToM4 agent, depicted in Fig. 7c. Our results in RPS showed that it is difficult for a {a mathematical formula}ToM3 agent to model his opponent correctly. In Fig. 6c, this presents itself as relatively low performance against an opponent with zero learning speed {a mathematical formula}λ=0. In contrast, Fig. 7c shows that the {a mathematical formula}ToM3 agent does not have this difficulty when playing ERPS against a similar opponent.</paragraph><paragraph>Since the richer action space of ERPS increases performance of the {a mathematical formula}ToM3 agent when playing against an opponent that does not learn, the structure of the game influences the effectiveness of theory of mind. However, performance of a {a mathematical formula}ToM3 agent playing ERPS against a {a mathematical formula}ToM2 opponent is still poor in comparison to performance of the {a mathematical formula}ToM1 and {a mathematical formula}ToM2 agents playing ERPS against opponents of a lower order of theory of mind. Although the {a mathematical formula}ToM1 and {a mathematical formula}ToM2 agents clearly outperform opponents of a lower order of theory of mind, the {a mathematical formula}ToM3 agent outperforms the {a mathematical formula}ToM2 agent at a small margin only.</paragraph><paragraph>Fig. 7d shows the performance of a {a mathematical formula}ToM4 agent playing ERPS against a {a mathematical formula}ToM3 opponent. Like the {a mathematical formula}ToM3 agent, the peak performance of the {a mathematical formula}ToM4 agent when playing against an opponent with learning speed {a mathematical formula}λj=0 shown in the figure indicates that the {a mathematical formula}ToM4 agent has no difficulty distinguishing agents that have learning speed zero from agents of a lower order of theory of mind. However, the ability to make use of fourth-order theory of mind does not present an agent with advantages in ERPS beyond those of third-order theory of mind. Fig. 7d shows that a {a mathematical formula}ToM4 agent that plays ERPS against a {a mathematical formula}ToM3 opponent only obtains a positive score on average if his learning speed {a mathematical formula}λi is higher than the learning speed {a mathematical formula}λj of his opponent. That is, when a {a mathematical formula}ToM4 agent plays ERPS against a {a mathematical formula}ToM3 opponent, whoever has the highest learning speed is expected to win.</paragraph><paragraph>In summary, we investigate the game of ERPS to determine whether the limited choice of actions for agents playing RPS had an effect on the advantage of making use of theory of mind. The results confirm our expectations (cf. hypothesis {a mathematical formula}HERPS, Section 2.4) that when agents choose from a limited action space, higher orders of theory of mind may experience difficulty modeling their opponent. However, the limited action space does not explain the relatively poor performance of a {a mathematical formula}ToM3 agent when playing against a {a mathematical formula}ToM2 opponent, which was found both in RPS and ERPS.</paragraph></section><section label="5.3"><section-title>Rock–paper–scissors–lizard–Spock</section-title><paragraph>The game of rock–paper–scissors–lizard–Spock, described in Section 2.1.3, is a variation on ERPS in which each action is defeated by exactly two other actions. As a result, the best response to each action is not unique. Our expectation was that it would be harder to predict an opponentʼs behaviour in this case, and that performance of theory of mind agents would be reduced. Fig. 8 shows that this is indeed generally the case. In the game of RPSLS, the advantage of making use of theory of mind is reduced compared to RPS and ERPS.</paragraph><paragraph>Fig. 8a shows the performance of a {a mathematical formula}ToM1 agent when playing RPSLS against a {a mathematical formula}ToM0 opponent. Unlike in RPS and ERPS, a {a mathematical formula}ToM1 agent performs better when his learning speed matches the learning speed of his opponent. In Fig. 8, this is reflected by high scores along the line of equal learning speeds {a mathematical formula}λi=λj. In this case, the {a mathematical formula}ToM1 agentʼs model of his opponentʼs beliefs matches her actual beliefs. However, even though modeling his opponentʼs beliefs correctly yields the agent a higher score, he is still expected to win in most cases when his learning speed does not match that of his opponent.</paragraph><paragraph>Performance of the {a mathematical formula}ToM1 agent is particularly low when his opponent has the maximum learning speed {a mathematical formula}λj=1. In this case, she only considers the agentʼs actions in the previous game, and ignores all information from previous games. For example, if the {a mathematical formula}ToM1 agent plays ‘paper’ in a game of RPSLS, the {a mathematical formula}ToM0 opponent will believe that he will repeat the same action in future games. This means that the {a mathematical formula}ToM0 opponent has two actions, ‘lizard’ or ‘scissors’, which maximize her expected payoff, and chooses either one of these two actions with 50% probability.</paragraph><paragraph>On the other hand, when the {a mathematical formula}ToM0 opponent learns at a lower speed, {a mathematical formula}λj&lt;1, she does not completely replace her beliefs when new information becomes available. In this case, the {a mathematical formula}ToM0 opponent believes that there is a small probability that the {a mathematical formula}ToM1 agent will play some action other than ‘paper’. In general, this prevents two actions from having exactly the same expected payoffs. Since agents choose the action that yields them the highest expected payoff, this causes the {a mathematical formula}ToM0 opponent to choose one of the possible actions with certainty.</paragraph><paragraph>As Fig. 8a shows, these two distinct types of behaviour make it more difficult for the {a mathematical formula}ToM1 agent to accurately model his opponent. In the present model, a {a mathematical formula}ToM1 agent that has a learning speed {a mathematical formula}λi&lt;1 believes that his opponent has the same learning speed. As a result, he believes that there is a single action that maximizes the opponentʼs expected payoff. However, when his opponent has the maximal learning speed {a mathematical formula}λj=1, she actually randomizes her choice over two possible actions. The {a mathematical formula}ToM1 agent is therefore expected to predict his opponentʼs behaviour incorrectly in half the cases.</paragraph><paragraph>Fig. 8b shows the performance of a {a mathematical formula}ToM2 agent when playing RPSLS against a {a mathematical formula}ToM1 opponent. Similar to the {a mathematical formula}ToM1 agent, performance of the {a mathematical formula}ToM2 agent is low when playing RPSLS against an opponent that learns at maximum speed, {a mathematical formula}λo=1. The {a mathematical formula}ToM2 agent also has particular difficulties modeling a {a mathematical formula}ToM1 opponent in RPSLS when his own learning speed {a mathematical formula}λi is low. In this case, the {a mathematical formula}ToM2 agent is outperformed by an opponent of lower order of theory of mind. However, the {a mathematical formula}ToM2 agent will on average win when his learning speed {a mathematical formula}λi is over 0.7.</paragraph><paragraph>The low performance of the {a mathematical formula}ToM2 agent in RPSLS when he learns at a low speed translates to a benefit for the {a mathematical formula}ToM3 agent. Fig. 8c shows the performance of the {a mathematical formula}ToM3 agent when playing RPSLS against a {a mathematical formula}ToM2 opponent. When his opponentʼs learning speed {a mathematical formula}λo is low, the {a mathematical formula}ToM3 agent performs better in RPSLS than he would have in the games of RPS and ERPS. However, the {a mathematical formula}ToM3 agent performs poorly when his {a mathematical formula}ToM2 opponent learns quickly enough. In particular, when facing a {a mathematical formula}ToM2 opponent that learns at the maximal learning speed {a mathematical formula}λj=1, the {a mathematical formula}ToM3 agent only obtains a positive score on average when he learns at the maximal learning speed {a mathematical formula}λi=1 as well.</paragraph><paragraph>Similar to the games of RPS and ERPS, performance of a {a mathematical formula}ToM4 agent playing RPSLS against a {a mathematical formula}ToM3 opponent is mostly determined by which player has the highest learning speed, as shown in Fig. 8d. However, unlike in RPS and ERPS, the {a mathematical formula}ToM4 agent is at a very small advantage over his {a mathematical formula}ToM3 opponent. That is, when the learning speed {a mathematical formula}λi of the {a mathematical formula}ToM4 agent and the learning speed {a mathematical formula}λj of his {a mathematical formula}ToM3 opponent are close together, the {a mathematical formula}ToM4 agent is expected to win more than predicted by chance performance.</paragraph><paragraph>In summary, our results from the game of RPSLS show that the effectiveness of theory of mind is strongly related to the predictability of lower-order agents. Theory of mind agents perform more poorly when their opponent is indifferent between two possible actions and her behaviour is less predictable. This confirms our expectations about the relationship between the performance of theory of mind agents and the predictability of their opponents (cf. hypothesis {a mathematical formula}HRPSLS, Section 2.4).</paragraph></section><section label="5.4"><section-title>Limited Bidding</section-title><paragraph>Unlike the variations on rock–paper–scissors, Limited Bidding is an extensive form game that spans several rounds. Although there is a unique best-response to each opponent action, there are multiple responses that yield a positive outcome. To determine the advantage of having the ability to explicitly represent mental states of others in the game of LB, agents that differ in their order of theory of mind have been placed in competition. Fig. 9 shows the performance of theory of mind agents as a function of the learning speed {a mathematical formula}λi of the focal agent and the learning speed {a mathematical formula}λj of his opponent. Performance has been normalized to range from 1, which means that the focal agent achieved the maximum possible payoff, to −1, in which case his opponent achieved the maximum possible payoff. As before, lighter areas highlight that the agent performed better than his opponent, while darker areas show that his opponent obtained a higher average score.</paragraph><paragraph>Fig. 9a shows that {a mathematical formula}ToM1 agents predominantly obtain a positive score when playing against {a mathematical formula}ToM0 opponents. A {a mathematical formula}ToM1 agent performs well when facing an opponent that does not learn, as shown by the high scores when the opponentʼs learning speed is zero ({a mathematical formula}λj=0). The bright area along the line of equal learning speeds indicates that the advantage of the {a mathematical formula}ToM1 agent is also particularly high when learning speeds are equal. In this case, the {a mathematical formula}ToM1 agentʼs implicit assumption that his opponent has the same learning speed as himself is correct. Fig. 9a shows that even when the {a mathematical formula}ToM1 agent fails to accurately model his opponent, he will on average obtain a positive score for any learning speed {a mathematical formula}λi&gt;0.08.</paragraph><paragraph>As for the cases of RPS and ERPS described above, applying theory of mind appears to be least effective when a {a mathematical formula}ToM1 agent is playing against a {a mathematical formula}ToM0 opponent that has a low learning speed. In LB, a {a mathematical formula}ToM0 opponent with high learning speed changes her beliefs radically, but with high confidence. That is, the effect of the random initialization of beliefs has less impact on opponent behaviour when her learning speed is high than when her learning speed is low. For a {a mathematical formula}ToM1 agent, a {a mathematical formula}ToM0 opponent with a high learning speed represents a more predictable situation, which he can use to his advantage.</paragraph><paragraph>Fig. 9b shows that a {a mathematical formula}ToM2 agent is at an advantage over a {a mathematical formula}ToM1 opponent. However, although Fig. 9b shows many of the same features as Fig. 9a, such as the brighter area along the main diagonal of equal learning speeds, {a mathematical formula}ToM2 agents playing against {a mathematical formula}ToM1 opponents obtain a score that is on average 0.13 lower than the score of {a mathematical formula}ToM1 agents playing against {a mathematical formula}ToM0 agents. As a result, a {a mathematical formula}ToM2 agent needs a higher learning speed of at least {a mathematical formula}λi&gt;0.12 in order to obtain, on average, a positive score when playing against a {a mathematical formula}ToM1 agent. Note that like a {a mathematical formula}ToM1 agent, a {a mathematical formula}ToM2 agent has more difficulty obtaining an advantage when playing against an opponent with low learning speed than when her learning speed is high.</paragraph><paragraph>Similar to the results found for the variations on RPS, the application of first-order and second-order theory of mind present an agent with a clear advantage over opponents of a lower order of theory of mind. However, the advantage of a {a mathematical formula}ToM3 agent over a {a mathematical formula}ToM2 opponent is only marginal. Fig. 9c shows that a {a mathematical formula}ToM3 agent barely outperforms a {a mathematical formula}ToM2 agent, with an average score that only exceeds 0.1 when the {a mathematical formula}ToM2 opponent has zero learning speed. Moreover, although it appears as if a {a mathematical formula}ToM3 agent can still on average obtain a positive score when his learning speed is at least {a mathematical formula}λi&gt;0.32, Fig. 9c shows that when the {a mathematical formula}ToM2 opponent has learning speed {a mathematical formula}0&lt;λj&lt;0.1, performance of the {a mathematical formula}ToM3 agent may still fall below the plane of zero performance. That is, a {a mathematical formula}ToM3 agent is no longer guaranteed to win when playing against a {a mathematical formula}ToM2 opponent for any value of his learning speed {a mathematical formula}λj.</paragraph><paragraph>Fig. 9d shows that a {a mathematical formula}ToM4 agent fails to obtain an advantage of any kind over a {a mathematical formula}ToM3 agent when playing LB. When neither the agent nor his opponent learns at a low speed, the game will, on average, end in a tie. The learning speed of the agent and the learning speed of his opponent do not have a strong effect on the expected outcome of the game.</paragraph><paragraph>In summary, agent performance in LB clearly shows diminishing returns on higher orders of theory of mind. The use of first-order and second-order theory of mind allows agents to obtain a reliable advantage over opponents that are more limited in their ability to explicitly represent mental states of others. However, a specialized system for third-order theory of mind barely allows {a mathematical formula}ToM3 agents to outperform {a mathematical formula}ToM2 agents, while a fourth-order theory of mind does not yield an agent any advantage that could not have been obtained with a third-order theory of mind. Qualitatively, the results are similar to those described for the RPS game in Section 5.1.</paragraph></section><section label="5.5"><section-title>Summary of results</section-title><paragraph>To determine the effectiveness of theory of mind, we simulated computational theory of mind agents, as described in Section 4, playing competitive games against one another. In hypothesis {a mathematical formula}HRPS (Section 2.4), we predicted that higher orders of theory of mind would benefit agents in competitive settings. Our results support this conclusion in the sense that the ability to make use of first-order and second-order theory of mind allows agents to obtain a clear advantage over opponents of a lower order of theory of mind. However, for orders of theory of mind beyond the second, the additional advantage is marginal.</paragraph><paragraph>This pattern of results was consistent across the variations on rock–paper–scissors we investigated. As we predicted in hypothesis {a mathematical formula}HERPS, the larger action space of elemental rock–paper–scissors was advantageous for higher-order theory of mind agents in some instances. However, the larger action space did not remove the diminishing returns on higher orders of theory of mind. Qualitatively similar results were found for the multi-stage limited bidding game, which confirms hypothesis {a mathematical formula}HLB.</paragraph><paragraph>The relatively limited advantage of {a mathematical formula}ToM3 agents playing against {a mathematical formula}ToM2 opponents appears to be caused by the model that the {a mathematical formula}ToM2 opponent holds of the {a mathematical formula}ToM3 agent. Agents start out by playing as if they were {a mathematical formula}ToM0 agents. When a {a mathematical formula}ToM3 agent is in competition with a {a mathematical formula}ToM2 opponent, both of them will notice that their predictions based on first-order theory of mind are correct. This causes both agents to grow more confident in application of first-order theory of mind. As a result, they both gradually start to play more as if they were {a mathematical formula}ToM1 agents. When this happens, predictions based on first-order theory of mind will become less accurate, but predictions based on second-order theory of mind become increasingly accurate, increasing confidence in the application of second-order theory of mind. Both the agent and his opponent will therefore start playing as if they were {a mathematical formula}ToM2 agents. At this point, the {a mathematical formula}ToM2 opponent can no longer model the behaviour of the agent. That is, she will notice that none of her predictions are correct. Because of this, she will lose confidence in the application of both first-order and second-order theory of mind, and gradually start to play as if she were a {a mathematical formula}ToM0 agent again. When the {a mathematical formula}ToM3 agent tries to take advantage of this by playing as if he were a {a mathematical formula}ToM1 agent, the {a mathematical formula}ToM2 opponent is once again able to recognize this behaviour, and she will grow more confident in her predictions based on second-order theory of mind again. This causes the {a mathematical formula}ToM2 opponent to constantly keep changing her strategy, which hinders the {a mathematical formula}ToM3 agent in his efforts of trying to model her behaviour.</paragraph><paragraph>The relation between the performance of a theory of mind agent and the predictability of his opponentʼs behaviour is also reflected in the results of the rock–paper–scissors–lizard–Spock game. As predicted in hypothesis {a mathematical formula}HRPSLS, higher-order theory of mind agents perform more poorly in this game than in RPS and ERPS.</paragraph></section></section><section label="6"><section-title>Discussion and conclusion</section-title><paragraph>The Machiavellian intelligence hypothesis [38] on the evolution of theory of mind predicts that there are competitive settings in which the use of higher-order theory of mind presents individuals with an evolutionary advantage. But the benefits of making use of higher-order theory of mind may not always outweigh the costs. For example, in settings in which a pure-strategy Nash equilibrium exists, individuals that make use of theory of mind are unlikely to outperform individuals that play the Nash strategy without explicitly reasoning about their opponentʼs mental states. In other cases, simple heuristics may be superior to methods that rely on sophisticated cognitive abilities like theory of mind [70], [71]. However, humans possess the ability to make use of higher-order theory of mind, which suggests that there may be settings in which this cognitively demanding skill is useful. For example, in using secret codes or negotiating climate change control, heuristics alone may not be enough.</paragraph><paragraph>In this paper, we have used agent-based models to show how the ability to make use of theory of mind can present individuals with an advantage over opponents that lack such an ability in certain competitive settings. The advantage was found to be qualitatively similar across the four competitive games we discussed, which included repeated single-shot games rock–paper–scissors, elemental rock–paper–scissors, rock–paper–scissors–lizard–Spock, and the repeated extensive form game limited bidding.</paragraph><paragraph>To our surprise, the results show diminishing returns on higher orders of theory of mind. Although both first-order and second-order theory of mind agents clearly outperform opponents that are more limited in their abilities to represent mental content of others, third-order theory of mind agents only marginally outperform second-order theory of mind opponents. Fourth-order theory of mind was only found to be beneficial under specific circumstances. These diminishing returns on higher orders of theory of mind were found not to be related to the number of actions available to the agents. Increasing the action space from which agents choose did not increase performance of a third-order theory of mind agent in competition with a second-order theory of mind opponent.</paragraph><paragraph>Although theory of mind allows agents to outperform opponents that are more limited in their ability to explicitly represent mental states, theory of mind may not always be an efficient use of memory capacity. Additional experiments show that in simple games such as rock–paper–scissors, an agent seems to benefit more from remembering past behaviour of his opponent rather than representing her mental states. However, for more complex games such as Limited Bidding, theory of mind appears to have benefits that go beyond remembering past opponent behaviour. Agents that are capable of both associative learning strategies and theory of mind strategies may therefore choose not to use their theory of mind when the task is simple. Tasks may need to be sufficiently complex to elicit a theory of mind response.</paragraph><paragraph>In our model, we have assumed that agents choose what action to perform rationally. That is, agents choose to perform the action that they believe to yield them the highest possible payoff. This results in a predictability that benefits theory of mind agents, as shown by our results in the game of rock–paper–scissors–lizard–Spock. When an opponent is indifferent between two actions in the sense that both actions maximize the expected payoff, the effectiveness of theory of mind suffers. However, when there is a slight asymmetry between the two actions, such that one action appears to be a slightly better alternative than the other, this creates a focal point [72] for agents. In this case, the opponent will choose the action that she believes to yield the better payoff. However, this behaviour can be predicted by higher-order theory of mind agents.</paragraph><paragraph>An agent of a lower order of theory of mind may therefore be able to avoid falling victim to an opponent capable of theory of mind of a higher order when he does not choose what action to play completely rationally. For example, agents could choose the action to perform with a probability proportional to the expected payoff. Similarly, utility proportional beliefs [73] may benefit the effectiveness of theory of mind agents, through the belief that opponents choose an action proportionally to its utility. In this case, the theory of mind agent is less reliant on his opponent playing completely rationally. Future research may reveal how a balance can be achieved between exploiting weaknesses in the opponentʼs actions, while remaining unpredictable enough to avoid exploitation.</paragraph><paragraph>In our model, a zero-order theory of mind agent does not believe that his opponent behaves randomly [10], [11], but attempts to model the opponentʼs behaviour by assuming her past actions predict what she will do in the future. A higher-order theory of mind agent therefore simultaneously updates his model of the mental content of the opponent and his belief about the opponentʼs theory of mind abilities. It would be interesting to compare the effectiveness of theory of mind in direct competition with more classical strategies and heuristics.</paragraph><paragraph>In future work, we aim to investigate whether theory of mind is effective in more complex interaction settings including various partners as well. Theory of mind may play an important role in cooperative settings, for example in teamwork, as well as mixed-motive settings such as negotiations (cf. [37]). This may provide further insights for automated agents that share their environment with human agents, such as in automated negotiation [3], [4].</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>This work was supported by the Netherlands Organisation for Scientific Research (NWO) Vici grant NWO 277-80-001, awarded to Rineke Verbrugge for the project ‘Cognitive systems in interaction: Logical and computational models of higher-order social cognition’. We would like to thank the three anonymous reviewers for their helpful comments.</paragraph></acknowledgements><references><reference label="[1]"><authors>H. de Weerd,B. Verheij</authors><title>The advantage of higher-order theory of mind in the game of limited bidding</title><host>J. van EijckR. VerbruggeProc. Workshop Reason. Other Minds: Log. Cogn. Perspect., CEUR Workshop Proceedings(2011) pp.149-164</host></reference><reference label="[2]"><authors>H. de Weerd,R. Verbrugge,B. Verheij</authors><title>Higher-order social cognition in the game of rock–paper–scissors: A simulation study</title><host>G. BonannoH. van DitmarschW. van der HoekProc. 10th Conf. Log. Found. Game Decis. Theory(2012) pp.218-232</host></reference><reference label="[3]"><authors>S. Kraus</authors><title>Negotiation and cooperation in multi-agent environments</title><host>Artif. Intell.94 (1997) pp.79-97</host></reference><reference label="[4]"><authors>R. Lin,S. Kraus,J. Wilkenfeld,J. Barry</authors><title>Negotiating with bounded rational agents in environments with incomplete information using an automated agent</title><host>Artif. Intell.172 (2008) pp.823-851</host></reference><reference label="[5]">R. Fagin,J. Halpern,Y. Moses,M. VardiReasoning About Knowledge(1995)MIT PressCambridge, MAsecond edition 2003</reference><reference label="[6]"><authors>H. van Ditmarsch,W. van der Hoek,B. Kooi</authors><title>Dynamic Epistemic Logic</title><host>(2007)Springer</host></reference><reference label="[7]"><authors>P. Gmytrasiewicz,E. Durfee</authors><title>A rigorous, operational formalization of recursive modeling</title><host>Proc. First Int. Conf. on Multi-Agent Syst.(1995) pp.125-132</host></reference><reference label="[8]"><authors>P. Gmytrasiewicz,P. Doshi</authors><title>A framework for sequential planning in multiagent settings</title><host>J. Artif. Intell. Res.24 (2005) pp.49-79</host></reference><reference label="[9]"><authors>A. Pfeffer</authors><title>Networks of influence diagrams: A formalism for representing agentsʼ beliefs and decision-making processes</title><host>J. Artif. Intell. Res.33 (2008) pp.109-147</host></reference><reference label="[10]"><authors>W. Yoshida,R. Dolan,K. Friston</authors><title>Game theory of mind</title><host>PLoS Comput. Biol.4 (2008) pp.e1000254-</host></reference><reference label="[11]"><authors>C. Camerer,T. Ho,J. Chong</authors><title>A cognitive hierarchy model of games</title><host>Q. J. Econ.119 (2004) pp.861-898</host></reference><reference label="[12]"><authors>D. Stahl,P. Wilson</authors><title>On playersʼ models of other players: Theory and experimental evidence</title><host>Games Econ. Behav.10 (1995) pp.218-254</host></reference><reference label="[13]"><authors>M. Bacharach,D.O. Stahl</authors><title>Variable-frame level-n theory</title><host>Games Econ. Behav.32 (2000) pp.220-246</host></reference><reference label="[14]"><authors>H. Simon</authors><title>A mechanism for social selection and successful altruism</title><host>Science250 (1990) pp.1665-1668</host></reference><reference label="[15]"><authors>D. Kahneman</authors><title>Maps of bounded rationality: Psychology for behavioral economics</title><host>Am. Econ. Rev. (2003) pp.1449-1475</host></reference><reference label="[16]"><authors>D. Premack,G. Woodruff</authors><title>Does the chimpanzee have a theory of mind?</title><host>Behav. Brain Sci.1 (1978) pp.515-526</host></reference><reference label="[17]"><authors>J. Perner,H. Wimmer</authors><title>“John thinks that Mary thinks that…”. Attribution of second-order beliefs by 5 to 10 year old children</title><host>J. Exp. Child Psychol.39 (1985) pp.437-471</host></reference><reference label="[18]"><authors>T. Hedden,J. Zhang</authors><title>What do you think I think you think?: Strategic reasoning in matrix games</title><host>Cognition85 (2002) pp.1-36</host></reference><reference label="[19]"><authors>L. Flobbe,R. Verbrugge,P. Hendriks,I. Krämer</authors><title>Childrenʼs application of theory of mind in reasoning and language</title><host>J. Log. Lang. Inf.17 (2008) pp.417-442</host></reference><reference label="[20]"><authors>B. Meijering,H. van Rijn,N. Taatgen,R. Verbrugge</authors><title>I do know what you think I think: Second-order theory of mind in strategic games is not that difficult</title><host>Proc. 33rd Annu. Conf. Cogn. Sci. Soc.(2011) pp.2486-2491</host></reference><reference label="[21]"><authors>V. Crawford,N. Iriberri</authors><title>Fatal attraction: Salience, naïvete, and sophistication in experimental “Hide-and-Seek” games</title><host>Am. Econ. Rev. (2007) pp.1731-1750</host></reference><reference label="[22]"><authors>H. Wimmer,J. Perner</authors><title>Beliefs about beliefs: Representation and constraining function of wrong beliefs in young childrenʼs understanding of deception</title><host>Cognition13 (1983) pp.103-128</host></reference><reference label="[23]"><authors>I. Apperly</authors><title>Mindreaders: The Cognitive Basis of “Theory of Mind”</title><host>(2011)Psychology PressHove, UK</host></reference><reference label="[24]"><authors>B. Meijering,L. Van Maanen,H. Van Rijn,R. Verbrugge</authors><title>The facilitative effect of context on second-order social reasoning</title><host>Proc. 32nd Annu. Conf. Cogn. Sci. Soc.(2010) pp.1423-1429</host></reference><reference label="[25]"><authors>M. Tomasello</authors><title>Why We Cooperate</title><host>(2009)MIT PressCambridge, MA</host></reference><reference label="[26]"><authors>M. Schmelz,J. Call,M. Tomasello</authors><title>Chimpanzees know that others make inferences</title><host>Proc. Natl. Acad. Sci. USA108 (2011) pp.3077-3079</host></reference><reference label="[27]"><authors>J. Burkart,A. Heschl</authors><title>Understanding visual access in common marmosets, Callithrix jacchus: Perspective taking or behaviour reading?</title><host>Anim. Behav.73 (2007) pp.457-469</host></reference><reference label="[28]"><authors>J. Kaminski,J. Call,M. Tomasello</authors><title>Goatsʼ behaviour in a competitive food paradigm: Evidence for perspective taking?</title><host>Behaviour143 (2006) pp.1341-1356</host></reference><reference label="[29]"><authors>J. Kaminski,J. Brauer,J. Call,M. Tomasello</authors><title>Domestic dogs are sensitive to a humanʼs perspective</title><host>Behaviour146 (2009) pp.979-998</host></reference><reference label="[30]"><authors>N. Clayton,J. Dally,N. Emery</authors><title>Social cognition by food-caching corvids. The western scrub–jay as a natural psychologist</title><host>Philos. Trans. R. Soc. B, Biol. Sci.362 (2007) pp.507-</host></reference><reference label="[31]"><authors>T. Bugnyar</authors><title>Knower-guesser differentiation in ravens: Othersʼ viewpoints matter</title><host>Proc. R. Soc. B, Biol. Sci.278 (2011) pp.634-640</host></reference><reference label="[32]"><authors>D. Penn,D. Povinelli</authors><title>On the lack of evidence that non-human animals possess anything remotely resembling a ‘theory of mind’</title><host>Philos. Trans. R. Soc. B, Biol. Sci.362 (2007) pp.731-</host></reference><reference label="[33]"><authors>P. Carruthers</authors><title>Meta-cognition in animals: A skeptical look</title><host>Mind Lang.23 (2008) pp.58-89</host></reference><reference label="[34]"><authors>E. van der Vaart,R. Verbrugge,C. Hemelrijk</authors><title>Corvid re-caching without ‘theory of mind’: A model</title><host>PLoS ONE7 (2012) pp.e32904-</host></reference><reference label="[35]"><authors>M. Balter</authors><title>‘Killjoys’ challenge claims of clever animals</title><host>Science335 (2012) pp.1036-1037</host></reference><reference label="[36]"><authors>B. Hare,J. Call,M. Tomasello</authors><title>Do chimpanzees know what conspecifics know?</title><host>Anim. Behav.61 (2001) pp.139-151</host></reference><reference label="[37]"><authors>R. Verbrugge</authors><title>Logic and social cognition: The facts matter, and so do computational models</title><host>J. Philos. Log.38 (2009) pp.649-680</host></reference><reference label="[38]"><authors>A. Whiten,R. Byrne</authors><title>Machiavellian Intelligence II: Extensions and Evaluations</title><host>(1997)Cambridge University PressCambridge</host></reference><reference label="[39]"><authors>J. Epstein</authors><title>Generative Social Science: Studies in Agent-based Computational Modeling</title><host>(2006)Princeton University PressPrinceton, NJ</host></reference><reference label="[40]"><authors>J. Epstein</authors><title>Agent-based computational models and generative social science</title><host>Complexity4 (1999) pp.41-60</host></reference><reference label="[41]"><authors>W. Jager,R. Popping,H. Van de Sande</authors><title>Clustering and fighting in two-party crowds: Simulating the approach-avoidance conflict</title><host>J. Artif. Soc. Soc. Simul.4 (2001) pp.1-18</host></reference><reference label="[42]"><authors>M. Harbers,R. Verbrugge,C. Sierra,J. Debenham</authors><title>The examination of an information-based approach to trust</title><host>Coord., Organ., Inst., and Norms in Agent Syst. III(2008) pp.71-82</host></reference><reference label="[43]"><authors>E. van der Vaart,B. de Boer,A. Hankel,B. Verheij</authors><title>Agents adopting agriculture: Modeling the agricultural transition</title><host>Proc. 9th Int. Conf. from Anim. to Animats: Simul. Adapt. Behav.(2006) pp.750-761</host></reference><reference label="[44]"><authors>H. Gintis</authors><title>Strong reciprocity and human sociality</title><host>J. Theor. Biol.206 (2000) pp.169-179</host></reference><reference label="[45]"><authors>R. Boyd,H. Gintis,S. Bowles,P. Richerson</authors><title>The evolution of altruistic punishment</title><host>Proc. Natl. Acad. Sci.100 (2003) pp.3531-3535</host></reference><reference label="[46]"><authors>H. de Weerd,R. Verbrugge</authors><title>Evolution of altruistic punishment in heterogeneous populations</title><host>J. Theor. Biol.290 (2011) pp.88-103</host></reference><reference label="[47]"><authors>A. Cangelosi,D. Parisi</authors><title>Simulating the Evolution of Language</title><host>(2002)Springer</host></reference><reference label="[48]"><authors>B. de Boer</authors><title>The Origins of Vowel Systems</title><host>(2001)Oxford University PressUSA</host></reference><reference label="[49]"><authors>I. Slingerland,M. Mulder,E. van der Vaart,R. Verbrugge</authors><title>A multi-agent systems approach to gossip and the evolution of language</title><host>Proc. 31st Annu. Meet. Cogn. Sci. Soc.(2009) pp.1609-1614</host></reference><reference label="[50]"><authors>D. Billings</authors><title>The first international RoShamBo programming competition</title><host>ICGA J.23 (2000) pp.42-50</host></reference><reference label="[51]"><authors>D. Egnor</authors><title>Iocaine powder</title><host>ICGA J.23 (2000) pp.33-35</host></reference><reference label="[52]"><authors>J. Von Neumann</authors><title>Zur Theorie der Gesellschaftsspiele</title><host>Math. Ann.100 (1928) pp.295-320</host></reference><reference label="[53]"><authors>K. Binmore</authors><title>Playing for Real</title><host>(2007)Oxford University PressOxford, UK</host></reference><reference label="[54]"><authors>W. Wagenaar</authors><title>Generation of random sequences by human subjects: A critical survey of literature</title><host>Psychol. Bull.77 (1972) pp.65-</host></reference><reference label="[55]"><authors>A. Rapoport,D. Budescu</authors><title>Randomization in individual choice behavior</title><host>Psychol. Rev.104 (1997) pp.603-</host></reference><reference label="[56]"><authors>R. West,C. Lebiere,D. Bothell</authors><title>Cognitive architectures, game playing, and human evolution</title><host>Cognition and Multi-Agent Interaction: From Cognitive Modeling to Social Simulation(2006)Cambridge University Press pp.103-123</host></reference><reference label="[57]"><authors>R. Cook,G. Bird,G. Lünser,S. Huck,C. Heyes</authors><title>Automatic imitation in a strategic context: Players of rock–paper–scissors imitate opponentsʼ gestures</title><host>Proc. R. Soc. B, Biol. Sci. (2011)</host></reference><reference label="[58]">S. Kass,K. BrylaRock paper scissors Spock lizardhttp://www.samkass.com/theories/RPSSL.html(2009)accessed 29/12/2012</reference><reference label="[59]"><authors>E. De Bono</authors><title>Edward de Bonoʼs Super Mind Pack: Expand Your Thinking Powers with Strategic Games &amp; Mental Exercises</title><host>(1998)Dorling Kindersley Publishers Ltd.London, UK</host></reference><reference label="[60]"><authors>M. Osborne,A. Rubinstein</authors><title>A Course in Game Theory</title><host>(1994)MIT PressCambridge, MA</host></reference><reference label="[61]"><authors>C. Bicchieri</authors><title>Common knowledge and backward induction: A solution to the paradox</title><host>Proc. 2nd Conf. Theor. Asp. Reason. Knowl.(1988) pp.381-393</host></reference><reference label="[62]">J. Von Neumann,O. MorgensternTheory of Games and Economic Behavior(1944)Princeton University PressPrinceton, NJcommemorative edition, 2007</reference><reference label="[63]"><authors>M. Davies</authors><title>The mental simulation debate</title><host>Philos. Issues5 (1994) pp.189-218</host></reference><reference label="[64]"><authors>S. Nichols,S. Stich</authors><title>Mindreading: An Integrated Account of Pretence, Self-Awareness, and Understanding Other Minds</title><host>(2003)Oxford University PressUSA</host></reference><reference label="[65]"><authors>S. Hurley</authors><title>The shared circuits model (SCM): How control, mirroring, and simulation can enable imitation, deliberation, and mindreading</title><host>Behav. Brain Sci.31 (2008) pp.1-22</host></reference><reference label="[66]"><authors>R. Falk,C. Konold</authors><title>Making sense of randomness: Implicit encoding as a basis for judgment</title><host>Psychol. Rev.104 (1997) pp.301-</host></reference><reference label="[67]"><authors>J. Barwise</authors><title>On the model theory of common knowledge</title><host>The Situation in Logic(1989)CSLI PressStanford, CA pp.201-220</host></reference><reference label="[68]"><authors>H. van Ditmarsch,J. van Eijck,R. Verbrugge</authors><title>Common knowledge and common belief</title><host>J. van EijckR. VerbruggeDiscourses on Social Software(2009)Amsterdam University PressAmsterdam pp.99-122</host></reference><reference label="[69]"><authors>R. Brown</authors><title>Smoothing, Forecasting and Prediction of Discrete Time Series</title><host>(1963)Prentice–HallEnglewood Cliffs, NJ</host></reference><reference label="[70]"><authors>G. Gigerenzer,R. Hertwig,T. Pachur</authors><title>Heuristics: The Foundations of Adaptive Behavior</title><host>(2011)Oxford University Press</host></reference><reference label="[71]"><authors>D. Kahneman</authors><title>Thinking, Fast and Slow</title><host>(2011)Farrar, Straus and GirouxNew York</host></reference><reference label="[72]"><authors>R. Sugden</authors><title>A theory of focal points</title><host>Econ. J.105 (1995) pp.533-550</host></reference><reference label="[73]">C. Bach,A. PereaUtility proportional beliefshttp://epicenter.name/Research.html(2011)accessed 29/12/2012</reference></references><footnote><note-para label="1">This research is a continuation of [1], [2].</note-para><note-para label="2">For a discussion of alternative hypotheses, see [37].</note-para><note-para label="3">The payoffs in elemental rock–paper–scissors are based on the overcoming cycle of elements in the Chinese philosophy Wu Xing.</note-para><note-para label="4">Limited Bidding is an adaptation of a game presented in [59].</note-para><note-para label="5">Results from additional simulations using different values of {a mathematical formula}c1∈[0,1] turned out to be visually indistinguishable from the ones presented here for any value of {a mathematical formula}c1 over 0.5.</note-para><note-para label="6">We have compared the results for trials of 20 games to longer trials of 50 and 100 games and found no qualitative differences.</note-para></footnote></root>