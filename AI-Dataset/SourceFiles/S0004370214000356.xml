<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370214000356</url><title>On our best behaviour</title><authors>Hector J. Levesque</authors><abstract>The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.</abstract><keywords>IJCAI Research Excellence</keywords><content><section label="1"><section-title>Intelligent behaviour</section-title><paragraph>This paper{sup:1} is about the science of AI. Unfortunately, it is the technology of AI that gets all the attention. The general public could be forgiven for thinking that AI is just about all those whiz-bang applications, smart this and autonomous that. Those of us in the field know that for many applications, the term “intelligent” is no more than a buzzword (like the term “delicious” in “red delicious apples”). And along with the many possibly beneficial AI applications under consideration, we often have serious misgivings about the potential misuse of AI technology (in areas like weaponry).</paragraph><paragraph>But AI is more than just technology. Many of us are motivated not by any of the AI applications currently being considered, but by the scientific enterprise, the attempt to understand the world around us. Different sciences have different subject matters, and AI is the study of intelligent behaviour in computational terms. What could be more fascinating? The human brain is a remarkable thing, perhaps the single most complex object we know of in the universe. But even more remarkable is what a human brain is capable of doing. Our intelligent behaviour at its best goes well beyond what we have any right to expect to emerge out of purely physical matter. Indeed, the overarching question for the science of AI is: How is it possible for something physical (like people, for instance) to actually do X? where X is one of the many instances of intelligent behaviour. This needs to be contrasted with a related question: Can we engineer a computer system to do something that is vaguely X-ish? about which we will have much more to say later.</paragraph><paragraph>Note that the science of AI studies intelligent behaviour, not who or what is producing the behaviour. It studies natural language understanding, for instance, not natural language understanders. This is what makes AI quite different from the study of people (in neuroscience, psychology, cognitive science, evolutionary biology, and so on).</paragraph><paragraph>What sort of behaviour do we care about? Different researchers will quite naturally focus on different aspects. The behaviour may or may not depend on perceptual or motor skills. It may or may not include learning. It may or may not be grounded in emotional responses, or in social interactions. For some researchers, the main concern is intelligent behaviour seen in a variety of animals, like the ability to find a desired object in a room. For others, the focus is on behaviour seen in humans only, like the ability to play chess. (These two groups sometimes engage in methodological disputes, with the former arguing that we cannot expect to understand human behaviour until we understand its more basic forms, and the latter responding that this is not how science works at all. At this stage of the game, there is really no reason to take a doctrinaire position one way or another.)</paragraph><section label="1.1"><section-title>Answering questions</section-title><paragraph>In this paper, I intend to examine one basic form of intelligent behaviour: answering certain ad-hoc questions posed in English. Consider a question like the following: Could a crocodile run a steeplechase? Even if you know what crocodiles and steeplechases are,{sup:2} you have never really thought about this question before, unless you happened to have read an early paper of mine [6]. Nor can you simply look up the correct answer somewhere. And yet, an answer does occur to you almost immediately. Here is another question from the same paper: Should baseball players be allowed to glue small wings onto their caps? Again, you have never thought of this before, but again an answer occurs to you. (In this case, you might even wonder if there is some sort of trick to the question that you may have missed. There is none.)</paragraph><paragraph>In this paper, I want to consider our ability to answer one-shot questions like these, and for four reasons:</paragraph><list><list-item label="1.">This is behaviour that is clearly exhibited by people. We are indeed capable of answering questions like these without any special training or instructions.</list-item><list-item label="2.">This is behaviour that is difficult to crack. We have as yet no good idea about what people do to answer them. No existing computer program can duplicate our ability.</list-item><list-item label="3.">Our behaviour in answering questions like these appears to underly other more complex (and more ecologically significant) forms of behaviour.</list-item><list-item label="4.">Being clear and precise about the form of behaviour we care about even in this simple case will also help clarify what it means for the science of AI to be successful.</list-item></list><paragraph> As we will see, however, there will be good reasons to move to answering questions of a more restricted form.</paragraph></section></section><section label="2"><section-title>Behavioural tests</section-title><paragraph>Given some form of intelligent behaviour, how do we know that the computational story told by AI researchers actually explains the behaviour? The answer, going all the way back to Turing, is this: a computational account is adequate if it is able to generate behaviour that cannot be distinguished over the long haul from the behaviour produced by people.</paragraph><paragraph>This, of course, harks back to the famous Turing Test [12]. We imagine an extended conversation over a teletype between an interrogator and two participants, a person and a computer. The conversation is natural, free-flowing, and about any topic whatsoever. The computer is said to pass the Turing Test if no matter how long the conversation, the interrogator cannot tell which of the two participants is the person.</paragraph><paragraph>Turing's point in all this, it seems to me, is this: Terms like “intelligent,” “thinking,” “understanding,” and the like are much too vague and emotionally charged to be worth arguing about. If we insist on using them in a scientific context at all, we should be willing to say that a program that can pass a suitable behavioural test has the property in question as much as the person. Adapting the dictum of the movie character Forest Gump who said “Stupid is as stupid does,” we can imagine Turing saying “Intelligent is as intelligent does.” This is a very sensible position, it seems to me, and I have defended it elsewhere [7].</paragraph><section label="2.1"><section-title>The trouble with the Turing Test</section-title><paragraph>However, I do feel that the Turing Test has a serious problem: it relies too much on deception. A computer program passes the test iff it can fool an interrogator into thinking she is dealing with a person not a computer. Consider the interrogator asking questions like these: How tall are you? or Tell me about your parents. To pass the test, a program will either have to be evasive (and duck the question) or manufacture some sort of false identity (and be prepared to lie convincingly). In fact, evasiveness is seen quite clearly in the annual Loebner Competition, a restricted version of the Turing Test.{sup:3} The “chatterbots” (as the computer entrants in the competition are called) rely heavily on wordplay, jokes, quotations, asides, emotional outbursts, points of order, and so on. Everything, it would appear, except clear and direct answers to questions!</paragraph><paragraph>The ability to fool people is interesting, no doubt, but not really what is at issue here.{sup:4} We might well ask: is there a better behaviour test than having a free-form conversation?</paragraph><paragraph>There are some quite reasonable non-English options to consider, such as “captchas” [13] and the program at www.areyouhuman.com. But English is an excellent medium since it allows us to range over topics broadly and flexibly (and guard for biases: age, education, culture, etc.).</paragraph><paragraph>But here is another option: what if instead of a conversation, the interrogator only asks a number of multiple-choice questions? This has some distinct advantages:</paragraph><list><list-item label="•">Verbal dodges are no longer possible. A program can no longer game the test using evasive manoeuvres.</list-item><list-item label="•">It does not require the ability to generate “credible” English. The program will not need to worry about choosing words or syntax to accurately mimic actual speakers.</list-item><list-item label="•">The tests can be automated (administered and graded by machine). Success on the test does not depend on the judged similarity to people, but on the correctness of the answers.</list-item></list></section><section label="2.2"><section-title>Cheap tricks</section-title><paragraph>We want multiple-choice questions that people can answer easily. But we also want to avoid as much as possible questions that can be answered using cheap tricks (aka heuristics).</paragraph><paragraph>Consider for example, the question posed earlier:{a mathematical formula} The intent here is clear. The question can be answered by thinking it through: a crocodile has short legs; the hedges in a steeplechase would be too tall for the crocodile to jump over; so no, a crocodile cannot run a steeplechase.</paragraph><paragraph>The trouble is that there is another way to answer the question that does not require this level of understanding. The idea is to use the closed world assumption[10], [3]. This assumption says (among other things) the following: If you can find no evidence for the existence of something, assume that it does not exist. For the question above, since I have never heard of a crocodile being able to run a steeplechase, I conclude that it cannot. End of story. Note that this is a cheap trick: it gets the answer right, but for dubious reasons. It would produce the wrong answer for a question about gazelles, for example. Nonetheless, if all we care about is answering the crocodile question correctly, then this cheap trick does the trick.</paragraph><paragraph>Can we find questions where cheap tricks like this will not be sufficient to produce the desired behaviour? This unfortunately has no easy answer. The best we can do, perhaps, is to come up with a suite of multiple-choice questions carefully and then study the sorts of computer programs that might be able to answer them. Here are some obvious guidelines:</paragraph><list><list-item label="•">Make the questions Google-proof. Access to a large corpus of English text data should not by itself be sufficient.</list-item><list-item label="•">Avoid questions with common patterns. An example is “Is x older than y?” Perhaps no single Google-accessible web page has the answer, but once we map the word “older” to “birth date,” the rest comes quickly.{sup:5}</list-item><list-item label="•">Watch for unintended bias. The word order, vocabulary, grammar and so on all need to be selected very carefully not to betray the desired answer.</list-item></list><paragraph> One existing promising approach in this direction is the recognising textual entailment challenge [4], [1], and another is CORA[11]. But these have problems of their own,{sup:6} and so here we propose a different one.</paragraph></section></section><section label="3"><section-title>Winograd schema questions</section-title><paragraph>Our approach is best illustrated with an example question:{a mathematical formula} A Winograd schema question is a binary-choice question with these properties:</paragraph><list><list-item label="•">Two parties are mentioned in the question (both are males, females, objects, or groups).</list-item><list-item label="•">A pronoun is used to refer to one of them (“he,” “she,” “it,” or “they,” according to the parties).</list-item><list-item label="•">The question is always the same: what is the referent of the pronoun?</list-item><list-item label="•">Behind the scenes, there are two special words for the schema. There is a slot in the schema that can be filled by either word. The correct answer depends on which special word is chosen.</list-item></list><paragraph> In the above, the special word used is “given,” and the other word is “received.” So each Winograd schema actually generates two very similar questions:{a mathematical formula} and{a mathematical formula} It is this one-word difference between the two questions that helps guard against using the cheapest of tricks on them.</paragraph><paragraph>Here are some additional examples. The first is one that is suitable even for young children:{a mathematical formula} In this case, the special word used is “small” and the other word is “big.” Here is the original example due to Terry Winograd [15] for whom the schema is named:{a mathematical formula} Here the special word is “feared” and the alternative word is “advocated.”</paragraph><paragraph>With a bit of care, it is possible to come up with Winograd schema questions that exercise different kinds of expertise. Here is an example concerning certain materials:{a mathematical formula} The special word is “styrofoam” and the alternative is “steel.” This one tests for problem-solving skill:{a mathematical formula} The special word is “below” and the alternative is “above.” This example tests for an ability to visualise:{a mathematical formula} The special word used is “golfers” and the other is “dogs.”</paragraph><paragraph>Of course not just any question in this form will do the job here. It is possible to construct questions that are too “easy,” like this one:{a mathematical formula} The problem is that this question can be answered using the following trick: ignore the given sentence, and check which two words co-occur more frequently (according to Google, say): “racecar” with “fast” or “school bus” with “fast.” Questions can also be too “hard,” like this one:{a mathematical formula} The problem is that this question is ambiguous when the “happy” variant is used. Frank could plausibly be happy because he is the winner or because Bill is. Further discussion on these and other issues can be found in [8].</paragraph><section label="3.1"><section-title>A new test</section-title><paragraph>It is now possible to formulate an alternative to the Turing Test. A collection of pre-tested Winograd schemas can be hidden in a library.{sup:7} A Winograd Schema Test involves asking a number of these questions with a strong penalty for wrong answers (to preclude guessing). A test can be administered and graded in a fully automated way:</paragraph><list><list-item label="1.">select N (e.g., {a mathematical formula}N=25) questions that are suitable (with respect to vocabulary, expertise, etc.);</list-item><list-item label="2.">randomly use one of the special words in the question;</list-item><list-item label="3.">present the test to the subject, and obtain the N binary replies.</list-item></list><paragraph> The final grade for the test is{a mathematical formula} where k codes the penalty for guessing (e.g., {a mathematical formula}k=5). The main claim here is that normally-abled English-speaking adults will pass the test easily. So, if we want to produce behaviour that is indistinguishable from that of people, we will need to come up with a program that can also pass the test.</paragraph><paragraph>To summarise: With respect to the Turing Test, we agree with Turing that the substantive question is whether or not a certain intelligent behaviour can be achieved by a computer program. But a free-form conversation as advocated by Turing may not be the best vehicle for a formal test, as it allows a cagey subject to hide behind a smokescreen of playfulness, verbal tricks, and canned responses. Our position is that an alternative test based on Winograd schema questions is less subject to abuse, though clearly much less demanding intellectually than engaging in a cooperative conversation (about sonnets, for example, as imagined by Turing).</paragraph></section></section><section label="4"><section-title>Passing the test</section-title><paragraph>What would it take for a computer program to pass a Winograd Schema Test. My feeling is that we can go quite some distance with the following:</paragraph><list><list-item label="1.">Take a Winograd schema question such as{a mathematical formula} and parse it into the following form: Two parties are in relation R. One of them has property P. Which? For the question above, this gives the following: {a mathematical formula}R=does not fit in;P=is so small.</list-item><list-item label="2.">Then use big data: search all the English text on the web to determine which is the more common pattern:</list-item></list><paragraph> This “big data” approach is an excellent trick, but unfortunately, it is still too cheap. Among other things, it ignores the connective between R and P. Consider this:{a mathematical formula} Note that the R and P here would be the same as before, even though the answer must be different this time.</paragraph><paragraph>Now consider the following example:{a mathematical formula} Here the relationship between any R and P is clearly much more complex.</paragraph><paragraph>So what do we conclude from this? Do we simply need a bigger bag of tricks?</paragraph><section label="4.1"><section-title>The lure of statistics</section-title><paragraph>There is a tendency in AI to focus on behaviour in a purely statistical sense. We ask: Can we engineer a system to produce a desired behaviour with no more errors than people would produce (with confidence level z)? Looking at behaviour this way can allow some of the more challenging examples that arise (like the question concerning Fred above) to simply be ignored when they are not statistically significant.</paragraph><paragraph>Unfortunately, this can lead us to systems with very impressive performance that are nonetheless idiot-savants. We might produce prodigies at chess, face-recognition, Jeopardy, and so on, that are completely hopeless outside their area of expertise.{sup:8}</paragraph><paragraph>But there is another way of looking at all this. Think of the behaviour of people on Winograd schema questions as a natural phenomenon to be explained, not unlike photosynthesis or gravity. In this case, even a single example can tell us something important about how people are able to behave, however insignificant statistically.</paragraph></section><section label="4.2"><section-title>A thought experiment</section-title><paragraph>Reconsider, for instance, the styrofoam/steel question from above. We might consider using other special words in the question: for “balsa wood,” the answer would be “the table,” for “granite,” it would be “the large ball,” and so on. But suppose we use an unknown word in the question:{a mathematical formula} Here there is no “correct” answer: subjects should not really favour one answer much over the other.</paragraph><paragraph>But suppose we had told the subjects some facts about the XYZZY material{sup:9}:</paragraph><list><list-item label="1.">It is a trademarked product of Dow Chemical.</list-item><list-item label="2.">It is usually white, but there are green and blue varieties.</list-item><list-item label="3.">It is ninety-eight percent air, making it lightweight and buoyant.</list-item><list-item label="4.">It was first discovered by a Swedish inventor, Carl Georg Munters.</list-item></list><paragraph> We can ask, on learning any of these facts, at what point do the subjects stop guessing? It should be clear that only one of these facts really matters, the third one. But more generally, people get the right answer for styrofoam precisely because they already know something like the third fact above about the makeup of styrofoam. This background knowledge is critical; without it, the behaviour is quite different.</paragraph></section><section label="4.3"><section-title>The lesson</section-title><paragraph>So what do we learn from this experiment about the answering of Winograd schema questions? From a pure technology point of view, a reasonable question to ask here is this: Can we produce a good semblance of the target behaviour without having to deal with background knowledge like this? But from a science point of view, we must take a different stance. We want to understand what it takes to produce the intelligent behaviour that people exhibit. So the question really needs to be more like this: What kind of system would have the necessary background knowledge to be able to behave the way people do?</paragraph></section><section label="4.4"><section-title>A radical approach</section-title><paragraph>So to account for what people are actually able to do, we need to consider what it would take to have a system that knows a lot about its world and can apply that knowledge as needed, the way people can.</paragraph><paragraph>One possibility is this:</paragraph><list><list-item label="•">some part of what needs to be known is represented symbolically (call it the knowledge base);</list-item><list-item label="•">procedures operate on this knowledge base, deriving new symbolic representations (call it reasoning);</list-item><list-item label="•">some of the derived conclusions concern what actions should be taken next (including answering questions).</list-item></list><paragraph> This is a very radical idea, first proposed by John McCarthy in a quite extraordinary and unprecedented paper [9]. It suggests that we should put aside any idea of tricks and shortcuts, and focus instead on what needs to be known, how to represent it symbolically, and how to use the representations.</paragraph></section></section><section label="5"><section-title>Two scientific hurdles</section-title><paragraph>I do not want to suggest that with McCarthy's radical idea on board, it is all smooth sailing from here. A good question to ask is why, after 55 years, we have so little to show for it regarding the science of intelligent behaviour. The answer, I believe, is that it leaves some major issues unresolved.</paragraph><paragraph>My Computers and Thought Lecture at IJCAI-85 [5] was in part a reaction to the “Knowledge is Power” slogan which was quite in vogue at the time. It all seemed too facile to me, even back then. My sense was that knowledge was not power if it could not be acquired in a suitable symbolic form, or if it could not be applied in a tractable way. These point to two significant hurdles faced by the McCarthy approach:</paragraph><list><list-item label="1.">Much of what we come to know about world and the people around us is not from personal experience, but is due to our use of language.<list>And yet, it appears that we need to use extensive knowledge to make good sense of all this language.</list></list-item><list-item label="2.">Even the most basic child-level knowledge seems to call upon a wide range of logical constructs.<list>And yet, symbolic reasoning over these constructs seems to be much too demanding computationally.</list></list-item></list><paragraph> I believe that these two hurdles are as serious and as challenging to the science of AI as an accelerating universe is to astrophysics. After 55 years, we might well wonder if an AI researcher will ever be able to overcome them.</paragraph><paragraph>Life being short (and “time to market” even shorter), it is perhaps not surprising that many AI researchers have returned to less radical methods (e.g., more biologically-based, more like statistical mechanics) to focus on behaviours that are seemingly less knowledge-intensive (e.g., recognising hand-written digits, following faces in a crowd, walking over rough terrain). And the results have been terrific!</paragraph><paragraph>But these terrific results should not put us into denial. Our best behaviour does include knowledge-intensive activities such as participating in natural conversations, or responding to Winograd schema questions. It is my hope that enough of us stay focused on this sort of intelligent behaviour to allow progress to continue here as well.</paragraph><paragraph>This will require hard work! I think it is unreasonable to expect solutions to emerge spontaneously out of a few general principles, obviating any real effort on our parts. For example, I do not think we will ever be able to build a small computer program, give it a camera and a microphone or put it on the web, and expect it to acquire what it needs all by itself.</paragraph><paragraph>So the work will be hard. But to my way of thinking, it will be more like scaling a mountain than shovelling a driveway. Hard work, yes, but an exhilarating adventure!</paragraph><section label="5.1"><section-title>Some suggestions</section-title><paragraph>What about those hurdles? Obviously, I have no solutions. However, I do have some suggestions for my colleagues in the Knowledge Representation area:</paragraph><list><list-item label="1.">We need to return to our roots in Knowledge Representation and Reasoning for language and from language.We should not treat English text as a monolithic source of information. Instead, we should carefully study how simple knowledge bases might be used to make sense of the simple language needed to build slightly more complex knowledge bases, and so on.</list-item><list-item label="2.">It is not enough to build knowledge bases without paying closer attention to the demands arising from their use.We should explore more thoroughly the space of computations between fact retrieval and full automated logical reasoning. We should study in detail the effectiveness of linear modes of reasoning (like unit propagation, say) over constructs that logically seem to demand more.</list-item></list><paragraph> As to the rest of the AI community, I do have a final recommendation: We should avoid being overly swayed by what appears to be the most promising approach of the day. As a field, I believe that we tend to suffer from what might be called serial silver bulletism, defined as follows: the tendency to believe in a silver bullet for AI, coupled with the belief that previous beliefs about silver bullets were hopelessly naïve. We see this in the fads and fashions of AI research over the years. Here is a partial list: first, automated theorem proving is going to solve it all; then, the methods appear too weak, and we favour expert systems; then the programs are not situated enough, and we move to behaviour-based robotics; then we come to believe that learning from big data is the answer; and on it goes.</paragraph><paragraph>I think there is a lot to be gained by recognising more fully what our own research does not address, and being willing to admit that other AI approaches may be needed for dealing with it. I believe this will help minimise the hype, put us in better standing with our colleagues, and allow progress in AI to proceed in a steadier fashion.</paragraph></section><section label="5.2"><section-title>The prospects</section-title><paragraph>Finally, let me conclude with a question about the future: Will a computer ever pass the Turing Test (as first envisaged by Turing) or even a broad Winograd Schema Test (without cheap tricks)? The answer to this question, I believe, lies in a quote from Alan Kay: “The best way to predict the future is to invent it.” I take this to mean that the question is not really for the pundits to debate. The question, in the end, is really about us, how much perseverance and inventiveness we will bring to the task. And I, for one, have the greatest confidence in what we can do when we set our minds to it.</paragraph></section></section></content><references><reference label="[1]"><authors>D.G. Bobrow,C. Condoravdi,R. Crouch,V. de Paiva,L. Karttunen,T.H. King,R. Mairn,L. Price,A. Zaenen</authors><title>Precision-focussed textual inference</title><host>Proc. of the ACL WorkshopPrague(2007)</host></reference><reference label="[2]"><authors>B. Christian</authors><title>The Most Human Human</title><host>(2011)Doubleday</host></reference><reference label="[3]"><authors>A. Collins,E. Warnock,N. Aiello,M. Miller</authors><title>Reasoning from incomplete knowledge</title><host>Representation and Understanding(1975)Academic Press</host></reference><reference label="[4]"><authors>I. Dagan,O. Glickman,B. Magnini</authors><title>The PASCAL recognising textual entailment challenge</title><host>Machine Learning Challenges(2006)Springer Verlag</host></reference><reference label="[5]"><authors>H.J. Levesque</authors><title>Making believers out of computers</title><host>Artif. Intell.30 (1986)</host></reference><reference label="[6]"><authors>H.J. Levesque</authors><title>Logic and the complexity of reasoning</title><host>J. Philos. Log.17 (1988) pp.355-389</host></reference><reference label="[7]"><authors>H.J. Levesque</authors><title>Is it enough to get the behaviour right?</title><host>Proc. of IJCAI-09Pasadena, CA(2009)</host></reference><reference label="[8]"><authors>H.J. Levesque,E. Davis,L. Morgenstern</authors><title>The Winograd schema challenge</title><host>Proc. of KR-2012Rome(2012)</host></reference><reference label="[9]"><authors>J. McCarthy</authors><title>The advice taker</title><host>Semantic Information Processing(1968)MIT Press</host></reference><reference label="[10]"><authors>R. Reiter</authors><title>On closed world databases</title><host>Logic and Databases(1978)Plenum Press</host></reference><reference label="[11]"><authors>M. Roemmele,C.A. Bejan,A. Gordon</authors><title>Choice of plausible alternatives: an evaluation of commonsense causal reasoning</title><host>Proceedings of the Commonsense-2011 Symposium(2011)</host></reference><reference label="[12]"><authors>A. Turing</authors><title>Computing machinery and intelligence</title><host>Mind59 (1950) pp.433-460</host></reference><reference label="[13]"><authors>L. von Ahn,M. Blum,N. Hopper,J. Langford</authors><title>CAPTCHA: using hard AI problems for security</title><host>Advances in Cryptology, Eurocrypt 2003(2003) pp.294-311</host></reference><reference label="[14]"><authors>J. Weizenbaum</authors><title>ELIZA</title><host>Commun. ACM9 (1966) pp.36-45</host></reference><reference label="[15]"><authors>T. Winograd</authors><title>Understanding Natural Language</title><host>(1972)Academic PressNew York</host></reference></references><footnote><note-para label="1">This paper is a written version of the Research Excellence Lecture presented in Beijing at the IJCAI-13 conference. Thanks to Vaishak Belle and Ernie Davis for helpful comments.</note-para><note-para label="2">For those who do not know, a steeplechase is a horse race, similar to the usual ones, but where the horses must jump over a number of hedges on the racetrack. So it is like hurdles for horses.</note-para><note-para label="3">See the book by Brian Christian [2] for an interesting account of what it was like to play the human in a Loebner contest.</note-para><note-para label="4">The ELIZA program [14] is a good place to start on that issue.</note-para><note-para label="5">The program at www.trueknowledge.com appears to work this way.</note-para><note-para label="6">See [8] for a discussion of these approaches. The next section is drawn mainly from that paper. I thank Ernie Davis and Leora Morgenstern for their contribution.</note-para><note-para label="7">See, for example, the collection at http://www.cs.nyu.edu/faculty/davise/papers/WS.html.</note-para><note-para label="8">Indeed, it would be good fun to try Watson on Winograd schema questions: the category is “Pronoun referents,” the clue is “Joan made sure to thank Susan for all the help she had given,” and the desired answer in the form of a question is “Who is Susan?”</note-para><note-para label="9">These facts were lifted from the Wikipedia page for styrofoam.</note-para></footnote></root>