<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370213000581</url><title>Multiple instance classification: Review, taxonomy and comparative study</title><authors>Jaume Amores</authors><abstract>Multiple Instance Learning (MIL) has become an important topic in the pattern recognition community, and many solutions to this problem have been proposed until now. Despite this fact, there is a lack of comparative studies that shed light into the characteristics and behavior of the different methods. In this work we provide such an analysis focused on the classification task (i.e., leaving out other learning tasks such as regression). In order to perform our study, we implemented fourteen methods grouped into three different families. We analyze the performance of the approaches across a variety of well-known databases, and we also study their behavior in synthetic scenarios in order to highlight their characteristics. As a result of this analysis, we conclude that methods that extract global bag-level information show a clearly superior performance in general. In this sense, the analysis permits us to understand why some types of methods are more successful than others, and it permits us to establish guidelines in the design of new MIL methods.</abstract><keywords>Multi-instance learning;Codebook;Bag-of-Words</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>In the standard supervised learning task, we learn a classifier based on a training set of feature vectors, where each feature vector has an associated class label. In the Multiple Instance Learning (MIL) task we learn a classifier based on a training set of bags, where each bag contains multiple feature vectors (called instances in the MIL terminology). In this setting, each bag has an associated label, but we do not know the labels of the individual instances that conform the bag. Furthermore, not all the instances are necessarily relevant, i.e., there might be instances inside one bag that do not convey any information about its class, or that are more related to other classes of bags, providing confusing information.</paragraph><paragraph>In many fields, we find problems that are most naturally formulated using the multiple instance learning setting. This is the case of drug discovery (pharmacy), classification of text documents (information retrieval), classification of images (computer vision), speaker identification (signal processing) and bankruptcy prediction (economy), to mention a few fields that make use of this framework (see Section 2 for a more detailed discussion about real examples). This makes the MIL problem an important topic in the machine learning community, where many methods have been published in the last years. Despite this fact, there is a lack of surveys or analytical studies that compare the performance of the different families of MIL algorithms.</paragraph><paragraph>In this work, we focus on Multiple Instance Classification (MIC), leaving out other learning tasks such as regression. We present an extensive review of the methods of the literature accompanied with a thorough empirical comparison. In our analysis, we grouped the methods into a small set of compact paradigms according to how they manage the information from the Multi-Instance (MI) data. As we will see, our characterization is complete in the sense that any MIC method must necessarily fall into one of the families of the proposed taxonomy. Furthermore, the methods falling into each paradigm tend to present a similar behavior, and this makes it easy to analyze and compare the paradigms in the experimental evaluation. As part of the proposed taxonomy we characterize for the first time the Vocabulary-based paradigm. The main difference between this and other paradigms is that in the Vocabulary-based one the instances are classified or discriminated into several classes, while in the other paradigms there is no such discrimination. Many authors [1], [2], [3], [4], [5], [6], [7], [8], [9] have proposed algorithms that fall into the Vocabulary-based paradigm, but the relationship between all these approaches has not been established until now. In this work, we show that all of them fall under the Vocabulary-based family and we provide a clear characterization of this family.</paragraph><paragraph>We are only aware of the recent review by Foulds and Frank [10], and the comparative study performed in the masterʼs thesis of Lin Dong [11]. Unfortunately, these publications do not include the family of Vocabulary-based techniques as such, which is an important paradigm as we show in this paper. In [11], Lin Dong shows a quantitative analysis of many methods but does not obtain conclusive results and leaves out many important algorithms from the Vocabulary-based paradigm. Recently, Foulds and Frank [10] categorized the MIC methods according to the assumption followed by each one. As we show in this work, many categories of methods proposed in [10] fall into the Vocabulary-based one characterized in our work. In our work, we present a complementary analysis in the sense that Foulds and Frank classify the methods according to the assumption followed by each one, while we perform this classification according to the type of information extracted by each method (instance-level or bag-level information) and how it is represented (implicitly or explicitly). In this sense, our analysis is not in conflict with [10]. Furthermore, we provide an empirical evaluation of the proposed paradigms and analyze their behavior, which is not done in [10].</paragraph><paragraph>In summary, this work contributes a novel analysis and taxonomy of the MIC methods and an exhaustive comparative analysis. In total, we analyze fourteen MIC algorithms implemented by us, and we use eight databases from four different fields of knowledge, plus a synthetic database where we studied the behavior of the methods under controlled conditions. A preliminary version of this work appeared in [12].</paragraph><paragraph>The rest of the paper is organized as follows. In Section 2 we motivate in detail the need of using Multiple Instance Classification through real examples. In Section 3 we describe the MIC problem and the taxonomy proposed. In Sections 4, 5 and 6 we describe the main paradigms of the taxonomy: the instance-space paradigm, the bag-space paradigm and the embedded-space paradigm. The latter paradigm contains the Vocabulary-based family of methods, which is described in detail in Section 7. Section 9 provides a comparative analysis of the different paradigms, and we present conclusions in Section 10.</paragraph></section><section label="2"><section-title>Examples of multiple-instance classification problems</section-title><paragraph>We describe here two real problems where the MI representation becomes necessary, i.e., where the objects to be classified are described by bags (containing multiple feature vectors) as opposed to the traditional learning problem where the objects to be classified are represented by means of a single feature vector.</paragraph><paragraph>The first one is the drug activity prediction problem [13]. In this problem, the objects to be classified are chemical molecules. Given a molecule, the system must decide if it is a good drug or it is not. A good drug is characterized by the fact that it is able to bind strongly to a target “binding site”, which is some sort of cavity existing in a much larger molecule [13]. The difficulty comes from the fact that one molecule can adopt multiple three-dimensional shapes (called conformations), and only one or a few of them bind well with the target binding site. In this type of problem, the complete molecule is described by a bag {a mathematical formula}X={x→1,…,x→N}, which is a set that gathers the description of the N possible conformations, where {a mathematical formula}x→i, {a mathematical formula}i=1,…,N, is a feature vector describing the i-th conformation, and the number of conformations N can vary in different molecules.</paragraph><paragraph>Another example of a real problem where MIC becomes important is the one of image classification. Here, given an image we must decide if it belongs to a target class, based on its visual content. For example, the target class might be “beach”, and in this case the positive images are those displaying a beach, while the negative images will be those displaying any other type of visual content. Fig. 6 shows an example of this image classification task, where we explain later the meaning of the red circles. In this figure, the images in the top row are positive, while the images in the bottom row are negative (one of them displays the sea, but without any beach in it, while the other image displays a desert). Although we only show two negative images in this figure, there are many other negative images which contain any other type of content such as countryside, cities, cars, offices, etc. If we look at the positive images in the top row of Fig. 6, we can see that there are regions of the image that are related with the target class (the regions that belong to the sand and sea), whereas there are regions that are not specifically related with it (e.g., the sky, mountain, trees, etc.). In order to obtain a beach image we need both sea and sand, while the rest of regions are not necessary. In order to classify the images, the usual procedure is to first extract a collection of regions in the image, and for each region we obtain a visual descriptor. This visual descriptor is a feature vector that describes the region. As a result, the image is described as a bag {a mathematical formula}X={x→1,…,x→N}, where N is the number of regions extracted and {a mathematical formula}x→i is the feature vector (called instance) describing the i-th region in the image. In Fig. 6 we use red circles for symbolizing the extraction of visual descriptors in different regions. The number of regions extracted depends on the specific algorithm for identifying interesting regions, and might vary from image to image.</paragraph><paragraph>These are just two examples of real problems where using a bag representation, and hence setting the problem as MIC, is necessary. In addition to these two problems, there are many other problem domains that require this type of formulation, as mentioned in the introduction, including classification tasks in information retrieval, audio processing, economic predictions, etc. In the rest of this paper we study the different approaches for solving MIC problems.</paragraph></section><section label="3"><section-title>Basic concepts and overview of paradigms</section-title><paragraph>A bag is a set {a mathematical formula}X={x→1,…,x→N}, where the elements {a mathematical formula}x→i are feature vectors called instances in the MIC terminology, and the cardinality N can vary across the bags. All the instances {a mathematical formula}x→i live in a d-dimensional feature space, {a mathematical formula}x→i∈Rd, called instance space.</paragraph><paragraph>The objective of the MIC problem is to learn a model, at training time, that can be used to predict the class labels of unseen bags. In this work, we only consider the binary classification problem, where a bag X can be either positive or negative. Our objective is to estimate a classification function {a mathematical formula}F(X)∈[0,1] that provides the likelihood that X is positive. In order to learn such a function, we are given a training set with M bags and their corresponding labels, {a mathematical formula}T={(X1,y1),…,(XM,yM)}, where {a mathematical formula}yi∈{0,1} is the label of {a mathematical formula}Xi ({a mathematical formula}yi=0 if {a mathematical formula}Xi is negative, and {a mathematical formula}yi=1 if it is positive).</paragraph><paragraph>In addition to the bag-level classification function {a mathematical formula}F(X), many methods try to learn an instance-level classification function {a mathematical formula}f(x→i) that operates directly on the instances {a mathematical formula}x→i. Throughout this work we will use uppercase to refer to bags X and to the bag-level classifier F, and we will use lowercase to refer to instances {a mathematical formula}x→ and to the instance-level classifier f.</paragraph><section label="3.1"><section-title>Overview of proposed taxonomy</section-title><paragraph>In this work, we categorize the MIC methods according to how the information existent in the MI data is exploited (see Fig. 4). In the Instance-Space (IS) paradigm, the discriminative information is considered to lie at the instance-level. Therefore, the discriminative learning process occurs at this level: a discriminative instance-level classifier {a mathematical formula}f(x→) is trained to separate the instances in positive bags from those in negative ones (see Fig. 1). Based on it, given a new bag X the bag-level classifier {a mathematical formula}F(X) is obtained by simply aggregating instance-level scores {a mathematical formula}f(x→), {a mathematical formula}x→∈X. We say that this type of paradigm is based on local, instance-level information, in the sense that the learning process considers the characteristics of individual instances, without looking at more global characteristics of the whole bag.</paragraph><paragraph>In the Bag-Space (BS) paradigm, the discriminative information is considered to lie at the bag-level. In this paradigm each bag X is treated as a whole entity, and the learning process discriminates between entire bags. As a result, it obtains a discriminative bag-level classifier {a mathematical formula}F(X) which makes use of the information from the whole bag X in order to take a discriminative decision about the class of X. We say that this type of paradigm is based on global, bag-level information, because the discriminative decision is taken by looking at the whole bag, instead of aggregating local instance-level decisions.</paragraph><paragraph>Given the fact that the bag space is a non-vector space, the BS methods make use of non-vectorial learning techniques. As far as we know, all the existent non-vectorial techniques work through the definition of a distance function {a mathematical formula}D(X,Y) that provides a way of comparing any two non-vectorial entities X and Y (where these entities are bags in our problem). Once this distance function has been defined, it can be used into any standard distance-based classifier such as K-Nearest Neighbor (K-NN), or similarly into any kernel-based classifier such as SVM.{sup:1}Fig. 2 illustrates the idea under this paradigm. Although we use the term “distance” in Fig. 2, the BS paradigm also includes methods that use other types of pairwise comparisons between bags, such as kernel-based comparisons {a mathematical formula}K(X,Y) in SVM-based methods. Regarding the bag-level classifier, we use the notation {a mathematical formula}F(X;Θ) in Fig. 2(b), in order to express the fact that the classifier makes use of the learned parameters Θ (see Fig. 2(a)). Along the paper, however, we use the notation {a mathematical formula}F(X) and drop the argument Θ for simplicity.</paragraph><paragraph>In the Embedded-Space (ES) paradigm, each bag X is mapped to a single feature vector which summarizes the relevant information about the whole bag X. As a result, the original bag space is mapped to a vectorial embedded space, where the discriminative classifier is learned. This effectively transforms the original MIC problem into a standard supervised learning problem, where each feature vector has an associated label and any standard classifier such as AdaBoost, Neural Networks or SVM can be applied. Fig. 3 illustrates the idea under this paradigm.</paragraph><paragraph>Note that the ES paradigm is also based on global, bag-level information, in the sense that the bag X is represented by a feature vector {a mathematical formula}v→ that summarizes the relevant information about the whole bag. Given this feature vector, the bag-level classifier {a mathematical formula}F(X) can be expressed as {a mathematical formula}F(X)=G(v→), where G is a discriminant classifier that makes it decision based on the vector {a mathematical formula}v→ summarizing the whole bag.</paragraph><paragraph>In this sense, both the ES and the BS paradigms exploit global, bag-level information. However, the difference between both paradigms lies in the way this bag-level information is extracted. In the BS paradigm, this is done implicitly through the definition of a distance or kernel function.{sup:2} In contrast, in the ES paradigm, the extraction of information from the whole bag is performed explicitly through the definition of a mapping function that defines how the relevant information is represented into a single vector {a mathematical formula}v→.</paragraph><paragraph>Therefore, we categorize the methods based on whether they focus on instance-level information ( IS paradigm) or global, bag-level information, and in the last case whether they extract the relevant information implicitly ( BS paradigm) or explicitly ( ES paradigm). In addition to this, there is also a characteristic computational cost for each paradigm.</paragraph></section><section label="3.2"><section-title>Completeness of the proposed categorization</section-title><paragraph>The presented categorization is complete in the sense that, given any MIC method from the literature it must necessarily fall into one of the three families: IS, BS or ES. If the MIC method obtains the bag-level classification {a mathematical formula}F(X)∈[0,1] as an aggregation of instance-level classifications {a mathematical formula}f(x→)∈[0,1] for all {a mathematical formula}x→∈X, then it falls into the IS paradigm. Otherwise, the method falls either in the BS or the ES paradigm. In the latter case, if the bag X is mapped into a feature vector {a mathematical formula}v→ and then classified by any standard classifier, then it belongs to the ES paradigm. Otherwise, if no such mapping is applied and the bag is classified as a whole, we have a BS method.</paragraph><paragraph>In any categorization we will always find methods that fall close to the boundaries of two categories. For example, in our taxonomy this happens with the BARTMIP method (see Section 7.6). This is an ES method where each bag X is explicitly mapped into a vector {a mathematical formula}v→. However, in order to perform this mapping, the bag X is compared with other bags Y from the training set through the definition of a bag-level distance function {a mathematical formula}D(X,Y). In this sense, this method lies close to the boundary between the ES and BS categories.</paragraph></section><section label="3.3"><section-title>Illustrative examples</section-title><paragraph>In the rest of the work we will use two illustrative synthetic examples. The first one is shown in Fig. 1, and illustrates the case where instance-level information is enough for solving the MIC problem. This happens when there are certain classes of instances that appear only in positive bags, so that learning an (instance-level) model of these classes is enough. In particular, in the example illustrated in Fig. 1 the class of instances 1 only appears in positive bags. Therefore, it is enough to learn an instance-level classifier {a mathematical formula}f(x→)∈[0,1] that provides the confidence that instance {a mathematical formula}x→ belongs to class 1. Once {a mathematical formula}f(x→) has been learned, the bag-level classifier {a mathematical formula}F(X) can be simply obtained by taking the maximum over the instance-level scores: {a mathematical formula}F(X)=maxx→∈Xf(x→). This way, the bag X is classified as positive if any of its instances {a mathematical formula}x→∈X belongs to class 1, and classified as negative otherwise.</paragraph><paragraph>Note that by using such an approach, the learning is performed only at the instance-level, i.e., for obtaining a model of the instances of class 1 which is used by the instance-level classifier {a mathematical formula}f(x→). At the bag-level, however, there is no learning, as the classifier {a mathematical formula}F(X) is obtained as an aggregation of instance-level scores. This type of approach forms part of the IS paradigm, which is characterized in Section 4, and it works for MIC problems such as the one in Fig. 1, where there is at least one class of instances that appears only in positive bags (or the other way around, there is at least one class of instances that appears only in negative bags).</paragraph><paragraph>Fig. 5 shows another synthetic MIC problem where this does not happen. Here, there are two classes of instances, and both of them appear in positive and negative bags. Hence, there is no single class of instances that appears only in positive or only in negative bags. In this type of problem, the learning cannot be performed only at the instance-level. For example, if we learn an instance-level model of class 1 in order to obtain {a mathematical formula}f(x→), then we cannot infer the classification {a mathematical formula}F(X) based only on the individual scores {a mathematical formula}f(x→), as we find that both positive and negative bags contain instances of class 1. The same happens if we learn an instance-level model of class 2.</paragraph><paragraph>If we look at the composition of the bags in Fig. 5, we find that positive bags are characterized by containing instances of both class 1 and class 2. In contrast, negative bags are characterized by containing either instances of class 1 or instances of class 2, but not both of them at the same time. Therefore, it is not enough to learn an (instance-level) model of the classes of instances, but we must learn a bag-level model about the composition of the whole bag. As we will see, this bag-level information can be learned if we use a BS or ES method. Indeed, we will see that both BS and ES are successful in both the problem shown in Fig. 1 and the one in Fig. 5, while IS methods only succeed in the first type of problem.</paragraph><paragraph>Fig. 6 illustrates a real MIC problem similar to the one in Fig. 5. The problem concerns the classification of images into beach (top row images) and non-beach (bottom row images). Here, each image is described as a bag of instances, where the i-th instance {a mathematical formula}x→i describes the i-th local region of the image. The idea is symbolized in the images by using red circles, each one corresponding to one local region. In this type of MIC problem, each bag contains several classes of instances, depending on the regions that conform the image. In order for the image to belong to the class beach, instances of class sand and class sea must co-occur. However, if only one of these classes occurs in the image then the class is non-beach. This type of MI data happens rather frequently in MIC problems, not only in image classification tasks. In addition to this, we will also discuss some other examples where a global bag-level approach to classification is fundamental.</paragraph></section><section label="3.4"><section-title>Related work</section-title><paragraph>In Fig. 7, we show the hierarchy of categories proposed in Foulds and Frank [10]. Comparing Fig. 4, Fig. 7, we see that [10] divides the methods into more paradigms, some of them disconnected from the rest. As we will see, the majority of the paradigms which are isolated in Fig. 7 are indeed part of the Vocabulary-based family, which is characterized for the first time in our review and extensively analyzed.</paragraph><paragraph>Note that Foulds and Frank obtain their taxonomy using a different underlying criteria. In their case, they pay attention to the assumption which each method uses about the relationship between bag labels and instances. Note that some assumptions were stated explicitly by the author of each method, and some others were not, so that they can only be guessed from the algorithm. In our case, we use as criteria at what level the discriminant information is extracted and how it is represented. The two criteria do not conflict and they both help obtain a deeper understanding of the different MIC solutions. Note also that the objectives of both taxonomies are not necessarily the same. In our case the proposed taxonomy has two objectives: first, to provide a clear picture about the existing approaches, and second, to allow an experimental analysis of the methods, in such a way that methods can be compared according to the paradigm they belong to. This last objective is not necessarily pursued by Foulds and Frank, as they do not provide a comparative analysis.</paragraph><paragraph>We proceed now to describe in detail the IS paradigm (Section 4), the BS paradigm (Section 5), and the ES one (Section 6).</paragraph></section></section><section label="4"><section-title>Instance-space paradigm</section-title><paragraph>As explained in Section 3, in the IS paradigm the idea is to infer an instance-based classifier {a mathematical formula}f(x→)∈[0,1] from the training data. Based on this classifier, the bag-level classification {a mathematical formula}F(X)∈[0,1] is constructed as an aggregation of instance-level responses{a mathematical formula} where ∘ denotes the aggregation operator, specific to each MIC algorithm (see below for a review of common operators), and Z represents an optional normalization factor such as {a mathematical formula}Z=N (i.e., dividing the score by the number of instances) or {a mathematical formula}Z=1 if there is no normalization.</paragraph><paragraph>The methods falling in this category must address the question of how to infer an instance-level classifier {a mathematical formula}f(x→) without having access to a training set of labelled instances. In order to solve this issue, some assumption must be made about the relationship between the labels of the bags in the training set and the labels of the instances contained in these bags. In this sense, two sub-categories of IS methods emerge clearly in the literature: the ones following the Standard MI (SMI) assumption and the ones following the Collective assumption.{sup:3}</paragraph><section label="4.1"><section-title>IS methods following the SMI assumption</section-title><paragraph>The SMI assumption states that every positive bag contains at least one positive instance (i.e. an instance belonging to some target positive class), while in every negative bag all of the instances are negative. This is an asymmetrical assumption which is used in many MIC problems such as the traditional one of drug discovery in [13]. Note that this assumption is that one of the instances has some desirable properties that make the bag positive. Therefore, the methods following this assumption try to identify the type of instance that makes the bag positive.</paragraph><paragraph>One of the traditional methods in this category is the Axis-Parallel Rectangle (APR) [13]. In this method, the objective is to estimate an instance-level classifier {a mathematical formula}f(x→;R) defined as:{a mathematical formula} where R describes an Axis-Parallel Rectangle in the instance space. The parameter R is optimized by maximizing the number of positive bags in the training set that contain at least one instance in R and, at the same time, the number of negative bags that do not contain any instance in R. Based on this, the bag-level classifier can be expressed by using the max rule:{a mathematical formula} i.e., X is considered positive if at least one of the instances {a mathematical formula}x→∈X is positive. The max rule is one of the possible aggregation rules used by the different IS methods. In particular, the max rule is also used in DD [15], EM-DD [16] and MI-SVM [17], among other methods. Note that in case of having a binary instance-level classifier {a mathematical formula}f(x→)∈{0,1}, the logical-or aggregation rule{a mathematical formula} is equivalent to the max aggregation rule in Eq. (3). However, if we have a real-valued classifier {a mathematical formula}f(x→), the max rule permits to obtain a real-valued bag-level score {a mathematical formula}F(X) which might be beneficial for some applications.</paragraph><paragraph>An algorithm similar to APR is the one based on Diverse Density (DD) [15]. In this algorithm, the instance-level classifier maximizes a DD measure which is high for those points in the instance space that are close to at least one instance of each positive bag and far away from all the instances of negative ones. We refer to [18], and in [19] we provide additional notes that compare this algorithm with APR.</paragraph><paragraph>In the MI-SVM method [17], the authors propose an IS classifier {a mathematical formula}f(x→;Θ), where Θ are parameters learned by SVM. In order to estimate the SVM, they propose an iterative EM-like approach. In the Expectation-Maximization Diverse Density (EM-DD) algorithm [16], the authors propose a similar iterative approach, maximizing the DD measure in this case. In [19] we provide more details on these algorithms from the point of view of the IS paradigm.</paragraph><paragraph>In [20], Bunescu and Mooney propose a Sparse MIL (SMIL) algorithm also based on SVM. The instance-level classifier {a mathematical formula}f(x→;Θ) is learned by using a training set of positive of instances {a mathematical formula}T=T+∪T− defined as follows:{a mathematical formula}{a mathematical formula} where {a mathematical formula}T+ and {a mathematical formula}T− are the sets of instances considered positive and the one of instances considered negative, {a mathematical formula}μ(X) denotes the average of instances inside X, and {a mathematical formula}B+ and {a mathematical formula}B− are the sets of positive and negative bags respectively.</paragraph><paragraph>Given this training set, the idea of SMIL is to learn an SVM classifier with a relaxed constraint on the classification of positive instances in {a mathematical formula}T+. The objective is to avoid forcing the SVM to provide a positive value for all the instances of a positive bag, but only to at least one of the instances. For this purpose, the algorithm estimates the parameters Θ of the SVM function {a mathematical formula}f(x→;Θ) by minimizing a standard SVM objective function (see [20] for details) subject to the following constraints:{a mathematical formula}{a mathematical formula} The first set of constraints (*) forces the SVM function to provide a negative value when applied to negative instances (allowing a certain degree of misclassification through the slack variable {a mathematical formula}ξ−). The second set of constraints (**) provides a more relaxed condition for positive instances. This condition depends on the size of the bag X from where {a mathematical formula}μ(X) is extracted: if X only contains one instance, we have the standard condition {a mathematical formula}f(μ(X);Θ)⩾1−ξ+, i.e., we require the SVM to provide a positive value (allowing again some misclassification). However, if the bag X contains many instances, the threshold imposed on the SVM is gradually more and more relaxed.</paragraph><paragraph>In [20], the same authors propose a second IS algorithm named Sparse balanced MIL (SbMIL), which is obtained in two steps: first an SMIL algorithm is trained on the MI data, and then the resulting instance-level classifier {a mathematical formula}f(x→) is applied for labelling the instances of the positive bags. For this purpose, the top ν instances with highest score are labelled as positive and the rest as negative, where ν is a parameter estimated by cross-validation. After this step, a standard SVM classifier is trained using the resulting training set of instances, obtaining the final classifier {a mathematical formula}f(x→). We refer to [20] for other SVM-based IS classifiers, being SbMIL the one with highest performance according to reported results [20].</paragraph><section label="4.1.1"><section-title>Synthetic examples</section-title><paragraph>The IS methods are not successful when applied to MI data such as the one illustrated in Fig. 5. Here, we need a bag-level discriminative classifier that considers information about the whole bag before taking its decision. Therefore, all of the IS methods will have a poor performance in these situations. This includes the methods discussed below in Sections 4.2 and 4.2.1.</paragraph><paragraph>In contrast, IS methods will be successful in the example shown in Fig. 1(a), where positive and negative bags have different types of instances. In Fig. 1(b) we showed a typical decision boundary obtained by methods following the Collective assumption, which we review below. In the case of SMI-based methods, however, the decision boundary obtained is similar to the one shown in Fig. 8. If we compare Figs. 1(b) and 8, the latter has a more asymmetrical division of the space, where the positive region is more adapted to the few instances that appear only in positive bags.</paragraph></section></section><section label="4.2"><section-title>IS methods following the collective assumption</section-title><paragraph>The methods of Section 4.1 follow the Standard MI assumption, which has roots in MIC problems such as the Musk drug classification explained in last section, where certain instances make the bag positive. Note that this does not mean that the rest of the instances do not provide relevant information about the bag. For example, in the Musk problem it might happen that all of the instances in a positive bag have certain properties that are characteristic of positive bags. Based on this fact, a more accurate approach is to exploit all the information in order to take a decision. However, the methods from the last section tend to discard a big part of the information, by either only modelling the characteristics of certain instances (as in MI-SVM [17], where only one instance per positive bag is considered in the learning stage, see also our technical report [19]), or by considering only the average vector of a positive bag (as in SMIL [20]).</paragraph><paragraph>In this section we present IS methods that make use of the so-called Collective assumption. This assumption states that “all instances in a bag contribute equally to the bagʼs label” [21]. Whereas the SMI assumption considers only a few instances per positive bag, the Collective one considers all of the instances. As discussed above, this type of approach can provide good results in many MIC databases, including the Musk database. In this database, there might be a few instances that are especially relevant, but all the instances inside the bag have characteristics that convey information about the fact that the bag is positive. In general, we find that something similar happens in virtually all the MIC databases.</paragraph><paragraph>In order to estimate the instance-level classifier {a mathematical formula}f(x→), the methods of this category use a training set of instances where each instance inherits the label of the bag where it lies. The simplest approach is the SIL algorithm described by Bunescu and Mooney [20], which simply trains a standard supervised classifier {a mathematical formula}f(x→) on the resulting training set. Given a new bag X, the bag-level classifier {a mathematical formula}F(X) is obtained by using the sum as aggregation rule:{a mathematical formula}</paragraph><paragraph>Xu et al. [21] and Frank and Xu [22] proposed several methods along these lines. In this work, we evaluate the Wrapper MI method [22], which is simple and representative of this sub-paradigm. The idea of the method is to build a training set using the inheritance rule explained before. In addition to this, the instances are weighted so that each bag receives the same total weight. This is achieved if each instance {a mathematical formula}x→∈X receives the weight {a mathematical formula}w(x→)=S|X|, where S is a constant. In [22] the authors argue that this weighting is fundamental to obtain good results, as it makes the different bags of the training set have the same total weight.</paragraph><section label="4.2.1"><section-title>Weighted Collective methods</section-title><paragraph>A generalization of the previous approach is to allow a different weight for each instance. This generalization gives rise to the weighted Collective assumption, as identified in [10]. Both Foulds [9] and Mangasarian and Wild [23] follow this type of approach. In particular, Foulds proposes an Iterative Framework for Learning Instance Weights (IFLIW) which is based on the Wrapper MI algorithm explained before. We refer to [9] and our technical report [19] for more details on this algorithm. Once the weights {a mathematical formula}w(x→) are obtained for each instance {a mathematical formula}x→, the bag classifier {a mathematical formula}F(X) is computed as a weighted sum of instance-level responses:{a mathematical formula}</paragraph><paragraph>In addition to the weighted Collective paradigm, Foulds and Frank [10] identify what they call the weighted linear threshold paradigm. As explained in [10], this paradigm is almost the same as the weighted Collective one, and just a bit more general. In practice, however, the only one algorithm that the authors found to implement this new paradigm is the YARDS method [9] proposed by the same authors. The YARDS algorithm is indeed a Vocabulary-based algorithm, as we will see in Section 7. Thus, we do not introduce the weighted linear threshold paradigm in this work, and instead we describe the YARDS algorithm in Section 7.</paragraph><paragraph>As explained in Section 4.1.1, the IS methods do not deal well in situations where the discriminative classifier should consider information beyond the single instance. This type of situations require either BS or ES method, which we review in Sections 5 and 6 respectively.</paragraph></section></section></section><section label="5"><section-title>Bag-space paradigm</section-title><paragraph>The idea of the IS paradigm just reviewed is to estimate a model that summarizes the properties of the single instances, by discriminating those typically found in positive bags versus those found in negative ones. This makes this type of methods consider local information, in the sense that the obtained model is about instances and not about bags as a whole. At classification time, the classifier {a mathematical formula}F(X) is obtained as an aggregation of local responses {a mathematical formula}f(x→), where each of them consider only one instance {a mathematical formula}x→ at a time.</paragraph><paragraph>In contrast, the methods of the BS paradigm treat the bags X as a whole, and the discriminant learning process is performed in the space of bags. This allows the algorithm to take into account more information while performing the inference of {a mathematical formula}F(X).</paragraph><paragraph>In order to learn a non-vectorial entity such as a bag, we can define a distance function {a mathematical formula}D(X,Y) that compares any two bags X and Y, and plug this distance function into a standard distance-based classifier such as K-NN or SVM (see Section 5.1 for details).</paragraph><paragraph>Note that a bag X is nothing else than a set of points in a d-dimensional space. Therefore, any distance function {a mathematical formula}D(X,Y) that compares two sets of points X and Y can be used in this context. In this work we study the minimal Hausdorff distance used in [24], the Earth Movers Distance (EMD) [25], the Chamfer distance [26], and the kernel by Gartner et al. [14]. Let us first see the definition of these functions and in Section 5.2 we discuss the intuition behind them. The minimal Hausdorff distance is defined as:{a mathematical formula} This is the distance between the closest points of X and Y. The EMD distance, on the other hand, is the result of an optimization process. Let {a mathematical formula}X={x→1,…,x→N}, and {a mathematical formula}Y={y→1,…,y→M}. The EMD distance is defined as:{a mathematical formula} where the weights {a mathematical formula}wij are obtained through an optimization process that globally minimizes {a mathematical formula}D(X,Y) subject to some constraints, see [27] for details. The Chamfer distance is defined as:{a mathematical formula} In addition to distance functions {a mathematical formula}D(X,Y), we can use kernel functions {a mathematical formula}K(X,Y) that provide a degree of the similarity between the sets X and Y. In particular, Gartner et al. [14] propose, among others, the following kernel:{a mathematical formula} where {a mathematical formula}k(x→,y→) is an instance-level kernel and p is in theory related with the size of the largest possible bag, but in practice can be obtained by cross-validation. Usual definitions of the instance-level kernel {a mathematical formula}k(x→,y→) such as the linear, polynomial or Gaussian one, can be seen to provide a measure of similarity or correlation between the instances {a mathematical formula}x→ and {a mathematical formula}y→, so that the bag-level kernel {a mathematical formula}K(X,Y) is the sum of the similarity between instances in X and those in Y. In [14] the authors prove that if the instances {a mathematical formula}x→ are separable in the space induced by the instance-level kernel, then the bags are also separable in the space induced by the bag-level kernel defined in Eq. (9), as long as each positive bag contains at least one positive instance, i.e., accomplishes the SMI assumption.{sup:4} Below we explain the intuitive idea under both this kernel and the distances reviewed above.</paragraph><paragraph>Along the same line, Zhou et al. [28] propose another kernel function {a mathematical formula}K(X,Y). This kernel not only uses the similarity between pairs {a mathematical formula}(x→,y→) where {a mathematical formula}x→∈X and {a mathematical formula}y→∈Y, but also uses the similarity between the neighborhood of {a mathematical formula}x→ in X and the neighborhood of {a mathematical formula}y→ in Y, see [28] for the definition. Again, although the authors use this kernel function with SVM, the same kernel function can be used with K-NN as well. The resulting algorithm is called MI-Graph by the authors.</paragraph><section label="5.1"><section-title>Distance-based and kernel-based classifiers</section-title><paragraph>The mentioned distance functions can be used with both distance-based classifiers such as K-NN and kernel-based classifiers such as SVM. In case of using an SVM classifier, the distance {a mathematical formula}D(X,Y) can be converted into a kernel {a mathematical formula}K(X,Y) by using the extended Gaussian kernel [29]{a mathematical formula}K(X,Y)=exp(−γD(X,Y)), where γ is a scale parameter estimated by cross-validation.</paragraph><paragraph>Conversely, the kernel functions provide some measure of similarity between bags, and they can be used in distance-based classifiers by using the following transformation: {a mathematical formula}D(X,Y)=K(X,X)−2K(X,Y)+K(Y,Y), as explained in [14].</paragraph><paragraph>In addition to K-NN and SVM, in [24] the authors propose a so-called “Citation K-NN” classifier. This classifier is a small modification to the classical K-NN and it can be used in general (not only for MIC problems).</paragraph><paragraph>In the results section we show results mostly with the SVM classifier, which is usually more accurate. However, we also evaluate different combinations of classifier and distance functions. In general, the definition of the distance function has a bigger impact in the robustness of the method. In this sense, certain types of distance functions are better at exploiting the information contained in the whole bag and thus make the classifier be more robust in general, no matter if we use SVM or K-NN. Indeed, by defining a distance function {a mathematical formula}D(X,Y) (thus defining an associated kernel {a mathematical formula}K(X,Y)) we are providing an implicit transformation ϕ from the original bag space to a certain vector space where the bags are described, similar to the ES paradigm that we will see below.</paragraph><paragraph>Foulds and Frank [10] identify two separate paradigms: the “Nearest Neighbor” (NN) and the MI-Graph, see Fig. 7. The NN paradigm contains algorithms such as the Citation-KNN, whereas the MI-Graph paradigm only contains the MI-Graph algorithm. As we have seen, our BS paradigm embraces both NN and MI-Graph as special cases and also embraces several other cases in addition to these two, thus being much more general.</paragraph></section><section label="5.2"><section-title>Synthetic examples</section-title><paragraph>Let us see how the different distance functions compare two bags, using as synthetic example the one in Fig. 5. As we discussed in Section 3.3, this figure illustrates the case when global, bag-level information is fundamental for obtaining a good classification of the bags. So it is interesting to see if the different distance functions exploit the global information about the bag by studying how they behave with this example.</paragraph><paragraph>Let us first consider the Chamfer distance in Eq. (8). In order to discuss this distance, let us define the distance {a mathematical formula}d(x→,Y), between an instance {a mathematical formula}x→∈X and a bag Y, as {a mathematical formula}d(x→,Y)=miny→∈Y‖x→−y→‖. This distance will be low if there is some instance in Y that belongs to a class that is similar to the one of {a mathematical formula}x→. Given this definition, we can rewrite the Chamfer distance as {a mathematical formula}D(X,Y)=1|X|∑x→∈Xd(x→,Y)+1|Y|∑y→∈Yd(y→,X). Thus, the distance {a mathematical formula}D(X,Y) will be low if two bags X and Y have the same or similar classes of instances.</paragraph><paragraph>In Fig. 9(a) this idea is illustrated when comparing two positive bags X and Y. In this case, the distance {a mathematical formula}D(X,Y) is low because each instance of class red star in X matches well with some instance of class red star in Y, and the same happens with blue triangles. Fig. 9(b) illustrates what happens when a positive bag X and negative bag Y are compared. In this case, the distance {a mathematical formula}D(X,Y) will be large because there are many instances that do not match well (in particular, blue triangles in X do not match well with any instance in Y).</paragraph><paragraph>A similar thing happens with the EMD and Gartner et al. [14] methods. In the former, an optimization is performed that matches each instance from X with the most similar from Y, in such a way that the global distance between both bags is minimized. In the Gartner et al. [14] method (Eq. (9)), let us consider the instance-level kernel {a mathematical formula}k(x→,y→)=exp(γ‖x→−y→‖), where γ is obtained by cross-validation.{sup:5} In this case, only those instances {a mathematical formula}x→ and {a mathematical formula}y→ that are similar will receive a value {a mathematical formula}k(x→,y→) significantly larger than zero, if γ is correctly estimated. The effect again is that two bags X and Y will receive a high similarity score {a mathematical formula}K(X,Y) if the proportion of instances from each class is similar in both bags.</paragraph><paragraph>An exception to these methods is the min Hausdorff distance (Eq. (6)), which only considers one matching: the one from the two closest instances in both bags. This is illustrated in Fig. 9(d). In this example, the distance between a positive and negative bag will be low, as there is at least one of the instances from one bag that match well with an instance from the other bag. In general, the min Hausdorff distance is problematic in many situations, as we only extract the information of a single best matching instance, thus missing a lot of information from the rest of the bag. In the experimental section we evaluate the effect of the different distance functions.</paragraph></section></section><section label="6"><section-title>Embedded-Space paradigm</section-title><paragraph>Both the last paradigm and the one presented in this section are based on extracting global information about the bag. In the BS paradigm this is done in an implicit way through the definition of the distance function {a mathematical formula}D(X,Y) or kernel function {a mathematical formula}K(X,Y). This function defines how bags are compared, and therefore, how the information about them is considered in the matching. v In the ES paradigm, this is done in an explicit way, by defining a mapping {a mathematical formula}M:X↦v→ from the bag X to a feature vector {a mathematical formula}v→ which summarizes the characteristics of the whole bag. Different definitions of this mapping function put emphasis on different types of information, and have a high impact on the performance of the method.</paragraph><paragraph>In this sense, we can split the existing ES methods in roughly two sub-categories. In the first one the methods simply aggregate the statistics of all the instances inside the bag, without making any type of differentiation among instances. In contrast, in the Vocabulary-based paradigm the mapping is constructed by analyzing how the instances of the bag match certain prototypes that have been previously discovered in the data. Let us analyze each of these two sub-paradigms in turn. The first one has only a few methods and is described in Section 6.1. The second one contains a very large number of methods and is described in a separate Section 7.</paragraph><section label="6.1"><section-title>ES methods without vocabularies</section-title><paragraph>These methods simply aggregate statistics about the attributes of all the instances contained in the bag, without making any differentiation among these instances. Dong et al. [11], for example, propose the so-called Simple MI method that maps each bag X to the average of the instances inside {a mathematical formula}M(X)=1|X|∑x→∈Xx→.</paragraph><paragraph>This simple strategy is also evaluated by Bunescu and Mooney [20]. In [14], the authors propose to map each bag to a max–min vector, i.e., {a mathematical formula}M(X)=(a1,…,ad,b1,…,bd), where {a mathematical formula}aj=minx→∈Xxj and {a mathematical formula}bj=maxx→∈Xxj, for {a mathematical formula}j=1,…,d, where d is the dimensionality of the instances. In this work we include the Simple MI method of [11] in our evaluation.</paragraph><paragraph>Let us consider the behavior of Simple MI in the scenario depicted in Fig. 5. In this case, the average of positive and negative bags is different, so that the method will be successful. However, when the number of classes of instances is large, using a simple average to describe all of the instances leads to poor performance. In this sense, we will find many cases where the average of two bags is similar, even though each one of them contains different classes of instances. This is evaluated in Section 9.3.</paragraph></section></section><section label="7"><section-title>Vocabulary-based methods</section-title><paragraph>The methods of this paradigm also use an ES, like the ones of Section 6.1. The difference here is that the instances of the bag are classified (at least in some sense) in order to obtain the embedding. This classification discriminates between different classes of instances, which is not done in the sub-paradigm of Section 6.1. Note that, although we talk about classes of instances, these classes are usually discovered in an unsupervised way, so that they do not have an associated semantic label.</paragraph><paragraph>We call this family of methods “Vocabulary-based paradigm” because they make use of a so-called vocabulary in order to perform the embedding. This vocabulary stores the information about all the classes of instances present in the training set, and this information is used in order to first classify the instances of a new bag and then perform the embedding of this bag. All the methods of this family follow exactly the steps described in Section 7.2. Before explaining these steps, however, let us explain the idea behind this family of methods.</paragraph><section label="7.1"><section-title>Idea behind the vocabulary-based methods</section-title><paragraph>The idea of this paradigm is to provide information about what classes of instances are present in X. In order to clarify the concepts, let us consider the Bag-of-Words (BoW) method, that belongs to this family. Here, the classes of instances are obtained by clustering. The vocabulary V stores the description of the K clusters of instances present in the data. Based on this, a bag X is represented by a histogram {a mathematical formula}v→ that counts how many instances from X fall into each cluster. In this way, the mapping provides information about the composition of X in terms of classes of instances present in X.</paragraph><paragraph>Now, let us consider how the BoW method works, using as synthetic example the one in Fig. 5. As we discussed in Section 3.3, this figure illustrates the case when global, bag-level information is fundamental for obtaining a good classification of the bags. So it is interesting to see how the method works in this case. In Fig. 5 the positive bags are characterized by having two classes of instances co-occurring in the bag, while negative bags are characterized by having only one of the two classes, but not both of them at the same time. In Section 3.3 we showed a real MIC problem where this happens.</paragraph><paragraph>Fig. 10(a) shows the clusters discovered in the data. In order to make it more realistic, we have considered that the clusters do not correspond strictly to classes of instances. Instead, the instances of each class are partitioned into two clusters, and there is an additional cluster that contains instances from both classes. Fig. 10(b) shows the mapping of bags into histograms. These histograms reflect the fact that positive bags are characterized by containing instances of both classes in similar proportions, while negative bags only contain one class of instance (i.e., in the latter case only certain components of the histogram will have a large value). These histogram vectors are used by the discriminative classifier in order to discriminate between positive and negative bags. Furthermore, it is able to disregard those regions of the instance space which contain ambiguous information. This is the case of cluster 3, which makes component 3 of the histograms have similar values for both positive and negative bags.</paragraph><paragraph>The rest of the Vocabulary-based methods share a similar philosophy, although not all of them are based on clustering. In general, the vocabulary stores a collection of prototypes that are used for describing the composition of the bags. In this sense, all the methods use the vocabulary for describing the content of the bag in terms of the classes of instances found inside. Although the vocabulary always stores prototypes (described in a more or less complex form), in next section we use the more general term “concept” in order to describe the items of the vocabulary.</paragraph></section><section label="7.2"><section-title>Characterization of the Vocabulary-based methods</section-title><paragraph>All the Vocabulary-based methods are based on the following components:</paragraph><list><list-item label="1.">A “vocabulary” V which is defined as a set {a mathematical formula}V={(C1,θ1),…,(CK,θK)} storing K “concepts”, where the j-th concept has the identifier {a mathematical formula}Cj and is described by the set of parameters {a mathematical formula}θj. Most of the times, the term “concept” means “class of instances”, so that the vocabulary V stores K classes, where the j-th class has identifier {a mathematical formula}Cj and is described by parameters {a mathematical formula}θj such as the mean and covariance of the j-th class of instances. Usually each class of instances corresponds to a cluster obtained by K-Means.</list-item><list-item label="2.">A mapping function{a mathematical formula}M(X,V)=v→ which, given a bag X and a vocabulary V, obtains a K-dimensional feature vector {a mathematical formula}v→=(v1,…,vK). This provides an embedding of the original bag space into a K-dimensional feature space where each bag X is represented by a feature vector {a mathematical formula}v→. In order to perform this embedding, the function {a mathematical formula}M(X,V) takes into account the matching between the instances {a mathematical formula}x→i∈X and the “concepts” {a mathematical formula}Cj∈V. In many cases, this matching can be understood as a classification of instances, i.e., if an instance {a mathematical formula}x→i∈X matches the concept {a mathematical formula}Cj∈V, then we say that {a mathematical formula}x→i is classified as class {a mathematical formula}Cj.</list-item><list-item label="3.">A standard supervised classifier{a mathematical formula}G(v→)∈[0,1] which classifies the feature vectors {a mathematical formula}v→ in the embedded space. This classifier is trained using an embedded training set {a mathematical formula}TM={(v→1,y1),…,(v→N,yN)}, where {a mathematical formula}v→i=M(Xi,V), and recall that {a mathematical formula}yi∈{0,1} is the label of {a mathematical formula}Xi. Given a new bag X to be classified, the bag classifier {a mathematical formula}F(X) can be expressed as {a mathematical formula}F(X)=G(M(X,V)), i.e., we first map the bag X into the feature vector {a mathematical formula}v→=M(X,V), and then apply the classifier {a mathematical formula}G(v→).</list-item></list><paragraph> The algorithms of this family differ in the first two components, i.e., how the vocabulary V and the mapping function {a mathematical formula}M are defined. Below we explain this for each algorithm of the Vocabulary-based family. The third component, the supervised classifier {a mathematical formula}G, is not so important, as we can use any standard classifier such as AdaBoost or SVM.</paragraph></section><section label="7.3"><section-title>Histogram-based methods</section-title><paragraph>The methods of this sub-paradigm use a function {a mathematical formula}M that maps each bag X into a histogram {a mathematical formula}v→=(v1,…,vK) where the j-th bin {a mathematical formula}vj counts how many instances of X fall into the j-th class {a mathematical formula}Cj of the vocabulary V. Let us explain how each point of the list in Section 7.2 is instantiated in this sub-paradigm.</paragraph><paragraph>The first point of Section 7.2 is the vocabulary V. Here, the “concepts” of the vocabulary are classes of instances. These classes are usually obtained by means of a clustering algorithm, which receives as input the collection of instances of the training set {a mathematical formula}T, and produces as output K classes {a mathematical formula}C1,…,CK.</paragraph><paragraph>The second point of Section 7.2 is the mapping function {a mathematical formula}M. This function can be expressed as follows: {a mathematical formula}M(X,V)=(v1,…,vK), where{a mathematical formula} Here, {a mathematical formula}fj(x→i)∈[0,1] provides the likelihood that the instance {a mathematical formula}x→i∈X belongs to class {a mathematical formula}Cj. Thus, {a mathematical formula}vj counts how many instances are classified into {a mathematical formula}Cj. The constant Z is a normalization factor so that {a mathematical formula}∑jvj=1. We can also set {a mathematical formula}Z=1 and leave the histogram un-normalized.</paragraph><paragraph>We see now representative algorithms of the Histogram-based sub-paradigm.</paragraph><section label="7.3.1"><section-title>Histogram-based Bag-of-Words with hard-assignment</section-title><paragraph>This algorithm uses hard-assignment (i.e., each instance is assigned to exactly one cluster) in both the clustering algorithm and in the instance classifier {a mathematical formula}fj(.) of Eq. (10). Let us see both components in turn.</paragraph><paragraph>In order to obtain the vocabulary V, we use a clustering algorithm with hard-assignment. Well known examples of such algorithms are K-Means (KM) and Mean-Shift. In this work we use KM, as it is the most widely used [5], [30]. Let {a mathematical formula}D be the set gathering all the instances of all the bags of the training set {a mathematical formula}T. The clustering algorithm receives as input the set of instances {a mathematical formula}D and produces as output K classes {a mathematical formula}C1,…,CK, where each instance in {a mathematical formula}D is assigned to one class. Let {a mathematical formula}Sj be the set of instances in {a mathematical formula}D assigned to class {a mathematical formula}Cj, and let {a mathematical formula}p→j be the average of the instances in {a mathematical formula}Sj, i.e., {a mathematical formula}p→j=1|Sj|∑x→i∈Sjx→i. The vector {a mathematical formula}p→j is called the “prototype” of {a mathematical formula}Cj, and we use it as the parameter that describes {a mathematical formula}Cj in the vocabulary V, i.e., using the notation of Section 7.2 we define {a mathematical formula}θj={p→j}. The number K of clusters is a parameter of the algorithm, and its choice is discussed in Section 9.2.1.</paragraph><paragraph>Regarding the mapping function {a mathematical formula}M expressed in Eq. (10), we define the instance classifier {a mathematical formula}fj as:{a mathematical formula} This way, we use hard-assignment i.e., each instance {a mathematical formula}x→ is assigned to exactly one cluster, which is the one with lowest distance to {a mathematical formula}x→. The constant Z in Eq. (10) is set to {a mathematical formula}Z=|X|, i.e., the number of instances of the bag, in order to normalize the histogram {a mathematical formula}v→.</paragraph><paragraph>This algorithm has been extensively used in Computer Vision under the name of Bag-of-Words, see for example [5], [30]. Despite its success it is not well known by the MIC community, and its relationship with other Vocabulary-based algorithms has not been discussed until now. In this work, we call it Histogram-based Bag-of-Words (H-BoW), in order to differentiate it from the Distance-based Bag-of-Words (D-BoW) which will be seen later.</paragraph><paragraph>In [4] the authors propose a similar algorithm which uses soft-assignment based on a Gaussian kernel. We refer to [4] and to our technical report [19], Section 1.1.1.</paragraph></section><section label="7.3.2"><section-title>Bag-of-Words with Gaussian Mixture Models</section-title><paragraph>Instead of using a clustering algorithm with hard-assignment, as in the last section, we can use a clustering algorithm with soft-assignment. This can be done if we estimate Gaussian Mixture Models with Expectation-Maximization. The resulting algorithm is very similar to the one of the last section, we refer to our technical report [19] for a more detailed discussion. We call the resulting algorithm H-BoW (EM).</paragraph></section><section label="7.3.3"><section-title>YARDS algorithm</section-title><paragraph>In [9] the authors propose the “Yet Another Radial Distance-based Similarity measure” (YARDS) algorithm. They claim that this algorithm implements a weighted linear threshold paradigm which in turn generalizes the weighted Collective paradigm, we refer to [9] and [10]. We see here that the YARDS method follows the characterization of Section 7.2 and thus belongs to the Vocabulary-based family according to our analysis. In particular, we classify it in the Histogram-based sub-paradigm, as the mapping function is expressed as in Eq. (10). We see now each point listed in Section 7.2.</paragraph><paragraph>The vocabulary V is similar to the one of H-BoW, i.e., the j-th concept {a mathematical formula}Cj is represented by one prototype vector {a mathematical formula}p→j. In order to obtain the prototypes, we might use a clustering function such as KM, as in the H-BoW algorithm. However, in [9] the authors use all the instances from all the training bags as prototypes. This is like using clusters of size one.</paragraph><paragraph>The mapping function {a mathematical formula}M is expressed again by Eq. (10), where now the function {a mathematical formula}fj is expressed as {a mathematical formula}fj(x→)=exp(−‖x→−p→j‖2σ2). Although {a mathematical formula}fj cannot be strictly considered a classification function, it can be seen as the un-normalized likelihood that {a mathematical formula}x→ falls in a Gaussian with center {a mathematical formula}p→j and scale σ. Alternatively, {a mathematical formula}fj(x→) can be seen as the similarity between the instance {a mathematical formula}x→ and the j-th prototype {a mathematical formula}p→j of the vocabulary. The constant Z in Eq. (10) is now set to 1 so that {a mathematical formula}v→ is left un-normalized.</paragraph></section><section label="7.3.4"><section-title>Weidmannʼs hierarchy</section-title><paragraph>This hierarchy consists of three assumptions in increasing order of generality: the presence-based, the threshold-based and the count-based assumptions (see Fig. 7). All of them make use of a set of target classes of instances {a mathematical formula}C={C1,…,CM}. Briefly, the first assumption states that a bag X is positive if, for each {a mathematical formula}Ci∈C the bag X contains at least one instance in {a mathematical formula}Ci. The threshold-based assumption is more general and states that X is positive if it contains at least {a mathematical formula}ti instances in {a mathematical formula}Ci. And the count-based assumption is the most general one and states that X must contain more than {a mathematical formula}ti instances and less than {a mathematical formula}ui instances in {a mathematical formula}Ci, for each {a mathematical formula}i=1,…,M.</paragraph><paragraph>Let us see in more detail the presence-based assumption, which is the less general of the hierarchy. This assumption was used by Foulds and Frank [10] in order to establish a link between the methods that follow the SMI assumption (see Section 4.1) with methods that, according to our analysis, fall in the Vocabulary-based paradigm. Using the notation of [10], let {a mathematical formula}Δ(X,Cj) be a function that counts the number of instances in the bag X that belong to class {a mathematical formula}Cj. The presence-based assumption states that a bag X is positive if {a mathematical formula}Δ(X,Cj)&gt;1{a mathematical formula}∀j=1,…,M, i.e., if there is at least one instance belonging to each one of the classes in {a mathematical formula}C. Note that Δ is nothing else than a histogram as the one used in the H-BoW method, and that, therefore, the H-BoW method can be considered to follow the presence-based assumption. For this purpose, the H-BoW histogram must be binarized, as it is only important to know the presence or absence of instances in each cluster.</paragraph><paragraph>In [10], Foulds and Frank discuss the relationship between the mentioned presence-based assumption and the methods of the IS paradigm that follow the SMI assumption (explained in Section 4.1). The latter methods classify the instances into two classes {a mathematical formula}C={C+,C−}, where {a mathematical formula}C+ denotes the set of positive instances and {a mathematical formula}C− the set of negative instances. Given such classes and the counting function Δ, we can map the bag X into a histogram with two entries, {a mathematical formula}v→=(v1,v2), where {a mathematical formula}v1=Δ(X,C+) and {a mathematical formula}v2=Δ(X,C−), counting the positive and negative instances respectively. The IS methods classify the bag X as positive if any of its instances belongs to {a mathematical formula}C+, so that we can express the bag-level classifier as {a mathematical formula}F(X)=[Δ(X,C+)&gt;0], where {a mathematical formula}[.] denotes the indicator function.</paragraph><paragraph>With this reasoning, Foulds and Frank [10] consider that the IS methods following the SMI assumption are part of the presence-based paradigm. This is consistent with their criteria for categorizing the methods, which is based upon the assumption followed by each method. In our analysis, however, we use a different criteria for categorizing the methods. We look at what level of information the discriminant learning takes place: at the instance-level or at the bag-level.</paragraph><paragraph>In this sense, we can note two things. First, in the IS methods, the discriminative learning takes place at the instance level. This is done for inferring the instance classifier {a mathematical formula}f(x→), which is learned in a discriminative way. This classifier {a mathematical formula}f(x→) is in turn used by the IS methods for classifying the instances into either {a mathematical formula}C+ or {a mathematical formula}C−. In contrast, in the ES methods there is no discriminative learning at the instance level. Instead, the clustering of instances {a mathematical formula}x→ is performed in an unsupervised way.</paragraph><paragraph>Second, and more important: in the ES methods the discriminative learning takes place at the bag level. For this purpose, the ES methods map all the bags {a mathematical formula}Xi of the training set to vectors {a mathematical formula}v→i. These vectors are then introduced to a standard supervised learner. By feeding these vectors {a mathematical formula}v→i into the supervised learner, the ES methods learn bag-level information (because each vector summarizes the content of a whole bag). In contrast, the IS methods do not learn bag-level information. We can see this because the expression {a mathematical formula}F(X)=[Δ(X,C+)&gt;0] is a fixed threshold over the first component of the vector: {a mathematical formula}F(X)=[v1&gt;0] and it does not involve learning the features of {a mathematical formula}v→.</paragraph><paragraph>A different thing would be that the whole vector {a mathematical formula}v→=(v1,v2) is passed into a learner. Based on this information, given for all the bags of the training set, the learner could obtain the optimal thresholds for each component {a mathematical formula}v1 and {a mathematical formula}v2 in order to obtain the bag-level classifier {a mathematical formula}F(X). This learning process would consider bag-level information represented by {a mathematical formula}v→=(v1,v2). Note that this happens in the ES methods, but not in the IS methods.</paragraph><paragraph>As a conclusion, we can see that the SMI assumption can be considered a particular case of the assumption followed by part of the Vocabulary-based methods (the one based on histograms, which form just a small subset). However, according to the criteria followed in this work (i.e., at what level the information is learned in a discriminative manner) the IS methods in Section 4.1 cannot be considered part of the ES methods using vocabularies.</paragraph></section></section><section label="7.4"><section-title>Distance-based methods</section-title><paragraph>The previous Histogram-based sub-paradigm is characterized by having a mapping function {a mathematical formula}M expressed by Eq. (10). This equation counts the number of instances that fall into class {a mathematical formula}Cj or that lie close to the prototype {a mathematical formula}p→j that represents {a mathematical formula}Cj. We explain now the distance-based sub-paradigm, where the mapping function is expressed as {a mathematical formula}M(X,V)=(v1,…,vK), where:{a mathematical formula} The function {a mathematical formula}dj(x→i) measures the distance between the instance {a mathematical formula}x→i∈X and the j-th concept {a mathematical formula}Cj∈V. If the concept {a mathematical formula}Cj is represented by just one prototype vector {a mathematical formula}p→j, we can define {a mathematical formula}dj(x→i) as the Euclidean distance, {a mathematical formula}dj(x→i)=‖x→i−p→j‖.</paragraph><paragraph>Note that, by using Eq. (12), the j-th element of the vector {a mathematical formula}v→ indicates the matching degree between the j-th concept {a mathematical formula}Cj and the instances of the bag X. If {a mathematical formula}vj is low, we can say that {a mathematical formula}Cj has a good matching with some instance of X. On the other hand, if {a mathematical formula}vj is high, all the instances in X are far away from {a mathematical formula}Cj, which means that {a mathematical formula}Cj does not have a good matching in X.</paragraph><paragraph>Therefore, both the Histogram-based methods and the distance-based methods map the bag X to a vector {a mathematical formula}v→ where the j-th element {a mathematical formula}vj measures the degree of “presence” of the class {a mathematical formula}Cj in the bag X. In the Histogram-based methods this is done by counting the number of instances that fall into {a mathematical formula}Cj, while in the distance-based methods this is done by providing the lowest distance from {a mathematical formula}Cj to any instance in X.</paragraph><paragraph>Instead of using a distance function {a mathematical formula}dj, some algorithms use a similarity function {a mathematical formula}sj. In this case, instead of using Eq. (12), we use an analog expression:{a mathematical formula}</paragraph><paragraph>We see now representative algorithms of the distance-based sub-paradigm.</paragraph><section label="7.4.1"><section-title>Distance-based Bag-of-Words</section-title><paragraph>Several authors [31], [32], [33], [1], [2] use this algorithm, although this fact is not pointed out in the mentioned papers. Among these methods, probably the most well-known are the one of Auer et al. [2], the DD-SVM method [33], and the MILES method [1]. We first explain our setting of the Distance-based Bag-of-Words (D-BoW) algorithm and then review the setting of other authors [31], [32], [33], [1], [2].</paragraph><paragraph>We obtain the vocabulary V as in the H-BoW algorithm: we use clustering with hard-assignment, and each cluster {a mathematical formula}Cj is represented by a single prototype vector {a mathematical formula}p→j which is the mean of the instances in the cluster. In our case, we use the KM clustering algorithm as in H-BoW.</paragraph><paragraph>Regarding the mapping function {a mathematical formula}M, we tested two approaches: using Eq. (12) with the Euclidean distance, i.e., {a mathematical formula}dj(x→)=‖x→−p→j‖, and using Eq. (13) where the similarity function {a mathematical formula}sj is defined as{a mathematical formula} We observed slightly better results when we use a similarity {a mathematical formula}sj instead of a distance {a mathematical formula}dj, as the values of {a mathematical formula}sj(x→) are constrained to the interval {a mathematical formula}[0,1] and rapidly saturate to 0 for far away instances. Therefore, this is the approach reported in the experimental section.</paragraph><paragraph>Regarding the methods in the literature, in [33], [1], [2], [32] the vocabulary V is obtained without clustering: the raw instances of the positive bags are used as prototypes {a mathematical formula}p→j of the vocabulary. In our experiments we saw that clustering produces better results than using the raw instances of the training set. In [31], the authors use a clustering algorithm with hard-assignment that is specific of their domain. In all the cases, the j-th concept {a mathematical formula}Cj is described by one prototype {a mathematical formula}p→j, except for [31], where the covariance of the cluster {a mathematical formula}Σj is also stored along with the mean of the cluster {a mathematical formula}p→j.</paragraph><paragraph>Regarding the mapping function, in [31] the authors use Eq. (12) where {a mathematical formula}dj is the Mahalanobis distance: {a mathematical formula}dj(x→)=(x→−p→j)TΣj−1(x→−p→j). In [2], {a mathematical formula}dj is the Euclidean distance {a mathematical formula}dj(x→)=‖x→−p→j‖, and in [33] it is the weighted Euclidean distance {a mathematical formula}dj(x→)=‖x→−p→j‖w→. Finally, in [1], [32], the authors use Eq. (13) where the similarity function {a mathematical formula}sj is defined as in Eq. (14), and σ is chosen heuristically.</paragraph><paragraph>Regarding the standard classifier {a mathematical formula}G, in [31], [2] the authors use AdaBoost with decision stumps, and in [33], [1], [32] they use SVM. In our additional notes [19] we show how the Auer and Ortnerʼs method [2] can be expressed as a D-BoW algorithm, although it is easy to see it from the original paper [2].</paragraph><paragraph>In [10], the authors put the Auer and Ortnerʼs method [2] into the Standard MI paradigm, and the DD-SVM [33] and MILES [1] methods into an isolated paradigm (see Fig. 7). As we have seen, these methods are Vocabulary-based and have the characteristics of the distance-based sub-paradigm (Eqs. (12) and (13)), and in particular we label them as D-BoW methods.</paragraph></section><section label="7.4.2"><section-title>GMIL and count-based GMIL</section-title><paragraph>The Generalized Multiple Instance Learning (GMIL) algorithm [34] explicitly enumerates all possible axis-parallel boxes. It maps the bag X into a boolean vector where the j-th element is set to 1 if at least one instance from the bag falls into the j-th box. It can be easily seen that this method falls into the distance-based sub-paradigm, we refer to our additional notes [19] for details.</paragraph></section></section><section label="7.5"><section-title>Attribute-based methods</section-title><paragraph>Currently, this sub-paradigm includes only the Intermediate Matching Kernel (IMK) algorithm [6]. In the previous methods, the mapping function {a mathematical formula}M obtains a vector {a mathematical formula}v→ where the j-th element indicates the level of presence of the j-th concept {a mathematical formula}Cj in the bag X. In the attribute-based sub-paradigm, this is different. Here, the mapping function returns a vector {a mathematical formula}v→ that is a concatenation of sub-vectors:{a mathematical formula} where the sub-vector {a mathematical formula}v→j summarizes the attributes of the instances in X that match the j-th concept {a mathematical formula}Cj. Note that this is similar to the Simple MI method, but there we only had one vector {a mathematical formula}v→ that summarized the attributes of all the instances of the bag, regardless of their class. In contrast, here we separate the instances of X in classes and then {a mathematical formula}v→j summarizes the attributes of the instances inside the class {a mathematical formula}Cj. In general, let the function {a mathematical formula}fj(x→i)∈[0,1] provide the matching degree between instance {a mathematical formula}x→i∈X and concept {a mathematical formula}Cj. The vector {a mathematical formula}v→j can be computed as:{a mathematical formula} For example, {a mathematical formula}fj might be an instance classifier with either hard-assignment or soft-assignment. This way, the vector {a mathematical formula}v→j is the weighted mean of the instances according to its degree of membership in the class {a mathematical formula}Cj. Let us now see the instantiation of these ideas in the IMK method.</paragraph><paragraph>In the IMK method, we obtain a vocabulary V as in H-BoW: K-Means is used to obtain K clusters, and for each one we store its center {a mathematical formula}p→j. Then, the mapping function {a mathematical formula}M is obtained by defining {a mathematical formula}fj in Eq. (16) as:{a mathematical formula} Thus, in the IMK method the vector {a mathematical formula}v→j is the instance from X that best matches the concept {a mathematical formula}Cj.</paragraph><paragraph>Regarding the standard classifier {a mathematical formula}G, in [6] the authors propose SVM with a kernel that is specific for this method. Let two bags X and Y be mapped to vectors {a mathematical formula}v→ and {a mathematical formula}w→ respectively, where {a mathematical formula}v→=v→1∘⋯∘v→K and {a mathematical formula}w→=w→1∘⋯∘w→K. We define the kernel {a mathematical formula}K(v→,w→) as:{a mathematical formula}</paragraph></section><section label="7.6"><section-title>Methods based on vocabularies of bags</section-title><paragraph>Currently, this sub-paradigm includes just the BARTMIP method [35]. In this sub-paradigm, the vocabulary V is formed with K concepts {a mathematical formula}Cj where each one represents a class of bags, instead of a class of instances. In order to obtain the vocabulary, the bags of the training set {a mathematical formula}T are partitioned into K clusters, using a clustering algorithm. For example, in [35] this clustering is performed by using the K-medoids algorithm together with a distance function such as the Hausdorff distance for comparing pairs of bags. The result is that the bags are clustered into K clusters {a mathematical formula}R1,…,RK. The j-th cluster {a mathematical formula}Rj represents the j-th concept {a mathematical formula}Cj of the vocabulary V. In [35], {a mathematical formula}Cj is parameterized by using the medoid of {a mathematical formula}Rj, which is the bag from {a mathematical formula}Rj with minimum average distance to the other bags of the cluster. Let {a mathematical formula}Pj be this bag medoid.</paragraph><paragraph>Regarding the mapping function {a mathematical formula}M, [35] proposes {a mathematical formula}M(X,V)=(v1,…,vK) where {a mathematical formula}vj=D(X,Pj) is the Hausdorff distance between the bag X and the bag medoid {a mathematical formula}Pj.</paragraph></section></section><section label="8"><section-title>Alternative multi-instance scenarios</section-title><paragraph>In this paper we have focused strictly on the Multiple Instance Classification problem. As we explained in Section 3, the objective of MIC is to classify bags, where each bag is a collection of instances. Let us see here other multi-instance problems. A few authors [36], [37] have addressed the problem of instance classification given a multi-instance setting, i.e., when the labels of the training set are assigned to bags, and not to individual instances. We must note that this problem is different from the one of our survey, and therefore the conclusions obtained with our experimental analysis do not necessarily hold for this other problem.</paragraph><paragraph>Other multi-instance problems include multi-instance regression [38], [35], [39], multi-instance multi-label learning [40] and multi-instance clustering [35], [41]. The first one is very similar to MIC, where the main difference is that the label associated to the bags is a real value instead of a discrete one, and thus the objective is to learn a real-valued function instead of a discrete-valued classifier. As analyzed in [10], the types of solutions can be categorized in a similar manner to the one employed for MIC (including instance-level solutions using the standard MI assumption [38], embedded-space solutions [35] and bag-space solutions [39]). However, again, the problem is different from MIC and requires its own specific analysis. Another problem is the multi-instance multi-label learning. This is an extension of MIC where each bag can receive several labels, i.e., it can belong to several classes at the same time. Zhou and Zhang [40] extensively analyze this problem and propose several solutions, including some that transform it into a series of MIC problems, we refer also to [10] for a short review. Finally, in multi-instance clustering the objective is to obtain an unsupervised bag-level classification [35], [41]. This type of problem is again out of the scope of our review.</paragraph></section><section label="9"><section-title>Experimental evaluation</section-title><section label="9.1"><section-title>Databases</section-title><paragraph>We implemented at least one method for each sub-paradigm described in the previous sections. In Table 1 we can find the acronyms of the implemented methods.</paragraph><paragraph>We used eight databases from four different fields: Drug Discovery (DD), Information Retrieval (IR), Audio Analysis (AA) and Computer Vision (CV). Table 2 lists the databases and their characteristics. Many of the databases are standard and well-known: the Musk1 and Musk2 databases [42] are the most popular ones in the MIC literature, since the early work of Dietterich et al. [13]. The task in these databases is to classify molecules as positive (Musk) or negative (Non-Musk) (see [13]). Text1 and Text2 were introduced in [17] and have also become standard since then. In this case, the task is to classify a series of documents as belonging or not to a predefined category. Each document is represented as a bag of feature vectors based on an analysis of the text in the document (see [17]). Corel1 was introduced in [1] and has also been used by many authors since then. In this case, the task is to classify images into 20 predefined categories, and each image is represented as a bag of instances. In order to convert the Corel data into binary classification problems, we use the well-known one-against-all strategy. Corel2 was created by us using exactly the same images and the same categories as Corel1, and representing the instances with almost the same type of feature vector. The only difference is that the number of instances per bag is more than two orders of magnitude larger than in the original database. This allowed us to evaluate the change in performance of the different methods when the number of instances increases. Finally, Speaker was created by us based on the audio database available in the website [43] and introduced in the paper [44]. In this case, the task is to identify the gender of the speaker, using as data an audio recording of a sentence spoken by the person. The audio recording is represented as a bag of feature vectors, using a standard representation in the audio processing community [45].</paragraph><section label="9.1.1"><section-title>Synthetic database</section-title><paragraph>We also used synthetic data sets for studying the behavior under controlled conditions. A synthetic data set is generated using instance vectors {a mathematical formula}x→i that are randomly generated from two Gaussian mixtures: one for the positive class, and another for the negative one. We chose the Gaussian mixture as the underlying distribution of the instances because it can approximate any other distribution if a sufficiently large number of components is used.</paragraph><paragraph>The parameters for generating the data set are the number of instances per bag n, the dimensionality of the instances d, the number of Gaussian components for the positive distribution {a mathematical formula}kp, and for the negative one {a mathematical formula}kn. By default, we used {a mathematical formula}kp=8 and {a mathematical formula}kn=32, i.e., we consider that the negative distribution is more complex and represents the “rest-of-classes”. The default values for the rest of parameters were {a mathematical formula}n=32 and {a mathematical formula}d=2. For each configuration of parameters, we randomly generate ten different data sets and report the average classification hit-rate.</paragraph></section></section><section label="9.2"><section-title>Experimental set up</section-title><paragraph>For the Musk1, Musk2, Text1 and Text2 databases, we used a ten-fold validation approach, as the majority of authors [1]: each round we take 90% of the data set for training and the remaining 10% for testing, and this is repeated ten times in order to test with all the bags. Furthermore, each ten-fold is repeated ten times (each time with a different random splitting), i.e., with a total of 100 rounds of training and testing. For the Speaker and Corel1 databases we used a two-fold validation approach which was repeated ten times, i.e., with a total of 20 rounds of training and testing. In all the cases the average classification accuracy is reported. Finally, for Corel2, we just used one round with 50% of the data in the training set and the remaining 50% in the test set. This last setting is frequently employed for databases of this type, as they have a large number of bags.</paragraph><paragraph>In the fifth column of Table 2 we indicate the number of bags in the positive and negative class for each data set. If we want to calculate the number of bags in the test set only, we need to multiply by 0.1 (for example, in the musk data set we have approximately 5 positive bags and 5 negative bags in the test set), except for the speaker, Corel1 and Corel2 data sets where we need to multiply by 0.5 (for example, in the Corel1 data set we have 50 positive bags and 950 negative bags in the test set).</paragraph><paragraph>The Text1 and Text2 databases have a very large dimensionality (Table 2), which makes it infeasible to apply methods such as H-BoW with EM (due to its little robustness against high dimensionality) and EM-DD due to its computational cost. In order to reduce the dimensionality of the data, PCA could not be applied directly to the original 66K dimensions, due to memory issues with the covariance matrix. We selected the 3000 dimensions that have higher variance, and then applied PCA to reduce to only 65 dimensions.{sup:6}</paragraph><paragraph>Many of the evaluated methods require the use of a standard supervised classifier, such as SVM or AdaBoost, as part of the algorithm. With SVM, we used by default an RBF kernel (also called extended Gaussian kernel [29]): {a mathematical formula}K(x→,y→)=exp(−1γD(x→,y→)), where {a mathematical formula}D(x→,y→) is a distance function between two vectors {a mathematical formula}x→ and {a mathematical formula}y→. By default, the Euclidean distance was used, i.e., {a mathematical formula}D(x→,y→)=‖x→−y→‖. The parameter γ was selected, together with the penalty cost C of SVM, using a 5-fold cross-validation as suggested in [46]. By default, SVM was used with vectors scaled to {a mathematical formula}[0,1] as suggested in [46]. This default setting was changed for EMD with SVM, where we use the EMD distance function, as explained in Section 5, and without scaling the vectors to {a mathematical formula}[0,1]. In the H-BoW algorithm, that use histogram vectors, the {a mathematical formula}χ2 distance function is employed without scaling the vectors. Finally, the IMK method used a particular kernel, described in Section 7.5, without scaling the vectors. Regarding AdaBoost, we used the version described in [47] with decision stumps and with a very high number of rounds, {a mathematical formula}K=10000, which assures a good performance.</paragraph><paragraph>Finally, regarding the classifier associated with the Vocabulary-based methods, we evaluated both AdaBoost and SVM in all the cases. We heuristically chose SVM as the best performing one for all the methods except for H-BoW (EM), where we chose AdaBoost because it provided slightly better results.</paragraph><section label="9.2.1"><section-title>Implementation details for Vocabulary-based methods</section-title><paragraph>In this work we use a common implementation for all the Vocabulary-based methods. We use six different vocabulary sizes {a mathematical formula}M1,…,M6 which were obtained as follows. First, the maximum vocabulary size {a mathematical formula}M6 was chosen as half the total number of instances {a mathematical formula}Ninst of the database, i.e., {a mathematical formula}M6=⌊Ninst2⌋, but without exceeding a maximum size of {a mathematical formula}M6=2048 (in case of very large data sets), in order to limit the computational cost. Based on the maximum size {a mathematical formula}M6, the rest of the sizes were obtained by successively dividing by powers of two: {a mathematical formula}M1=M625,…,M5=M62. Table 3 indicates the minimum and maximum vocabulary sizes for each database.</paragraph><paragraph>For each vocabulary size {a mathematical formula}Mi,i=1,…,6, we compute ten vocabularies by using different random initializations in the clustering algorithm. In order to combine all these vocabularies, we saw experimentally that a good approach consists of concatenating the vectors (i.e., the histograms in the case of H-BoW) obtained from all the vocabularies. We tested other settings and the results obtained were similar. The important thing is to use several vocabulary sizes and several initializations. In [19] we include a detailed algorithmic description of our implementation.</paragraph><paragraph>Regarding the D-BoW algorithm explained in Section 7.4.1, σ is chosen heuristically as {a mathematical formula}σ=1, which produces good results. As explained in Section 7.4.1, we observed slightly better results when we use a similarity {a mathematical formula}sj instead of a distance {a mathematical formula}dj, as the values of {a mathematical formula}sj(x→) are constrained to the interval {a mathematical formula}[0,1] and rapidly saturate to 0 for far away instances. Therefore, this is the approach reported in the experimental section.</paragraph></section></section><section label="9.3"><section-title>Impact of using bag-level information: evaluation on synthetic data</section-title><paragraph>Probably the most important difference between paradigms is at what level the discriminant information is extracted: at the instance-level, or at the bag-level. In this section we evaluate the performance of the methods in a situation where bag-level information is necessary. In order to do so, we use synthetic data similar to the toy example in Fig. 5, that has been used along the work as a running example. In particular, we evaluate the situation where positive bags have N classes of instances co-occurring at the same time in the bag. As N increases, the information provided by each individual class of instances is less discriminative, and we must consider the combination or co-occurrence of several classes in the bag, which can only be done with bag-level information.</paragraph><paragraph>In Fig. 11 we evaluate the performance as a function of N, where we model each class of instances as a Gaussian. Regarding the negative bags, we used a separate set of Gaussian classes of instances. Furthermore, we used a constant high value {a mathematical formula}M=32 for the number of instances. This is done in order to build negative bags with heterogeneous data. This permits simulating a scenario that is common in real problems: while positive objects usually present homogeneous characteristics, negative objects belong to the “rest-of-the-world”, i.e., they form a heterogeneous class.</paragraph><paragraph>The results in Fig. 11(a)–(b) confirm the following remarks done along the review. First, the IS methods perform poorly, having a high drop in performance as the number of classes N that co-occur increases. This is due to the fact that, in this situation, global bag-level information becomes necessary. Second, the type of assumption, SMI or Collective, does not characterize the performance of the methods in this situation, as all the IS methods (MI-SVM, EM-DD and Wrapper MI) have similar low performance. Third, BS methods perform well as long as the distance function fully exploits information about the whole bag. In this sense, all the distance functions are good except for the min Hausdorff, as we commented in Section 5.2. In Fig. 11(b) we show that the EMD distance performs well while the min Hausdorff does not, and in Section 9.5 we see that all the distances perform well, except for the min Hausdorff.</paragraph><paragraph>Fourth, ES methods perform well as long as the embedding conserves the relevant information. In this sense, H-BoW (a Vocabulary-based method) performs well (see Fig. 11(a)), while Simple MI (a method not based on vocabularies) is not robust (see Fig. 11(b)). These observations are confirmed later with real data.</paragraph><paragraph>In conclusion, none of the IS methods are robust in a situation where bag-level information is necessary. On the other hand, both the BS and ES paradigms provide an appropriate framework for dealing with this situation. However, the BS paradigm depends on defining a distance function that exploits information from the whole bag, and the ES paradigm depends on defining a mapping that conserves the relevant information of the bag. In this sense, reducing the information to a simple average per bag (as in Simple MI) is not robust.</paragraph><paragraph>In the rest of the experimental section we evaluate if these conclusions are confirmed with real data.</paragraph></section><section label="9.4"><section-title>Performance of IS methods</section-title><paragraph>In order to see if the results on synthetic data are confirmed on real data we should take into account especially the Corel1 and Corel2 databases. These databases define image classification problems similar to the one discussed in Section 3.3 and displayed in Fig. 6. As we saw in that section, in this type of database the bags are characterized by the co-occurrence of several classes of instances, so that using bag-level information becomes fundamental.</paragraph><paragraph>Fig. 12 shows the performance of the IS methods using a bar chart.</paragraph><paragraph>Fig. 12(a) shows the results of all the IS methods, and also includes the results of the EMD+SVM method. The latter method will be included as a baseline in all the comparisons, as this method provides consistently good results for all the databases. We can see that the performance is dramatically low in Corel1 and Corel2 for all the IS methods, as compared with EMD+SVM. As we see later, this behavior is unique to IS methods, i.e., other paradigms do not show this dramatic drop in performance (see Figs. 13(a) and 14(a)). Looking at the results in the rest of the databases, the difference is not so dramatic. But still, the IS methods do not compare well in the rest of databases either. Section 9.7 provides ranking results with statistical significance tests, which show that IS methods perform significantly worse in general, i.e., taking into account all the databases.</paragraph><paragraph>Fig. 12(b) focus on the performance of two IS methods: Wrapper MI, which follows the Collective assumption, and MI-SVM, which follows the SMI assumption. MI-SVM performs better in some databases and Wrapper MI in some others. It does not seem that the SMI assumption makes any clear difference, for example in databases such as Musk2 where it is considered by many authors that the SMI assumption applies well. These conclusions are confirmed by synthetic results as we will see in Section 9.8.</paragraph></section><section label="9.5"><section-title>Performance of BS methods</section-title><paragraph>Fig. 13(a) shows the results of BS methods where the distance function is EMD, Chamfer and the one of Gartner et al. [14]. In all the cases SVM is used. We can see that the performance is comparable and there are no big drops in accuracy for any database. Fig. 13(b) shows the results when the min Hausdorff distance is used. Results for both SVM and Citation-KNN classifiers are shown. We see that the performance drops very significantly for Corel1 and Corel2 databases, in both cases. This confirms the results obtained in Section 9.3 under synthetic conditions. In addition to this, we see that min Hausdorff is not appropriate for databases such as Speaker where there is a very large number of instances per bag (see Table 2). The reason is simple: min Hausdorff only considers the matching between the closest two instances, and does not consider whether the rest of the instances are similar or not. Therefore, when we have a large number of instances in the bag we miss a lot of information. The conclusion again is that, while the BS paradigm provides an appropriate framework for dealing with global, bag-level information, the distance function must be defined in an appropriate way.</paragraph></section><section label="9.6"><section-title>Performance of ES methods</section-title><paragraph>Fig. 14(a) shows the results of Vocabulary-based ES methods, including the EMD+SVM for reference purposes. We can see that the performance is comparable and there are no big drops in accuracy for any database. Fig. 14(b) shows the performance of the Simple MI method, which is an ES method not based on vocabularies. We can see again that the performance drops heavily for the Corel1 and Corel2 databases. As in the BS case, the results confirm that, while the ES paradigm provides an appropriate framework for dealing with global, bag-level information, the mapping function must be defined in an appropriate way. In this sense we see that the Vocabulary-based family provides an appropriate framework.</paragraph></section><section label="9.7"><section-title>Global ranking</section-title><paragraph>In order to summarize the performance on all the databases, we computed the mean rank and computed the Meneyi test for evaluating the statistical significance, following the work of [48] which is in turn based on the recommendations in [49]. As discussed in these papers, very rarely a classification method ranks first or has a constant ranking across different databases, and the only way to statistically test if one method is superior to others is to evaluate the average rank of each method across many databases. In particular [49] recommends to use at least ten databases. Therefore, we added four more databases in order to perform the statistical significance test, but restricted the number of evaluated methods to eight. The new databases are fox, tiger, elephant, and Text3 which are well-known and were proposed in [17]. Here we focused on the performance of ten methods for computational reasons.</paragraph><paragraph>Fig. 15(b) shows the mean rank of the methods evaluated in the eleven databases, and the result of the statistical significance test: those methods linked with a blue bar cannot be said to be statistically different (i.e., there is no sufficient evidence given the number of databases and variability of the performance). However, the test shows that there is sufficient statistical evidence that all of the IS methods are significantly worse than the EMD+SVM method, as they tend to rank in the lowest positions in all the databases.</paragraph></section><section label="9.8"><section-title>Evaluation of SMI assumption</section-title><paragraph>We also evaluated whether the IS methods based on the SMI assumption are successful in a situation where this assumption holds. Remember that the SMI assumption states that a bag must be classified as positive if and only if it contains at least one positive instance. This means that these methods should be able to classify the bags even if they contain a small proportion of positive instances, being the rest of instances negative. This is evaluated in Fig. 16(a). This figure shows the performance of two SMI-based IS methods (MI-SVM and EM-DD) and an ES method (H-BoW). Clearly, the performance of all the methods is very low when the proportion of instances is small, at the level of random chance. We also show the confidence intervals as vertical bars. The intervals are overlapped in the left part of the curve, meaning that difference in performance between methods is not statistically significant when the proportion of relevant instance is small.</paragraph><paragraph>Fig. 16(b) evaluates the performance of Gartner et al.ʼs method [14]. As explained in Section 5, the authors prove that, if the instances are separable in the space induced by the instance-level kernel k, then the resulting bags are separable in the space induced by the bag-level kernel K defined in Eq. (9). To test this, we generated linearly separable instances and we used a linear kernel k in order to obtain the bag kernel K. Then, we tested the performance with varying proportions of positive instances in the positive bag. Fig. 16(b) shows the results, for various choices of the parameter p in Eq. (9). The best performance is obtained with {a mathematical formula}p=1, which is consistent with the paper that suggests {a mathematical formula}p=1 when the instances are completely separable. However, the method has poor performance when the proportion of positive instances is low. This could be due to the size of the training set, which should be very large for obtaining good performance. However, we used a training set with one thousand bags, which is ten times larger than the one needed for perfect classification of instances (we only needed to train with 100 instances), and still the performance was poor. We did not test larger training sets to keep the computational cost reasonably low, and also because, in practice, many real MIC problems are defined with much smaller sets.</paragraph><paragraph>Given these results, we can conclude that the performance of the methods does not seem to be affected by the fact that they follow or not the SMI assumption. This has been shown here with synthetic data and also above with real data. In contrast, the methods are clearly more affected by whether the discriminant information is extracted at the instance or at the bag level. In this sense, the IS methods following the SMI assumption perform similarly to other IS methods that do not follow it (and all of them have a low performance in general), while the BS method of Gartner et al. performs similarly to other BS methods.</paragraph></section><section label="9.9"><section-title>Computational cost</section-title><paragraph>Another variable that should be considered when choosing a method is its cost. Let M be the number of bags in the training set, N the average number of instances per bag, and D the dimensionality of the instances. The cost of the BS paradigm is dominated by the computation of the distance function, which is {a mathematical formula}O(N2×M2×D), i.e., for every pair of bags X and Y in the training set we must compute the distance between any instance from X to any other from Y, which has cost {a mathematical formula}N2×D. This cost quickly becomes very high when both the number of instances per bag N and the number of bags M is large. This cost is even higher if we use the EMD distance, as the algorithm is based on optimization and has cost {a mathematical formula}O(N3×M2×D) in the worst case. This is problematic, for example in problems such as Content-Based Image Retrieval where there are thousands of instances per bag and thousands of bags. For the Corel2 database we had to reduce the number of instances per bag by taking only 32 cluster means per bag. The consequence is that the accuracy decreases.</paragraph><paragraph>Regarding the ES paradigm, the cost is divided into first computing the vocabulary and then mapping every bag to a single vector. Regarding the latter, the cost is typically {a mathematical formula}O(M×N×K×D), where K is the size of the vocabulary. This is, for every bag X, we compute the distance from every instance in X to every prototype in the vocabulary, which typically involves a distance between two D-dimensional vectors, although it could be higher depending on the particular algorithm. Note that the BS cost is quadratic while the ES one is linear. Regarding the construction of the vocabulary, the cost typically has the same magnitude as the mapping, i.e., {a mathematical formula}O(niter×M×N×K×D), for algorithms such as K-Means (similar for GMM-EM if we use diagonal covariance matrices), where {a mathematical formula}niter is the number of iterations. Other algorithms might have other costs.</paragraph><paragraph>Regarding the IS methods, the cost is dominated by the fact that the instance-level classifier is estimated with a very large training set of instances. Note that each bag might contain a large number of instances, so that the training set used in IS methods is much larger than the one of BS or ES methods. In particular the IS methods are not practical when the number of instances per bag is very large (e.g., Speaker or Corel2 databases) and the learning algorithm is SVM. In this case we have to reduce the number of instances by clustering. Apart from these considerations, the cost of the IS algorithms depends also on the optimization algorithm used to identify potential positive instances. This is very dependent on the particular algorithm. In practice, the ES paradigm was significantly less expensive and scaled much better when N or M increases than the other two paradigms.</paragraph><paragraph>Another aspect related with the computational cost is the complexity of the learning algorithms. This is discussed in more detail in our technical report [19], but let us explain briefly why the implemented methods have a similar underlying complexity. This is due to the fact that we use the same learning algorithm (SVM with a standard RBF kernel) for almost all the methods, and therefore the learning complexity is similar. In this sense, the difference between the three paradigms (IS, BS and ES) does not lie in the specific learning algorithm, but in the way the information is extracted and introduced to SVM. This is discussed in more detail in our technical report [19].</paragraph></section><section label="9.10"><section-title>Summary of results</section-title><paragraph>The results consistently show the following conclusions. First, using global, bag-level information becomes fundamental in order to obtain accuracy under different MI data. This is confirmed by both synthetic and real data. In this sense, both the BS and ES paradigms present appropriate frameworks for designing MIC methods. In contrast, the IS paradigm provides non-robust methods which have big drops in performance. Overall, statistical tests show that they are significantly worse than the other paradigms.</paragraph><paragraph>Second, the classical SMI-based paradigm does not seem to provide a good framework for characterizing methods in terms of performance. In contrast, the fact that a method is IS or BS does have a clear impact on the performance. In this sense, the result shows a similar behavior for all IS methods, regardless of whether they follow the SMI assumption or not. The same happens with the BS methods. This can be seen both in real and synthetic databases.</paragraph><paragraph>Third, both BS and ES provide an appropriate framework for exploiting information from the whole bag. However, an appropriate distance function must be defined if we use BS, and an appropriate mapping must be defined if we use ES. In this sense, we have shown that the distance function must evaluate to what extent every instance in one bag has a similar instance in the other bag. The intuition behind this is discussed in Section 7.1. In fact, this idea is followed, in one way or the other, by all the distance functions except for the min Hausdorff. Regarding the ES methods, we have shown that using a Vocabulary provides a better framework than other types of embedding such as Simple MI. The intuition behind the Vocabulary-based paradigm is discussed in Section 7.1.</paragraph><paragraph>Fourth, BS methods become infeasible when either the number of instances per bag or the number of bags are large. In this situation we should consider using vocabulary-based algorithms.</paragraph></section></section><section label="10"><section-title>Conclusions</section-title><paragraph>This work presents the first analysis of the MIC methods that include both a complete review and empirical comparison of the different paradigms. In our analysis, we obtain a compact categorization of the methods based on how they manage the information. The performed categorization pursues two important objectives: first, to provide a clear picture about the existing approaches, and second to allow an experimental analysis of the different categories of methods, in such a way that methods of the same category perform similarly under different situations.</paragraph><paragraph>Regarding the empirical study, we provided an extensive analysis that included fourteen algorithms from the three paradigms. In order to test these algorithms, we used seven databases from four different domains of knowledge. We also included a synthetic database where we were able to study the behavior under controlled conditions. Altogether, the results show clearly that using global, bag-level information leads to superior performance. In this sense, both BS and ES methods can be used, but taking into account certain important design considerations: in the BS methods we must consider the similarity between all the instances, and in the ES method using a Vocabulary-based mapping is important. This confirmed the analysis provided in the review section, where these design considerations were justified. Finally, we saw that, under certain conditions, the BS approaches become computationally very expensive and, therefore, using vocabulary-based ES methods is a better choice for certain databases.</paragraph><paragraph>Summarizing, in addition to provide a clear picture of the type of existent MIC solutions, we also studied important guidelines for obtaining MIC methods that are robust in different databases. In this sense, we must note that, given a new MIC problem we are faced with, it is difficult to know the type of method that is most suitable for it. Therefore, finding a set of methods that have a consistently good performance in many situations is important. Furthermore, this analysis can also help to improve the design of future algorithms by providing a basic framework. In this sense, it becomes clear in our analysis that the discriminative classifier must be based on global information from the whole bag, and it also becomes clear how this can be done appropriately. Furthermore, we showed that certain design choices such as following the well-known SMI assumption do not seem to have an impact on the performance, and that it has more impact to focus on new effective ways to extract global information in such a way that the interrelations between instances inside the bag are characterized.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>We would like to thank Dr. Razvan Bunescu for providing the implementation of methods in [20], and Dr. Antonio M. López for valuable feedback. We would also like to thank the anonymous reviewers for their valuable comments. This work was supported by the fellowshipRYC-2008-03789 and the spanish projectTRA2011-29454-C03-01.</paragraph></acknowledgements><references><reference label="[1]"><authors>Y. Chen,J. Bi,J.Z. Wang</authors><title>Miles: Multiple-instance learning via embedded instance selection</title><host>IEEE Trans. Pattern Anal. Mach. Intell.28 (12)(2006) pp.1931-1947</host></reference><reference label="[2]"><authors>P. Auer,R. Ortner</authors><title>A boosting approach to multiple instance learning</title><host>Proc. of European Conference on Computer Vision(2004) pp.63-74</host></reference><reference label="[3]"><authors>H. Lee,A. Battle,R. Raina,A.Y. Ng</authors><title>Efficient sparse coding algorithms</title><host>Proc. of Neural Information Processing Systems(2007) pp.801-808</host></reference><reference label="[4]"><authors>J.C. van Gemert,C. Veenman,A. Smeulders,J. Geusebroek</authors><title>Visual word ambiguity</title><host>IEEE Trans. Pattern Anal. Mach. Intell.32 (7)(2010) pp.1271-1283</host></reference><reference label="[5]"><authors>J. Sivic,A. Zisserman</authors><title>Video google: A text retrieval approach to object matching in videos</title><host>Proc. of IEEE Computer Vision and Pattern Recognition(2003) pp.1470-1477</host></reference><reference label="[6]"><authors>S. Boughorbel,J.-P. Tarel,N. Boujemaa</authors><title>The intermediate matching kernel for image local features</title><host>Proc. of International Joint Conference on Neural Networks(2005) pp.889-894</host></reference><reference label="[7]"><authors>Z.-H. Zhou,M.-L. Zhang</authors><title>Solving multi-instance problems with classifier ensemble based on constructive clustering</title><host>Knowl. Inf. Syst.11 (2)(2007) pp.155-170</host></reference><reference label="[8]"><authors>N. Weidmann,E. Frank,B. Pfahringer</authors><title>A two-level learning method for generalized multi-instance problems</title><host>Proc. of European Conference on Machine Learning(2003) pp.468-479</host></reference><reference label="[9]">J. FouldsLearning instance weights in multi-instance learningMasterʼs thesis<host>(2008)University of Waikato</host></reference><reference label="[10]"><authors>J. Foulds,E. Frank</authors><title>A review of multi-instance learning assumptions</title><host>Knowl. Eng. Rev.25 (1)(2010) pp.1-25</host></reference><reference label="[11]">L. DongA comparison of multi-instance learning algorithmsMasterʼs thesis<host>(2006)University of Waikato</host></reference><reference label="[12]"><authors>J. Amores</authors><title>Vocabulary-based approaches for multiple-instance data: A comparative study</title><host>Proc. of International Conference on Pattern Recognition(2010) pp.4246-4250</host></reference><reference label="[13]"><authors>T.G. Dietterich,R.H. Lathrop,T. Lozano-Pérez</authors><title>Solving the multiple-instance problem with axis-parallel rectangles</title><host>Artificial Intelligence89 (1997) pp.31-71</host></reference><reference label="[14]"><authors>T. Gärtner,P.A. Flach,A. Kowalczyk,A.J. Smola</authors><title>Multi-instance kernels</title><host>Proc. of International Conference on Machine Learning(2002) pp.179-186</host></reference><reference label="[15]"><authors>O. Maron,T. Lozano-Pérez</authors><title>A framework for multiple-instance learning</title><host>Proc. of Neural Information Processing Systems(1998) pp.570-576</host></reference><reference label="[16]"><authors>Q. Zhang,S.A. Goldman</authors><title>EM-DD: An improved multiple-instance learning technique</title><host>Proc. of Neural Information Processing Systems(2001) pp.1073-1080</host></reference><reference label="[17]"><authors>S. Andrews,I. Tsochantaridis,T. Hofmann</authors><title>Support vector machines for multiple-instance learning</title><host>Proc. of Neural Information Processing Systems(2003) pp.561-568</host></reference><reference label="[18]">M. OdedLearning from ambiguityPhD thesis<host>(May 1998)Massachusetts Institute of Technology</host></reference><reference label="[19]">J. AmoresNotes on “Multiple instance classification: Review, taxonomy and comparative study”Tech. rep.<host>(2012)Universitat Autònoma de Barcelona</host><host>http://www.cvc.uab.es/~jaume/additionalNotes.pdf</host></reference><reference label="[20]"><authors>R. Bunescu,R. Mooney</authors><title>Multiple instance learning for sparse positive bags</title><host>Proc. of International Conference on Machine Learning(2007) pp.105-112</host></reference><reference label="[21]">X. XuStatistical learning in multiple instance problemsMasterʼs thesis<host>(2003)University of Waikato</host></reference><reference label="[22]">E. Frank,X. XuApplying propositional learning algorithms to multi-instance dataTech. rep.<host>(2003)Department of Computer Science, University of Waikato</host></reference><reference label="[23]"><authors>O. Mangasarian,E. Wild</authors><title>Multiple instance learning via successive linear programming</title><host>J. Optim. Theory Appl.137 (3)(2008) pp.555-568</host></reference><reference label="[24]"><authors>J. Wang,J.-D. Zucker</authors><title>Solving the multiple-instance problem: A lazy learning approach</title><host>Proc. of International Conference on Machine Learning(2000) pp.1119-1125</host></reference><reference label="[25]"><authors>J. Zhang,M. Marszalek,S. Lazebnik,C. Schmid</authors><title>Local features and kernels for classification of texture and object categories: A comprehensive study</title><host>Int. J. Comput. Vis.73 (2)(2007) pp.213-238</host></reference><reference label="[26]"><authors>S. Belongie,J. Malik,J. Puzicha</authors><title>Shape matching and object recognition using shape contexts</title><host>IEEE Trans. Pattern Anal. Mach. Intell.24 (24)(2002) pp.509-522</host></reference><reference label="[27]"><authors>Y. Rubner,C. Tomasi,L.J. Guibas</authors><title>The earth moverʼs distance as a metric for image retrieval</title><host>Int. J. Comput. Vis.40 (2)(2000) pp.99-121</host></reference><reference label="[28]"><authors>Z.-H. Zhou,Y.-Y. Sun,Y.-F. Li</authors><title>Multi-instance learning by treating instances as non i.i.d. samples</title><host>Proc. of International Conference on Machine Learning(2009) pp.1249-1256</host></reference><reference label="[29]"><authors>O. Chapelle,P. Haffner,V. Vapnik</authors><title>Support vector machines for histogram-based image classification</title><host>IEEE Trans. Neural Netw.10 (5)(1999) pp.1055-1064</host></reference><reference label="[30]"><authors>E. Nowak,F. Jurie,B. Triggs</authors><title>Sampling strategies for bag-of-features image classification</title><host>Proc. of European Conference on Computer Vision(2006) pp.490-503</host></reference><reference label="[31]"><authors>A. Opelt,A. Pinz,M. Fussenegger,P. Auer</authors><title>Generic object recognition with boosting</title><host>IEEE Trans. Pattern Anal. Mach. Intell.28 (3)(2006) pp.416-431</host></reference><reference label="[32]"><authors>T. Serre,L. Wolf,S. Bileschi,M. Riesenhuber,T. Poggio</authors><title>Robust object recognition with cortex-like mechanisms</title><host>IEEE Trans. Pattern Anal. Mach. Intell.29 (3)(2007) pp.411-426</host></reference><reference label="[33]"><authors>Y. Chen,J.Z. Wang</authors><title>Image categorization by learning and reasoning with regions</title><host>J. Mach. Learn. Res.5 (2004) pp.913-939</host></reference><reference label="[34]"><authors>S. Scott,J. Zhang,J. Brown</authors><title>On generalized multiple-instance learning</title><host>Int. J. Comput. Intell. Appl.5 (1)(2003) pp.21-35</host></reference><reference label="[35]"><authors>M.-L. Zhang,Z.-H. Zhou</authors><title>Multi-instance clustering with applications to multi-instance prediction</title><host>Appl. Intell.31 (1)(2009) pp.47-68</host></reference><reference label="[36]"><authors>A. Vezhnevets,J.M. Buhmann</authors><title>Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning</title><host>IEEE Proc. of Computer Vision and Pattern Recognition(2010) pp.3249-3256</host></reference><reference label="[37]"><authors>B. Settles,S. Ray,M. Craven</authors><title>Multiple-instance active learning</title><host>Proc. of Neural Information Processing Systems(2008) pp.1289-1296</host></reference><reference label="[38]"><authors>S. Ray,D. Page</authors><title>Multiple instance regression</title><host>Proc. of International Conference on Machine Learning(2001) pp.425-432</host></reference><reference label="[39]"><authors>R. Amar,D. Dooly,S. Goldman,Q. Zhang</authors><title>Multiple-instance learning of real-valued data</title><host>Proc. of International Conference on Machine Learning(2001) pp.3-10</host></reference><reference label="[40]"><authors>Z.-H. Zhou,M.-L. Zhang</authors><title>Multi-instance multi-label learning with application to scene classification</title><host>Proc. of Neural Information Processing Systems(2006) pp.1609-1616</host></reference><reference label="[41]"><authors>H. Kriegel,A. Pryakhin,M. Schubert</authors><title>An EM-approach for clustering multi-instance objects</title><host>Pacific-Asia Conference on Knowledge Discovery and Data Mining(2006) pp.139-148</host></reference><reference label="[42]"><authors>A. Frank,A. Asuncion</authors><title>UCI machine learning repository</title><host>http://archive.ics.uci.edu/ml(2010)</host></reference><reference label="[43]"><authors>C. Sanderson</authors><host>http://www.itee.uq.edu.au/~conrad/vidtimit/</host></reference><reference label="[44]"><authors>C. Sanderson,B.C. Lovell</authors><title>Multi-region probabilistic histograms for robust and scalable identity inference</title><host>Lecture Notes in Computer Sciencevol. 5558 (2009) pp.199-208</host></reference><reference label="[45]"><authors>D.A. Reynolds,T.F. Quatieri,R.B. Dunn</authors><title>Speaker verification using adapted Gaussian mixture models</title><host>Digital Signal Processing10 (2000) pp.19-41</host></reference><reference label="[46]">C.-W. Hsu,C.-C. Chang,C.-J. LinA practical guide to support vector classificationTech. rep.<host>(2003)National Taiwan University</host></reference><reference label="[47]"><authors>P. Viola,M.J. Jones</authors><title>Robust real-time face detection</title><host>Int. J. Comput. Vis.57 (2)(2004) pp.137-154</host></reference><reference label="[48]"><authors>P. Dollar,C. Wojek,B. Schiele,P. Perona</authors><title>Pedestrian detection: An evaluation of the state of the art</title><host>IEEE Trans. Pattern Anal. Mach. Intell.34 (4)(2012) pp.743-76110.1109/TPAMI.2011.155</host></reference><reference label="[49]"><authors>J. Demšar</authors><title>Statistical comparisons of classifiers over multiple data sets</title><host>J. Mach. Learn. Res.7 (2006) pp.1-30</host></reference></references><footnote><note-para label="1">As we will see, any distance function {a mathematical formula}D(X,Y) can be transformed into a kernel function {a mathematical formula}K(X,Y). Similarly, any kernel function can be transformed into a distance function.</note-para><note-para label="2">Indeed, a kernel function {a mathematical formula}K(X,Y) defines an implicit mapping {a mathematical formula}ϕ(X)↦v→. The function ϕ maps the original bag space (where the bag X lives) into a new vector space, where the vector {a mathematical formula}v→ lives [14].</note-para><note-para label="3">According to [10] some methods from the BS and ES paradigms follow other assumptions in addition to the mentioned ones, we refer to [10] for a review according to this criteria.</note-para><note-para label="4">Note that the fact that the bags are separable does not guarantee that an SVM will find an accurate hyperplane that separates test bags with low error. In the experimental section we present an analysis of this fact.</note-para><note-para label="5">The parameter γ also subsumes the constant p of Eq. (9) as explained in [14].</note-para><note-para label="6">We also tried higher dimensionalities, but the results were not better, so we selected a low dimensionality for computational efficiency.</note-para></footnote></root>