<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370214001398</url><title>Anticipatory action selection for human–robot table tennis</title><authors>Zhikun Wang,Abdeslam Boularias,Katharina Mülling,Bernhard Schölkopf,Jan Peters</authors><abstract>Anticipation can enhance the capability of a robot in its interaction with humans, where the robot predicts the humans' intention for selecting its own action. We present a novel framework of anticipatory action selection for human–robot interaction, which is capable to handle nonlinear and stochastic human behaviors such as table tennis strokes and allows the robot to choose the optimal action based on prediction of the human partner's intention with uncertainty. The presented framework is generic and can be used in many human–robot interaction scenarios, for example, in navigation and human–robot co-manipulation. In this article, we conduct a case study on human–robot table tennis. Due to the limited amount of time for executing hitting movements, a robot usually needs to initiate its hitting movement before the opponent hits the ball, which requires the robot to be anticipatory based on visual observation of the opponent's movement. Previous work on Intention-Driven Dynamics Models (IDDM) allowed the robot to predict the intended target of the opponent. In this article, we address the problem of action selection and optimal timing for initiating a chosen action by formulating the anticipatory action selection as a Partially Observable Markov Decision Process (POMDP), where the transition and observation are modeled by the IDDM framework. We present two approaches to anticipatory action selection based on the POMDP formulation, i.e., a model-free policy learning method based on Least-Squares Policy Iteration (LSPI) that employs the IDDM for belief updates, and a model-based Monte-Carlo Planning (MCP) method, which benefits from the transition and observation model by the IDDM. Experimental results using real data in a simulated environment show the importance of anticipatory action selection, and that POMDPs are suitable to formulate the anticipatory action selection problem by taking into account the uncertainties in prediction. We also show that existing algorithms for POMDPs, such as LSPI and MCP, can be applied to substantially improve the robot's performance in its interaction with humans.</abstract><keywords>Anticipation;Intention-driven dynamics model;Partially observable Markov decision process;Policy iteration;Monte-Carlo planning</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Humans possess the ability to coordinate their actions with others in joint activities, where anticipation plays an important role to integrate the prediction of the others' actions in one's own action planning [38]. Such ability of anticipation is also crucial in Human–Robot Interaction (HRI), where an anticipatory robot needs to coordinate for its actions while interacting with humans [18], in both competitive and cooperative contexts. An anticipatory robot usually relies on a predictive model of the environment, especially its human partners, which “allows it to change state at an instant in accord with the model's predictions pertaining to a later instant” [36]. Predicting the underlying intention and the future actions of the human partners has been extensively studied in robotics (e.g. [23], [17], [22], [53], [5]). This prediction capability offers the promise of anticipatory robots that incorporate anticipatory action selection in their planning.</paragraph><paragraph>In this article, we focus on the anticipatory action selection based on a prediction model. Specifically, the robot chooses an action from a repertoire of motor skills based on the prediction of the human partner's intention. One important problem for the anticipatory action selection is prediction uncertainties that naturally arose due to the complexity and stochasticity of human behavior. The prediction usually becomes more accurate and less uncertain as more input, e.g., observed human movements, is obtained. However, waiting for a confident prediction causes delayed action selection and reduces the available time for the robot to execute its action [53]. The anticipatory robot is, thus, forced to make decisions given a sequence of uncertain predictions, where a trade-off between prediction accuracy and reaction delay needs to be addressed. To address this issue, we formulate the decision making process as a Partially Observable Markov Decision Process (POMDP), and propose approaches to efficiently choose the robot's optimal action and decide the timing to initiate action. The framework is based on Intention-Driven Dynamics Model (IDDM), a nonparametric Bayesian model for human movements that allows to infer the intention of human partner while he is executing his action [53].</paragraph><paragraph>The presented framework is generic and can be used in many human–robot interaction scenarios. A typical application would be one where the robot observes the actions of a human, makes predictions about the human's intention with the IDDM, and reacts accordingly by either waiting for more observations or by executing a physical action. For example, a robot that navigates in an indoor environment alongside humans needs to constantly predict their trajectories in order to avoid collisions and to optimize its path [56]. In such environments, humans tend to walk to a target that belongs to a small set of specific predefined targets such as doors, chairs, stairs, etc. Therefore, the human's intention can be discretized and IDDM can be used to compute a belief on where the human is heading based on the observed images and on the images used for training. The robot can either adjust its path or slow down and wait for more observations, by using the Monte Carlo POMDP framework to forecast the effect of each decision. It is interesting to note here that humans use seemingly similar mechanisms of stopping, or slowing down, and belief updating in order to avoid collisions with pedestrians.</paragraph><paragraph>Another example is the problem of cooperative human–robot co-manipulation that is receiving increasing attention in the domain of intelligent prosthetics. Modern prosthetics contain semi-autonomous robotic joints that react to neurological signals [48]. These signals are high-dimensional and inherently noised. One can use IDDM to learn to predict the joint movement intended by the human user and to execute it. However, before executing the joint movement, it is also important to wait until the system is sufficiently certain that the human wants that movement. This is also a clearly optimal stopping movement that can be casted in the proposed framework [40]. Maybe a better example of human–robot co-manipulation is the scenario where the human uses a separate, fully autonomous, robotic arm for lifting objects [20]. In this case, the human simply reaches to an object, the robot perceives the human's arm trajectory and predicts the intended object and reaches to it. During the lifting action, the robot also needs to predict the direction that the human is trying to follow for moving the object, by using the force and torque feedback, as well as observed trajectories with IDDM. This is also an anticipatory action selection problem that can be solved in the proposed framework.</paragraph><paragraph>In this article, we conduct a case study on human–robot table tennis, where anticipation is crucial for giving the robot sufficient time to execute its hitting movements [52]. This choice is mainly motivated by the need of fast perception and decision-making in table tennis, in addition to the high level of noise encountered in this type of applications.</paragraph><section label="1.1"><section-title>Anticipation in human–robot table tennis</section-title><paragraph>Playing table tennis is a challenging task for robots due to reasons ranging from the robot's deficiencies in perceiving the environment to the hardware limitations that restrict the actions. Hence, robot table tennis has been used as a benchmark task for high-speed vision [1], [12], fast movement generation [3], [29], learning [25], [27], [28], and many other research problems in robotics. Inspired by the fact that human players reconstruct and improve their dynamics during matches [39], Mülling et al. [28] showed that a single type of hitting movement (action), e.g., forehand hitting movement, can be learned and constantly improved while interacting with human players. In practice, the robot often needs a repertoire of actions to cover the potential hitting points in its entire accessible workspace. For example, the biomimetic robot player [29] used in this article employs three actions, namely, forehand, default, and backhand hitting movements, as shown in Fig. 6. Each hitting movement is parametrized by duration, amplitude, and final goal position. Although the robot is faster and more precise than a human being, the robot player still suffers from certain hardware limitations, such as torque and acceleration limits, which severely restrict its movement abilities in this high-speed scenario. Even a beginner human player would have the upper hand simply by choosing regions that the robot cannot reach in time if the robot's action is only based on the extrapolated trajectory of the incoming ball.</paragraph><paragraph>The robot's hitting movement imitates typical human table tennis strokes, as shown in Fig. 1, which consist of four stages [34]. In the awaiting stage, the ball moves towards the human opponent and is returned back. The robot stays at the awaiting pose during this stage. The preparation stage starts when the coming ball passes over the net, and the robot moves to a preparation pose. The hitting stage begins shortly after the ball bounces of the table and the human has enough information to decide for a hitting state. The racket moves towards this hitting state and hits the ball at the end of the hitting stage. It follows through in the finishing stage and recovers to the awaiting pose. The duration of the hitting stage is constant for expert players and lasts approximately 80 ms. Even against a slower opponent, which allows less than 300 ms for the robot's hitting movement, this short time often does not suffice for initializing the racket to the preparation position and then moving it to reach the hitting state. Therefore, many desired hitting movements are not feasible due to time limits; the robot needs to initiate its hitting movement during the awaiting stage, that is, before the human opponent returns the ball.</paragraph><paragraph>To gain sufficient time for initializing and executing a hitting movement, the robot needs to be anticipatory about its human opponent's intention, which human players rely heavily on [2]. The prediction of the human partner's intention can be realized by modeling how the intention directs the dynamics of hitting movement [53]. In the table tennis scenario, this Intention-Driven Dynamics Model (IDDM) leads to an online algorithm that, given a time series of observations, continually predicts the human player's intended target, i.e., where the human intends to shoot the ball [53], as shown in Fig. 2.</paragraph><paragraph>Anticipatory action selection needs to take into account the prediction uncertainty. The robot is likely to fail to return the ball if it has initiated a forehand (“right” in Fig. 2) hitting movement and the ball is shot to its backhand (“left”) region, or vice versa. The prediction of the intended target tends to become increasingly accurate and confident as the human opponent finishes his movement [53]. On the other hand, the robot requires a certain minimum time to execute its hitting movement. Therefore, the essence of the anticipatory action selection is deciding when and how to initiate the hitting movement based on the increasingly confident predictions.</paragraph></section><section label="1.2"><section-title>Related work and contributions</section-title><paragraph>Intention inference has been investigated in different settings, for example, using Hidden Markov Models (HMMs) to model and predict human behavior where different dynamics models were adopted to the corresponding behaviors [30]. Online learning of intentional motion patterns and prediction of intentions based on HMMs was proposed by Vasquez et al. [46], which allows efficient inference in real time. The HMM can be learned incrementally to cope with new motion patterns in parallel with prediction [47].</paragraph><paragraph>Anticipation is important in many human–robot interaction scenarios (e.g., [56], [10], [51]). Decision making can also be achieved jointly with intention inference, in scenarios such as autonomous driving [5], control [17], or navigation in human crowds [22]. For example, when the state space is finite, the problem can be formulated as a partially observable Markov decision process [23] and solved efficiently [49]. The issue of planning a robot's actions while predicting actions of humans has also been tackled in the literature of human-aware planning. For example, Cirillo et al. [8] presented a task planner for robots that recognizes activities of humans in a shared living space (e.g. kitchen). In the presented system, an HMM is used for activity recognition, based on partial observations. The planner computes a probability distribution on the actions of the humans, and finds the best robot plan accordingly. The planner also adjusts to detected changes. Compared to our work, the aforementioned work is based on the same concept of combining predictions of human behavior with planning. Our work is however tailored to predict human motions and trajectories, instead of general plans or activities. Sisbot et al. [42] also proposed a human-aware motion planner. The proposed system allows robots to take into account human partners by reasoning about their accessibility, vision field and preferences. The proposed system did not include, however, a dynamical model of human movements.</paragraph><paragraph>The anticipatory action selection decides when it is time to initiate the hitting movement. This decision is based on a trade-off between prediction accuracy and reaction delay, which is a generalization of optimal stopping problems. The optimal stopping problems have been extensively investigated in sequential analysis using Markov decision process (MDP), where the focus is usually the existence of optimal stopping rules given the transition model or obtaining closed-form solutions in specific problems (e.g., [40]). We do not have closed-form solutions for the decision making problem in our application due to the complexity of the human dynamics. The optimal stopping problem can be applied in the context of classification and feature selection [33], [14], [11], using reinforcement learning to obtain an optimal stopping policy. The optimal stopping problem has also been studied under partial observation [55], and in many applications, such as quality control [21] and finance [9], [35].</paragraph><paragraph>POMDPs have been used to formulate planning for human–robot interaction, for example, to assist elderly individuals with mild cognitive and physical impairments [32], [6]. Previous work in human–robot interaction focused on POMDPs with representational restrictions, such as finite states [16] or time-aggregated state space [6], [7]. Such restrictions make the model difficult to represent complicated and nonlinear human behaviors. This article presents a novel framework that formulates action selection in human–robot interaction with continuous-state POMDPs and handles non-linearity and stochasticity in human movements with Bayesian nonparametric models. This formulation is sufficiently expressive for complicated human movements such as table tennis strokes.</paragraph><paragraph>Human–robot table tennis has been used as a benchmark task for high-speed vision [1], [12], fast movement generation [3], [29], learning [25], [27], [28], and many other research problems in robotics. However, previous work in robot table tennis focused on reactive action planning solely based on the ball's trajectory. This article presents a novel proactive planning framework by modeling and predicting the behavior of human opponent, such that the robot can start planning even before the human has hit the ball. To the best of our knowledge, this is the first work on human–robot table tennis that makes use of perceived human movements.</paragraph><paragraph>In addition, this article substantially extends previous work on the Intention-Driven Dynamics Models (IDDM), which was originally a dynamics model for intention inference and for predicting future observations [53]. We propose an original formulation of the action selection problem in robot table tennis as POMDPs, wherein a Monte-Carlo planning algorithm that makes use of the IDDM was presented to address the trade-off between prediction accuracy and reaction delay. We show that this novel formulation leads to a significant improvement of the robot's performance.</paragraph><paragraph>At a more general level, the most important contribution of this paper to Artificial Intelligence is presenting a well developed study on the combination of anticipation and planning. The anticipation module forecasts future states of a dynamical system based on past observations. Often, the dynamical system corresponds to human actions and intentions that are complex and difficult to model. Therefore, the anticipation module is typically learned from examples and used as a black-box by the planning module. The planning module relates to the robot's actions. Starting from a given state, the planner inquirers the anticipation module about the probabilities of different states in the future, and chooses its actions accordingly. These two modules are complementary because they solve different problems. The two modules can be developed separately. For instance, the anticipation module might have been developed for analyzing table tennis games between human players, while the planner was developed using a simulator or playing against a simple automatic ball launcher. Yet, the two modules can be combined with a minimum effort. We believe that this type of anticipation-planning combinations will play a crucial role in the development of future intelligent robots, as robots are moving out of their traditional controlled environments and increasingly interacting with humans.</paragraph><paragraph>The remainder of this article is organized as follows. We formulate the anticipatory action selection using POMDPs, and present approaches to policy learning and decision making in Section 2. In Section 3, we first introduce the setup of the considered robot table tennis player. Subsequently, we evaluate the effectiveness of the proposed approaches, and show that they substantially enhance the capability of the considered robot player. Finally, we conclude and summarize the contributions in Section 4.</paragraph></section></section><section label="2"><section-title>Anticipatory action selection</section-title><paragraph>The essence of anticipatory action selection is decision making given a time series of predictions, where two fundamental issues have to be addressed. First, uncertainties in the prediction and outcome need to be considered. For example in the robot table tennis setup, the uncertainty in prediction is mainly due to the fact that the opponent may still change the target before his racket hits the ball. The prediction of the opponent's intended target, based on the observed partial movement of his stroke movement, tends to become increasingly accurate as the opponent finishes the hitting movement. Furthermore, the outcome of executing a selected action, e.g., the robot's success of returning the ball to the opponent's court with the chosen hitting movement, is not deterministic as the underlying dynamics of the robot arm are often too complicated to be modeled precisely at high speed. The decision making algorithm should be able to deal with the associated uncertainties.</paragraph><paragraph>The second fundamental issue is the timing for the robot to initiate the selected action, as the robot often requires sufficient time to execute an action. In the table tennis setup, while the predictions tend to become increasingly accurate, the robot needs sufficient time to move the arm from the awaiting pose to the desired preparation pose. The anticipatory action selection needs to trade off between prediction accuracy and delay in action selection, as both influence the success probability of the selected action. Hence, it is essential to choose the optimal action at the right time.</paragraph><paragraph>Partially Observable Markov Decision Processes (POMDPs) are suitable to model the uncertainties in prediction and outcome and formulate anticipatory action selection as a Markov decision process under uncertainty. We present two different approaches to decision making. In the first approach, we transform the POMDP into an equivalent fully observable Markov Decision Process (MDP). The states in the equivalent MDP are the belief states of the POMDP, which are the posterior distribution of the unobserved state given the observation history. We adopt the Intention-Driven Dynamics Model (IDDM) for belief updates and the Least-Squares Policy Iteration (LSPI) for policy learning. However, the model-free LSPI algorithm does not make use of the estimated transition model in the IDDM framework, and can be sample-insufficient in the considered application. Consequently, we present a more sample-efficient approach using the Monte-Carlo planning, where actions are chosen according to the value function estimated using the Monte-Carlo method [45]. The considered optimal stopping problem is a special case of POMDPs; only one action, i.e., waiting, needs to be explored. We applied upper confidence bounds [4] to avoid unnecessary exploration of the waiting action and effectively prune search trees. The resulting algorithm is equivalent to a tailored POMCP [41] algorithm for optimal stopping.</paragraph><section label="2.1"><section-title>Optimal stopping under partial observability</section-title><paragraph>The anticipatory action selection is a generalization of optimal stopping under partial observation [26], and, similarly, can be formulated as a POMDP. A discrete-time POMDP is defined as a tuple {a mathematical formula}(S,A,Z,P,Ω,R), where {a mathematical formula}S is a state space, {a mathematical formula}A={0,…,n} is a set of actions, and {a mathematical formula}Z is a observation space. The states evolve following a Markov transition model, governed by {a mathematical formula}P, where {a mathematical formula}P(s′|s,a) represents the probability of going to state {a mathematical formula}s′ from state s when taking action a. The observations are generated from the states following the observation model Ω, where {a mathematical formula}Ω(z|s) is the probability of observing z in state s. The reward function {a mathematical formula}R(s,a) represents the expected immediate reward obtained for taking action a in state s.</paragraph><paragraph>The set of actions {a mathematical formula}A consists of a waiting action {a mathematical formula}a=0 and a set of stopping actions {a mathematical formula}a∈A∖{0}. In the table tennis scenario, taking the waiting action {a mathematical formula}a=0 means to postpone the selection of a hitting action and to wait for the subsequent observation to be available; and each stopping action {a mathematical formula}a≠0 leads to selecting and initiating a particular type of hitting movement, and, hence, to the termination of an episode. The immediate reward is only nonzero when a stopping action is taken, and corresponds to the outcome of the selected type of hitting movement. We consider a continuous state representation {a mathematical formula}s=[x,g] that consists of the state x of the environment and the intention g of the human. Here, the intention g is assumed to be invariant during an episode; for example, the intended target does not change during the human player's hitting movement. An observation {a mathematical formula}z∈Z includes perceived features of the environment, such as the position and velocity of the ball and the configuration of the opponent's racket.</paragraph><paragraph>Finding the optimal timing to initiate the appropriate action is a POMDP problem. The decision at every time step is made by maximizing the expected future reward that it leads to, e.g., the chance of successfully returning the ball in the table tennis setup. Specifically, we want to maximize the aggregated expected reward during an episode with indefinite horizon [15], given by{a mathematical formula} where the variable T denotes the stopping time, and a reward is obtained only by taking a stopping action {a mathematical formula}aT. Note that, generally, the stopping time T is unknown and determined by when a stopping action is taken. However, in many scenarios, there exists a maximum length of episode. For example in the case of human–robot table tennis, the robot's hitting point can usually be precisely predicted 120 ms after the human player hits the ball [53], and, thus, an optimal policy would stop waiting and initiate a hitting movement. In practice, we enforce that the robot takes a stopping action when the robot's hitting point can be reliably predicted by the vision system.</paragraph></section><section label="2.2"><section-title>Belief update with intention-driven dynamics model</section-title><paragraph>A key step towards solving the optimal stopping problem is updating the belief on state {a mathematical formula}st=[xt,g] according to the history {a mathematical formula}z1:t, given by {a mathematical formula}p(xt,g|z1:t).</paragraph><paragraph>We apply the online target prediction algorithm using the Intention-Driven Dynamics Model (IDDM). The IDDM is a discrete-time dynamics model for movement modeling and intention inference. In robotics scenarios, we often rely on noisy and high-dimensional sensor data. However, the intrinsic states are typically not observable, and may have lower dimensions. Therefore, we seek a latent state representation of the relevant information in the data, and then model how the intention governs the dynamics in the latent states {a mathematical formula}xt, as shown in Fig. 3. The resulting model jointly learns both the latent state representation and the dynamics in the state space.</paragraph><paragraph>Designing a parametric dynamics model is difficult due to the complexity of nonlinear and stochastic human movements. Hence, the IDDM uses Gaussian processes to handle both the transition model {a mathematical formula}p(xt+1|xt,g) in the latent state space and the observation model {a mathematical formula}p(zt|xt) from the latent states to the observations. The IDDM considers the dynamics of latent states x to follow an unknown function f, given by{a mathematical formula} The latent state {a mathematical formula}xt+1 at time {a mathematical formula}t+1 depends on the latent state {a mathematical formula}xt at time t as well as on the intention g, as demonstrated in the graphical model shown in Fig. 3. A GP prior {a mathematical formula}GP(mx(⋅),kx(⋅,⋅)) is placed on every dimension of f with shared mean and covariance functions. Subsequently, the predictive distribution of the latent state {a mathematical formula}xt+1 conditioned on the current state {a mathematical formula}xt and intention g is a Gaussian distribution given by {a mathematical formula}xt+1∼N(mx([xt,g]),Σx([xt,g])) based on training inputs {a mathematical formula}Xx and outputs {a mathematical formula}Yx. Similarly, the measurement mapping function h from latent state x to observations z, given by{a mathematical formula} is modeled by another set of GPs. The predictive probability of the observations {a mathematical formula}zt is given by a Gaussian distribution {a mathematical formula}zt∼N(mz(xt),Σz(xt)), where the predictive mean and covariance are computed based on training inputs {a mathematical formula}Xz and outputs {a mathematical formula}Yz.</paragraph><paragraph>The IDDM is a special case of the Hierarchical Gaussian Process Dynamics Model [50], where the intention is assumed to be time-invariant when learning the model. Nevertheless, this assumption does not limit the model's capability to infer time-varying intention. In fact, the online inference method [53] is able to deal with the change of intention. For more technical details of learning and approximate inference, we refer the reader to Wang et al. [53] and Wang [50].</paragraph><paragraph>Assuming that the dynamics of the human player's racket is driven by the intended target g, we can apply the IDDM to predict the target g given a time series of observations {a mathematical formula}z1:t that are generated from corresponding latent states {a mathematical formula}x1:t. While exact inference of the intention g and states {a mathematical formula}xt is not tractable, Wang et al. [53] presented an efficient online inference algorithm to update the belief {a mathematical formula}p(g,xt|z1:t), i.e., the posterior probability of the intention g and the latent states {a mathematical formula}xt once a new observation {a mathematical formula}zt is obtained. Fig. 4 shows that the predictive uncertainties decrease as the human player finishes the hitting movement. To summarize, the IDDM provides an estimate of the transition model {a mathematical formula}T and of the measurement model Ω in the considered POMDP with Gaussian processes, which are used for updating the belief.</paragraph></section><section label="2.3"><section-title>Policy learning with belief MDP</section-title><paragraph>A POMDP can be transformed to an equivalent fully observable MDP by using the belief state {a mathematical formula}θt∈Θ as the observable state, which is the belief of the unobserved state {a mathematical formula}st=[xt,g] given the observation history {a mathematical formula}z1:t. Subsequently, we can apply reinforcement learning algorithms to learn and improve a policy π, mapping a belief state {a mathematical formula}θt to the action that maximizes the expected reward. Estimating the transition model {a mathematical formula}P(θ′|θ,a) is difficult mainly due to the complicated behavior of the nonparametric dynamics model employed. Hence, we need model-free reinforcement learning algorithms that do not rely on an estimate of the transition model for belief states. Specifically, we consider the Q-learning algorithm [43], and learn a state-action value function {a mathematical formula}Qπ(θ,a) of a policy π. The value function {a mathematical formula}Qπ(θ,a) measures the expected future reward when taking action a according to the current belief θ and following the policy π thereafter. We can write the value function according to the Bellman equation, given by{a mathematical formula} for the waiting action, and{a mathematical formula} for any stopping action.</paragraph><paragraph>Since the belief states are continuous, the value function {a mathematical formula}Qπ cannot be written in a tabular form. A common way to deal with large or infinite state space is value function approximation by a linear combination of basis functions {a mathematical formula}ϕ(θ,a), given by{a mathematical formula} with parameters w. Taking into account these factors, we choose the Least-Squares Policy Iteration (LSPI) algorithm [24], a model-free reinforcement learning algorithm with the function approximation. The LSPI has been used successfully to solve several large scale problems.</paragraph><paragraph>The LSPI algorithm consists of a policy evaluation step, which estimates the value function {a mathematical formula}Qˆπ for the current policy π, and a policy improvement step, which improves the policy π by fixing the obtained value function {a mathematical formula}Qˆπ. Using the linear function approximation, the policy evaluation step boils down to estimating the parameters w. The integration in Eq. (4) is intractable due to the unknown transition model {a mathematical formula}P, and is replaced by sampling in the Q-learning framework. Derived from MDP with finite state space, the update rule of the approximation parameters w can be straightforwardly applied for the considered continuous state space. The approximation parameters w can be obtained from a finite set of samples {a mathematical formula}D={(θi,ai,ri,θi′)|i=1,…,L}, where {a mathematical formula}θi is the output of the online inference algorithm [53]. Given a policy π, the estimates{a mathematical formula}{a mathematical formula} are used to update the approximation parameters{a mathematical formula} where the sufficient small {a mathematical formula}δ2 is used to avoid numerical error in the inversion of {a mathematical formula}Aˆ[24].</paragraph><paragraph>In the policy improvement step, we improve the policy π by a new policy {a mathematical formula}π′ that maximizes the expected reward according to the estimated value function {a mathematical formula}Qˆπ. The optimal policy {a mathematical formula}π′ greedily chooses the action that maximizes the corresponding value function {a mathematical formula}Qˆπ. Therefore, we obtain an improved policy{a mathematical formula} We can obtain the optimal policy by iteratively executing the policy evaluation and improvement steps, as summarized in Algorithm 1.</paragraph></section><section label="2.4"><section-title>Monte-Carlo planning with POMDP</section-title><paragraph>The LSPI algorithm as described above employs a model-free approach to policy learning, using the Intention-Driven Dynamics Model as a black box for updating belief {a mathematical formula}θt given a history of observations {a mathematical formula}z1:t. Besides the capability of belief update, the IDDM in fact provides a transition model in state space, estimated from its training data, which has not been exploited by the LSPI algorithm. Here, we present Monte-Carlo Planning (MCP), a model-based approach to action selection as an alternative to the LSPI algorithm.</paragraph><paragraph>Rather than estimating the value function {a mathematical formula}Qπ given a policy π, we directly consider the value function Q for an optimal policy. The considered optimal stopping problem is a special case of POMDPs; there is only one single waiting action that needs to be explored. As a stopping action terminates the decision process immediately, the value function for the stopping actions {a mathematical formula}∀a∈A∖{0}:Qπ(θ,a)=Q(θ,a) holds for any belief state θ and any policy π. It is sensible to estimate the value function for stopping actions offline, as running simulation repeatedly, especially with real-time simulator (e.g., simulated robot table tennis), during planning is expensive. We, hence, reuse the estimated value function for stopping actions by LSPI, and focus on the value function for the waiting action {a mathematical formula}Q(θt,at=waiting), given by the expected future reward{a mathematical formula} with respect to the subsequent belief state {a mathematical formula}θt+1.</paragraph><paragraph>The value function {a mathematical formula}Q(θt,at=waiting) measures the expected reward if the agent chooses to wait for more observation. While computing the exact expectation is intractable, the value function can be estimated using Monte-Carlo approximation [45], where we replace the expectation operator by an empirical average over sampled belief states. Each time we draw a sample of the current state {a mathematical formula}st, the subsequent state {a mathematical formula}st+1, and the subsequent observation {a mathematical formula}zt+1, compute the subsequent belief state {a mathematical formula}θt+1 based on the IDDM, and estimate {a mathematical formula}maxat+1⁡Q(θt+1,at+1) recursively. One can see that estimating the value function for waiting at the next time step {a mathematical formula}Q(θt+1,at+1=waiting) again relies on the Monte-Carlo approximation, and, hence, that the number of sampled decision trees grows exponentially with the horizon. Although the horizon is often finite for the anticipatory action selection, e.g., the optimal hitting action can be chosen straightforwardly once the human player has hit the ball, we need to limit the depth of sampled decision trees in consideration of restrictive time constraints in robotics. Here, we only plan for one step ahead; namely, we estimate the value function of postponing the decision for one time step, given by{a mathematical formula} This estimation can be achieved by using particle projection routine [45], as described in Algorithm 2.</paragraph><paragraph>The particle projection may still be too time-consuming to be applicable to online planning, as the Monte-Carlo approximation requires a certain amount of samples to achieve a reliable estimate. Nevertheless, consider the online planning that finds the optimal action{a mathematical formula} where one only needs to know if the waiting action leads to higher expected reward than all the stopping actions, rather than the actual expected reward of waiting. We can terminate the particle projection routine if the expected reward of waiting is very likely to be higher or lower than that of the optimal stopping action {a mathematical formula}maxa≠0⁡Q(θt,a). Inspired by upper confidence bound algorithms [4], we use the confidence interval estimate of the expected reward for waiting to terminate the particle projection routine before the Monte-Carlo sampling completes. To obtain a confidence interval estimator, we assume that the future reward for waiting at time step t is Gaussian distributed. Given a set of sampled rewards {a mathematical formula}Φ={r1,…,rn}, the confidence interval given a confidence level α is{a mathematical formula} where {a mathematical formula}n=|Φ| is the number of samples, {a mathematical formula}r¯ the sample mean, s the sample standard deviation, and c is the α percentile of a Student's t-distribution with {a mathematical formula}n−1 degrees of freedom. Algorithm 3 describes the resulting Monte-Carlo planning algorithm. Note that this pruning algorithm can be seen as a Partially Observable Monte-Carlo Planning (POMCP) algorithm ([41]Algorithm 1) tailored for optimal stopping problems, where the search procedure can be even more effectively pruned before reaching timeout. The differences between the presented algorithm and POMCP are twofold. First, simulation is often computationally demanding in many robotics applications; it is sensible to estimate the value function of stopping actions offline, as stopping actions terminate the POMDP immediately and no exploration is needed. Therefore, the presented algorithm estimates the value of stopping actions using all samples obtained from simulation, whereas the vanilla POMCP algorithm explores the stopping action during planning, which is not realistic for real-time simulators such as simulated robot table tennis [29]. Second, only the waiting action needs to be explored in each step, and the search procedure in POMCP can stop before timeout when either (1) the upper confidence bound of expected reward for waiting is lower than expected reward for any stopping action or (2) the lower confidence bound is higher than expected reward for all stopping actions.</paragraph><paragraph>In comparison to the model-free LSPI method, the MCP algorithm exploits the estimated transition model in the IDDM, and is expected to be more sample-efficient.</paragraph></section><section label="2.5"><section-title>Basis functions</section-title><paragraph>The presented LSPI and MCP methods both employ function approximation, relying on a set of basis functions of the belief state θ. The belief state obtained by the IDDM is represented by a vector that consists of the mean and covariance of the belief on the latent state x and a discretized histogram over the intended target g. The discretized histogram is illustrated in Fig. 4.</paragraph><paragraph>We consider a set of radial basis functions for approximating the value function. We collected all the encountered belief states on the training data, and chose K centers {a mathematical formula}θ¯1,…,θ¯K using K-means clustering. The basis functions are given by{a mathematical formula} where δ is the Kronecker delta and we consider the radial basis functions{a mathematical formula} for the belief states.</paragraph></section></section><section label="3"><section-title>Application in human–robot table tennis</section-title><paragraph>We evaluated the LSPI and the MCP algorithms for anticipatory action selection in human–robot table tennis setup. The presented action selection methods were evaluated on the biomimetic robot table tennis player [29], as this setup allowed exhibiting how much of an advantage such an anticipation system may offer. We expect that the system will help similarly when deployed within the skill learning framework [28] as well as many of the recent table tennis learning systems [19], [54], [25].</paragraph><section label="3.1"><section-title>Robot player</section-title><paragraph>To quantitatively evaluate the performance of the proposed methods in terms of success rates, we used the SL framework [37], which consisted of a real-world setup and a sufficiently realistic simulation. The setup included a Barrett WAM{sup:TM} arm with seven degrees of freedom, which is capable of high speed motion. A racket was attached to the end-effector. Table, racket and ball were compliant with the international rules of table tennis. We used a vision system of seven cameras to collect real data during a table tennis game between two human players [52], recording the players' movement and ball's trajectory. The collected data was used by the SL simulation for the following experiments, as demonstrated in Fig. 5. Previous work showed that the SL framework is sufficiently realistic to simulate the robot table tennis setting,{sup:1} and that similar performance can be expected when the real robot is used [29], [28], [53].</paragraph><paragraph>In the considered setting, the robot always hits the ball on a virtual hitting plane 80 cm behind the table, as shown in Fig. 2. We defined the human's intended target as the intersection of the returned ball's trajectory with the robot's virtual hitting plane. As the x-coordinate (see Fig. 2) was most important for choosing the type of hitting movements [53], the intention g considered here was the x-coordinate of the hitting point. Physical limitations of the robot restricted the x-coordinate to the range of {a mathematical formula}±1.2m from the robot's base (the table is 1.52 m wide).</paragraph><paragraph>The robot player can execute three types of hitting movement (actions) that were refined and optimized for hitting points in a specific region, as shown in Fig. 6. Hence, the action set {a mathematical formula}A consisted of one waiting action and three stopping actions, each stopping action corresponding to a type hitting movement. Note that the action selection was only used to choose a hitting type, i.e., default, forehand, or backhand. Fine-tuning of the robot's movement can be done when the robot has initiated an action and once the returned ball can be reliably predicted from the ball's trajectory alone. However, returning the ball outside the corresponding hitting region is difficult once the robot has initiated the chosen action [53].</paragraph><paragraph>When the robot's hitting point can be precisely predicted, an optimal policy would take a stopping action to maximize the time for executing planned movement. Therefore, we enforce that the robot takes a stopping action when the robot's hitting point can be reliably predicted by the vision system, which typically happens within 120 ms after the human player returns the ball [53].</paragraph><paragraph>We used a data set with recorded 270 trials. Each trial started when the ball passed over the net while flying towards the human player, and ended when the ball returned by the human player reached the robot's hitting plane (see Fig. 2), including the trajectories of the ball and the player's racket. We excluded the trials where the ball was shot outside the overall hitting region of the robot, as shown in Fig. 6, and evaluated the performance of the policy learning algorithm on the remaining 207 trials. For the basis functions used for function approximation in Eq. (16), we chose 100 centers in the experiments. Note that the IDDM model had been trained with 100 additional trials [53], and this set of data was not used in the experiments in this article.</paragraph></section><section label="3.2"><section-title>Experimental results</section-title><paragraph>We evaluated the algorithms using ten-fold cross-validation. We carried out each round of the cross-validation for ten times to reduce the randomness in the robot player. In the first experiment, for each round of the cross-validation, we obtained sampled episodes {a mathematical formula}D={(θi,ai,ri,θi′)|i=1,…,L} on the nine subsamples for training, where all valid combinations of action and time to initiate were enumerated on each trial. For each episode, the reward, i.e., success of the robot's return, was obtained from the SL simulation, where a reward of 1 was given for successful return of the ball and −1 for failing to return. For the LSPI algorithm, the policy was learned from the sample {a mathematical formula}D using Algorithm 1, and evaluated on the one subsample for test. The anticipatory action selection following the learned policy by LSPI led to successful returns for the robots in {a mathematical formula}153.1±0.8 times, with an average success rate of 74%. We subsequently evaluated the MCP algorithm in the same manner. The action selection following Algorithm 3 led to successful returns for the robots in {a mathematical formula}153.3±0.6 times, an average success rate of 74%.</paragraph><paragraph>To demonstrate the importance of action selection, we evaluated a baseline that exclusively used a single action, i.e., the robot always use the same type of hitting movement. Every type of hitting movement yielded a relatively high success rate in its designated regions. However, the overall rate on the entire data set was considerably reduced due to its poor performance in the other regions. Fig. 7 showed the number of successful returns for using each action initiated at 240 ms before the opponent hits the ball, such that the robot had sufficient time to complete the movement. The robot player without anticipation would achieve an average success rate of 64% for using only the default hitting movement, and 53% and 59% for using only the forehand and backhand movements, respectively. Therefore, both the LSPI algorithm and the MCP algorithm significantly improved the performance of the robot player (p-value 0.03, using chi-squared test) by learning which action should be executed.</paragraph><paragraph>We considered another baseline with oracle timing, i.e., the action was always chosen at a specific time before the opponent has hit the ball. This hypothetical setting allows to demonstrate the importance of choosing an optimal stopping time. Without the waiting action, the LSPI algorithm and the MCP algorithm were equivalent, and both achieved the success return {a mathematical formula}143.4±1.0 times at 240 ms before the opponent hits the ball, {a mathematical formula}141.0±1.1 times at 160 ms, and {a mathematical formula}123.4±1.6 times at 80 ms, where the performance tended to decline as the reaction delay was increased. One can see that the waiting action played an important role in trading off the prediction accuracy and the reaction delay. Note that these time steps were only used for this baseline method with oracle timing, and that the other evaluated algorithms were not aware of the hitting time of the opponent in advance. Nevertheless, both the LSPI algorithm and the MCP algorithm outperformed this hypothetical baseline by learning when a chosen action should be initiated.</paragraph><paragraph>In addition, we consider a naive baseline where the action is chosen only based on the most likely prediction without considering predictive uncertainty, i.e., the belief state θ is represented only by the mean of latent state and the most likely intention. We first consider the setting with oracle timing. The robot successfully returned the ball {a mathematical formula}131.0±4.9, {a mathematical formula}128.8±5.5, and {a mathematical formula}111.7.4±5.8 times when the hitting movement was chosen at 240 ms, 160 ms, and 80 ms, respectively, before the opponent hits the ball. The performance is not better than simply using a single action, as it is crucial to take into account the uncertainty in prediction for action selection. We then consider the setting where the timing for action selection is decided by the learned policy using LSPI. The robot successfully returned the ball {a mathematical formula}127.2±4.2 times. The significantly degenerated performance (p-value 0.006) indicates that the uncertainty in belief state is also crucial for optimal timing. The presented formulation with POMDPs in this article is suitable to represent such optimal stopping problems and propagate the uncertainty in prediction. We do not consider Monte-Carlo planning algorithm in this baseline setting, as the uncertainty in belief state are needed for sampling future states.</paragraph><paragraph>Furthermore, we also compared to policy gradient methods [44]. Specifically, we adopted episodic Natural Actor-Critic (NAC) algorithm [31] and formulated the policy by soft-max function [43]. The NAC is a policy search method that directly estimates the gradient of the value function, which can be easier to do than estimating the value function itself. Moreover, one can use available domain knowledge to choose the type of policies that are searched. However, as an on-policy approach, the NAC is not very efficient in the use of sampled data. In addition, as in all gradient methods, the NAC can suffer from a premature convergence to a local optimum depending on the chosen step-size. In this application, the LSPI and MCP algorithms both outperformed the NAC algorithm, as shown in Fig. 7.</paragraph><paragraph>In the second experiment, we compared LSPI and MCP in terms of sample efficiency. We obtained samples {a mathematical formula}D from the training data by following a uniform policy, which chose the waiting action with a probability of {a mathematical formula}12 and each of the three stopping actions with a probability of {a mathematical formula}16. The first group of bars in Fig. 8 was obtained by sampling one episode for each trial in the training data. This amount of data was insufficient for LSPI to acquire a reliable estimate of the value function, especially for the waiting action. While the LSPI method performed even worse than the baseline that only chose the default hitting movement, the MCP method already achieved a substantial improvement by taking advantage of the estimated transition model in the IDDM. As we increased the number of sampled episode for each trial, the performance was improved for both methods. The MCP method always outperformed the LSPI method, although the advantages became smaller as more samples were available. The experimental results shown in Fig. 8 verified that the MCP can be more sample-efficient than the LSPI. Due to the fact that sampling is expensive in many real-robot applications, we advocate the use of the MCP method for anticipatory action selection.</paragraph></section><section label="3.3"><section-title>Discussion</section-title><paragraph>We conducted a case study with a simplified human–robot table tennis scenario. We assume that the recruited non-professional players do not deliberately mislead the opponent by changing their intended target during the execution of stroke. More generally, IDDM assumes that the intention is time-invariant and the main driving factor when learning the dynamics model. For example, the speed and spin of the table tennis ball, which also affect the dynamics of strokes, are not explicitly considered in the model.{sup:2} This assumption does not necessarily limit the capability of IDDM for intention inference [53] and planning. Moreover, this assumption can be further relaxed by taking into account other driving factors as exogenous variables in the dynamics model, leading to Hierarchical Gaussian Process Dynamics Models (H-GPDMs), discussed by [50]. Note that, however, the method is not applicable for adversarial planning, as the assumption of time-invariant intention is violated in adversarial scenarios. Incorporation of game-theoretic perspectives in the framework of H-GPDMs is a future direction of this work.</paragraph><paragraph>Despite the simplification, the case study clearly showed the importance of combining anticipation and planning and the feasibility of presented methods. We discuss implications of the experimental results in the general context of human–robot interaction as follows. Action selection is important. Robot players with action selection (baseline: oracle timing, NAC, LSPI, and MCP) outperformed that using a single action (baseline: single action), as shown in Fig. 7. Anticipatory action selection substantially improves the robot's performance. The baseline (oracle timing) in Fig. 7 shows that it is important for the robot to be anticipatory and to choose optimal action even before the human's intention can be accurately predicted. Optimal stopping is essential in anticipatory action selection. Comparison between methods for optimal stopping (NAC, LSPI, and MCP) and baseline (oracle timing) shows that it is important to trade off between confidence in prediction and time for executing action. POMDPs are suitable formulation of such optimal stopping problems, and existing algorithms for POMDPs can be applied to solve the optimal stopping problem. Uncertainties in prediction needs to be taken into account for optimal stopping. The baseline that only used predictive mean of latent state and most likely intention has substantially degenerated performance, indicating that uncertainties in prediction are important and informative for optimal stopping. Model-based planning can be more sample-efficient than model-free policy learning. Comparison between LSPI (model-free policy learning) and MCP (model-based planning) in Fig. 8 shows that model-based planning tends to be more sample-efficient than model-free policy learning.</paragraph><paragraph>Besides the considered scenario, the method is generic and can be used in other human–robot interaction scenarios. Human–robot co-manipulation is another type of problems that can benefit from the proposed approach. In this problem, a robot assists a human in lifting and moving objects. Co-manipulation is needed in industrial settings where the objects are often too heavy, it is also increasingly used in robotic prosthetics where a robotic arm replaces a lost human arm. In these scenarios, the robot should predict which object among several ones the human is trying to reach and whether the human will need help. Upon becoming certain about the intended target, the robotic arm's trajectory should be planned and the robotic hand should start moving toward the intended object. Adapting our approach to this problem seems straightforward. We only need to re-define the intention g as the coordinates of the intended object in a three-dimensional space. Observations {a mathematical formula}z1..t can remain the observed trajectory of the human's arm. The robot decides to initiate a movement toward the intended object, to wait until it becomes more certain, or to do nothing if the object can be lifted by the human without any help. The presented method is applicable when the intention is multi-dimensional variables, in which case MCP is a preferred tool for solving POMDPs; it is non-trivial for LSPI to model and parametrize the distribution of multi-dimensional and potentially multi-modal intention.</paragraph><paragraph>Note that, however, the proposed method does not straightforwardly work for structured intention prediction such as predicting trajectories of cyclist. There are two potential approaches based on the proposed method for dealing with structured intention. Take the avoidance of a cyclist as an example. Rather than predicting the trajectory, we can formulate the intention as the direction where the cyclist wants to go in a short period of time. To take into account the intention change over time, one can model the dynamics of the cyclist with a H-GPDM [50], where the intention {a mathematical formula}gt follows a Markov chain. The other potential approach is to design a new covariance function {a mathematical formula}kx(⋅,⋅) in the transition function in Eq. (2) for structured inputs [13], for example measuring the similarity of cyclist trajectories.</paragraph></section></section><section label="4"><section-title>Conclusions</section-title><paragraph>In this article, we introduced novel formulation and methods for anticipatory action selection for human–robot interaction, combining anticipation and planning. We formulated the anticipatory action selection as optimal stopping in a partially observable Markov decision processes. The presented formulation, using latent state variables and Bayesian nonparametric model, is expressive for nonlinear and stochastic human behaviors, such as table tennis strokes. We first presented a policy learning approach using the Least-Squares Policy Iteration algorithm. However, the LSPI can be sample-inefficient, as it does not exploit the transition model in the Intention-Driven Dynamics Models. Consequently, we presented the Monte-Carlo Planning approach, which benefits from the transition model estimated by the IDDM framework. The presented framework is generic and can be used in many other human–robot interaction scenarios, for example, in navigation and human–robot co-manipulation. A typical application would be one where the robot observes the actions of a human, makes predictions about the human's intention with the IDDM, and reacts accordingly by either waiting for more observations or by executing a physical action, where the uncertainties in prediction needs to be considered in deciding when and how to plan the robot's own actions.</paragraph><paragraph>In this article, we motivated and evaluated the proposed framework with human–robot table tennis games, where a bottleneck is the limited amount of time for the robot to execute a hitting movement. Movement initiation requires an early decision on the type of action, such as a forehand or backhand movement, before the opponent has hit the ball. Experimental results using real data and a simulated environment showed that the anticipatory action selection can be used for a robot table tennis player to enhance its performance against human players, where the robot decided the timing to initiate a selected hitting movement according to the prediction of the human opponent. We concluded that anticipatory action selection substantially improves the robot's performance, and that uncertainties in prediction needs to be taken into account for optimal stopping. POMDPs had been shown to be suitable to formulate the anticipatory action selection problem by taking into account the uncertainties in prediction. We also showed that existing algorithms for POMDPs, such as LSPI and MCP, can be applied to substantially improve the robot's performance in its interaction with humans.</paragraph></section></content><references><reference label="[1]"><authors>L. Acosta,J. Rodrigo,J. Mendez,G. Marichal,M. Sigut</authors><title>Ping-pong player prototype</title><host>IEEE Robot. Autom. Mag.10 (4)(2003) pp.44-52</host></reference><reference label="[2]">M. Alexander,A. HonishTable tennis: a brief overview of biomechanical aspects of the game for coaches and playersTech. report<host>(2009)Faculty of Kinesiology and Recreation ManagementUniversity of Manitoba</host><host>http://umanitoba.ca/faculties/kinrec/research/media/table_tennis.pdf</host></reference><reference label="[3]"><authors>L. Ángel,J. Sebastián,R. Saltarén,R. Aracil,R. Gutiérrez</authors><title>RoboTenis: design, dynamic modeling and preliminary control</title><host>Proceedings of IEEE/ASME International Conference on Advanced Intelligent Mechatronics(2005) pp.747-752</host></reference><reference label="[4]"><authors>P. Auer</authors><title>Using confidence bounds for exploitation–exploration trade-offs</title><host>J. Mach. Learn. Res.3 (2003) pp.397-422</host></reference><reference label="[5]"><authors>T. Bandyopadhyay,K.S. Won,E. Frazzoli,D. Hsu,W.S. Lee,D. Rus</authors><title>Intention-aware motion planning</title><host>Algorithmic Foundations of Robotics XSpringer Tracts in Advanced Roboticsvol. 86 (2013)SpringerBerlin/Heidelberg pp.475-491</host></reference><reference label="[6]"><authors>J. Boger,P. Poupart,J. Hoey,C. Boutilier,G. Fernie,A. Mihailidis</authors><title>A decision-theoretic approach to task assistance for persons with dementia</title><host>Proceedings of the 19th International Joint Conference on Artificial intelligence(2005) pp.1293-1299</host></reference><reference label="[7]"><authors>F. Broz,I. Nourbakhsh,R. Simmons</authors><title>Planning for human–robot interaction in socially situated tasks</title><host>Int. J. Soc. Robot.5 (2)(2013) pp.193-214</host></reference><reference label="[8]"><authors>M. Cirillo,L. Karlsson,A. Saffiotti</authors><title>Human-aware task planning: an application to mobile robots</title><host>ACM Trans. Intell. Syst. Technol.1 (2)(2010) pp.15-</host></reference><reference label="[9]"><authors>J.-P. Décamps,T. Mariotti,S. Villeneuve</authors><title>Investment timing under incomplete information</title><host>Math. Oper. Res.30 (2)(2005) pp.472-500</host></reference><reference label="[10]"><authors>A.D. Dragan,S.S. Srinivasa</authors><title>Formalizing assistive teleoperation</title><host>Proceedings of Robotics: Science and Systems(2012)</host></reference><reference label="[11]"><authors>G. Dulac-Arnold,L. Denoyer,P. Preux,P. Gallinari</authors><title>Sequential approaches for learning datum-wise sparse representations</title><host>Mach. Learn.89 (1–2)(2012) pp.87-122</host></reference><reference label="[12]"><authors>H. Fässler,H. Beyer,J. Wen</authors><title>A robot ping pong player: optimized mechanics, high pheromones 3d vision, and intelligent sensor control</title><host>Robotersysteme6 (3)(1990) pp.161-170</host></reference><reference label="[13]"><authors>T. Gärtner</authors><title>A survey of kernels for structured data</title><host>ACM SIGKDD Explor. Newsl.5 (1)(2003) pp.49-58</host></reference><reference label="[14]"><authors>R. Gaudel,M. Sebag</authors><title>Feature selection as a one-player game</title><host>Proceedings of the 27th International Conference on Machine Learning(2010) pp.359-366</host></reference><reference label="[15]"><authors>E.A. Hansen</authors><title>Indefinite-horizon POMDPs with action-based termination</title><host>Proceedings of the National Conference on Artificial Intelligencevol. 22 (2007)AAAI Press/MIT PressMenlo Park, CA/Cambridge, MA/London pp.1237-</host></reference><reference label="[16]"><authors>E.A. Hansen,R. Zhou</authors><title>Synthesis of hierarchical finite-state controllers for POMDPs</title><host>Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling(2003) pp.113-122</host></reference><reference label="[17]"><authors>K. Hauser</authors><title>Recognition, prediction, and planning for assisted teleoperation of freeform tasks</title><host>Proceedings of Robotics: Science and Systems(2012)</host></reference><reference label="[18]"><authors>G. Hoffman,C. Breazeal</authors><title>Cost-based anticipatory action selection for human–robot fluency</title><host>IEEE Trans. Robot.23 (5)(2007) pp.952-961</host></reference><reference label="[19]"><authors>Y. Huang,D. Xu,M. Tan,H. Su</authors><title>Adding active learning to LWR for ping-pong playing robot</title><host>IEEE Trans. Control Syst. Technol.21 (4)(2013) pp.1489-1494</host></reference><reference label="[20]"><authors>N. Jarrassé,J. Paik,V. Pasqui,G. Morel</authors><title>How can human motion prediction increase transparency?</title><host>IEEE International Conference on Robotics and Automation(2008)IEEE pp.2134-2139</host></reference><reference label="[21]"><authors>U. Jensen,G.-H. Hsu</authors><title>Optimal stopping by means of point process observations with applications in reliability</title><host>Math. Oper. Res.18 (3)(1993) pp.645-657</host></reference><reference label="[22]"><authors>M. Kuderer,H. Kretzschmar,C. Sprunk,W. Burgard</authors><title>Feature-based prediction of trajectories for socially compliant navigation</title><host>Proceedings of Robotics: Science and Systems(2012)</host></reference><reference label="[23]"><authors>H. Kurniawati,Y. Du,D. Hsu,W. Lee</authors><title>Motion planning under uncertainty for robotic tasks with long time horizons</title><host>Int. J. Robot. Res.30 (3)(2011) pp.308-323</host></reference><reference label="[24]"><authors>M. Lagoudakis,R. Parr</authors><title>Least-squares policy iteration</title><host>J. Mach. Learn. Res.4 (2003) pp.1107-1149</host></reference><reference label="[25]"><authors>M. Matsushima,T. Hashimoto,M. Takeuchi,F. Miyazaki</authors><title>A learning approach to robotic table tennis</title><host>IEEE Trans. Robot.21 (4)(2005) pp.767-771</host></reference><reference label="[26]"><authors>G. Mazziotto</authors><title>Approximations of the optimal stopping problem in partial observation</title><host>J. Appl. Probab.23 (1986) pp.341-354</host></reference><reference label="[27]"><authors>F. Miyazaki,M. Matsushima,M. Takeuchi</authors><title>Learning to dynamically manipulate: a table tennis robot controls a ball and rallies with a human being</title><host>Advances in Robot Control(2005) pp.3137-3341</host></reference><reference label="[28]"><authors>K. Mülling,J. Kober,O. Kroemer,J. Peters</authors><title>Learning to select and generalize striking movements in robot table tennis</title><host>Int. J. Robot. Res.32 (3)(2013) pp.263-279</host></reference><reference label="[29]"><authors>K. Mülling,J. Kober,J. Peters</authors><title>A biomimetic approach to robot table tennis</title><host>Adapt. Behav.19 (5)(2011) pp.359-376</host></reference><reference label="[30]"><authors>A. Pentland,A. Liu</authors><title>Modeling and prediction of human behavior</title><host>Neural Comput.11 (1)(1999) pp.229-242</host></reference><reference label="[31]"><authors>J. Peters,S. Schaal</authors><title>Natural actor–critic</title><host>Neurocomputing71 (7)(2008) pp.1180-1190</host></reference><reference label="[32]"><authors>J. Pineau,M. Montemerlo,M. Pollack,N. Roy,S. Thrun</authors><title>Towards robotic assistants in nursing homes: challenges and results</title><host>Robot. Auton. Syst.42 (3)(2003) pp.271-281</host></reference><reference label="[33]"><authors>B. Póczos,Y. Abbasi-Yadkori,C. Szepesvári,R. Greiner,N. Sturtevant</authors><title>Learning when to stop thinking and do something!</title><host>Proceedings of the 26th Annual International Conference on Machine Learning(2009)ACM pp.825-832</host></reference><reference label="[34]"><authors>M. Ramanantsoa,A. Durey</authors><title>Towards a stroke construction model</title><host>Int. J. Table Tennis Sci.2 (1994) pp.97-114</host></reference><reference label="[35]"><authors>R. Rishel,K. Helmes</authors><title>A variational inequality sufficient condition for optimal stopping with application to an optimal stock selling problem</title><host>SIAM J. Control Optim.45 (2)(2006) pp.580-598</host></reference><reference label="[36]"><authors>R. Rosen,J. Rosen,J. Kineman,M. Nadin</authors><title>Anticipatory Systems: Philosophical, Mathematical, and Methodological Foundations</title><host>IFSR International Series on Systems Science and Engineering (2012)Springer</host></reference><reference label="[37]">S. SchaalThe SL simulation and real-time control software packageTech. rep.<host>(2009)University of Southern California</host></reference><reference label="[38]"><authors>N. Sebanz,H. Bekkering,G. Knoblich</authors><title>Joint action: bodies and minds moving together</title><host>Trends Cogn. Sci.10 (2)(2006) pp.70-76</host></reference><reference label="[39]"><authors>C. Sève,J. Saury,J. Theureau,M. Durand</authors><title>Activity organization and knowledge construction during competitive interaction in table tennis</title><host>Cogn. Syst. Res.3 (3)(2002) pp.501-522</host></reference><reference label="[40]"><authors>A.N. Shiryaev</authors><title>Optimal Stopping Rules, vol. 8</title><host>(2007)Springer</host></reference><reference label="[41]"><authors>D. Silver,J. Veness</authors><title>Monte-Carlo planning in large POMDPs</title><host>Advances in Neural Information Processing Systemsvol. 23 (2010) pp.2164-2172</host></reference><reference label="[42]"><authors>E.A. Sisbot,L.F. Marin-Urias,R. Alami,T. Simeon</authors><title>A human aware mobile robot motion planner</title><host>IEEE Trans. Robot.23 (5)(2007) pp.874-883</host></reference><reference label="[43]"><authors>R. Sutton,A. Barto</authors><title>Reinforcement Learning: An Introduction</title><host>(1998)The MIT Press</host></reference><reference label="[44]"><authors>R.S. Sutton,D.A. McAllester,S.P. Singh,Y. Mansour</authors><title>Policy gradient methods for reinforcement learning with function approximation</title><host>Advances in Neural Information Processing Systemsvol. 12 (1999) pp.1057-1063</host></reference><reference label="[45]"><authors>S. Thrun</authors><title>Monte Carlo POMDPs</title><host>Advances in Neural Information Processing Systemsvol. 12 (2000)MIT Press pp.1064-1070</host></reference><reference label="[46]"><authors>D. Vasquez,T. Fraichard,O. Aycard,C. Laugier</authors><title>Intentional motion on-line learning and prediction</title><host>Mach. Vis. Appl.19 (5)(2008) pp.411-425</host></reference><reference label="[47]"><authors>D. Vasquez,T. Fraichard,C. Laugier</authors><title>Growing hidden Markov models: an incremental tool for learning and predicting human and vehicle motion</title><host>Int. J. Robot. Res.28 (11–12)(2009) pp.1486-1506</host></reference><reference label="[48]"><authors>M. Velliste,S. Perel,M.C. Spalding,A.S. Whitford,A.B. Schwartz</authors><title>Cortical control of a prosthetic arm for self-feeding</title><host>Nature453 (7198)(2008) pp.1098-1101</host></reference><reference label="[49]"><authors>Y. Wang,K. Won,D. Hsu,W. Lee</authors><title>Monte Carlo Bayesian reinforcement learning</title><host>Proceedings of the 29th International Conference on Machine Learning(2012) pp.1135-1142</host></reference><reference label="[50]">Z. WangIntention inference and decision making with hierarchical gaussian process dynamics modelsPh.D. thesis<host>(2013)TU Darmstadt</host><host>http://tuprints.ulb.tu-darmstadt.de/3617/</host></reference><reference label="[51]">Z. Wang,M.P. Deisenroth,H.B. Amor,D. Vogt,B. Schölkopf,J. PetersProbabilistic modeling of human movements for intention inferenceProceedings of Robotics: Science and Systems(2012)(R:SS)</reference><reference label="[52]"><authors>Z. Wang,C.H. Lampert,K. Mulling,B. Scholkopf,J. Peters</authors><title>Learning anticipation policies for robot table tennis</title><host>Proceedings of IEEE/RSJ International Conference on Intelligent Robots and SystemsIROS(2011) pp.332-337</host></reference><reference label="[53]"><authors>Z. Wang,K. Mülling,M.P. Deisenroth,H.B. Amor,D. Vogt,B. Schölkopf,J. Peters</authors><title>Probabilistic movement modeling for intention inference in human–robot interaction</title><host>Int. J. Robot. Res.32 (7)(2013) pp.841-858</host></reference><reference label="[54]"><authors>P. Yang,D. Xu,H. Wang,Z. Zhang</authors><title>Control system design for a 5-DOF table tennis robot</title><host>Proceedings of International Conference on Control Automation Robotics and Vision(2010) pp.1731-1735</host></reference><reference label="[55]"><authors>E. Zhou</authors><title>Optimal stopping under partial observation: near-value iteration</title><host>IEEE Trans. Autom. Control58 (2)(2013) pp.500-506</host></reference><reference label="[56]"><authors>B.D. Ziebart,N. Ratliff,G. Gallagher,C. Mertz,K. Peterson,J.A. Bagnell,M. Hebert,A.K. Dey,S. Srinivasa</authors><title>Planning-based prediction for pedestrians</title><host>Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)(2009) pp.3931-3936</host></reference></references><footnote><note-para label="1">See http://robot-learning.de/Research/ProbabilisticMovementModeling for a demonstration of the real-robot setup.</note-para><note-para label="2">The speed and spin of the ball are implicitly modeled by the data-driven, non-parametric dynamics model.</note-para></footnote></root>