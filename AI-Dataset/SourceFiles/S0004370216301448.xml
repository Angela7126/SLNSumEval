<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370216301448</url><title>State space search nogood learning: Online refinement of critical-path dead-end detectors in planning</title><authors>Marcel Steinmetz,Jörg Hoffmann</authors><abstract>Conflict-directed learning is ubiquitous in constraint satisfaction problems like SAT, but has been elusive for state space search on reachability problems like classical planning. Almost all existing approaches learn nogoods relative to a fixed solution-length bound, in which case planning/reachability reduces to a constraint satisfaction problem. Here we introduce an approach to learning more powerful nogoods, that are sound regardless of solution length, i.e., that identify dead-end states for which no solution exists. The key technique we build on are critical-path heuristics hC, relative to a set C of conjunctions. These recognize a dead-end state s, returning hC(s)=∞, if s has no solution even when allowing to break up conjunctive subgoals into the elements of C. Our key idea is to learn C during search. Whenever forward search has identified an unrecognized dead-end s, where hC(s)&lt;∞, we analyze the situation at s, and add new conjunctions into C in a way guaranteeing to obtain hC(s)=∞. Thus we learn to recognize s, as well as similar dead-ends search may encounter in the future. We furthermore learn clauses ϕ where s′⊭ϕ implies hC(s′)=∞, to avoid the overhead of computing hC on every search state. Arranging these techniques in a depth-first search, we obtain an algorithm approaching the elegance of nogood learning in constraint satisfaction, learning to refute search subtrees. We run comprehensive experiments on solvable and unsolvable planning benchmarks. In cases where forward search can identify dead-ends, and where hC dead-end detection is effective, our techniques reduce the depth-first search space size by several orders of magnitude, and often result in state-of-the-art performance.</abstract><keywords>Search;Heuristic search;Planning</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>The ability to analyze conflicts, and to learn nogoods (or, dually, implied clauses) that avoid similar mistakes in the future, is a key algorithm ingredient in constraint satisfaction, including but not limited to SAT (e.g. [1], [2], [3], [4], [5], [6], [7]). For reachability problems in large transition systems, like goal reachability in classical planning which we consider here, progress in this direction has been more elusive. Existing techniques almost entirely pertain to length-bounded reachability, where the learned nogoods are valid only relative to a fixed bound on the length of the (remaining) solution path. In this setting, planning/reachability reduces to a constraint satisfaction problem. Via a SAT encoding (e.g. [8], [9]), the respective nogood learning techniques apply unmodified. For state space search, conflicts in length-bounded search take the form of states unsolvable within the bound. This has been thoroughly investigated in the context of Graphplan[10], [11], [12], and more recently in property-directed reachability (PDR)[13], [14] (which, as pointed out by Suda [14], can be viewed as an extension of Graphplan).</paragraph><paragraph>From the perspective of reachability testing, length-bounded reachability is a limitation, as one needs to iterate over different length bounds until some termination criterion applies. So, do we actually need a length bound to be able to do conflict analysis and nogood learning?</paragraph><section label="1.1"><section-title>Nogood learning without a length bound</section-title><paragraph>Research results in this direction are very sparse. For the identification of dead-end states in forward state space search – states from which no solution exists, arguably the most canonical form of “conflicts” in forward search – nogood learning techniques are unavailable.</paragraph><paragraph>In work done as part of the Prodigy system development in the late 80s/early 90s, sound action-pruning rules were learned by analyzing the structure of a backward search [15]. Kambhampati et al. thoroughly analyzed conflict-based learning, and its relation with CSP methods, in partial-order planning, i.e., a plan-space search framework [16], [17]. Bhatnagar and Mostow [18] considered forward state space search nogood learning, yet their techniques are not sound (do not guarantee that pruned states actually are dead-ends). Kolobov et al.'s SixthSense technique [19] learns to detect dead-ends in probabilistic forward search planning, yet incorporates classical planning as a sub-procedure (SixthSense did, however, inspire part of our techniques, as we detail below). Value function refinement using Bellman updates (e.g. [20], [21], [22], [23]) will eventually learn that a state is a dead-end, yet does not generalize that knowledge so is not a meaningful form of “nogood learning”.</paragraph><paragraph>Nevertheless, the question of learning nogoods for dead-end detection is quite relevant. Such conflicts, though not as quintessential as in constraint satisfaction (including length-bounded reachability), are important in many applications. For example, bad decisions often lead to dead-ends in over-subscription planning (e.g. [24], [25], [26]), in planning with limited resources (e.g. [27], [28], [29]), and in single-agent puzzles like Sokoban (e.g. [30]) or Solitaire card games (e.g. [31]). In explicit-state model checking of safety properties (e.g. [32], [33], [34]), a dead-end is any state from which the error property cannot be reached – which one would expect to be the case for most states.</paragraph><paragraph>We introduce a method that learns sound and generalizable nogoods from dead-end states during forward state space search. To our knowledge, this is the first of its kind. Our work is placed in classical planning, where the state space is modeled in terms of finite-domain state variables, an initial state, deterministic transition rules (actions), and a conjunctive goal condition. But, in principle, the approach applies to reachability checking in other transition system models as well, so long as they are suitable for the design of so-called critical-path heuristics. We briefly discuss applications beyond classical planning at the end of the paper.</paragraph></section><section label="1.2"><section-title>Dead-end detection methods in classical planning</section-title><paragraph>We distinguish between (a) dead-end detection vs. (b) proving unsolvability. Both are closely related, but while (a) is dedicated to the detection of dead-ends during a forward state space search, (b) is dedicated to proving that the initial state is a dead-end. The difference lies in the attribution of computational resources: (a) will be done on every state during a search so should be fast, while (b) will be done just once so can use the entire computational resources available. At the algorithm design level, this means that techniques (a) will typically employ easy-to-test sufficient criteria, while (b) will explore a search space in some form. Forward state space search employing (a) is one method for doing (b), but is not limited to (b): dead-end detection can be useful also on solvable problems, as it allows to avoid fruitless parts of the search space. Our approach here falls into class (a).</paragraph><paragraph>Both (a) and (b) have been traditionally neglected in classical planning; (b) has even been completely ignored, the focus being exclusively on solvable problems. In particular, the benchmarks used in the International Planning Competition (IPC) are all solvable. First works designing techniques dedicated to (b) appeared in 2013 and 2014 [35], [36]; (b) became the center of attention for the first time in 2016, with the inaugural Unsolvability International Planning Competition (UIPC'16).{sup:1}</paragraph><paragraph>That said, heuristic functions – estimators of goal distance – have been intensely investigated in classical planning (e.g. [39], [40], [41], [42], [43], [44], [45]), and most of these have the ability to detect dead-ends. They return heuristic value {a mathematical formula}h(s)=∞ if state s is unsolvable even in the relaxation underlying h. Yet this has traditionally been treated as a by-product of estimating goal distance. Hoffmann et al. [36] were the first to break with this tradition. They introduced the concept of unsolvability heuristics, reducing heuristic functions to dead-end detectors which either return ∞ (“dead-end”) or 0 (“don't know”). They designed unsolvability heuristics based on state-space abstractions, specifically merge-and-shrink abstractions [46], [47], [48], [45]. In UIPC'16, apart from merge-and-shrink abstractions [49], new unsolvability heuristics participated based on pattern databases[42], [50], potential heuristics[51], [52], [53], and critical-path heuristics[39], [54], [55]. The latter is the approach we introduce here. All other dead-end detectors in UIPC'16 – all that exist at the time of writing – are statically fixed before search begins, i.e., they do not learn from search experience at all.</paragraph><paragraph>Apart from the mentioned unsolvability heuristics, UIPC'16 had many participants falling into class (b), including BDDs [56], [57], partial delete-relaxation [58], [59], [60], and admissible pruning [61], [62], [63]. We will get back to this in the experiments.</paragraph></section><section label="1.3"><section-title>Our approach</section-title><paragraph>Critical-path heuristics lower-bound goal distance through the relaxing assumption that, to achieve a conjunctive subgoal G, it suffices to achieve the most costly atomic conjunction contained in G. In the original critical-path heuristics {a mathematical formula}hm, where m is a parameter, the atomic conjunctions are all conjunctions of size ≤m[39]. This restriction was later on relaxed by Haslum [64]. As part of recent works on partial delete relaxation [65], [66], [67], [68], the critical-path heuristic {a mathematical formula}hC was defined, whose parameter is a set C of atomic conjunctions that can be chosen freely.</paragraph><paragraph>It is well known that, for sufficiently large m, {a mathematical formula}hm delivers perfect goal distance estimates: simply set m to the number of state variables, reasoning over all relevant conjunctions. The latter is impractical of course, and indeed this guarantee is only of theoretical interest. Goal distance is preserved under the {a mathematical formula}hC relaxing assumption only if achieving the most costly atomic {a mathematical formula}c⊆G involves achieving the remaining subgoal {a mathematical formula}G∖c as a side effect. Practically feasible critical-path heuristics, in particular {a mathematical formula}h1, are known to typically be weak lower bounds (e.g. [40]).</paragraph><paragraph>However, matters change when we shift focus to dead-end detection rather than goal distance estimation. As a corollary of the above, for appropriately chosen C,{a mathematical formula}hCdetects all dead-ends. This is a much more realistic ambition. First, it requires accuracy, not on all states, but only on the dead-ends. Second, it is quite natural for G to be unsolvable because some small {a mathematical formula}c⊆G is. For example, in resource-constrained planning, say our remaining fuel does not suffice to reach the 3 most distant locations, out of the 100 locations G requires us to visit.</paragraph><paragraph>In summary, {a mathematical formula}hC has the necessary power to refute arbitrary dead-end states in principle, and there is reason to believe it will be useful for that purpose in practice. Our idea is to use {a mathematical formula}hC for dead-end detection, learning the conjunctions C from the conflicts – the unrecognized dead-end states – encountered during search.</paragraph><paragraph>We denote the unsolvability-heuristic variant of {a mathematical formula}hC, that returns ∞ iff {a mathematical formula}hC does, by {a mathematical formula}uC. We initialize C, before search begins, to the set of singleton conjunctions. During search, components {a mathematical formula}Sˆ of undetected dead-ends, where {a mathematical formula}uC(s)=0 for all {a mathematical formula}s∈Sˆ, are identified (become known) when all their descendants have been explored. We show how to refine{a mathematical formula}uC on such components {a mathematical formula}Sˆ, adding new conjunctions into C in a manner guaranteeing that, after the refinement, {a mathematical formula}uC(s)=∞ for all {a mathematical formula}s∈Sˆ.{sup:2} The refined {a mathematical formula}uC may generalize to other dead-ends search may encounter in the future, i.e., refining {a mathematical formula}uC on {a mathematical formula}Sˆ may result in detecting also other dead-end states {a mathematical formula}s′∉Sˆ. As we show in our experiments, this can happen at massive scale.</paragraph><paragraph>The computation of {a mathematical formula}uC, like that of {a mathematical formula}hC, is low-order polynomial time in the number of atomic conjunctions {a mathematical formula}|C|. Nevertheless, as C becomes larger, recomputing {a mathematical formula}uC on every search state may cause substantial runtime overhead. We tackle this with a form of clause learning inspired by Kolobov et al.'s [19] aforementioned SixthSense method. Whenever {a mathematical formula}uC(s)=∞, we learn a minimal clause ϕ that any non-dead-end state must satisfy, specifically where {a mathematical formula}s′⊭ϕ implies {a mathematical formula}uC(s′)=∞. We do so by starting with the disjunction of facts p false in s, and iteratively removing p – adding p into s – while preserving {a mathematical formula}uC(s)=∞; this method is inherited from SixthSense, which does essentially the same though based on {a mathematical formula}h2. Whenever we need to test whether a state {a mathematical formula}s′ is a dead-end, we first evaluate the clauses ϕ, and invoke the computation of {a mathematical formula}uC(s′) only in case {a mathematical formula}s′ satisfies all of these.</paragraph><paragraph>Arranging these techniques in a depth-first search, we obtain an algorithm approaching the elegance of nogood learning in constraint satisfaction: When a subtree is fully explored, the {a mathematical formula}uC-refinement and clause learning (i) learns to refute that subtree, (ii) backjumps to the shallowest non-refuted ancestor, and (iii) generalizes to other similar search branches in the future. Our experiments show that this can be quite powerful.</paragraph><paragraph>We provide a comprehensive evaluation with respect to the state of the art, for finding plans in solvable benchmarks with dead-ends, and for proving unsolvability in unsolvable benchmarks. We find that the success of our techniques depends on the extent of three structural properties of the input planning task: conflict identification, the ability of forward search to quickly find conflicts and thus enable the learning in the first place; effective learning, the ability to recognize dead-ends with small conjunction sets C; and generalization, the ability of {a mathematical formula}uC to detect states {a mathematical formula}s′ it was not refined on. On cases where the extent of all three properties is large, our techniques reduce depth-first search space size by orders of magnitude, and often result in performance competitive with, or surpassing, the state of the art. This is most pronounced on resource-constrained planning, specifically the benchmarks by Nakhost et al. [28] as well as over-constrained versions thereof. On competition benchmarks, this kind of structure is less frequent, but does appear in several domains from both the IPC and the UIPC.</paragraph><paragraph>For unsolvable benchmarks, we also evaluate the usefulness of the learned conjunction sets C as unsolvability certificates – a role they are suited to in principle, given they are efficiently verifiable (polynomial time in {a mathematical formula}|C|), while potentially exponentially smaller than the state space itself.</paragraph><paragraph>Section 2 provides our basic notations and the necessary planning background. Section 3 provides an example walkthrough to illustrate the workings of our techniques. Section 4 explains how we arrange forward search to identify, and learn from, dead-ends. Section 5 introduces two alternative methods for {a mathematical formula}uC refinement, applicable under different conditions. Section 6 discusses our clause learning technique. We describe our experiments in Section 7 and conclude in Section 8. Some proofs are moved out of the main text, and are available in Appendix A.</paragraph></section></section><section label="2"><section-title>Background</section-title><paragraph>We consider the STRIPS framework for classical planning. A planning task is a quadruple {a mathematical formula}Π=〈F,A,I,G〉, where {a mathematical formula}F is a set of propositions (facts), {a mathematical formula}I⊆F is the initial state, {a mathematical formula}G⊆F is the goal, and {a mathematical formula}A is a set of actions. Associated with each action {a mathematical formula}a∈A is a precondition{a mathematical formula}pre(a)⊆F, an add list{a mathematical formula}add(a)⊆F, and a delete list{a mathematical formula}del(a)⊆F. Since we are only concerned with goal reachability, which is independent of action costs, we assume unit action costs of 1 throughout.</paragraph><paragraph>A planning task is a compact representation of a transition system, its state space{a mathematical formula}ΘΠ=〈S,T,I,SG〉, where</paragraph><list><list-item label="•">{a mathematical formula}S=2F is the set of all possible states. A state{a mathematical formula}s∈S contains the facts considered to be true in s. All other facts are considered to be false.</list-item><list-item label="•">{a mathematical formula}T⊆S×A×S is the set of transitions. There is a transition from state {a mathematical formula}s∈S to state {a mathematical formula}s[[a]]∈S via action {a mathematical formula}a∈A, denoted {a mathematical formula}s→as[[a]], if a is applicable to s, i.e. {a mathematical formula}pre(a)⊆s, and {a mathematical formula}s[[a]] is given by {a mathematical formula}(s∖del(a))∪add(a). If the action does not matter, we write transitions as {a mathematical formula}s→s′.</list-item><list-item label="•">{a mathematical formula}I is Π's initial state.</list-item><list-item label="•">{a mathematical formula}SG⊆S is the set of goal states, i.e. all those {a mathematical formula}s∈S where {a mathematical formula}G⊆s.</list-item></list><paragraph> A plan for a state s is a path from s to some {a mathematical formula}t∈SG in {a mathematical formula}ΘΠ. A plan for Π is a plan for {a mathematical formula}I. If a state s does not have any plan, we call s a dead-end. If {a mathematical formula}I is a dead-end, we say that Π is unsolvable. Deciding whether a plan for a state/for Π exists is PSPACE-complete [70].</paragraph><paragraph label="Example 1">We will illustrate our techniques with a (simple) transportation planning task. Consider Fig. 1. There is one truck that should bring the two packages on the given map to their destinations, namely package 1 to {a mathematical formula}l3 and package 2 to {a mathematical formula}l1. The truck movements are subject to fuel consumption. There is no refueling so we need to make do with what's initially available.For an initial fuel amount of 5, the problem can be modeled as the following STRIPS planning task {a mathematical formula}Π=〈F,A,I,G〉. {a mathematical formula}F consists of the facts {a mathematical formula}t(x) for {a mathematical formula}x∈{l1,l2,l3} denoting the position of the truck; the location of the packages {a mathematical formula}p1(x) and {a mathematical formula}p2(x) for {a mathematical formula}x∈{l1,l2,l3,T} (T denoting that the package has been loaded into the truck); and {a mathematical formula}f(x) for {a mathematical formula}x∈{0,1,…,5} specifying the available amount of fuel. The initial state is {a mathematical formula}I={t(l2),p1(l1),p2(l3),f(5)}. The goal is {a mathematical formula}G={p1(l3),p2(l1)}. There are actions {a mathematical formula}drive(x,y,z) to drive the truck from location x to location y, assuming that z fuel units are available and driving the truck consumes one fuel unit; actions {a mathematical formula}load(p,x) to load package p at location x; and actions {a mathematical formula}unload(p,x) to unload package p at location x. For example, {a mathematical formula}a=drive(l1,l2,3) has precondition {a mathematical formula}pre(a)={t(l1),f(3)}, add list {a mathematical formula}add(a)={t(l2),f(2)}, and delete list {a mathematical formula}del(a)={t(l1),f(3)}; and {a mathematical formula}a=unload(p2,l1) has precondition {a mathematical formula}pre(a)={p2(T),t(l1)}, add list {a mathematical formula}add(a)={p2(l1)}, and delete list {a mathematical formula}del(a)={p2(T)}.A plan for Π is to drive from {a mathematical formula}l2 to {a mathematical formula}l1, load {a mathematical formula}p1, drive to {a mathematical formula}l3, unload {a mathematical formula}p1 and load {a mathematical formula}p2, drive back to {a mathematical formula}l1, and unload {a mathematical formula}p2. This plan consumes all 5 fuel units. There is no plan that consumes less than 5 fuel units, so if we set the initial fuel amount to a value smaller than 5, then Π becomes unsolvable.</paragraph><paragraph>A heuristic is a function {a mathematical formula}h:S→N0+∪{∞}. By {a mathematical formula}h⁎, we denote the perfect heuristic, which maps every s to the length of a shortest plan for s, or to ∞ if s is a dead-end.</paragraph><paragraph>Following Hoffmann et al. [36], we say that a heuristic u is an unsolvability heuristic, or dead-end detector, if u assigns each state to either ∞ or 0. The interpretation of {a mathematical formula}u(s)=∞ will be “dead-end”, that of {a mathematical formula}u(s)=0 will be “don't know”. We require u to be sound, i.e., whenever {a mathematical formula}u(s)=∞ then s is indeed a dead-end. In other words, there are no false positives. This is to preserve optimality/completeness when pruning states where {a mathematical formula}u(s)=∞. On the other hand, the dead-end detector may return {a mathematical formula}u(s)=0 although s is actually a dead-end, i.e., false negatives are possible. Following established terminology [71], [72], we refer to dead-end states s where {a mathematical formula}u(s)=∞ as recognized, and to those where {a mathematical formula}u(s)=0 as unrecognized. The ideal dead-end detector would be the perfect one, denoted {a mathematical formula}u⁎, that recognizes all dead-ends, as this would allow us to prune all dead-ends during search. However, like {a mathematical formula}h⁎, computing {a mathematical formula}u⁎ corresponds to solving the input planning task in the first place.</paragraph><paragraph>The family of critical-path heuristics, which underlie Graphplan [10] and were formally introduced by Haslum and Geffner [39], approximate the cost of achieving a fact conjunction, e.g. the goal, by the cost of achieving the most expensive atomic conjunction taken into account by the heuristic. In the original formulation, the critical-path heuristic {a mathematical formula}hm considered as atomic all fact conjunctions of size of at most m, {a mathematical formula}m∈N being a parameter of the heuristic. This restriction was later on relaxed by Haslum [64], who designs a procedure approximating {a mathematical formula}hm, considering only part of the size-m subgoals. Most recently, Hoffmann and Fickert [73] defined the heuristic {a mathematical formula}hC, parameterized by an arbitrary set C of fact conjunctions.</paragraph><paragraph>To formalize critical-path heuristics, we define the regression of a fact set G over an action a as {a mathematical formula}R(G,a):=(G∖add(a))∪pre(a) in case that {a mathematical formula}add(a)∩G≠∅ and {a mathematical formula}del(a)∩G=∅; otherwise, the regression is undefined and we write {a mathematical formula}R(G,a)=⊥. By {a mathematical formula}A[G] we denote the set of actions where {a mathematical formula}R(G,a)≠⊥. We identify fact conjunctions with fact sets; let {a mathematical formula}C⊆2F be any set of conjunctions. The generalized critical-path heuristic {a mathematical formula}hC(s) is defined through {a mathematical formula}hC(s):=hC(s,G) where{a mathematical formula} Note here that we overload {a mathematical formula}hC to denote both, a function of state s in which case the estimated distance from s to the global goal {a mathematical formula}G is returned, and a function of state s and subgoal G in which case the estimated distance from s to G is returned. We will use this notation convention throughout.</paragraph><paragraph>The definition of {a mathematical formula}hC distinguishes between three cases. If the subgoal is already true in the considered state s (top case), then its value is 0. If the subgoal G is not an atomic conjunction (bottom case), then its {a mathematical formula}hC value is estimated by the most expensive atomic subgoal that is a subset of G. Otherwise (middle case), G is an atomic subgoal that is not already true in the considered state. Then the {a mathematical formula}hC value is set to the cheapest possible way of achieving G, by minimizing over the actions that can be used to achieve G, and computing the resulting costs recursively for each.</paragraph><paragraph>Observe that, in the middle case, if there exists no action that can be used to achieve G, then the minimization evaluates to ∞, and thus the overall outcome value may be {a mathematical formula}hC(s)=∞. Intuitively, this happens if s has no solution even when allowing to break up subgoals into the elements of C. As we are interested only in dead-end detection, not goal distance estimation, this is the main ability of {a mathematical formula}hC we are interested in. Consequently, most of the time we will consider not {a mathematical formula}hC but the critical-path unsolvability heuristic, denoted {a mathematical formula}uC, defined by {a mathematical formula}uC(s):=∞ if {a mathematical formula}hC(s)=∞, and {a mathematical formula}uC(s):=0 otherwise.</paragraph><paragraph>One can compute {a mathematical formula}hC, solving Equation (1), in time polynomial in {a mathematical formula}|C| and the size of Π, using simple dynamic programming algorithms similar to those for {a mathematical formula}hm[39]. It is known that, in practice, {a mathematical formula}hm is reasonably fast to compute for {a mathematical formula}m=1, consumes substantial runtime for {a mathematical formula}m=2, and is typically infeasible for {a mathematical formula}m=3. The behavior is similar when using general conjunction sets C, in the sense that very large C can incur substantial runtime overhead. As hinted, we will use a clause-learning technique to alleviate this.</paragraph><paragraph>The conjunction set C is a very powerful algorithm parameter. The downside is, how to choose the value of that parameter? This question has been previously addressed only in the context of partial delete-relaxation heuristics [66], [67], [73], which extract delete-relaxed plans on top of {a mathematical formula}hC. All known methods learn C offline, prior to search, by iteratively refining a delete-relaxed plan for the initial state. Once this refinement process stops, the same set C is used throughout the search. Departing from this, here we learn C online, based on conflict analysis during search.</paragraph></section><section label="3"><section-title>An illustrative example walkthrough</section-title><paragraph>To provide the reader with an intuition before delving into the technical details, we next give an example walkthrough. We illustrate the overall search process, and how the learning (i) refutes completed parts of the search, (ii) leads to backjumping, and (iii) generalizes to other similar search branches.</paragraph><paragraph>Reconsider Example 1, and assume an initial fuel amount of 2. This is insufficient to transport the packages to their goal locations, so the planning task is unsolvable. However, to prove unsolvability, a standard state space search needs to explore all action sequences containing at most two {a mathematical formula}drive actions. In particular, the search needs to explore two very similar main branches, driving first to {a mathematical formula}l1 vs. driving first to {a mathematical formula}l3. Using our methods, the learning on one of these branches immediately excludes the other branch.</paragraph><paragraph>Fig. 2 illustrates the search space for a depth-oriented search (with open &amp; closed lists but expanding the deepest node first) using our methods. The set of atomic conjunctions C is initialized to the singleton conjunctions, i.e. single facts, {a mathematical formula}C={{p}|p∈F}. In other words, with the initial conjunction set C, {a mathematical formula}uC(s)=∞ iff {a mathematical formula}h1(s)=∞. As regression over singleton subgoals ignores the delete lists, this is equivalent to the goal being delete-relaxed unreachable (relaxed-unreachable for short). In our example, this basically means to ignore fuel consumption so long as at least one fuel unit is left.</paragraph><paragraph>In the initial state {a mathematical formula}I, the goal is relaxed-reachable and we get {a mathematical formula}uC(I)=0. Thus, {a mathematical formula}I is expanded: the truck can be driven to {a mathematical formula}l1 (resulting in state {a mathematical formula}s1) or to {a mathematical formula}l3 (resulting in state {a mathematical formula}s2). In both cases, the remaining fuel is decreased to 1. In particular, some fuel is still available, so the goal remains relaxed-reachable, {a mathematical formula}uC(s1)=uC(s2)=0. Say we expand {a mathematical formula}s1 next. Loading package {a mathematical formula}p1 results in state {a mathematical formula}s3, and since loading a package does not affect the fuel, we still have {a mathematical formula}uC(s3)=0. On the other hand, driving the truck back to {a mathematical formula}l2 (state {a mathematical formula}s4) sets the available fuel to 0, and thus the goal becomes relaxed-unreachable from {a mathematical formula}s4, {a mathematical formula}uC(s4)=∞ (in fact, {a mathematical formula}s4 can be recognized trivially as a dead-end, since there are no actions).</paragraph><paragraph>Whenever we obtain {a mathematical formula}uC(s)=∞ on some state s, in the present case {a mathematical formula}uC(s4)=∞, we call clause learning on s to identify a clause ϕ where, for all states {a mathematical formula}s′, {a mathematical formula}s′⊭ϕ implies {a mathematical formula}uC(s′)=∞. This is not mandatory in theory – the dead-end detection power of the learned clauses is dominated by that of {a mathematical formula}uC – but can be useful in practice, as it can reduce the number of calls to {a mathematical formula}uC and hence the runtime overhead. To identify the clause ϕ, we minimize the inverse state {a mathematical formula}F∖s – the set of facts false in s – to obtain a minimal reason for {a mathematical formula}uC(s)=∞. In the specific case of {a mathematical formula}s4, the learned clause is {a mathematical formula}ϕ=t(l3)∨f(1)∨f(2)∨p1(l3). To understand that clause, observe that {a mathematical formula}s4 remains a dead-end regardless of the position of {a mathematical formula}p2; and, if {a mathematical formula}p1 is not at the goal location {a mathematical formula}l3, then it has to be transported to {a mathematical formula}l3 meaning that either the truck has to be at {a mathematical formula}l3, or there must be sufficient fuel to drive to {a mathematical formula}l3.</paragraph><paragraph>After the computation of ϕ has finished, search continues with the expansion of {a mathematical formula}s3, where {a mathematical formula}p1 can be unloaded (leading back to {a mathematical formula}s1), or the truck can be driven to {a mathematical formula}l2 (resulting in {a mathematical formula}s5). The latter state is a dead-end because we ran out of fuel. That will be recognized by {a mathematical formula}uC of course. Observe, however, that {a mathematical formula}s5⊭ϕ, so we recognize {a mathematical formula}s5 to be a dead-end without even invoking the computation of {a mathematical formula}uC(s5).</paragraph><paragraph>Now the descendants of {a mathematical formula}s3 have been fully explored, so {a mathematical formula}s3 becomes a known, yet unrecognized, dead-end. In other words: search has encountered a conflict.</paragraph><paragraph>To learn from that conflict, we start a refinement of C, in a manner guaranteeing that {a mathematical formula}uC recognizes {a mathematical formula}s3 to be a dead-end after the refinement. In the present case, the refinement algorithm – precisely, “neighbors refinement” which will be explained in Section 5.2 – extends C by a single atomic conjunction, {a mathematical formula}c={t(l2),f(1)}. The details of the refinement algorithm are technically involved, so we omit them here. For now, observe that adding c into C indeed suffices to obtain {a mathematical formula}uC(s3)=∞. This is because, to achieve the goal {a mathematical formula}p1(l3), we need to unload {a mathematical formula}p1 at {a mathematical formula}l3 which has {a mathematical formula}t(l3) in its precondition. The {a mathematical formula}drive(l2,l3,z) actions achieving {a mathematical formula}t(l3) have {a mathematical formula}f(z) in their precondition, which in {a mathematical formula}s3 is possible only for {a mathematical formula}z=1. However, {a mathematical formula}drive(l2,l3,1) has the new atomic conjunction {a mathematical formula}c={t(l2),f(1)} in its precondition. Although each fact {a mathematical formula}t(l2) and {a mathematical formula}f(1) is reachable from {a mathematical formula}s3 individually – drive to {a mathematical formula}l2 for {a mathematical formula}t(l2), do nothing for {a mathematical formula}f(1) – they are not reachable in conjunction. The latter is recognized by {a mathematical formula}uC when including c into C, i.e., {a mathematical formula}uC(s3,c)=∞ for {a mathematical formula}C={{p}|p∈F}∪{c}. In summary, adding c to C lets {a mathematical formula}uC recognize that a single fuel unit is not enough to solve {a mathematical formula}s3, because there is insufficient fuel to drive to location {a mathematical formula}l3.</paragraph><paragraph>Having finished the {a mathematical formula}uC refinement on {a mathematical formula}s3, we call the clause learning on {a mathematical formula}s3. We learn the clause {a mathematical formula}t(l2)∨t(l3)∨f(2)∨p1(l3). That clause does not hold in {a mathematical formula}s1, so we have shown (without invoking {a mathematical formula}uC on {a mathematical formula}s1) that {a mathematical formula}uC(s1)=∞. In particular, as advertised, (i) {a mathematical formula}uC now refutes the entire state space below the conflict node {a mathematical formula}s3, and (ii) search can backjump to the shallowest non-refuted ancestor, {a mathematical formula}I.{sup:3}</paragraph><paragraph>Finally, {a mathematical formula}s2 is the only state left open. However, re-evaluating {a mathematical formula}uC(s2) before expanding {a mathematical formula}s2, we find that {a mathematical formula}uC(s2)=∞. Similarly as in {a mathematical formula}s3, getting the goal {a mathematical formula}p2(l3) requires to achieve the atomic conjunction {a mathematical formula}c={t(l2),f(1)} in the first place, yet that conjunction is unreachable from {a mathematical formula}s2. In other words, (iii) the knowledge learned on the previous search branch, in the form of the reasoning encapsulated by the extended conjunctions set {a mathematical formula}C={{p}|p∈F}∪{c}, generalizes to the present search branch.</paragraph><paragraph>With {a mathematical formula}s2 pruned, there are no more open nodes, and unsolvability is proved without ever exploring the option to drive to {a mathematical formula}l3 first. We could at this point continue, running the learning process on the now known-yet-unrecognized dead-end {a mathematical formula}I: if we keep running our search on an unsolvable task, then {a mathematical formula}uC eventually learns to refute the initial state itself.</paragraph><paragraph>We now explain these algorithms in detail. We cover the identification of conflicts during search, conflict analysis &amp; refinement for {a mathematical formula}uC, and clause learning, in this order.</paragraph></section><section label="4"><section-title>Forward search with conflict identification &amp; learning</section-title><paragraph>Assume some unsolvability heuristic u. We will henceforth refer to dead-end states s unrecognized by u, {a mathematical formula}u(s)=0, as conflicts. We wish to learn from conflicts, refining u, during search. For that purpose, we need to augment search to identify the conflicts in the first place; and we need to say where exactly to learn.</paragraph><paragraph>This is straightforward in principle, but subtleties arise from the need to do so efficiently. After any search step, how to navigate directly to the known conflict states? And actually, how to directly identify the components of such states, which may be (and is, for one of our {a mathematical formula}uC refinement methods) relevant to the learning?</paragraph><paragraph>We spell out these subtleties in what follows. We first (Section 4.1) design a generic extension to search algorithms using open &amp; closed lists, like {a mathematical formula}A⁎, greedy best-first search, etc., preserving their optimality and completeness guarantees. We then (Section 4.2) design a dedicated depth-first search variant, which has turned out to be most useful in our experiments as it identifies conflicts much more quickly, facilitating the learning process. Throughout this section, we consider an arbitrary unsolvability heuristic u, the only assumption being that there exists a refinement method which, given a conflict state s, refines u to recognize s.</paragraph><section label="4.1"><section-title>Generic search algorithm</section-title><paragraph>Algorithm 1 shows pseudo-code for our procedure. Consider first only the main loop, a generic search that can be instantiated into standard search algorithms in the obvious manner by suitable handling of the open and closed lists. The only difference to the standard algorithms then lies in the dead-end pruning, (a) via {a mathematical formula}u(s) at node expansion time, (b) via {a mathematical formula}u(s′) at node generation time, and (c) via a call to the {a mathematical formula}CheckAndLearn() procedure after state expansion. Of these, (a) and (b) are straightforward. A state is pruned, and considered closed, if it is detected to be a dead-end. Note that (a) makes sense despite the fact that s was already tested by (b) when it was first generated. This is because u may have been refined in the meantime, and may now recognize s to be a dead-end.</paragraph><paragraph>The conflict identification and learning process is organized by (c), the {a mathematical formula}CheckAndLearn() procedure. Before explaining that procedure, we introduce some basic observations and terminology: we will capture the “knowledge” of the search in terms of a partial state-space graph. We require some notations identifying such graphs, and we require a simple comparison concept that will serve to identify the class of all possible search graphs consistent with the search knowledge.</paragraph><paragraph>Denote by {a mathematical formula}ΘΠ=〈SΠ,TΠ,IΠ,SGΠ〉 the state space of our input task Π. We consider transition systems {a mathematical formula}Θ=〈S,T,IΠ,SG〉 over subsets {a mathematical formula}S⊆SΠ of states, with {a mathematical formula}SG=SGΠ∩S, and with potentially arbitrary transitions {a mathematical formula}T. Given such Θ, for a subset {a mathematical formula}S′⊆S of states, by {a mathematical formula}Θ|S′ we denote the subgraph of Θ induced by {a mathematical formula}S′. Given two such transition systems {a mathematical formula}Θ1=〈S1,T1,IΠ,SG1〉 and {a mathematical formula}Θ2=〈S2,T2,IΠ,SG2〉, and a state set {a mathematical formula}S′⊆S1 ({a mathematical formula}S′ will be the closed list below), we say that {a mathematical formula}Θ2coincides with {a mathematical formula}Θ1 on {a mathematical formula}S′ if {a mathematical formula}S′⊆S2 and, for all {a mathematical formula}s∈S′, {a mathematical formula}s→2s′ if and only if {a mathematical formula}s′∈S1 and {a mathematical formula}s→1s′.</paragraph><paragraph>Assume now any time point during search using Algorithm 1. We define a transition system reflecting the current search, namely, the transition system which is like {a mathematical formula}ΘΠ|Open∪Closed except that closed states {a mathematical formula}s∈Closed that were pruned – that were detected to be dead-ends – do not have any outgoing transitions. We refer to this transition system as the search graph, and we denote it by {a mathematical formula}Θsearch. We say that a state s is a known dead-end if, given the search graph – intuitively, given the search “knowledge” about {a mathematical formula}ΘΠ so far – s must be a dead end. Formally, capturing the search knowledge as the set of transition systems Θ that coincide with {a mathematical formula}Θsearch on Closed, s is a known dead-end if it is a dead-end in every such Θ. In other words, search knows s to be a dead-end if that is so in all state spaces indistinguishable from the present one given the search so far.</paragraph><paragraph>It is easy to see that the known dead-ends are exactly the states all of whose descendants in the search graph are already closed. That is, denoting{a mathematical formula} we have:</paragraph><paragraph label="Proposition 1">At any time point during the execution ofAlgorithm 1, the known dead-ends are exactly those states s where{a mathematical formula}R[s]⊆Closed.</paragraph><paragraph label="Proof">First, say that {a mathematical formula}R[s]⊆Closed. Consider any descendant state t of s in the current search graph. Then t is closed, either because it has already been expanded, or because it has been detected as a dead-end. In the former case, all outgoing transitions of t lead to states in Closed; in the latter case, t does not have any outgoing transitions. Let now Θ be a transition system over {a mathematical formula}S⊆SΠ as above, that coincides with {a mathematical formula}Θsearch on Closed. Then all states reachable from s in Θ are contained in Closed. As {a mathematical formula}Closed∩SG=∅, s must be a dead-end in Θ, which is what we needed to prove.Vice versa, if {a mathematical formula}R[s]⊈Closed, then some descendant t of s in the current search graph is still open. We can construct a counter-example Θ simply by extending {a mathematical formula}Θsearch|Open∪Closed with a direct transition from t to some goal state.  □</paragraph><paragraph>Given this, a naïve means to identify all known conflicts is to evaluate, after every state expansion and for every {a mathematical formula}s∈Closed, whether {a mathematical formula}R[s]⊆Closed. But one can do much better than this, by a dead-end labeling procedure.</paragraph><paragraph>One might, at first sight, expect such a labeling procedure to be trivial, doing a simple bottom-up labeling following the reasoning that, if all direct successors of s are already known dead-ends, then s is a known dead-end as well. Such a simple procedure would, however, be incomplete, i.e., would in general not label all known dead-ends, due to cycles. If states {a mathematical formula}s1 and {a mathematical formula}s2 are dead-ends but have outgoing transitions to each other, then neither of the two will ever be labeled. Our labeling method, conducted as part of {a mathematical formula}CheckAndLearn(), thus involves complete lookaheads on the current search graph, but on only those states that might actually have become a known dead-end given the last state expansion. Namely, a state t can only become a known dead-end after the expansion of a descendant s of t where {a mathematical formula}R[s]⊆Closed after the expansion: otherwise, either the descendants of t have not changed at all, or t still has at least one open descendant.</paragraph><paragraph>Consider now the bottom part of Algorithm 1. In the top-level invocation of {a mathematical formula}CheckAndLearn(s), s cannot yet be labeled; the label check at the start of {a mathematical formula}CheckAndLearn() is needed only for loop detection in recursive invocations, cf. below. The test on {a mathematical formula}R[s] corresponds to Proposition 1. For {a mathematical formula}t∈R[s], as t is reachable from s, we have {a mathematical formula}R[t]⊆R[s], and thus {a mathematical formula}R[t]⊆Closed, so all {a mathematical formula}t∈R[s] are known dead-ends as well. Some t may be recognized already, {a mathematical formula}u(t)=∞, and thus (be dead-ends but) not be conflicts. If that is so for all {a mathematical formula}t∈R[s], then the refinement step is void and can be skipped.</paragraph><paragraph>Backward propagation on the parents of s is needed to identify all dead-ends known at this time. Observe that the recursion will eventually reach all ancestors t of s, and thus all states t that might have become a known dead-end. Given the label check at the start of {a mathematical formula}CheckAndLearn(), every state is labeled at most once and hence {a mathematical formula}|Closed| is an obvious upper bound on the number of recursive invocations, even if the state space contains cycles. Note that, in each recursive call, we can only label s itself, not all {a mathematical formula}t∈R[s]. This is because {a mathematical formula}R[s] may contain ancestors t of s, and some other ancestor {a mathematical formula}t′ of s may be connected to s only via such t. In that case, {a mathematical formula}t′ would not be reached by the recursion.</paragraph><paragraph>In short, we label known dead-end states bottom-up along forward search transition paths, conducting a full lookahead of the current search graph in each. With the arguments outlined above (a full proof is available in Appendix A), this is sound and complete relative to the search knowledge:</paragraph><paragraph label="Theorem 1">At the start of the while loop inAlgorithm 1, the labeled states are exactly the known dead-ends.</paragraph><paragraph label="Example 2">Reconsider the search space on our running example, depicted in Fig. 2. After expansion of {a mathematical formula}s3, the call to {a mathematical formula}CheckAndLearn(s3) constructs {a mathematical formula}R[s3]={s3,s1}, and finds that {a mathematical formula}R[s3]⊆Closed. Thus {a mathematical formula}s3 is labeled, and u is refined to recognize {a mathematical formula}s3 and {a mathematical formula}s1. Backward propagation then calls {a mathematical formula}CheckAndLearn(s1), the parent of {a mathematical formula}s3. As we have the special case of an ancestor {a mathematical formula}t∈R[s], all states in {a mathematical formula}R[s1] are already recognized so the refinement step is skipped. The recursive calls on the parents of {a mathematical formula}s1, {a mathematical formula}CheckAndLearn(s3) and {a mathematical formula}CheckAndLearn(I), find that {a mathematical formula}s3 is already labeled, respectively that {a mathematical formula}R[I]⊈Closed, so the procedure terminates here.</paragraph><paragraph>It is worth noting that the search “knowledge” considered in the above is only the explicit knowledge, about states the search has already expanded or pruned. This disregards the implicit knowledge potentially present due to generalization: refining u on {a mathematical formula}R[s] might recognize dead-ends {a mathematical formula}t′∉R[s]. In particular, the search might have already visited {a mathematical formula}t′, {a mathematical formula}t′∈Open∪Closed, and then, via u, the search might actually already know that {a mathematical formula}t′ (and potentially some of its ancestors) are dead-ends.</paragraph><paragraph>One can capture this formally by denoting with {a mathematical formula}U:={t′|t′∈S,u(t′)=∞} the currently recognized dead-end states; defining {a mathematical formula}Θsearch[U] to be like {a mathematical formula}Θsearch except that all {a mathematical formula}t′∈U have no outgoing transitions; and defining a state to be a u-known dead end if it is a dead-end in all Θ that coincide with {a mathematical formula}Θsearch[U] on {a mathematical formula}Closed∪(Open∩U). The u-known dead-ends then are exactly those s where {a mathematical formula}R[s]⊆Closed∪U. To find all these s during search – and thus immediately learn from all already identified dead-end states – after every state expansion, we would have to reevaluate u on the entire open and closed lists (plus backward propagation whenever a new dead-end is found). This would cause prohibitive overhead. Hence we stick to learning only on the known dead-ends, explicitly captured by the search.</paragraph><paragraph>An optimization we do apply in our instantiation of Algorithm 1 is to reevaluate the learned clauses (see Section 6) every time {a mathematical formula}R[s] is computed: if a descendant of s is recognized by the clauses in the meantime, consider it closed. As clause evaluation is fast, this tends to pay off. In particular, it allows to not include detected dead-ends s into the closed list in the first place, as such s will be recognized by the clauses anyway.</paragraph><paragraph>Algorithm 1 has several desirable properties regardless of its concrete instantiation:</paragraph><list><list-item label="(1)">Preserving guarantees: Instantiating the main loop to reflect any standard search algorithm, the optimality and/or completeness guarantees of that algorithm are preserved, as the only change is the pruning of dead-end states.</list-item><list-item label="(2)">Unsolvability certificate: Upon termination, we have {a mathematical formula}u(I)=∞, due to the final call to {a mathematical formula}CheckAndLearn(), doing backward propagation when all nodes are closed.In case an unsolvability certificate is not required, the final call to {a mathematical formula}CheckAndLearn() is redundant work. In our implementation, we provide an early termination option, which skips that step when the open list is already empty.</list-item><list-item label="(3)">Bail-out: Provided the unsolvability heuristic is transitive, search terminates without any further state expansions if an unsolvability certificate is already found. Here, we say that u is transitive if {a mathematical formula}u(s)=∞ implies {a mathematical formula}u(t)=∞ for all descendants t of s. This is a natural property for u to have – after all, proving s to be a dead end involves some form of reasoning about its descendants – and it does hold for the {a mathematical formula}uC unsolvability heuristic considered here. With transitive u, if {a mathematical formula}u(I)=∞ then {a mathematical formula}u(s)=∞ for all s, hence no more states will be expanded.</list-item></list><paragraph>We remark that the problem of labeling the known dead-end states relates closely to cost revision steps in cyclic AND-OR graphs, as done in {a mathematical formula}AO⁎ and other AND-OR search algorithms (e.g. [74], [75], [76]). It is equivalent to labeling solved nodes when viewing states as AND nodes, and viewing actions as trivial OR nodes with a single outgoing edge (action outcome). Effective methods for labeling solved nodes, without complete forward lookaheads, have been designed, yet these exploit non-trivial OR nodes (through, e.g., focusing on a current greedy policy [75]). It remains an open question whether such methods can be beneficial for our purposes. In any case, as we shall see next, in depth-first search – which turns out to be most useful in practice – the issue of forward lookaheads disappears.</paragraph></section><section label="4.2"><section-title>Depth-first search</section-title><paragraph>Depth-first search (DFS) is particularly well suited for our purposes, because it fully explores the descendants of a state before proceeding with anything else. In other words, DFS is geared at obtaining {a mathematical formula}R[s]⊆Closed as quickly as possible. This is key to identifying conflicts quickly.</paragraph><paragraph>But what exactly does DFS look like in our context? The issue is that state spaces are, in general, cyclic, and nodes may have solutions via their parents. In our running example, {a mathematical formula}s3 has a transition to its parent {a mathematical formula}s1. A simple way to tackle this is what we will refer to as depth-oriented search (DOS) (as previously indicated in Section 3), instantiating Algorithm 1 with a depth-first search order, ordering the open list by decreasing distance from the initial state.</paragraph><paragraph>It turns out that one can do better though. We next design an elegant DFS variant of our approach, similar to backtracking in constraint satisfaction problems. Consider first, to get some intuitions, the acyclic case. This is restricted yet not entirely unrealistic: acyclic state spaces naturally occur, e.g., if every action consumes a non-0 amount of budget or resource. In DFS on an acyclic state space, state s becomes a known dead-end exactly the moment its subtree has been completed, i.e., when we backtrack out of s. Hence we can simply refine u at this point. As the same has previously been done on the children {a mathematical formula}s′ of s, we will have {a mathematical formula}u(s′)=∞ for every such {a mathematical formula}s′, so the conflict component {a mathematical formula}R[s] simplifies to just s. Overall, the complex CheckAndLearn procedure can be replaced by refining u on s at backtracking time. But then, we do not need the open and closed lists anymore, and can instead use a classical DFS.</paragraph><paragraph>In the cyclic case, matters are not that easy. But it turns out that one can obtain a valid DFS algorithm (which defaults to classical DFS in the acyclic case) from Tarjan's algorithm to compute maximal strongly connected components (SCCs) [77]. (Which has previously been put to use in certain dynamic programming algorithms for probabilistic planning [78], [23].)</paragraph><paragraph>Algorithm 2 shows the pseudo-code. The key observation is that s becomes a known dead-end exactly at the moment when we have identified the maximal SCC {a mathematical formula}S⊆S that contains s, i.e., once DFS backtracks out of the last state in S. This is simply because, with {a mathematical formula}R[s]⊆Closed, we must also have {a mathematical formula}R[t]⊆Closed for any ancestor state t of s reachable from s. Thus, to get rid of the expensive CheckAndLearn procedure, DFS can use Tarjan's algorithm to identify the maximal SCCs, and refine u whenever a maximal SCC has been found. Henceforth, whenever we say “DFS”, we mean DFS as per Algorithm 2.</paragraph><paragraph>Regarding the properties of DFS, obviously property (1 preserving guarantees) from above is not meaningful here, and DFS is complete but not optimal. DFS inherits properties (2 unsolvability certificate) and (3 bail-out). Like for Algorithm 1, we implemented a simple early termination option in case an unsolvability certificate is not desired. DFS furthermore has several desirable properties beyond (2) and (3):</paragraph><list><list-item label="(4)">Backjumping: Due to the pruning test on {a mathematical formula}u(s)=∞ inside the state-expansion loop, DFS will backjump across predecessor states s that are now recognized dead-ends. For transitive unsolvability heuristics, the backjump will be across all recognized dead-ends on the current search path, as {a mathematical formula}u(s)=∞ implies {a mathematical formula}u(t)=∞ for all t below s.</list-item><list-item label="(5)">Immediate u-known learning: DFS guarantees to learn, before the next state expansion, on all dead-ends {a mathematical formula}t′ that are u-known but not known, and where {a mathematical formula}u(t′)≠∞ (there is still something to learn on {a mathematical formula}t′). To see this, let {a mathematical formula}t′ be such a state. As {a mathematical formula}t′ is not a known dead-end, it must lie along the current search path {a mathematical formula}t→. As {a mathematical formula}u(t′)≠∞ but {a mathematical formula}t′ is a u-known dead-end, {a mathematical formula}t′ must be an inner node along {a mathematical formula}t→, and every leaf node s along {a mathematical formula}t→ reachable from {a mathematical formula}t′ must satisfy {a mathematical formula}u(s)=∞. But then, search backtracks out of the SCC containing {a mathematical formula}t′ without any further state expansions, which is what we needed to show.</list-item><list-item label="(6)">Duplicate pruning for free: As u learns to refute the subtree below s, it subsumes the duplicate pruning that would be afforded by a closed list. Due to generalization, if will often surpass that pruning by far.</list-item></list><paragraph> Compared to this, in the generic search of Algorithm 1, towards (4) one can test, at the start of the {a mathematical formula}CheckAndLearn() procedure, whether {a mathematical formula}u(s)=∞. This leads to backjumping in depth-oriented search, and leads to aggressive pruning of search paths in other open list based searches like greedy best-first search. For (5), as discussed this does not hold for Algorithm 1 in general, as the new u-known states may lie on arbitrary search paths; it does hold for depth-oriented search though. Finally, (6) is specific to DFS, and cannot be exploited by Algorithm 1 regardless of the search order, as that algorithm needs to maintain a closed list anyway. Intuitively, depth-first search is closer to the structure of dead-end detection, and combines more gracefully with it than other search algorithms.</paragraph><paragraph>For both Algorithm 1 and Algorithm 2, in practice it is often useful to combine several dead-end detectors {a mathematical formula}{u1,…,uk}, instantiating u in the respective pseudo-code with {a mathematical formula}maxi⁡ui to profit from complementary dead-end detection capabilities. The refinement step then in principle allows arbitrary combinations of refinements on the individual {a mathematical formula}ui. Here, we will empirically investigate the combination of {a mathematical formula}uC with the aforementioned dead-end detectors u based on merge-and-shrink abstraction [36], [49] respectively potential heuristics [52], [53]. As refinement methods for the latter are not available at the time of writing, we will refine {a mathematical formula}uC only. A subtlety that arises in this context regards the handling of dead-end states recognized by u but not by {a mathematical formula}uC. We now consider the refinement step in detail.</paragraph></section></section><section label="5"><section-title>Conflict analysis &amp; refinement for critical-path heuristics</section-title><paragraph>We now tackle the refinement step in Algorithm 1, Algorithm 2, for the dead-end detector {a mathematical formula}uC. Given {a mathematical formula}R[s] where all {a mathematical formula}t∈R[s] are dead-ends, how to refine {a mathematical formula}uC on {a mathematical formula}R[s] to recognize all these dead-ends?</paragraph><paragraph>Naturally, the refinement will add a set X of conjunctions into C. A suitable refinement is always possible, i.e., there exists X s.t. {a mathematical formula}uC∪X(s)=∞ for all {a mathematical formula}t∈R[s]. But how to find such X?</paragraph><paragraph>One possibility is to use known conjunction-learning methods from the literature [66], [69], [67], which iteratively remove conflicts in delete-relaxed plans for a given state s. These methods do guarantee to eventually recognize s if it is a dead-end. But they are not geared to this purpose, and as we shall see, are not effective in practice for that purpose. Here we introduce two methods specifically designed for dead-end detection: path-cut refinement and neighbors refinement.</paragraph><paragraph>The major difference between the two methods lies in their applicability. Neighbors refinement applies only to {a mathematical formula}R[s] that satisfy what we call the {a mathematical formula}uC-recognized neighbors property. Consider the neighboring states t of {a mathematical formula}R[s], i.e., those with an incoming transition from {a mathematical formula}R[s]. All these t must already be recognized as dead-ends. But recognized by which dead-end detector? We say that {a mathematical formula}R[s] has {a mathematical formula}uC-recognized neighbors if all t are recognized by {a mathematical formula}uC, {a mathematical formula}uC(t)=∞. This necessarily holds if {a mathematical formula}uC is the only dead-end detector used. But if {a mathematical formula}uC is combined with some other dead-end detector u, then some of the states t may be recognized only by u, not by {a mathematical formula}uC.</paragraph><paragraph>It turns out that the recognized neighbors property can be exploited for an especially effective refinement method, neighbors refinement. For the general case, we design the alternate path-cut refinement method.</paragraph><paragraph>Path-cut refinement (Section 5.1) learns conjunctions X cutting off the critical paths in a {a mathematical formula}hC computation reaching the goal. One such refinement step guarantees to strictly increase the value of {a mathematical formula}hC. To render {a mathematical formula}hC infinite as desired, we need to iterate these refinement steps, recomputing {a mathematical formula}hC in between iterations. Neighbors refinement (Section 5.2), in contrast, is a constructive method, identifying the new conjunctions X directly from those for the neighbor states, without necessitating any intermediate recomputations of {a mathematical formula}uC.</paragraph><section label="5.1"><section-title>Path-cut refinement</section-title><paragraph>Path-cut refinement assumes some arbitrary dead-end state s as input, and augments C to recognize s. To recognize all dead-ends within the component {a mathematical formula}R[s], we run the method on s only. Due to the aforementioned transitivity property of {a mathematical formula}uC, this suffices to recognize all states in {a mathematical formula}R[s].</paragraph><paragraph>The refinement is based on cutting off critical paths, i.e., the recursion paths in the definition of {a mathematical formula}hC (Equation (1)). The refinement is iterative, where each iteration identifies a set X of conjunctions adding which into C guarantees to strictly increase {a mathematical formula}hC(s). Given this, the method really pertains to {a mathematical formula}hC rather than the simplified {a mathematical formula}uC, and it applies not only to dead-end states, but to any state s where {a mathematical formula}hC(s)&lt;h⁎(s). Therefore, for the remainder of this subsection, we will talk about {a mathematical formula}hC, not {a mathematical formula}uC. At the end of the refinement on a dead-end state s, we will have {a mathematical formula}hC(s)=uC(s)=∞.</paragraph><paragraph>We consider now in detail a single refinement step (one iteration of the overall refinement). In what follows, like in Equation (1) we use {a mathematical formula}hC(s,G) to denote the {a mathematical formula}hC value of subgoal fact set G, i.e., the approximated cost, given the {a mathematical formula}hC relaxation of achieving G from s. Correspondingly, we use {a mathematical formula}h⁎(s,G) to denote the real cost of achieving G from s. The {a mathematical formula}hC recursion path on a current subgoal G is cut off by identifying a small conjunction {a mathematical formula}x⊆G that cannot be achieved with action sequences of length at most {a mathematical formula}hC(s,G). The union of these x over all critical recursion paths yields the desired set X. Algorithm 3 shows the pseudo-code.</paragraph><paragraph>To understand Algorithm 3, consider the initializing call on {a mathematical formula}G=G and {a mathematical formula}n=hC(s). Our aim is to identify a (small) conjunction {a mathematical formula}x⊆G that cannot be achieved from s by any action sequence of length at most {a mathematical formula}hC(s,G). Towards finding such x, we start by selecting an arbitrary critical (maximum {a mathematical formula}hC value) atomic conjunction {a mathematical formula}c∈C, {a mathematical formula}c⊆G. We initialize {a mathematical formula}x:=c. As {a mathematical formula}hC(s,c)=n, c is achieved, under the {a mathematical formula}hC approximation, by an action sequence of length n. However, as {a mathematical formula}n=hC(s,G)&lt;h⁎(s,G), we know that we can extend {a mathematical formula}x=c with additional facts {a mathematical formula}p∈G∖c in a way excluding that case, i.e., making x achievable under {a mathematical formula}hC only by action sequences of length &gt;n.</paragraph><paragraph>To find suitable facts p for extending x, we recursively consider the actions {a mathematical formula}a∈A[c], i.e., the actions that can achieve c (that add part of c and delete none of it). For each of these, we augment x so that there is a conjunction {a mathematical formula}x′⊆R(x,a) that cannot be achieved with action sequences of length at most {a mathematical formula}n−1. If a deletes part of G, we can tackle a simply by adding one such deleted fact p into x, effectively removing a from the set of achievers of x. For the remaining actions a, we recursively identify a suitable {a mathematical formula}x′⊆R(G,a). The latter is necessarily possible as we will always have {a mathematical formula}hC(s,G)&lt;h⁎(s,G) (in particular, {a mathematical formula}G⊈s at the recursion termination {a mathematical formula}n=0). We then extend x in a way ensuring that {a mathematical formula}x′ is contained in the regression {a mathematical formula}R(x,a), implying that x cannot be reached at time n.</paragraph><paragraph>Spelling out these arguments (see the proof in Appendix A), one obtains that PathCutRefine is correct:</paragraph><paragraph label="Theorem 2">Let C be any set of atomic conjunctions. Let s be a state with{a mathematical formula}hC(s)&lt;h⁎(s). Then:</paragraph><list><list-item label="(i)">The execution of{a mathematical formula}PathCutRefine(G,hC(s))is well defined, i.e., (a) in any call{a mathematical formula}PathCutRefine(G,n)there exists{a mathematical formula}c∈Cso that{a mathematical formula}c⊆Gand{a mathematical formula}hC(s,c)≥n; and (b) if{a mathematical formula}n=0, then{a mathematical formula}G⊈s.</list-item><list-item label="(ii)">If X is the set of conjunctions resulting from{a mathematical formula}PathCutRefine(G,hC(s)), then{a mathematical formula}hC∪X(s)&gt;hC(s).</list-item></list><paragraph>As a single call to PathCutRefine only guarantees to increase {a mathematical formula}hC(s) by at least 1, for dead-end refinement we need to iterate these calls, setting {a mathematical formula}C:=C∪X after each call, until {a mathematical formula}hC(s)=∞ holds. This is guaranteed to eventually happen, simply because every iteration adds at least one new conjunction to C (otherwise, the value of {a mathematical formula}hC could not have increased), and the number of conjunctions is finite. In the worst case, C eventually contains all conjunctions, and {a mathematical formula}hC(s)=∞ holds trivially.</paragraph><paragraph label="Example 3">Consider again the search space of our running example in Fig. 2. After expanding {a mathematical formula}s3, all of its children are either closed or recognized under {a mathematical formula}uC. Thus, {a mathematical formula}s3 becomes a known, though unrecognized dead-end. At this point in time C consists only of the unit conjunctions, {a mathematical formula}C={{p}|p∈F}, and hence {a mathematical formula}hC=h1. Say that we now conduct path-cut refinement on {a mathematical formula}s3 to suitably extend C.In the first call to PathCutRefine, we have {a mathematical formula}G4:=G={p1(l3),p2(l1)} and {a mathematical formula}n=hC(s)=h1(s)=4. There is only one option for the selection of c, because {a mathematical formula}h1(s,{p1(l3)})=3&lt;4=h1(s,{p2(l1)}). So we choose {a mathematical formula}c4={p2(l1)} and initialize the conflict to {a mathematical formula}x4:=c4. To see whether {a mathematical formula}p2(l1) can be reached with an action sequence of length no longer than 4, and thus to determine whether we have to augment {a mathematical formula}x4 by {a mathematical formula}p1(l3), we continue with the recursion on the only achiever of {a mathematical formula}c4, {a mathematical formula}unload(p2,l1). This yields a recursive call on {a mathematical formula}G3:=R(G4,unload(p2,l1))={p1(l3),t(l1),p2(T)}.In {a mathematical formula}PathCutRefine(G3,3), there are two options for choosing the conjunction {a mathematical formula}c3, namely {a mathematical formula}c3={p1(l3)} and {a mathematical formula}c3={p2(T)}. Say that we consider the critical path responsible for the top-level goal {a mathematical formula}p2(l1), i.e., we choose {a mathematical formula}c3={p2(T)}, and we set {a mathematical formula}x3:=c3. With the critical-path action {a mathematical formula}load(p2,l3), this yields a recursive call on {a mathematical formula}G2:=R(G3,load(p2,l3))={p1(l3),t(l1),t(l3),p2(l3)}. From the options in that recursive call, say we consider again the critical path responsible for {a mathematical formula}p2(l1), setting {a mathematical formula}c2={t(l3)} and {a mathematical formula}x2:=c2. With the critical-path action {a mathematical formula}drive(l2,l3,1), this yields a recursive call on {a mathematical formula}G1:=R(G2,drive(l2,l3,1))={p1(l3),t(l1),p2(l3),t(l2),f(1)}. Considering again the critical path responsible for {a mathematical formula}p2(l1), we select {a mathematical formula}c1={t(l2)} and set {a mathematical formula}x1:=c1. Now, the only supporting action to be considered is {a mathematical formula}drive(l1,l2,1) (higher fuel levels yield unreachable preconditions under the current {a mathematical formula}hC already). However, we do not require a recursive call over that action, as {a mathematical formula}drive(l1,l2,1) deletes {a mathematical formula}t(l1) and {a mathematical formula}f(1), which are both part of our subgoal {a mathematical formula}G1. Say we extend {a mathematical formula}x1 with the deleted fact {a mathematical formula}f(1).The recursion now goes back up the recursion path over the levels {a mathematical formula}i∈{2,3,4}, corresponding to the actions {a mathematical formula}drive(l2,l3,1), {a mathematical formula}load(p2,l3), {a mathematical formula}unload(p2,l1), with the current conjunctions {a mathematical formula}x2={t(l3)}, {a mathematical formula}x3={p2(T)}, {a mathematical formula}x4={p2(l1)}. At each step, we extend {a mathematical formula}xi with {a mathematical formula}xi−1 minus the respective action's precondition. At {a mathematical formula}i=2, {a mathematical formula}xi−1={t(l2),f(1)}; both are in the precondition of {a mathematical formula}drive(l2,l3,1), so {a mathematical formula}x2 remains the same. But then, {a mathematical formula}x3 and {a mathematical formula}x4 also remain the same in the remaining steps. Therefore, upon termination the only non-singleton conjunction in X is {a mathematical formula}{t(l2),f(1)}. Recall from our example walkthrough in Section 3 that this is exactly the single conjunction needed to render {a mathematical formula}uC∪X(s3)=uC∪X(s1)=∞.</paragraph></section><section label="5.2"><section-title>Neighbors refinement</section-title><paragraph>Neighbors refinement assumes as input a set {a mathematical formula}Sˆ of dead-end states that satisfies the {a mathematical formula}uC-recognized neighbors property. Namely, we denote by {a mathematical formula}Tˆ the neighbors of {a mathematical formula}Sˆ, i.e., the set of states {a mathematical formula}t∉Sˆ where there exists {a mathematical formula}s∈Sˆ s.t. {a mathematical formula}s→t. The {a mathematical formula}uC-recognized neighbors property requires that {a mathematical formula}uC(t)=∞ for all {a mathematical formula}t∈Tˆ.</paragraph><paragraph>In the context of Algorithm 1, Algorithm 2, we set {a mathematical formula}Sˆ:={s′|s′∈R[s],uC(s′)=0}. Provided that {a mathematical formula}uC is the only dead-end detector used, it is easy to see that the {a mathematical formula}uC-recognized neighbors property always holds at the refinement step on {a mathematical formula}R[s]: {a mathematical formula}R[s] contains only closed states, so it contains all states {a mathematical formula}s′ reachable from s, up to the neighbor states {a mathematical formula}t∈R[s]∖Sˆ where {a mathematical formula}uC(t)=∞.</paragraph><paragraph>For illustration, consider again our running example, specifically the search space in Fig. 2. At the refinement step on {a mathematical formula}s3, we have {a mathematical formula}Sˆ={s′|s′∈R[s3],uC(s′)=0}={s3,s1}. The neighbor states are {a mathematical formula}Tˆ={s4,s5}. The {a mathematical formula}uC-recognized neighbors property is satisfied: each of the neighbor states is already recognized by {a mathematical formula}uC, using the singleton conjunctions only, as there is no more fuel left.</paragraph><paragraph>We use {a mathematical formula}uC(s,G) to denote the {a mathematical formula}uC value of subgoal fact set G. Our refinement method is based on what we refer to as the {a mathematical formula}uCneighbors information: the values {a mathematical formula}uC(t,c) for all {a mathematical formula}t∈Tˆ and {a mathematical formula}c∈C. We compute this information once, at the start of the refinement procedure.{sup:4} Thanks to that information, in contrast to path-cut refinement as well as all previous conjunction learning methods, we do not require any intermediate recomputation of {a mathematical formula}uC during the refinement. Instead, neighbors refinement uses the {a mathematical formula}uC neighbors information to directly pick suitable conjunctions x for the desired set X. The method is inspired by the following simple characterizing condition for {a mathematical formula}uC dead-end recognition:</paragraph><paragraph label="Lemma 1">Let C be any set of atomic conjunctions, let s be a state, and let{a mathematical formula}G⊆F. Then{a mathematical formula}uC(s,G)=∞if and only if there exists{a mathematical formula}c∈Csuch that:</paragraph><list><list-item label="(i)">{a mathematical formula}c⊆Gand{a mathematical formula}c⊈s; and</list-item><list-item label="(ii)">for every{a mathematical formula}a∈A[c],{a mathematical formula}uC(s,R(c,a))=∞.</list-item></list><paragraph label="Proof">“⇒”: By definition of {a mathematical formula}uC, there must be a conjunction {a mathematical formula}c∈C so that {a mathematical formula}c⊆G and {a mathematical formula}uC(s,c)=∞. The latter directly implies that {a mathematical formula}c⊈s, and that {a mathematical formula}uC(s,R(c,a))=∞ for every {a mathematical formula}a∈A[c].“⇐”: As {a mathematical formula}c⊆G, we have {a mathematical formula}uC(s,G)≥uC(s,c). As {a mathematical formula}c⊈s, we have {a mathematical formula}uC(s,c)=mina∈A[G]⁡uC(s,R(G,a)). For every {a mathematical formula}a∈A[G], {a mathematical formula}uC(s,R(c,a))= ∞, so we have {a mathematical formula}uC(s,c)=∞ as desired.  □</paragraph><paragraph>Say now that s is any state in {a mathematical formula}Sˆ. We need to find X so that {a mathematical formula}uC∪X(s)=uC∪X(s,G)=∞. Given Lemma 1, we can do so by (i) picking some conjunction c with {a mathematical formula}c⊆G and {a mathematical formula}c⊈s, and then, recursively in the same manner, (ii) picking for every possible supporter {a mathematical formula}a∈A[c] an unreachable conjunction {a mathematical formula}c′ for {a mathematical formula}R(c,a). As s is a dead-end, and as {a mathematical formula}uC recognizes all dead-ends in the limit, Lemma 1 tells us that a suitable conjunction c exists at every recursion level. But the lemma does not tell us what that conjunction is. In particular, c must actually be unreachable, i.e., it must hold that {a mathematical formula}h⁎(s,c)=∞. But, given s and any one conjunction c, this is the same as asking whether a plan for c exists, which is PSPACE-complete to decide.</paragraph><paragraph>Now, we already know that the states {a mathematical formula}s∈Sˆ are dead-ends. Therefore, we can in principle use the full subgoals as our conjunctions, i.e., in (i) we can use {a mathematical formula}c:=G because we know that {a mathematical formula}h⁎(s,G)=∞, and in (ii) we can use {a mathematical formula}c′:=R(c,a) because we know that {a mathematical formula}h⁎(s,R(c,a))=∞. However, this naïve solution is pointless. It effectively constructs a full regression search tree from {a mathematical formula}G, selecting conjunctions corresponding to the regressed search states. For the method to be practically useful, what we need to find are small unreachable subgoals. It turns out that one can exploit the {a mathematical formula}uC-recognized neighbors property to that end.</paragraph><paragraph>Pseudo-code for our neighbors refinement procedure is shown in Algorithm 4. The purpose of a call to {a mathematical formula}NeighborsRefine(G), invoking the refinement on a target subgoal G, is to include conjunctions into X making G unreachable from{a mathematical formula}Sˆunder{a mathematical formula}uC∪X, i.e., so that {a mathematical formula}uC∪X(s,G)=∞ for all {a mathematical formula}s∈Sˆ. For this to be possible, of course G must be unreachable from{a mathematical formula}Sˆ, i.e., {a mathematical formula}h⁎(s,G)=∞ for every {a mathematical formula}s∈Sˆ. That prerequisite is obviously true at the top-level call, where {a mathematical formula}G=G, because the states {a mathematical formula}s∈Sˆ are dead-ends. As we shall see below, the prerequisite is invariant over calls to {a mathematical formula}Extract(G), i.e., the returned x also is unreachable from {a mathematical formula}Sˆ. As, for an unreachable subgoal, all regressed subgoals also are unreachable, the prerequisite thus holds at every invocation of {a mathematical formula}NeighborsRefine(G).</paragraph><paragraph>But how to identify the conjunctions X? To this end, the top-level procedure of {a mathematical formula}NeighborsRefine(G) mirrors the structure of Lemma 1. Following Lemma 1 (i), it starts by calling {a mathematical formula}Extract(G), which identifies a subgoal {a mathematical formula}x⊆G unreachable from {a mathematical formula}Sˆ. Following Lemma 1 (ii), the procedure then finds conjunctions making x, and thus the target subgoal G which contains x, unreachable from {a mathematical formula}Sˆ under {a mathematical formula}uC∪X. To this end, the refinement is called recursively on the regressed subgoals {a mathematical formula}R(x,a) for the actions a supporting x.</paragraph><paragraph>More precisely, a recursive call is needed only for those supporters a not dealt with by the previous conjunctions C, i.e., those where, on some state {a mathematical formula}s∈Sˆ, the regressed subgoal {a mathematical formula}R(x,a) is actually reachable under {a mathematical formula}uC. Furthermore, such a supporting action has already been dealt with by the new conjunctions X in case there is some {a mathematical formula}x′⊆R(x,a). That is so because the conjunctions X are constructed so that, upon termination, they are unreachable from {a mathematical formula}Sˆ under {a mathematical formula}uC∪X. (We get back to the latter below.)</paragraph><paragraph>Consider now the {a mathematical formula}Extract(G) sub-procedure, called on a target subgoal G. The procedure assumes that (a) G is unreachable from {a mathematical formula}Sˆ. It identifies a smaller subgoal {a mathematical formula}x⊆G, giving the guarantees that (b) x is still unreachable from {a mathematical formula}Sˆ, and (c) x is unreachable from the neighbor states {a mathematical formula}Tˆ under {a mathematical formula}uC. To be precise, (c) means that, for every {a mathematical formula}t∈Tˆ, there exists {a mathematical formula}c0∈C such that {a mathematical formula}c0⊆x and {a mathematical formula}uC(t,c0)=∞. To guarantee (c), the sub-procedure relies on the {a mathematical formula}uC neighbors information.</paragraph><paragraph>The first loop in the sub-procedure is over the neighbor states {a mathematical formula}t∈Tˆ. Drawing on the {a mathematical formula}uC neighbors information, it selects for each of these a prior atomic conjunction, {a mathematical formula}c0∈C, justifying that {a mathematical formula}uC(t,G)=∞. Such a {a mathematical formula}c0 must always exist: For the top-level goal {a mathematical formula}G=G, by construction we have that {a mathematical formula}uC(t,G)=∞, so by the definition of {a mathematical formula}uC there exists {a mathematical formula}c0⊆G with {a mathematical formula}uC(t,c0)=∞. For later invocations of {a mathematical formula}Extract(G), we have that {a mathematical formula}G=R(x,a), where x was constructed by a previous invocation of {a mathematical formula}Extract(G). By property (c) of that previous invocation, there exists {a mathematical formula}c0′∈C such that {a mathematical formula}x⊇c0′ and {a mathematical formula}uC(t,c0′)=∞. But then, in particular we have that {a mathematical formula}uC(t,x)=∞. Given this, we must also have that {a mathematical formula}uC(t,R(x,a))=∞, i.e., {a mathematical formula}uC(t,G)=∞. But then, as desired there exists a conjunction {a mathematical formula}c0⊆G with {a mathematical formula}uC(t,c0)=∞.</paragraph><paragraph>The loop over neighbor states accumulates all the {a mathematical formula}c0 into x. Subsequently, the sub-procedure loops over the dead-end states s. In case the current x is contained in s, it adds a fact {a mathematical formula}p∈G∖s into x. Such a p necessarily exists due to the assumed property (a), G is unreachable from {a mathematical formula}Sˆ. (In practice, to keep x small, we use simple greedy strategies in Extract, trying to select {a mathematical formula}c0 and p shared by many t and s.)</paragraph><paragraph>Observe that the returned x satisfies property (c) by construction. It remains to prove that x also satisfies property (b), i.e., that x is, again, unreachable from {a mathematical formula}Sˆ:</paragraph><paragraph label="Lemma 2">Let C be any set of atomic conjunctions. Let{a mathematical formula}Sˆbe a set of dead-end states, and let{a mathematical formula}Tˆbe its neighbors with the{a mathematical formula}uC-recognized neighbors property. Let{a mathematical formula}x⊆F. If</paragraph><list><list-item label="(i)">for every{a mathematical formula}t∈Tˆ, there exists{a mathematical formula}c0∈Csuch that{a mathematical formula}c0⊆xand{a mathematical formula}uC(t,c0)=∞; and</list-item><list-item label="(ii)">for every{a mathematical formula}s∈Sˆ,{a mathematical formula}x⊈s;</list-item></list><paragraph label="Proof">Assume for contradiction that there is a state {a mathematical formula}s∈Sˆ where {a mathematical formula}h⁎(s,x)&lt;∞. Then there exists a transition path {a mathematical formula}s=s0→s1…→sn from s to some state {a mathematical formula}sn with {a mathematical formula}x⊆sn. Let i be the largest index such that {a mathematical formula}si∈Sˆ. Such i exists because {a mathematical formula}s0=s∈Sˆ, and {a mathematical formula}i&lt;n because otherwise we get a contradiction to (ii). But then, {a mathematical formula}si+1∉Sˆ, and thus {a mathematical formula}si+1∈Tˆ by definition. By (i), there exists {a mathematical formula}c0⊆x such that {a mathematical formula}uC(si+1,c0)=∞. This implies that {a mathematical formula}h⁎(si+1,c0)=∞, which implies that {a mathematical formula}h⁎(si+1,x)=∞. The latter is in contradiction to the selection of the path. The claim follows.  □</paragraph><paragraph>Altogether, Algorithm 4 is correct in the following sense:</paragraph><paragraph label="Theorem 3">Let C be any set of atomic conjunctions. Let{a mathematical formula}Sˆbe a set of dead-end states, and let{a mathematical formula}Tˆbe its neighbors with the{a mathematical formula}uC-recognized neighbors property. Then:</paragraph><list><list-item label="(i)">The execution of{a mathematical formula}NeighborsRefine(G)is well defined, i.e., it is always possible to extract an x as specified.</list-item><list-item label="(ii)">The execution of{a mathematical formula}NeighborsRefine(G)terminates.</list-item><list-item label="(iii)">Upon termination of{a mathematical formula}NeighborsRefine(G),{a mathematical formula}uC∪X(s)=∞for every{a mathematical formula}s∈Sˆ.</list-item></list><paragraph>The arguments for (i) were outlined above. (ii) holds because every recursive call adds a new conjunction {a mathematical formula}x∉X: before the recursive call to {a mathematical formula}NeighborsRefine(R(x,a)) in the top-level procedure, there is no {a mathematical formula}x′∈X s.t. {a mathematical formula}x′⊆R(x,a); but that condition holds for the {a mathematical formula}x′ constructed in that recursive call. The number of possible conjunctions is finite, so the recursion must eventually terminate.</paragraph><paragraph>Finally, (iii) is a corollary of the aforementioned property that, upon termination, every {a mathematical formula}x∈X is unreachable from {a mathematical formula}Sˆ under {a mathematical formula}uC∪X. To see that the latter property holds, assume to the contrary that {a mathematical formula}uC∪X(s,x)=0. Then {a mathematical formula}hC∪X(s,x)=n for some finite number n. Let a be a best achiever of x under {a mathematical formula}hC∪X, i.e., {a mathematical formula}hC∪X(s,R(x,a))=n−1. By construction, in the recursive call that included x into X, either an {a mathematical formula}x′⊆R(x,a) was already present in X, or such an {a mathematical formula}x′ was included in the recursive call on {a mathematical formula}R(x,a). But then, {a mathematical formula}hC∪X(s,x′)≤n−1. Iterating this argument, we obtain a conjunction {a mathematical formula}x0∈X where {a mathematical formula}hC∪X(s,x0)=0, i.e., {a mathematical formula}x0⊆s. Such {a mathematical formula}x0 are never included into X by construction, in contradiction, concluding the argument.</paragraph><paragraph>In short, all input states {a mathematical formula}s∈Sˆ are refuted by {a mathematical formula}uC∪X upon termination, because {a mathematical formula}NeighborsRefine(G) finds a conjunction refuting G itself (Lemma 1 (i)), then makes sure to find conjunctions refuting the regressed subgoal for every supporting action (Lemma 1 (ii)). A detailed proof of Theorem 3 is available in Appendix A.</paragraph><paragraph label="Example 4">Consider again the search space of our running example in Fig. 2, and the refinement process on the states {a mathematical formula}Sˆ=R[s3]={s3,s1}, where the truck has driven to {a mathematical formula}l1 and has a single fuel unit left. The recognized neighbor states are {a mathematical formula}Tˆ={s4,s5} where the truck has driven back to {a mathematical formula}l2 and has no fuel left.We initialize {a mathematical formula}X=∅ and call {a mathematical formula}NeighborsRefine({p1(l3),p2(l2)}). Consider the call to the sub-procedure, {a mathematical formula}Extract({p1(l3),p2(l2)}). Here, we find that {a mathematical formula}c0={p1(l3)} is suitable for each of {a mathematical formula}s4 and {a mathematical formula}s5: it is unreachable under {a mathematical formula}uC because there is no fuel left. Furthermore, {a mathematical formula}c0={p1(l3)} is neither contained in {a mathematical formula}s3 nor in {a mathematical formula}s1. So we return {a mathematical formula}x={p1(l3)}. Note that this is not a new conjunction; it is already contained in C. The x extracted is guaranteed to not already be in X, but it may be an element of C. In other words, as the x in each recursive call, we may use a conjunction that was already atomic beforehand.Back in {a mathematical formula}NeighborsRefine({p1(l3),p2(l2)}), we see that x can be achieved by unloading at {a mathematical formula}l3. We need to tackle the regressed subgoal, through the recursive call {a mathematical formula}NeighborsRefine({t(l3),p1(T)}). In that call, the extraction sub-procedure returns {a mathematical formula}x={t(l3)}, which is suitable for the same reasons as above. Observe that, again, this conjunction was already atomic beforehand. We next need to exclude the drive action from {a mathematical formula}l2 to {a mathematical formula}l3, via the recursive call {a mathematical formula}NeighborsRefine({t(l2),f(1)}).Consider finally the extraction sub-procedure in that recursive call, i.e., the call to {a mathematical formula}Extract({t(l2),f(1)}). For the {a mathematical formula}Tˆ neighbor states, {a mathematical formula}s4 and {a mathematical formula}s5, where the truck is at {a mathematical formula}l2 but there is no more fuel, only the subgoal fact {a mathematical formula}f(1) is eligible. That is, we get {a mathematical formula}c0={f(1)} for each of them. However, {a mathematical formula}f(1) is contained in the {a mathematical formula}Sˆ states {a mathematical formula}s3 and {a mathematical formula}s4. So we need to add also the other subgoal fact into x, ending up with {a mathematical formula}x=G={t(l2),f(1)}. Observe that this last x is the first non-atomic x extracted.The refinement process stops here, because the actions achieving x, driving to {a mathematical formula}l2 from {a mathematical formula}l1, and driving to {a mathematical formula}l2 from {a mathematical formula}l3, both incur the regressed subgoal {a mathematical formula}f(2), for which we have {a mathematical formula}uC(s3,{f(2)})=uC(s1,{f(2)})=∞. Hence the set X returned contains, like for path-cut refinement above, just the one new conjunction {a mathematical formula}{t(l2),f(1)}, which is exactly what is needed to render {a mathematical formula}uC∪X(s3)=uC∪X(s1)=∞.</paragraph></section></section><section label="6"><section-title>Clause learning</section-title><paragraph>As previously discussed, the computation of {a mathematical formula}uC is low-order polynomial time in the number {a mathematical formula}|C| of atomic conjunctions, through simple dynamic programming techniques. Yet in practice it may incur a substantial runtime overhead for large C. We now introduce a clause learning technique to alleviate this. Essentially, we learn weaker nogoods in addition to {a mathematical formula}uC, that are easier to evaluate than {a mathematical formula}uC and serve as a filter in front of {a mathematical formula}uC.</paragraph><paragraph>The clause learning method is technically quite simple. It follows an earlier proposal by Kolobov et al. [19], in their SixthSense dead-end detection method for forward search probabilistic planning, where essentially the same technique was used for verifying and minimizing nogood candidates (the candidates having been previously derived from information obtained in a determinization, i.e., via classical planning).</paragraph><paragraph>Consider any state s where {a mathematical formula}uC(s)=∞. Denote by {a mathematical formula}ϕ:=⋁p∈F∖sp the disjunction of facts false in s. Then ϕ is a valid clause: for any state {a mathematical formula}s′, if {a mathematical formula}s′ does not satisfy ϕ, written {a mathematical formula}s′⊭ϕ as usual, then {a mathematical formula}uC(s′)=∞; in particular, if {a mathematical formula}s′ does not satisfy ϕ then it is a dead-end. To see this, just note that, as {a mathematical formula}uC(s)=∞, the goal is unreachable from s under {a mathematical formula}uC. To make the goal reachable under {a mathematical formula}uC, we need to make true at least one of the facts that are false in s.</paragraph><paragraph>The valid clause ϕ just defined is, per se, not useful as it generalizes to only those states subsumed by s, i.e., whose true facts are contained in those of s. This changes when minimizing ϕ, testing whether individual facts p can be removed while preserving validity. In other words, we aim at obtaining a minimal reason for {a mathematical formula}uC(s)=∞ (similarly as done for failed “obligations” in property-directed reachability [14]). Our minimization method is straightforward, testing the facts {a mathematical formula}p∈F∖s one by one.</paragraph><paragraph>We start with {a mathematical formula}s′:=s. We then loop over all {a mathematical formula}p∈F∖s. In each loop iteration, we test whether {a mathematical formula}uC(s′∪{p})=∞; if so, we set {a mathematical formula}s′:=s′∪{p}. Upon termination of the loop, {a mathematical formula}s′ is a set-inclusion maximal superset of s that preserves goal unreachability under {a mathematical formula}uC, i.e., where {a mathematical formula}uC(s′)=∞. We then obtain our clause as the disjunction of the remaining facts, {a mathematical formula}ϕ:=⋁p∈F∖s′p. SixthSense does the same except for using {a mathematical formula}h2, rather than {a mathematical formula}uC.</paragraph><paragraph label="Example 5">Consider again our running example, and the clause learning on state {a mathematical formula}s4 from Fig. 2, where the truck has moved from {a mathematical formula}l2 to {a mathematical formula}l1 and back to {a mathematical formula}l2, so that the packages are still in their initial locations but there is no more fuel left.The minimization loop starts with {a mathematical formula}s′=s={t(l2),f(0),p1(l1),p2(l3)}. Adding the other possible locations of {a mathematical formula}p2, i.e. the facts {a mathematical formula}p2(l1) and {a mathematical formula}p2(l2), the goal is still unreachable under {a mathematical formula}uC because we cannot achieve the goal {a mathematical formula}p1(l3). So we set {a mathematical formula}s′:=s′∪{p2(l1),p2(l2)}. Considering now the other possible locations of {a mathematical formula}p1, i.e. the facts {a mathematical formula}p1(l2) and {a mathematical formula}p1(l3), {a mathematical formula}p1(l2) can be added as we still cannot reach {a mathematical formula}p1(l3). But {a mathematical formula}p1(l3) cannot be added as we would then have {a mathematical formula}G⊆s′. So we set {a mathematical formula}s′:=s′∪{p2(l2)}. If we add any amount of fuel, the goal becomes reachable under {a mathematical formula}uC again (with singleton conjunctions only, fuel consumption is ignored), so neither {a mathematical formula}f(1) nor {a mathematical formula}f(2) can be added. Finally, considering the other possible locations of the truck, {a mathematical formula}t(l1) and {a mathematical formula}t(l3), we can add {a mathematical formula}t(l1) as the truck still cannot reach {a mathematical formula}l3. But we cannot add {a mathematical formula}t(l3) as, then, in the extended {a mathematical formula}s′ the truck would be at {a mathematical formula}l2 and {a mathematical formula}l3 simultaneously, and able to transport {a mathematical formula}p1 from {a mathematical formula}l2 to {a mathematical formula}l3 without fuel consumption.We end up with {a mathematical formula}s′={t(l1),t(l2),f(0),p1(l1),p1(l2),p2(l1),p2(l2),p2(l3)}. This yields the clause {a mathematical formula}ϕ=t(l3)∨f(1)∨f(2)∨p1(l3) as previously advertised in our example walkthrough (Section 3).Note that the clause we end up with depends on the ordering of facts in the minimization loop. If, for example, we test {a mathematical formula}t(l3) at the very beginning of the loop, then it can be added: with {a mathematical formula}p1 being only at {a mathematical formula}l1, having t at both {a mathematical formula}l2 and {a mathematical formula}l3 still requires fuel to achieve {a mathematical formula}p1(l3). On the other hand, if we do add {a mathematical formula}t(l3) into {a mathematical formula}s′, then later on we cannot add {a mathematical formula}p1(l2), nor {a mathematical formula}t(l1). We use an arbitrary fact ordering in our implementation, i.e., we do not attempt to find clever orderings.</paragraph><paragraph>The recomputation of {a mathematical formula}uC(s′∪{p}) for each fact p in the minimization loop can be optimized by doing it in an incremental manner. Omitting implementation details, we essentially store the dynamic programming table (an index from atomic conjunctions into {a mathematical formula}{0,∞}) of the previous iteration, identify the table cells changing from ∞ to 0 due to the inclusion of p, and propagate these changes. Nevertheless, the minimization sometimes incurs a significant computational overhead. To reduce that overhead, in our implementation we slightly diverge from the above. We test, in each iteration of the minimization loop, not individual facts p, but entire “state variables” v in the internal representation of the Fast Downward planning system [79], [80], which our implementation is based on. A state variable v here corresponds to a set {a mathematical formula}F(v) of facts exactly one of which is true in every state. In each loop iteration, we test whether the entire {a mathematical formula}F(v) for some v can be included into {a mathematical formula}s′. This yields weaker clauses, but takes less runtime.</paragraph><paragraph>During search, we maintain a conjunction Φ of clauses, starting with empty Φ and adding ϕ to Φ whenever a new clause ϕ is learned. Clearly, whenever {a mathematical formula}s′⊭Φ, then {a mathematical formula}uC(s′)=∞. So Φ is a weaker dead-end detector than {a mathematical formula}uC; yet it can be evaluated more effectively. Our implementation does so through a counter-based scheme, where we loop over Fast Downward's state variables v just once, incrementing a counter for every clause ϕ that v's value in {a mathematical formula}s′ does not comply with. Then {a mathematical formula}s′⊭Φ iff one of these counters reaches the total number of state variables.</paragraph><paragraph>The question remains how to arrange the search: (1) When exactly to learn clauses, (2) when exactly to test them, and (3) when to make a full call to the unsolvability heuristic {a mathematical formula}uC? Regarding (1), a clear outcome from our preliminary experiments is that a clause should be learned every time we evaluate {a mathematical formula}uC on a state s and find that {a mathematical formula}uC(s)=∞. Regarding (2) and (3), the obvious arrangement is to use the clauses as a filter, i.e., whenever a state s is scheduled for evaluation by {a mathematical formula}uC, first test whether {a mathematical formula}s⊭Φ, and if so, return ∞ without invoking {a mathematical formula}uC.</paragraph><paragraph>Observe that, in particular, while we get duplicate pruning “for free” from the refutation knowledge accumulated in {a mathematical formula}uC (cf. property (6) in Section 4.2), in practice, computing {a mathematical formula}uC may be a lot more time-consuming than duplicate checking. That is dealt with by the clauses, which also subsume duplicate pruning and are similarly cheap to evaluate.</paragraph></section><section label="7"><section-title>Experiments</section-title><paragraph>We evaluate our techniques for three different purposes: finding plans in solvable planning tasks with dead-ends, proving unsolvability, generating unsolvability certificates. We next detail our experiment setup, then cover these different purposes in this order.</paragraph><section label="7.1"><section-title>Experiment setup: algorithms and benchmarks</section-title><paragraph>Variants of our technique  We implemented all the described techniques in Fast Downward [79], short FD. In our preliminary experiments, we found that search algorithms other than depth-first search hardly ever benefited from conflict-driven learning, as they did not identify enough conflicts. This pertains especially to {a mathematical formula}A⁎, which explores the search space in a breadth-oriented fashion, considering many options. In contrast, for the identification of conflicts, it is beneficial to search deeply not broadly, pushing the state at hand to either the goal or a dead-end situation.</paragraph><paragraph>Given this, we consider depth-first searches only. In particular, for solvable planning tasks, we consider satisficing planning, not giving a plan quality guarantee. We focus on DFS, i.e., our extension of Tarjan's algorithm (cf. Section 4.2), our most elegant and effective search algorithm. For ordering children nodes in DFS, we focus on an ordering by smaller value of the delete relaxation heuristic {a mathematical formula}hFF[41], which turns out to be beneficial for both, finding plans and (to a lesser degree) proving unsolvability.</paragraph><paragraph>We experiment with three different conjunction learning methods, namely path-cut refinement and neighbors refinement as introduced here, plus Keyder et al.'s [69], [67] as a representative of prior conjunction-learning methods. Keyder et al.'s method, like Haslum's preceding one [66], is based on iterative removal of conflicts in a delete-relaxed plan, and we will refer to it as conflict refinement. We also run DFS without any refinement, as a direct comparison pointing out the impact of learning vs. an identical search without learning.</paragraph><paragraph>For finding plans and for proving unsolvability, we run DFS with early termination, stopping search as soon as there are no more open nodes. For generating unsolvability certificates, we run DFS without early termination, refining {a mathematical formula}uC until it refutes the initial state.</paragraph><paragraph>We also run offline learning variants, for proving unsolvability and for dead-end detection. In either case, we use path-cut refinement and conflict refinement, as neighbors refinement is not applicable in the offline context. For proving unsolvability, we simply refine {a mathematical formula}uC on the initial state until {a mathematical formula}uC(I)=∞. For dead-end detection, we refine {a mathematical formula}uC on the initial state until a size bound N is reached. Then we use the same set C for {a mathematical formula}uC dead-end detection throughout the search. This is inspired by previous work on partial delete relaxation heuristics [69], [67], [68], and like that work the size bound N is multiplicative: we stop when the number of “counters” – atomic entities in the implementation of {a mathematical formula}uC – reaches N times that when C contains the singleton conjunctions only.</paragraph><paragraph>To evaluate the complementarity of our method vs. strong competing methods, we design simple combinations with the two strongest alternate dead-end detection techniques, namely unsolvability heuristics u obtained from merge-and-shrink abstractions respectively potential heuristics (more on these below). The combination uses both dead-end detectors every time we test whether a state s is a dead-end. The only subtlety here is during refinement at s, where, because of the additional dead-end detector u, as already mentioned the {a mathematical formula}uC-recognized neighbors property may not hold. Distinguishing the two possible cases, (a) if s has {a mathematical formula}uC-recognized neighbors, then we use neighbors refinement which generally works best if applicable. Case (b), if s does not have {a mathematical formula}uC-recognized neighbors, then we can use path-cut refinement. Observe though that the latter will force {a mathematical formula}uC to recognize all the dead-ends below s already recognized by u. Therefore, we experiment with two combination methods, one using both (a) and (b), and one using only (b).</paragraph><paragraph>For ablation study purposes, we finally run variants of our strongest configuration (DFS with neighbors refinement), with vs. without the clause learning mechanism, and using depth-oriented search (open list preferring deepest states), short DOS, instead of Tarjan's algorithm.</paragraph><paragraph>State-of-the-art competing algorithms  Apart from standard search algorithms and heuristic functions, the relevant techniques in our context are (a) dead-end detection, (b) admissible pruning techniques, and (c) other methods for proving unsolvability. For all of these, the state of the art at the time of writing is represented by the participants of the 2016 inaugural Unsolvability International Planning Competition (UIPC'16). To provide a comprehensive picture, we include all UIPC'16 participants, save those vastly dominated in that competition. However, our interest is in understanding algorithm behavior, as opposed to running a systems competition. For systems composed of several distinct algorithm components, we therefore consider, not the systems, but their components. This pertains primarily to the winning system Aidos [53], an algorithm portfolio; for the other UIPC'16 participants, our modifications are minor.</paragraph><paragraph>In detail, we run the following algorithms. Throughout, we use the original planning task representation produced by the FD translator [80], without additional preprocessors. From category (a), we run the following algorithms. We run merge-and-shrink (M&amp;S) abstractions, in the two most competitive unsolvability-heuristic variants of Hoffmann et al. [36]. One of the two M&amp;S variants computes the perfect unsolvability heuristic {a mathematical formula}u⁎, the other imposes an abstraction size limit and yields an approximate dead-end detector. Very similar M&amp;S heuristics were used in UIPC'16 [49], in combination with additional irrelevance pruning [63] and dominance pruning [62]; we separate out the latter components to keep things clean. We furthermore run the new pattern database unsolvability heuristic [50], using a more restricted class of abstractions based on projection (e.g. [42]), which participated in UIPC'16 on its own and as one component of Aidos. We run potential heuristics[51], [52] for dead-end detection, another component of Aidos, and the one that contributed most to its overall performance. These potential heuristics use an LP solver to find an invariant proving that the goal is unreachable (because the current state's potential cannot be reduced to that of the goal states).</paragraph><paragraph>From category (b), we run simulation-based dominance pruning [62], used in the UIPC'16 M&amp;S system [49], which finds a simulation relation over states and prunes dominated states during search. We run strong stubborn sets (SSS) pruning, used in two UIPC'16 entries [81], [53], a long-standing partial-order reduction method (e.g. [82], [61]) exploiting permutability. We run irrelevance pruning[63], also used in two UIPC'16 entries [57], [49], which detects irrelevant operators (that cannot be part of an optimal solution) based on dominance analysis in a merge-and-shrink abstraction.</paragraph><paragraph>From category (c), we run BDD-based symbolic search (e.g. [83], [84], [63]), specifically the UIPC'16 Sympa system [57], separating out the irrelevance pruning (same as for M&amp;S heuristics above). We run property-directed reachability (PDR)[13], [14] as in UIPC'16 [85]. We run resource-variable detection, another component of Aidos, which performs domain analysis to identify a state variable encoding a consumed resource-budget, and which during search uses a cartesian abstraction[86], [87] lower bound to prune against the remaining budget. We run star-topology decoupled state space search[88], [89], a decomposition technique exploiting possible factorizations into star topologies. We separate the standard state space search and strong stubborn sets pruning components used as alternatives to star-topology decoupled state space search in the UIPC'16 system. We run partial delete-relaxation via red–black planning[58], [60], in its most competitive configuration established after UIPC'16 [59]. This approach searches in a relaxation where “red” state variables (but not “black” ones) are delete-relaxed. If there is no relaxed plan, there cannot be a real plan either, so unsolvability of the input task can be proved this way. The relaxation is iteratively strengthened by painting one more variable black. The only difference between our version and the UIPC'16 one is the variable order in which that is done.</paragraph><paragraph>We do not run the UIPC'16 theorem proving approach [90], as this was vastly outperformed by the other competition entries. We do not run Haslum's UIPC'16 entry [91], which also performed badly, and is very similar to our configuration doing offline learning with conflict refinement. The main difference is that it uses a less effective representation, where {a mathematical formula}hC is computed in a compiled planning task {a mathematical formula}ΠC whose size is worst-case exponential in {a mathematical formula}|C|.</paragraph><paragraph>Benchmarks  For unsolvable benchmarks, there is exactly one standard benchmark set at the time of writing, namely that of UIPC'16, which we use.</paragraph><paragraph>As solvable benchmarks, naturally we use the benchmark suites of the International Planning Competition (IPC). We consider all IPC editions, 1998–2014, specifically the STRIPS benchmarks for satisficing planning (where these distinctions are made). Yet more specifically, our learning techniques are interesting only in domains that actually contain conflicts, i.e., dead-ends unrecognized under {a mathematical formula}h1. Therefore, from IPC'98–IPC'08 we use the subset of domains, as determined in Hoffmann's [72], [92] analyses, where that is the case. From IPC'11 and IPC'14, where a formal analysis has not yet been carried out, we use those domains where DFS with {a mathematical formula}h1 dead-end detection (with either of the two children ordering methods) identifies at least one conflict, i.e., where DFS backtracks out of a strongly connected component at least once on at least one instance.</paragraph><paragraph>In addition to the competition benchmarks just described, we also consider planning with resources (e.g. [93], [94], [27], [28], [29]), specifically so-called resource-constrained planning[28] where the goal must be achieved subject to a limited resource budget. We use the benchmarks by Nakhost et al. [28], which are controlled in that the minimum required budget {a mathematical formula}bmin is known, and the actual budget is set to {a mathematical formula}W⁎bmin, where the constrainedness level W is a benchmark instance parameter. For constrainedness levels W much larger than 1, resources are plenty and a plan is typically easy to find; for constrainedness levels W much smaller than 1, resources are extremely scarce and unsolvability is typically easy to prove; for constrainedness levels W close to 1, resource-constrained planning is notoriously difficult. Intuitively, the constrainedness level controls the frequency and detection difficulty of dead-ends, which is of obvious interest to our purposes here. For the solvable case, we use the exact benchmark suites provided by Nakhost et al. For the unsolvable case, we use those same benchmarks but with {a mathematical formula}W∈{0.5,0.6,0.7,0.8,0.9}, not considered by Nakhost et al. (but previously considered by Hoffmann et al. [36]).</paragraph><paragraph>All experiments were run on a cluster of Intel E5-2660 machines running at 2.20 GHz, with runtime (memory) limits of 30 minutes (4 GB).</paragraph></section><section label="7.2"><section-title>Dead-end detection in solvable planning tasks</section-title><paragraph>We consider first the solvable case. We run the six variants of our technique described above: DFS without learning; DFS with learning using one of the three refinement methods; and the DFS combinations with dead-end detectors from UIPC'16, M&amp;S respectively potential heuristics. We run these algorithms with {a mathematical formula}hFF children ordering, and compare them to DFS with arbitrary children ordering. We furthermore compare them with search algorithms using heuristic search as the baseline, specifically FD's lazy-greedy best-first search (GBFS) using {a mathematical formula}hFF with a dual open queue for preferred operators [79]. This is the canonical baseline algorithm for satisficing heuristic search planning, and yields competitive performance while being reasonably simple. We use M&amp;S respectively potential heuristics for dead-end detection in that baseline search. We use simulation-based dominance, strong stubborn sets (SSS), respectively irrelevance pruning to prune the state space in that baseline search.</paragraph><paragraph>We finally run offline learning with a size bound x to generate static {a mathematical formula}uC dead-end detectors. We use these in both, DFS without learning for direct comparison to our techniques, and in the GBFS baseline search for direct comparison to the other static dead-end detectors. We experiment with {a mathematical formula}N=2 as that was the best size bound in prior work on partial delete relaxation heuristics, and we experiment with {a mathematical formula}N=32 as a larger yet still reasonable setting.</paragraph><paragraph>The main base algorithm our learning methods start from, DFS with {a mathematical formula}hFF children ordering, is quite different from the GBFS {a mathematical formula}hFF baseline. So we start with a brief experiment comparing these two, without learning. Consider Table 1. We see that the two base algorithms have complementary strengths in different domains. Sometimes the differences are drastic, most notably in Sokoban, Woodworking, as well as resource-constrained Rovers and TPP where DFS is much stronger; and Mprime, Mystery, NoMystery, and Thoughtful where GBFS is much stronger. In total, these differences cancel each other out though, and the two algorithms are on a similar level. This is remarkable in itself, seeing as GBFS is widely used in heuristic search planning while, to the authors' knowledge, DFS has not been used at all in this context yet. For our purposes here though, the main conclusion is that, overall, performance is comparable so we are not a priori much disadvantaging either side.</paragraph><paragraph>Table 1 also includes a variant of DFS disallowing backtracking. This serves to point out the cases where, with {a mathematical formula}hFF tie breaking, despite the presence of conflicts in principle, no learning will happen simply because a goal state is found without ever encountering a conflict. As the table shows, this happens to a surprising extent. In particular, this simplistic search procedure is an extremely effective solver for ParcPrinter, Pathways, and TPP, where it solves all instances solved by the common baseline (almost all, in case of Pathways), but within split seconds or a few seconds at most. We conclude from this that someone should put this algorithm into their portfolio at the next planning competition. That aside, we continue with our own research focus here, which is on those benchmarks where learning does indeed happen.</paragraph><paragraph>Table 2 shows coverage data. We will consider offline learning separately below. For the resource-constrained domains, as the constrainedness level has a large impact on performance for most algorithms, we show data for each level separately. Note here that IPC Mprime, Mystery, and NoMystery also are resource-constrained domains. However, their constrainedness levels are not known or only partially known, so we do not separate these. (We will however do so for the unsolvable resource-constrained benchmarks from UIPC'16 below.)</paragraph><paragraph>Consider first the different refinement variants within DFS {a mathematical formula}hFF, i.e., our neighbors refinement (“Nei”) and path-cut refinement (“Pat”) methods vs. the conflict refinement (“Con”) from prior work. There are large performance differences in many domains, showing that the way we learn conjunctions is important. In particular, conflict refinement has the lead, and a marginal one at that, in only one case (IPC Trucks), showing that it is important to target the conjunction learning to dead-end detection. Path-cut refinement is best overall on the IPC, but neighbors refinement has a huge advantage in the resource-constrained benchmarks so has best overall coverage.</paragraph><paragraph>Consider now the effect of learning vs. no learning (“No”). On the resource-constrained domains, the improvement is consistently dramatic, with some minor exceptions in TPP. On the IPC benchmarks, the picture is much more mixed. On Airport, Freecell, PegSol, Pipesworld-Tankage, Sokoban, and Trucks, the learning has a detrimental effect. We will analyze in some detail below why that is so. On Tidybot and Woodworking, as well as ParcPrinter, Pathways, and TPP for the DFS {a mathematical formula}hFF variants, the learning has no impact at all. That is mostly because DFS does not identify many conflicts here, so the learning is seldom, or never, invoked (we also get back to this in more detail below). On the other domains, improvements are possible. These are most pronounced in Floortile for path-cut and conflict refinement; in Mystery, NoMprime, and NoMystery for neighbors refinement; and in ParcPrinter and Pathways for neighbors refinement in DFS without {a mathematical formula}hFF children ordering.</paragraph><paragraph>Overall, compared to the baselines without learning (including also GBFS), on the IPC the learning is detrimental, as the size of its losses outweighs that of its gains. On the resource-constrained domains, the picture is very different, with a substantial and consistent win over the baselines. The only exception is NoMystery with {a mathematical formula}W=2.0, where fuel is relatively plentiful and the baseline does not struggle with unrecognized dead-ends as much as it does with constrainedness levels closer to 1.0.</paragraph><paragraph>Ordering children nodes in DFS by {a mathematical formula}hFF (“DFS {a mathematical formula}hFF”) outperforms arbitrary children ordering (“DFS”) dramatically overall, and almost consistently across domains. Therefore, from now on, we consider {a mathematical formula}hFF children ordering exclusively. We will do so not only for the solvable case, but also for unsolvable benchmarks, where {a mathematical formula}hFF children ordering does not have as large an impact, but is never worse and sometimes helps (intuitively because DFS is drawn towards more relevant dead-end situations, learning more relevant knowledge). Henceforth, whenever we say “DFS” we mean “DFS with {a mathematical formula}hFF children ordering”.</paragraph><paragraph>Consider next the UIPC'16 algorithms. On the IPC benchmarks, compared to their GBFS baseline, only strong stubborn sets pruning (“SSS”) improves overall coverage, and only SSS and irrelevance pruning (“Irr”) have higher overall coverage than our methods (DFS {a mathematical formula}hFF with neighbors refinement respectively path-cut refinement). The strengths of these two methods lie in being less risky, not deteriorating the baseline as much, especially in Openstacks and Pipesworld-Tankage; and in the improvements they yield in Airport, Sokoban, and (for irrelevance pruning) Woodworking. The strong cases for our methods are Floortile where DFS with path-cut refinement, and also with conflict refinement, vastly outperforms all competitors; NoMystery where DFS with neighbors refinement performs best; Openstacks, Tidybot, and TPP, where the learning methods incur less overhead than many of the UIPC'16 methods; as well as Woodworking where the DFS baseline is much better than the GBFS baseline, and there is no learning overhead.</paragraph><paragraph>On the resource-constrained benchmarks, the picture is again much clearer. Potential heuristics (“Pot”) and SSS are basically useless. Irrelevance pruning, merge-and-shrink (“MSa”), and especially simulation-based dominance pruning (“Sim”) excel in NoMystery. In the other two domains, our learning methods tend to be superior. In Rovers, DFS with neighbors refinement (“Nei”) vastly outperforms all UIPC'16 competitors. Similarly in TPP, except for simulation-based dominance which does equally well in coverage (though typically a lot worse in runtime, on commonly solved instances). In terms of overall coverage, DFS with neighbors refinement is the clear winner. Other algorithms perform better in one part (IPC vs. Nakhost et al.) of the benchmark set, but DFS with neighbors refinement is most consistently good overall.</paragraph><paragraph>Consider finally the combinations of neighbors-refinement DFS with merge-and-shrink respectively potential heuristics. For merge-and-shrink, this basically does not work well here. The DFS {a mathematical formula}hFF component dominates the combined methods consistently, the only notable exception being NoMystery with small values of W where the “N” combination does better. Yet merge-and-shrink alone does still a lot better there so this is not a valuable result. There is no case where a combination outperforms both its components. (As we shall see below, matters are quite different on the unsolvable benchmarks.)</paragraph><paragraph>For the combination with potential heuristics, the picture is similar, though not quite as bleak. The “N” combination outperforms the DFS {a mathematical formula}hFF component on PegSol, where the potential heuristic prevents most (but not all) of the loss compared to the DFS baseline. The “N” combination does better than both of its components on Thoughtful (though by only 1 instance relative to the DFS component).</paragraph><paragraph>Fig. 3 provides a view on the search space sizes under our different DFS learning techniques, i.e., no learning vs. the three different refinement methods. We use neighbors refinement, the strongest method, as the comparison baseline.</paragraph><paragraph>Consider first the comparison to no learning, Fig. 3(a). On the IPC benchmarks, in line with the above, we see that the learning is risky, improving performance in some cases but deteriorating it in many others. On the resource-constrained benchmarks, on the other hand, the benefits of learning are dramatic. Most benchmark instances are not solved at all without learning. On those that are solved, we get search space reductions of several orders of magnitude. Observe that the only reason for this is generalization, i.e., refinements of {a mathematical formula}uC on states s leading to pruning on states other than s. Without generalization, the search spaces would be identical, including tie-breaking. Generalization is what lifts a hopeless planner (DFS with {a mathematical formula}h1 dead-end detection) to a planner competitive with the state of the art in resource-constrained planning.</paragraph><paragraph>In the comparison Fig. 3(b) between neighbors refinement and path-cut refinement, we see that the methods either perform very similarly, or are highly complementary (with one of the two methods failing to solve the task).</paragraph><paragraph>The comparison Fig. 3(c) between neighbors refinement and conflict refinement provides further evidence that tailoring the refinement method to dead-end detection is typically beneficial. In the vast majority of cases where learning takes place (where conflicts are identified by search), neighbors refinement leads to better generalization, and thus to smaller search spaces, than conflict refinement.</paragraph><paragraph>We close this subsection by a consideration of offline learning, a performance analysis, and ablation studies. Table 3 shows the data, covering these topics in parts (A), (B), and (C) respectively.</paragraph><paragraph>Consider first part (A) of the table, and within that part consider first the IPC benchmarks. With {a mathematical formula}N=2, though not with {a mathematical formula}N=32, the offline methods are, generally, superior on these benchmarks compared to the same refinement methods when used online. This is, however, simply because having a small size bound means to be less risky. The offline methods, especially {a mathematical formula}N=2, avoid the dramatic performance losses in Airport, PegSol, and Sokoban ({a mathematical formula}N=32 is more risky, e.g. in Openstacks). However, the risk reduction also comes with a benefits reduction (“no risk no fun”) on those domains where the online variants excel, most notably Floortile and NoMystery. Indeed, the offline-learning DFS variants never beat the coverage of the DFS no-learning baseline (“DFS No”), whereas the online learning variants beat it in 6 domains.</paragraph><paragraph>On the resource-constrained benchmarks, matters are very clear-cut. The baselines are weak, and online learning improves them vastly. The same is not true for offline-learning DFS, which never improves over the baseline at all. Offline-learning GBFS is more successful, improving over the GBFS baseline in almost all cases with path-cut refinement. But it becomes competitive with online-learning DFS only in NoMystery for large values of W.</paragraph><paragraph>Consider now part (B) of Table 3, which shows data supporting a performance analysis with respect to the three prerequisites for online learning to work well: (1) conflict identification, i.e., the ability of forward search to find conflicts and thus enable the learning in the first place; (2) effective learning, i.e., the ability of recognizing dead-ends with small conjunction sets C; (3) strong generalization, i.e., the ability of {a mathematical formula}uC to detect states {a mathematical formula}s′ it was not trained on. Part (B) of Table 3 captures (1) in terms of the “CI #” data, the number of instances on which at least one conflict was identified, and hence some learning was done. It captures (2) in terms of the “Slo N” data, the size of the representation underlying the {a mathematical formula}uC computation, as a multiple of that for singleton conjunctions; i.e., the slowdown relative to {a mathematical formula}h1. It captures (3) in terms of the “Red {a mathematical formula}S” data, the search space size reduction factor relative to using only {a mathematical formula}h1 for dead-end detection.</paragraph><paragraph>On commonly solved instances, the slowdown and search reduction factors are directly comparable, and a performance advantage should be expected, roughly, when the former exceeds the latter. Indeed, this is a good indicator in our data here. The domains with large reduction yet small slowdown are IPC Mystery and NoMystery, as well as all resource-constrained domains, where indeed neighbors refinement vastly improves the coverage of DFS. Conversely, small reductions yet large slowdowns occur in Airport, Floortile, Freecell, Mprime, PegSol, Pipesworld-Tankage, Sokoban, and Trucks. Except for Mprime, these are precisely the cases where neighbors refinement is detrimental. In Mprime, neighbors refinement actually (slightly) improves coverage. Another unclear case is NoMprime, where neighbors refinement significantly improves coverage, yet the slowdown is larger than the reduction. In both these cases, there are only few commonly solved instances where at least one conflict is identified, which may contribute to the unclear picture. Similarly in Childsnack, where coverage is improved from 0 to 1 so there is no common instance basis to use for comparison. In all other domains – Openstacks, ParcPrinter, Pathways, Thoughtful, Tidybot, TPP, and Woodworking – the lack of advantages through learning are due to a lack of ability (1), with no or hardly any conflicts being identified by forward search.</paragraph><paragraph>Let us finally consider the ablation study, Table 3 part (C), fixing neighbors refinement but varying the search algorithm – DFS vs. DOS (depth-oriented search, cf. Section 4) – as well as switching clause learning on/off. (The data for DFS with clause learning is identical to that in column “DFS Nei”; we repeat it here for convenience.) On the IPC benchmarks, both DFS and clause learning, as opposed to DOS and no clause learning, have little impact on coverage. On the resource-constrained benchmarks though, both clearly and significantly improve coverage. Overall, they are useful algorithm improvements. Fig. 4 provides further evidence towards this through a per-instance runtime comparison.</paragraph></section><section label="7.3"><section-title>Proving unsolvability</section-title><paragraph>We now consider the unsolvable case. We again run the six variants of our technique (with early termination, i.e., not generating unsolvability certificates). We run offline learning without a size bound to prove unsolvability directly on the initial state; and we run offline learning with size bound N to generate static {a mathematical formula}uC dead-end detectors. We use {a mathematical formula}N∈{2,32} like above, and we use the resulting static dead-end detectors in DFS without learning.</paragraph><paragraph>We run blind forward search as a simple reference baseline. We run search with {a mathematical formula}h1 as a canonical dead-end detector. We run all the competing algorithms described above, i.e., search with alternate dead-end detectors, search with admissible pruning techniques, as well as the other UIPC'16 techniques including BDD-based symbolic search etc. The admissible pruning techniques are run along with {a mathematical formula}h1, which is more competitive than blind search.</paragraph><paragraph>Table 4 shows coverage data (as before, we will consider offline learning separately below). Compared to DFS without learning, all refinement methods result in a performance boost on resource-constrained domains, where conflict learning is key, especially with constrainedness levels close to 1 where conflicts abound. On the non-resource UIPC'16 benchmarks though, this kind of learning simply does not work well. It helps only, for some configurations, in Diagnosis and DocTransfer. We will analyze the reasons below.</paragraph><paragraph>Compared to the baselines, DFS without learning has basically the same coverage as search with {a mathematical formula}h1, which makes sense as both use the same dead-end detector throughout. Blind search is consistently outclassed except in BagBarman where it is state of the art (the pattern database heuristic does not detect any dead-ends in this domain, so defaults to blind search).</paragraph><paragraph>Comparing the different refinement variants within our DFS framework, they behave similarly overall, though there are remarkable differences in individual domains, namely Bottleneck, Diagnosis, DocTransfer, and PegSol. On resource-constrained domains, neighbors refinement is clearly superior, followed by path-cut refinement and the conflict refinement from prior work.</paragraph><paragraph>Consider now the UIPC'16 algorithms. Potential heuristics clearly dominate on the non-resource UIPC'16 benchmarks, outclassing the competition (including our techniques), beat only on BagBarman, Diagnosis, and DocTransfer. On resource-constrained domains, on the other hand, potential heuristics are very weak. The next-best techniques from UIPC'16 are approximate merge-and-shrink, PDBs, and red–black state space search, with reasonable results on non-resource domains, and with strong results on resource-constrained ones. Strong stubborn sets pruning does not yield any benefits here. Dominance pruning and property-directed reachability work well on resource-constrained domains, yet still worse than the other competitors. The latter applies also to resource-variable detection. Irrelevance pruning is typically detrimental here, with benefits only in Diagnosis and DocTransfer.</paragraph><paragraph>Comparing the UIPC'16 algorithms to our techniques, there is no strong case for conflict learning on the non-resource domains. On the resource-constrained domains though, our techniques, especially neighbors refinement, are competitive. On the Nakhost et al. benchmarks, DFS with neighbors refinement is second in overall coverage only to approximate merge-and-shrink and red–black state space search, both completely unrelated algorithms. It beats approximate merge-and-shrink in Rovers, and it beats red–black state space search in TPP.</paragraph><paragraph>In difference to the solvable benchmarks discussed above, on the unsolvable benchmarks combinations of our learning methods with other dead-end detectors exhibit considerable synergy. This is most pronounced for the “NP” combination with merge-and-shrink, using neighbors refinement or path-cut refinement depending on the situation. This combination outperforms its components in each of the Nakhost et al. domains, and it has the best overall coverage on these domains, of all the algorithms tested here.</paragraph><paragraph>For the other combinations, the synergy is weaker. The only case where the combination outperforms its components is “N” with merge-and-shrink in UIPC'16 TPP with {a mathematical formula}W=0.5. Across domains, though, almost all combinations exhibit more consistent strength than their components. Indeed, all combinations except “N” with merge-and-shrink dominate their components in overall coverage.</paragraph><paragraph>Similarly to our discussion of solvable benchmarks above, we next provide a view of search space sizes under our different DFS learning techniques, with neighbors refinement as the comparison baseline. Fig. 5 gives the data.</paragraph><paragraph>The main conclusions are very similar to those we made for solvable planning tasks (cf. Fig. 3). The comparison to no learning in (a) shows that learning is often detrimental on the non-resource UIPC'16 benchmarks, yet has dramatic benefits on the resource-constrained ones. Remember that the only reason for this is generalization. Search space size reduction factors provide an impressive view on how dramatic the improvements are. Over those instances commonly solved by both configurations in Fig. 5(a), the minimum/geometric mean/maximum reduction factors on the Nakhost et al. domains are 6.7/436.5/37561.5 for NoMystery; 65.0/1286.6/69668.1 for Rovers; and 190.0/711.9/1900.5 for TPP. That is, we get reductions of 2–3 orders of magnitude on average, and even the minimum reductions are of 1–2 orders of magnitude.</paragraph><paragraph>The comparison between refinement methods in Fig. 5(b) and (c) yields, like on the solvable benchmarks, the conclusion that neighbors refinement and path-cut refinement either perform very similarly or are highly complementary, and that tailoring the refinement method to dead-end detection is typically beneficial.</paragraph><paragraph>Table 5 shows the data for (A) offline learning, (B) performance analysis, and (C) ablation studies. Consider first part (A) of the table. Regarding unbounded offline learning, where unsolvability is proved without search on the initial state, the data shows that this has very little merit compared to online learning. Each of path-cut refinement and conflict refinement is almost consistently dominated by the respective online learning method, the only noteworthy exceptions being BagTransport, PegSol, and NoMystery. Comparing to neighbors refinement which is not available in the offline context, the only strong cases for offline refinement are BagTransport for offline path-cut refinement, and Diagnosis for offline conflict refinement. In all cases, the online learning approaches are superior in overall coverage.</paragraph><paragraph>For bounded offline learning, i.e., static {a mathematical formula}uC dead-end detectors, the picture changes dramatically on the non-resource UIPC'16 benchmarks. This is, however, simply due to the size bound, which avoids the slowdown incurred by learning too many conjunctions – all the bounded offline learning configurations are dominated consistently here by DFS without any learning at all. Furthermore, on the resource-constrained benchmarks, the bounded offline learning configurations are dominated, typically outperformed, by their online learning counterparts. The single exception to the latter is the UIPC TPP domain with {a mathematical formula}W=0.99, where the static dead-end detectors do better. Overall, the picture is quite clearly in favor of search with online learning.</paragraph><paragraph>Consider now part (B) of Table 5, showing data assessing the dependency of our techniques on (1) conflict identification, (2) effective learning, and (3) strong generalization. Regarding (1), on the resource-constrained benchmarks, unsurprisingly, the “D.I. #” data shows that conflicts are identified on (almost) every instance. From the other benchmarks though, on half of the domains ability (1) is not given, i.e., no or not enough learning can take place. Namely, in all Bag* domains, in SlidingTiles, and in Tetris, no conflicts are identified at all; in PegSolRow5 it is almost that bad. In SlidingTiles and Tetris, this is simply because all actions are invertible, so the state space is strongly connected, and the first conflict is identified only after the entire state space is already explored. In the Bag* domains, in the rare cases where a conflict is identified, the learning is ineffective and prevents the search from terminating successfully.</paragraph><paragraph>Regarding (2) and (3), consider the “Slo N” and “Red {a mathematical formula}S” columns. We should expect good performance if the value for “Red {a mathematical formula}S” (search reduction factor) is larger than that for “Slo N” (per-node slowdown factor), on commonly solved instances. On the resource-constrained benchmarks, this is consistently the case. On the non-resource UIPC, the only domain where the reduction exceeds the slowdown is Diagnosis, precisely the only domain where neighbors refinement improves coverage relative to the baseline. In all other domains where ability (1) is given, the slowdown is much larger than the reduction, to a particularly striking extent in Bottleneck, ChessBoard, and PegSol, precisely the domains where neighbors refinement is most detrimental.</paragraph><paragraph>Let us finally consider the ablation study, Table 5 part (C). The different configuration settings have little impact on coverage here. DFS does a little worse than DOS on the non-resource UIPC, but a little better on the resource-constrained benchmarks; similarly for clause learning vs. no clause learning. As Fig. 6 shows, however, both DFS and clause learning are useful algorithm improvements, that rarely hurt runtime performance, while improving it in the most challenging cases.</paragraph></section><section label="7.4"><section-title>Generating unsolvability certificates</section-title><paragraph>Let us finally consider the generation of unsolvability certificates. An unsolvability certificate must (1) be verifiable in its size; must (2) be feasible to compute; and (3) is useful only if it is compact, i.e., loosely speaking, it is much smaller than the state space itself. Conjunction sets C where {a mathematical formula}uC(I)=∞ qualify for (1). But how feasible is it to compute them, and how compact are they?</paragraph><paragraph>All our online learning variants guarantee to terminate with {a mathematical formula}uC(I)=∞, provided the early termination option is switched off. In difference to the previous section, we now consider that setting. For comparison, we run the two unbounded offline learning variants. Table 6 shows the data.</paragraph><paragraph>We consider DFS with neighbors refinement, the overall most effective refinement method. Our main interest lies in comparing this online learning method to offline learning. We include both refinement methods applicable to the latter purpose. We include a comparison to DFS with (neighbors refinement and) early termination, not producing an unsolvability certificate, to assess the overhead incurred by the final refinement step when the search space is already empty. We include the baselines only for reference.</paragraph><paragraph>Consider first coverage, part (A) of the table, measuring how effective the three different strategies are at producing unsolvability certificates. Neighbors refinement is clearly best overall, and it consistently dominates on the resource-constrained benchmarks. On the non-resource benchmarks though, the offline methods are competitive, path-cut refinement even better overall, with substantial advantages in BagTransport, Bottleneck, and Diagnosis.</paragraph><paragraph>Observe that, with early termination, neighbors refinement “beats” the offline methods on the non-resource UIPC, while without early termination it does not. The advantage of neighbors refinement without early termination here mainly stems from the SlidingTiles and Tetris domains, which is simply due to the aforementioned fact that no learning takes place here (cf. Table 5). So the superiority of DFS with early termination here is one of search, not of learning. On the resource-constrained benchmarks, switching early termination off does not have any adverse impact on coverage.</paragraph><paragraph>Consider now part (B) of Table 6, giving a view of absolute certificate size (while (C) is relative to state space size). Online learning with neighbors refinement is superior on the resource-constrained benchmarks, except for Nakhost et al. Rovers (and one case of UIPC TPP), where the picture is more mixed. On the non-resource UIPC benchmarks, the methods are complementary, with differing strengths depending on the domain.</paragraph><paragraph>By definition, the relative performance of learning methods is the same in parts (C) and (B). What's remarkable in (C) is that the certificates found often are extremely compact, several orders of magnitude smaller than the state space itself. This is especially pronounced in the resource-constrained benchmarks, but also happens in some of the other domains, most notably in Bottleneck, Diagnosis, and DocTransfer.</paragraph></section></section><section label="8"><section-title>Conclusion</section-title><paragraph>Our work pioneers conflict-directed learning, of sound generalizable knowledge, from conflicts – dead-end states – in forward state space search. The basis are critical-path heuristic functions {a mathematical formula}hC, that allow to consider an arbitrary set C of atomic conjunctions, and that detect all dead-ends in the limit. Our key technical contributions are search methods identifying conflict states, and refinement methods extending C so that {a mathematical formula}hC recognizes these states. The resulting technique is, in our humble opinion, quite elegant, and suggests that the learning from “true” conflicts in state space search, not necessitating a solution length bound, is worth the community's attention.</paragraph><paragraph>Beauty contests aside, from a pragmatical point of view our techniques certainly do not, as they stand, deliver an empirical breakthrough. They require a rather specific kind of problem structure to work well, namely structure that allows for (1) quick conflict identification, (2) effective learning, and (3) strong generalization. This kind of problem structure is typical of resource-constrained planning, as far as reflected by the current benchmarks from that area. On other domains, as far as reflected by the competition benchmarks, this structure is scarce, though it does sometimes appear.</paragraph><paragraph>An interesting question in this context is the relation between requirement (1) vs. a plan length bound. The two requirements are correlated in that conflict identification will be easier on problems whose search paths are typically short. Furthermore, if a bound is available, then manifold alternate conflict analysis techniques can be used, simply via the correspondence to constraint satisfaction.</paragraph><paragraph>However, having “typically” short search paths is a much weaker assumption than having a globally valid length bound, in particular a bound that is known a-priori before search begins. In the non-resource domains where our techniques work well – Floortile, ParcPrinter, Pathways, Childsnack, Diagnosis, DocTransfer – it is completely unclear how an upper bound should be derived. Even in the resource-constrained benchmarks, this is not obvious: not all actions consume resources, so some reasoning over the possible non-consuming action subsequences would be required.</paragraph><paragraph>Regarding future work, ours is merely a first foray into forward search conflict-learning techniques, and lots more remains to be explored. We hope and expect our work to be the beginning of the story, not its end.</paragraph><paragraph>For conjunctions learning, important open questions pertain, e.g., to ranking criteria allowing a more informed choice of which new conjunctions to construct during refinement, as well as allowing to forget conjunctions learned previously in case they did not prove useful for the search.</paragraph><paragraph>For clause learning, exciting questions pertain to extending its, as yet, very limited role. Can we learn easily testable criteria that, in conjunction, are sufficient and necessary for {a mathematical formula}uC=∞, thus matching the pruning power of {a mathematical formula}uC itself? Can such criteria form building blocks for later learning steps, like the clauses in SAT? Can we do some form of reasoning over clauses, deducing new knowledge from the old one, given the action specifications?</paragraph><paragraph>Critical-path heuristics are merely one means for dead-end detection, and an exciting big line of research is the design of refinement methods for other dead-end detectors. Can we refine merge-and-shrink unsolvability heuristics on the fly? What about potential heuristics? If so, how to make the most out of the combination of all three methods?</paragraph><paragraph>Last but not least, one thing we would particularly like to see is the export of this (kind of) technique to game-playing and model checking, where dead-end detection is at least as, probably more, important than in classical planning. For {a mathematical formula}hC refinement, this works “out of the box” modulo the applicability of Equation (1), i.e., the definition of critical-path heuristics. As is, this requires conjunctive subgoaling behavior. But more general logics (e.g. minimization to handle disjunctions) should be manageable.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>This work was partially supported by the German Research Foundation (DFG), under grant HO 2169/5-1, “Critically Constrained Planning via Partial Delete Relaxation”.</paragraph></acknowledgements><appendices><section label="Appendix A"><section-title>Proofs</section-title><paragraph label="Theorem 1">At the start of the while loop inAlgorithm 1, the labeled states are exactly the known dead-ends.</paragraph><paragraph label="Proof">Soundness, i.e., t labeled ⇒ t is a known dead-end, follows immediately from construction because at the moment a state t is labeled we have {a mathematical formula}R[t]⊆Closed, and once that condition is true obviously it remains true for the remainder of the search.Completeness, i.e., t is a known dead-end ⇒ t labeled, can be proved by induction on the number of expansions. Assume that the claim holds before a state s is expanded; we need to show that, for any states t that were not known dead-ends before but are known dead-ends now, t will be labeled. Call such t new-label states. Clearly, any new-label state must be an ancestor of s. Therefore, a new-label state can exist only if, after the expansion, {a mathematical formula}R[s]⊆Closed: else, an open state is reachable from s, and transitively is reachable from all ancestors of s. In the case where {a mathematical formula}R[s]⊆Closed, s is labeled and so is every new-label state parent t of s, due to the recursive invocation on t. It remains to show that each new-label state t will indeed be reached, and thus labeled, during the reverse traversal of the search space induced by the recursive invocations of CheckAndLearn. This is a direct consequence of the following two observations: (1) each ancestor t of s has not been labeled dead end so far, and (2) for each new-label state t, the search space contains a path of new-label states {a mathematical formula}t=t0,t1,…,tn=s. The first observation follows immediately from the algorithm: since t is an ancestor of s, t must have been expanded at some point (which means that t could not have been labeled dead end before its expansion), and t could not have been labeled dead end during any previous call to CheckAndLearn because at least one open state was reachable from t during any such call. The second observation follows immediately from {a mathematical formula}R[t]⊆Closed: (as above) t is an ancestor of s, i.e., the search space contains a path {a mathematical formula}t=t0,t1,…,tn=s, and due to the transitivity of reachability, for every {a mathematical formula}1≤i&lt;n, {a mathematical formula}R[ti]⊆Closed. Obviously, for every {a mathematical formula}1≤i≤n, s is also reachable from {a mathematical formula}ti, meaning that {a mathematical formula}ti must be a new-label state, too. Since CheckAndLearn will traverse at least one such path from t to s in reverse order, t will indeed be labeled eventually.  □</paragraph><paragraph label="Theorem 2">Let C be any set of atomic conjunctions. Let s be a state with{a mathematical formula}hC(s)&lt;h⁎(s). Then:</paragraph><list><list-item label="(i)">The execution of{a mathematical formula}PathCutRefine(G,hC(s))is well defined, i.e., (a) in any call{a mathematical formula}PathCutRefine(G,n)there exists{a mathematical formula}c∈Cso that{a mathematical formula}c⊆Gand{a mathematical formula}hC(s,c)≥n; and (b) if{a mathematical formula}n=0, then{a mathematical formula}G⊈s.</list-item><list-item label="(ii)">If X is the set of conjunctions resulting from{a mathematical formula}PathCutRefine(G,hC(s)), then{a mathematical formula}hC∪X(s)&gt;hC(s).</list-item></list><paragraph label="Proof"><list><list-item label="(i)">(a) follows directly from Equation (1). Initially, there must be some {a mathematical formula}c∈C so that {a mathematical formula}c⊆G and {a mathematical formula}hC(s,c)=hC(s,G) (last case of Equation (1)). Consider a recursive call {a mathematical formula}PathCutRefine(G,n). Let {a mathematical formula}G′, {a mathematical formula}n′ be the arguments of the call to PathCutRefine that caused the recursion, let a be the corresponding action, and let {a mathematical formula}c′⊆G′ be the conjunction selected in {a mathematical formula}PathCutRefine(G′,n′). Due to the selection of {a mathematical formula}c′, we have {a mathematical formula}hC(s,c′)=hC(s,G′)=n′=n+1; and because {a mathematical formula}c′⊆G′, we also have {a mathematical formula}R(c′,a)⊆R(G′,a)=G. Hence, by definition of {a mathematical formula}hC, {a mathematical formula}hC(s,G)≥n, i.e. there is a conjunction {a mathematical formula}c∈C, {a mathematical formula}c⊆G so that {a mathematical formula}hC(s,c)≥n.For (b), assume for contradiction that there is a call {a mathematical formula}PathCutRefine(G,0) where {a mathematical formula}G⊆s. Let {a mathematical formula}an,…,a1 be the actions that label the recursion path down to the call {a mathematical formula}PathCutRefine(G,0). It is easy to show by induction that {a mathematical formula}〈a1,…,an〉 is actually a plan for s. However, n is exactly {a mathematical formula}hC(s), which means that {a mathematical formula}hC(s)=h⁎(s), a contradiction to the assumption.</list-item><list-item label="(ii)">We show for every call {a mathematical formula}PathCutRefine(G,n) and for the constructed conflict x that {a mathematical formula}hC∪X(s,x)&gt;n when {a mathematical formula}PathCutRefine(G,n) terminates. In other words, when {a mathematical formula}PathCutRefine(G,hC(s)) terminates, then we have {a mathematical formula}hC∪X(s)&gt;hC(s), as desired. The proof is by induction on n. For {a mathematical formula}n=0, the conflict {a mathematical formula}x⊆G is chosen such that {a mathematical formula}x⊈s. Hence, {a mathematical formula}hC∪X(s,x)&gt;0=n due to Equation (1). For the induction step, {a mathematical formula}n&gt;0, let x be the conflict that is constructed in the call {a mathematical formula}PathCutRefine(G,n). Since {a mathematical formula}n&gt;0, there must be an atomic conjunction {a mathematical formula}c∈C that is part of x, {a mathematical formula}c⊆x, and so that {a mathematical formula}hC(s,c)≥n. If {a mathematical formula}hC(s,c)&gt;n, then clearly {a mathematical formula}hC(s,x)&gt;n and the claim follows. So, assume that {a mathematical formula}hC(s,c)=n, and let {a mathematical formula}a∈A[x] be an arbitrary achiever of x (i.e., {a mathematical formula}R(x,a)≠⊥). In case {a mathematical formula}A[x] is empty, it directly follows that {a mathematical formula}hC∪X(s,x)=∞&gt;n (Equation (1)). Otherwise, distinguish between the cases {a mathematical formula}a∈A[c] and {a mathematical formula}a∉A[c]. If {a mathematical formula}a∉A[c], then, since {a mathematical formula}c⊆x and {a mathematical formula}x∩del(a)=∅, i.e. {a mathematical formula}c∩del(a)=∅, we have that {a mathematical formula}add(a)∩c=∅. Therefore, {a mathematical formula}c⊆R(x,a) and thus {a mathematical formula}hC∪X(s,R(x,a))≥n. On the other hand, if {a mathematical formula}a∈A[c], then we must have recurred on {a mathematical formula}G′=R(G,a) and {a mathematical formula}n′=n−1. If {a mathematical formula}x′ is the conflict constructed in this call, then we know by induction hypothesis that {a mathematical formula}hC∪X(s,x′)&gt;n′. Because of the selection of x, we ensured that {a mathematical formula}x′⊆R(x,a), and as a consequence {a mathematical formula}hC∪X(s,R(x,a))&gt;n′=n−1. Since a was chosen arbitrarily, this shows that {a mathematical formula}hC∪X(s,x)&gt;n. □</list-item></list></paragraph><paragraph label="Theorem 3">Let C be any set of atomic conjunctions. Let{a mathematical formula}Sˆbe a set of dead-end states, and let{a mathematical formula}Tˆbe its neighbors with the{a mathematical formula}uC-recognized neighbors property. Then:</paragraph><list><list-item label="(i)">The execution of{a mathematical formula}NeighborsRefine(G)is well defined, i.e., it is always possible to extract an x as specified.</list-item><list-item label="(ii)">The execution of{a mathematical formula}NeighborsRefine(G)terminates.</list-item><list-item label="(iii)">Upon termination of{a mathematical formula}NeighborsRefine(G),{a mathematical formula}uC∪X(s)=∞for every{a mathematical formula}s∈Sˆ.</list-item></list><paragraph label="Proof">(i) follows by induction on the recursion depth. For the induction beginning, note that the selection of {a mathematical formula}x⊆G=G in ExtractX is well-defined because of the recognized neighbors property (*), and because {a mathematical formula}Sˆ does not contain a goal state. For the induction step, assume for contradiction that ExtractX fails to select a conjunction {a mathematical formula}x⊆G that satisfies the condition of Lemma 2, where {a mathematical formula}G=R(x′,a) is given as input, i.e., {a mathematical formula}x′ is chosen as in Lemma 2, and a is an action from {a mathematical formula}A[x′]. In other words, (i) there is a state {a mathematical formula}s∈Sˆ with {a mathematical formula}G⊆s, or (ii) there is a state {a mathematical formula}t∈Tˆ so that for all conjunctions {a mathematical formula}c0∈C with {a mathematical formula}c0⊆G, {a mathematical formula}uC(t,c0)&lt;∞. It cannot be (i) because otherwise, it follows from {a mathematical formula}a∈A[x′] and {a mathematical formula}R(x′,a)=G⊆s that {a mathematical formula}s[[a]] is defined and {a mathematical formula}x′⊆s[[a]], i.e., {a mathematical formula}h⁎(s,x′)&lt;∞. This is a contradiction to the selection of {a mathematical formula}x′. For (ii), let {a mathematical formula}c0′∈C, {a mathematical formula}c0′⊆x′ be some conjunction with {a mathematical formula}uC(t,c0′)=∞. Such a conjunction must exist due to the selection of {a mathematical formula}x′. Since {a mathematical formula}uC(t,c0′)=∞, it directly follows that {a mathematical formula}c0′⊈R(x′,a). However, {a mathematical formula}c0′⊆x′, so {a mathematical formula}c0′⊆add(a), and thus {a mathematical formula}a∈A[c0′]. Now plugging in the definition of {a mathematical formula}uC, we get from {a mathematical formula}uC(t,R(x′,a))&lt;∞ and {a mathematical formula}R(c0′,a)⊆R(x′,a) that {a mathematical formula}uC(t,R(c0′,a))&lt;∞. In other words: {a mathematical formula}uC(t,c0′)&lt;∞. This is clearly a contradiction to the selection of {a mathematical formula}c0′. We conclude that there must be a conjunction {a mathematical formula}x⊆G that satisfies the conditions of Lemma 2.For (ii) note that in every single recursion, a new conjunction x is added to X. This is true because before going into recursion on some {a mathematical formula}R(x,a), we make sure that there does not exist {a mathematical formula}x′∈X so that {a mathematical formula}x′⊆R(x,a). Thus, regardless of the selection of the conflict {a mathematical formula}x′⊆R(x,a) in the corresponding call to {a mathematical formula}ExtractX(R(x,a)), {a mathematical formula}x′ cannot be contained in X. After selecting the conflict {a mathematical formula}x′, it is added to X. So X is extended by a new conflict in each recursion. But since the overall number of conjunctions is bounded, it immediately follows that the number of recursions is bounded.To show (iii), we make use of the observation {a mathematical formula}uC(s,G)=∞ iff {a mathematical formula}hC(s,G)=∞ for any set of facts {a mathematical formula}G⊆F. Let {a mathematical formula}s∈Sˆ be arbitrary, and let {a mathematical formula}x∈X be a conjunction with minimal {a mathematical formula}hC∪X-value, i.e., let {a mathematical formula}x∈X be so that for all {a mathematical formula}x′∈X: {a mathematical formula}hC∪X(s,x)≤hC∪X(s,x′). Assume for contradiction that {a mathematical formula}hC∪X(s,x)&lt;∞. Due to the construction of X, it must be {a mathematical formula}x⊈s, meaning that there must be an action {a mathematical formula}a∈A[x] with {a mathematical formula}hC∪X(s,R(x,a))&lt;hC∪X(s,x) (definition of {a mathematical formula}hC∪X). However, the refinement algorithm ensures that X contains a conjunction {a mathematical formula}x′⊆R(x,a): in the call to Refine where x is added to X, the algorithm makes sure that for each action {a mathematical formula}a∈A[x], either there is already a conjunction {a mathematical formula}x′∈X so that {a mathematical formula}x′⊆R(x,a), or it calls {a mathematical formula}Refine(R(x,a)) which in turn adds a conjunction {a mathematical formula}x′⊆R(x,a) to X. But this is a contradiction to the {a mathematical formula}hC∪X minimality assumption: as there is a conjunction {a mathematical formula}x′∈X with {a mathematical formula}x′⊆R(x,a), it is {a mathematical formula}hC∪X(s,x′)≤hC∪X(s,R(x,a))&lt;hC∪X(s,x). This shows that {a mathematical formula}hC∪X(s,x)=∞ for every {a mathematical formula}x∈X, and for every state {a mathematical formula}s∈Sˆ, and thus {a mathematical formula}uC(s)=∞ for every {a mathematical formula}s∈Sˆ.  □</paragraph></section></appendices><references><reference label="[1]"><authors>R. Dechter</authors><title>Enhancement schemes for constraint processing: backjumping, learning, and cutset decomposition</title><host>Artif. Intell.41 (1990) pp.273-312</host></reference><reference label="[2]"><authors>P. Prosser</authors><title>Hybrid algorithms for the constraint satisfaction problem</title><host>Comput. Intell.9 (1993) pp.268-299</host></reference><reference label="[3]"><authors>J. Marques-Silva,K. Sakallah</authors><title>GRASP: a search algorithm for propositional satisfiability</title><host>IEEE Trans. Comput.48 (1999) pp.506-521</host></reference><reference label="[4]"><authors>M. Moskewicz,C. Madigan,Y. Zhao,L. Zhang,S. Malik</authors><title>Chaff: engineering an efficient SAT solver</title><host>Proceedings of the 38th Conference on Design AutomationDAC-01(2001)IEEE Computer SocietyLas Vegas, Nevada, USA</host></reference><reference label="[5]"><authors>R. Dechter,D. Frost</authors><title>Backjump-based backtracking for constraint satisfaction problems</title><host>Artif. Intell.136 (2002) pp.147-188</host></reference><reference label="[6]"><authors>N. Eén,N. Sörensson</authors><title>An extensible sat-solver</title><host>Proceedings of the 6th International Conference Theory and Applications of Satisfiability TestingSAT'03(2003) pp.502-518</host></reference><reference label="[7]"><authors>P. Beame,H.A. Kautz,A. Sabharwal</authors><title>Towards understanding and harnessing the potential of clause learning</title><host>J. Artif. Intell. Res.22 (2004) pp.319-351</host></reference><reference label="[8]"><authors>H. Kautz,B. Selman</authors><title>Unifying SAT-based and graph-based planning</title><host>M. PollackProceedings of the 16th International Joint Conference on Artificial IntelligenceIJCAI'99(1999)Morgan KaufmannStockholm, Sweden pp.318-325</host></reference><reference label="[9]"><authors>J. Rintanen,K. Heljanko,I. Niemelä</authors><title>Planning as satisfiability: parallel plans and algorithms for plan search</title><host>Artif. Intell.170 (2006) pp.1031-1080</host></reference><reference label="[10]"><authors>A.L. Blum,M.L. Furst</authors><title>Fast planning through planning graph analysis</title><host>Artif. Intell.90 (1997) pp.279-298</host></reference><reference label="[11]"><authors>D. Long,M. Fox</authors><title>Efficient implementation of the plan graph in STAN</title><host>J. Artif. Intell. Res.10 (1999) pp.87-115</host></reference><reference label="[12]"><authors>S. Kambhampati</authors><title>Planning graph as a (dynamic) CSP: exploiting EBL, DDB and other CSP search techniques in graphplan</title><host>J. Artif. Intell. Res.12 (2000) pp.1-34</host></reference><reference label="[13]"><authors>A.R. Bradley</authors><title>Sat-based model checking without unrolling</title><host>Proceedings of the 12th International Conference on Verification, Model Checking, and Abstract InterpretationVMCAI'11(2011) pp.70-87</host></reference><reference label="[14]"><authors>M. Suda</authors><title>Property directed reachability for automated planning</title><host>J. Artif. Intell. Res.50 (2014) pp.265-319</host></reference><reference label="[15]"><authors>S. Minton,J.G. Carbonell,C.A. Knoblock,D. Kuokka,O. Etzioni,Y. Gil</authors><title>Explanation-based learning: a problem solving perspective</title><host>Artif. Intell.40 (1989) pp.63-118</host></reference><reference label="[16]"><authors>S. Kambhampati,S. Katukam,Y. Qu</authors><title>Failure driven dynamic search control for partial order planners: an explanation based approach</title><host>Artif. Intell.88 (1996) pp.253-315</host></reference><reference label="[17]"><authors>S. Kambhampati</authors><title>On the relations between intelligent backtracking and failure-driven explanation-based learning in constraint satisfaction and planning</title><host>Artif. Intell.105 (1998) pp.161-208</host></reference><reference label="[18]"><authors>N. Bhatnagar,J. Mostow</authors><title>On-line learning from search failures</title><host>Mach. Learn.15 (1994) pp.69-117</host></reference><reference label="[19]"><authors>A. Kolobov,Mausam,D.S. Weld</authors><title>Discovering hidden structure in factored MDPs</title><host>Artif. Intell.189 (2012) pp.19-47</host></reference><reference label="[20]"><authors>R.E. Korf</authors><title>Real-time heuristic search</title><host>Artif. Intell.42 (1990) pp.189-211</host></reference><reference label="[21]"><authors>A. Reinefeld,T.A. Marsland</authors><title>Enhanced iterative-deepening search</title><host>IEEE Trans. Pattern Anal. Mach. Intell.16 (1994) pp.701-710</host></reference><reference label="[22]"><authors>A.G. Barto,S.J. Bradtke,S.P. Singh</authors><title>Learning to act using real-time dynamic programming</title><host>Artif. Intell.72 (1995) pp.81-138</host></reference><reference label="[23]"><authors>B. Bonet,H. Geffner</authors><title>Learning depth-first search: a unified approach to heuristic search in deterministic and non-deterministic settings, and its application to MDPs</title><host>D. LongS. SmithProceedings of the 16th International Conference on Automated Planning and SchedulingICAPS'06(2006)Morgan KaufmannAmbleside, UK pp.142-151</host></reference><reference label="[24]"><authors>D.E. Smith</authors><title>Choosing objectives in over-subscription planning</title><host>S. KoenigS. ZilbersteinJ. KoehlerProceedings of the 14th International Conference on Automated Planning and SchedulingICAPS'04(2004)Morgan KaufmannWhistler, Canada pp.393-401</host></reference><reference label="[25]"><authors>A. Gerevini,P. Haslum,D. Long,A. Saetti,Y. Dimopoulos</authors><title>Deterministic planning in the fifth international planning competition: PDDL3 and experimental evaluation of the planners</title><host>Artif. Intell.173 (2009) pp.619-668</host></reference><reference label="[26]"><authors>C. Domshlak,V. Mirkis</authors><title>Deterministic oversubscription planning as heuristic search: abstractions and reformulations</title><host>J. Artif. Intell. Res.52 (2015) pp.97-169</host></reference><reference label="[27]"><authors>P. Haslum,H. Geffner</authors><title>Heuristic planning with time and resources</title><host>A. CestaD. BorrajoProceedings of the 6th European Conference on PlanningECP'01(2001)Springer-Verlag pp.121-132</host></reference><reference label="[28]"><authors>H. Nakhost,J. Hoffmann,M. Müller</authors><title>Resource-constrained planning: a Monte Carlo random walk approach</title><host>B. BonetL. McCluskeyJ.R. SilvaB. WilliamsProceedings of the 22nd International Conference on Automated Planning and SchedulingICAPS'12(2012)AAAI Press pp.181-189</host></reference><reference label="[29]"><authors>A.J. Coles,A. Coles,M. Fox,D. Long</authors><title>A hybrid LP-RPG heuristic for modelling numeric resource flows in planning</title><host>J. Artif. Intell. Res.46 (2013) pp.343-412</host></reference><reference label="[30]"><authors>A. Junghanns,J. Schaeffer</authors><title>Sokoban: evaluating standard single-agent search techniques in the presence of deadlock</title><host>Proceedings of the 12th Biennial Conference of the Canadian Society for Computational Studies of Intelligence(1998) pp.1-15</host></reference><reference label="[31]"><authors>R. Bjarnason,P. Tadepalli,A. Fern</authors><title>Searching solitaire in real time</title><host>ICGA J.30 (2007) pp.131-142</host></reference><reference label="[32]"><authors>G. Behrmann,J. Bengtsson,A. David,K.G. Larsen,P. Pettersson,W. Yi</authors><title>UPPAAL implementation secrets</title><host>Proceedings of the 7th International Symposium on Formal Techniques in Real-Time and Fault Tolerant Systems(2002)</host></reference><reference label="[33]"><authors>G. Holzmann</authors><title>The Spin Model Checker – Primer and Reference Manual</title><host>(2004)Addison–Wesley</host></reference><reference label="[34]"><authors>S. Edelkamp,A. Lluch-Lafuente,S. Leue</authors><title>Directed explicit-state model checking in the validation of communication protocols</title><host>Int. J. Softw. Tools Technol. Transf.5 (2004) pp.247-267</host></reference><reference label="[35]"><authors>C. Bäckström,P. Jonsson,S. Ståhlberg</authors><title>Fast detection of unsolvable planning instances using local consistency</title><host>M. HelmertG. RögerProceedings of the 6th Annual Symposium on Combinatorial SearchSOCS'13(2013)AAAI Press pp.29-37</host></reference><reference label="[36]"><authors>J. Hoffmann,P. Kissmann,Á. Torralba</authors><title>“Distance”? Who Cares? Tailoring merge-and-shrink heuristics to detect unsolvability</title><host>T. SchaubProceedings of the 21st European Conference on Artificial IntelligenceECAI'14(2014)IOS PressPrague, Czech Republic</host></reference><reference label="[37]"><authors>C.J. Muise,S.A. McIlraith,J.C. Beck</authors><title>Improved non-deterministic planning by exploiting state relevance</title><host>B. BonetL. McCluskeyJ.R. SilvaB. WilliamsProceedings of the 22nd International Conference on Automated Planning and SchedulingICAPS'12(2012)AAAI Press</host></reference><reference label="[38]"><authors>A. Camacho,C. Muise,S.A. McIlraith</authors><title>From FOND to robust probabilistic planning: computing compact policies that bypass avoidable deadends</title><host>A. ColesA. ColesS. EdelkampD. MagazzeniS. SannerProceedings of the 26th International Conference on Automated Planning and SchedulingICAPS'16(2016)AAAI Press</host></reference><reference label="[39]"><authors>P. Haslum,H. Geffner</authors><title>Admissible heuristics for optimal planning</title><host>S. ChienR. KambhampatiC. KnoblockProceedings of the 5th International Conference on Artificial Intelligence Planning SystemsAIPS'00(2000)AAAI PressMenlo Park, Breckenridge, CO pp.140-149</host></reference><reference label="[40]"><authors>B. Bonet,H. Geffner</authors><title>Planning as heuristic search</title><host>Artif. Intell.129 (2001) pp.5-33</host></reference><reference label="[41]"><authors>J. Hoffmann,B. Nebel</authors><title>The FF planning system: fast plan generation through heuristic search</title><host>J. Artif. Intell. Res.14 (2001) pp.253-302</host></reference><reference label="[42]"><authors>S. Edelkamp</authors><title>Planning with pattern databases</title><host>A. CestaD. BorrajoProceedings of the 6th European Conference on PlanningECP'01(2001)Springer-Verlag pp.13-24</host></reference><reference label="[43]"><authors>M. Helmert,C. Domshlak</authors><title>Landmarks, critical paths and abstractions: what's the difference anyway?</title><host>A. GereviniA. HoweA. CestaI. RefanidisProceedings of the 19th International Conference on Automated Planning and SchedulingICAPS'09(2009)AAAI Press pp.162-169</host></reference><reference label="[44]"><authors>S. Richter,M. Westphal</authors><title>The LAMA planner: guiding cost-based anytime planning with landmarks</title><host>J. Artif. Intell. Res.39 (2010) pp.127-177</host></reference><reference label="[45]"><authors>M. Helmert,P. Haslum,J. Hoffmann,R. Nissim</authors><title>Merge &amp; shrink abstraction: a method for generating lower bounds in factored state spaces</title><host>J. Assoc. Comput. Mach.61 (2014)</host></reference><reference label="[46]"><authors>K. Dräger,B. Finkbeiner,A. Podelski</authors><title>Directed model checking with distance-preserving abstractions</title><host>A. ValmariProceedings of the 13th International SPIN WorkshopSPIN 2006Lecture Notes in Computer Sciencevol. 3925 (2006)Springer-Verlag pp.19-34</host></reference><reference label="[47]"><authors>M. Helmert,P. Haslum,J. Hoffmann</authors><title>Flexible abstraction heuristics for optimal sequential planning</title><host>M. BoddyM. FoxS. ThiebauxProceedings of the 17th International Conference on Automated Planning and SchedulingICAPS'07(2007)Morgan KaufmannProvidence, Rhode Island, USA pp.176-183</host></reference><reference label="[48]"><authors>K. Dräger,B. Finkbeiner,A. Podelski</authors><title>Directed model checking with distance-preserving abstractions</title><host>Int. J. Softw. Tools Technol. Transf.11 (2009) pp.27-37</host></reference><reference label="[49]"><authors>Á. Torralba,J. Hoffmann,P. Kissmann</authors><title>MS-Unsat and SimulationDominance: merge-and-shrink and dominance pruning for proving unsolvability</title><host>UIPC 2016 Planner Abstracts(2016) pp.12-15</host></reference><reference label="[50]"><authors>F. Pommerening,J. Seipp</authors><title>Fast downward dead-end pattern database</title><host>UIPC 2016 Planner Abstracts(2016) pp.2-</host></reference><reference label="[51]"><authors>F. Pommerening,M. Helmert,G. Röger,J. Seipp</authors><title>From non-negative to general operator cost partitioning</title><host>B. BonetS. KoenigProceedings of the 29th AAAI Conference on Artificial IntelligenceAAAI'15(2015)AAAI Press pp.3335-3341</host></reference><reference label="[52]"><authors>J. Seipp,F. Pommerening,M. Helmert</authors><title>New optimization functions for potential heuristics</title><host>R. BrafmanC. DomshlakP. HaslumS. ZilbersteinProceedings of the 25th International Conference on Automated Planning and SchedulingICAPS'15(2015)AAAI Press pp.193-201</host></reference><reference label="[53]"><authors>J. Seipp,F. Pommerening,S. Sievers,M. Wehrle</authors><title>Fast downward Aidos</title><host>UIPC 2016 Planner Abstracts(2016) pp.28-38</host></reference><reference label="[54]"><authors>M. Steinmetz,J. Hoffmann</authors><title>Towards clause-learning state space search: learning to recognize dead-ends</title><host>D. SchuurmansM. WellmanProceedings of the 30th AAAI Conference on Artificial IntelligenceAAAI'16(2016)AAAI Press</host></reference><reference label="[55]"><authors>M. Steinmetz,J. Hoffmann</authors><title>Clone: a critical-path driven clause learner</title><host>UIPC 2016 Planner Abstracts(2016) pp.24-27</host></reference><reference label="[56]"><authors>Á. Torralba,V. Alcázar</authors><title>Constrained symbolic search: on mutexes, BDD minimization and more</title><host>M. HelmertG. RögerProceedings of the 6th Annual Symposium on Combinatorial SearchSOCS'13(2013)AAAI Press pp.175-183</host></reference><reference label="[57]"><authors>Á. Torralba</authors><title>Sympa: symbolic perimeter abstractions for proving unsolvability</title><host>UIPC 2016 Planner Abstracts(2016) pp.8-11</host></reference><reference label="[58]"><authors>C. Domshlak,J. Hoffmann,M. Katz</authors><title>Red–black planning: a new systematic approach to partial delete relaxation</title><host>Artif. Intell.221 (2015) pp.73-114</host></reference><reference label="[59]"><authors>D. Gnad,M. Steinmetz,M. Jany,J. Hoffmann,I. Serina,A. Gerevini</authors><title>Partial delete relaxation, unchained: on intractable red–black planning and its applications</title><host>J. BaierA. BoteaProceedings of the 9th Annual Symposium on Combinatorial SearchSOCS'16(2015)AAAI Press</host></reference><reference label="[60]"><authors>D. Gnad,M. Steinmetz,J. Hoffmann</authors><title>Django: unchaining the power of red–black planning</title><host>UIPC 2016 Planner Abstracts(2016) pp.19-23</host></reference><reference label="[61]"><authors>M. Wehrle,M. Helmert</authors><title>Efficient stubborn sets: generalized algorithms and selection strategies</title><host>S. ChienM. DoA. FernW. RumlProceedings of the 24th International Conference on Automated Planning and SchedulingICAPS'14(2014)AAAI Press</host></reference><reference label="[62]"><authors>Á. Torralba,J. Hoffmann</authors><title>Simulation-based admissible dominance pruning</title><host>Q. YangProceedings of the 24th International Joint Conference on Artificial IntelligenceIJCAI'15(2015)AAAI Press/IJCAI pp.1689-1695</host></reference><reference label="[63]"><authors>Á. Torralba,P. Kissmann</authors><title>Focusing on what really matters: irrelevance pruning in merge-and-shrink</title><host>L. LelisR. SternProceedings of the 8th Annual Symposium on Combinatorial SearchSOCS'15(2015)AAAI Press pp.122-130</host></reference><reference label="[64]"><authors>P. Haslum</authors><title>Improving heuristics through relaxed search – an analysis of TP4 and HSP*a in the 2004 planning competition</title><host>J. Artif. Intell. Res.25 (2006) pp.233-267</host></reference><reference label="[65]"><authors>P. Haslum</authors><title>hm(P)=h1(Pm): alternative characterisations of the generalisation from hmax to hm</title><host>A. GereviniA. HoweA. CestaI. RefanidisProceedings of the 19th International Conference on Automated Planning and SchedulingICAPS'09(2009)AAAI Press pp.354-357</host></reference><reference label="[66]"><authors>P. Haslum</authors><title>Incremental lower bounds for additive cost planning problems</title><host>B. BonetL. McCluskeyJ.R. SilvaB. WilliamsProceedings of the 22nd International Conference on Automated Planning and SchedulingICAPS'12(2012)AAAI Press pp.74-82</host></reference><reference label="[67]"><authors>E. Keyder,J. Hoffmann,P. Haslum</authors><title>Improving delete relaxation heuristics through explicitly represented conjunctions</title><host>J. Artif. Intell. Res.50 (2014) pp.487-533</host></reference><reference label="[68]"><authors>M. Fickert,J. Hoffmann,M. Steinmetz</authors><title>Combining the delete relaxation with critical-path heuristics: a direct characterization</title><host>J. Artif. Intell. Res.56 (2016) pp.269-327</host></reference><reference label="[69]"><authors>E. Keyder,J. Hoffmann,P. Haslum</authors><title>Semi-relaxed plan heuristics</title><host>B. BonetL. McCluskeyJ.R. SilvaB. WilliamsProceedings of the 22nd International Conference on Automated Planning and SchedulingICAPS'12(2012)AAAI Press pp.128-136</host></reference><reference label="[70]"><authors>T. Bylander</authors><title>The computational complexity of propositional STRIPS planning</title><host>Artif. Intell.69 (1994) pp.165-204</host></reference><reference label="[71]"><authors>J. Hoffmann</authors><title>Local search topology in planning benchmarks: an empirical analysis</title><host>B. NebelProceedings of the 17th International Joint Conference on Artificial IntelligenceIJCAI'01(2001)Morgan KaufmannSeattle, Washington, USA pp.453-458</host></reference><reference label="[72]"><authors>J. Hoffmann</authors><title>Where ‘ignoring delete lists’ works: local search topology in planning benchmarks</title><host>J. Artif. Intell. Res.24 (2005) pp.685-758</host></reference><reference label="[73]"><authors>J. Hoffmann,M. Fickert</authors><title>Explicit conjunctions w/o compilation: computing hFF(ΠC) in polynomial time</title><host>R. BrafmanC. DomshlakP. HaslumS. ZilbersteinProceedings of the 25th International Conference on Automated Planning and SchedulingICAPS'15(2015)AAAI Press</host></reference><reference label="[74]"><authors>N.J. Nilsson</authors><title>Problem Solving Methods in Artificial Intelligence</title><host>(1971)McGraw–Hill</host></reference><reference label="[75]"><authors>P. Jiménez,C. Torras</authors><title>An efficient algorithm for searching implicit AND/OR graphs with cycles</title><host>Artif. Intell.124 (1)(2000) pp.1-30</host></reference><reference label="[76]"><authors>B. Bonet,H. Geffner</authors><title>Labeled RTDP: improving the convergence of real-time dynamic programming</title><host>E. GiunchigliaN. MuscettolaD. NauProceedings of the 13th International Conference on Automated Planning and SchedulingICAPS'03(2003)Morgan KaufmannTrento, Italy pp.12-21</host></reference><reference label="[77]"><authors>R.E. Tarjan</authors><title>Depth first search and linear graph algorithms</title><host>SIAM J. Comput.1 (1972) pp.146-160</host></reference><reference label="[78]"><authors>B. Bonet,H. Geffner</authors><title>Faster heuristic search algorithms for planning with uncertainty and full feedback</title><host>G. GottlobProceedings of the 18th International Joint Conference on Artificial IntelligenceIJCAI'03(2003)Morgan KaufmannAcapulco, Mexico pp.1233-1238</host></reference><reference label="[79]"><authors>M. Helmert</authors><title>The Fast Downward planning system</title><host>J. Artif. Intell. Res.26 (2006) pp.191-246</host></reference><reference label="[80]"><authors>M. Helmert</authors><title>Concise finite-domain representations for PDDL planning tasks</title><host>Artif. Intell.173 (2009) pp.503-535</host></reference><reference label="[81]"><authors>D. Gnad,Á. Torralba,J. Hoffmann,M. Wehrle</authors><title>Decoupled search for proving unsolvability</title><host>UIPC 2016 Planner Abstracts(2016) pp.16-18</host></reference><reference label="[82]"><authors>A. Valmari</authors><title>Stubborn sets for reduced state space generation</title><host>Proceedings of the 10th International Conference on Applications and Theory of Petri Nets(1989) pp.491-515</host></reference><reference label="[83]"><authors>R.E. Bryant</authors><title>Graph-based algorithms for boolean function manipulation</title><host>IEEE Trans. Comput.35 (1986) pp.677-691</host></reference><reference label="[84]"><authors>S. Edelkamp,M. Helmert</authors><title>Exhibiting knowledge in planning problems to minimize state encoding length</title><host>S. BiundoM. FoxProceedings of the 5th European Conference on PlanningECP'99(1999)Springer-Verlag pp.135-147</host></reference><reference label="[85]"><authors>T. Balyo,M. Suda</authors><title>Reachlunch entering the Unsolvability IPC 2016</title><host>UIPC 2016 Planner Abstracts(2016) pp.3-5</host></reference><reference label="[86]"><authors>J. Seipp,M. Helmert</authors><title>Counterexample-guided Cartesian abstraction refinement</title><host>D. BorrajoS. FratiniS. KambhampatiA. OddiProceedings of the 23rd International Conference on Automated Planning and SchedulingICAPS'13(2013)AAAI PressRome, Italy pp.347-351</host></reference><reference label="[87]"><authors>J. Seipp,M. Helmert</authors><title>Diverse and additive cartesian abstraction heuristics</title><host>S. ChienM. DoA. FernW. RumlProceedings of the 24th International Conference on Automated Planning and SchedulingICAPS'14(2014)AAAI Press</host></reference><reference label="[88]"><authors>D. Gnad,J. Hoffmann</authors><title>Beating LM-cut with hmax (sometimes): fork-decoupled state space search</title><host>R. BrafmanC. DomshlakP. HaslumS. ZilbersteinProceedings of the 25th International Conference on Automated Planning and SchedulingICAPS'15(2015)AAAI Press</host></reference><reference label="[89]"><authors>D. Gnad,J. Hoffmann</authors><title>Red–black planning: a new tractability analysis and heuristic function</title><host>L. LelisR. SternProceedings of the 8th Annual Symposium on Combinatorial SearchSOCS'15(2015)AAAI Press</host></reference><reference label="[90]"><authors>K. Korovin,M. Suda</authors><title>iProverPlan: a system description</title><host>UIPC 2016 Planner Abstracts(2016) pp.6-7</host></reference><reference label="[91]"><authors>P. Haslum</authors><title>Adapting h++ for proving plan non-existence</title><host>UIPC 2016 Planner Abstracts(2016) pp.1-</host></reference><reference label="[92]"><authors>J. Hoffmann</authors><title>Analyzing search topology without running any search: on the connection between causal graphs and h+</title><host>J. Artif. Intell. Res.41 (2011) pp.155-229</host></reference><reference label="[93]"><authors>P. Laborie,M. Ghallab</authors><title>Planning with sharable resource constraints</title><host>S. MellishProceedings of the 14th International Joint Conference on Artificial IntelligenceIJCAI'95(1995)Morgan KaufmannMontreal, Canada pp.1643-1649</host></reference><reference label="[94]"><authors>J. Koehler</authors><title>Planning under resource constraints</title><host>H. PradeProceedings of the 13th European Conference on Artificial IntelligenceECAI'98(1998)WileyBrighton, UK pp.489-493</host></reference></references><footnote><note-para label="1">Some attention has been given to (a) in richer planning frameworks, in the aforementioned work by Kolobov et al. [19], as well as in work on the PRP system [37], [38]. Both rely on expensive sub-procedures though, solving non-deterministic or classical planning problems as part of the dead-end detection.</note-para><note-para label="2">Refinement methods adding new conjunctions into C were previously designed for the purpose of goal distance estimation via partial delete relaxation [66], [69], [67]. These methods can, in principle, be used in our context as well. Yet, as we will show, they are not well suited for dead-end detection in practice. Our new refinement methods, geared to that purpose, are typically superior.</note-para><note-para label="3">The latter would happen here anyway as {a mathematical formula}s1 has no open children, which furthermore (given the transition from {a mathematical formula}s3 back to its parent {a mathematical formula}s1) was necessary to identify the conflict at {a mathematical formula}s3. For an example with non-trivial backjumping, say we have packages {a mathematical formula}p1,…,pn all initially at {a mathematical formula}l1 and with goal {a mathematical formula}l3, and one can unload a package only at its goal location. Then our method expands a single sequence of loading actions below {a mathematical formula}s1, learns the same conjunction {a mathematical formula}c={t(l2),f(1)} at the bottom, and backjumps all the way to {a mathematical formula}I. Similar situations can be constructed for non-symmetric packages.</note-para><note-para label="4">Alternatively, one can cache the {a mathematical formula}uC neighbors information during search prior to the refinement on {a mathematical formula}Sˆ. But that turns out to be detrimental. Intuitively, as new conjunctions are continually added to C, the cached {a mathematical formula}uC information is “outdated”. Using up-to-date C yields more effective learning.</note-para></footnote></root>