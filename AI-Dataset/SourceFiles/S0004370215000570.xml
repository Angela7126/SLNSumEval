<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370215000570</url><title>Robotic manipulation of multiple objects as a POMDP</title><authors>Joni Pajarinen,Ville Kyrki</authors><abstract>This paper investigates manipulation of multiple unknown objects in a crowded environment. Because of incomplete knowledge due to unknown objects and occlusions in visual observations, object observations are imperfect and action success is uncertain, making planning challenging. We model the problem as a partially observable Markov decision process (POMDP), which allows a general reward based optimization objective and takes uncertainty in temporal evolution and partial observations into account. In addition to occlusion dependent observation and action success probabilities, our POMDP model also automatically adapts object specific action success probabilities. To cope with the changing system dynamics and performance constraints, we present a new online POMDP method based on particle filtering that produces compact policies. The approach is validated both in simulation and in physical experiments in a scenario of moving dirty dishes into a dishwasher. The results indicate that: 1) a greedy heuristic manipulation approach is not sufficient, multi-object manipulation requires multi-step POMDP planning, and 2) on-line planning is beneficial since it allows the adaptation of the system dynamics model based on actual experience.</abstract><keywords>POMDP;Planning under uncertainty;Task planning;Manipulation;Unknown objects;Multiple objects;Cluttered environment</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>For a service robot, physical interaction with its environment is an essential capability. As the application areas of service robotics are extending to complex unstructured environments, robotic manipulation has become an important focus area within robotics research.</paragraph><paragraph>In complex environments, the robot's knowledge about its environment is incomplete and uncertain. To operate in such environments, robots can employ different mechanisms. First, a robot may use on-line sensing to attempt to gain more information about the environment. Second, sensory measurements can be used directly in a feedback control loop to adapt to small disturbances. Third, the uncertainty can be taken into account on the level of planning the actions.</paragraph><paragraph>Planning under uncertainty with imperfect sensing can be modeled as a partially observable Markov decision process (POMDP). While POMDPs have been one of the actively pursued research directions within AI, they have not been applied very widely in robotic manipulation. This is partly because the manipulation planning problems have intrinsic characteristics such as continuous state spaces which make the application of POMDP solvers less straightforward, and partly because the manipulation planning approaches have only recently advanced to the point where the explicit modeling of uncertainty becomes tractable. The question which robotic manipulation problems benefit from the explicit modeling of uncertainty in a POMDP framework remains open to a large extent.</paragraph><paragraph>In this paper, we consider a multi-object manipulation planning problem with the environment state estimated by imperfect sensors. The dynamics of the system are considered to be partly unknown, which supports the goal of long term autonomy of the robotic system. Our high-level research question is in which situations explicit planning under uncertainty is beneficial. The manipulation planning problem is considered on task level, that is, the desired result of planning is the best action to be performed. While motion planning for an individual action, such as grasping an object, is out-of-the-scope for this paper, the relationship of such an individual action to observations of the overall system state and the world model dynamics are modeled and learned during operation.</paragraph><paragraph>The theoretical contributions of the paper are two-fold: first, a POMDP model for multi-object manipulation is proposed. As a particular novel contribution, the model considers the effect of visual occlusions on observations and success of actions. Moreover, the dynamics of the world model, in particular related to the success of actions, are updated during operation as more information is gained. Second, we propose a new POMDP method which is applicable to manipulation planning. In particular, it does not require a discrete world model but instead samples the world model to construct policies. The method is also scalable, does not require heuristics, can handle uncertainty in the world model, and allows online planning, which is important when the world model is not accurate. Furthermore, the method produces compact policies in a predefined time. This can be beneficial in a robotic setting where easy to inspect policies may give new insights into the problem.</paragraph><paragraph>The proposed approaches are experimentally evaluated both in simulation and with a real robot. The experiments demonstrate that multi-step, longer horizon planning is beneficial in complex environments with clutter. In particular, POMDPs are beneficial if a particular problem has some of the following characteristics: 1) the problem requires weighting the value of information gathering versus collecting immediate rewards such as lifting objects to get a better view on other objects, 2) the world model is uncertain and thus it should be updated, for example when some objects are harder to grasp than others, or 3) the sequence of actions matters such as when objects occlude each other even partially. Altogether, the paper is the first to propose long term POMDP planning for manipulating many objects in a high dimensional, unknown, and cluttered environment.</paragraph></section><section label="2"><section-title>Related work</section-title><section label="2.1"><section-title>Partially observable Markov decision processes</section-title><paragraph>A partially observable Markov decision process (POMDP) [1] defines the optimal policy for a sequential decision making problem while taking into account uncertain state transitions and partially observable states. This makes POMDPs applicable to diverse application domains such as robotics [2], elder care [3], tiger conservation [4], and wireless networking [5]. However, versatility comes with a price, the computational complexity of finite-horizon POMDPs is PSPACE-complete [6] in the worst case.</paragraph><paragraph>Because of the high computational complexity, state-of-the-art POMDP methods [7], [8], [9], [10], [11] use different kinds of approximations. There are at least two causes for the intractability of POMDPs: 1) state space size, and 2) policy size. State-of-the-art POMDP methods yield good policies even for POMDP problems with hundreds of thousands of states [7], [8] by trying to limit policy search to state space parts that are reachable and relevant for finding good policies. However, in complex real-world problems the state space can be still much larger. In POMDPs with discrete variables, the state space size grows exponentially w.r.t the number of state variables. In order to make POMDPs with large state spaces tractable, there are a few approaches: compressing probability distributions into a lower dimension [12], using factored probability distributions [13], [14], or using particle filtering to represent probability distributions [9], [10]. Particle filtering is particularly attractive, because an explicit probability model of the problem is not needed. In fact, in order to cope with a complex state space, we use particle filtering in the online POMDP method presented in more detail in Section 3.2.</paragraph><paragraph>Assuming the problem of state space size solved, the problem of policy size still remains. In the worst case, the size of a POMDP policy grows exponentially with the planning horizon. Offline POMDP methods [11] take advantage of the piecewise linear convexity (PWLC) of the POMDP value function to keep the size of the policy reasonable. Some offline POMDP methods use fixed size policies. A common approach is to use a fixed size (stochastic) finite state controller [15], [16] as a policy. The monotonic policy graph improvement method [17] utilizes a fixed size policy graph, an idea which is also adopted here.</paragraph><paragraph>Contrary to offline approaches, online POMDP methods [18] compute a new policy at each time step. Online planning starts from the current belief and can thus concentrate on only the part of the search space that is currently reachable. Moreover, restarting planning from the current belief allows the online planner to correct planning “mistakes”, which an inaccurate world model caused in earlier time steps. This is especially relevant in robotic manipulation in service settings where an accurate world model is difficult to estimate for example due to unknown objects. Online POMDP methods usually represent the policy as a policy tree. Techniques such as pruning can be used in order to reduce the size of the policy tree, but this does not solve the problem of exponential growth of the policy tree w.r.t. the planning horizon. The online POMCP method of Silver et al. [9] uses particle filtering to address the state space size problem and Monte-Carlo tree search to explore the policy space. However, for best results, POMCP requires a problem specific heuristic [9]. POMCP is also not designed to produce compact policies. The online POMDP approach that we propose allows for long planning horizons by using a compact, fixed-size policy graph [17]. Because of the compact policy size, the policy can be inspected by a domain expert.</paragraph><paragraph>It is useful to notice the relationship of on-line POMDP solvers to receding horizon control (RHC) [19], [20], a popular feedback control technique. With RHC, a control problem is formulated as optimization over a fixed-length time horizon and the solution to the optimization provides a plan. At each time step the optimization problem is solved anew and the first step of the resulting plan is applied. The idea of fixed-length receding horizon has been applied also to POMDPs, e.g. in [13]. However, there is one significant difference between RHC and POMDP solvers: In RHC the controls are typically optimized directly. In contrast, our approach as a POMDP solver optimizes a feedback policy for the moving horizon.</paragraph><paragraph>In manipulation, and more generally in robotics, the world model is often uncertain and thus it is necessary to learn the world model during online operation. The goal then is to maximize total reward while taking into account that the current world model is not accurate and that actions can yield information about the world model. For a discrete POMDP, a natural Bayesian approach is to model transition and observation probabilities with Dirichlet distributions [21], [22], [23]. Following this, our probability model uses Beta distributions to model uncertainty in object specific grasp probabilities. Few papers [24], [25] exist on using a POMDP model with uncertain probabilities in robotics. Ross et al. [24] apply an online POMDP approach to simulated robot navigation with Gaussian distributions with unknown parameters. Bai et al. [25] present an offline POMDP planning approach for robot motion planning with unknown model parameters and validate their approach in simulation. However, we are not aware of prior robotic manipulation research that uses a POMDP model with uncertain probabilities.</paragraph></section><section label="2.2"><section-title>Manipulation under uncertainty</section-title><paragraph>Manipulation planning under uncertainty is not a new problem to be considered in robotics. Already in the early 1980's, Lozano-Péres et al. [26] considered the automatic synthesis of fine-motion actions under initial robot pose uncertainty using compliant motion, preimages and backward chaining. The originally sensorless decision-theoretic line of research can be seen to continue to this day with extensions over the years to e.g. grasping [27] and a probabilistic setting [28]. A good summary of the current state in this line of work can be found in [29].</paragraph><paragraph>There is a recent trend to integrate task and motion planning, see [30] for an overview. However, these approaches do not address directly the problem of uncertainty. The only exception is the recent work by Kaelbling and Lozano-Péres [31], where preimage backchaining is used for belief-space planning in a hierarchical framework. The approach handles probabilistic uncertainty by using a deterministic approximation of the domain and replanning after each time step. Our work differs from this approach: we consider the interactions of the manipulation actions, that often occur in multi-object manipulation, as well as update the world model based on on-line experience.</paragraph><paragraph>There is a tradition to formulate robot navigation problems as POMDPs, for an overview see [32], or a recent study for long time horizon POMDP planning [33]. In manipulation, the use of the formulation is not common. Hsiao et al. [34] proposed the partitioning of the configuration space of grasping with one uncertain degree of freedom to yield a discrete POMDP which can be solved for an optimal policy. In grasp planning, the state-of-the-art includes probabilistic approaches with a short time horizon. The goal can be formulated either as positioning the robot accurately as in [35] or maximizing the probability of a successful grasp as in [36], [37]. The short-term planning can also be extended to include information gathering actions [38]. In contrast to the above, this paper considers manipulation of multiple objects which are unknown and where the sequence of actions has a significant effect.</paragraph><paragraph>Recent work by Dogar and Srinivasa [39] proposes manipulation of multiple objects using grasping and pushing primitives. The approach uses pushing to collapse the uncertainties of the object locations as well as to clear clutter in the scene. The planning is performed at the level of object poses. Monso et al. [40] proposed to formulate clothes separation as a POMDP. In contrast to our work, the approach of Monso et al. is environment specific. Monso et al. rely on a clothes separation specific state space definition, which models the number of clothes in each area. We model object attributes, associated probabilities, and grasp probabilities, in any kind of environment.</paragraph></section></section><section label="3"><section-title>Multi-object manipulation: a POMDP</section-title><paragraph>In multi-object manipulation, a robot performs actions on several objects. In particular, the robot may grasp objects, move them, or use them in another way to accomplish some predefined goals. In this paper, we focus on the problem of deciding how to manipulate unknown objects in a crowded environment. Because the environment is crowded, only parts of the objects can be observed by visual sensors. In addition to uncertain observations, real-world manipulation problems have uncertain action consequences, especially when the robot does not have a model of the objects beforehand, or when the robot does not observe the objects well. For example grasping or moving an object may fail, because the shape or location of the object differs from the observed one. Real-world problems often have several (possibly conflicting) goals. As a practical example consider putting dirty dishes from a table full of dishes into a dishwasher: the goal is to maximize the number of dirty dishes in the dishwasher, minimize the number of clean dishes in the dishwasher, and minimize the execution time. In order to address the issue of complex objectives, uncertain observations, and uncertain action effects, this paper models the problem of manipulating multiple unknown objects as a partially observable Markov decision process (POMDP).</paragraph><paragraph>Planning of manipulation, for instance grasp planning, is traditionally considered as a geometrical problem. However, in unstructured environments with unknown objects the current state-of-the-art approaches often plan individual actions (e.g., a grasp) directly based on the observed environment [41]. We follow the same idea so that the planning is performed on the level of semantic actions and locations, while the execution of individual actions is then performed based on the currently observed scene. However, our approach also models the interplay of the immediate sensor measurements to both observation and system models. For example, the rate of visual occlusion modulates the probability of correct observations and successful completion of actions.</paragraph><paragraph>We begin by defining a POMDP, then describe a new online POMDP planning method which is suitable for complex problems such as multi-object manipulation, and finally describe how to model multi-object manipulation in crowded environments as a POMDP with an application of moving dirty dishes into a dishwasher.</paragraph><section label="3.1"><section-title>What is a POMDP?</section-title><paragraph>A POMDP is a model that defines optimal behavior for a given Markovian problem, taking into account uncertainty in observations as well as action effects over a potentially long time horizon. In a POMDP, a set of hidden Markov models, one for each action choice, describes the temporal dynamics of the problem and the optimization objective is defined by assigning a reward to each action in each possible situation. In a specific application, rewards should reflect real value, e.g. monetary cost.</paragraph><paragraph>Formally a POMDP is defined by the tuple {a mathematical formula}〈S,A,O,P,R,O,b0〉, where {a mathematical formula}S is the set of states, {a mathematical formula}A is the set of actions, and {a mathematical formula}O is the set of observations. The state set includes all possible states of the world, in which the agent is assumed to operate in. {a mathematical formula}P(s′|s,a) is the transition probability to move from state s to the next time step state {a mathematical formula}s′, when action a is executed. {a mathematical formula}R(s,a) yields the real-valued reward for executing action a in state s and O denotes the observation probabilities {a mathematical formula}P(o|s′,a), where o is the observation made by the agent, when action a was executed and the world moved to the state {a mathematical formula}s′. Lastly, {a mathematical formula}b0(s) is the initial state probability distribution, also known as the initial belief. In a finite-horizon POMDP, the goal is to optimize the expected reward{a mathematical formula} where T is the horizon, {a mathematical formula}s(t) is the state, and {a mathematical formula}a(t) the action chosen at time step t by the policy π.</paragraph><paragraph>Because the states are not fully observable, the current state cannot be used for decision making as in fully observed models. Instead, the belief{a mathematical formula}b(s), a probability distribution over world states, is maintained to make (optimal) decisions at each time step. Starting from the initial belief {a mathematical formula}b0(s), the belief is updated at each time step. After performing action a and observing o the updated belief {a mathematical formula}b′=b′(s′|b,a,o) can be obtained from the current belief {a mathematical formula}b=b(s) using the Bayes formula {a mathematical formula}b′(s′|b,a,o)=P(o|s′,a)C∑sP(s′|s,a)b(s), where C is a normalizing constant.</paragraph><paragraph>To give an intuitive idea of how POMDPs can be applied in practice, we will now give short examples for the transition and observation probabilities, and the reward function. In a POMDP, the transition probability {a mathematical formula}P(s′|s,a) models the uncertainty in action effects: what is the probability to move a cup successfully from a table (part of state s) into a dishwasher (part of {a mathematical formula}s′), when the action a is “move cup into dishwasher”? The observation probability {a mathematical formula}P(o|s′,a) models the uncertainty in observations: what is the probability of observing a cup as dirty (observation o), when it is dirty (part of state {a mathematical formula}s′) and we are executing action a “look at cup”? Finally, the reward {a mathematical formula}R(s,a) explicitly specifies the optimization goal: gain positive reward for moving (action a) a dirty cup (part of state s) into the dishwasher.</paragraph></section><section label="3.2"><section-title>Online policy graph POMDP using monotonic value improvement</section-title><paragraph>In this paper, as often in robotic applications, the state space of the robotic manipulation task is high dimensional. The state space has exponential size in the number of discrete state variables, and includes uncertain grasp success probability distributions. Because of the complex state space, POMDP methods based on exact probability representations are not applicable. We present a new online POMDP method based on the monotonic policy value improvement algorithm [17] proposed by us earlier. The next subsection briefly introduces the method from [17] followed by the extensions: 1) the new method uses particle filtering to represent probability distributions and estimate values in a way that takes advantage of the policy graph (Section 3.2.2), instead of using a discrete tabular probability distribution representation [17], and 2) the new method is transformed from an offline [17] method into an online method in a policy graph specific way (Section 3.2.3).</paragraph><section label="3.2.1"><section-title>POMDP policy and method</section-title><paragraph>We represent the policy of the agent (the robot) as a policy graph G (see Fig. 1 for an example policy graph) beginning from time step {a mathematical formula}t=0 and ending at the planning horizon {a mathematical formula}t=T−1. The policy graph consists of layers of graph nodes, one layer for each time step. Each graph node defines a conditional plan for the robot to follow: which action to perform, and depending on the observation made, to which next layer node to transition next.</paragraph><paragraph>The policy improvement approach in [17] uses dynamic programming to improve each policy graph layer at a time. First, the approach computes the belief {a mathematical formula}bt(s,q) at each layer t and each graph node q starting from the initial belief {a mathematical formula}b0(s) in the first layer. Then, starting from the last layer and moving one layer at a time towards the first layer, the approach computes for each node in the layer a new policy (action, observation edges), which maximizes the expected reward for the belief at the current node. The expected reward is computed from the immediate reward and the next layer value function {a mathematical formula}Vt+1(s,q), which yields the expected reward when starting from state s and graph node q in layer {a mathematical formula}t+1 and following the policy graph until layer {a mathematical formula}T−1. After optimizing the policy for a policy graph node: If the new policy at the node is identical to the policy of another already optimized node in the same layer, we sample a new belief and re-optimize the policy at the node for the sampled new belief. Because having multiple graph nodes at the same layer with the same policy does not improve the value of the policy graph, re-optimizing such “redundant nodes” does not decrease the value of the policy graph. The whole policy graph improvement procedure guarantees monotonic improvement of policy value. For algorithmic details, see [17].</paragraph><paragraph>In order to keep computations tractable, we use a policy graph with fixed width and depth. This circumvents the problem of exponential growth of a search tree, allows for manual inspection of a compact policy, and enables us to convert the offline approach to an online one.</paragraph></section><section label="3.2.2"><section-title>Particle filtering</section-title><paragraph>The method in [17] assumes a discrete “flat” POMDP. In order to deal with a large state space, we use particle filtering to approximate beliefs and for estimating values.</paragraph><paragraph>Belief representation and update. We represent a belief {a mathematical formula}b(s), a probability distribution over s, as a finite set of particles [32], that is, a weighted set of state instances {a mathematical formula}sj. The belief is {a mathematical formula}b(s)=∑jwjδ(s,sj);∑jwj=1;0≤wj≤1, where {a mathematical formula}wj is the particle weight and {a mathematical formula}δ(s,sj)=1 when {a mathematical formula}s=sj and zero otherwise. What a state actually is depends on the application: Section 3.3 defines a state for multi-object manipulation.</paragraph><paragraph>We use two kinds of belief updates. The first one is the commonly used update of the current belief {a mathematical formula}b(s), when an action has been executed and an observation made. This belief update is used in initializing the policy graph and for sampling new beliefs for redundant policy graph nodes (please, see Section 3.2.1 on redundant nodes). The second kind of belief update projects the belief {a mathematical formula}bt(s,q) over world states and policy graph nodes to the next layer belief {a mathematical formula}bt+1(s,q), using the current policy. The second belief update is used in each improvement round. We will next discuss the belief updates in more detail.</paragraph><paragraph>In the first belief update, the action {a mathematical formula}a(t) and observation {a mathematical formula}o(t+1) are given. We sample a next time step state {a mathematical formula}sj(t+1) for each current state {a mathematical formula}sj(t) according to the application specific dynamics (state transition) model {a mathematical formula}P(sj(t+1)|sj(t),a(t)). We then compute the new particle weight {a mathematical formula}wj(t+1) as the product of the old weight and the observation probability: {a mathematical formula}wj(t+1)=wj(t)P(o(t+1)|sj(t+1),a(t)). As usual, to prevent particle impoverishment, we resample particles, when the effective sample size drops below a threshold (0.1 in the experiments).</paragraph><paragraph>In the second belief update, for updating the belief {a mathematical formula}bt(s,q), a particle consists of a weight {a mathematical formula}wj(t) and a state/node pair {a mathematical formula}(sj(t),qj(t)). To sample a new particle {a mathematical formula}(sj(t+1),qj(t+1)) using an {a mathematical formula}(sj(t),qj(t)) pair, we first get the action {a mathematical formula}a(t) for node {a mathematical formula}qj(t). Then, we sample a new state {a mathematical formula}sj(t+1) from {a mathematical formula}P(sj(t+1)|,a(t)). Next, we sample an observation {a mathematical formula}o(t+1) from {a mathematical formula}P(o(t+1)|sj(t+1),a(t)). Finally, the observation edge for observation {a mathematical formula}o(t+1) of the graph node {a mathematical formula}qj(t) yields the new graph node {a mathematical formula}qj(t+1). In this update, the particle weights do not change.</paragraph><paragraph>As a side remark, note that our approach differs from existing particle filtering based approaches. In order to improve the policy, we use the current policy for finding a belief distribution over graph nodes, but other state-of-the-art POMDP methods based on particle filtering [9], [10] select an action and observation to find a new belief for which to compute a policy. In other words, other POMDP methods use a constant amount of particles to represent a single belief, but we use a constant amount of particles to represent the belief over a policy graph layer (a time step) and each graph node is assigned particles proportional to the probability of the graph node.</paragraph><paragraph>Value estimation. In order to determine the best action and observation edges for a policy graph node, the method has to estimate the value for each action–observation-next node triplet. From these triplets the method can then select for each action the highest value observation-next node pairs and based on these select the highest value action. To do this efficiently, we follow Algorithm 1 in [10]. The algorithm samples state transitions and observations for each action and for the sampled observation simulates the value for each next controller node. Bai et al. [10] represent the policy as a possibly cyclic finite state controller, but we use instead an acyclic policy graph. However, no significant modifications are necessary because the algorithm is based on simulation. Furthermore, the bound for the approximation error induced by sampling, shown in Theorem 1 in [10], also applies here: the error is bounded by a term that decreases at the rate of {a mathematical formula}O(1/N), where N is the number of samples.</paragraph><paragraph>In the implementation we do not actually sample states from a belief, but just go through all particles, one at a time, and utilize the particle weight for value estimation. When particles have identical weights, going through particles is more efficient than sampling them. Particles often have identical weights during policy improvement.</paragraph><paragraph>Complexity. In one improvement round, the method improves the policy at each policy graph node by going through policy graph layers from the last layer to the first. To improve the policy at a policy graph node the method uses the value estimation technique described above. Value estimation simulates state trajectories from a policy graph node until the end of the policy graph taking linear time w.r.t. the planning horizon. That is, the method improves a linear number of policy graph layers and at each policy graph layer propagates state samples for a linear number of times w.r.t. the planning horizon. Therefore, the worst case complexity of one policy improvement round of the POMDP method is quadratic w.r.t. the planning horizon. In the experiments in Section 4, the method performed well. In the future, one could parallelize the algorithm to utilize multiple CPU cores (easily because of the particle representation of probabilities), or use a fixed sampling depth.</paragraph></section><section label="3.2.3"><section-title>From offline to online</section-title><paragraph>Because of the computational and modeling restrictions discussed previously, we transform the offline POMDP method into an online one. Similarly to the receding horizon control (RHC) approach [20], [42] in automatic control we re-plan at each time step up to a finite horizon. Intuitively, we use a moving window that at each time step shifts one step to the right over the policy graph (imagine this with the help of the policy graph in Fig. 1), discards the first layer, and adds a new layer at the end. At the beginning, the agent optimizes the policy graph for several improvement rounds for the initial belief. Then in following time steps the agent estimates the new belief and constructs a new policy for the belief, as follows: 1) initialize the new policy graph with the layers {a mathematical formula}2,…,T−1 of the previous policy graph; 2) add a new last layer to the policy graph with random actions, and add random observation edges to the layer preceeding the last layer; 3) use the regular policy graph improvement method on the new policy graph. The basic idea here is to initialize the current policy graph using the policy graph of the previous time step, and then optimize the policy graph for the current belief. Because of the initialization, the required number of improvement rounds during online operation is then less when compared to offline optimization.</paragraph></section></section><section label="3.3"><section-title>Multi-object manipulation as a POMDP</section-title><paragraph>We discuss now a general POMDP framework for modeling multi-object manipulation. Later, in Section 3.3.1, we then show how the POMDP framework can be applied to the problem of moving dirty dishes into a dishwasher.</paragraph><paragraph>In multi-object manipulation, the robot has to decide at each time instance which object to manipulate. We consider problems, where the world consists of N objects with varying attributes. The total number of actions is {a mathematical formula}∑i|Ai|, where {a mathematical formula}|Ai| denotes the number of possible actions for object i. In each time step, the action of the robot changes the spatial locations and poses of the objects, and the robot makes an observation about the changed state of the world. Our POMDP model uses discrete actions and observations. However, instead of forcing the robotic planning problem into a manageable discrete state space as is done e.g. in [40], we use a POMDP method based on particle filtering (discussed in Section 3.2) that allows us to maintain complex object information required for efficient multi-object manipulation.</paragraph><paragraph>State space and actions. The state space consists of semantic object locations (e.g. “on table”, “in a dishwasher”), object attributes, and historical data of observations and action successes for each object. The model assumes that the semantic location of an object is constant over time unless a manipulation action successfully changes it. However, because an online planning approach is used, the planning always restarts from the current belief taking into account the most recent measurements.</paragraph><paragraph>Formally, the POMDP state {a mathematical formula}s=(s1,s2,…,sN) is a combination of object states {a mathematical formula}si=(siloc,siattr,sihist) where {a mathematical formula}siloc is the semantic object location, {a mathematical formula}siattr the object attributes, and {a mathematical formula}sihist compressed historical information of action successes and object attribute observations. The action success information consists of a count of succeeded {a mathematical formula}nisucc and failed {a mathematical formula}nifail grasps for each object. Because of the finite number of objects the number of action counts is finite. Similarly, as discussed in more detail below, the number of different object attribute observations is finite. Therefore, {a mathematical formula}sihist has finite dimensionality, and the POMDP state can be stored and operated on efficiently. Note that the POMDP states have the Markov property because the probability for the next state depends only on the current state (and action).</paragraph><paragraph>The observation history contains information of past observations of object attributes. Past object attribute observations can be used to compute the probability distribution over an object's attributes. Additionally, these are needed during planning because future observations of the attributes cannot be assumed statistically independent, because the main source of observation uncertainty is occlusion. In contrast, unless the occlusion changes, we assume that an identical observation of the attribute is made (note that we assume differently occluded observations independent). We assume that the probability of making the correct observation depends on how occluded the object is (we discuss this in more detail shortly). In more detail, {a mathematical formula}sihist contains the observation made in each occlusion setting. For example, in the experiments objects can be temporarily lifted: in addition to the current occlusion setting, we store the observation for each object which was temporarily lifted and which is otherwise in front of the observed object. Note that because of the finite number of objects the number of occlusion settings is finite, and thus the observation history has finite size.</paragraph><paragraph>Occlusion ratio. The action success probability and the observation probability of an object depend on how occluded the object is. Because we do not have models for the objects, the occlusion is modeled using a model free occlusion ratio. The reasoning is that the higher the occlusion, the smaller the probability of success in actions or observations. In the experiments, we capture a point cloud, segment the point cloud into objects, compute edges for all objects using 2-D information, and then find out how much the edges of objects touch each other. The right hand side figure in Fig. 2 shows edges found for segmented objects in a scene. When the edge of object A, which is closer to the visual sensor, touches the edge of object B, object A occludes object B.</paragraph><paragraph>Consider computing the occlusion ratio for object B. Denote with TOT the perimeter of the 2D contour of object B, that is, the total number of 2D pixels for which the number of neighboring 2D pixels, which are part of object B, is less than eight. Denote with TOU the touching edge between A and B, that is, the number of 2D pixels in B which have atleast one neighboring 2D pixel in object A (when B is occluded by several objects, just use the 2D pixels of the occluding objects). The occlusion ratio for object B is 1, when TOT subtracted by TOU is smaller than TOU, 0 when {a mathematical formula}TOU=0, and otherwise {a mathematical formula}TOU/(TOT−TOU). The reasoning is that when an object almost completely occludes another object, TOT is roughly double TOU. Thus an occlusion ratio of 1 corresponds to totally occluded and an occlusion ratio of 0 to no occlusion at all. The convenience variable {a mathematical formula}sioccl denotes the occlusion ratio of object i.</paragraph><paragraph>In this paper, POMDP state transitions are based on sampling. When an object A is sampled to be moved, so that it does not occlude another object B anymore, it is straightforward to update the occlusion ratio of B by removing the touching edge between A and B. However, if there is an object C, which occludes A (edges of A and C touch), but not B, and A is moved away, then there is a possibility that C could occlude B after the removal of A. We call this occlusion inheritance. For simplicity, we do not take occlusion inheritance into account in the experiments and leave it as future work.</paragraph><paragraph>Grasp probability. We assume that occlusion affects the grasp probability of all objects in a similar way, but, in addition, we assume that each object has unknown properties that affect the grasp probability of that specific object: we do not know beforehand what kind of grasp properties each object has. For example a cup that has fallen down may be harder to grasp, than another cup, which is standing upright (see Fig. 8b for an example). The probability of a successful grasp is modeled as{a mathematical formula} where {a mathematical formula}pisucc prior is the occlusion ratio specific grasp success prior probability and {a mathematical formula}nprior is the strength of the prior. In the experiments, we mapped the occlusion ratio to the grasp success prior probability {a mathematical formula}pisucc prior using a simple exponential function{a mathematical formula} where {a mathematical formula}θG1 and {a mathematical formula}θG2 are parameters that can be experimentally estimated from object grasps, for example, using two different occlusion ratios.</paragraph><paragraph>Note that we model the grasp probability as the mean of the Beta distributed random variable {a mathematical formula}pisucc. It would be possible to use a more complex model during planning, in which one would sample the grasp probability from the Beta distribution, but we expect this would increase the number of particles needed for planning.</paragraph><paragraph>Observations. We assume that the semantic locations and dependencies (which cup is in front of which cup) are fully observed and that grasp success is also fully observed. At each time step the agent observes whether the grasp succeeded and makes an observation about object attributes. Using these observations, we can compute a probability distribution over object attributes, which is needed for sampling the initial POMDP belief and for displaying attribute probabilities. Note that if grasp success or semantic locations are not fully observed, then we cannot estimate the initial POMDP belief directly using grasp success and object attribute observations. Instead, we could update at each time step an (approximate) belief according to the current action and observation and use that as the initial POMDP belief. However, in many applications, including the dishwasher application further down, semantic locations such as “object on table”, “object in dishwasher”, and thus also grasp success, are fully observed. Specifically, in the experiments, we use visual sensory input. To determine whether an object has been successfully grasped, we compare the image before and after the grasping action. If we do not detect the object at the same location anymore, then we assume the grasp succeeded. One could also use a tactile sensor for detecting grasp success.</paragraph><paragraph>As discussed earlier, we assume that the robot observes an object identically unless the occlusion changes. Denote with {a mathematical formula}oij, the observation for object i when in the jth occlusion setting, and with {a mathematical formula}aij the action performed when observing {a mathematical formula}oij, then the attribute probability given the history is{a mathematical formula} where we assumed that observations are conditionally independent given the object attributes, but if needed and computationally possible one can use joint probabilities. We assume that attributes (e.g. color) do not change over time, and thus actions do not influence object attributes: {a mathematical formula}P(siattr|ai1,…,aiM)=P(siattr). In the experiments, we assumed {a mathematical formula}P(siattr) is uniform.</paragraph><section label="3.3.1"><section-title>Dirty cups into dishwasher</section-title><paragraph>We now demonstrate how the framework can be used to model the problem of moving dirty cups from a table into a dishwasher as a POMDP (another realistic application could be moving dishwasher-safe cups, instead of dirty cups, into the dishwasher). In this problem, the robot can gain more information of attributes by removing occlusions and gain information about the object specific grasp probability through successful and failed grasps.</paragraph><paragraph>State space and actions. In addition to the grasping and observation history discussed in Section 3.3, the world state consists of the semantic location {a mathematical formula}siloc={table, dishwasher}, and the attributes {a mathematical formula}siattr of an object include dirtyness {a mathematical formula}sidirty={clean, dirty}. The robot can perform three kinds of actions. The FINISH action terminates the robot actions and assigns a negative reward to dirty dishes remaining on the table. The LIFT action tries to lift an object to expose the objects behind it and allows the agent to gather more information about the occluded objects. A small negative reward representing time cost is associated with the action. Note that the action takes less time than moving the object into the dishwasher. The WASH action tries to move an object into the dishwasher (in the experiments, a box). If the move succeeds, the state of the object changes from table to dishwasher. If the move succeeds and the moved object is dirty, then a large reward is obtained. If the move succeeds and the object is clean, a large negative reward is obtained. Failed grasps cause a small negative reward accounting for the time cost. Note that when implementing the model, we can compute the reward for the WASH action as the expected next time step reward using the grasp success probability, instead of deferring reward computation until the grasp has happened in the next time step.</paragraph><paragraph>Observations. At each time step the agent observes whether the grasp succeeded and the dirtyness of the k nearest objects (in the experiments {a mathematical formula}k=2) which were occluded by the moved cup. In total, {a mathematical formula}2k+1 possible observations. In the experiments, we model the conditional probability of observing cup i as dirty when it is dirty with{a mathematical formula} where {a mathematical formula}θD1 and {a mathematical formula}θD2 are parameters that can be, similarly to the grasp probability, experimentally estimated from captured point clouds and object labels. The probability of observing a cup as dirty when it is clean is modeled identically with{a mathematical formula} where parameters {a mathematical formula}θC1 and {a mathematical formula}θC2 are also estimated in the same way.</paragraph></section></section></section><section label="4"><section-title>Experiments</section-title><paragraph>The experiments follow the scenario described above. The scene is observed by an RGB-D sensor (Microsoft Kinect) and a 6-DOF Kinova Jaco arm with an integrated 3-fingered hand is used to manipulate the objects. The objects belong to two classes: clean white cups and cups with green “dirt” representing dirty objects. Fig. 2 illustrates the experimental setup: to the left a picture of the setup, and to the right an image captured by the Kinect sensor.</paragraph><paragraph>Rewards. The robot receives a reward at each time step. The reward depends on the action executed and the current state of the world. As discussed in Section 3.3.1, the robot can execute three different kinds of actions. The FINISH action terminates the problem and accumulates a reward of −5 for each dirty cup on the table. Similarly, to limit experiment run times, after ten time steps, the problem is terminated and a reward of −5 for each dirty on the table given. The LIFT action lifts an object up and yields a reward of −0.5 for both failed and succeeded grasps. The WASH action moves an object into the dishwasher. If the move succeeds, then the reward is +5 for a dirty object and −10 for a clean object. For a failed move the reward is −0.5. In our dishwasher application, there was no well determined objective. Rewards were designed based on the researchers' understanding of what the objective could be, for example, for a person employing the robot. In general, rewards should reflect the actual goal [43]. For example, if the robot owner sees equal value in a successfully washed dirty cup and skipping the waiting time on ten robot actions, then the owner could assign a reward of +5 to a washed dirty cup and a penalty of −0.5 to performing any action.</paragraph><paragraph>Methods. The POMDP planning method described in Section 3.2 is initialized by 10 offline policy improvement rounds. Then, at each time step 4 improvement rounds for the current belief are executed. To evaluate the benefit of planning under uncertainty, the POMDP approach is also compared against heuristic decision making: The heuristic manipulation method assumes that observations are accurate and deterministic. It tries to move the dirty cup that has the highest grasp success probability into the dishwasher. If no cup is observed dirty it performs the FINISH action. In the experiments, we used two versions of the heuristic method: one which updates grasp probabilities according to the grasp success history and another which does not remember any grasp history.</paragraph><paragraph>Point cloud into a world model. In the experiments, the visual sensor captures a point cloud, from which we extract objects, their color, and information on how they occlude each other. From these we estimate grasp and observation probabilities and use these probabilities to plan which action to perform. In more detail, first the Kinect sensor captures an RGB-D point cloud of the visual scene. Without using prior information we segment{sup:1} the point cloud into objects. From the 2D-image, we determine the edge of each object and how much it touches other objects' edges (see edges in the right hand side image of Fig. 2). Using the object edges, we compute, as discussed in Section 3.3, occlusion ratios. However, because of occlusion, segmentation may produce multiple objects for one complete object. Therefore, we merge objects that occlude each other and are close (occlusion ratio above 0.5 and centroid distance below 8cm) into one object and re-compute its occlusion ratio. Next, we compute object specific grasp (Eq. (2)) and observation probabilities (Eqs. (5) and (6)) using the occlusion ratios, observation history, and initially estimated parameters. We set the grasp prior count {a mathematical formula}nprior=0.5. Finally, we make an observation if an object is dirty or clean based on the distance of the object color to precomputed color prototypes.</paragraph><paragraph>Grasping. Grasping an unknown object is performed by executing a top grasp, closing fingers around the centroid of the point cloud of the object to grasp, similar to e.g. [44].</paragraph><paragraph>Estimating initial parameters. Before actual experimental runs, we estimated experimentally the parameters of grasping and observation probability functions defined in Eqs. (3), (5), and (6). To estimate grasp parameters we attempted to lift cups positioned on the table using the robot arm, both when the cups were occluded and when not, and estimated grasp parameters ({a mathematical formula}θG1=−0.904, {a mathematical formula}θG2=−0.087) from the recorded success rates. For the occluded case we used the average occlusion ratio. We estimated observation function parameters for dirty ({a mathematical formula}θD1=−0.895, {a mathematical formula}θD2=−0.087) and clean ({a mathematical formula}θC1=−0.193, {a mathematical formula}θC2=0.0) cups similarly, but instead of the lifting success rate, we used the observation success rate.</paragraph><section label="4.1"><section-title>Experiments with simulated dynamics</section-title><paragraph>In multi-object manipulation, robot actions may have far reaching consequences: lifting first cup A and then cup B, may increase the probability of cup C being observed dirty from low to high by exposing it more fully. The robot has to consider at each time step, whether the information gain from lifting a cup yields more reward in the long run than executing an action which may yield higher immediate reward. Of course, because of the uncertainty in actions and observations, the real decision making problem can be even more complicated than this simple example implies. Consequently, our hypothesis is that a heuristic greedy manipulation approach is not sufficient and that planning several time steps into the future is needed. In order to study this hypothesis, we experimentally compared heuristic manipulation and the proposed POMDP approach with different planning horizons. Note that even though we simulate world dynamics, we estimate the grasp and observation probabilities using the physical robot arm and real observed occlusions. Moreover, we estimate the occlusions and locations of objects from point clouds captured by the Kinect sensor.</paragraph><paragraph>In the simulated dynamics experiments, we used ten different captured point clouds shown in Fig. 3 as the starting point for simulations. In these experiments, we form a world model from the point cloud and then repeatedly sample an initial belief and simulate the system using the probability model for 10 time steps. To get an initial belief, we sample particles using the cup dirtyness probability, which depends on past observations and which is defined in Eq. (4) (dirtyness is an object attribute). For evaluation purposes we also sample hidden object specific grasp success probabilities. In more detail, we sample for object i the total amount of observed grasps {a mathematical formula}ni=nisucc+nifail from a Gamma probability distribution with shape 0.2 and scale 5.0, that is, a probability distribution where small {a mathematical formula}ni are common, but also large {a mathematical formula}ni are possible. We sample {a mathematical formula}nisucc from the uniform distribution between 0 and {a mathematical formula}ni, and keep {a mathematical formula}nisucc and {a mathematical formula}nifail constant during each simulation run. Note that the magnitude of {a mathematical formula}ni determines how much object specific grasp properties affect the grasp success probability compared to occlusion.</paragraph><section label="4.1.1"><section-title>Results</section-title><paragraph>Fig. 4 compares POMDP planning with different planning horizons, ranging from two to five, with the heuristic manipulation approach. The POMDP policy graph had a width of three. Fig. 4 shows the average total reward over 100 simulation runs for each of the ten different cup configurations shown in Fig. 3. Overall, POMDP planning achieves higher reward than the heuristic manipulation approach. Interestingly, the performance difference between the heuristic approach with and without grasp history is not significant. To study this further, we ran over 2000 simulation runs for the scene shown in the third image, upper row, in Fig. 3. In this scene dirty cups are in front and thus the heuristic approach can select between several cups to move. Not surprisingly, the approach utilizing grasp history performed better (with non-overlapping average reward confidence intervals; not shown in Fig. 3).</paragraph><paragraph>It is also interesting that a POMDP planning horizon of three works significantly better than a horizon of two. Intuitively, one could imagine that short conditional plans, such as “lift a cup, and then, if the cup behind the lifted cup is dirty, move it into the dishwasher”, would already perform very well. However, the results suggest that many problems require a complex policy to gain high reward. Fig. 5 shows a compact policy graph computed by the POMDP method for the first scene in Fig. 3. The policy illustrates information gathering through lifting cups, the effect of failed grasps, and complex conditional planning. In the policy graph, the agent lifts e.g. cups 8 and 12 (for reference, first RGB image in Fig. 3 shows cups 2, 4, 8, and 12) in order to gain information, and then when observing cups 4 or 2 as dirty, tries to move them into the dishwasher. In time step two, when the move of cup 4 into the dishwasher fails, the grasp probability of cup 4 decreases. In time step three, the agent tries to move cup 4 again. This highlights the important feature of principled uncertainty handling in POMDP planning. Even though grasping failed previously, the planner tries to move the same cup, because compared to the alternatives the grasp probability is still high enough.</paragraph><paragraph>We also tested different reward scenarios. Fig. 6 shows performance for the heuristic manipulation method and the POMDP method with a planning horizon of three for different reward choices. In the experiment, we varied the reward for lifting a cup/a failed grasp attempt and the reward for putting a clean object into the dishwasher. The POMDP method outperformed the heuristic method in each reward scenario. The reward for failed grasps/lifting a cup had a significant effect on the POMDP method's performance. One explanation is that when lifting cups becomes more expensive the benefit of planning over complex action–observation sequences decreases.</paragraph></section></section><section label="4.2"><section-title>Robot arm experiments</section-title><paragraph>In the previous section, we simulated world dynamics using a world model created from real robot grasps and point clouds captured by the visual sensor. In this section, we present experiments with a physical robot arm. In Section 4.2.1, we demonstrate crucial parts of our world model. In Section 4.2.2, we compare quantitatively the performance of the greedy heuristic approach with the proposed POMDP approach. In the demonstrations, we show the usefulness of information gathering actions, such as lifting cups, in occluded settings. Furthermore, we experimentally investigate when object specific adaptive grasp probabilities are required. In addition, we examine in which situations the heuristic manipulation approach suffices for efficient operation, and when instead more comprehensive POMDP based decision making is required. The quantitative experiments show that the POMDP based approach significantly outperforms the simple greedy approach and yield insights, for instance, on why online planning is beneficial. Overall, the experiments show that real world problems require a model that takes occlusion into account, that multi-object manipulation problems require multi-step POMDP planning, and that adaptive action success probabilities are necessary in many situations.</paragraph><paragraph>We performed robot arm experiments using the Kinova Jaco arm. In the robot arm experiments, the Kinect sensor observes the scene, a method decides which action to execute, and then the robot arm executes the action. At each time step we estimate a belief from the captured point cloud and add the observation history information to this belief, to get the current belief. The method under evaluation decides on an action using the current belief. In order to maintain a consistent observation history and for detecting when a grasp succeeded or failed, we match current objects to objects in the previous time step: if an object is less than 4cm from its last spatial position, we assume it is the same object. If an object exists at the same location after it was moved or lifted, we assume the grasp failed.</paragraph><section label="4.2.1"><section-title>Demonstrations</section-title><paragraph>We claim that in multi-object manipulation, the robot may need to perform information gathering actions when objects are occluded, or when the grasp success probabilities of objects differ. However, when objects are in plain sight and easy to grasp decision making is easier. In this case, the problem requires no multi-step planning, and the heuristic policy of moving all cups that appear dirty into the dishwasher is sufficient. To test this, and to test whether our observation and state space models are applicable in physical robot arm experiments (we tested the model also in several other robot arm experiments which are discussed below), we performed robotic manipulation in a setup with dirty cups which are not occluded. Fig. 7a shows how the heuristic manipulation approach successfully moves the dirty cups into the dishwasher in this setup.</paragraph><paragraph>To test our occlusion model, and to test whether occlusion requires more complex decision making, we performed an experiment where the dirtyness of a cup is not apparent because another cup partly occludes the view on the dirty cup. The experiment in Fig. 7b demonstrates how the heuristic manipulation approach does not consider information gathering, and thus fails in the task. On the other hand, multi-step POMDP planning takes into account that the dirty cup may in fact be dirty, even though the robot observes it as clean, because the robot makes wrong observations on occluded cups with a high probability. The POMDP approach lifts the clean cup, gains new information on the dirty cup, that is, observes the dirty cup as dirty, which increases the probability of the cup being actually dirty, and then successfully moves the dirty cup into the dishwasher.</paragraph><paragraph>Previously, we claimed that real world multi-object manipulation problems require an object specific adaptive grasp success probability. To test this claim and to verify that our adaptive grasp success model works, we performed an experiment with two dirty cups where the first cup is slightly occluded, and the second cup contains drinking straws that make correct grasping more difficult. The robot tries to move the second cup always first, because the occlusion on the first cup makes the initial grasp success of the second cup higher. For simplicity, we compared the heuristic manipulation approach with and without adaptive grasp success probabilities. As shown in Fig. 8a, both methods fail to grasp the second cup because of the drinking straws. The adaptive grasp success probability method updates the grasp success probability after observing a failed grasp, and moves the first dirty cup successfully into the dishwasher. The method that does not take grasp success history into account tries to grasp the same second cup again, even though an easier to grasp dirty cup would be available. These kind of situations occur often in practice. During experimentation with the robotic arm for example, as shown in Fig. 8b, the robot moves dirty cups, but when it moves the third dirty cup, the cup falls down and remains in a harder to grasp pose. We observed that when further grasps on the object failed, the grasp success probability decreased as expected.</paragraph></section><section label="4.2.2"><section-title>Quantitative results</section-title><paragraph>In addition to the demonstrations, we performed a quantitative comparison between the simple greedy heuristic approach and the proposed POMDP approach in physical robot arm experiments. Similar to the experiments with simulated dynamics in Section 4.1, the goal was to move dirty, that is, partly green objects, into a “dishwasher”. An object was observed dirty if the number of green pixels was at least 100.</paragraph><paragraph>Fig. 9 shows the four different scenes used. The fourth scene contains also toys to demonstrate the genericity of our approach. In each scene, we placed the objects on the table, and then ran the simple heuristic method and the POMDP method with a planning horizon of 3 after each other, five times each, yielding a total of twenty runs for each method over all four scenes. We reconstructed a scene after each run. Fig. 10 shows the results. Overall, the POMDP approach significantly outperformed the heuristic approach. Moreover, in each scene, the POMDP approach received higher rewards on average.</paragraph><paragraph>Regarding planning times, on a single low performance AMD A10-4600M CPU core the heuristic approach took on average 2 ms per time step while the POMDP approach took 2.6 s per time step. To put this in context, one timestep corresponding to moving an object from one location to another took on average 55 s so that the proportion of time spent on planning was 4.7% using the POMDP approach. Most of the time was taken by robot motion which was slow to guarantee safety. The visual capture and processing time was not optimized and was a few times the time of planning, typically around 10 s. Altogether, the time needed for the entire loading operation depended on the number of objects, being typically a few minutes.</paragraph><paragraph>Performance wise the heuristic approach was closest to the POMDP approach in scene 3. In scene 3, the two partly green objects closest to the Kinect were easy to grasp and the heuristic approach always successfully moved them to the dishwasher. Because of heavy occlusion the two partly green objects farthest from the Kinect were very hard to grasp. Therefore, while being usually able to move the easy to grasp objects, the POMDP approach had more difficulty in moving the other two partly green objects. Interestingly, among individual experiment runs, the POMDP approach had both the lowest (−20) and highest (17) reward. The lowest reward was possible because of the grasp and observation uncertainty, and because the POMDP approach was more active than the heuristic approach. Another interesting observation from the experiments was that occasionally an object could be dropped or tipped over. Our POMDP model does not explicitly take these kinds of events into account. However, in spite of this, the POMDP approach adapted to these unexpected situations because it always planned actions based on the belief estimated from current sensor readings.</paragraph></section></section><section label="4.3"><section-title>Discussion</section-title><paragraph>The experiments confirm that multi-step POMDP planning is useful, when the order of actions is critical to the successful completion of the task. In particular, a POMDP estimates the value of information optimally. In an uncertain world, the probabilistic model used in POMDPs can weight different action choices in a principled manner. In contrast to a greedy approach, a POMDP may select actions that gather information, but do not yield immediate reward, when the problem so requires. In the multi-object manipulation experiments, the robot had to decide between lifting objects to gather information or moving objects that appear dirty into the dishwasher. Our POMDP model includes grasping success and learns grasping probabilities. Grasping unknown objects requires object specific grasp probabilities because each object may be different. However, even when predefined object models are available, adaptive object specific grasp probabilities may be useful; especially in heavily cluttered settings, with multiple objects, the large uncertainty about object pose and identity make grasping some objects harder than others and requires an adaptive approach.</paragraph><paragraph>The experimental setting in this paper was restricted to finding and moving dirty dishes into a dishwasher. However, a significant number of other applications, manipulation or otherwise, require information gathering or planning under uncertainty. When a robot is employed in an unstructured environment, for example, an ordinary home, the robot typically does not know the exact location of objects in advance and does not possess an exact model of the objects. These kinds of environments require reasoning about uncertainty and reasoning about when and how to obtain more information needed by the assigned task. Organizing objects, such as toys [46], is such a common task. The robot may need to find specific toys from several objects placed on top of a table or on the floor and put them into a box or on a shelf. Because of occlusions the robot does not see all objects completely and cannot be certain which object is which. Therefore, the robot has to consider information gathering actions. Moreover, in these kinds of settings grasp success is uncertain and often not known in advance.</paragraph><paragraph>As a concrete example, consider cleaning toys from a table into a box after a child has been playing. Successful identification and grasping of toys would depend on their visibility and the reward function could consist of a positive reward for putting a toy into the box and a negative reward for putting another object into the box. Another application is finding a particular object. For example, there could be several piles of objects and the robot has to plan how to remove objects from the piles, while taking into account how much information each removal will yield and how likely it is that the removal succeeds. A positive reward would be assigned to finding the object.</paragraph></section></section><section label="5"><section-title>Conclusion</section-title><paragraph>We presented a POMDP model for multi-object manipulation of unknown objects in a crowded environment. Because objects are occluded, their attributes are harder to observe and they are harder to manipulate. To address this, our POMDP model uses an occlusion ratio to define how much an object occludes another one. We use the occlusion ratio as a parameter in the observation and grasp probabilities of objects. In addition to occlusion specific grasp probabilities, our model also includes automatically adapting object specific grasp probabilities. To compute compact policies for the computationally complex POMDP model, we presented a new POMDP method that optimizes a policy graph using particle filtering. The method allows multi-step POMDP planning, both offline and online.</paragraph><paragraph>Experiments confirm that a heuristic greedy manipulation approach is not adequate for multi-object manipulation, but instead, the problem requires complex conditional multi-step POMDP plans that take long term effects into account. Moreover, object specific grasp probabilities are needed in many real-world situations.</paragraph><paragraph>In the future we plan to apply the presented POMDP model to other kinds of robotic tasks. Currently, we are extending the POMDP model to take into account the uncertainty in the composition of objects from segments. In general, to obtain true long-term autonomy, we believe that a robot should base its decisions on prior learned knowledge and adjust its world model to the specific environment it operates in. For this purpose a probabilistic Bayesian framework should be used that allows the robot to operate and learn in an uncertain, unstructured environment. In contrast to engineered solutions, learning offers the possibility to find solutions that generalize to unexpected situations and a possibility for autonomous adaptation. Our goal is an autonomous robot which can be placed in a complex new environment and which then knows how to adapt to the new environment. The work presented here is a step towards that goal.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>This work was supported by the Academy of Finland, decision 271394.</paragraph></acknowledgements><references><reference label="[1]"><authors>L.P. Kaelbling,M.L. Littman,A.R. Cassandra</authors><title>Planning and acting in partially observable stochastic domains</title><host>Artif. Intell.101 (1–2)(1998) pp.99-134</host></reference><reference label="[2]"><authors>S. Thrun</authors><title>Probabilistic robotics</title><host>Commun. ACM45 (3)(2002) pp.52-57</host></reference><reference label="[3]"><authors>J. Hoey,A. Von Bertoldi,P. Poupart,A. Mihailidis</authors><title>Assisting persons with dementia during handwashing using a partially observable Markov decision process</title><host>Proceedings of the 5th International Conference on Vision SystemsICVS(2007)Bielefeld University Library</host></reference><reference label="[4]"><authors>I. Chadès,E. McDonald-Madden,M.A. McCarthy,B. Wintle,M. Linkie,H.P. Possingham</authors><title>When to stop managing or surveying cryptic threatened species</title><host>Proc. Natl. Acad. Sci.105 (37)(2008) pp.13936-13940</host></reference><reference label="[5]"><authors>J. Pajarinen,J. Peltonen,M.A. Uusitalo,A. Hottinen</authors><title>Latent state models of primary user behavior for opportunistic spectrum access</title><host>Proceedings of IEEE International Symposium on Personal, Indoor and Mobile Radio CommunicationsPIMRC(2009)IEEE</host></reference><reference label="[6]"><authors>C.H. Papadimitriou,J.N. Tsitsiklis</authors><title>The complexity of Markov decision processes</title><host>Math. Oper. Res.12 (1987) pp.441-450</host></reference><reference label="[7]"><authors>T. Smith,R. Simmons</authors><title>Point-based POMDP algorithms: improved analysis and implementation</title><host>Proceedings of the Twenty-First Annual Conference on Uncertainty in Artificial IntelligenceUAI(2005)AUAI Press pp.542-549</host></reference><reference label="[8]"><authors>H. Kurniawati,D. Hsu,W.S. Lee</authors><title>SARSOP: efficient point-based POMDP planning by approximating optimally reachable belief spaces</title><host>Proceedings of Robotics: Science and Systems IV(2008)MIT Press pp.65-72</host></reference><reference label="[9]"><authors>D. Silver,J. Veness</authors><title>Monte-Carlo planning in large POMDPs</title><host>J. LaffertyC.K.I. WilliamsJ. Shawe-TaylorR.S. ZemelA. CulottaNIPSAdv. Neural Inf. Process. Syst.vol. 23 (2011)Curran Associates Inc. pp.2164-2172</host></reference><reference label="[10]"><authors>H. Bai,D. Hsu,W. Lee,V. Ngo</authors><title>Monte Carlo value iteration for continuous-state POMDPs</title><host>Algorithmic Foundations of Robotics IX(2011) pp.175-191</host></reference><reference label="[11]"><authors>G. Shani,J. Pineau,R. Kaplow</authors><title>A survey of point-based POMDP solvers</title><host>Auton. Agents Multi-Agent Syst.27 (1)(2013) pp.1-51</host></reference><reference label="[12]"><authors>P. Poupart,C. Boutilier</authors><title>Value-directed compression of POMDPs</title><host>S. BeckerS. ThrunK. ObermayerNIPSAdv. Neural Inf. Process. Syst.vol. 15 (2003)MIT Press pp.1547-1554</host></reference><reference label="[13]"><authors>D. McAllester,S. Singh</authors><title>Approximate planning for factored POMDPs using belief state simplification</title><host>Proceedings of the Fifteenth Annual Conference on Uncertainty in Artificial IntelligenceUAI(1999)Morgan Kaufmann pp.409-417</host></reference><reference label="[14]"><authors>J. Pajarinen,J. Peltonen,A. Hottinen,M. Uusitalo</authors><title>Efficient planning in large POMDPs through policy graph based factorized approximations</title><host>Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in DatabasesECML PKDDLecture Notes in Computer Sciencevol. 6323 (2010)Springer pp.1-16</host></reference><reference label="[15]"><authors>P. Poupart,C. Boutilier</authors><title>Bounded finite state controllers</title><host>S. ThrunL. SaulB. SchölkopfNIPSAdv. Neural Inf. Process. Syst.vol. 16 (2004)MIT Press pp.823-830</host></reference><reference label="[16]"><authors>C. Amato,B. Bonet,S. Zilberstein</authors><title>Finite-state controllers based on Mealy machines for centralized and decentralized POMDPs</title><host>Proceedings of the Twenty-Fourth National Conference on Artificial IntelligenceAAAI(2010)AAAI Press</host></reference><reference label="[17]"><authors>J. Pajarinen,J. Peltonen</authors><title>Periodic finite state controllers for efficient POMDP and DEC-POMDP planning</title><host>Proceedings of the 25th Annual Conference on Neural Information Processing SystemsNIPS(2011) pp.2636-2644</host></reference><reference label="[18]"><authors>S. Ross,J. Pineau,S. Paquet,B. Chaib-Draa</authors><title>Online planning algorithms for POMDPs</title><host>J. Artif. Intell. Res.32 (1)(2008) pp.663-704</host></reference><reference label="[19]"><authors>S.S. Keerthi,E.G. Gilbert</authors><title>Optimal infinite-horizon feedback laws for a class of constrained discrete-time systems: stability and moving-horizon approximations</title><host>J. Optim. Theory Appl.57 (2)(1988) pp.265-293</host></reference><reference label="[20]"><authors>J. Mattingley,Y. Wang,S. Boyd</authors><title>Receding horizon control</title><host>IEEE Control Syst.31 (3)(2011) pp.52-65</host></reference><reference label="[21]"><authors>F. Doshi,J. Pineau,N. Roy</authors><title>Reinforcement learning with limited reinforcement: using Bayes risk for active learning in POMDPs</title><host>Proceedings of the 25th International Conference on Machine LearningICML(2008)ACM pp.256-263</host></reference><reference label="[22]"><authors>P. Poupart,N. Vlassis</authors><title>Model-based Bayesian reinforcement learning in partially observable domains</title><host>Proceedings of the Tenth International Symposium on Artificial Intelligence and MathematicsISAIM(2008)</host></reference><reference label="[23]"><authors>S. Ross,J. Pineau,B. Chaib-Draa,P. Kreitmann</authors><title>A Bayesian approach for learning and planning in partially observable Markov decision processes</title><host>J. Mach. Learn. Res.12 (May)(2011) pp.1729-1770</host></reference><reference label="[24]"><authors>S. Ross,B. Chaib-draa,J. Pineau</authors><title>Bayesian reinforcement learning in continuous POMDPs with application to robot navigation</title><host>IEEE International Conference on Robotics and AutomationICRA(2008)IEEE pp.2845-2851</host></reference><reference label="[25]"><authors>H. Bai,D. Hsu,W.S. Lee</authors><title>Planning how to learn</title><host>IEEE International Conference on Robotics and AutomationICRA(2013)</host></reference><reference label="[26]"><authors>T. Lozano-Pérez,M.T. Mason,R.H. Taylor</authors><title>Automatic synthesis of fine-motion strategies for robots</title><host>Int. J. Robot. Res.3 (1)(1984) pp.3-24</host></reference><reference label="[27]"><authors>R.C. Brost</authors><title>Automatic grasp planning in the presence of uncertainty</title><host>Int. J. Robot. Res.7 (1)(1988) pp.3-17</host></reference><reference label="[28]"><authors>S.M. LaValle,S. Hutchinson</authors><title>An objective-based framework for motion planning under sensing and control uncertainties</title><host>Int. J. Robot. Res.17 (1)(1998) pp.19-42</host></reference><reference label="[29]"><authors>S.M. LaValle</authors><title>Planning Algorithms</title><host>(2006)Cambridge University Press</host></reference><reference label="[30]">L.P. Kaelbling,T. Lozano-PeresIntegrated task and motion planning in the nowTech. Rep. MIT-CSAIL-TR-2012-018<host>(2012)MIT CSAIL</host></reference><reference label="[31]">L.P. Kaelbling,T. Lozano-PeresIntegrated robot task and motion planning in belief spaceTech. Rep. MIT-CSAIL-TR-2012-019<host>(2012)MIT CSAIL</host></reference><reference label="[32]"><authors>S. Thrun,W. Burgard,D. Fox</authors><title>Probabilistic Robotics</title><host>(2005)MIT Press</host></reference><reference label="[33]"><authors>H. Kurniawati,Y. Du,D. Hsu,W.S. Lee</authors><title>Motion planning under uncertainty for robotic tasks with long time horizons</title><host>Int. J. Robot. Res.30 (3)(2011) pp.308-323</host></reference><reference label="[34]"><authors>K. Hsiao,L.P. Kaelbling,T. Lozano-Peres</authors><title>Grasping POMDPs</title><host>IEEE International Conference on Robotics and AutomationRome, Italy(2007)</host></reference><reference label="[35]"><authors>K. Hsiao,L.P. Kaelbling,T. Lozano-Pérez</authors><title>Robust grasping under object pose uncertainty</title><host>Auton. Robots31 (2–3)(2011) pp.253-268</host></reference><reference label="[36]"><authors>K. Hsiao,M. Ciocarlie,P. Brook</authors><title>Bayesian grasp planning</title><host>ICRA 2011 Workshop on Mobile Manipulation(2011)</host></reference><reference label="[37]"><authors>J. Laaksonen,E. Nikandrova,V. Kyrki</authors><title>Probabilistic sensor-based grasping</title><host>IEEE/RSJ International Conference on Intelligent Robots and SystemsIROS 2012, Vilamoura, Portugal(2012) pp.2019-2026</host></reference><reference label="[38]">E. Nikandrova,J. Laaksonen,V. KyrkiTowards informative sensor-based grasp planningRobot. Auton. Syst. (2015)submitted for publication</reference><reference label="[39]"><authors>M. Dogar,S. Srinivasa</authors><title>A planning framework for non-prehensile manipulation under clutter and uncertainty</title><host>Auton. Robots33 (3)(2012) pp.217-236</host></reference><reference label="[40]"><authors>P. Monso,G. Alenya,C. Torras</authors><title>POMDP approach to robotized clothes separation</title><host>IEEE/RSJ International Conference on Intelligent Robots and Systems(2012)VilamouraPortugal pp.1324-1329</host></reference><reference label="[41]"><authors>D. Fischinger,M. Vincze,Y. Jiang</authors><title>Learning grasps for unknown objects in cluttered scenes</title><host>IEEE International Conference on Robotics and AutomationKarlsruhe, Germany(2013)</host></reference><reference label="[42]"><authors>S. Chakravorty,R. Erwin</authors><title>Information space receding horizon control</title><host>IEEE Symposium on Adaptive Dynamic Programming and Reinforcement LearningADPRL(2011)IEEE pp.302-309</host></reference><reference label="[43]"><authors>A. Barto</authors><title>Reinforcement Learning: An Introduction</title><host>(1998)MIT Press</host></reference><reference label="[44]"><authors>J. Felip,J. Laaksonen,A. Morales,V. Kyrki</authors><title>Manipulation primitives: a paradigm for abstraction and execution of grasping and manipulation tasks</title><host>Robot. Auton. Syst.61 (3)(2013) pp.283-296</host></reference><reference label="[45]"><authors>H.B. Mann,D.R. Whitney</authors><title>On a test of whether one of two random variables is stochastically larger than the other</title><host>Ann. Math. Stat.18 (1)(1947) pp.50-60</host></reference><reference label="[46]"><authors>J. Pajarinen,V. Kyrki</authors><title>Robotic manipulation in object composition space</title><host>IEEE/RSJ International Conference on Intelligent Robots and SystemsIROS(2014)IEEE pp.1-6</host></reference></references><footnote><note-para label="1">For segmentation we use organized multiplane segmentation and organized Euclidean cluster extraction, part of the point cloud library http://www.pointclouds.org/.</note-para></footnote></root>