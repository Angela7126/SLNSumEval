<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370214000794</url><title>Multicategory large margin classification methods: Hinge losses vs. coherence functions</title><authors>Zhihua Zhang,Cheng Chen,Guang Dai,Wu-Jun Li,Dit-Yan Yeung</authors><abstract>Generalization of large margin classification methods from the binary classification setting to the more general multicategory setting is often found to be non-trivial. In this paper, we study large margin classification methods that can be seamlessly applied to both settings, with the binary setting simply as a special case. In particular, we explore the Fisher consistency properties of multicategory majorization losses and present a construction framework of majorization losses of the 0–1 loss. Under this framework, we conduct an in-depth analysis about three widely used multicategory hinge losses. Corresponding to the three hinge losses, we propose three multicategory majorization losses based on a coherence function. The limits of the three coherence losses as the temperature approaches zero are the corresponding hinge losses, and the limits of the minimizers of their expected errors are the minimizers of the expected errors of the corresponding hinge losses. Finally, we develop multicategory large margin classification methods by using a so-called multiclass C-loss.</abstract><keywords>Multiclass margin classification;Fisher consistency;Multicategory hinge losses;Coherence losses;Multicategory boosting algorithm</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Large margin classification methods have become increasingly popular since the advent of the support vector machine (SVM) [4] and boosting [7], [8]. Recent developments include the large-margin unified machine of Liu et al. [16] and the flexible assortment machine of Qiao and Zhang [19]. Typically, large margin classification methods approximately solve an otherwise intractable optimization problem defined with the 0–1 loss. These algorithms were originally designed for binary classification problems. Unfortunately, generalization of them to the multicategory setting is often found to be non-trivial. The goal of this paper is to solve multicategory classification problems using the same margin principle as that for binary problems.</paragraph><paragraph>The conventional SVM based on the hinge loss function possesses support vector interpretation (or data sparsity) but does not have uncertainty (that is, the SVM does not directly estimate the conditional class probability). The non-differentiable hinge loss function also makes it non-trivial to extend the conventional SVM from binary classification problems to multiclass classification problems in the same margin principle [25], [3], [26], [12], [5], [13]. Thus, one seemingly natural approach to constructing a classifier for the binary and multiclass problems is to consider a smooth loss function.</paragraph><paragraph>For example, regularized logistic regression models based on the negative multinomial log-likelihood function (also called the logit loss) [31], [10] are competitive with SVMs. Moreover, it is natural to exploit the logit loss in the development of a multicategory boosting algorithm [9]. Recently, Zhang et al. [30] proposed a smooth loss function that called coherence function for developing binary large margin classification methods. The coherence function establishes a bridge between the hinge loss and the logit loss. In this paper, we study the application of the coherence function in the multiclass classification problem.</paragraph><section label="1.1"><section-title>Multicategory margin classification methods</section-title><paragraph>We are concerned with an m-class ({a mathematical formula}m&gt;2) classification problem with a set of training data points {a mathematical formula}{(xi,ci)}i=1n where {a mathematical formula}xi∈X⊂Rp is an input vector and {a mathematical formula}ci∈{1,2,…,m} is its corresponding class label. We assume that each x belongs to one and only one class. Our goal is to find a classifier {a mathematical formula}ϕ(x):x→c∈{1,…,m}.</paragraph><paragraph>Let {a mathematical formula}Pc(x)=Pr⁡(C=c|X=x), {a mathematical formula}c=1,…,m, be the class conditional probabilities given x. The expected error at x is then defined by{a mathematical formula} where {a mathematical formula}I{#} is 1 if # is true and 0 otherwise. The empirical error on the training data is thus given by{a mathematical formula} Given that ϵ is equal to its minimum value zero when all training data points are correctly classified, we wish to use ϵ as a basis for devising classification methods.</paragraph><paragraph>Suppose the classifier is modeled using an m-vector {a mathematical formula}g(x)=(g1(x),…,gm(x))T, where the induced classifier is obtained via maximization in a manner akin to discriminant analysis: {a mathematical formula}ϕ(x)=argmaxj{gj(x)}. For simplicity of our analysis, we assume that for a fixed x, each {a mathematical formula}gj itself lies in a compact set. We also assume that the maximizing argument of {a mathematical formula}maxj⁡gj(x) is unique. Of course this excludes the trivial case that {a mathematical formula}gj=0 for all {a mathematical formula}j∈{1,…,m}. However, this assumption does not imply that the maximum value is unique; indeed, adding a constant to each component {a mathematical formula}gj(x) does not change the maximizing argument. To remove this redundancy, it is convenient to impose a sum-to-zero constraint. Thus we define{a mathematical formula} and assume {a mathematical formula}g∈G in this paper unless otherwise specified. Zou et al. [33] referred to such a g as the margin vector. Liu and Shen [15] referred to {a mathematical formula}maxj⁡(gj(x)−gc(x)) as the generalized margin of {a mathematical formula}(x,c) with respect to (w.r.t.) g.</paragraph><paragraph>Since a margin vector g induces a classifier, we explore the minimization of ϵ w.r.t. g. However, this minimization problem is intractable because {a mathematical formula}I{ϕ(x)≠c} is the 0–1 function. A wide variety of margin-based classifiers can be understood as minimizers of a surrogate loss function {a mathematical formula}ψc(g(x)), which upper bounds the 0–1 loss {a mathematical formula}I{ϕ(x)≠c}. That is, various tractable surrogate loss functions {a mathematical formula}ψc(g(x)) are thus used to upper approximate {a mathematical formula}I{ϕ(x)≠c}. The corresponding empirical risk function is given by{a mathematical formula}</paragraph><paragraph>If α is a positive constant that does not depend on {a mathematical formula}(x,c), {a mathematical formula}argming(x)∈G1αRˆ(g) is equivalent to {a mathematical formula}argming(x)∈GRˆ(g). We thus present the following definition.</paragraph><paragraph label="Definition 1">A surrogate loss {a mathematical formula}ψc(g(x)) is said to be the majorization of {a mathematical formula}I{ϕ(x)≠c} w.r.t. {a mathematical formula}(x,c) if {a mathematical formula}ψc(g(x))≥αI{ϕ(x)≠c} where α is a positive constant that does not depend on {a mathematical formula}(x,c).</paragraph><paragraph>In practice, convex majorization functions play an important role in the development of classification algorithms. On one hand, the convexity makes the resulting optimization problems computationally tractable. On the other hand, the classification methods usually have better statistical properties.</paragraph><paragraph>Given a majorization function {a mathematical formula}ψc(g(x)), the classifier resulted from the minimization of {a mathematical formula}Rˆ(g) w.r.t. the margin vector g is called a large margin classifier or a margin-based classification method. In the binary classification setting, a wide variety of classifiers can be understood as minimizers of a majorization loss function of the 0–1 loss. If such functions satisfy other technical conditions, the resulting classifiers can be shown to be Bayes consistent [1]. It seems reasonable to pursue a similar development in the case of multicategory classification, and indeed such a proposal has been made by Zou et al. [33] (also see [24], [23]).</paragraph><paragraph label="Definition 2">A surrogate function {a mathematical formula}ψc(g(x)) is said to be Fisher-consistent w.r.t. a margin vector {a mathematical formula}g(x)=(g1(x),…,gm(x))T at {a mathematical formula}(x,c) if (i) the following risk minimization problem{a mathematical formula} has a unique solution {a mathematical formula}gˆ(x)=(gˆ1(x),…,gˆm(x))T; and (ii){a mathematical formula}</paragraph><paragraph>Zou et al. [33] assumed {a mathematical formula}ψc(g(x)) as an independent and identical setting; that is, {a mathematical formula}ψc(g(x))≜η(gc(x)) where η is some loss function. As we see, Definition 2 does not require that the function {a mathematical formula}ψc(g(x)) depends only on {a mathematical formula}gc(x). Thus, this definition refines the definition of Zou et al. [33]. The definition is related to the notion of infinite-sample consistency (ISC) of Zhang [28]. ISC says that an exact solution of Problem (1) leads to a Bayes rule. However, it does not require that the solution of Problem (1) be unique. Additionally, Zhang [28] especially discussed two other settings: pairwise comparison {a mathematical formula}ψc(g(x))≜∑j≠cη(gc(x)−gj(x)) and constrained comparison {a mathematical formula}ψc(g(x))≜∑j≠cη(−gj(x)).</paragraph><paragraph>In this paper, we are concerned with multicategory classification methods in which binary and multicategory problems are solved following the same principle. One of the principled approaches is due to Lee et al. [13]. The authors proposed a multicategory SVM (MSVM) which treats the m-class problem simultaneously. Moreover, Lee et al. [13] proved that their MSVM satisfies a Fisher consistency condition. Unfortunately, this desirable property does not hold for many other multiclass SVMs (see, e.g., [25], [3], [26], [12]). The multiclass SVM of [5] possesses this property only if there is a dominating class (that is, {a mathematical formula}maxj⁡Pj(x)&gt;1/2).</paragraph><paragraph>Recently, Liu and Shen [15] proposed a so-called multicategory ψ-learning algorithm by using a multicategory ψ loss, and Wu and Liu [27] devised robust truncated-hinge-loss SVMs. These two algorithms are parallel to the multiclass SVM of Crammer and Singer [5] and enjoy a generalized pairwise comparison setting.</paragraph><paragraph>Additionally, Zhu et al. [32] and Saberian and Vasconcelos [21] devised several multiclass boosting algorithms, which solve binary and multicategory problems under the same principle. Mukherjee and Schapire [17] created a general framework for studying multiclass boosting, which formalizes the interaction between the boosting algorithm and the weak learner. We note that Gao and Koller [11] applied the multiclass hinge loss of Crammer and Singer [5] to devise a multiclass boosting algorithm. However, this algorithm is cast under an output coding framework.</paragraph></section><section label="1.2"><section-title>Contributions and outline</section-title><paragraph>In this paper, we study the Fisher consistency properties of multicategory surrogate losses. First, assuming that losses are twice differentiable, we present a Fisher consistency property under a more general setting, including the independent and identical, constrained comparison and generalized pairwise comparison settings. We next propose a framework for constructing a majorization function of the 0–1 loss. This framework provides us with a natural and intuitive perspective for construction of three extant multicategory hinge losses. Under this framework, we conduct an in-depth analysis on the Fisher consistency properties of these three extant multicategory hinge losses. In particular, we give a sufficient condition that the multiclass hinge loss used by Vapnik [25], Bredensteiner and Bennett [3], Weston and Watkins [26], Guermeur [12] satisfies the Fisher consistency. Moreover, we constructively derive the minimizers of the expected errors of the multiclass hinge losses of Crammer and Singer [5].</paragraph><paragraph>The framework also inspires us to propose a class of multicategory majorization functions which are based on the coherence function [30]. The coherence function is a smooth and convex majorization of the hinge function. Especially, its limit as the temperature approaches zero gives the hinge loss. Moreover, its relationship with the logit loss is also shown. Zhang et al. [30] originally exploited the coherence function in binary classification problems. We investigate its application in the development of multicategory margin classification methods. Based on the coherence function, we in particular present three multicategory coherence losses which correspond to the three extant multicategory hinge losses. These multicategory coherence losses are infinitely smooth and convex and they satisfy the Fisher consistency condition.</paragraph><paragraph>The coherence losses have the advantage over the hinge losses that they provide an estimate of the conditional class probability, and over the multicategory logit loss that their limiting versions at zero temperature are just their corresponding multicategory hinge loss functions. Thus they are very appropriate for use in the development of multicategory large margin classification methods, especially boosting algorithms. We propose in this paper a multiclass {a mathematical formula}C learning algorithm and a multiclass GentleBoost algorithm, both based on our multicategory coherence loss functions.</paragraph><paragraph>The remainder of this paper is organized as follows. Section 2 gives a general result on Fisher consistency. In Section 3, we discuss the methodology for the construction of multicategory majorization losses and present two majorization losses based on the coherence function. Section 4 develops another multicategory coherence loss that we call the multiclass {a mathematical formula}C-loss. Based on the multiclass {a mathematical formula}C-loss, a multiclass {a mathematical formula}C learning algorithm and a multiclass GentleBoost algorithm are given in Section 5. We conduct empirical analysis for the multicategory large margin algorithms in Section 6, and conclude our work in Section 7. All proofs are deferred to the appendix.</paragraph></section></section><section label="2"><section-title>A general result on Fisher consistency</section-title><paragraph>Using the notion and notation given in Section 1.1, we now consider a more general setting than the pairwise comparison. Let {a mathematical formula}gc(x)=(g1(x)−gc(x),…,gc−1(x)−gc(x),gc+1(x)−gc(x),…,gm(x)−gc(x))T. We define {a mathematical formula}ψc(g(x)) as a function of {a mathematical formula}gc(x) (thereafter denoted {a mathematical formula}f(gc(x))). It is clear that the pairwise comparison {a mathematical formula}ψc(g(x))=∑j≠cη(gc(x)−gj(x)), the multiclass hinge loss of Crammer and Singer [5], the multicategory ψ-loss of Liu and Shen [15], and the truncated hinge loss of Wu and Liu [27] follow this generalized definition. Moreover, for these cases, we note that {a mathematical formula}f(gc) is symmetric.{sup:1}</paragraph><paragraph>Furthermore, we present a unifying definition of {a mathematical formula}ψ(g)=(ψ1(g),…,ψm(g))T:Rm→Rm where we ignore the dependency of g on x. Let Ψ be a set of mappings {a mathematical formula}ψ(g) satisfying the conditions: (i) when fixed {a mathematical formula}gc{a mathematical formula}ψc(g) is symmetric w.r.t. the remaining arguments and (ii) {a mathematical formula}ψc(g)=ψj(gjc) where {a mathematical formula}gjc is obtained by only exchanging {a mathematical formula}gc and {a mathematical formula}gj of g. Obviously, the mapping {a mathematical formula}ψ(g) defined via the independent and identical setting, the constrained comparison, or the generalized pairwise comparison with symmetric f belongs to Ψ. With this notion, we give an important theorem of this paper as follows.</paragraph><paragraph label="Theorem 3">Let{a mathematical formula}ψ(g)∈Ψbe a twice differentiable function from{a mathematical formula}Rmto{a mathematical formula}Rm. Assume that the Hessian matrix of{a mathematical formula}ψc(g)w.r.t.gis conditionally positive definite for{a mathematical formula}c=1,…,m. Then the minimizer{a mathematical formula}gˆ=(gˆ1,…,gˆm)Tof{a mathematical formula}∑cψc(g(x))Pc(x)in{a mathematical formula}Gexists and is unique. Furthermore, if{a mathematical formula}∂ψc(g)∂gc−∂ψc(g)∂gjwhere{a mathematical formula}j≠cis negative for any{a mathematical formula}g∈G, then{a mathematical formula}Pl&gt;Pkimplies{a mathematical formula}gˆl&gt;gˆk.</paragraph><paragraph>The proof of the theorem is given in Appendix A.1. Note that an {a mathematical formula}m×m real matrix A is said to be conditionally positive definite if {a mathematical formula}yTAy&gt;0 for any nonzero real vector {a mathematical formula}y=(y1,…,ym)T with {a mathematical formula}∑j=1myj=0. The condition that {a mathematical formula}∂ψc(g)∂gc−∂ψc(g)∂gj&lt;0 on {a mathematical formula}G for {a mathematical formula}j≠c is not necessary for Fisher consistency. For example, in the setting {a mathematical formula}ψc(g(x))=η(gc(x)), Zou et al. [33] proved that if {a mathematical formula}η(z) is a twice differentiable function with {a mathematical formula}η′(0)&lt;0 and {a mathematical formula}η″(z)&gt;0∀z, then {a mathematical formula}ψc(g(x)) is Fisher-consistent. In the setting {a mathematical formula}ψc(g(x))=∑j≠cη(−gj), we note that {a mathematical formula}∑c∑j≠cη(−gj)Pc=∑c=1η(−gc)(1−Pc). Based on the proof of Zou et al. [33], we have that if {a mathematical formula}η(z) is a twice differentiable function with {a mathematical formula}η′(0)&lt;0 and {a mathematical formula}η″(z)&gt;0∀z, then {a mathematical formula}ψc(g(x))=∑j≠cη(−gj(x)) is Fisher-consistent. That is, in these two cases, we can relax the condition that {a mathematical formula}∂ψc(g)∂gc−∂ψc(g)∂gj&lt;0 for any {a mathematical formula}g∈G as {a mathematical formula}∂ψc(0)∂gc−∂ψc(0)∂gj&lt;0, for {a mathematical formula}j≠c.</paragraph><paragraph>We have the following corollary, whose proof is given in Appendix A.2. We will see two concrete cases of this corollary (that is, Theorem 10, Theorem 13).</paragraph><paragraph label="Corollary 4">Assume{a mathematical formula}ψc(g)=f(gc)where{a mathematical formula}f(z)is a symmetric and twice differentiable function from{a mathematical formula}Rm−1to{a mathematical formula}R. If the Hessian matrix of{a mathematical formula}f(z)w.r.t.zis positive definite, then the minimizer{a mathematical formula}gˆ=(gˆ1,…,gˆm)Tof{a mathematical formula}∑cψc(g(x))Pc(x)in{a mathematical formula}Gexists and is unique. Furthermore, if{a mathematical formula}∂ψc(g)∂gc−∂ψc(g)∂gjwhere{a mathematical formula}j≠cis negative for anyg, then{a mathematical formula}Pl&gt;Pkimplies{a mathematical formula}gˆl&gt;gˆk.</paragraph><paragraph>Theorem 3 or Corollary 4 shows that {a mathematical formula}ψc(g) admits the ISC of Zhang [28]. Thus, under the conditions in Theorem 3 or Corollary 4, we also have the relationship between the approximate minimization of the risk based on {a mathematical formula}ψc and the approximate minimization of the classification error. In particular, if{a mathematical formula} for some {a mathematical formula}ϵ1&gt;0, then there exists an {a mathematical formula}ϵ2&gt;0 such that{a mathematical formula} where {a mathematical formula}ϕˆ(X)=argmaxj{gˆj(X)}, {a mathematical formula}ϕ⁎(X)=argmaxj{Pj(X)} and {a mathematical formula}EX[∑c=1,c≠ϕ⁎(X)mPc(X)] is the optimal error. This result directly follows from Theorem 3 in Zhang [28].</paragraph></section><section label="3"><section-title>Multicategory majorization losses</section-title><paragraph>Given x and its label c, we let {a mathematical formula}g(x) be a margin vector at x and the induced classifier be {a mathematical formula}ϕ(x)=argmaxjgj(x). In the binary case, it is clear that {a mathematical formula}ϕ(x)=c if and only if {a mathematical formula}gc(x)&gt;0, and that {a mathematical formula}gc(x)≤0 is a necessary and sufficient condition of {a mathematical formula}ϕ(x)≠c. Thus, we always have {a mathematical formula}I{gc(x)≤0}=I{ϕ(x)≠c}. Furthermore, let {a mathematical formula}g1(x)=−g2(x)=12f(x) and encode {a mathematical formula}y=1 if {a mathematical formula}c=1 and {a mathematical formula}y=−1 if {a mathematical formula}c=2. Then the empirical error is{a mathematical formula}</paragraph><paragraph>In the multicategory case, {a mathematical formula}ϕ(x)=c implies {a mathematical formula}gc(x)&gt;0 but {a mathematical formula}ϕ(x)≠c does not imply {a mathematical formula}gc(x)≤0. We shall see that {a mathematical formula}gc(x)≤0 is a sufficient but not necessary condition of {a mathematical formula}ϕ(x)≠c. In general, we only have {a mathematical formula}I{gc(x)≤0}≤I{ϕ(x)≠c}. Although {a mathematical formula}gj(x)−gc(x)&gt;0 for some {a mathematical formula}j≠c is a necessary and sufficient condition of {a mathematical formula}ϕ(x)≠c, it in most cases yields an optimization problem which is not easily solved. This is an important reason why it is not trivial to develop multicategory AdaBoost and SVMs with the same principle as binary AdaBoost and SVMs.</paragraph><section label="3.1"><section-title>Methodology</section-title><paragraph>Recall that {a mathematical formula}∑j=1mgj(x)=0 and there is at least one {a mathematical formula}j∈{1,…,m} such that {a mathematical formula}gj(x)≠0. If {a mathematical formula}gc(x)≤0, then there exists one {a mathematical formula}l∈{1,…,m} such that {a mathematical formula}l≠c and {a mathematical formula}gl(x)&gt;0. As a result, we have {a mathematical formula}ϕ(x)≠c. Therefore, {a mathematical formula}gc(x)≤0 implies {a mathematical formula}ϕ(x)≠c. Unfortunately, if {a mathematical formula}ϕ(x)≠c, {a mathematical formula}gc(x)≤0 does not necessarily hold. For example, consider the case that {a mathematical formula}m=3 and {a mathematical formula}c=1. Assume that {a mathematical formula}g(x)=(2,3,−5). Then we have {a mathematical formula}ϕ(x)=2≠1 and {a mathematical formula}g1(x)=2&gt;0. In addition, it is clear that {a mathematical formula}ϕ(x)=c implies {a mathematical formula}gc(x)&gt;0. However, {a mathematical formula}gc(x)&gt;0 does not imply {a mathematical formula}ϕ(x)=c.</paragraph><paragraph>On the other hand, it is obvious that {a mathematical formula}ϕ(x)=c is equivalent to {a mathematical formula}ϕ(x)≠j for all {a mathematical formula}j≠c. In terms of the above discussions, a condition of making {a mathematical formula}ϕ(x)=c is {a mathematical formula}gj(x)≤0 for {a mathematical formula}j≠c. To summarize, we immediately have the following theorem.</paragraph><paragraph label="Proposition 5">For{a mathematical formula}(x,c), let{a mathematical formula}g(x)be a margin vector atxand the induced classifier be{a mathematical formula}ϕ(x)=arg⁡maxj⁡gj(x). Then{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph><paragraph>Proposition 5 shows that {a mathematical formula}gc(x)≤0 is the sufficient condition of {a mathematical formula}ϕ(x)≠c, while {a mathematical formula}gj(x)&gt;0 for some {a mathematical formula}j≠c is its necessary condition. The following theorem shows that they become sufficient and necessary when g has one and only one positive element.</paragraph><paragraph label="Proposition 6">Under the conditions inProposition 5. The relationship of{a mathematical formula}holds if and only if the margin vector{a mathematical formula}g(x)has only one positive element.</paragraph><paragraph>In the binary case, this relationship always holds because {a mathematical formula}g1(x)=−g2(x). Recently, Zou et al. [33] derived multicategory boosting algorithms using {a mathematical formula}exp⁡(−gc(x)). In their discrete boosting algorithm, the margin vector {a mathematical formula}g(x) is modeled as an m-vector function with one and only one positive element. In this case, {a mathematical formula}I{gc(x)≤0} is equal to {a mathematical formula}I{ϕ(x)≠c}. Consequently, {a mathematical formula}exp⁡(−gc(x)) is a majorization of {a mathematical formula}I{ϕ(x)≠c} because {a mathematical formula}exp⁡(−gc(x)) is an upper bound of {a mathematical formula}I{gc(x)≤0}. Therefore, this discrete AdaBoost algorithm still approximates the original empirical 0–1 loss function. In the general case, however, Proposition 6 implies that {a mathematical formula}exp⁡(−gc(x)) is not the majorization of {a mathematical formula}I{ϕ(x)≠c}.</paragraph></section><section label="3.2"><section-title>Approaches</section-title><paragraph>Proposition 5 provides us with approaches for constructing majorization functions of the 0–1 loss function {a mathematical formula}I{ϕ(x)≠c}. Clearly, {a mathematical formula}∑j≠cI{gj(x)&gt;0} and {a mathematical formula}∑j≠cI{gj(x)−gc(x)&gt;0} are separable, so they are more tractable respectively than {a mathematical formula}I{⋃j≠cgj(x)&gt;0} and {a mathematical formula}I{⋃j≠cgj(x)−gc(x)&gt;0}. Thus, {a mathematical formula}∑j≠cI{gj(x)&gt;0} and {a mathematical formula}∑j≠cI{gj(x)−gc(x)&gt;0} are popularly employed in practical applications.</paragraph><paragraph>In particular, suppose {a mathematical formula}η(gj(x)) upper bounds {a mathematical formula}I{gj(x)≤0}; that is, {a mathematical formula}η(gj(x))≥I{gj(x)≤0}. Note that {a mathematical formula}η(gj(x))≥I{gj(x)≤0} if and only if {a mathematical formula}η(−gj(x))≥I{gj(x)≥0}. Thus {a mathematical formula}η(−gj(x)) upper bounds {a mathematical formula}I{gj(x)≥0}, and hence {a mathematical formula}η(gj(x)−gl(x)) upper bounds {a mathematical formula}I{gl(x)−gj(x)&gt;0}. It then follows from Proposition 5 that {a mathematical formula}∑j≠cη(−gj(x)) and {a mathematical formula}∑j≠cη(gc(x)−gj(x)) are majorizations of {a mathematical formula}I{ϕ(x)≠c}. Consequently, we can define two classes of majorizations for {a mathematical formula}I{ϕ(x)≠c}. The first one is{a mathematical formula} while the second one is{a mathematical formula}</paragraph><paragraph>This leads us to two approaches for constructing majorization {a mathematical formula}ψc(g(x)) of {a mathematical formula}I{ϕ(x)≠c}. Zhang [28] referred to them as pairwise comparison and constrained comparison. A theoretical analysis of these two classes of majorization functions has also been presented by Zhang [28]. His analysis mainly focused on consistency of empirical risk minimization and the ISC property of surrogate losses. Our results in Section 3.1 show a direct and intuitive connection of these two approaches with the original 0–1 loss.</paragraph></section><section label="3.3"><section-title>Multicategory hinge losses</section-title><paragraph>Using distinct {a mathematical formula}η(gj(x)) ({a mathematical formula}≥I{gj(x)≤0}) in the two approaches, we can construct different multicategory losses for large margin classifiers. For example, let {a mathematical formula}η(gj(x))=(1−gj(x))+ which upper bounds {a mathematical formula}I{gj(x)≤0}. Then {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+ and {a mathematical formula}∑j≠c(1+gj(x))+ are candidate majorizations for {a mathematical formula}I{ϕ(x)≠c}, which yield two multiclass SVM methods.</paragraph><paragraph>In the multicategory SVM (MSVM), Lee et al. [13] employed {a mathematical formula}∑j≠c(1+gj(x))+ as a multicategory hinge loss. Moreover, Lee et al. [13] proved that this multicategory hinge loss is Fisher-consistent. In particular, the minimizer of {a mathematical formula}∑c=1m∑j≠c(1+gj(x))+Pc(x) w.r.t. {a mathematical formula}g∈G is {a mathematical formula}gˆl(x)=m−1 if {a mathematical formula}l=argmaxj(Pj(x)) and {a mathematical formula}gˆl(x)=−1 otherwise.</paragraph><paragraph>The pairwise comparison {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+ was used by Vapnik [25], Weston and Watkins [26], Bredensteiner and Bennett [3], Guermeur [12]. Unfortunately, Lee et al. [13], Zhang [28], Liu [14] showed that solutions of the corresponding optimization problem do not always implement the Bayes decision rule. However, we find that it is still Fisher-consistent under certain conditions. In particular, we have the following theorem (the proof is given in Appendix B.1).</paragraph><paragraph label="Theorem 7">Let{a mathematical formula}Pj(x)&gt;0for{a mathematical formula}j=1,…,m,{a mathematical formula}Pl(x)=maxj⁡Pj(x)and{a mathematical formula}Pk(x)=maxj≠l⁡Pj(x), and let{a mathematical formula}If{a mathematical formula}Pl(x)&gt;1/2or{a mathematical formula}Pk(x)&lt;1/m, then{a mathematical formula}gˆl(x)=1+gˆk(x)≥1+gˆj(x)for{a mathematical formula}j≠l,k.</paragraph><paragraph>This theorem implies that {a mathematical formula}gˆl(x)&gt;gˆj(x), so the majorization function {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+ is Fisher-consistent when {a mathematical formula}Pl(x)&gt;1/2 or {a mathematical formula}Pk(x)&lt;1/m. In the case that {a mathematical formula}m=3, Liu [14] showed that this majorization function yields the Fisher consistency when {a mathematical formula}Pk&lt;13, while the consistency is not always satisfied when {a mathematical formula}1/2&gt;Pl&gt;Pk≥1/3. Theorem 7 shows that for any {a mathematical formula}m≥3 the consistency is also satisfied whenever {a mathematical formula}Pk&lt;1m.</paragraph><paragraph>As we have seen, {a mathematical formula}I{⋃j≠cgj(x)−gc(x)&gt;0} can be also used as a starting point to construct a majorization of {a mathematical formula}I{ϕ(x)≠c}. Since {a mathematical formula}I{⋃j≠cgj(x)−gc(x)&gt;0}=I{maxj≠c⁡gj(x)−gc(x)&gt;0}, we call this construction approach the maximum pairwise comparison. In fact, this approach was employed by Crammer and Singer [5], Liu and Shen [15] and Wu and Liu [27]. Especially, Crammer and Singer [5] used the surrogate:{a mathematical formula} It is easily seen that{a mathematical formula} which implies that {a mathematical formula}ξc(g(x)) is a tighter upper bound of {a mathematical formula}I{ϕ(x)≠c} than {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+. Note that Crammer and Singer [5] did not assume {a mathematical formula}g∈G, but Liu and Shen [15] argued that this assumption is also necessary. Zhang [28] showed that {a mathematical formula}ξc(g(x)) is Fisher-consistent only when {a mathematical formula}Pl(x)&gt;1/2. However, the author did not give an explicit expression of the minimizer of the expected error in question in the literature. Here we present the constructive solution of the corresponding minimization problem in the following theorem (the proof is given in Appendix B.2).</paragraph><paragraph label="Theorem 8">Consider the following optimization problem of{a mathematical formula}Assume that{a mathematical formula}Pl(x)=maxj⁡Pj(x).</paragraph><list><list-item label="(1)">If{a mathematical formula}Pl(x)&gt;1/2, then{a mathematical formula}gˆj(x)=I{j=l}−1mfor{a mathematical formula}j=1,…,m;</list-item><list-item label="(2)">If{a mathematical formula}Pl(x)=1/2, then{a mathematical formula}0≤gˆl(x)−gˆj(x)≤1and{a mathematical formula}gˆj(x)=gˆc(x)for{a mathematical formula}c,j≠l;</list-item><list-item label="(3)">If{a mathematical formula}Pl(x)&lt;1/2, then{a mathematical formula}gcˆ(x)=0for{a mathematical formula}c=1,…,m.</list-item></list><paragraph>This theorem shows that the majorization function {a mathematical formula}maxj⁡{gj(x)+1−I{j=c}}−gc(x) is Fisher-consistent when {a mathematical formula}Pl(x)&gt;1/2. Otherwise, the solution of (5) degenerates to the trivial point. As we have seen from Theorem 7, Theorem 8, {a mathematical formula}Pl(x)&gt;1/2 is a sufficient condition for both {a mathematical formula}maxj⁡{gj(x)+1−I{j=c}}−gc(x) and {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+ to be Fisher-consistent. Moreover, they satisfy the condition {a mathematical formula}gˆl(x)=1+gˆk(x) where {a mathematical formula}k=argmaxj≠lPj(x). However, as shown in Theorem 7, {a mathematical formula}∑j≠c(1−gc(x)+gj(x))+ still yields the Fisher-consistent property when {a mathematical formula}Pk&lt;1m. Thus, the consistency condition for the pairwise comparison hinge loss is weaker than that for the maximum pairwise comparison hinge loss.</paragraph></section><section label="3.4"><section-title>Multicategory coherence losses</section-title><paragraph>To construct a smooth majorization function of {a mathematical formula}I{ϕ(x)≠c}, we define {a mathematical formula}η(gc(x)) as the coherence function which was proposed by Zhang et al. [30]. The coherence function is{a mathematical formula} where T is called the temperature parameter. Clearly, {a mathematical formula}ηT(z)≥(1−z)+≥I{z≤0}. Moreover, {a mathematical formula}limT→0⁡ηT(z)=(1−z)+. Thus, we directly have two majorizations of {a mathematical formula}I{ϕ(x)≠c} based on the constrained comparison method and the pairwise comparison method.</paragraph><paragraph>Using the constrained comparison, we give a smooth approximation to {a mathematical formula}∑j≠c(1+gj(x))+ for the MSVM of Lee et al. [13]. That is,{a mathematical formula} It is immediate that {a mathematical formula}LT(g(x),c)≥∑j≠c(1+gj(x))+ and {a mathematical formula}limT→0⁡LT(g(x),c)=∑j≠c(1+gj(x))+. Furthermore, we have the following theorem (the proof is given in Appendix B.3).</paragraph><paragraph label="Theorem 9">Assume that{a mathematical formula}Pc(x)&gt;0for{a mathematical formula}c=1,…,m. Consider the optimization problem{a mathematical formula}for a fixed{a mathematical formula}T&gt;0and let{a mathematical formula}gˆ(x)=(gˆ1(x),…,gˆm(x))Tbe its solution. Then{a mathematical formula}gˆ(x)is unique. Moreover, if{a mathematical formula}Pl(x)&lt;Pj(x), we have{a mathematical formula}gˆl(x)&lt;gˆj(x). Furthermore, we have{a mathematical formula}Additionally, having obtained{a mathematical formula}gˆ(x),{a mathematical formula}Pc(x)is given by{a mathematical formula}</paragraph><paragraph>Although there is no explicit expression for {a mathematical formula}gˆ(x) in Problem (7), Theorem 9 shows that its limit at {a mathematical formula}T=0 is equal to the minimizer of {a mathematical formula}∑c=1m∑j≠c(1+gj(x))+Pc(x), which was studied by Lee et al. [13].</paragraph><paragraph>Based on the pairwise comparison, we have a smooth alternative to multiclass hinge loss {a mathematical formula}∑j≠c(1+gc(x)−gj(x))+, which is{a mathematical formula} It is also immediate that {a mathematical formula}GT(g(x),c)≥∑j≠c(1+gc(x)−gj(x))+ and {a mathematical formula}limT→0⁡GT(g(x),c)=∑j≠c(1+gc(x)−gj(x))+.</paragraph><paragraph label="Theorem 10">Assume that{a mathematical formula}Pc(x)&gt;0for{a mathematical formula}c=1,…,m. Let{a mathematical formula}Pl=maxj⁡Pj(x)and{a mathematical formula}Pk(x)=maxj≠l⁡Pj(x). Consider the optimization problem{a mathematical formula}for a fixed{a mathematical formula}T&gt;0and let{a mathematical formula}gˆ(x)=(gˆ1(x),…,gˆm(x))Tbe its solution. Then{a mathematical formula}gˆ(x)is unique. Moreover, if{a mathematical formula}Pi(x)&lt;Pj(x), we have{a mathematical formula}gˆi(x)&lt;gˆj(x). Additionally, if{a mathematical formula}Pl(x)&gt;1/2or{a mathematical formula}Pk(x)&lt;1/m, then{a mathematical formula}whenever the limits exist.</paragraph><paragraph>The proof of Theorem 10 is given in Appendix B.4. We see that the limit of {a mathematical formula}gˆl(x) at {a mathematical formula}T=0 agrees with that shown in Theorem 7. Unfortunately, based on {a mathematical formula}GT(g(x),c), it is hard to obtain an explicit expression of the class conditional probabilities {a mathematical formula}Pc(x) via the {a mathematical formula}gˆc(x).</paragraph></section></section><section label="4">Multiclass {a mathematical formula}C-losses<paragraph>In this section, we present a smooth and Fisher-consistent majorization of the multiclass hinge loss {a mathematical formula}ξc(g(x)) in (4) using the idea behind the coherence function. We call this new majorization multiclass{a mathematical formula}C-loss. We will see that this multiclass {a mathematical formula}C-loss bridges the multiclass hinge loss {a mathematical formula}ξc(g(x)) and the negative multinomial log-likelihood (logit) of the form{a mathematical formula}</paragraph><paragraph>In the 0–1 loss the misclassification costs are specified as 1. It is natural to set the misclassification costs as a positive constant {a mathematical formula}u&gt;0. This setting will reveal an important connection between the hinge loss and the logit loss. The empirical error on the training data is then{a mathematical formula} In this setting, we can extend the multiclass hinge loss {a mathematical formula}ξc(g(x)) as{a mathematical formula} It is clear that {a mathematical formula}Hu(g(x),c)≥uI{ϕ(x)≠c}. To establish the connection among the multiclass {a mathematical formula}C-loss, the multiclass hinge loss and the logit loss, we employ this setting to present the definition of the multiclass {a mathematical formula}C-loss.</paragraph><paragraph>We now express {a mathematical formula}max⁡{gj(x)+u−uI{j=c}} as {a mathematical formula}∑j=1mωjc(x)[gj(x)+u−uI{j=c}] where{a mathematical formula}</paragraph><paragraph>Motivated by the idea behind deterministic annealing [20], we relax this hard function {a mathematical formula}ωjc(x), retaining only {a mathematical formula}ωjc(x)≥0 and {a mathematical formula}∑j=1mωjc(x)=1. With such soft {a mathematical formula}ωjc(x), we maximize {a mathematical formula}∑j=1mωjc(x)[gj(x)+u−uI{j=c}]−gc(x) under entropy penalization; namely,{a mathematical formula} where {a mathematical formula}T&gt;0 is also referred to as the temperature. The maximization of F w.r.t. {a mathematical formula}ωjc(x) is straightforward, and it gives rise to the following distribution{a mathematical formula} based on the Karush–Kuhn–Tucker condition. The corresponding maximum of F is obtained by plugging (13) back into (12):{a mathematical formula}</paragraph><paragraph>Note that for {a mathematical formula}T&gt;0 we have{a mathematical formula} We thus call {a mathematical formula}CT,u(g(x),c) the multiclass {a mathematical formula}C-loss. Clearly, {a mathematical formula}CT,u(g(x),c) is infinitely smooth and convex in {a mathematical formula}g(x) (see Appendix C for the proof). Moreover, the Hessian matrix of {a mathematical formula}CT,u(g(x),c) w.r.t. {a mathematical formula}g(x) is conditionally positive definite.</paragraph><section label="4.1"><section-title>Properties</section-title><paragraph>We now investigate the relationships between the multiclass {a mathematical formula}C-loss {a mathematical formula}CT,u(g(x),c) and the multiclass hinge loss {a mathematical formula}Hu(g(x),c), and between {a mathematical formula}CT,u(g(x),c) and multiclass coherence loss {a mathematical formula}GT(g(x),c). In particular, we have the following proposition.</paragraph><paragraph label="Proposition 11">Let{a mathematical formula}GT(g(x),c),{a mathematical formula}Hu(g(x),c)and{a mathematical formula}CT,u(g(x),c)be defined by(9),(11)and(14), respectively. Then,</paragraph><list><list-item label="(i)">{a mathematical formula}I{ϕ(x)≠c}≤CT,1(g(x),c)&lt;GT(g(x),c).</list-item><list-item label="(ii)">{a mathematical formula}Hu(g(x),c)≤CT,u(g(x),c)≤Hu(g(x),c)+Tlog⁡m.</list-item></list><paragraph>The proof is given in Appendix C.1. We see from this proposition that {a mathematical formula}CT,1(g(x),c) is a majorization of {a mathematical formula}I{ϕ(x)≠c} tighter than {a mathematical formula}GT(g(x),c). When treating {a mathematical formula}g(x) fixed and considering {a mathematical formula}ωjc(x) and {a mathematical formula}CT,u(g(x),c) as functions of T, we have the following proposition.</paragraph><paragraph label="Proposition 12">For fixed{a mathematical formula}g(x)≠0and{a mathematical formula}u&gt;0, we have</paragraph><list><list-item label="(i)">{a mathematical formula}limT→∞⁡CT,u(g(x),c)−Tlog⁡m=1m∑j≠c(u+gj(x)−gc(x))and{a mathematical formula}</list-item><list-item label="(ii)">{a mathematical formula}limT→0⁡CT,u(g(x),c)=Hu(g(x),c)and{a mathematical formula}</list-item><list-item label="(iii)">{a mathematical formula}CT,u(g(x),c)is increasing in T.</list-item></list><paragraph>The proof is given in Appendix C.2. It is worth noting that Proposition 12-(ii) shows that at {a mathematical formula}T=0, {a mathematical formula}CT,1(g(x),c) reduces to the multiclass hinge loss {a mathematical formula}ξc(g(x)) of Crammer and Singer [5]. Additionally, when {a mathematical formula}u=0, we have{a mathematical formula} which was proposed by Zhang et al. [29]. When {a mathematical formula}T=1, it is the logit loss {a mathematical formula}γc(g(x)) in (10). Thus, {a mathematical formula}C1,1(g(x),c) bridges the hinge loss {a mathematical formula}ξc(g(x)) and the logit loss {a mathematical formula}γc(g(x)).</paragraph><paragraph>Consider that{a mathematical formula} This shows that {a mathematical formula}CT,0(g(x),c) no longer converges to the majorizations of {a mathematical formula}I{ϕ(x)≠c} as {a mathematical formula}T→0. However, as a special case of {a mathematical formula}u=1, we have {a mathematical formula}limT→0⁡CT,1(g(x),c)=ξc(g(x))≥I{ϕ(x)≠c}; that is, {a mathematical formula}CT,1(g(x),c) converges to the majorization of {a mathematical formula}I{ϕ(x)≠c}. In fact, Proposition 12-(ii) implies that for an arbitrary {a mathematical formula}u&gt;0, the limit of {a mathematical formula}CT,u(g(x),c) at {a mathematical formula}T=0 is still the majorization of {a mathematical formula}I{ϕ(x)≠c}. We thus see an essential difference between {a mathematical formula}CT,1(g(x),c) and {a mathematical formula}CT,0(g(x),c), which are respectively the generalizations of the {a mathematical formula}C-loss and the logit loss.</paragraph><paragraph>For notational simplicity, here and later we denote {a mathematical formula}CT,1(g(x),c) by {a mathematical formula}CT(g(x),c). Throughout our analysis in this section, we assume that the maximizing argument {a mathematical formula}l=argmaxjgj(x) is unique. This implies that {a mathematical formula}gl(x)&gt;gj(x) for {a mathematical formula}j≠l. The following theorem shows that the {a mathematical formula}C-loss is Fisher-consistent.</paragraph><paragraph label="Theorem 13">Assume that{a mathematical formula}Pc(x)&gt;0for{a mathematical formula}c=1,…,m. Consider the optimization problem:{a mathematical formula}for fixed{a mathematical formula}T&gt;0and{a mathematical formula}u≥0. Let{a mathematical formula}gˆ(x)=(gˆ1(x),…,gˆm(x))Tbe the solution. Then{a mathematical formula}gˆ(x)is unique. Moreover, if{a mathematical formula}Pi(x)&lt;Pj(x), we have{a mathematical formula}gˆi(x)&lt;gˆj(x). Furthermore, after obtaining{a mathematical formula}gˆ(x),{a mathematical formula}Pc(x)is given by{a mathematical formula}</paragraph><paragraph>In the case of {a mathematical formula}u=0 and {a mathematical formula}T=1, it follows from Theorem 13 that {a mathematical formula}Pc(x)=exp⁡(gˆc(x))∑j=1mexp⁡(gˆj(x)). This is identical to the solution for logistic regression.</paragraph><paragraph label="Theorem 14">Let{a mathematical formula}gˆ(x)=(gˆ1(x),…,gˆm(x))Tbe the solution of optimization problem(15)where{a mathematical formula}Pc(x)&gt;0for{a mathematical formula}c=1,…,m, and let{a mathematical formula}Pl(x)=maxc⁡Pc(x).</paragraph><list><list-item label="(1)">If{a mathematical formula}Pl(x)&gt;1/2, then{a mathematical formula}</list-item><list-item label="(2)">If{a mathematical formula}Pl(x)&lt;1/2, then{a mathematical formula}</list-item></list><paragraph>The proofs of Theorem 13, Theorem 14 are given in Appendix B.5. Theorem 14 shows a very important asymptotic property of the solution {a mathematical formula}gˆc(x). Especially when {a mathematical formula}u=1, {a mathematical formula}gˆc(x) as {a mathematical formula}T→0 converges to the solution of Problem (5) which is based on the multiclass hinge loss {a mathematical formula}ξc(g(x)) of Crammer and Singer [5] (see Theorem 8).</paragraph><paragraph label="Remark 1">We present three multicategory coherence functions {a mathematical formula}LT(g(x),c), {a mathematical formula}GT(g(x),c) and {a mathematical formula}CT(g(x),c). They are respectively upper bounds of three multicategory hinge losses studied in Section 3.3, so they are majorizations of the 0–1 loss {a mathematical formula}I{ϕ(x)≠c}. When {a mathematical formula}m=2, these three losses become identical. Our theoretical analysis shows that their limits as the temperature approaches zero become the corresponding hinge losses, and the limits of the minimizers of their expected errors are the minimizers of the expected errors of the corresponding hinge losses (see Theorem 9, Theorem 10, Theorem 14). We summarize the multicategory loss functions discussed in the paper in Table 1.</paragraph><paragraph label="Remark 2">The coherence losses {a mathematical formula}LT(g(x),c) and {a mathematical formula}CT(g(x),c) can result in explicit expressions for the class conditional probabilities (see (8) and (16)). Thus, this can provide us with an approach for conditional class probability estimation in the multicategory SVMs of Lee et al. [13] and of Crammer and Singer [5]. Roughly speaking, one replaces the solutions of classification models based on the multicategory coherence losses with those of the corresponding multiclass SVMs in (8) and (16), respectively. Based on {a mathematical formula}GT(g(x),c), however, there does not exist an explicit expression for the class probability similar to (8) or (16). In this case, the above approach for class probability estimation does not apply to the multiclass SVM model of Vapnik [25], Bredensteiner and Bennett [3], Weston and Watkins [26], Guermeur [12].</paragraph><paragraph label="Remark 3">An advantage of {a mathematical formula}CT(g(x),c) over {a mathematical formula}LT(g(x),c) is in that it can make condition {a mathematical formula}g(x)∈G automatically satisfy in developing a classification method. Moreover, we see that the multiclass {a mathematical formula}C-loss {a mathematical formula}CT(g(x),c) bridges the hinge loss and the logit loss. Thus, it is applicable to the construction of multiclass large margin classification methods. This motivates us to devise multiclass large margin classification methods based on {a mathematical formula}CT(g(x),c).</paragraph></section></section><section label="5">Applications of the multiclass {a mathematical formula}C-loss in classification problems<paragraph>In this section, we develop a multiclass large margin classifier and a multiclass boosting algorithm. Recall that we let {a mathematical formula}CT(g(x),c) denote {a mathematical formula}CT,1(g(x),c); that is, we always set {a mathematical formula}u=1 here and later.</paragraph><section label="5.1">The multiclass {a mathematical formula}C learning<paragraph>Using the multiclass {a mathematical formula}C-loss {a mathematical formula}CT(g(x),c), we now construct margin-based classifiers that we refer to as multiclass {a mathematical formula}C learning (MCL). We first consider the linear case and then turn to the kernelized case. In the linear case, where {a mathematical formula}gj(x)=aj+xTbj, we pose the following optimization problem:{a mathematical formula} where {a mathematical formula}γ&gt;0 is the regularization parameter, {a mathematical formula}X=[x1,…,xn]T is the {a mathematical formula}n×p input data matrix, and {a mathematical formula}1n represents the {a mathematical formula}n×1 of ones. Note that here we use the result from Liu and Shen [15] that the infinite constraint {a mathematical formula}∑j=1mgj(x)∀x∈X can be reduced to {a mathematical formula}∑j=1maj1n+X∑j=1mbj=0, which is a function solely of the training data.</paragraph><paragraph>Given a reproducing kernel {a mathematical formula}K(⋅,⋅) from {a mathematical formula}X×X→R, we attempt to find a margin vector {a mathematical formula}(g1(x),…,gm(x))=(a1+h1(x),…,am+hm(x))∈∏j=1m({1}+HK), where {a mathematical formula}HK is a reproducing kernel Hilbert space. The solution of the following problem{a mathematical formula} under the constraints {a mathematical formula}∑j=1mgj(x)=0∀x∈X is{a mathematical formula} with constraints {a mathematical formula}∑j=1mgj(xi)=0 for {a mathematical formula}i=1,…,n. This result follows readily from that of Lee et al. [13]. We see that kernel-based MCL solves the following optimization problem:{a mathematical formula} where {a mathematical formula}βj=(βj1,…,βjn)T and {a mathematical formula}K=[K(xi,xj)] is the {a mathematical formula}n×n kernel matrix.</paragraph><paragraph>The minimization problem in (19) or (17) is a convex minimization problem and the objective function is differentiable; thus, the problem is readily solved. In particular, we make use of Newton-type methods to solve this problem. We further alternatively update {a mathematical formula}aj's and {a mathematical formula}βj's. The details are given in Appendix D.</paragraph><paragraph>To end this subsection, we establish a connection of multiclass {a mathematical formula}C learning with the multiclass SVM of Crammer and Singer [5], which is defined by{a mathematical formula} under the constraints {a mathematical formula}∑j=1mgj(x)=0∀x∈X. From Proposition 12, MCL reduces to the multiclass SVM of Crammer and Singer [5] as {a mathematical formula}T→0. In fact, we have the following theorem (the proof is given in Appendix E).</paragraph><paragraph label="Theorem 15">Assume that γ in Problems(20)and(18)are same. The minimizer of(18)approaches the minimizer of(20)as{a mathematical formula}T→0.</paragraph></section><section label="5.2"><section-title>The multiclass GentleBoost algorithm</section-title><paragraph>Like the negative multinomial log-likelihood function, when the multiclass {a mathematical formula}C-loss is used to devise multicategory discrete boosting algorithms, a closed-form solution no longer exists. We instead use the multiclass {a mathematical formula}C-loss to devise a genuine multicategory margin-based boosting algorithm. With a derivation similar (also see Appendix F for a brief derivation) to that in Friedman et al. [9], Zou et al. [33], Zhu et al. [32], our GentleBoost algorithm is shown in Algorithm 1.</paragraph></section></section><section label="6"><section-title>Experimental evaluation</section-title><paragraph>Our primary goal in this paper has been to provide statistical analysis of multicategory large margin classification methods based on hinge losses and coherence losses. However, we have also developed a multiclass {a mathematical formula}C learning algorithm and a multiclass gentleBoost algorithm using the multiclass {a mathematical formula}C-loss. In this section, we conduct empirical analysis of these algorithms.</paragraph><section label="6.1">Results of multiclass {a mathematical formula}C learning<paragraph>We present the results of experiments evaluating multiclass {a mathematical formula}C learning (MCL) and comparing it with the multiclass SVM [5], multiclass ψ-learning [15] and penalized logistic regression (PLR) [31]. All the algorithms were implemented in the linear setting.</paragraph><paragraph>Our first two experiments used the setup presented by Liu and Shen [15]. The first two datasets were generated from three bivariate t-distributions: {a mathematical formula}t((3,1)T,I2), {a mathematical formula}t((−3,1)T,I2) and {a mathematical formula}t((0,−2)T,I2). Here {a mathematical formula}I2 is the {a mathematical formula}2×2 identity matrix. In the first dataset, the degree of freedom (df) is equal to 1, while it is equal to 3 in the second dataset. All algorithms were trained using 150 samples and tested using an additional 10{sup:6} samples.</paragraph><paragraph>These authors found the multiclass SVM and multiclass ψ-learning to work best on these datasets and we have reported their results for these approaches in the first two columns of Table 2. This table displays the test errors, which were averaged over 100 randomly repeated simulations.</paragraph><paragraph>We implemented both MCL and PLR, using the same Newton-type method in both cases. The Newton iteration stops when the maximum iteration number (200) is reached or when the difference of successive loss values is less than 0.001. The initial values of {a mathematical formula}aj and {a mathematical formula}bj are set to 0.</paragraph><paragraph>Adopting the procedure of Liu and Shen [15], our reported results were based on choosing the optimal value of the regularization parameter {a mathematical formula}τ=2γn via a simple grid search on {a mathematical formula}[10−3,103]. As shown in Table 2, ψ-learning has the lowest testing error for the first dataset, slightly outperforming MCL. MCL is best on the second dataset.</paragraph><paragraph>The third dataset can be obtained from Statlog (http://www.liacc.up.pt/ML/) and it consists of images of the letters “D,” “O” and “Q,” with 805, 753 and 783 cases respectively. 200 of the 2341 letters were randomly selected for training and the rest were retained for testing. The results are summarized in the third column of Table 2, where the test errors were averaged over 10 randomly repeated simulations. We see that MCL has the smallest test error, followed by ψ-learning and PLR.</paragraph><paragraph>Finally, we also performed experiments on text categorization using the WebKB dataset [6]. This dataset contains web pages gathered from computer science departments in several universities. The pages can be divided into seven categories. In the experiments, we used the four most populous categories, namely, student, faculty, course, and project, resulting in a total of 4192 pages. Based on information gain, 300 features were selected. We then randomly selected {a mathematical formula}70% of the data for training while the remaining {a mathematical formula}30% were used for testing. We repeated this procedure 30 times, and reported the final errors as an average over the 30 replicates. The results are shown in the final row of Table 2, where we have restricted the comparison to MCL and PLR. We see that MCL yields an improvement over PLR.</paragraph><paragraph>We also conducted a systematic study of the effect of the hyperparameters τ and T on the letter dataset. We found that the results were relatively insensitive to particular values of these hyperparameters over an order of magnitude for T and three orders of magnitude for τ. There was a tradeoff; larger τ favors a smaller value of T.</paragraph></section><section label="6.2"><section-title>Results of multiclass GentleBoost algorithm</section-title><paragraph>We also compare our multiclass gentleBoost algorithm (called GBoost.C) with some representative multicategory boosting algorithms, including AdaBoost.MH[22], multicategory LogitBoost (MBoost.L) [9], multicategory GentleBoost (GBoost.E) [33] and GD-MCBoost[21], on six publicly available datasets (Vowel, Waveform, Image Segmentation, Optdigits, Pendigits and Satimage) from the UCI Machine Learning Repository. Following the settings in Friedman et al. [9], Zou et al. [33], we use predefined training samples and test samples for these six datasets. Summary information for the datasets is given in Table 3. We use the code released by Saberian and Vasconcelos [21] to implement their GD-MCBoost algorithm.</paragraph><paragraph>Based on the experimental strategy in Zou et al. [33], eight-node regression trees are used as weak learners for all the boosting algorithms with the exception of AdaBoost.MH, which is based on eight-node classification trees. From the experiments, we observe that the performance of all the methods becomes stable after about 50 boosting steps. Hence, the number of boosting steps for all the methods is set to 100 ({a mathematical formula}H=100) in all the experiments. The test error rates (in %) of all the boosting algorithms are shown in Table 4, from which we can see that all the boosting methods achieve much better results than CART, and our method slightly outperforms the other boosting algorithms.</paragraph><paragraph>Among all the datasets tested, Vowel and Waveform are the most difficult for classification. The notably better performance of our method for these two datasets reveals its promising properties. Fig. 1 depicts the test error curves of MBoost.L, GBoost.E, GBoost.C and GD-MCBoost on these two datasets.</paragraph><paragraph>As we established in Section 3.1, GBoost.E does not implement a margin-based decision because the loss function used in this algorithm is not the majorization function of the 0–1 loss. Our experiments show that GD-MCBoost, MBoost.L and GBoost.C are comparable, and outperform GBoost.E. The results reported in Table 4 and Fig. 1 are based on the setting of {a mathematical formula}T=1. Recall that {a mathematical formula}γc(g(x)) (see Eq. (10)) is the special case of {a mathematical formula}CT,0(g(x),c) with {a mathematical formula}T=1, so the comparison of GBoost.C with MBoost.L is fair based on {a mathematical formula}T=1.</paragraph><paragraph>Proposition 12 shows that {a mathematical formula}CT(g(x),c) ({a mathematical formula}=CT,1(g(x),c)) approaches {a mathematical formula}maxj⁡{gj(x)+1−I{j=c}}−gc(x) as {a mathematical formula}T→0. This encourages us to try to decrease T gradually over the boosting steps. However, when T gets very small, it can lead to numerical problems and often makes the algorithm unstable. To observe the effect of T, we use the different values of T to implement our boosting algorithm. The results are shown in Table 5. The experiments show that when T takes a value in {a mathematical formula}[0.1,20], our algorithm (GBoost.C) is always able to obtain promising performance. In other words, the performance of our algorithm is less sensitive to the value of T.</paragraph></section></section><section label="7"><section-title>Conclusion</section-title><paragraph>In this paper, we have studied a class of multicategory coherence loss functions as well as the relationship between the multicategory coherence and hinge losses. As majorization functions of the 0–1 loss, the multicategory coherence loss functions are Fisher-consistent, infinitely smooth and convex. Thus, it is appropriate for the design of margin-based boosting algorithms. In particular, we have devised a multiclass {a mathematical formula}C learning algorithm and a multiclass GentleBoost algorithm. While our main focus has been theoretical, we have also shown experimentally that our algorithms are effective.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>The authors would like to thank the three anonymous referees for their insightful comments on the original version of this paper. Wu-Jun Li is supported by the NSFC (No. 61100125), the 863 Program of China (No. 2012AA011003), and the Program for Changjiang Scholars and Innovative Research Team in University of China (IRT1158, PCSIRT). Z. Zhang is supported in part by the Natural Science Foundation of China (No. 61070239).</paragraph></acknowledgements><appendices><section label="Appendix A">The proof of Theorem 3 and Corollary 4<paragraph>In our derivation, we just write {a mathematical formula}Pc and {a mathematical formula}gc for {a mathematical formula}Pc(x) and {a mathematical formula}gc(x) for notational simplicity. In order to prove the theorem, we first present the following definition and lemma, which can be found in Ortega and Rheinboldt [18].</paragraph><paragraph label="Definition 16">A mapping {a mathematical formula}f:D⊂Rp→Rp is monotone on {a mathematical formula}D0⊂D if{a mathematical formula} and f is strictly monotone on {a mathematical formula}D0 if the above strict inequality holds whenever {a mathematical formula}u≠v.</paragraph><paragraph label="Lemma 17">Let{a mathematical formula}f:D⊂Rp→Rpbe continuously differentiable on an open convex set{a mathematical formula}D0⊂D. Then</paragraph><list><list-item label="(a)">{a mathematical formula}f(u)is monotone on{a mathematical formula}D0if and only if{a mathematical formula}f′(u)is positive semidefinite for all{a mathematical formula}u∈D0.</list-item><list-item label="(b)">If{a mathematical formula}f′(u)is positive definite for all{a mathematical formula}u∈D0, then f is strictly monotone on{a mathematical formula}D0.</list-item><list-item label="(c)">If{a mathematical formula}f′(u)is conditionally positive definite for all{a mathematical formula}u∈D0, then f is strictly monotone on{a mathematical formula}{u|∑j=1puj=0}⊂D0.</list-item></list><section label="A.1">The proof of Theorem 3<paragraph>To solve the constrained minimization problem in (1), we define the Lagrangian as follows.{a mathematical formula} Since the Hessian matrix of L w.r.t. g is {a mathematical formula}∑cHcPc where {a mathematical formula}Hc=[∂2ψc(g)∂gj∂gk] is conditionally positive definite, the solution {a mathematical formula}gˆc exists and is unique. Moreover, we have {a mathematical formula}∇ψc(g)=(∂ψc(g)∂g1,…,∂ψc(g)∂gm)T is strictly monotone for {a mathematical formula}g∈G.</paragraph><paragraph>The first partial derivative of L w.r.t. {a mathematical formula}gk is{a mathematical formula} Based on the Karush–Kuhn–Tucker (KKT) conditions, we have{a mathematical formula} Without loss of generality, we assume {a mathematical formula}P1&gt;P2. Hence,{a mathematical formula}</paragraph><paragraph>We now prove {a mathematical formula}gˆ1&gt;gˆ2 by contradiction. Let us assume {a mathematical formula}gˆ1≤gˆ2. On one hand, using the strict monotony of {a mathematical formula}∇ψc and the assumption of {a mathematical formula}ψ∈Ψ yields{a mathematical formula} whenever {a mathematical formula}g2≠g1. Here {a mathematical formula}gˆ12=(gˆ2,gˆ1,gˆ3,…,gˆm)T and {a mathematical formula}gˆ−gˆ12=(gˆ1−gˆ2)(1,−1,0,…,0)T. We thus have{a mathematical formula} which implies that the right-hand side of Eq. (21) is negative. The above first inequality is based on the assumption of the theorem.</paragraph><paragraph>On the other hand, for {a mathematical formula}c≠1,2, using the strict monotony of {a mathematical formula}∇ψc, we have{a mathematical formula} whenever {a mathematical formula}gˆ1≠gˆ2. Furthermore, the symmetry of {a mathematical formula}ψc(g) when fixed {a mathematical formula}gc implies that {a mathematical formula}∂ψc(gˆ)∂g2=∂ψc(gˆ12)∂g1 and {a mathematical formula}∂ψc(gˆ)∂g1=∂ψc(gˆ12)∂g2. Hence,{a mathematical formula} which implies that the left-hand side of Eq. (21) is nonnegative. Thus, the assumption that {a mathematical formula}gˆ1≤gˆ2 is impossible.</paragraph></section><section label="A.2">The proof of Corollary 4<paragraph>In order to prove Corollary 4, it suffices to prove the following lemma.</paragraph><paragraph label="Lemma 18">Let{a mathematical formula}u=(u1,…,um)T∈Rmand{a mathematical formula}uc=(u1c,…,umc)Twith{a mathematical formula}ujc=uj−uc. Then{a mathematical formula}Bc=[∂2f(uc)∂ujc∂ukk]j,k≠cis an{a mathematical formula}(m−1)×(m−1)positive definite matrix, if and only if{a mathematical formula}Hc=[∂ψc(u)∂ujuk]j,k=1mis an{a mathematical formula}m×mconditionally positive definite matrix.</paragraph><paragraph label="Proof">Without loss of generality, we only consider the case of {a mathematical formula}c=m. Clearly,{a mathematical formula} Subsequently,{a mathematical formula}{a mathematical formula}∂2ψm(u)∂ul∂uk=flk″(um) for {a mathematical formula}l≠m and {a mathematical formula}k≠m, {a mathematical formula}∂2ψm(u)∂um∂uk=−∑j=1m−1fjk″(um) for {a mathematical formula}k≠m, and {a mathematical formula}∂2ψm(u)∂ul∂um=−∑j=1m−1flj″(um) for {a mathematical formula}l≠m. We thus can express {a mathematical formula}Hm as{a mathematical formula}Given any nonzero {a mathematical formula}z∈Rm−1, we have{a mathematical formula} where {a mathematical formula}Im is the {a mathematical formula}m×m identity matrix and {a mathematical formula}1m is the {a mathematical formula}m×1 vector of ones. Consider that {a mathematical formula}Im−1+1m−11m−1T is positive definite. Thus, we obtain that {a mathematical formula}Hm is conditionally positive definite if and only if {a mathematical formula}Bm is positive definite.  □</paragraph></section></section><section label="Appendix B"><section-title>Fisher consistency</section-title><section label="B.1">The proof of Theorem 7<paragraph>Without loss of generality, we assume that {a mathematical formula}P1&gt;1/2&gt;P2≥⋯Pm&gt;0.</paragraph><paragraph>Suppose that {a mathematical formula}P1&gt;1/2. First, it is immediate to obtain {a mathematical formula}gˆ1≥gˆ2≥⋯≥gˆm from Theorem 5 of Zhang [28]. Now note that{a mathematical formula} We are given {a mathematical formula}u=(u1,…,um)T∈G such that {a mathematical formula}u1≥u2≥um. Let {a mathematical formula}ρ≜u1−u2. We define {a mathematical formula}f1=u1+m−1m(1−ρ) and {a mathematical formula}fj=uj−1−ρm for {a mathematical formula}j=2,…,m. Clearly, {a mathematical formula}∑j=1mfj=0 and {a mathematical formula}f1=f2+1. We consider two cases.</paragraph><paragraph>In the first case where {a mathematical formula}0≤ρ≤1, we have{a mathematical formula} which implies that {a mathematical formula}L(u)−L(f)&gt;0 whenever {a mathematical formula}ρ≠1.</paragraph><paragraph>In the second case where {a mathematical formula}ρ&gt;1, we then have{a mathematical formula} In summary, the minimizer {a mathematical formula}gˆj should satisfy {a mathematical formula}gˆ1=1+gˆ2 whenever {a mathematical formula}P1&gt;12.</paragraph><paragraph>Now suppose that {a mathematical formula}P2&lt;1m. Assume that {a mathematical formula}u=(u1,…,um)T is a minimizer of L. We first show that {a mathematical formula}ui−ui+1≤1. If there were an integer k such that {a mathematical formula}1≤k≤m−1 and {a mathematical formula}uk−uk+1&gt;1, we would be able to give a new minimizer {a mathematical formula}v=(v1,…,vm)T by letting {a mathematical formula}vi=ui+[1−(uk−uk+1)] for {a mathematical formula}i=1,…,k and {a mathematical formula}vj=uj for {a mathematical formula}j=k+1,…,m. Then, for any pair {a mathematical formula}(i,j) where {a mathematical formula}i∈{1,…,k} and {a mathematical formula}j∈{k+1,…,m}, we have the following four inequalities: {a mathematical formula}1+vj−vi≤0, {a mathematical formula}1+uj−ui≤0, {a mathematical formula}1+vi−vj&gt;0, and {a mathematical formula}1+ui−uj&gt;0. Therefore, we can get{a mathematical formula}</paragraph><paragraph>Second, we consider two cases. In the first case we assume {a mathematical formula}u2=u3=⋯=um. Letting {a mathematical formula}a≜u1−u2, we can obtain {a mathematical formula}a≤1 from the previous discussion. We thus have{a mathematical formula} Noting that {a mathematical formula}P1&gt;1m (due to {a mathematical formula}Pm≤⋯≤P2&lt;1m), we obtain that u is a minimizer of L if and only if {a mathematical formula}a=1. Consequently, we have {a mathematical formula}u1=1+u2.</paragraph><paragraph>In the second case we assume there exists a {a mathematical formula}k∈{2,…,m−1} such that {a mathematical formula}u2=⋯=uk and {a mathematical formula}uk−uk+1&gt;0. In this case we let {a mathematical formula}a≜u1−u2 and {a mathematical formula}b≜uk−uk+1. If a were smaller than 1, we would be able to find a new minimizer v of L. Since {a mathematical formula}0≤a&lt;1 and {a mathematical formula}0&lt;b≤1, we have {a mathematical formula}ρ≜min⁡{b,1−a}&gt;0. Let {a mathematical formula}vi=ui−ρ for {a mathematical formula}i=2,⋯,k and {a mathematical formula}vj=uj for {a mathematical formula}j=1,k+1,⋯,m. Then for any pair {a mathematical formula}(i,j) where {a mathematical formula}i∈{2,⋯,k} and {a mathematical formula}j∈{1,k+1,⋯,m} we have the following three inequalities: {a mathematical formula}1+vi−vj≥0, {a mathematical formula}1+ui−uj≥0 and {a mathematical formula}(1+vj−vi)+−(1+uj−ui)+≤(ui−vi). The third inequality follows from the convexity of the hinge function. Then, we have{a mathematical formula} Since {a mathematical formula}P2&lt;1m and {a mathematical formula}P2≥⋯≥Pm, we obtain that {a mathematical formula}∑i=2kPi&lt;k−1m. Hence, {a mathematical formula}L(v)−L(u)&lt;0.</paragraph><paragraph>In summary, the minimizer u should satisfy {a mathematical formula}u1=u2+1. Recall that in the previous proof, we ignore the assumption that {a mathematical formula}∑i=1mui=0. In fact, if L attains its minimum at u with {a mathematical formula}∑i=1mui=C, we can obtain a new vector {a mathematical formula}u′ by letting {a mathematical formula}ui′=ui−Cm. Then, we have {a mathematical formula}L(u)=L(u′) and {a mathematical formula}∑i=1mui′=0.</paragraph></section><section label="B.2">The proof of Theorem 8<paragraph>Without loss of generality, we assume that {a mathematical formula}Pl=maxj⁡(Pj(x)). This implies {a mathematical formula}gl=maxj⁡(gj(x)). Thus,{a mathematical formula}</paragraph><paragraph label="Case 1">{a mathematical formula}gl≥maxj≠l⁡{1+gj}. In this case, we have{a mathematical formula} due to {a mathematical formula}gl−gc≥1 for {a mathematical formula}c≠l. It is obvious that L attains its minimum value{a mathematical formula} when {a mathematical formula}gl−gc=1 for {a mathematical formula}c≠l. Combining {a mathematical formula}∑j=1mgj=0, we have {a mathematical formula}gl=(m−1)/m and {a mathematical formula}gc=−1/m for {a mathematical formula}c≠l. Furthermore, we have {a mathematical formula}Lmin≥1 if {a mathematical formula}Pl≤1/2 and {a mathematical formula}Lmin&lt;1 otherwise.</paragraph><paragraph label="Case 2">{a mathematical formula}maxj⁡[gj+1−I{j=l}]=1+gk for {a mathematical formula}k≠l. In this case, we have {a mathematical formula}0≤gl−gk≤1 and {a mathematical formula}gk−gj≥0 for {a mathematical formula}j≠l. Note that{a mathematical formula} If {a mathematical formula}Pl&lt;1/2, then {a mathematical formula}L≥1. Especially, L attains the minimum value 1 when {a mathematical formula}gl−gk=0 and {a mathematical formula}gk−gc=0 for {a mathematical formula}c≠l. That is, {a mathematical formula}gc=0 for {a mathematical formula}c=1,…,m.If {a mathematical formula}Pl=1/2, L attains the minimum value 1 whenever the {a mathematical formula}gc satisfy that {a mathematical formula}gl−gc≤1 and {a mathematical formula}gk−gc=0 for {a mathematical formula}c≠l.If {a mathematical formula}Pl&gt;1/2, then {a mathematical formula}L≥2(1−Pl). Further, L attains the minimum value {a mathematical formula}2(1−Pl) when {a mathematical formula}gl−gk=1 and {a mathematical formula}gk−gc=0 for {a mathematical formula}c≠l; that is, {a mathematical formula}gl=(m−1)/m and {a mathematical formula}gc=−1/m for {a mathematical formula}c≠l.</paragraph></section><section label="B.3">The proof of Theorem 9<paragraph>Consider the following Lagrangian function:{a mathematical formula} where λ is the Lagrange multiplier. The first-order derivatives of L w.r.t. the {a mathematical formula}gc are{a mathematical formula} The Hessian matrix {a mathematical formula}[∂2L∂gl∂gj]=diag(∂2L∂g12,…,∂2L∂gm2) where{a mathematical formula} is positive definite and the minimizer {a mathematical formula}gˆc of the optimization problem (7) exists and is unique. This minimizer is obtained as the solution of {a mathematical formula}∂L∂gc=0 for {a mathematical formula}c=1,…,m:{a mathematical formula} Since {a mathematical formula}λ1−Pc−λ&gt;0, we have {a mathematical formula}0&lt;λ&lt;1−Pc for {a mathematical formula}c=1,…,m. We thus have {a mathematical formula}gˆl&gt;gˆj if and only if {a mathematical formula}Pl&gt;Pj. Moreover, we obtain (8).</paragraph><paragraph>Let {a mathematical formula}l=argmaxcPc. It then follows from {a mathematical formula}∑c=1mgˆc=0 that {a mathematical formula}gˆl&gt;0, and hence,{a mathematical formula} This implies {a mathematical formula}limT→0⁡λ=1−Pl. As a result, we have {a mathematical formula}limT→0⁡gˆc=−1 for {a mathematical formula}c≠l and {a mathematical formula}limT→0⁡gˆl=m−1 due to {a mathematical formula}∑c=1mgˆc=0.</paragraph></section><section label="B.4">The proof of Theorem 10<paragraph>We prove the theorem according to Corollary 4 where {a mathematical formula}ψc(g)=f(gc)=GT(g(x),c). It is directly calculated that for {a mathematical formula}j≠c,{a mathematical formula}{a mathematical formula}fjj″(gc)=βjc(1−βjc) and {a mathematical formula}fjk″(gc)=0 for {a mathematical formula}k≠j,c. Thus, the Hessian matrix {a mathematical formula}Bc=[fjk″(gc)]j,k≠c is positive definite. As a result, Corollary 4 shows that the minimizer {a mathematical formula}gˆ exists and is unique.</paragraph><paragraph>Additionally, it is always satisfied that {a mathematical formula}∂ψc(g)∂gc=−∑j≠cfj′(gc)&lt;0 for {a mathematical formula}j≠c. Thus, we obtain that {a mathematical formula}Pl&gt;Pk implies {a mathematical formula}gˆl&gt;gˆk from Corollary 4.</paragraph><paragraph>Recall that the minimizer {a mathematical formula}gˆc satisfies the condition of{a mathematical formula} where {a mathematical formula}βcj is defined via the {a mathematical formula}gˆc and we still denote them by the {a mathematical formula}βcj for simplicity. By the implicit function theorem, obviously, {a mathematical formula}gˆc is a continuous function of T on {a mathematical formula}(0,∞). Thus, its limit at {a mathematical formula}T=0 is bounded due to {a mathematical formula}∑c=1mgˆc=0 and the boundedness of the {a mathematical formula}gˆc. Without loss of generality, we assume that {a mathematical formula}P1&gt;P2≥⋯≥Pm. In this case, we always have {a mathematical formula}limT→0⁡gˆ1≥limT→0⁡gˆ2≥⋯≥limT→0⁡gˆm,{a mathematical formula} If {a mathematical formula}limT→0⁡gˆ1&gt;limT→0⁡gˆ2+1 had been satisfied, we would obtain{a mathematical formula} On the other hand, if {a mathematical formula}limT→0⁡gˆ1&lt;limT→0⁡gˆ2+1 had been satisfied, we would obtain{a mathematical formula}{a mathematical formula} Therefore, we obtain that {a mathematical formula}limT→0⁡gˆ1=limT→0⁡gˆ2+1 whenever {a mathematical formula}P1&gt;12 or {a mathematical formula}P2&lt;1m.</paragraph></section><section label="B.5">The proof of Theorems 13 and 14<paragraph>We also prove the theorem in terms of Corollary 4. In the current case we have {a mathematical formula}ψc(g)=f(gc)=CT(g(x),c). We take computations for {a mathematical formula}j≠c as{a mathematical formula}{a mathematical formula}fjj″(gc)=βjc(1−βjc)/T and {a mathematical formula}fjk″(gc)=βjcβjk/T for {a mathematical formula}k≠l,c.</paragraph><paragraph>We denote {a mathematical formula}Δm=diag(β1m,…,βm−1m) and {a mathematical formula}βm=(β1c,…,βm−1m)T. Then the Hessian matrix {a mathematical formula}Bm is{a mathematical formula} Since {a mathematical formula}∑k≠j,mβjkβkm&lt;βjm(1−βjm), the matrix {a mathematical formula}Bm is strictly diagonally dominant. Thus, {a mathematical formula}Bm is positive definite. It then follows from Corollary 4 that the minimizer {a mathematical formula}gˆ exists and is unique. Noting that {a mathematical formula}∂ψc(g)∂gc=−∑j≠cβjc&lt;0, we further obtain that {a mathematical formula}Pl&gt;Pk implies {a mathematical formula}gˆl&gt;gˆk.</paragraph><paragraph>Since {a mathematical formula}gˆ is the solution of the optimization problem in question, it should satisfy the first-order condition:{a mathematical formula} from which we get{a mathematical formula} From (22) and using the fact that {a mathematical formula}∑c=1mPc=1, we have (16).</paragraph><paragraph>We now consider the proof of Theorem 14. First, it is clear that {a mathematical formula}gˆc is continuous in T on {a mathematical formula}(0,∞), so its limit at {a mathematical formula}T=0 exists (∞ allowed). For notational simplicity, we just use the {a mathematical formula}gc instead of the {a mathematical formula}gˆc(x). Second, the above proof shows that {a mathematical formula}λ=0. And {a mathematical formula}∂L∂gc=0 yields {a mathematical formula}Pc=∑j=1mβcjPj. Namely, for {a mathematical formula}c=1,…,m,{a mathematical formula} Let {a mathematical formula}l=argmaxcPc and {a mathematical formula}k=argmincPc. We thus have that {a mathematical formula}limT→0⁡gk≤limT→0⁡gj≤limT→0⁡gl for {a mathematical formula}j≠k,l. Note that{a mathematical formula} The first term of the right-hand side of the above equation approaches 0 at {a mathematical formula}T→0 due to {a mathematical formula}limT→0⁡u+gi−gkT=+∞ for {a mathematical formula}u&gt;0. If there were {a mathematical formula}i≠k,l such that {a mathematical formula}limT→0⁡(gi−gk)&gt;0, we would have that the right-hand side of the above equation approaches 0 at {a mathematical formula}T→0. This implies that {a mathematical formula}limT→0⁡(gi−gk)=0 and {a mathematical formula}limT→0⁡(gi−gl)≤0 for any {a mathematical formula}i≠l.</paragraph><paragraph>On the other hand, take{a mathematical formula} We are able to show that {a mathematical formula}limT→0⁡(u+gi−gl)&lt;0 cannot be satisfied, otherwise the first term is current is {a mathematical formula}Pl and the second term is {a mathematical formula}1−Pl.</paragraph><paragraph label="Case 1">{a mathematical formula}Pl&gt;1/2. We can also obtain that {a mathematical formula}limT→0⁡(u+gi−gl)&gt;0 for any {a mathematical formula}i≠l cannot be satisfied, because the first term of the right-hand side of the above equation is 0 and the second term is always less than 1/2 otherwise. Thus, we have {a mathematical formula}limT→0⁡(u+gi−gl)=0 for any {a mathematical formula}i≠l. As a result, {a mathematical formula}limT→0⁡gl=u(m−1)/m and {a mathematical formula}limT→0⁡gli=−u/m for {a mathematical formula}i≠l due to {a mathematical formula}limT→0⁡∑i=1mgi=0.</paragraph><paragraph label="Case 2">{a mathematical formula}Pl&lt;1/2. In this case, we always have {a mathematical formula}limT→0⁡(gi−gl)=0. Otherwise, the second term is {a mathematical formula}1−Pl which is greater than 1/2.</paragraph></section></section><section label="Appendix C">The properties of the multiclass {a mathematical formula}C-loss<paragraph>We first prove that {a mathematical formula}CT,u(g(x),c) is convex. From Appendix B.5, we can obtain the Hessian matrix of {a mathematical formula}CT,u(g(x),c) w.r.t. {a mathematical formula}g(x). That is,{a mathematical formula} where {a mathematical formula}Δc=diag(β1c,…,βmc) and {a mathematical formula}βc=(β1c,…,βmc)T. We also have from Appendix B.5 that {a mathematical formula}Hc is positive semidefinite (in fact, it is conditionally positive definite). Thus, {a mathematical formula}CT,u(g(x),c) is convex.</paragraph><section label="C.1">The proof of Proposition 11<paragraph>Noting that{a mathematical formula} we have {a mathematical formula}CT,1(g(x),c)&lt;GT(g(x),c).</paragraph><paragraph>Now assume that {a mathematical formula}l=argmaxj{gj(x)+u−uI{j=c}}. Then{a mathematical formula}</paragraph></section><section label="C.2">The proof of Proposition 12<paragraph>First, it is easily obtained that {a mathematical formula}limT→∞⁡ωjc(x)=1m. Second, consider that{a mathematical formula} It immediately follows from {a mathematical formula}Hu(g(x),c)≤CT,u(g(x),c)≤Hu(g(x),c)+Tlog⁡(m) that {a mathematical formula}limT→0⁡CT,u(g(x),c)=Hu(g(x),c).</paragraph><paragraph>Third, the derivative of {a mathematical formula}CT,u(g(x),c) w.r.t. T is given by{a mathematical formula} Thus, {a mathematical formula}CT,u(g(x),c) is an increasing function of T.</paragraph></section></section><section label="Appendix D"><section-title>The learning algorithm for MCL</section-title><paragraph>For simplicity, we only consider the learning algorithm based on (17). Let{a mathematical formula} where {a mathematical formula}λi's are Lagrangian multipliers, and calculate{a mathematical formula}{a mathematical formula} where {a mathematical formula}wij=wjci(xi) is defined in (13), and {a mathematical formula}eij=1 if {a mathematical formula}j=ci and {a mathematical formula}eij=0 otherwise. It follows from {a mathematical formula}∑j=1m∂L∂bj=0 and {a mathematical formula}∑j=1m∂L∂aj=0 that{a mathematical formula}{a mathematical formula} where {a mathematical formula}b¯=1m∑j=1mbj. Thus, the Lagrangian multipliers {a mathematical formula}λi are automatically eliminated. Denoting {a mathematical formula}ei=(ei1,…,eim)T, {a mathematical formula}wi=(wi1,…,wim)T, {a mathematical formula}a=(a1,…,am)T, {a mathematical formula}B=[b1,…,bm] and {a mathematical formula}vec(B)T=(b1T,…,bmT), we have{a mathematical formula}{a mathematical formula} where {a mathematical formula}A⊗B denotes the Kronecker product of matrices A and B, and {a mathematical formula}Cm=Im−1m1m1mT is the {a mathematical formula}m×m centering matrix. We now use the Newton–Raphson method to alternatively solve the nonlinear equation systems in (23) and (24). Since the Hessian matrices are positive semidefinite, the method converges. Considering that the Newton–Raphson method requires inverting the Hessian matrix in each iteration, we employ a quadratic lower bound algorithm [2]. In particular,{a mathematical formula} where {a mathematical formula}A⪯M means {a mathematical formula}A−M is positive semidefinite and we use the fact that {a mathematical formula}(diag(wi)−wiwiT)⪯12Cm. We use the pseudoinverse of {a mathematical formula}Cm (which is itself), and thus we need to invert {a mathematical formula}Ip+γ2nXTX only once.</paragraph></section><section label="Appendix E">The proof of Theorem 15<paragraph>Consider that the first-order derivative of {a mathematical formula}CT,1(g(x),c) w.r.t. {a mathematical formula}gj(x) is{a mathematical formula} where{a mathematical formula} Given a {a mathematical formula}g¯=(g¯1,…,g¯m)T∈G, we denote {a mathematical formula}J={j≠c:(1+g¯j−g¯c)=ξc(g¯)≜maxl⁡(1+g¯l−g¯c−I{l=c})} and {a mathematical formula}k=|J|. It is directly obtained that{a mathematical formula} if {a mathematical formula}k≠0 and {a mathematical formula}maxl≠c⁡(1+g¯l−g¯c)&gt;0, and that{a mathematical formula} if {a mathematical formula}k≠0 and {a mathematical formula}maxl≠c⁡(1+g¯l−g¯c)=0. On the other hand, let {a mathematical formula}∂ξc(g¯) be the subdifferential of {a mathematical formula}ξc at {a mathematical formula}g¯. Assume that {a mathematical formula}maxl≠c⁡(1+g¯l−g¯c)=0. For any {a mathematical formula}z=(z1,…,zm)∈∂ξc(g¯), if and only if we have {a mathematical formula}zj∈[0,1] if {a mathematical formula}j∈J, {a mathematical formula}zj=0 if {a mathematical formula}j∉J and {a mathematical formula}j≠c, and {a mathematical formula}zj=−∑l∈Jzl if {a mathematical formula}j=c. This implies that{a mathematical formula} Accordingly, we conclude the theorem.</paragraph></section><section label="Appendix F"><section-title>Derivation of multiclass GentleBoost algorithm</section-title><paragraph>The empirical risk over the training data is given by{a mathematical formula} Let {a mathematical formula}h(x)=(h1(x),…,hm(x))T∈G be the increments. Following the derivation of the LogitBoost algorithm, we consider the second-order Taylor expansion of {a mathematical formula}e(g+h) around g and employ a diagonal approximation to the Hessian as{a mathematical formula} where{a mathematical formula} For each j, one can find {a mathematical formula}hj(x) by minimizing{a mathematical formula} The solution is obtained by fitting the regression function {a mathematical formula}hj(x) based on weighted least-squares of {a mathematical formula}zij to {a mathematical formula}xi with wights {a mathematical formula}wij. We thus have the algorithm in Algorithm 1.</paragraph></section></appendices><references><reference label="[1]"><authors>P.L. Bartlett,M.I. Jordan,J.D. McAuliffe</authors><title>Convexity, classification, and risk bounds</title><host>J. Am. Stat. Assoc.101 (473)(2006) pp.138-156</host></reference><reference label="[2]"><authors>D. Böhning</authors><title>Multinomial logistic regression algorithm</title><host>Ann. Inst. Stat. Math.44 (1)(1992) pp.197-200</host></reference><reference label="[3]"><authors>E.J. Bredensteiner,K.P. Bennett</authors><title>Multicategory classification by support vector machines</title><host>Comput. Optim. Appl.12 (1999) pp.35-46</host></reference><reference label="[4]"><authors>C. Cortes,V. Vapnik</authors><title>Support-vector networks</title><host>Mach. Learn.20 (1995) pp.273-297</host></reference><reference label="[5]"><authors>K. Crammer,Y. Singer</authors><title>On the algorithmic implementation of multiclass kernel-based vector machines</title><host>J. Mach. Learn. Res.2 (2001) pp.265-292</host></reference><reference label="[6]"><authors>M. Craven,D. Dopasquo,D. Freitag,A. McCallum,T. Mitchell,K. Nigam,S. Slattery</authors><title>Learning to extract symbolic knowledge from the World Web Wide</title><host>The Fifteenth National Conference on Artificial Intelligence(1998)</host></reference><reference label="[7]"><authors>Y. Freund</authors><title>Boosting a weak learning algorithm by majority</title><host>Inf. Comput.21 (1995) pp.256-285</host></reference><reference label="[8]"><authors>Y. Freund,R.E. Schapire</authors><title>A decision-theoretic generalization of on-line learning and an application to boosting</title><host>J. Comput. Syst. Sci.55 (1)(1997) pp.119-139</host></reference><reference label="[9]"><authors>J.H. Friedman,T. Hastie,R. Tibshirani</authors><title>Additive logistic regression: a statistical view of boosting</title><host>Ann. Stat.28 (2)(2000) pp.337-374</host></reference><reference label="[10]"><authors>J. Friedman,T. Hastie,R. Tibshirani</authors><title>Regularization paths for generalized linear models via coordinate descent</title><host>J. Stat. Softw.33 (1)(2010) pp.1-22</host></reference><reference label="[11]"><authors>T. Gao,D. Koller</authors><title>Multiclass boosting with hinge loss based on output coding</title><host>Proceedings of the 22nd International Conference on Machine Learning (ICML)(2010)</host></reference><reference label="[12]"><authors>Y. Guermeur</authors><title>Combining discriminant models with new multi-class SVMs</title><host>Pattern Anal. Appl.5 (2)(2002) pp.168-179</host></reference><reference label="[13]"><authors>Y. Lee,Y. Lin,G. Wahba</authors><title>Multicategory support vector machines: theory and application to the classification of microarray data and satellite radiance data</title><host>J. Am. Stat. Assoc.99 (465)(2004) pp.67-81</host></reference><reference label="[14]"><authors>Y. Liu</authors><title>Fisher consistency of multicategory support vector machines</title><host>The Eleventh International Conference on Artificial Intelligence and Statistics(2007) pp.289-296</host></reference><reference label="[15]"><authors>Y. Liu,X. Shen</authors><title>Multicategory ψ-learning</title><host>J. Am. Stat. Assoc.101 (474)(2006) pp.500-509</host></reference><reference label="[16]"><authors>Y. Liu,H.H. Zhang,Y. Wu</authors><title>Hard or soft classification? Large-margin unified machines</title><host>J. Am. Stat. Assoc.106 (493)(2011) pp.166-177</host></reference><reference label="[17]"><authors>I. Mukherjee,R. Schapire</authors><title>A theory of multiclass boosting</title><host>Advances in Neural Information Processing Systems (NIPS)vol. 24 (2010)</host></reference><reference label="[18]"><authors>J.M. Ortega,W.C. Rheinboldt</authors><title>Iterative Solution of Nonlinear Equations in Several Variables</title><host>(2000)SIAMPhiladelphia</host></reference><reference label="[19]">X. Qiao,L. ZhangFlexible high-dimensional classification machines and their asymptotic propertiesTechnical report<host>arXiv:1310.3004(2013)</host></reference><reference label="[20]"><authors>K. Rose,E. Gurewitz,G.C. Fox</authors><title>Statistical mechanics and phase transitions in clustering</title><host>Phys. Rev. Lett.65 (1990) pp.945-948</host></reference><reference label="[21]"><authors>M. Saberian,N. Vasconcelos</authors><title>Multiclass boosting: theory and algorithms</title><host>Advances in Neural Information Processing Systems (NIPS)vol. 25 (2011)</host></reference><reference label="[22]"><authors>R.E. Schapire,Y. Singer</authors><title>Improved boosting algorithms using confidence-rated predictions</title><host>Mach. Learn.37 (1999) pp.297-336</host></reference><reference label="[23]"><authors>I. Steinwart,C. Scovel</authors><title>Fast rates for support vector machines using Gaussian kernels</title><host>Ann. Stat.35 (2)(2007) pp.575-607</host></reference><reference label="[24]"><authors>A. Tewari,P.L. Bartlett</authors><title>On the consistency of multiclass classification methods</title><host>J. Mach. Learn. Res.8 (2007) pp.1007-1025</host></reference><reference label="[25]"><authors>V. Vapnik</authors><title>Statistical Learning Theory</title><host>(1998)John Wiley and SonsNew York</host></reference><reference label="[26]"><authors>J. Weston,C. Watkins</authors><title>Support vector machines for multiclass pattern recognition</title><host>The Seventh European Symposium on Artificial Neural Networks(1999) pp.219-224</host></reference><reference label="[27]"><authors>Y. Wu,Y. Liu</authors><title>Robust truncated-hinge-loss support vector machines</title><host>J. Am. Stat. Assoc.102 (479)(2007) pp.974-983</host></reference><reference label="[28]"><authors>T. Zhang</authors><title>Statistical analysis of some multi-category large margin classification methods</title><host>J. Mach. Learn. Res.5 (2004) pp.1225-1251</host></reference><reference label="[29]"><authors>Z. Zhang,G. Wang,D.-Y. Yeung,G. Dai,F. Lochovsky</authors><title>A regularization framework for multiclass classification: a deterministic annealing approach</title><host>Pattern Recognit.43 (7)(2010) pp.2466-2475</host></reference><reference label="[30]"><authors>Z. Zhang,D. Liu,G. Dai,M.I. Jordan</authors><title>Coherence functions with applications in large-margin classification methods</title><host>J. Mach. Learn. Res.13 (2012) pp.2705-2734</host></reference><reference label="[31]"><authors>J. Zhu,T. Hastie</authors><title>Classification of gene microarrays by penalized logistic regression</title><host>Biostatistics5 (3)(2004) pp.427-443</host></reference><reference label="[32]"><authors>J. Zhu,H. Zou,S. Rosset,T. Hastie</authors><title>Multi-class Adaboost</title><host>Stat. Interface2 (2009) pp.349-360</host></reference><reference label="[33]"><authors>H. Zou,J. Zhu,T. Hastie</authors><title>New multicategory boosting algorithms based on multicategory Fisher-consistent losses</title><host>Ann. Appl. Stat.2 (4)(2008) pp.1290-1306</host></reference></references><footnote><note-para label="1">A symmetric function of p variables is one whose value at any p-tuple of arguments is the same as its value at any permutation of that p-tuple.</note-para></footnote></root>