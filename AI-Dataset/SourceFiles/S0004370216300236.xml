<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370216300236</url><title>Belief and truth in hypothesised behaviours</title><authors>Stefano V. Albrecht,Jacob W. Crandall,Subramanian Ramamoorthy</authors><abstract>There is a long history in game theory on the topic of Bayesian or “rational” learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.</abstract><keywords>Autonomous agents;Multiagent systems;Game theory;Type-based method</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>There is a long history in game theory on the topic of Bayesian or “rational” learning (e.g. [73], [35], [65], [63]). Therein, players maintain beliefs about the behaviours, or “types”, of other players in the form of a probability distribution over a set of alternative types. These beliefs are updated based on the observed actions, and each player chooses an action which is expected to maximise the payoffs received by the player, given the current beliefs of the player. The principal questions studied in this context are the degree to which players can learn to make correct predictions, and whether the interaction process converges to solutions such as Nash equilibrium [74].</paragraph><paragraph>This general idea, which we here refer to as the type-based method, has received increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents (e.g. [4], [11], [52], [25]). This interest is, in part, motivated by applications that require efficient and flexible interaction with agents whose behaviours are initially unknown. Example applications include adaptive user interfaces, robotic elderly care, and automated trading agents. Learning to interact from scratch in such settings is notoriously difficult, due to the essentially unconstrained nature of what the other agents may be doing and the fact that their behaviours are a priori unknown. The type-based method is seen as a way to reduce the complexity of such problems by focusing on a relatively small set of points in the infinite space of possible behaviours.</paragraph><paragraph>More concretely, the idea is to hypothesise (“guess”) a set of types, each of which specifies a possible behaviour for the other agents. A type may be of any structural form, and here we simply view it as a “blackbox” programme which takes as input the interaction history and chooses actions for the next step in the interaction. Such types may be specified manually by a domain expert or generated automatically, e.g. from a corpus of historical data or the problem description. By comparing the predictions of the types with the observed actions of the agents, we can form posterior beliefs about the relative likelihood of types. The beliefs and types are in turn utilised in a planning procedure to find an action which maximises our expected payoffs with respect to our beliefs. A useful feature of this method is the fact that we may hypothesise any types of behaviours, which gives us the flexibility to interact with a variety of agents. Moreover, since each type specifies a complete behaviour, we can plan actions in the entire interaction space, including in situations that have not been encountered before.</paragraph><paragraph>Nonetheless, there are several questions and concerns associated with this method, pertaining to the evolution and impact of beliefs as well as the implications and detection of incorrect hypothesised types. Specifically, how should evidence (i.e. observed actions) be incorporated into beliefs and under what conditions will the beliefs be correct? What impact do prior beliefs have on our ability to maximise payoffs in the long-term? Furthermore, under what conditions will we be able to complete our task even if our hypothesised types are incorrect? And, finally, how can we ascertain the correctness of our hypothesised types during the interaction?</paragraph><paragraph>The AI literature on the type-based method has focused on experimental evaluations, exploration mechanisms, and computational issues arising from recursive beliefs, but not or only partially on the questions outlined above. (We defer a detailed discussion of related works to Section 2). On the other hand, the game theory literature addresses such questions primarily in the context of equilibrium attainment in repeated games (cf. Section 2). However, there are several reasons why this renders the game theory literature of limited applicability to domains such as the ones mentioned earlier. First, equilibrium concepts such as Nash equilibrium are based on normative assumptions, including perfect rationality with respect to one's payoffs. However, such normative assumptions are difficult to justify in situations in which we assume no prior knowledge about the behaviour of other agents. For example, there is evidence that humans do not satisfy such strict assumptions (e.g. [64]). Second, an equilibrium solution prescribes behaviours for all involved agents, whereas we control only a single agent and assume no control over the choice of behaviour for the other agents. Finally, the existence of multiple equilibria with possibly differing payoff profiles means that equilibrium attainment may itself not be synonymous with payoff maximisation for our controlled agent.</paragraph><paragraph>The purpose of the present article is to improve our understanding of the type-based method by providing insight into the questions outlined above. Our analysis is based on stochastic Bayesian games, which are an extension of Bayesian games [56] that include stochastic state transitions, and Harsanyi–Bellman Ad Hoc Coordination (HBA), which can be viewed as a general algorithmic description of the type-based method [4]. After discussing related work in Section 2 and technical preliminaries in Section 3, the article makes the following contributions:</paragraph><list><list-item label="•">Section4 considers three basic methods to incorporate observations into posterior beliefs and analyses the conditions under which they converge to the true distribution of types, including in processes in which type assignments may be randomised and correlated. We also discuss examples to show when beliefs may fail to converge to the correct distribution.</list-item><list-item label="•">Section5 investigates the impact of prior beliefs on payoff maximisation in a comprehensive empirical study. We show that prior beliefs can indeed have a significant impact on the long-term performance of HBA, and that the magnitude of the impact depends on the depth of the planning horizon (i.e. how far we look into the future). Moreover, we show that automatic methods can compute prior beliefs with consistent performance effects.</list-item><list-item label="•">Section6 analyses what relation the hypothesised types must have to the true types in order for HBA to be able to complete its task, despite inaccuracies in the hypothesised types. We formulate a hierarchy of increasingly desirable termination guarantees and analyse the conditions under which they are met. In particular, we give a novel characterisation of optimality which is based on the concept of probabilistic bisimulation [69].</list-item><list-item label="•">Section7 shows how the truth of hypothesised types can be contemplated during the interaction in the form of an automated statistical analysis. The presented algorithm can incorporate multiple statistical features into the test statistic and learns its distribution during the interaction process, with asymptotic correctness guarantees. We show in a comprehensive set of experiments that the algorithm achieves high accuracy and scalability at low computational costs.</list-item></list><paragraph> Finally, Section 8 concludes this work and discusses directions for future work. Elements of this work appeared in [2], [7], [6], [5].</paragraph></section><section label="2"><section-title>Related work</section-title><paragraph>This section discusses related work and situates our work within the literature. We distinguish between research on the type-based method in the areas of game theory and artificial intelligence.</paragraph><section label="2.1"><section-title>Type-based method in game theory</section-title><paragraph>Perhaps the earliest formulation of the type-based method was in the form of Bayesian games[56], [57], [58]. Bayesian games were introduced to address incomplete information games, in which certain aspects of the game are known to some players and unknown to others. Harsanyi proposed to model this “private information” as types: every player has one of a number of types which govern the player's behaviour,{sup:1} and the assignment of types is governed by some distribution over types. By assuming that the type spaces and distribution are common knowledge, this reduces the incomplete information game to a complete (but imperfect) information game, admitting a solution in the form of the Bayesian Nash equilibrium. While this idea was controversial at the time,{sup:2} Bayesian games have become a firm part of game theory.</paragraph><paragraph>The model used in our work builds on Bayesian games but includes stochastic state transitions, making it more naturally applicable to many problems of interest in artificial intelligence. This also allows us to define concisely what it means to complete a task, namely to drive the game from an initial state into a terminal state. Moreover, in contrast to Bayesian games, we explicitly consider cases in which the type spaces and distribution are unknown to our agent, and we do not assume that other agents necessarily use a type-based reasoning or have common prior beliefs.</paragraph><paragraph>Much work in game theory has focused on equilibrium attainment as the result of learning through repeated interaction, in games in which players maintain Bayesian beliefs about the strategies of other players. In the seminal work of Kalai and Lehrer [65], the authors show that under a certain assumption about players' beliefs called “absolute continuity” (essentially, every event that has true positive probability is assigned positive probability under the player's belief), players prediction of future play will become arbitrarily close to the true future play. A related result was shown by Jordan [63] for myopic players which consider only immediate payoffs. In Section 4, we show that the convergence result of Kalai and Lehrer [65] carries over to our model, and we also provide convergence results for different formulations of posterior beliefs which can recognise randomised and correlated type assignments.</paragraph><paragraph>In addition to posterior beliefs, it has been shown that prior beliefs are intimately connected to the equilibrium solution that emerges as a result of learning. For example, Nyarko [76] use a similar but weaker condition than absolute continuity and show that the resulting subjective equilibrium may not be a Nash equilibrium if the players have different prior beliefs. Similarly, Dekel et al. [35] show under certain conditions that learning without common prior beliefs may converge to a self-confirming equilibrium [48] which is not a Nash equilibrium. While important in the context of equilibrium attainment, these results are less applicable to our focus on individual payoff maximisation and task completion (cf. Section 1). In Section 5, we show that prior beliefs can, nevertheless, have a significant impact on our ability to maximise payoffs in the long-term. Moreover, our results indicate that prior beliefs can be computed automatically with consistent performance effects.</paragraph><paragraph>The possibility of discrepancies between predicted and true behaviour has been recognised in works such as [73], [46], [72]. Essentially, these works show for certain games and conditions that players maintaining beliefs over behaviours cannot simultaneously make correct predictions and play optimally with respect to their beliefs. In Section 6, we consider the impact of incorrect hypothesised types on our ability to complete tasks and show that a certain form of optimality is preserved under a bisimulation relation, which can be verified in practice. Furthermore, in Section 7 we describe an automatic statistical analysis to allow an agent to contemplate the correctness of its behavioural hypotheses.</paragraph></section><section label="2.2"><section-title>Type-based method in artificial intelligence</section-title><paragraph>There is a substantial body of work in the AI literature on coordination (e.g. [66], [86], [53]) and learning (e.g. [28], [61], [20], [70]) in multiagent systems. However, it has been noted (e.g. [83]) that many of these methods depend on some form of prior coordination between agents. The type-based method has been studied as an alternative method of interaction with agents whose behaviours are initially unknown.</paragraph><paragraph>Barrett et al. [11] implement a variant of the type-based method in the “pursuit” grid-world domain and demonstrate its practical potential. Albrecht and Ramamoorthy [4] introduce a general algorithm called Harsanyi–Bellman Ad Hoc Coordination (HBA) (cf. Section 3) and evaluate it in the “level-based foraging” grid-world domain and in matrix games played against humans. Both works propose various implementations of the type-based method, including tree expansion, dynamic programming, and reinforcement learning with stochastic sampling.</paragraph><paragraph>Carmel and Markovitch [25] define types as deterministic finite state machines and study optimal exploration in repeated games. Similarly, Chalkiadakis and Boutilier [26] use types in the context of multiagent reinforcement learning and develop exploration methods based on the concept of “value of information” [60]. Their work is essentially an extension of Dearden et al. [34], which study the related idea of maintaining Bayesian beliefs over a set of environment models in reinforcement learning.</paragraph><paragraph>Southey et al. [82] apply the type-based method to variants of the poker game. The poker domain differs from the above works in that the state of the interaction process (i.e. player hands) is only partially observable. The authors show how beliefs can be maintained in this setting and compare various methods to compute optimal responses with respect to beliefs.</paragraph><paragraph>In interactive partially observable Markov decision processes (I-POMDPs) [52], agents make decisions in the presence of uncertainty regarding the state of the environment, the types of other agents, and their action choices. Several solution methods have been developed for I-POMDPs (e.g. [41], [38], [39]) and there have been attempts to apply I-POMDPs in practice (e.g. [40], [75]). An interesting parallel to our work is that the convergence result of Kalai and Lehrer [65] has also been extended to I-POMDPs [37].</paragraph><paragraph>Bowling and McCracken [19] use “play books” to control a single agent in a team of agents. Plays are similar to types but specify behaviours for a complete team and include additional structure such as applicability and termination conditions, and roles for each agent. Similarly, “plan libraries” have been used to infer an agent's goals [24], [27]. Plans resemble types but may include intricate structure such as temporal and causal orderings, and grammars [84], [50].</paragraph><paragraph>The above works investigate various aspects of the type-based method, but they do not or only partially address the questions outlined in Section 1. Specifically, most of the above works use a posterior formulation in which the likelihood is defined as a product of action probabilities. In Section 4, we show under what conditions this formulation will produce correct and incorrect beliefs, and we also investigate alternative posterior formulations. Moreover, only Chalkiadakis and Boutilier [26] consider the effects of prior beliefs by comparing “uninformed” (i.e. uniform) and “informed” (uniform with narrowed support) prior beliefs, but they provide no detailed analysis. In Section 5, we investigate how prior beliefs affect our ability to maximise payoffs in the long-term and how they can be computed automatically. Finally, none of the above works consider the implications and detection of incorrect hypothesised types.</paragraph></section></section><section label="3"><section-title>Model and algorithm</section-title><paragraph>This section introduces the general model and algorithm used in our work, and further elaborates on connections to other related works.</paragraph><section label="3.1"><section-title>Stochastic Bayesian game</section-title><paragraph>We model the interaction process as a stochastic Bayesian game (SBG) [4] which can be viewed as a combination of the Bayesian game [56] and the stochastic game [81]. This combination is useful because it allows us to study the type-based method of interaction via the established framework of Bayesian games while also providing a means to specify an environment (via states) and the task to be completed. The structural definition of SBGs is as follows:</paragraph><paragraph label="Definition 1">A stochastic Bayesian game (SBG) consists of:</paragraph><list><list-item label="•">finite state space S with initial state {a mathematical formula}s0∈S and terminal states {a mathematical formula}S¯⊂S</list-item><list-item label="•">players {a mathematical formula}N={1,…,n} and for each {a mathematical formula}i∈N:</list-item><list-item label="•">state transition function {a mathematical formula}T:S×A×S→[0,1]</list-item><list-item label="•">type distribution {a mathematical formula}ϒ:Θ+→[0,1], where {a mathematical formula}Θ+ is a finite subset of Θ</list-item></list><paragraph>A SBG defines the interaction process as follows:</paragraph><paragraph label="Definition 2">A SBG starts at time {a mathematical formula}t=0 in state {a mathematical formula}s0:</paragraph><list><list-item label="1.">In state {a mathematical formula}st, the types {a mathematical formula}θ1t,…,θnt are sampled from {a mathematical formula}Θ+ with probability {a mathematical formula}ϒ(θ1t,…,θnt), and each player i is informed only about its own type {a mathematical formula}θit.</list-item><list-item label="2.">Based on the history {a mathematical formula}Ht, each player i chooses an action {a mathematical formula}ait∈Ai with probability {a mathematical formula}πi(Ht,ait,θit), resulting in the joint action {a mathematical formula}at=(a1t,…,ant).</list-item><list-item label="3.">Each player i receives an individual payoff given by {a mathematical formula}ui(st,at,θit), and the game transitions into a successor state {a mathematical formula}st+1∈S with probability {a mathematical formula}T(st,at,st+1).</list-item></list><paragraph>Throughout this work, we will use the contextual notation {a mathematical formula}Hτ to denote the τ-prefix of {a mathematical formula}Ht (i.e. {a mathematical formula}Hτ is the initial segment of {a mathematical formula}Ht up until state {a mathematical formula}sτ, with {a mathematical formula}τ≤t). Similarly, we use {a mathematical formula}sτ and {a mathematical formula}aτ to denote the respective τ-elements of {a mathematical formula}Ht.</paragraph><paragraph>The set S can be used to specify the environment within which the players interact, where each state {a mathematical formula}s∈S is a specific configuration of the environment. For instance, the environment may be a maze in a two-dimensional grid and the states may specify the positions of players and walls. The task in the SBG is to drive the interaction process from the initial state to a terminal state. Once a terminal state is reached, we say that the task is completed.</paragraph><paragraph>The type space {a mathematical formula}Θi contains all possible behaviours for player i. Each type {a mathematical formula}θi∈Θi corresponds to a complete behaviour for player i by specifying its preferences, via {a mathematical formula}ui, and the way in which it chooses actions, via {a mathematical formula}πi (see also Footnote 1). We place no restrictions on the behaviours that players can exhibit; in particular, each player can make decisions based on the entire history {a mathematical formula}Ht. This includes behaviours that learn and change over time. In practice, it is useful to view a type as a blackbox programme which, through {a mathematical formula}πi, takes as input the current interaction history and returns probabilities for each action available to the player. (Sections 4 and 5 provide various examples of types; see also [4] for examples of types in complex SBGs.)</paragraph><paragraph>The types are assigned during the game via the type distribution ϒ. In this work, we consider two classes of types distributions:</paragraph><paragraph label="Definition 3">A type distribution ϒ is called pure if {a mathematical formula}∃θ∈Θ+:ϒ(θ)=1. A type distribution which is not pure is called mixed.</paragraph><paragraph>Pure type distributions specify one fixed type for each player, throughout the game. This is what we would normally expect, since it means that each player has a single coherent behaviour. However, there are cases in which it may make sense to assume a mixed type distribution. For example, Albrecht and Ramamoorthy [4] used a mixed type distribution in their human-machine experiments to allow for the possibility that human subjects may change between several simple types (as opposed to defining one complex type which includes the simple types).</paragraph><paragraph>Note that the type space {a mathematical formula}Θi is uncountable because the strategy {a mathematical formula}πi assigns probabilities to actions, and the interval {a mathematical formula}[0,1] is itself uncountable. Therefore, in order for ϒ to be a well-defined probability distribution, we define it over a finite or countable subset {a mathematical formula}Θ+⊂Θ. (Otherwise, ϒ would need to be defined as a density.) To differentiate the two spaces, we sometimes refer to {a mathematical formula}Θi as the full type space and to {a mathematical formula}Θi+ as the true types of player i. For convenience (and by abuse of notation), we will allow {a mathematical formula}ϒ(θ) for any {a mathematical formula}θ∈Θ, with {a mathematical formula}ϒ(θ)=0 if {a mathematical formula}θ∉Θ+.</paragraph></section><section label="3.2"><section-title>Harsanyi–Bellman Ad Hoc Coordination</section-title><paragraph>As outlined in Section 1, we consider a single agent which employs the type-based method to interact with other agents with unknown behaviours. Throughout this work, we use Harsanyi–Bellman Ad Hoc Coordination (HBA) [4] as a general algorithmic description of the type-based method. Algorithm 1 provides a formal definition of HBA.</paragraph><paragraph>Given a SBG Γ, we use i to denote our player and j and −i to denote the other players (such as in {a mathematical formula}A−i=×j≠iAj). The behaviour of player i is completely specified by HBA. In other words, i has a single fixed type, {a mathematical formula}Θi+={θiHBA}, where {a mathematical formula}θiHBA is defined by Algorithm 1. Thus, we may omit {a mathematical formula}θiHBA in {a mathematical formula}ui and {a mathematical formula}πi for compactness. The behaviour of the other players is governed by ϒ and a priori unknown to us. Formally, we assume that all elements of Γ are known to us except for {a mathematical formula}Θj+ and ϒ, which are latent elements.</paragraph><paragraph>In HBA, these latent elements are essentially substituted for by the hypothesised type space {a mathematical formula}Θj⁎ and the posterior belief Pr, respectively. Like {a mathematical formula}Θj+, {a mathematical formula}Θj⁎ is a finite or countable subset of the full type space {a mathematical formula}Θj. The posterior belief (probability) {a mathematical formula}Pr(θ−i⁎|Ht) quantifies the relative likelihood that players {a mathematical formula}j≠i are of types {a mathematical formula}θ−i⁎=(θ1⁎,…,θi−1⁎,θi+1⁎,…,θn⁎), given the history {a mathematical formula}Ht. If we assume independence of types, we can define Pr as{a mathematical formula}{a mathematical formula} where {a mathematical formula}Pj(θj⁎) is the prior belief (probability) that player j is of type {a mathematical formula}θj⁎before any actions are observed, and {a mathematical formula}L(Ht|θj⁎) is the (non-negative) likelihood of history {a mathematical formula}Ht assuming that player j is of type {a mathematical formula}θj⁎. It is convenient to define {a mathematical formula}Prj(θj⁎|H0)=Pj(θj⁎). Note that the likelihood L in (2) is unspecified at this point; we will consider two variants for L in Section 4.</paragraph><paragraph>The independence assumption of types is prevalent in the works discussed in Section 2. In the game theory literature (cf. Section 2.1), it is justified by the fact that the Nash equilibrium assumes that players choose actions independently. (This is opposed to concepts such as correlated equilibrium [8] in which action choices may be correlated.) From a practical perspective, another justification is the fact that, while types are assumed to be independent, the behaviours they encode may very well depend on the behaviour of other players. This is since each player can make decisions based on the entire interaction history, which includes the observed actions of other players. Nonetheless, Section 4 also discusses the possibility of correlated types.</paragraph><paragraph>Where do the hypothesised types {a mathematical formula}θj⁎∈Θj⁎ come from? In this work, we assume that the user has some means to generate such hypotheses. One way is to have them specified manually by domain experts, based on their experience with the problem (e.g. [4]). Another method is to generate types automatically from the problem description. For example, in Sections 5 and 7 we use three different methods to automatically generate sets of types for any given matrix game. Finally, one may use machine learning methods to extract types from a corpus of historical data (e.g. [12], [49]).</paragraph><paragraph>HBA performs a planning procedure, defined by (3)/(4), to find an action which maximises its expected long-term payoff with respect to its current beliefs and hypothesised types. Formally, (3) corresponds to player i's component of the Bayesian Nash equilibrium [57] and (4) corresponds to the Bellman optimality equation [15]. Intuitively, (3)/(4) expand a tree of all possible future trajectories of the interaction process and weight each trajectory based on the posterior beliefs and predicted action probabilities of the hypothesised types. Note that {a mathematical formula}Ht denotes the current history while {a mathematical formula}Hˆ is used to construct all future trajectories (histories), where the notation {a mathematical formula}〈Hˆ,a,s′〉 in (4) denotes concatenation of {a mathematical formula}Hˆ and {a mathematical formula}(a,s′).</paragraph><paragraph>In practice, HBA may be implemented by limiting the recursion in (3)/(4) to some fixed depth (e.g. as in Section 5). However, it is easy to see that this procedure has time complexity which is exponential in factors such as the number of players, actions, and states in the game. This can make it a very costly operation and usually requires more sophisticated approximate methods when applied to complex domains. In this regard, a promising approach is given by stochastic sampling methods such as those used in [4], [11]. In this work, unless stated otherwise, we assume that (3)/(4) are implemented as given.</paragraph><paragraph>It is worth noting that HBA does not require explicit exploration methods (i.e. deliberately choosing actions which do not maximise {a mathematical formula}Estai(Ht)) because exploration is implicit in the calculation of {a mathematical formula}Estai(Ht). Specifically, for each action {a mathematical formula}ai, {a mathematical formula}Estai(Ht) predicts the impact of {a mathematical formula}ai on HBA's beliefs and future interaction. This allows HBA to reason about the benefit of choosing a particular action, in the sense of what information that action can potentially reveal to HBA [60]. Of course, this assumes that the true types of other players are included in the hypothesised types. Nonetheless, when the predictive ability of HBA is limited (e.g. due to a fixed recursion depth; cf. Section 5) or if we use opponent modelling to learn new types during the interaction (e.g. [4], [11]), then it may still be worthwhile to use explicit exploration methods such as those discussed in [25] or approximations as in [26].</paragraph></section><section label="3.3"><section-title>Relation to other interactive decision models</section-title><paragraph>Section 2 provided an overview of related works and models used therein. Here, we further elaborate on the connections and differences to some of these models and other models.</paragraph><paragraph>As pointed out earlier, our SBG model can be viewed as a combination of Bayesian games and stochastic games. If we remove states from the definition of SBGs (or, equivalently, assume a single state and no terminal states), then this reduces to a standard Bayesian game. In this case, {a mathematical formula}Θj+ corresponds to the type spaces used in Bayesian games and ϒ corresponds to the “basic probability distribution” [56]. (However, note that in contrast to Bayesian games, we assume no knowledge of {a mathematical formula}Θj+ and ϒ.) On the other hand, if we remove types from the definition of SBGs, then the model reduces to a standard stochastic game. However, note that Shapley [81] considers Markovian (“stationary”) strategies whereas we allow strategies to depend on the entire interaction history. This definition of strategies is consistent with the model used by Kalai and Lehrer [65], in which strategies are mappings from histories to probability distributions over actions.</paragraph><paragraph>A central assumption in SBGs is that the states and chosen actions are fully observable by the players. This is in contrast to I-POMDPs (cf. Section 2) in which states and actions are not directly observed. Instead, players receive noisy and possibly incomplete signals that depend on the state, based on which players infer beliefs over states. This makes I-POMDPs a very general model, but it also increases their computational complexity significantly. Another difference is that SBGs allow for mixed type distributions while I-POMDPs generally assume fixed types. These differences mean that the results of our work may not directly carry over to I-POMDPs.</paragraph><paragraph>Other models of interactive decision making exist, such as the decentralised POMDP [18] and partially observable stochastic game (e.g. [42]). Both of these models allow for partial observability of process states as described above. While these models do not explicitly encode types, it is possible to emulate types by using factored states which are composed of individual elements.{sup:3} Essentially, we can define the factored state space {a mathematical formula}Sˆ=S×Θ1+×…×Θn+, where the S-element is observed by all players and controlled by their joint actions while the {a mathematical formula}Θi+-elements are privately observed by the players and controlled by the type distribution. An interesting question, then, is to what extent solving this model may produce a similar or better solution than HBA. However, as we will discuss next, this leads to another crucial difference between our work and the above works.</paragraph><paragraph>Once a model is fully specified, the usual goal is to solve it via some procedure. In the context of game theory, a solution may be a profile of strategies that satisfy some equilibrium property (e.g. [43], [29]). In the context of artificial intelligence, a solution is a control policy for one or more agents which satisfies certain guarantees such as payoff maximisation (e.g. [36], [41], [54]). This is in contrast to our work, in which we do not attempt to solve SBGs in this sense. Instead, we prescribe a specific normative solution for a single agent, in the form of HBA. This is similar in spirit to works such as [65], except that we only consider a single agent that uses HBA. The advantage of this approach is that HBA can be applied “instantly”, without the need to solve the model beforehand. This means that HBA may be applied to problems which are too complex to be solved in the conventional sense. Of course, the disadvantage is that we do not exactly know how HBA will perform, and the purpose of the present work is precisely to provide answers to this question.</paragraph></section></section><section label="4"><section-title>Correctness of posterior beliefs</section-title><paragraph>A central aspect of the type-based method are the beliefs over types. Beginning with some initial beliefs about the relative likelihood of types, we compare the predictions of types with the observed actions and update our beliefs to reflect the given evidence. Associated with this process are two key questions: how may evidence be incorporated into beliefs, and under what conditions will the beliefs be correct? As can be seen in Algorithm 1, these are important questions since the accuracy of the expected payoffs (3) depends on the accuracy of the posterior belief Pr.</paragraph><paragraph>In this section, we consider three classes of type distributions to cover a broad spectrum of scenarios: pure distributions, in which all agents have a fixed type; mixed distributions, in which types are randomly re-allocated; and correlated distributions, in which type assignments may be correlated. Corresponding to these classes, we consider three formulations of posterior beliefs which prescribe different ways to incorporate evidence into beliefs. We provide theoretical conditions under which these formulations produce correct beliefs, and we provide examples to show when they may fail to do so.</paragraph><paragraph>Our definition of correctness is with respect to the type distribution ϒ: beliefs are said to be correct if they assign the same probabilities to true types as ϒ. This requires that the beliefs can point to the types in the support of the type distribution. Therefore, the results in this section pertain to a situation in which the user knows that the true type space {a mathematical formula}Θj+ must be a subset of the hypothesised type space {a mathematical formula}Θj⁎. Formally, we assume:</paragraph><paragraph label="Assumption 1">{a mathematical formula}∀j≠i:Θj+⊆Θj⁎.</paragraph><paragraph>The case in which beliefs cannot be correct as defined above, due to incomplete or incorrect hypothesised types, is examined in Sections 6 and 7.</paragraph><paragraph>Finally, recall from Section 3.2 that posterior beliefs of the form (1) assume independence of player types. That is, they assume that the type distribution ϒ can be represented as a product of n independent factors {a mathematical formula}ϒj (one for each player), such that {a mathematical formula}ϒ(θ)=∏jϒj(θj). Hence, in the following, unless states otherwise, we assume that ϒ satisfies this independence property. Section 4.3 also considers the case of correlated type distributions.</paragraph><section label="4.1"><section-title>Product posterior</section-title><paragraph>We begin our analysis with the product posterior:</paragraph><paragraph label="Definition 4">The product posterior is defined as (1) with{a mathematical formula}</paragraph><paragraph>This is the standard posterior formulation used in Bayesian games and most of the works discussed in Section 2. It can be shown that, under a pure type distribution and if HBA does not a priori rule out any of the types in {a mathematical formula}Θj⁎, then it will learn to make correct future predictions. Let {a mathematical formula}H∞ be an infinite history with prefix {a mathematical formula}Hτ, and denote by {a mathematical formula}Pϒ(Hτ,H∞) and {a mathematical formula}PPr(Hτ,H∞), respectively, the true probability (based on ϒ) and the probability assigned by HBA (based on Pr) that {a mathematical formula}Hτ will continue as prescribed by {a mathematical formula}H∞.</paragraph><paragraph label="Theorem 1">Let Γ be a SBG with a pure type distribution ϒ. If HBA uses a product posterior and if the prior beliefs{a mathematical formula}Pjare positive (i.e.{a mathematical formula}∀θj⁎∈Θj⁎:Pj(θj⁎)&gt;0), then:for any{a mathematical formula}ϵ&gt;0, there is a time t from which ({a mathematical formula}τ≥t){a mathematical formula}for all{a mathematical formula}H∞with{a mathematical formula}Pϒ(Hτ,H∞)&gt;0.</paragraph><paragraph label="Proof">The proof is not difficult but tedious, hence we defer it to Appendix A. Proof sketch: Kalai and Lehrer [65] studied a model which can be equivalently described as a single-state SBG ({a mathematical formula}|S|=1) with pure type distribution ϒ and proved Theorem 1 within their model. Their convergence result can be extended to multi-state SBGs by translating the multi-state SBG Γ into a single-state SBG {a mathematical formula}Γˆ which is equivalent to Γ in the sense that the players behave identically. Essentially, the trick is to remove the states in Γ by introducing a new player whose action choices correspond to the state transitions in Γ. □</paragraph><paragraph>Theorem 1 states that HBA will eventually make correct future predictions when using a product posterior against a pure type distribution (assuming the prior beliefs are positive). However, there is a subtle but important asymmetry between making correct future predictions and knowing the true type distribution: while the latter implies the former, the reverse is not generally true. The following example{sup:4} illustrates this:</paragraph><paragraph label="Example 1">Consider a SBG with two players and two actions, C and D. Player 1 is controlled by HBA using a product posterior while player 2 has two types, {a mathematical formula}Θ2+={θλ=0.1,θλ=0.5}, which are assigned by some pure type distribution. The two types choose action C if player 1 chose C in the previous round. Otherwise, with probability λ, they will forever play action D. In this case, HBA will never know the correct type with absolute certainty. Even if HBA chooses D and player 2 responds by playing D indefinitely, there is still no certainty because {a mathematical formula}λ&gt;0 in both types.</paragraph><paragraph>Therefore, while HBA is guaranteed to make correct future predictions after some time, it is not guaranteed to learn the type distribution of the game. Finally, note that Theorem 1 pertains to pure type distributions only. The following example shows that the product posterior may fail in SBGs with mixed type distributions:</paragraph><paragraph label="Example 2">Consider a SBG with two players. Player 1 is controlled by HBA using a product posterior while player 2 has two types, {a mathematical formula}Θ2+={θA,θB}, which are assigned by a mixed type distribution ϒ with {a mathematical formula}ϒ(θA)=ϒ(θB)=0.5. The type {a mathematical formula}θA always chooses action A while {a mathematical formula}θB always chooses action B. In this case, there will be a time t after which both types have been assigned at least once, and so both actions A and B have been played at least once by player 2. This means that from time t and all subsequent times {a mathematical formula}τ≥t, we have {a mathematical formula}Pr2(θA|Hτ)=Pr2(θB|Hτ)=0 (that is, {a mathematical formula}Pr2 is undefined), and HBA will fail to make correct future predictions.</paragraph></section><section label="4.2"><section-title>Sum posterior</section-title><paragraph>We continue our analysis with the sum posterior:</paragraph><paragraph label="Definition 5">The sum posterior is defined as (1) with{a mathematical formula}</paragraph><paragraph>The sum posterior allows HBA to recognise changing types. In other words, the purpose of the sum posterior is to learn mixed type distributions. It is easy to see that a sum posterior would indeed learn the mixed type distribution in Example 2. However, we now give an example to show that, without additional requirements, the sum posterior does not necessarily learn any (pure or mixed) type distribution:</paragraph><paragraph label="Example 3">Consider a SBG with two players. Player 1 is controlled by HBA using a sum posterior while player 2 has two types, {a mathematical formula}Θ2+={θA,θAB}, which are assigned by a pure type distribution ϒ with {a mathematical formula}ϒ(θA)=1. The type {a mathematical formula}θA always chooses action A while {a mathematical formula}θAB chooses actions A and B with equal probability. While the product posterior converges to the correct probabilities ϒ, the sum posterior converges to probabilities {a mathematical formula}〈23,13〉, which is incorrect.</paragraph><paragraph>Note that this example can be readily modified to use a mixed type distribution, with similar results. Therefore, we conclude that, without further assumptions, the sum posterior does not necessarily learn any type distribution.</paragraph><paragraph>Under what condition is the sum posterior guaranteed to learn the true type distribution of the game? Consider the following two quantities, which can be computed from a given history {a mathematical formula}Ht:</paragraph><paragraph label="Definition 6">The average overlap of player j in {a mathematical formula}Ht is defined as{a mathematical formula}{a mathematical formula} where {a mathematical formula}[b]1=1 if b is true, else 0.</paragraph><paragraph label="Definition 7">The average stochasticity of player j in {a mathematical formula}Ht is defined as{a mathematical formula} where {a mathematical formula}aˆjτ∈arg⁡maxaj⁡πj(Hτ,aj,θj⁎).</paragraph><paragraph>Both quantities are bounded by 0 and 1. The average overlap describes the similarity of the types, where {a mathematical formula}AOj(Ht)=0 means that player j's types (on average) never chose the same action in history {a mathematical formula}Ht, whereas {a mathematical formula}AOj(Ht)=1 means that they behaved identically. The average stochasticity describes the uncertainty of the types, where {a mathematical formula}ASj(Ht)=0 means that player j's types (on average) were fully deterministic in the action choices in history {a mathematical formula}Ht, whereas {a mathematical formula}ASj(Ht)=1 means that they chose actions uniformly randomly.</paragraph><paragraph label="Example 4">Consider the SBG from Example 3. Here, player 2 always chooses action A, since its type is always {a mathematical formula}θA. Therefore, for any history {a mathematical formula}Ht, we have {a mathematical formula}AO2(Ht)=0.75, which indicates a substantial amount of overlap between {a mathematical formula}θA and {a mathematical formula}θAB. Furthermore, we have {a mathematical formula}AS2(Ht)=0.5, which indicates a certain degree of randomisation. In fact, {a mathematical formula}θA is fully deterministic while {a mathematical formula}θAB is uniformly random, hence the average stochasticity in the centre of the spectrum {a mathematical formula}[0,1].</paragraph><paragraph>It can be shown that, if the average overlap and stochasticity of player j converge to zero as {a mathematical formula}t→∞, then the sum posterior is guaranteed to converge to any pure or mixed type distribution:</paragraph><paragraph label="Theorem 2">Let Γ be a SBG with a pure or mixed type distribution ϒ. If HBA uses a sum posterior, then, for{a mathematical formula}t→∞: If{a mathematical formula}AOj(Ht)=0and{a mathematical formula}ASj(Ht)=0for all players{a mathematical formula}j≠i, then{a mathematical formula}Pr(θ−i|Ht)=ϒ(θ−i)for all{a mathematical formula}θ−i∈Θ−i+.</paragraph><paragraph label="Proof">Throughout this proof, let {a mathematical formula}t→∞. The sum posterior is defined as (1) where L is defined as (7). Given the definition of L, both the numerator and the denominator in (2) may be infinite. We invoke L'Hôpital's rule which states that, in such cases, the quotient {a mathematical formula}u(t)v(t) is equal to the quotient {a mathematical formula}u′(t)v′(t) of the respective derivatives with respect to t. The derivative of L with respect to t is the average growth per time step, which in general may depend on the history {a mathematical formula}Ht of states and actions. The average growth of L is{a mathematical formula} where{a mathematical formula} is the probability of action {a mathematical formula}aj after history {a mathematical formula}Ht, with {a mathematical formula}ϒ(θj) being the marginal probability that player j is assigned type {a mathematical formula}θj. As we will see shortly, we can make an asymptotic growth prediction irrespective of {a mathematical formula}Ht. Given that {a mathematical formula}AOj(Ht)=0, we can infer that whenever {a mathematical formula}πj(Ht,aj,θj⁎)&gt;0 for action {a mathematical formula}aj and type {a mathematical formula}θj⁎, then {a mathematical formula}πj(Ht,aj,θˆj⁎)=0 for all other types {a mathematical formula}θˆj⁎≠θj⁎ with {a mathematical formula}θˆj⁎∈Θj⁎. Therefore, we can write (11) as{a mathematical formula} Next, given that {a mathematical formula}ASj(Ht)=0, we know that there exists an action {a mathematical formula}aj in (13) with {a mathematical formula}πj(Ht,aj,θj)=1, and, therefore, we can conclude that {a mathematical formula}L′(Ht|θj)=ϒ(θj). This shows that the history {a mathematical formula}Ht is irrelevant to the asymptotic growth rate of L. Finally, since {a mathematical formula}∑θj∈Θj+ϒ(θj)=1, we know that the denominator in (2) will be 1, and we conclude that {a mathematical formula}Prj(θj|Ht)=ϒ(θj). □</paragraph><paragraph>Theorem 2 explains why the sum posterior converges to the correct type distribution in Example 2. Since the types {a mathematical formula}θA and {a mathematical formula}θB always choose different actions and are completely deterministic (i.e. the average overlap and stochasticity are always zero), the sum posterior is guaranteed to converge to the type distribution. On the other hand, in Example 3 the types {a mathematical formula}θA and {a mathematical formula}θAB produce an overlap whenever action A is chosen, and {a mathematical formula}θAB is completely random. Therefore, the average overlap and stochasticity are always positive, and an incorrect type distribution was learned.</paragraph><paragraph>The assumptions made in Theorem 2, namely that the average overlap and stochasticity converge to zero, require practical justification. First of all, it is important to note that it is only required that these converge to zero on average as {a mathematical formula}t→∞. This means that in the beginning there may be arbitrary overlap and stochasticity, as long as these go to zero as the game proceeds. In fact, with respect to stochasticity, this is precisely how the exploration-exploitation dilemma [85] is solved in practice: In the early stages, the agent randomises deliberately over its actions in order to obtain more information about the environment (exploration) while, as the game proceeds, the agent becomes gradually more deterministic in its action choices so as to maximise its payoffs (exploitation). Typical mechanisms which implement this are ϵ-greedy and Softmax/Boltzmann exploration [85]. Fig. 1 demonstrates this in a SBG in which player j has three reinforcement learning types. The payoffs for the types were such that the average overlap would eventually go to zero.</paragraph><paragraph>Regarding the average overlap converging to zero, we believe that this is a property which should be guaranteed by design, for the following reason: If the hypothesised type space {a mathematical formula}Θj⁎ is such that there is a constantly-high average overlap, then this means that the types in {a mathematical formula}Θj⁎ are in effect very similar. However, types which are very similar are likely to produce very similar trajectories in the planning step of HBA (cf. {a mathematical formula}Hˆ in (3)/(4)) and, therefore, constitute redundancy in both time and space. Thus, we believe it is advisable to use type spaces which have low average overlap.</paragraph></section><section label="4.3"><section-title>Correlated posterior</section-title><paragraph>As noted earlier, an implicit assumption in (1) is that the type distribution ϒ can be represented as a product of n independent factors (one for each player), such that {a mathematical formula}ϒ(θ)=∏jϒj(θj). Therefore, since the sum posterior is in the form of (1), it is in fact only guaranteed to learn independent type distributions. This is opposed to correlated type distributions, which cannot be represented as a product of n independent factors. Correlated type distributions can be used to specify constraints on type combinations, such as “player j can only have type {a mathematical formula}θj if player k has type {a mathematical formula}θk”. The following example shows how the sum posterior may fail to converge to a correlated type distribution:</paragraph><paragraph label="Example 5">Consider a SBG with 3 players. Player 1 is controlled by HBA using a sum posterior. Players 2 and 3 each have two types, {a mathematical formula}Θ2+=Θ3+={θA,θB}, which are defined as in Example 2. The type distribution ϒ chooses types with probabilities {a mathematical formula}ϒ(θA,θB)=ϒ(θB,θA)=0.5 and {a mathematical formula}ϒ(θA,θA)=ϒ(θB,θB)=0. In other words, player 2 can never have the same type as player 3. From the perspective of HBA, each type (and hence action) is chosen with equal probability for both players. Thus, despite the fact that there is zero overlap and stochasticity, the sum posterior will eventually assign probability 0.25 to all constellations of types, which is incorrect. This means that HBA fails to recognise that the other players never choose the same action.</paragraph><paragraph>We now propose a posterior formulation which can learn any correlated type distribution:</paragraph><paragraph label="Definition 8">The correlated posterior is defined as{a mathematical formula} where P specifies prior beliefs over {a mathematical formula}Θ−i⁎ (analogous to {a mathematical formula}Pj) and η is a normaliser.</paragraph><paragraph>The correlated posterior is closely related to the sum posterior. In fact, it converges to the correct type distribution under the same conditions as the sum posterior:</paragraph><paragraph label="Theorem 3">Let Γ be a SBG with a correlated type distribution ϒ. If HBA uses the correlated posterior, then, for{a mathematical formula}t→∞: If{a mathematical formula}AOj(Ht)=0and{a mathematical formula}ASj(Ht)=0for all players{a mathematical formula}j≠i, then{a mathematical formula}Pr(θ−i|Ht)=ϒ(θ−i)for all{a mathematical formula}θ−i∈Θ−i+.</paragraph><paragraph label="Proof">The proof is analogous to the proof of Theorem 2. □</paragraph><paragraph>It is easy to see that the correlated posterior would learn the correct type distribution in Example 5. Note that, since it is guaranteed to learn any correlated type distribution, it is also guaranteed to learn any independent type distribution. Therefore, the correlated posterior would also learn the correct type distribution in Example 2. This means that the correlated posterior is complete in the sense that it covers the entire spectrum of pure/mixed and independent/correlated type distributions. However, this completeness comes at a higher computational complexity. While the sum posterior is in {a mathematical formula}O(nmaxj⁡|Θj⁎|) time and space, the correlated posterior is in {a mathematical formula}O(maxj⁡|Θj⁎|n) time and space. In practice, however, the time complexity can be reduced substantially by computing the probabilities {a mathematical formula}πj(Hτ,ajτ,θj⁎) only once for each j and {a mathematical formula}θj⁎∈Θj⁎ (as in the sum posterior), and then reusing them in subsequent computations.</paragraph></section></section><section label="5"><section-title>Practical impact of prior beliefs</section-title><paragraph>The previous section was concerned with the evolution of posterior beliefs as we observe more evidence. However, before we observe any evidence based on which to form our posterior beliefs, we will have to make an initial judgement as to the relative likelihood of types. This initial judgement is called the prior belief.</paragraph><paragraph>Given the lack of evidence, it may be tempting to use uniform prior beliefs in which all types have equal probability. Indeed, the fact that beliefs can change rapidly after only a few observations suggests that prior beliefs may have negligible effect. On the other hand, there is a substantial body of work in the game theory literature arguing the importance of prior beliefs (cf. Section 2). However, these works consider the impact of prior beliefs on equilibrium attainment when all players use the same type-based reasoning. In contrast, our interest is in the practical impact of prior beliefs, i.e. payoff maximisation, for a single agent using the type-based method.</paragraph><paragraph>In addition, there is the work of Bernardo [17], Jaynes [62], and others on “uninformed” priors. The purpose of such priors is to express a state of complete uncertainty, whilst possibly incorporating subjective prior information. (What this means and whether this is possible has been the subject of a long debate, e.g. De Finetti [33].) However, this again differs from our interest in the impact of prior beliefs on payoff maximisation.</paragraph><paragraph>Thus, we are left with the following questions: Do prior beliefs have an impact on our ability to maximise payoffs in the long-term? If so, how? And, crucially, can we automatically compute prior beliefs so as to improve our long-term performance?</paragraph><paragraph>To find answers to these questions, we conducted a comprehensive empirical study which compared 10 methods to automatically compute prior beliefs from a given set of types. The results show that prior beliefs can indeed have a significant impact on the long-term performance, and that the depth of the planning horizon (i.e. how far we look into the future) plays a central role. Finally, and perhaps most intriguingly, we show that automatic methods can compute prior beliefs with consistent performance effects across a variety of scenarios. An implication of this is that prior beliefs could be eliminated as a manual parameter and instead be computed automatically.</paragraph><paragraph>The following subsections describe the experimental setup used in our study. The results are discussed in Section 5.7.</paragraph><section label="5.1"><section-title>Games</section-title><paragraph>We used a comprehensive set of benchmark games introduced by Rapoport and Guyer [78], which consists of 78 repeated {a mathematical formula}2×2 matrix games. The games are strictly ordinal, meaning that each player ranks each of the four possible outcomes from 1 (least preferred) to 4 (most preferred), and no two outcomes have the same rank. Furthermore, the games are distinct in that no game can be obtained by transformation of any other game, which includes interchanging the rows, columns, and players (and any combination thereof) in the payoff matrix of the game.</paragraph><paragraph>The games can be grouped into 21 no-conflict games and 57 conflict games. In a no-conflict game, the two players have the same most preferred outcome, and so it is relatively easy to arrive at a solution that is best for both players. In a conflict game, the players disagree on the best outcome, hence they will have to find some form of a compromise.</paragraph><paragraph>We note that the games in this benchmark correspond to SBGs with single states. The simplicity of these games facilitates a thorough inspection of the interaction process and, thereby, explanation of observations. It also allows us to specify a complete benchmark set in the sense that it contains all games that satisfy the above description, which in turn allows us to draw more general conclusions. Finally, the fact that we use single-state games does not limit the inherent complexity of the interaction, since multi-state SBGs can always be emulated as single-state SBGs via an additional “nature” player (cf. Appendix A). Therefore, we expect that the principal observations we make will also hold in multi-state SBGs.</paragraph></section><section label="5.2"><section-title>Performance criteria</section-title><paragraph>Each play of a game was partitioned into time slices which consist of an equal number of consecutive time steps. For each time slice, we measured the following performance criteria:</paragraph><paragraph>Convergence An agent converged in a time slice if its action probabilities in the time slice did not deviate by more than 0.05 from its initial action probabilities in the same time slice. Returns 1 (true) or 0 (false) for each agent.</paragraph><paragraph>Average payoff Average of payoffs an agent received in the time slice. Returns value in {a mathematical formula}[1,4] for each agent.</paragraph><paragraph>Welfare and fairness Average sum and product, respectively, of the joint payoffs received in the time slice. Returns values in {a mathematical formula}[2,8] and {a mathematical formula}[1,16], respectively.</paragraph><paragraph>Game solutions Tests if the averaged action probabilities in the time slice formed an approximate stage-game Nash equilibrium, Pareto optimum, Welfare optimum, or Fairness optimum. Returns 1 (true) or 0 (false) for each game solution.</paragraph><paragraph>Precise formal definitions of these performance criteria can be found in [3].</paragraph></section><section label="5.3"><section-title>Algorithm</section-title><paragraph>We used HBA to control player 1 and a fixed type in each play to control player 2, which was included in the set of hypothesised types {a mathematical formula}Θ2⁎ provided to HBA (discussed in detail in Section 5.6). Therefore, we used the product posterior formulation (cf. Section 4.1) to update HBA's beliefs. The planning step in HBA was implemented by expanding a finite tree of all future trajectories. Formally, HBA chooses an action {a mathematical formula}ai which maximises the expected payoff {a mathematical formula}Ehai(Ht), defined as{a mathematical formula}{a mathematical formula} where h specifies the depth of the planning horizon (i.e. HBA predicts the next h actions of player j). Note that (15) and (16) correspond closely to (3) and (4), respectively. The difference is that (15)/(16) use h to specify the planning depth while (3)/(4) use the discount factor γ. Hence, a “deeper” planning horizon h translates into a greater discount factor γ. All results reported in this section hold for both variants.</paragraph></section><section label="5.4"><section-title>Types</section-title><paragraph>We used three different methods to automatically generate parameterised sets of types {a mathematical formula}Θj⁎ for any given game. The generated types cover a broad spectrum of adaptive behaviours, including deterministic (CDT), randomised (CNN), and hybrid (LFT) policies. Algorithmic details and parameter settings can be found in Appendix B of [1].</paragraph><paragraph>Leader–Follower–Trigger Agents (LFT) Crandall [31] described a method to generate sets of “leader” and “follower” agents which seek to play specific sequences of joint actions, called “target solutions”. A leader agent plays its part of the target solution as long as the other player does. If the other player deviates, the leader agent punishes the player by playing a minimax strategy. The follower agent is similar except that it does not punish. Rather, if the other player deviates, the follower agent randomly resets its position within the target solution and continues play as usual. We augmented this set by a “trigger” agent which is similar to the leader and follower agents, except that it plays its maximin strategy indefinitely once the other player deviates.</paragraph><paragraph>Co-Evolved Decision Trees (CDT) We used genetic programming [67] to automatically breed sets of decision trees. A decision tree takes as input the past n actions of the other player (in our case, {a mathematical formula}n=3) and deterministically returns an action to be played in response. The breeding process is co-evolutional, meaning that two pools of trees are bred concurrently (one for each player). In each evolution, a random selection of the trees for player 1 is evaluated against a random selection of the trees for player 2. The fitness criterion includes the payoffs generated by a tree as well as its dissimilarity to other trees in the same pool. This was done to encourage a more diverse breeding of trees, as otherwise the trees tend to become very similar or identical.</paragraph><paragraph>Co-Evolved Neural Networks (CNN) We used a string-based genetic algorithm [59] to breed sets of artificial neural networks. The process is basically the same as the one used for decision trees. However, the difference is that artificial neural networks can learn to play stochastic strategies while decision trees always play deterministic strategies. Our networks consist of one input layer with 4 nodes (one for each of the two previous actions of both players), a hidden layer with 5 nodes, and an output layer with 1 node. The node in the output layer specifies the probability of choosing action 1 (and, since we play {a mathematical formula}2×2 games, of action 2). All nodes use a sigmoidal threshold function and are fully connected to the nodes in the next layer.</paragraph></section><section label="5.5"><section-title>Prior beliefs</section-title><paragraph>We specified a total of 10 different methods to automatically compute prior beliefs {a mathematical formula}Pj for a given set of types {a mathematical formula}Θj⁎:</paragraph><paragraph>Uniform prior The uniform prior sets {a mathematical formula}Pj(θj⁎)=|Θj⁎|−1 for all {a mathematical formula}θj⁎∈Θj⁎. This is the baseline prior against which the other priors are compared.</paragraph><paragraph>Random prior The random prior specifies {a mathematical formula}Pj(θj⁎)=0.0001 for a random half of the types in {a mathematical formula}Θj⁎. The remaining probability mass is uniformly spread over the other half. The random prior is used to check if the performance differences of the various priors may be purely due to the fact that they concentrate the probability mass on fewer types.</paragraph><paragraph>Value priors Let {a mathematical formula}Ukt(θj⁎) be the expected cumulative payoff to player k, from the start up until time t, if player j (i.e. the other player) is of type {a mathematical formula}θj⁎ and player i (i.e. HBA) plays optimally against it. Each value prior is in the general form of {a mathematical formula}Pj(θj⁎)=ηψ(θj⁎)b, where η is a normalisation constant and b is a “booster” exponent used to magnify the differences between types {a mathematical formula}θj⁎. Based on this general form, we define four different value priors:</paragraph><list><list-item label="•">Utility prior: {a mathematical formula}ψU(θj⁎)=Uit(θj⁎)</list-item><list-item label="•">Stackelberg prior: {a mathematical formula}ψS(θj⁎)=Ujt(θj⁎)</list-item><list-item label="•">Welfare prior: {a mathematical formula}ψW(θj⁎)=Uit(θj⁎)+Ujt(θj⁎)</list-item><list-item label="•">Fairness prior: {a mathematical formula}ψF(θj⁎)=Uit(θj⁎)⁎Ujt(θj⁎)</list-item></list><paragraph>Our choice of value priors is motivated by the variety of metrics they cover. As a result, these priors can produce substantially different probabilities for the same set of types. In this study, we set {a mathematical formula}t=5 and {a mathematical formula}b=10.</paragraph><paragraph>LP-priors LP-priors are based on the idea that optimal priors can be formulated as the solution to a mathematical optimisation problem (in this case, a linear program). Each LP-prior generates a quadratic matrix A, where each element {a mathematical formula}Aj,j′ contains the “loss” that HBA would incur if it planned its actions against the type {a mathematical formula}θj′⁎ while the true type of player j is {a mathematical formula}θj⁎. Formally, let {a mathematical formula}Ukt(θj⁎|θj′⁎) be like {a mathematical formula}Ukt(θj⁎) except that HBA believes that player j is of type {a mathematical formula}θj′⁎ instead of {a mathematical formula}θj⁎. We define four different LP-priors:</paragraph><list><list-item label="•">LP-Utility: {a mathematical formula}Aj,j′=ψU(θj⁎)−Uit(θj⁎|θj′⁎)</list-item><list-item label="•">LP-Stackelberg: {a mathematical formula}Aj,j′=ψS(θj⁎)−Ujt(θj⁎|θj′⁎)</list-item><list-item label="•">LP-Welfare: {a mathematical formula}Aj,j′=ψW(θj⁎)−[Uit(θj⁎|θj′⁎)+Ujt(θj⁎|θj′⁎)]</list-item><list-item label="•">LP-Fairness: {a mathematical formula}Aj,j′=ψF(θj⁎)−[Uit(θj⁎|θj′⁎)⁎Ujt(θj⁎|θj′⁎)]</list-item></list><paragraph>The matrix A can be fed into a linear program of the form {a mathematical formula}minc⁡cTx s.t. {a mathematical formula}[z,A]x≤0, with {a mathematical formula}n=|Θj⁎|, {a mathematical formula}c=(1,{0}n)T, {a mathematical formula}z=({−1}n)T, to find a vector {a mathematical formula}x=(l,p1,…,pn) in which l is the minimised expected loss to HBA when using the probabilities {a mathematical formula}p1,…,pn (one for each type) as the prior belief {a mathematical formula}Pj. In order to avoid premature elimination of types, we furthermore require that {a mathematical formula}pv&gt;0 for all {a mathematical formula}1≤v≤n. As before, we set {a mathematical formula}t=5 and {a mathematical formula}b=10.</paragraph><paragraph>While this is a mathematically rigorous formulation, it is important to note that it is a simplification of how HBA really works. HBA incorporates its beliefs in every recursion of its planning procedure, whereas the LP formulation implicitly assumes that HBA uses its prior beliefs to randomly sample one of the types against which it then plans optimally. Nonetheless, this is often a reasonable approximation.</paragraph></section><section label="5.6"><section-title>Experimental procedure</section-title><paragraph>We performed identical experiments for every type generation method described in Section 5.4. Each of the 78 games was played 10 times with different random seeds, and each play was repeated against three opponents (30 plays in total): (RT) A randomly generated type was used to control player 2 and the play lasted 100 rounds. (FP) A fictitious player [23] was used to control player 2 and the play lasted 10 000 rounds. (CFP) A conditioned fictitious player (which learns action distributions conditioned on the previous joint action) was used to control player 2 and the play lasted 10 000 rounds.</paragraph><paragraph>In each play, we randomly generated 9 unique types and provided them to HBA along with the true type of player 2, such that {a mathematical formula}|Θ2⁎|=10. (That is, each play had a pure type distribution; cf. Section 3.1) Thus, the true type of player 2 was always included in the set of hypothesised types {a mathematical formula}Θ2⁎. To avoid “end-game” effects, the players were unaware the number of rounds. We included FP and CFP because they try to learn the behaviour of HBA. (While the generated types are adaptive, they do not create models of HBA's behaviour.) To facilitate the learning, we allowed for 10 000 rounds. Finally, since FP and CFP will always choose dominating actions if they exist (in which case there is no interaction), we filtered out all games in the FP and CFP plays that had a dominating action for player 2 (leaving 15 no-conflict and 33 conflict games for the C/FP plays).</paragraph></section><section label="5.7"><section-title>Results</section-title><paragraph>We report three main observations:</paragraph><paragraph label="Observation 1">Prior beliefs can have a significant impact on the long-term performance of HBA.</paragraph><paragraph>This was observed in all classes of types, against all classes of opponents, and in all classes of games used in this study. Fig. 2 provides three representative examples from a range of scenarios. Many of the relative differences due to prior beliefs were statistically significant, based on paired two-sided t-tests with a 5% significance level.</paragraph><paragraph>Our data explain this as follows: Different prior beliefs may cause HBA to take different actions at the beginning of the game. These actions will shape the beliefs of the other player (i.e. how it models and adapts to HBA's actions) which in turn will affect HBA's next actions. Thus, if different prior beliefs lead to different initial actions, they may lead to different play trajectories with different payoffs.</paragraph><paragraph>Given that there is a time after which HBA will know the true type of player 2 (since it is provided to HBA), it may seem surprising that this process would lead to differences in the long-term. In fact, in our experiments, HBA often learned the true type after only 3 to 5 rounds, and in most cases in under 20 rounds. After that point, if the planning horizon of HBA is sufficiently deep, it will realise if its initial actions were sub-optimal and if it can manipulate the play trajectory to achieve higher payoffs in the long-term, thus diminishing the impact of prior beliefs.</paragraph><paragraph>However, deep planning horizons can be problematic in practice since the time complexity of HBA is exponential in the depth of the planning horizon. Therefore, the planning horizon constitutes a trade-off between decision quality and computational tractability. Interestingly, our data show that if we increase the depth, but stay below a sufficient depth (“sufficient” as described above), it may also amplify the impact of prior beliefs:</paragraph><paragraph label="Observation 2">Deeper planning horizons can diminish and amplify the impact of prior beliefs.</paragraph><paragraph>Again, this was observed in all tested scenarios. Fig. 3, Fig. 4 show examples in which deeper planning horizons diminish and amplify the impact of prior beliefs, respectively.</paragraph><paragraph>How can deeper planning horizons amplify the impact of prior beliefs? Our data show that whether or not different prior beliefs cause HBA to take different initial actions depends not only on the prior beliefs and types, but also on the depth of the planning horizon. In some cases, differences between types (i.e. in their action choices) may be less visible in the near future and more visible in the distant future. In such cases, an HBA agent with a myopic planning horizon may choose the same (or similar) initial actions, despite different prior beliefs, because the differences in the types may not be visible within its planning horizon. On the other hand, an HBA agent with a deeper planning horizon may see the differences between the types and decide to choose different initial actions based on the prior beliefs.</paragraph><paragraph>We now turn to a comparison between the different prior beliefs. Here, our data reveal an intriguing property:</paragraph><paragraph label="Observation 3">Automatic methods can compute prior beliefs with consistent performance effects.</paragraph><paragraph>Fig. 5 shows that the prior beliefs had consistent performance effects across a wide variety of scenarios. For example, the Utility prior produced consistently higher payoffs for player 1 (i.e. HBA) while the Stackelberg prior produced consistently higher payoffs for player 2 as well as higher welfare and fairness. The Welfare and Fairness priors were similar to the Stackelberg prior, but not quite as consistent. Similar results were observed for the LP variants of the priors, despite the fact that the LP formulation is a simplification of how HBA works (cf. Section 5.5).</paragraph><paragraph>We note that none of the prior beliefs, including the Uniform prior, produced high rates for the game solutions (i.e. Nash equilibrium, Pareto optimality, etc.). This is because we measured stage-game solutions, which have no notion of time. These can be hard to attain in repeated games, especially if the other player does not actively seek a specific solution, as was often the case in our study.</paragraph><paragraph>Observation 3 is intriguing because it indicates that prior beliefs could be eliminated as a manual parameter and instead be computed automatically, using methods such as the ones specified in Section 5.5. The fact that our methods produced consistent results means that prior beliefs can be constructed to optimise specific performance criteria. Note that this result is particularly interesting because the prior beliefs have no influence, whatsoever, on the true type of player 2.</paragraph><paragraph>This observation is further supported by the fact that the Random prior did not produce consistently different values (for any criterion) from the Uniform prior. This means that the differences in the prior beliefs are not merely due to the fact that they concentrate the probability mass on fewer types, but rather that the prior beliefs reflect the intrinsic metrics based on which they are computed (e.g. player 1 payoffs for Utility prior, player 2 payoffs for Stackelberg prior).</paragraph><paragraph>How is this phenomenon explained? We believe this may be an interesting analogy to the “optimism in uncertainty” principle (e.g. [22]). The optimism lies in the fact that HBA commits to a specific class of types – those with high prior belief – while, in truth and without further evidence, there is no reason to believe that any one type is a priori more likely than others.</paragraph><paragraph>Each class of types is characterised by the intrinsic metric of the prior belief. For instance, the Utility prior assigns high probability to those types which would yield high payoffs to HBA if it played optimally against the types. By committing to such a characterisation, HBA can effectively utilise Observation 1 by choosing initial actions so as to shape the interaction to maximise the intrinsic metric. If the true type of player 2 is indeed in this class of types, then the interaction will proceed as planned by HBA and the intrinsic metric will be optimised. However, if the true type is not in this class, then HBA will quickly learn the correct type and adjust its play accordingly, albeit without necessarily maximising the intrinsic metric.</paragraph><paragraph>This is in contrast to the Uniform and Random priors, which have no intrinsic metric. Under these priors, HBA will plan its actions with respect to types which are not characterised by a common theme (i.e., all types under the Uniform prior, and a random half under the Random prior). Therefore, HBA cannot effectively utilise Observation 1.</paragraph></section></section><section label="6"><section-title>Optimal type spaces</section-title><paragraph>A potential concern in the type-based method is the fact that the hypothesised types may be incorrect. This can range from slight deviations in predicted action probabilities, to predicting entirely different actions from what was observed. The following example illustrates this:</paragraph><paragraph label="Example 6">Consider a SBG with two players and actions L and R. Player 1 is controlled by HBA while player 2 has a single type, {a mathematical formula}θLR, which chooses L,R,L,R, etc. HBA is provided with hypothesised types {a mathematical formula}Θj⁎={θR⁎,θLRR⁎}, where {a mathematical formula}θR⁎ always chooses R while {a mathematical formula}θLRR⁎ chooses L,R,R,L,R,R etc. Both hypothesised types are incorrect in the sense that they predict player 2's actions in only ≈ 50% of the game.</paragraph><paragraph>Such inaccuracies may have a significant impact on our choice of actions: if the hypothesised types are incorrect, then our predictions of future interactions may be incorrect, which in turn may lead to suboptimal action choices. Therefore, an important question is what relation the hypothesised types must have to the true types in order for HBA to be able to complete its task? In particular, what does it mean for the hypothesised types to be optimal?</paragraph><paragraph>Given the complexity of behaviours agents may exhibit, this is an extremely difficult question. In addition, it is not generally sufficient to consider types alone, since actions are planned with respect to both types and beliefs over types. Rather, we have to consider a stochastic process in which our actions depend on the correctness of types as well as the evolution of our beliefs.</paragraph><paragraph>In this spirit, we describe a formal methodology whereby we compare two interactive processes: one in which the true types are known, and one in which this knowledge is approximated through beliefs over hypothesised types. Based on these processes, we use a probabilistic temporal logic to define a hierarchy of desirable termination guarantees, and analyse the theoretical conditions under which they are met. The main result of this analysis is a novel characterisation of optimality which is based on the concept of probabilistic bisimulation [69]. In addition to concisely defining what constitutes optimality of hypothesised types, this allows the user to apply efficient model checking algorithms to verify optimality in practice.</paragraph><section label="6.1"><section-title>Task completion</section-title><paragraph>We are interested in task completion, which we formally capture by the following assumption:</paragraph><paragraph label="Assumption 2">Let player i be controlled by HBA. Then {a mathematical formula}ui(s,a)=1 iff. s∈S¯, else 0.</paragraph><paragraph>Assumption 2 specifies that we are only interested in reaching a terminal state, since this is the only way to obtain a none-zero payoff. In our analysis, we consider discount factors γ (cf. Algorithm 1) with {a mathematical formula}γ=1 and {a mathematical formula}γ&lt;1. While all our results hold for both cases, there is an important distinction: If {a mathematical formula}γ=1, then the expected payoffs (3) correspond to the actual probability that the following state can lead to (or is) a terminal state (we call this the success rate), whereas this is not necessarily the case if {a mathematical formula}γ&lt;1. This is since {a mathematical formula}γ&lt;1 tends to prefer shorter paths, which means that actions with lower success rates may be preferred if they lead to faster termination. Therefore, if {a mathematical formula}γ=1 then HBA is solely interested in termination, and if {a mathematical formula}γ&lt;1 then it is interested in fast termination, where lower γ prefers faster termination.</paragraph></section><section label="6.2"><section-title>Methodology of analysis</section-title><paragraph>Given a SBG Γ, we define the ideal process, X, as the process induced by Γ in which player i is controlled by HBA and in which HBA always knows the current and all future types of all players. Then, given a posterior formulation Pr and hypothesised type spaces {a mathematical formula}Θj⁎ for all {a mathematical formula}j≠i, we define the user process, Y, as the process induced by Γ in which player i is controlled by HBA (same as in X) and in which HBA uses Pr and {a mathematical formula}Θj⁎ in the usual way. Thus, the only difference between X and Y is that X can always predict the player types whereas Y approximates this knowledge through Pr and {a mathematical formula}Θj⁎. We write {a mathematical formula}Estai(Ht|C) to denote the expected payoff (as defined by (3)) of action {a mathematical formula}ai in state {a mathematical formula}st after history {a mathematical formula}Ht, in process {a mathematical formula}C∈{X,Y}.</paragraph><paragraph>The idea is that X constitutes the ideal solution in the sense that {a mathematical formula}Estai(Ht|X) corresponds to the actual expected payoff, which means that HBA chooses the truly best-possible actions in X. This is opposed to {a mathematical formula}Estai(Ht|Y), which is merely the estimated expected payoff based on Pr and {a mathematical formula}Θj⁎, so that HBA may choose suboptimal actions in Y. The methodology of our analysis is to specify what relation Y must have to X to satisfy certain guarantees for termination.</paragraph><paragraph>We specify such guarantees in PCTL [55], a probabilistic modal logic which also allows for the specification of time constraints. PCTL expressions are interpreted over infinite histories in labelled transition systems with atomic propositions (i.e. Kripke structures). In order to interpret PCTL expressions over X and Y, we make the following modifications without loss of generality: Firstly, any terminal state {a mathematical formula}s¯∈S¯ is an absorbing state, meaning that if a process is in {a mathematical formula}s¯, then the next state will be {a mathematical formula}s¯ with probability 1 and all players receive a zero payoff. Secondly, we introduce the atomic proposition term and label each terminal state with it, so that term is true in s if and only if {a mathematical formula}s∈S¯.</paragraph><paragraph>We will use the following two PCTL expressions:{a mathematical formula} where {a mathematical formula}t∈N, {a mathematical formula}p∈[0,1], and {a mathematical formula}≻∈{&gt;,≥}.</paragraph><paragraph>{a mathematical formula}F≻p≤tterm specifies that, given a state s, with a probability of ≻p a state {a mathematical formula}s′ will be reached from s within t time steps such that {a mathematical formula}s′ satisfies term. The semantics of {a mathematical formula}F≻p&lt;∞term is similar except that {a mathematical formula}s′ will be reached in arbitrary but finite time. We write {a mathematical formula}s⊨Cϕ to say that a state s satisfies the PCTL expression ϕ in process {a mathematical formula}C∈{X,Y}.</paragraph></section><section label="6.3"><section-title>Critical type spaces</section-title><paragraph>In our analysis, we will sometimes assume that the hypothesised type spaces {a mathematical formula}Θj⁎ are uncritical:</paragraph><paragraph label="Definition 9">The hypothesised type spaces {a mathematical formula}Θj⁎ are critical if there is a set {a mathematical formula}Sc⊆S∖S¯ which satisfies all of the following:</paragraph><list><list-item label="1.">For each {a mathematical formula}Ht∈H with {a mathematical formula}st∈Sc, there is {a mathematical formula}ai∈Ai such that {a mathematical formula}Estai(Ht|Y)&gt;0 and {a mathematical formula}Estai(Ht|X)&gt;0.</list-item><list-item label="2.">There is a positive probability that Y may eventually get into a state {a mathematical formula}sc∈Sc from {a mathematical formula}s0.</list-item><list-item label="3.">If Y is in a state in {a mathematical formula}Sc, then with probability 1 it will always be in a state in {a mathematical formula}Sc.</list-item></list><paragraph>Intuitively, critical type spaces have the potential to lead HBA into a state space in which it believes it chooses the right actions to complete the task, while other actions are actually required to complete the task. The only effect that its actions have is to induce an infinite cycle, due to a critical inconsistency between the hypothesised and true type spaces. The following example demonstrates this:</paragraph><paragraph label="Example 7">Recall Example 6 and let the task be to choose the same action as player j. Then, {a mathematical formula}Θj⁎ is uncritical because HBA will always complete the task at {a mathematical formula}t=1, regardless of its posterior beliefs and despite the fact that {a mathematical formula}Θj⁎ is inaccurate. Now, assume that {a mathematical formula}Θj⁎={θRL⁎} where {a mathematical formula}θRL⁎ chooses actions R,L,R,L etc. Then, {a mathematical formula}Θj⁎ is critical since HBA will always choose the opposite action of player j, thinking that it would complete the task, when a different action would actually complete it.</paragraph><paragraph>A practical way to ensure that the type spaces {a mathematical formula}Θj⁎ are (eventually) uncritical is to include methods for opponent modelling in each {a mathematical formula}Θj⁎. If the opponent models are guaranteed to learn the correct behaviours, then the type spaces {a mathematical formula}Θj⁎ are guaranteed to become uncritical. In Example 7, any standard modelling method would eventually learn that the true strategy of player j is {a mathematical formula}θLR. As the model becomes more accurate, the posterior beliefs gradually shift towards it and eventually allow HBA to take the right action.</paragraph></section><section label="6.4"><section-title>Termination guarantees</section-title><paragraph>Our first termination guarantee states that if X has a positive probability of solving the task, then so does Y:</paragraph><paragraph label="Property 1">{a mathematical formula}s0⊨XF&gt;0&lt;∞term⇒s0⊨YF&gt;0&lt;∞term.</paragraph><paragraph>We can show that Property 1 holds if the hypothesised type spaces {a mathematical formula}Θj⁎ are uncritical and if Y only chooses actions for player i with positive expected payoff in X.</paragraph><paragraph>Let {a mathematical formula}A(Ht|C) denote the set of actions that process C may choose from in state {a mathematical formula}st after history {a mathematical formula}Ht, i.e. {a mathematical formula}A(Ht|C)=arg⁡maxai⁡Estai(Ht|C) (cf. step 3 in Algorithm 1).</paragraph><paragraph label="Theorem 4">Property 1holds if{a mathematical formula}Θj⁎are uncritical and{a mathematical formula}</paragraph><paragraph label="Proof">Assume {a mathematical formula}s0⊨XF&gt;0&lt;∞term. Then, we know that X chooses actions {a mathematical formula}ai which may lead into a state {a mathematical formula}s′ such that {a mathematical formula}s′⊨XF&gt;0&lt;∞term, and the same holds for all such states {a mathematical formula}s′. Now, given (18) it is tempting to infer the same result for Y, since Y only chooses actions {a mathematical formula}ai which have positive expected payoff in X and, therefore, could truly lead into a terminal state. However, (18) alone is not sufficient to infer {a mathematical formula}s′⊨YF&gt;0&lt;∞term because of the special case in which Y chooses actions {a mathematical formula}ai such that {a mathematical formula}Estai(Ht|X)&gt;0 but without ever reaching a terminal state. This is why we require that the hypothesised type spaces {a mathematical formula}Θj⁎ are uncritical, which prevents this special case. Thus, we can infer that {a mathematical formula}s′⊨YF&gt;0&lt;∞term, and, hence, Property 1 holds. □</paragraph><paragraph>The second guarantee states that if X always completes the task, then so does Y:</paragraph><paragraph label="Property 2">{a mathematical formula}s0⊨XF≥1&lt;∞term⇒s0⊨YF≥1&lt;∞term.</paragraph><paragraph>We can show that Property 2 holds if the type spaces {a mathematical formula}Θj⁎ are uncritical and if Y only chooses actions for player i which lead to states into which X may get as well.</paragraph><paragraph>Let {a mathematical formula}μ(Ht,s|C) be the probability that process C transitions into state s from state {a mathematical formula}st after history {a mathematical formula}Ht, i.e.{a mathematical formula} with {a mathematical formula}A≡A(Ht|C), and let {a mathematical formula}μ(Ht,S′|C)=∑s∈S′μ(Ht,s|C) for {a mathematical formula}S′⊂S.</paragraph><paragraph label="Theorem 5">Property 2holds if{a mathematical formula}Θj⁎are uncritical and{a mathematical formula}</paragraph><paragraph label="Proof">The fact that {a mathematical formula}s0⊨XF≥1&lt;∞term means that, throughout the process, X only transitions into states s with {a mathematical formula}s⊨XF≥1&lt;∞term. As before, it is tempting to infer the same result for Y based on (20), since it only transitions into states which have maximum success rate in X. However, (20) alone is not sufficient since Y may choose actions such that (20) holds true but Y will never reach a terminal state. Nevertheless, since the hypothesised type spaces {a mathematical formula}Θj⁎ are uncritical, we know that this special case will not occur, and, thus, Property 2 holds. □</paragraph><paragraph>We note that, in both Property 1, Property 2, the reverse direction holds true regardless of Theorem 4, Theorem 5. Furthermore, we can combine the requirements of Theorem 4, Theorem 5 to ensure that both properties hold.</paragraph><paragraph>The third guarantee subsumes the previous guarantees by stating that X and Y have the same minimum probability of solving the task:</paragraph><paragraph label="Property 3">{a mathematical formula}s0⊨XF≥p&lt;∞term⇒s0⊨YF≥p&lt;∞term.</paragraph><paragraph>We can show that Property 3 holds if the hypothesised type spaces {a mathematical formula}Θj⁎ are uncritical and if Y only chooses actions for player i which X might have chosen as well.</paragraph><paragraph>Let {a mathematical formula}R(ai,Ht|C) be the success rate of action {a mathematical formula}ai, formally {a mathematical formula}R(ai,Ht|C)=Estai(Ht|C) with {a mathematical formula}γ=1 (so that it corresponds to the actual probability with which {a mathematical formula}ai may lead to termination in the future). Define {a mathematical formula}Xmin and {a mathematical formula}Xmax to be the processes which for each {a mathematical formula}Ht choose actions {a mathematical formula}ai∈A(Ht|X) with, respectively, minimal and maximal success rate {a mathematical formula}R(ai,Ht|X).</paragraph><paragraph label="Theorem 6">If{a mathematical formula}Θj⁎are uncritical and{a mathematical formula}then</paragraph><list><list-item label="(i)">for{a mathematical formula}γ=1:Proposition 3holds in both directions</list-item><list-item label="(ii)">for{a mathematical formula}γ&lt;1:{a mathematical formula}s0⊨XF≥p&lt;∞term⇒s0⊨YF≥p′&lt;∞term</list-item></list><paragraph label="Proof">(i): Since {a mathematical formula}γ=1, all actions {a mathematical formula}ai∈A(Ht|X) have the same success rate for a given {a mathematical formula}Ht, and given (21) we know that Y's actions always have the same success rate as X's actions. Provided that the type spaces {a mathematical formula}Θj⁎ are uncritical, we can conclude that Property 3 must hold, and for the same reasons the reverse direction must hold as well.(ii): Since {a mathematical formula}γ&lt;1, the actions {a mathematical formula}ai∈A(Ht|X) may have different success rates. The lowest and highest chances that X completes the task are precisely modelled by {a mathematical formula}Xmin and {a mathematical formula}Xmax, and given (21) and the fact that {a mathematical formula}Θj⁎ are uncritical, the same holds for Y. Therefore, we can infer the common bound {a mathematical formula}pmin≤{p,p′}≤pmax as defined in Theorem 6. □</paragraph><paragraph>Properties 1 to 3 are indefinite in the sense that they make no restrictions on time requirements. Our fourth and final guarantee subsumes all previous guarantees and states that if there is a probability p such that X terminates within t time steps, then so does Y for the same p and t:</paragraph><paragraph label="Property 4">{a mathematical formula}s0⊨XF≥p≤tterm⇒s0⊨YF≥p≤tterm.</paragraph><paragraph>We believe that Property 4 is an adequate criterion of optimality for hypothesised type spaces {a mathematical formula}Θj⁎ since, if it holds, {a mathematical formula}Θj⁎ must approximate the true type spaces {a mathematical formula}Θj+ in a way which allows HBA to plan (almost) as accurately – in terms of solving the task – as the “ideal” HBA in X which always knows the true types.</paragraph><paragraph>What relation must Y have to X in order to satisfy Property 4? The fact that Y and X are processes over state transition systems means that we can draw on methods from the model checking literature to answer this question. Specifically, we will use the concept of probabilistic bisimulation[69], which we here define within the context of our work:</paragraph><paragraph label="Definition 10">A probabilistic bisimulation between X and Y, denoted {a mathematical formula}X∼Y, is an equivalence relation {a mathematical formula}B⊆S×S such that</paragraph><list><list-item label="(i)">{a mathematical formula}(s0,s0)∈B</list-item><list-item label="(ii)">{a mathematical formula}sX⊨Xterm⇔sY⊨Yterm for all {a mathematical formula}(sX,sY)∈B</list-item><list-item label="(iii)">{a mathematical formula}μ(HXt,Sˆ|X)=μ(HYt,Sˆ|Y) for any histories {a mathematical formula}HXt,HYt with {a mathematical formula}(sXt,sYt)∈B and all equivalence classes {a mathematical formula}Sˆ under B.</list-item></list><paragraph>Intuitively, a probabilistic bisimulation states that X and Y do (on average) match each other's transitions. Our definition of probabilistic bisimulation is most general in that it does not require that transitions are matched by the same action or that related states satisfy the same atomic propositions other than termination. However, we do note that other definitions exist which make such additional requirements, and our results hold for each of these refinements.</paragraph><paragraph>The main contribution in this section is to show that the optimality criterion expressed by Property 4 holds in both directions if there exists a probabilistic bisimulation between X and Y. Thus, we offer an alternative formal characterisation of optimality for the hypothesised type spaces {a mathematical formula}Θj⁎:</paragraph><paragraph label="Theorem 7">Property 4holds in both directions if there exists a probabilistic bisimulation{a mathematical formula}X∼Y.</paragraph><paragraph label="Proof">First of all, we note that, strictly speaking, the standard definitions of bisimulation (e.g. [10], [69]) assume the Markov property, which means that the next state of a process depends only on its current state. In contrast, we consider the more general case in which the next state may depend on the history {a mathematical formula}Ht of previous states and joint actions (since the player strategies {a mathematical formula}πj depend on {a mathematical formula}Ht). However, one can always enforce the Markov property by design, i.e. by augmenting the state space S to account for the relevant factors of the past. In fact, we could postulate that the histories as a whole constitute the states of the system, i.e. {a mathematical formula}S=H. Therefore, to simplify the exposition, we assume the Markov property and we write {a mathematical formula}μ(s,Sˆ|C) to denote the cumulative probability that C transitions from state s into any state in {a mathematical formula}Sˆ.Given the Markov property, the fact that B is an equivalence relation, and {a mathematical formula}μ(sX,Sˆ|X)=μ(sY,Sˆ|Y) for {a mathematical formula}(sX,sY)∈B, we can represent the dynamics of X and Y in a common graph, such as the following one:{a mathematical formula}The nodes correspond to the equivalence classes under B. A directed edge from {a mathematical formula}Sˆa to {a mathematical formula}Sˆb specifies that there is a positive probability {a mathematical formula}μAB=μ(sX,Sˆb|X)=μ(sY,Sˆb|Y) that X and Y transition from states {a mathematical formula}sX,sY∈Sˆa to states {a mathematical formula}sX′,sY′∈Sˆb, respectively. Note that {a mathematical formula}sX,sY and {a mathematical formula}sX′,sY′ need not be equal but merely equivalent, i.e. {a mathematical formula}(sX,sY)∈B and {a mathematical formula}(sX′,sY′)∈B. There is one node ({a mathematical formula}Sˆ0) that contains the initial state {a mathematical formula}s0 and one node ({a mathematical formula}Sˆ6) that contains all terminal states {a mathematical formula}S¯ and no other states. This is because once X and Y reach a terminal state they will always stay in it (i.e. {a mathematical formula}μ(s,S¯|X)=μ(s,S¯|Y)=1 for {a mathematical formula}s∈S¯) and since they are the only states that satisfy term. Thus, the graph starts in {a mathematical formula}Sˆ0 and terminates (if at all) in {a mathematical formula}Sˆ6.Since the graph represents the dynamics of both X and Y, it is easy to see that Property 4 must hold in both directions. In particular, the probabilities that X and Y are in node {a mathematical formula}Sˆ at time t are identical. One simply needs to add the probabilities of all directed paths of length t which end in {a mathematical formula}Sˆ (provided that such paths exist), where the probability of a path is the product of the {a mathematical formula}μAB along the path. Therefore, X and Y terminate with equal probability, and on average within the same number of time steps. □</paragraph><paragraph>Some remarks to clarify the usefulness of this result: First of all, in contrast to Theorems 4 to 6, Theorem 7 does not explicitly require {a mathematical formula}Θj⁎ to be uncritical. In fact, this is implicit in the definition of probabilistic bisimulation. Moreover, while the other theorems relate Y and X for identical histories {a mathematical formula}Ht, Theorem 7 relates Y and X for related histories {a mathematical formula}HYt and {a mathematical formula}HXt, making it more generally applicable. Finally, Theorem 7 has an important practical implication: it tells us that we can use efficient methods for model checking (e.g. [10], [69]) to verify optimality of {a mathematical formula}Θj⁎. In fact, it can be shown that for Property 4 to hold (albeit not in the other direction) it suffices that Y be a probabilistic simulation[10] of X, which is a coarser preorder than probabilistic bisimulation. However, algorithms for checking probabilistic simulation (e.g. [10]) are computationally much more expensive (and fewer) than those for probabilistic bisimulation, hence their practical use is currently limited.</paragraph></section></section><section label="7"><section-title>Behavioural hypothesis testing</section-title><paragraph>In the previous section, we considered the possibility of incorrect hypothesised types and analysed the conditions under which HBA is nevertheless able to complete its task. While the analysis is rigorous and complete, it is performed before any interaction and with respect to the true types of other agents. How can we decide during the interaction and with no knowledge of the true types whether our hypothesised types are correct?</paragraph><paragraph>There are several ways in which an answer to this question could be used. For example, if we persistently reject our hypothesised types, we may hypothesise an alternative set of types or resort to some default plan of action, such as a “maximin” strategy. Unfortunately, posterior beliefs do not provide an answer to this question because they quantify the relative likelihood of types (relative to a set of alternative types), but they are no measure of truth. That is, even if our beliefs point to one type, this does not tell us that the observed agent is indeed of that type. Instead, it only tells us that all other types have been discarded after the current interaction history.</paragraph><paragraph>To illustrate the source of difficulty, consider an interaction process between two agents which can choose from three actions. The table below shows the first 5 time steps of the interaction. The columns show, respectively, the current time t of the interaction, the actions chosen by the agents at time t, and agent 1's hypothesised probabilities with which agent 2 will choose its actions at time t, based on the prior interaction history.{a mathematical formula}</paragraph><paragraph>Assuming the process continues in this fashion, and without any restrictions on the behaviour of agent 2, how should agent 1 decide whether or not to reject its hypothesis about the behaviour of agent 2? Note that agent 1 cannot outright reject its hypothesis because all observed actions of agent 2 were supported by agent 1's hypothesis (i.e. had positive probability).</paragraph><paragraph>There exists a large body of literature on what is often referred to as model criticism (e.g. [14], [71], [79], [21]). Model criticism attempts to answer the analogous question of whether a given data set could have been generated by a given model. However, in contrast to our work, model criticism usually assumes that the data are independent and identically distributed, which is not the case in the interactive setting we consider. A related problem, sometimes referred to as identity testing, is to test if a given sequence of data was generated by some given stochastic process [80], [13]. Instead of independent and identical distributions, this line of work assumes other properties such as stationarity and ergodicity. Unfortunately, these assumptions are also unlikely in interaction processes, and the proposed solutions are very costly.</paragraph><paragraph>A perhaps more natural way to address this question is to compute some kind of score from the information given in the above table, and to compare this score with some manually chosen rejecting threshold. A prominent example of such a score is the empirical frequency distribution (e.g. [28], [47]). However, while the simplicity of this method is appealing, there are two significant problems: (a) it is far from trivial to devise a scoring scheme that reliably quantifies “correctness” of hypotheses (for instance, an empirical frequency distribution taken over all past actions would be insufficient in the above example since the hypothesised action distributions are changing), and (b) it is unclear how one should choose the threshold parameter for any given scoring scheme.</paragraph><paragraph>In this section, we show how a particular form of model criticism, namely frequentist hypothesis testing, can be combined with the concept of scores to decide whether to reject a behavioural hypothesis. Our proposed algorithm addresses (a) by allowing for multiple scoring criteria in the construction of the test statistic, with the intent of obtaining an overall more reliable scoring scheme. The distribution of the test statistic is learned during the interaction process, and we show that the learning is asymptotically correct. Analogous to standard frequentist testing, the hypothesis is rejected at a given point in time if the resulting p-value is below some “significance level”. This eliminates (b) by providing a uniform semantics for rejection that is invariant to the employed scoring scheme. We present results from a comprehensive set of experiments, demonstrating that the algorithm achieves high accuracy and scalability at low computational costs.</paragraph><paragraph>Of course, there is a long-standing debate on the role of statistical hypothesis tests and quantities such as p-values (e.g. [51], [16], [30]). The usual consensus is that p-values should be combined with other forms of evidence to reach a final conclusion [45], and this is the view we adopt as well. In this sense, our method may be used as part of a larger machinery to decide the truth of a hypothesis.</paragraph><section label="7.1"><section-title>Individual hypotheses and beliefs</section-title><paragraph>As noted in Section 6, it does not generally suffice to consider the correctness of individual types, since we plan our actions with respect to both types and our beliefs regarding the relative likelihood of types (cf. (3)). In this regard, we note that any combination of beliefs Pr and types {a mathematical formula}Θj⁎ can be described as a single type {a mathematical formula}θˆj⁎ of the form{a mathematical formula}</paragraph><paragraph>This combination is equivalent to sampling a single type {a mathematical formula}θj⁎∈Θj⁎ using probabilities {a mathematical formula}Pr(θj⁎|Ht), and then using {a mathematical formula}θj⁎ to choose actions {a mathematical formula}aj∈Aj via {a mathematical formula}πj(Ht,aj,θj⁎)[68]. Analogously, we may combine the true types {a mathematical formula}Θj+⊂Θj of player j, using the type distribution ϒ, into a single type {a mathematical formula}θˆj+ such that{a mathematical formula}</paragraph><paragraph>Therefore, to simplify the notation in this section, we will generally assume a single hypothesised type {a mathematical formula}θj⁎∈Θj and a single true type {a mathematical formula}θj+∈Θj. Note that this means that our method can be applied to the combination of beliefs and hypothesised types, as well as to individual types in {a mathematical formula}Θj⁎. Furthermore, we will write {a mathematical formula}πj(Ht,θj) to denote the probability distribution over actions {a mathematical formula}Aj (rather than probabilities of individual actions).</paragraph></section><section label="7.2"><section-title>A method for behavioural hypothesis testing</section-title><paragraph>Let i denote our agent and let j denote another agent. Moreover, let {a mathematical formula}θj⁎∈Θj denote our hypothesis for j's behaviour and let {a mathematical formula}θj+∈Θj denote j's true behaviour. The central question we ask is if {a mathematical formula}θj⁎=θj+?</paragraph><paragraph>Unfortunately, since we do not know {a mathematical formula}θj+, we cannot directly answer this question. However, at each time t, we know j's past actions {a mathematical formula}ajt=(aj0,…,ajt−1) which were generated by {a mathematical formula}θj+. If we use {a mathematical formula}θj⁎ to generate a vector {a mathematical formula}aˆjt=(aˆj0,…,aˆjt−1), where {a mathematical formula}aˆjτ is sampled using {a mathematical formula}πj(Hτ,θj⁎), we can formulate the related two-sample problem of whether {a mathematical formula}ajt and {a mathematical formula}aˆjt were generated from the same behaviour, namely {a mathematical formula}θj⁎.</paragraph><paragraph>In this section, we propose a general and efficient algorithm to decide this problem. At its core, the algorithm computes a frequentist p-value{a mathematical formula} where {a mathematical formula}a˜jt∼δt(θj⁎)=(πj(H0,θj⁎),…,πj(Ht−1,θj⁎)). The value of p corresponds to the probability with which we expect to observe a test statistic at least as extreme as {a mathematical formula}T(ajt,aˆjt), under the null-hypothesis that {a mathematical formula}θj⁎=θj+. Thus, we reject {a mathematical formula}θj⁎ if p is below some “significance level” {a mathematical formula}α⁎.</paragraph><paragraph>In the following subsections, we describe the test statistic T and its asymptotic properties, and how our algorithm learns the distribution of {a mathematical formula}T(a˜jt,aˆjt). A summary of the algorithm is given in Algorithm 2.</paragraph><section label="7.2.1"><section-title>Test statistic</section-title><paragraph>We follow the general approach outlined earlier by which we compute a score from a vector of actions and their hypothesised distributions. Formally, we define a score function as {a mathematical formula}z:(Aj)t×Δ(Aj)t→R, where {a mathematical formula}Δ(Aj) is the set of all probability distributions over {a mathematical formula}Aj. Thus, {a mathematical formula}z(ajt,δt(θj⁎)) is the score for observed actions {a mathematical formula}ajt and hypothesised distributions {a mathematical formula}δt(θj⁎), and we sometimes abbreviate this to {a mathematical formula}z(ajt,θj⁎). We use Z to denote the space of all score functions.</paragraph><paragraph>Given a score function z, we define the test statistic T as{a mathematical formula}{a mathematical formula} where {a mathematical formula}a˜jτ and {a mathematical formula}aˆjτ denote the τ-prefixes of {a mathematical formula}a˜jt and {a mathematical formula}aˆjt, respectively.</paragraph><paragraph>In this work, we assume that z is provided by the user. While formally unnecessary (in the sense that our analysis does not require it), we find it a useful design guideline to interpret a score as a kind of likelihood, such that higher scores suggest higher likelihood of {a mathematical formula}θj⁎ being correct. Under this interpretation, a minimum requirement for z should be that it is consistent, such that, for any {a mathematical formula}t&gt;0 and {a mathematical formula}θj⁎∈Θj,{a mathematical formula} where {a mathematical formula}Eη denotes the expectation under η. This ensures that if the null-hypothesis {a mathematical formula}θj⁎=θj+ is true, then the score {a mathematical formula}z(ajt,θj⁎) is maximised on expectation.</paragraph><paragraph>Ideally, we would like a score function z which is perfect in that it is consistent and {a mathematical formula}|Πz|=1. This means that {a mathematical formula}θj⁎ can maximise {a mathematical formula}z(ajt,θj⁎) (where {a mathematical formula}ajt∼δt(θj+)) only if {a mathematical formula}θj⁎=θj+. Unfortunately, it is unclear if such a score function exists for the general case and how it should look. Even if we restrict the behaviours agents may exhibit, it can still be difficult to find a perfect score function. On the other hand, it is a relatively simple task to specify a small set of score functions {a mathematical formula}z1,…,zK which are consistent but imperfect. (Examples are given in Section 7.3.) Given that these score functions are consistent, we know that the cardinality {a mathematical formula}|∩kΠzk| can only monotonically decrease. Therefore, it seems a reasonable approach to combine multiple imperfect score functions in an attempt to approximate a perfect score function.</paragraph><paragraph>Given score functions {a mathematical formula}z1,…,zK∈Z which are all bounded by the same interval {a mathematical formula}[a,b]⊂R, we redefine {a mathematical formula}Tτ to{a mathematical formula} where {a mathematical formula}wk∈R is a weight for score function {a mathematical formula}zk. In this work, we set {a mathematical formula}wk=1K. (We also experiment with alternative weighting schemes in Section 7.3.) However, we believe that {a mathematical formula}wk may serve as an interface for useful modifications of our algorithm. For example, Yue et al. [87] compute weights to increase the power of their hypothesis tests.</paragraph></section><section label="7.2.2"><section-title>Asymptotic properties</section-title><paragraph>The vectors {a mathematical formula}ajt and {a mathematical formula}aˆjt are constructed iteratively. That is, at time t, we observe agent j's past action {a mathematical formula}ajt−1, which was generated from {a mathematical formula}πj(Ht−1,θj+), and set {a mathematical formula}ajt=〈ajt−1,ajt−1〉. At the same time, we sample an action {a mathematical formula}aˆjt−1 using {a mathematical formula}πj(Ht−1,θj⁎) and set {a mathematical formula}aˆjt=〈aˆjt−1,aˆjt−1〉. Assuming the null-hypothesis {a mathematical formula}θj⁎=θj+, will {a mathematical formula}T(ajt,aˆjt) converge in the process?</paragraph><paragraph>Unfortunately, T might not converge. This may seem surprising at first glance given that {a mathematical formula}ajt−1 and {a mathematical formula}aˆjt−1 have the same distribution {a mathematical formula}πj(Ht−1,θj+)=πj(Ht−1,θj⁎), since {a mathematical formula}Ex,y∼ψ[x−y]=0 for any distribution ψ. However, there is a subtle but important difference: while {a mathematical formula}ajt−1 and {a mathematical formula}aˆjt−1 have the same distribution, {a mathematical formula}zk(ajt,θj⁎) and {a mathematical formula}zk(aˆjt,θj⁎) may have arbitrarily different distributions. This is because these scores may depend on the entire prefix vectors {a mathematical formula}ajt−1 and {a mathematical formula}aˆjt−1, respectively, which means that their distributions may be different if {a mathematical formula}ajt−1≠aˆjt−1. Fortunately, our algorithm does not require T to converge because it learns the distribution of T during the interaction process, as we will discuss in Section 7.2.3.</paragraph><paragraph>Interestingly, while T may not converge, it can be shown that the fluctuation of T is eventually normally distributed, for any set of score functions {a mathematical formula}z1,…,zK with bound {a mathematical formula}[a,b]. Formally, let {a mathematical formula}E[Tτ(ajτ,aˆjτ)] and {a mathematical formula}Var[Tτ(ajτ,aˆjτ)] denote the finite expectation and variance of {a mathematical formula}Tτ(ajτ,aˆjτ), where it is irrelevant if {a mathematical formula}ajτ,aˆjτ are sampled directly from {a mathematical formula}δτ(θj⁎) or generated iteratively as prescribed above. Furthermore, let {a mathematical formula}σt2=∑τ=1tVar[Tτ(ajτ,aˆjτ)] denote the cumulative variance. Then, the standardised stochastic sum{a mathematical formula} will converge in distribution to the standard normal distribution as {a mathematical formula}t→∞. Thus, T is normally distributed as well.</paragraph><paragraph>To see this, first recall that the standard central limit theorem requires the random variables {a mathematical formula}Tτ to be independent and identically distributed. In our case, {a mathematical formula}Tτ are independent in that the random outcome of {a mathematical formula}Tτ has no effect on the outcome of {a mathematical formula}Tτ′. However, {a mathematical formula}Tτ and {a mathematical formula}Tτ′ depend on different action sequences, and may therefore have different distributions. Hence, we have to show an additional property, commonly known as Lyapunov's condition (e.g. [44]), which states that there exists a positive integer d such that{a mathematical formula}{a mathematical formula}</paragraph><paragraph>Since {a mathematical formula}zk are bounded, we know that {a mathematical formula}Tτ are bounded. Hence, the summands in (31) are uniformly bounded, say by U for brevity. Setting {a mathematical formula}d=1, we obtain{a mathematical formula} The last part goes to zero if {a mathematical formula}σt→∞, and hence Lyapunov's condition holds. If, on the other hand, {a mathematical formula}σt converges, then this means that the variance of {a mathematical formula}Tτ is zero from some point onward (or that it has an appropriate convergence to zero). From this point, {a mathematical formula}θj⁎ prescribes fully deterministic action choices for agent j (i.e. {a mathematical formula}∃aj:πj(Hτ,aj,θj⁎)=1), and a statistical analysis is no longer necessary.</paragraph></section><section label="7.2.3"><section-title>Learning the test distribution</section-title><paragraph>Given that T is eventually normal, it may seem reasonable to compute (24) using a normal distribution whose parameters are fitted during the interaction. However, this fails to recognise that the distribution of T is shaped gradually over an extended time period, and that the fluctuation around T can be heavily skewed in either direction until convergence to a normal distribution emerges. Thus, a normal distribution may be a poor fit during this shaping period.</paragraph><paragraph>What is needed is a distribution which can represent any normal distribution, and which is flexible enough to faithfully represent the gradual shaping. One distribution which has these properties is the skew-normal distribution[9], [77]. Given the PDF ϕ and CDF Φ of the standard normal distribution, the skew-normal PDF is defined as{a mathematical formula} where {a mathematical formula}ξ∈R is the location parameter, {a mathematical formula}ω∈R+ is the scale parameter, and {a mathematical formula}β∈R is the shape parameter. Note that this reduces to the normal PDF for {a mathematical formula}β=0, in which case ξ and ω correspond to the mean and standard deviation, respectively. Hence, the normal distribution is a sub-class of the skew-normal distribution.</paragraph><paragraph>Our algorithm learns the shifting parameters of f during the interaction process, using a simple but effective sampling procedure. Essentially, we use {a mathematical formula}θj⁎ to iteratively generate N additional action vectors {a mathematical formula}a˜jt,1,…,a˜jt,N in the exact same way as {a mathematical formula}aˆjt. The vectors {a mathematical formula}a˜jt,n are then mapped into data points{a mathematical formula} which are used to estimate the parameters {a mathematical formula}ξ,ω,β by minimising the negative log-likelihood{a mathematical formula} whilst ensuring that ω is positive. An alternative is the method-of-moments estimator, which can also be used to obtain initial values for (35). Note that it is usually unnecessary to estimate the parameters at every point in time; it seems reasonable to update the parameters less frequently as the amount of evidence (i.e. observed actions) grows.</paragraph><paragraph>Given the asymmetry of the skew-normal distribution, the semantics of “as extreme as” in (24) may no longer be obvious (e.g. is this with respect to the mean or mode?). In addition, the usual tail-area calculation of the p-value requires the CDF, but there is no closed form for the skew-normal CDF and approximating it is rather cumbersome. To circumvent these issues, we approximate the p-value as{a mathematical formula} where μ is the mode of the fitted skew-normal distribution. This avoids the asymmetry issue and is easier to compute.</paragraph></section></section><section label="7.3"><section-title>Experiments</section-title><paragraph>We conducted a comprehensive set of experiments to investigate the accuracy (correct and incorrect rejection), scalability (with number of actions), and sampling complexity of our algorithm. The following three score functions and their combinations were used:{a mathematical formula}{a mathematical formula}{a mathematical formula} where {a mathematical formula}[b]1=1 if b is true and 0 otherwise. Note that {a mathematical formula}z1,z3 are generally consistent (cf. Section 7.2.1), while {a mathematical formula}z2 is consistent for {a mathematical formula}|Aj|=2 but not necessarily for {a mathematical formula}|Aj|&gt;2. Furthermore, {a mathematical formula}z1,z2,z3 are all imperfect. The score function {a mathematical formula}z3 is based on the empirical frequency distribution.</paragraph><paragraph>The parameters of the test distribution (cf. Section 7.2.3) were estimated less frequently as t increased. The first estimation was performed at time {a mathematical formula}t=1 (i.e. after observing one action). After estimating the parameters at time t, we waited {a mathematical formula}⌊t⌋−1 time steps until the parameters were re-fitted. Throughout our experiments, we used a significance level of {a mathematical formula}α⁎=0.01 (i.e. reject {a mathematical formula}θj⁎ if the p-value is below 0.01).</paragraph><section label="7.3.1"><section-title>Random behaviours</section-title><paragraph>In the first set of experiments, the behaviour (type) spaces {a mathematical formula}Θi and {a mathematical formula}Θj were restricted to “random” behaviours. Each random behaviour is defined by a sequence of random probability distributions over {a mathematical formula}Aj. The distributions are created by drawing uniform random numbers from {a mathematical formula}(0,1) for each action {a mathematical formula}aj∈Aj, and subsequent normalisation so that the values sum up to 1.</paragraph><paragraph>Random behaviours are a good baseline for our experiments because they are usually hard to distinguish. This is due to the fact that the entire set {a mathematical formula}Aj is always in the support of the behaviours, and since they do not react to any past actions. These properties mean that there is little structure in the interaction that can be used to distinguish behaviours.</paragraph><paragraph>We simulated 1000 interaction processes, each lasting 10 000 time steps. In each process, we randomly sampled behaviours {a mathematical formula}θi∈Θi,θj+∈Θj to control agents i and j, respectively. In half of these processes, we used a correct hypothesis {a mathematical formula}θj⁎=θj+. In the other half, we sampled a random hypothesis {a mathematical formula}θj⁎∈Θj with {a mathematical formula}θj⁎≠θj+. We repeated each set of simulations for {a mathematical formula}|Aj|=2,10,20 (with {a mathematical formula}|Ai|=|Aj|) and {a mathematical formula}N=10,50,100 (cf. Section 7.2.3).</paragraph><paragraph>Fig. 6 shows the average accuracy of our algorithm (for {a mathematical formula}N=50), by which we mean the average percentage of time steps in which the algorithm made correct decisions (i.e. no reject if {a mathematical formula}θj⁎=θj+; reject if {a mathematical formula}θj⁎≠θj+). The x-axis shows the combination of score functions used to compute the test statistic (e.g. [1 2] means that we combined {a mathematical formula}z1,z2).</paragraph><paragraph>The results show that our algorithm achieved excellent accuracy, often bordering the 100% mark. They also show that the algorithm scaled well with the number of actions, with no degradation in accuracy. However, there were two exceptions to these observations: using {a mathematical formula}z3 resulted in very poor accuracy for {a mathematical formula}θj⁎≠θj+, and the combination of {a mathematical formula}z2,z3 scaled badly for {a mathematical formula}θj⁎≠θj+.</paragraph><paragraph>The reason for both of these exceptions is that {a mathematical formula}z3 is not a good scoring scheme for random behaviours. The function {a mathematical formula}z3 quantifies a similarity between the empirical frequency distribution and the averaged hypothesised distributions. For random behaviours (as defined in this work), both of these distributions will converge to the uniform distribution. Thus, under {a mathematical formula}z3, any two random behaviours will eventually be the same, which explains the low accuracy for {a mathematical formula}θj⁎≠θj+.</paragraph><paragraph>As can be seen in Fig. 6, the inadequacy of {a mathematical formula}z3 is solved when adding any of the other score functions {a mathematical formula}z1,z2. These functions add discriminative information to the test statistic, which technically means that the cardinality {a mathematical formula}|Πz| in (27) is reduced. However, in the case of {a mathematical formula}[z2,z3], the converge is substantially slower for higher {a mathematical formula}|Aj|, meaning that more evidence is needed until {a mathematical formula}θj⁎ can be rejected. Fig. 7 shows how a higher number of actions affects the average convergence rate of p-values computed with {a mathematical formula}z2,z3.</paragraph><paragraph>In addition to the score functions {a mathematical formula}zk, a central aspect for the convergence of p-values are the corresponding weights {a mathematical formula}wk (cf. (28)). As mentioned in Section 7.2.1, we use uniform weights {a mathematical formula}wk=1K. However, to show that the weighting is no trivial matter, we repeated our experiments with four alternative weighting schemes: Let {a mathematical formula}zkτ=zk(a˜jτ,θj⁎)−zk(aˆjτ,θj⁎) denote the summands in (28). The weighting schemes truemax / truemin assign {a mathematical formula}wk=1 for the first k that maximises / minimises {a mathematical formula}|zkτ|, and 0 otherwise. Similarly, the weighting schemes max / min assign {a mathematical formula}wk=1 for the first k that maximises / minimises {a mathematical formula}zkτ, and 0 otherwise.</paragraph><paragraph>Fig. 8, Fig. 9 show the results for truemax and truemin. As can be seen in the figures, truemax is very similar to uniform weights while truemin improves the convergence for {a mathematical formula}[z2,z3] but compromises elsewhere. The results for max and min are very similar to those of truemin and truemax, respectively, hence we omit them.</paragraph><paragraph>Finally, we recomputed all accuracies using a more lenient significance level of {a mathematical formula}α⁎=0.05. As could be expected, this marginally decreased and increased (i.e. by a few percentage points) the accuracy for {a mathematical formula}θj⁎=θj+ and {a mathematical formula}θj⁎≠θj+, respectively. This was primarily observed in the early stages of the interaction. Overall, however, the results were very similar to those obtained with {a mathematical formula}α⁎=0.01.</paragraph><paragraph>Recall that N specifies the number of sampled action vectors {a mathematical formula}a˜jt,n used to learn the distribution of the test statistic (cf. Section 7.2.3). In the previous section, we reported results for {a mathematical formula}N=50. In this section, we investigate differences in accuracy for {a mathematical formula}N=10,50,100.</paragraph><paragraph>Fig. 10, Fig. 11 show the differences for {a mathematical formula}|Aj|=2,20, respectively. (The figure for {a mathematical formula}|Aj|=10 was virtually the same as the one for {a mathematical formula}|Aj|=20, except with minor improvements in accuracy for the {a mathematical formula}[z2,z3] cluster. Hence, we omit it here.) As can be seen, there were improvements of up to 10% from {a mathematical formula}N=10 to {a mathematical formula}N=50, and no (or very marginal) improvements from {a mathematical formula}N=50 to {a mathematical formula}N=100. This was observed for all {a mathematical formula}|Aj|=2,10,20, and all constellations of score functions. The fact that {a mathematical formula}N=50 was sufficient even for {a mathematical formula}|Aj|=20 is remarkable, since, under random behaviours, there are {a mathematical formula}20t possible action vectors to sample at any time t.</paragraph><paragraph>We also compared the learned skew-normal distributions and found that they fitted the data very well. Fig. 12, Fig. 13 show the histograms and fitted skew-normal distributions for two example processes after 1000 time steps. In Fig. 13, we deliberately chose an example in which the learned distribution was maximally skewed for {a mathematical formula}N=10, which is a sign that N was too small. Nonetheless, in the majority of the processes, the learned distribution was only moderately skewed and our algorithm achieved an average accuracy of 90% even for {a mathematical formula}N=10. Moreover, if one wants to avoid maximally skewed distributions, one can simply restrict the parameter space when fitting the skew-normal (specifically, the shape parameter β; cf. Section 7.2.3).</paragraph><paragraph>The flexibility of the skew-normal distribution was particularly useful in the early stages of the interaction, in which the test statistic typically does not follow a normal distribution. Fig. 14 shows the test distribution for an example process after 10 time steps, using {a mathematical formula}z2 for the test statistic and {a mathematical formula}N=100 (the histogram was created using {a mathematical formula}N=10000). The learned skew-normal approximated the true test distribution very closely. Note that, in such examples, the normal and Student distributions do not produce good fits.</paragraph><paragraph>Our implementation of the algorithm performed all calculations as iterative updates (except for the skew-normal fitting). Hence, it used little (fixed) memory and had very low computation times. For example, using all three score functions and {a mathematical formula}|Aj|=20, {a mathematical formula}N=100, one cycle in the algorithm (cf. Algorithm 2) took on average less than 1 millisecond without fitting the skew-normal parameters, and less than 10 milliseconds when fitting the skew-normal parameters (using an off-the-shelf Simplex-optimiser with default parameters). The times were measured using Matlab R2014a on a Unix machine with a 2.6 GHz Intel Core i5 processor.</paragraph></section><section label="7.3.2"><section-title>Adaptive behaviours</section-title><paragraph>We complemented the “structure-free” interaction of random behaviours by conducting analogous experiments with three additional classes of behaviours. Specifically, we used the benchmark framework specified in Section 5, which consists of 78 distinct {a mathematical formula}2×2 matrix games and three methods to automatically generate sets of behaviours for any given game. The three behaviour classes are Leader-Follower-Trigger Agents (LFT), Co-Evolved Decision Trees (CDT), and Co-Evolved Neural Networks (CNN). These classes cover a broad spectrum of possible behaviours, including fully deterministic (CDT), fully stochastic (CNN), and hybrid (LFT) behaviours. Furthermore, all generated behaviours are adaptive to varying degrees (i.e. they adapt their action choices based on the other player's choices). Detailed descriptions of the games and behaviour classes can be found in the appendix of [1].</paragraph><paragraph>The following experiments were performed for each behaviour class, using identical randomisation: For each of the 78 games, we simulated 10 interaction processes, each lasting 10 000 time steps. For each process, we randomly sampled behaviours {a mathematical formula}θi∈Θi, {a mathematical formula}θj+∈Θj to control agents i and j, respectively, where {a mathematical formula}Θi, {a mathematical formula}Θj were restricted to the same behaviour class. In half of these processes, we used a correct hypothesis {a mathematical formula}θj⁎=θj+, and in the other half, we sampled a random hypothesis {a mathematical formula}θj⁎∈Θj with {a mathematical formula}θj⁎≠θj+. As before, we repeated each simulation for {a mathematical formula}N=10,50,100 and all constellations of score functions, but found that there were virtually no differences. Hence, in the following, we report results for {a mathematical formula}N=50 and the {a mathematical formula}[z1,z2,z3] cluster.</paragraph><paragraph>Fig. 15a shows the average accuracy achieved by our algorithm for all three behaviour classes. While the accuracy for {a mathematical formula}θj⁎=θj+ was generally good, the accuracy for {a mathematical formula}θj⁎≠θj+ was mixed. Note that this was not merely due to the fact that the score functions were imperfect (cf. Section 7.2.1), since we obtained the same results for all combinations. Rather, this reveals an inherent limitation of our approach, which is that we do not actively probe aspects of the hypothesis{a mathematical formula}θj⁎. In other words, our algorithm performs statistical hypothesis tests based only on evidence that was generated by {a mathematical formula}θi.</paragraph><paragraph>To illustrate this, it is useful to consider the tree structure of behaviours in the CDT class. Each node in a tree {a mathematical formula}θj+ corresponds to a past action taken by {a mathematical formula}θi. Depending on how {a mathematical formula}θi chooses actions, we may only ever see a subset of the entire tree that defines {a mathematical formula}θj+. However, if our hypothesis {a mathematical formula}θj⁎ differs from {a mathematical formula}θj+ only in the unseen aspects of {a mathematical formula}θj+, then there is no way for our algorithm to differentiate the two. Hence the asymmetry in accuracy for {a mathematical formula}θj⁎=θj+ and {a mathematical formula}θj⁎≠θj+. Note that this problem did not occur in random behaviours because, there, all aspects are eventually visible.</paragraph><paragraph>Following this observation, we repeated the same experiments but restricted {a mathematical formula}Θi to random behaviours (cf. Section 7.3.1), with the goal of exploring {a mathematical formula}θj⁎ more thoroughly. As shown in Fig. 15b, this led to significant improvements in accuracy, especially for the CDT class. Nonetheless, choosing actions purely randomly may not be a sufficient probing strategy, hence the accuracy for CNN was still relatively low. For CNN, this was further complicated by the fact that two neural networks {a mathematical formula}θj,θj′ may formally be different ({a mathematical formula}θj≠θj′) but have essentially the same action probabilities (with extremely small differences). Hence, in such cases, we would require much more evidence to distinguish the behaviours.</paragraph></section></section></section><section label="8"><section-title>Conclusion</section-title><paragraph>Much work in artificial intelligence is focused on innovative applications such as adaptive user interfaces, robotic elderly care, and automated trading agents. A key technological challenge in these applications is to design an intelligent agent which can quickly learn to interact effectively with other agents whose behaviours are initially unknown. Learning from scratch in such problems is not a viable solution, since time is a crucial factor and exploration via trial-and-error may not be feasible or desirable. Instead, it is likely that any solution to this problem will have to draw heavily on prior experience and intuition, such as in the form of hypothesised behaviours. Indeed, if we have a strong intuition regarding the behaviour of other agents, e.g. based on past experience or structural constraints of the task to be completed, then this intuition should be utilised in the interaction. This is the motivation behind the type-based method studied in this work.</paragraph><paragraph>The idea in the type-based method is to hypothesise a set of possible behaviours, or types, which the other agents might have, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. In this regard, we identified and addressed a spectrum of important questions, pertaining to properties of beliefs over types and the possibility of incorrect types. Specifically, we formulated three alternative methods to incorporate observations into beliefs and studied the conditions under which the resulting beliefs will be correct or incorrect. We then investigated the impact of prior beliefs on payoff maximisation and methods to automatically compute prior beliefs. For the case in which our hypothesised types are incorrect, we analysed the conditions under which we are nevertheless able to complete our task, despite the incorrectness of types. Finally, we described an automatic statistical analysis which can be used to ascertain the correctness of hypothesised types during the interaction.</paragraph><paragraph>In addition to the theoretical insights, the results presented in this article have a number of practical implications: First of all, our analysis in Section 4 shows that the standard posterior formulation, in which the likelihood is defined as a product of action probabilities, may not always be an appropriate choice. Rather, one should also consider alternative formulations for posterior beliefs, such as the sum or correlated posteriors. Furthermore, our empirical analysis in Section 5 shows that prior beliefs can be crucial to our ability to maximise payoffs in the long-term. Indeed, we can often do better than a conservative uniform prior belief, by using automatic methods such as the ones used in this work. Another important practical implication is that we can use efficient model checking methods to verify optimality of hypothesised types. Specifically, in Section 6, we show a useful connection to probabilistic bisimulation checking. Moreover, for the case in which a prior analysis based on bisimulation is not possible, we show that the correctness of types can still be contemplated during the interaction. Our algorithm in Section 7 is simple to implement, highly efficient, and achieves high accuracy and scalability.</paragraph><paragraph>There are several potential directions for future work: Further formulations of posterior beliefs could be developed, and it would be interesting to know if the asymptotic correctness analysis in Section 4 could be complemented by useful finite-time error bounds. Our empirical analysis of prior beliefs in Section 5 could be refined by a theoretical analysis, and an important question is if prior beliefs can be computed with useful error bounds (the LP-priors are a step in this direction). Furthermore, the optimality analysis in Section 6 is focused on task completion and could be extended by an analysis focusing on payoff maximisation. Finally, it is unclear if the concept of perfect scores in Section 7 is generally feasible or even necessary, and what impact score weights have on convergence and decision quality.</paragraph><paragraph>Two aspects which we did not address, yet which are crucial to a successful deployment of the type-based method, are the complexity of the planning step and the size of the hypothesised type spaces. Regarding the former, it can be seen in Algorithm 1 (specifically (3)/(4)) that the time complexity of the planning is exponential in factors such as the number of agents, actions, and states, making it a very costly operation in complex systems. A promising solution are stochastic sampling procedures such as those used in [4], [11]. Regarding the latter, the problem is that the number of types one may wish to hypothesise can grow dramatically with the size of the interaction problem (e.g. states, actions, agents). This is problematic because the predictions of each type must be computed at each point in time, hence it is desirable to minimise the number of hypothesised types. One way to do so is to develop methods which can produce small sets of reasonable types with good coverage of behaviours, in the spirit of works such as [32]. Another method would be to introduce learnable structure in types (i.e. parameters) such that each type covers a spectrum of behaviours. However, this would require an ability to infer the parameters from the interaction history.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>The authors acknowledge the support of the German National Academic Foundation, the Masdar Institute–MIT collaborative agreement (Flagship Project 13CAMA1), the UKEngineering and Physical Sciences Research Council (EP/H012338/1), the European Commission (TOMSY Grant 270436, FP7-ICT-2009.2.1 Call 6), the Royal Academy of Engineering (Ingenious Grant MEI01ING14 and RAM01ING12), and the European Commission through SmartSociety Grant agreement no. 600854 (FOCAS ICT-2011.9.10). The authors wish to thank anonymous reviewers for their comments and suggestions.</paragraph></acknowledgements><appendices><section label="Appendix A">Proof of Theorem 1<paragraph label="Proof">Kalai and Lehrer [65] studied a model which can be equivalently described as a single-state SBG (i.e. {a mathematical formula}|S|=1) with a pure type distribution and product posterior. They showed that, if the player's assessment of future play is absolutely continuous with respect to the true probabilities of future play (i.e. any event that has true positive probability is assigned positive probability by the player), then (6) must hold. In our case, absolute continuity always holds by Assumption 1 and the fact that the prior probabilities {a mathematical formula}Pj are positive as well as the fact that the type distribution is pure, from which we can infer that the true types always have positive posterior probability.In this proof, we seek to extend the convergence result of Kalai and Lehrer [65] (henceforth KL) to multi-state SBGs with pure type distributions. Our strategy is to translate a SBG Γ into a modified SBG{a mathematical formula}Γˆ which is equivalent to Γ in the sense that the players behave identically, and which is compatible to the model used in KL in the sense that the informational assumptions therein ignore the differences. We achieve this by introducing a new player nature, denoted ξ, which emulates the transitions of Γ in {a mathematical formula}Γˆ.Given a SBG {a mathematical formula}Γ=(S,s0,S¯,N,Ai,Θi,ui,πi,T,ϒ), we define the modified SBG {a mathematical formula}Γˆ as follows: Firstly, {a mathematical formula}Γˆ has only one state, which can be arbitrary since it has no effect. The players in {a mathematical formula}Γˆ are {a mathematical formula}Nˆ=N∪{ξ} where {a mathematical formula}i∈N have the same actions and types as in Γ (i.e. {a mathematical formula}Ai and {a mathematical formula}Θi), and where we define the actions and types of ξ to be {a mathematical formula}Aξ=Θξ=S (i.e. nature's actions and types correspond to the states of Γ). The payoffs of ξ are always zero and the strategy of ξ at time t is defined as{a mathematical formula} where {a mathematical formula}Hτ is any history of length {a mathematical formula}τ≥t. ({a mathematical formula}Hτ allows the players {a mathematical formula}i∈N to use {a mathematical formula}πξt for future predictions about ξ's actions. This will be necessary to establish equivalence of {a mathematical formula}Γˆ and Γ.)The purpose of ξ is to emulate the state transitions of Γ. Therefore, the modified strategies {a mathematical formula}πˆi and payoffs {a mathematical formula}uˆi of {a mathematical formula}i∈N are now defined with respect to the actions and types (since the current type of ξ determines its next action) of ξ. Formally, {a mathematical formula}πˆi(Ht,ai,θi)=πi(H¯t,ai,θi) where{a mathematical formula} and {a mathematical formula}uˆi(s,at,θit)=ui(θξt,(ajt)j∈N,θit), where s is the only state of {a mathematical formula}Γˆ and {a mathematical formula}at∈×i∈NˆAi.Finally, {a mathematical formula}Γˆ uses two type distributions, ϒ and {a mathematical formula}ϒξ, where ϒ is the type distribution of Γ and {a mathematical formula}ϒξ is defined as {a mathematical formula}ϒξ(Ht,θξ)=T(aξt−1,(ait−1)i∈N,θξ). If {a mathematical formula}s0 is the initial state of Γ, then {a mathematical formula}ϒξ(H0,θξ)=1 for {a mathematical formula}θξ≡s0.The modified SBG {a mathematical formula}Γˆ proceeds as the original SBG Γ, except for the following changes: (a) ϒ is used to sample the types for {a mathematical formula}i∈N (as usual) while {a mathematical formula}ϒξ is used to sample the types for ξ; (b) each player is informed about its own type and the type of ξ. This completes the definition of {a mathematical formula}Γˆ.The modified SBG {a mathematical formula}Γˆ is equivalent to the original SBG Γ in the sense that the players {a mathematical formula}i∈N have identical behaviour in both SBGs. Since the players always know the type of ξ, they also know the next action of ξ, which corresponds to knowing the current state of the game. Furthermore, note that the strategy of ξ uses two time indices, t and τ, which allow it to distinguish between the current time ({a mathematical formula}τ=t) and a future time ({a mathematical formula}τ&gt;t). This means that {a mathematical formula}πξt can be used to compute expected payoffs in {a mathematical formula}Γˆ in the same way as T is used to compute expected payoffs in Γ. In other words, the formulas (2) and (3) can be modified in a straightforward manner by replacing the original components of Γ with the modified components of {a mathematical formula}Γˆ, yielding the same results. Finally, since {a mathematical formula}Γˆ uses the same type distribution as Γ to sample types for {a mathematical formula}i∈N, there are no differences in their payoffs and strategies.To complete the proof, we note that (a) and (b) are the only procedural differences between the modified SBG and the model used in KL. However, since we specify that the players always know the type of ξ, there is no need to learn the type distribution {a mathematical formula}ϒξ, hence (a) and (b) have no effect in KL. The important point is that KL assume a model in which the players only interact with other players, but not with an environment. Since we eliminated the environment by replacing it with a player ξ, this is precisely what happens in the modified SBG. Therefore, the convergence result of KL carries over to multi-state SBGs with pure type distributions. □</paragraph></section></appendices><references><reference label="[1]">S. AlbrechtUtilising policy types for effective ad hoc coordination in multiagent systemsPh.D. thesis<host>(2015)The University of Edinburgh</host></reference><reference label="[2]"><authors>S. Albrecht,J. Crandall,S. Ramamoorthy</authors><title>An empirical study on the practical impact of prior beliefs over policy types</title><host>Proceedings of the 29th AAAI Conference on Artificial Intelligence(2015) pp.1988-1994</host></reference><reference label="[3]"><authors>S. Albrecht,S. Ramamoorthy</authors><title>Comparative evaluation of MAL algorithms in a diverse set of ad hoc team problems</title><host>Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems(2012) pp.349-356</host></reference><reference label="[4]">S. Albrecht,S. RamamoorthyA game-theoretic model and best-response learning method for ad hoc coordination in multiagent systemsTech. rep.<host>(2013)School of Informatics, The University of Edinburgh</host><host>arXiv:1506.01170</host></reference><reference label="[5]"><authors>S. Albrecht,S. Ramamoorthy</authors><title>A game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems (extended abstract)</title><host>Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems(2013) pp.1155-1156</host></reference><reference label="[6]"><authors>S. Albrecht,S. Ramamoorthy</authors><title>On convergence and optimality of best-response learning with policy types in multiagent systems</title><host>Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence(2014) pp.12-21</host></reference><reference label="[7]"><authors>S. Albrecht,S. Ramamoorthy</authors><title>Are you doing what I think you are doing? Criticising uncertain agent models</title><host>Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence(2015) pp.52-61</host></reference><reference label="[8]"><authors>R. Aumann</authors><title>Subjectivity and correlation in randomized strategies</title><host>J. Math. Econ.1 (1974) pp.67-96</host></reference><reference label="[9]"><authors>A. Azzalini</authors><title>A class of distributions which includes the normal ones</title><host>Scand. J. Stat.12 (1985) pp.171-178</host></reference><reference label="[10]"><authors>C. Baier</authors><title>Polynomial time algorithms for testing probabilistic bisimulation and simulation</title><host>Proceedings of the 8th International Conference on Computer Aided VerificationLecture Notes in Computer Sciencevol. 1102 (1996)Springer pp.38-49</host></reference><reference label="[11]"><authors>S. Barrett,P. Stone,S. Kraus</authors><title>Empirical evaluation of ad hoc teamwork in the pursuit domain</title><host>Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems(2011) pp.567-574</host></reference><reference label="[12]"><authors>S. Barrett,P. Stone,S. Kraus,A. Rosenfeld</authors><title>Teamwork with limited knowledge of teammates</title><host>Proceedings of the 27th AAAI Conference on Artificial Intelligence(2013) pp.102-108</host></reference><reference label="[13]"><authors>I. Basawa,D. Scott</authors><title>Efficient tests for stochastic processes</title><host>Sankhya, Ser. A (1977) pp.21-31</host></reference><reference label="[14]"><authors>M. Bayarri,J. Berger</authors><title>P values for composite null models</title><host>J. Am. Stat. Assoc.95 (452)(2000) pp.1127-1142</host></reference><reference label="[15]"><authors>R. Bellman</authors><title>Dynamic Programming</title><host>(1957)Princeton University Press</host></reference><reference label="[16]"><authors>J. Berger,T. Sellke</authors><title>Testing a point null hypothesis: the irreconcilability of P values and evidence (with discussion)</title><host>J. Am. Stat. Assoc.82 (1987) pp.112-122</host></reference><reference label="[17]"><authors>J. Bernardo</authors><title>Reference posterior distributions for Bayesian inference</title><host>J. R. Stat. Soc. B41 (2)(1979) pp.113-147</host></reference><reference label="[18]"><authors>D. Bernstein,S. Zilberstein,N. Immerman</authors><title>The complexity of decentralized control of Markov decision processes</title><host>Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence(2000) pp.32-37</host></reference><reference label="[19]"><authors>M. Bowling,P. McCracken</authors><title>Coordination and adaptation in impromptu teams</title><host>Proceedings of the 20th National Conference on Artificial Intelligence(2005) pp.53-58</host></reference><reference label="[20]"><authors>M. Bowling,M. Veloso</authors><title>Multiagent learning using a variable learning rate</title><host>Artif. Intell.136 (2)(2002) pp.215-250</host></reference><reference label="[21]"><authors>G. Box</authors><title>Sampling and Bayes' inference in scientific modelling and robustness</title><host>J. R. Stat. Soc. A (1980) pp.383-430</host></reference><reference label="[22]"><authors>R. Brafman,M. Tennenholtz</authors><title>R-max—a general polynomial time algorithm for near-optimal reinforcement learning</title><host>J. Mach. Learn. Res.3 (2003) pp.213-231</host></reference><reference label="[23]"><authors>G. Brown</authors><title>Iterative solution of games by fictitious play</title><host>Proceedings of the Conference on Activity Analysis of Production and Allocation(1951) pp.374-376</host></reference><reference label="[24]"><authors>S. Carberry</authors><title>Techniques for plan recognition</title><host>User Model. User-Adapt. Interact.11 (1–2)(2001) pp.31-48</host></reference><reference label="[25]"><authors>D. Carmel,S. Markovitch</authors><title>Exploration strategies for model-based learning in multi-agent systems: exploration strategies</title><host>Auton. Agents Multi-Agent Syst.2 (2)(1999) pp.141-172</host></reference><reference label="[26]"><authors>G. Chalkiadakis,C. Boutilier</authors><title>Coordination in multiagent reinforcement learning: a Bayesian approach</title><host>Proceedings of the 2nd International Conference on Autonomous Agents and Multiagent Systems(2003) pp.709-716</host></reference><reference label="[27]"><authors>E. Charniak,R. Goldman</authors><title>A Bayesian model of plan recognition</title><host>Artif. Intell.64 (1)(1993) pp.53-79</host></reference><reference label="[28]"><authors>V. Conitzer,T. Sandholm</authors><title>AWESOME: a general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents</title><host>Mach. Learn.67 (1–2)(2007) pp.23-43</host></reference><reference label="[29]"><authors>V. Conitzer,T. Sandholm</authors><title>New complexity results about Nash equilibria</title><host>Games Econ. Behav.63 (2)(2008) pp.621-641</host></reference><reference label="[30]"><authors>D. Cox</authors><title>The role of significance tests (with discussion)</title><host>Scand. J. Stat.4 (1977) pp.49-70</host></reference><reference label="[31]"><authors>J. Crandall</authors><title>Towards minimizing disappointment in repeated games</title><host>J. Artif. Intell. Res.49 (2014) pp.111-142</host></reference><reference label="[32]"><authors>J. Crandall</authors><title>Robust learning in repeated stochastic games using meta-gaming</title><host>Proceedings of the 24th International Joint Conference on Artificial Intelligence(2015) pp.3416-3422</host></reference><reference label="[33]"><authors>B. De Finetti</authors><title>Philosophical Lectures on Probability: Collected, Edited, and Annotated by Alberto Mura</title><host>(2008)Springer</host></reference><reference label="[34]"><authors>R. Dearden,N. Friedman,D. Andre</authors><title>Model based Bayesian exploration</title><host>Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence(1999) pp.150-159</host></reference><reference label="[35]"><authors>E. Dekel,D. Fudenberg,D. Levine</authors><title>Learning to play Bayesian games</title><host>Games Econ. Behav.46 (2)(2004) pp.282-303</host></reference><reference label="[36]"><authors>J. Dibangoye,C. Amato,A. Doniec,F. Charpillet</authors><title>Producing efficient error-bounded solutions for transition independent decentralized MDPs</title><host>Proceedings of the 12th International Conference on Autonomous Agents and Multiagent Systems(2013) pp.539-546</host></reference><reference label="[37]"><authors>P. Doshi,P. Gmytrasiewicz</authors><title>On the difficulty of achieving equilibrium in interactive POMDPs</title><host>Proceedings of the 21st National Conference on Artificial Intelligence(2006) pp.1131-1136</host></reference><reference label="[38]"><authors>P. Doshi,P. Gmytrasiewicz</authors><title>Monte Carlo sampling methods for approximating interactive POMDPs</title><host>J. Artif. Intell. Res. (2009) pp.297-337</host></reference><reference label="[39]"><authors>P. Doshi,D. Perez</authors><title>Generalized point based value iteration for interactive POMDPs</title><host>Proceedings of the 23rd AAAI Conference on Artificial Intelligence(2008) pp.63-68</host></reference><reference label="[40]"><authors>P. Doshi,X. Qu,A. Goodie,D. Young</authors><title>Modeling recursive reasoning by humans using empirically informed interactive POMDPs</title><host>Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems(2010) pp.1223-1230</host></reference><reference label="[41]"><authors>P. Doshi,Y. Zeng,Q. Chen</authors><title>Graphical models for interactive POMDPs: representations and solutions</title><host>Auton. Agents Multi-Agent Syst.18 (3)(2009) pp.376-416</host></reference><reference label="[42]"><authors>R. Emery-Montemerlo,G. Gordon,J. Schneider,S. Thrun</authors><title>Approximate solutions for partially observable stochastic games with common payoffs</title><host>Proceedings of the 3rd International Conference on Autonomous Agents and Multiagent Systems(2004) pp.136-143</host></reference><reference label="[43]"><authors>K. Etessami,M. Yannakakis</authors><title>On the complexity of Nash equilibria and other fixed points</title><host>SIAM J. Comput.39 (6)(2010) pp.2531-2597</host></reference><reference label="[44]"><authors>H. Fischer</authors><title>A History of the Central Limit Theorem: From Classical to Modern Probability Theory</title><host>(2010)Springer Science &amp; Business Media</host></reference><reference label="[45]"><authors>R. Fisher</authors><title>The Design of Experiments</title><host>(1935)Oliver &amp; Boyd</host></reference><reference label="[46]"><authors>D. Foster,H. Young</authors><title>On the impossibility of predicting the behavior of rational agents</title><host>Proc. Natl. Acad. Sci.98 (22)(2001) pp.12848-12853</host></reference><reference label="[47]"><authors>D. Foster,H. Young</authors><title>Learning, hypothesis testing, and Nash equilibrium</title><host>Games Econ. Behav.45 (1)(2003) pp.73-96</host></reference><reference label="[48]"><authors>D. Fudenberg,D. Levine</authors><title>Self-confirming equilibrium</title><host>Econometrica (1993) pp.523-545</host></reference><reference label="[49]"><authors>Y. Gal,A. Pfeffer,F. Marzo,B. Grosz</authors><title>Learning social preferences in games</title><host>Proceedings of the 19th National Conference on Artificial Intelligence(2004) pp.226-231</host></reference><reference label="[50]"><authors>C. Geib,R. Goldman</authors><title>A probabilistic plan recognition algorithm based on plan tree grammars</title><host>Artif. Intell.173 (11)(2009) pp.1101-1132</host></reference><reference label="[51]"><authors>A. Gelman,C. Shalizi</authors><title>Philosophy and the practice of Bayesian statistics</title><host>Br. J. Math. Stat. Psychol.66 (1)(2013) pp.8-38</host></reference><reference label="[52]"><authors>P. Gmytrasiewicz,P. Doshi</authors><title>A framework for sequential planning in multiagent settings</title><host>J. Artif. Intell. Res.24 (1)(2005) pp.49-79</host></reference><reference label="[53]"><authors>B. Grosz,S. Kraus</authors><title>Collaborative plans for complex group action</title><host>Artif. Intell.86 (2)(1996) pp.269-357</host></reference><reference label="[54]"><authors>E. Hansen,D. Bernstein,S. Zilberstein</authors><title>Dynamic programming for partially observable stochastic games</title><host>Proceedings of the 19th National Conference on Artificial Intelligence(2004) pp.709-715</host></reference><reference label="[55]"><authors>H. Hansson,B. Jonsson</authors><title>A logic for reasoning about time and reliability</title><host>Form. Asp. Comput.6 (5)(1994) pp.512-535</host></reference><reference label="[56]"><authors>J. Harsanyi</authors><title>Games with incomplete information played by “Bayesian” players. Part I. The basic model</title><host>Manag. Sci.14 (3)(1967) pp.159-182</host></reference><reference label="[57]"><authors>J. Harsanyi</authors><title>Games with incomplete information played by “Bayesian” players. Part II. Bayesian equilibrium points</title><host>Manag. Sci.14 (5)(1968) pp.320-334</host></reference><reference label="[58]"><authors>J. Harsanyi</authors><title>Games with incomplete information played by “Bayesian” players. Part III. The basic probability distribution of the game</title><host>Manag. Sci.14 (7)(1968) pp.486-502</host></reference><reference label="[59]"><authors>J. Holland</authors><title>Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence</title><host>(1975)The MIT Press</host></reference><reference label="[60]"><authors>R. Howard</authors><title>Information value theory</title><host>IEEE Trans. Syst. Sci. Cybern.2 (1)(1966) pp.22-26</host></reference><reference label="[61]"><authors>J. Hu,M. Wellman</authors><title>Nash q-learning for general-sum stochastic games</title><host>J. Mach. Learn. Res.4 (2003) pp.1039-1069</host></reference><reference label="[62]"><authors>E. Jaynes</authors><title>Prior probabilities</title><host>IEEE Trans. Syst. Sci. Cybern.4 (3)(1968) pp.227-241</host></reference><reference label="[63]"><authors>J. Jordan</authors><title>Bayesian learning in normal form games</title><host>Games Econ. Behav.3 (1)(1991) pp.60-81</host></reference><reference label="[64]"><authors>D. Kahneman,A. Tversky</authors><title>Prospect theory: an analysis of decision under risk</title><host>Econometrica47 (1979) pp.263-292</host></reference><reference label="[65]"><authors>E. Kalai,E. Lehrer</authors><title>Rational learning leads to Nash equilibrium</title><host>Econometrica61 (5)(1993) pp.1019-1045</host></reference><reference label="[66]"><authors>G. Kaminka,I. Frenkel</authors><title>Integration of coordination mechanisms in the BITE multi-robot architecture</title><host>Proceedings of the International Conference on Robotics and Automation(2007) pp.2859-2866</host></reference><reference label="[67]"><authors>J. Koza</authors><title>Genetic Programming: On the Programming of Computers by Means of Natural Selection</title><host>(1992)The MIT Press</host></reference><reference label="[68]"><authors>H. Kuhn</authors><title>Extensive games and the problem of information</title><host>Contributions to the Theory of Games, vol. 2Annals of Mathematics Studiesvol. 28 (1953) pp.193-216</host></reference><reference label="[69]"><authors>K. Larsen,A. Skou</authors><title>Bisimulation through probabilistic testing</title><host>Inf. Comput.94 (1)(1991) pp.1-28</host></reference><reference label="[70]"><authors>M. Littman</authors><title>Markov games as a framework for multi-agent reinforcement learning</title><host>Proceedings of the 11th International Conference on Machine Learningvol. 157 (1994) pp.157-163</host></reference><reference label="[71]"><authors>X.-L. Meng</authors><title>Posterior predictive p-values</title><host>Ann. Stat. (1994) pp.1142-1160</host></reference><reference label="[72]"><authors>J. Nachbar</authors><title>Prediction, optimization, and learning in repeated games</title><host>Econometrica65 (2)(1997) pp.275-309</host></reference><reference label="[73]"><authors>J. Nachbar</authors><title>Beliefs in repeated games</title><host>Econometrica73 (2)(2005) pp.459-480</host></reference><reference label="[74]"><authors>J. Nash</authors><title>Equilibrium points in n-person games</title><host>Proc. Natl. Acad. Sci.36 (1)(1950) pp.48-49</host></reference><reference label="[75]"><authors>B. Ng,C. Meyers,K. Boakye,J. Nitao</authors><title>Towards applying interactive POMDPs to real-world adversary modeling</title><host>Proceedings of the 22nd Innovative Applications of Artificial Intelligence Conference(2010) pp.1814-1820</host></reference><reference label="[76]"><authors>Y. Nyarko</authors><title>Bayesian learning and convergence to Nash equilibria without common priors</title><host>Econ. Theory11 (3)(1998) pp.643-655</host></reference><reference label="[77]"><authors>A. O'Hagan,T. Leonard</authors><title>Bayes estimation subject to uncertainty about parameter constraints</title><host>Biometrika63 (1)(1976) pp.201-203</host></reference><reference label="[78]"><authors>A. Rapoport,M. Guyer</authors><title>A taxonomy of 2×2 games</title><host>General Systems: Yearbook of the Society for General Systems Researchvol. 11 (1966) pp.203-214</host></reference><reference label="[79]"><authors>D. Rubin</authors><title>Bayesianly justifiable and relevant frequency calculations for the applied statistician</title><host>Ann. Stat.12 (4)(1984) pp.1151-1172</host></reference><reference label="[80]"><authors>D. Ryabko,B. Ryabko</authors><title>On hypotheses testing for ergodic processes</title><host>Proceedings of IEEE Information Theory Workshop(2008) pp.281-283</host></reference><reference label="[81]"><authors>L. Shapley</authors><title>Stochastic games</title><host>Proc. Natl. Acad. Sci. USA39 (10)(1953) pp.1095-</host></reference><reference label="[82]"><authors>F. Southey,M. Bowling,B. Larson,C. Piccione,N. Burch,D. Billings,C. Rayner</authors><title>Bayes' bluff: opponent modelling in poker</title><host>Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence(2005) pp.550-558</host></reference><reference label="[83]"><authors>P. Stone,G. Kaminka,S. Kraus,J. Rosenschein</authors><title>Ad hoc autonomous agent teams: collaboration without pre-coordination</title><host>Proceedings of the 24th AAAI Conference on Artificial Intelligence(2010) pp.1504-1509</host></reference><reference label="[84]"><authors>G. Sukthankar,R. Goldman,C. Geib,D. Pynadath,H. Bui</authors><title>Plan, Activity, and Intent Recognition: Theory and Practice</title><host>(2014)Morgan Kaufmann</host></reference><reference label="[85]"><authors>R. Sutton,A. Barto</authors><title>Reinforcement Learning: An Introduction</title><host>(1998)The MIT Press</host></reference><reference label="[86]"><authors>M. Tambe</authors><title>Towards flexible teamwork</title><host>J. Artif. Intell. Res.7 (1997) pp.83-124</host></reference><reference label="[87]"><authors>Y. Yue,Y. Gao,O. Chapelle,Y. Zhang,T. Joachims</authors><title>Learning more powerful test statistics for click-based retrieval evaluation</title><host>Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval(2010) pp.507-514</host></reference></references><footnote><note-para label="1">The interpretation of types as behaviours is consistent with the original definition of Harsanyi, who defines types as parameters for both payoff and strategy functions (cf. Section 7 in [56]). See also Dekel et al. [35].</note-para><note-para label="2">The controversy was centred around the assumption that players a priori know the true distribution of types. (From a personal conversation with Reinhard Selten.)</note-para><note-para label="3">We thank an anonymous reviewer for suggesting this line of thought.</note-para><note-para label="4">All examples in this section assume {a mathematical formula}Θj⁎=Θj+ and uniform prior beliefs {a mathematical formula}Pj(θj⁎)=|Θj⁎|−1.</note-para></footnote></root>