<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370217300905</url><title>MM: A bidirectional search algorithm that is guaranteed to meet in the middle</title><authors>Robert C. Holte,Ariel Felner,Guni Sharon,Nathan R. Sturtevant,Jingwei Chen</authors><abstract>Bidirectional search algorithms interleave two separate searches, a normal search forward from the start state, and a search backward from the goal. It is well known that adding a heuristic to unidirectional search dramatically reduces the search effort. By contrast, despite decades of research, bidirectional heuristic search has not yet had a major impact. Additionally, no comprehensive theory was ever devised to understand the nature of bidirectional heuristic search. In this paper we aim to close this gap. We first present MM, a novel bidirectional heuristic search algorithm. Unlike previous bidirectional heuristic search algorithms, MM's forward and backward searches are guaranteed to “meet in the middle”, i.e. never expand a node beyond the solution midpoint. Based on this unique attribute we present a novel framework for comparing MM, A*, and their brute-force variants. We do this by dividing the entire state space into disjoint regions based on their distance from the start and goal. This allows us to perform a comparison of these algorithms on a per region basis and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis.</abstract><keywords>Heuristic search;Bidirectional search</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>In a least-cost path problem over a state space the task is to find a least-cost path from an initial state ({a mathematical formula}start) to a goal state (goal). Breadth-first search and its weighted version uniform cost search (Dijkstra's algorithm [12]) are best-first search algorithms designed to solve least-cost path problems. They are guided by the cost function {a mathematical formula}f(n)=g(n) where {a mathematical formula}g(n) is the cost of the cheapest known path from {a mathematical formula}start to node n. We use the term unidirectional brute-force search, denoted Uni-BS, to refer to these algorithms.</paragraph><paragraph>The A* algorithm [23] enhances Uni-BS by using {a mathematical formula}f(n)=g(n)+h(n) to prioritize nodes, where {a mathematical formula}h(n) is a heuristic function estimating the cost from n to {a mathematical formula}goal. If {a mathematical formula}h(n) is admissible (i.e., is always a lower bound) then A* is guaranteed to find optimal (least-cost) solutions. The main purpose of the heuristic function is to focus the search towards the goal. This is depicted in Fig. 1 (left). {a mathematical formula}C⁎ is the cost of an optimal solution, i.e. the distance from {a mathematical formula}start to {a mathematical formula}goal. The circle of radius {a mathematical formula}C⁎ represents the states expanded by Uni-BS. The oval inside this circle represents the states expanded by A*. Nodes inside the circle will be expanded by Uni-BS but not by A* if they have a sufficiently large heuristic value (h-value). By using a heuristic A* can reduce the number of nodes expanded by many orders of magnitude compared to Uni-BS. A* and its many variants are therefore commonly used when solving search problems.</paragraph><paragraph>A bidirectional search algorithm interleaves two separate searches, a normal search forward from {a mathematical formula}start, and a search backward (i.e. using reverse operators) from {a mathematical formula}goal. In the backward search {a mathematical formula}g(n) measures the cost of the reverse path from {a mathematical formula}goal to n, which is the cost of the forward version of the path from n to {a mathematical formula}goal. Every node that has been generated in both directions represents a solution, i.e. a path, possibly suboptimal, from {a mathematical formula}start to {a mathematical formula}goal. The first solution found is not, in general, optimal, so additional search, or some sort of additional processing, is required after the two searches have met for the first time to ensure the solution returned is optimal (see Section 2 for details). In its simplest form [47], bidirectional brute-force search, denoted Bi-BS, is guided by {a mathematical formula}g(n), just like Uni-BS, i.e. it selects for expansion a node n with minimum {a mathematical formula}g(n) among all the nodes that are open in either search direction. Selecting nodes in this way guarantees that the forward and backward searches “meet in the middle”, in the sense that the forward search expands no state whose distance from {a mathematical formula}start is greater than {a mathematical formula}12C⁎ and, likewise, the backward search expands no state whose distance to {a mathematical formula}goal is greater than {a mathematical formula}12C⁎. This is the definition of “meet in the middle” we will use throughout this paper.{sup:1} The nodes expanded by a Bi-BS system that meets in the middle are depicted by the two smaller circles in Fig. 1 (right). In exponentially growing spaces, a Bi-BS system that meets in the middle can expand exponentially fewer nodes than Uni-BS.</paragraph><paragraph>Since A* and Bi-BS speed up Uni-BS in two different ways, it is natural to try to combine them together into bidirectional heuristic search (denoted Bi-HS). The expectation in combining them is that their benefits will be compounded. By being bidirectional (as Bi-BS is) Bi-HS will be constrained to only expand nodes around {a mathematical formula}start and {a mathematical formula}goal. By using a heuristic (as A* does), it will expand a small subset of those nodes. This expectation is depicted graphically in Fig. 2, where the ovals representing nodes expanded by Bi-HS are a small subset of the circles representing nodes expanded by Bi-BS. Pursuit of this idea began 50 years ago [14], [15] but to date has produced little success in meeting these expectations.</paragraph><paragraph>An intuitive explanation for the failure of Bi-HS to live up to expectations was presented by Barker and Korf [4]. Their analysis resulted in two main conclusions (for caveats, see their Section 3), which we call BK1 and BK2:</paragraph><list><list-item label="•">BK1: if more than half of the nodes expanded by Uni-HS have {a mathematical formula}g(n)≤{a mathematical formula}12C⁎, then Uni-HS will expand fewer nodes than Bi-HS.</list-item><list-item label="•">BK2: If fewer than half of the nodes expanded by Uni-HS using heuristic h have {a mathematical formula}g(n)≤12C⁎, then adding h to Bi-BS will not decrease the number of nodes it expands.</list-item></list><paragraph> In other words, Barker and Korf claim that except for pathological cases (see their Section 3) there is no situation in which Bi-HS will be the method of choice; either Uni-HS will be best or Bi-BS will.{sup:2}</paragraph><paragraph>A central assumption in Barker and Korf's analysis is that Bi-HS's forward and backward searches meet in the middle, in the sense we have defined. However, no known Bi-HS algorithm is guaranteed to meet in the middle under all circumstances (see Section 3). As a consequence, Barker and Korf's analysis does not immediately apply to existing Bi-HS systems. More importantly, because existing Bi-HS systems are not constrained to search within the two circles in Fig. 2 they can expand nodes that are further than {a mathematical formula}12C⁎ from both {a mathematical formula}start and {a mathematical formula}goal. For example, in Barker and Korf's Rubik's Cube experiment BS* [40] often expanded nodes at depth 13 in each direction even though {a mathematical formula}C⁎ was 16 or less.{sup:3} Because of this, it is easy to imagine existing Bi-HS systems expanding more nodes than Bi-BS instead of fewer.</paragraph><paragraph>This paper aims to address these issues and makes the following contributions:</paragraph><list><list-item label="1.">A new Bi-HS algorithm, MM, along with a formal proof that given an admissible heuristic (not necessarily consistent), MM is guaranteed to meet in the middle and to return an optimal solution.</list-item><list-item label="2.">The brute-force version of MM (using {a mathematical formula}h(s)=0 ∀s), MM0, is equivalent to Nicholson's Bi-BS algorithm [47] but with an improved termination condition.</list-item><list-item label="3.">An enhanced variant of MM called MMe which has an enhanced priority rule for expanding nodes.</list-item><list-item label="4.">A version of MM adapted for parallel external-memory search called PEMM. PEMM can solve much larger problems than would be possible in RAM.</list-item><list-item label="5.">A new analytical framework that divides the entire state space into disjoint regions based on the distances of nodes from {a mathematical formula}start and {a mathematical formula}goal. Because MM meets in the middle, we can use this framework to do a careful comparison of MM0, Uni-BS, MM, and A* on a region-by-region basis. We use this framework to identify conditions under which one method is expected to expand fewer nodes than another. We summarize this analysis by providing general rules, based on certain characteristics of the state space and the relative strength of the heuristic, that predict which algorithm is expected to expand the fewest nodes.</list-item><list-item label="6.">The general rules we propose are tested experimentally in three domains, the 10-Pancake puzzle, a grid-based map from the game Dragon Age: Origins, and Rubik's Cube. The first two domains are small enough to allow a fully detailed examination of each algorithm's node expansions in each of the regions we defined. Rubik's Cube is a large state space (approximately 10{sup:19} states). It does not allow a detailed analysis, but gives confirmation that our general rules continue to make correct predictions at that scale. Our experiments with Rubik's Cube are the first experiments using general-purpose search methods to solve problems with solutions at the greatest depth possible ({a mathematical formula}C⁎=20).</list-item><list-item label="7.">We show that Bi-HS is fundamentally different than Uni-HS. With a consistent{sup:4} non-zero heuristic ({a mathematical formula}h(n)≠0 for every non-goal node) Uni-HS cannot possibly expand more nodes than Uni-BS (Result 6, p. 81 [49]). The corresponding statement does not hold for bidirectional search. We present an example in which MM, or any Bi-HS algorithm guided by {a mathematical formula}f(n), expands more nodes than MM0, and witness this occurring in our experiments.</list-item></list><paragraph> Although we introduce a new algorithm (MM), we do not claim that MM0 or MM are the best bidirectional search algorithms in terms of minimizing run time or the number of nodes expanded under all circumstances. MM's significance is that it is the only Bi-HS algorithm that is guaranteed to meet in the middle. This has numerous benefits (see Section 3.3), one of which is that our analysis, and Barker and Korf's, applies to MM. These theories give strong justification for bidirectional search algorithms that meet in the middle. As the first of its breed, MM represents a new direction for developing highly competitive bidirectional heuristic search algorithms.</paragraph><paragraph>A preliminary version of this paper appeared in AAAI-2016 [29]. The MMe variant of MM was presented at SoCS-2016 [59] and the PEMM variant was presented at IJCAI-2016 [61]. In addition to drawing together the material in those papers, the current paper extends them in the following ways. First, it provides a deeper study of the algorithm and a deeper analysis of the new framework. Second, it includes all the theorems about MM with full proofs (Appendix A). Third, it contains a comprehensive discussion of previous work on bidirectional search. Finally, a more thorough experimental section (including more domains) is provided that supports the theoretical claims.</paragraph></section><section label="2"><section-title>Terminology and previous work</section-title><paragraph>A problem instance is a pair {a mathematical formula}(start,goal) of states. If x and y are states, with y a successor of x, then {a mathematical formula}cost(x,y) is the cost of the edge from x to y. We assume all edge costs are non-negative (a cost of 0 is permitted). The aim of search is to find a least-cost path from {a mathematical formula}start to {a mathematical formula}goal. For any two states, u and v, {a mathematical formula}d(u,v) is the distance (cost of a least-cost path) from u to v. {a mathematical formula}C⁎=d(start,goal) is the cost of an optimal solution. Unless otherwise stated, we assume the heuristics used are admissible but not necessarily consistent.</paragraph><paragraph>We use the usual notation—{a mathematical formula}f,g,Open, etc.—and use {a mathematical formula}gmin and {a mathematical formula}fmin for the minimum g- and f-value on {a mathematical formula}Open. For bidirectional search algorithms, we have separate copies of these variables for each search direction, with a subscript (F or B) indicating the direction{sup:5}:</paragraph><list><list-item label="•">Forward search:{a mathematical formula}fF, {a mathematical formula}gF, {a mathematical formula}hF, {a mathematical formula}OpenF, {a mathematical formula}ClosedF, {a mathematical formula}gminF, etc.</list-item><list-item label="•">Backward search:{a mathematical formula}fB, {a mathematical formula}gB, {a mathematical formula}hB, {a mathematical formula}OpenB, {a mathematical formula}ClosedB, {a mathematical formula}gminB, etc.</list-item></list><paragraph> There are three main algorithmic design decisions on which bidirectional search algorithms differ:</paragraph><list><list-item label="•">stopping condition</list-item><list-item label="•">selecting which node to expand</list-item><list-item label="•">the nature of the heuristic used (for Bi-HS)</list-item></list><paragraph> The alternatives used for these design decisions in existing bidirectional search systems are discussed individually in the following subsections.</paragraph><section label="2.1"><section-title>Stopping condition</section-title><paragraph>For Bi-BS there are two different stopping conditions that guarantee optimal solutions are returned, Nicholson's [47] and Dreyfus's [16].{sup:6} In our notation, Nicholson's condition terminates the search when there is a node n closed in both directions such that {a mathematical formula}gF(n)+gB(n)≤gminF+gminB. In Section 4.2 we will give two improvements to this stopping condition. We use the name “{a mathematical formula}gmin stopping condition” to refer to any stopping condition of the form {a mathematical formula}U≤gminF+gminB, where U is the cost of some solution (path from {a mathematical formula}start to {a mathematical formula}goal) that the search has so far found. Nicholson's stopping condition is a {a mathematical formula}gmin stopping condition in which U is the minimum cost solution path passing through a node closed in both directions. Dreyfus [16] showed that the solution path created by the first node that becomes closed in both directions is not necessarily optimal.{sup:7} Dreyfus further observed that search can indeed stop as soon as the first node n is closed in both directions, but must be followed by some final processing. Once a node n is closed in both directions the optimal path is either that path through n or a path through some node that is currently closed in one direction and open in the other. Those nodes need to be inspected to see if they form a cheaper path than the one through n. Helgason et al. [26] refer to this final checking for optimal paths after search has stopped as the “mop-up” phase. Variations of Dreyfus's stopping condition have been used by several authors [21], [30], [43].</paragraph><paragraph>For Bi-HS, Pohl (p. 92 [53]) showed that Dreyfus's stopping condition is not correct when generalized to Bi-HS and proposed a stopping condition more like Nicholson's, but based on f not g. The search algorithm keeps track of the cost of the cheapest path from {a mathematical formula}start to {a mathematical formula}goal it has found so far—we use the variable U for this value—and stops when {a mathematical formula}U≤max⁡(fminF,fminB). We call this the {a mathematical formula}fmin stopping condition. Most Bi-HS algorithms to date have used Pohl's {a mathematical formula}fmin stopping condition, the only difference being that some update U only when a node becomes closed in both directions (e.g. [3], [8], [46], [53]) while others update U when a node becomes open in both directions (e.g. [22], [38], [40], [52], [64]). Barker and Korf [3] were the first to recognize that Bi-HS could use a {a mathematical formula}gmin stopping condition in addition to Pohl's {a mathematical formula}fmin stopping condition. An entirely different approach to terminating a Bi-HS is used in “two-phase” systems [33], [55]. These stop their bidirectional search as soon as a node is open in both directions, choose a search direction, and proceed thenceforth with a unidirectional heuristic search until the usual Uni-HS stopping condition is satisfied. These can be regarded as generalizing the “mop-up” phase associated with Dreyfus's Bi-BS stopping condition.</paragraph></section><section label="2.2"><section-title>Selecting the next node to expand</section-title><paragraph>For Bi-BS there are two main methods for deciding which node to expand next. The most common method is to strictly alternate between the search directions: expand a node with a minimum {a mathematical formula}gF-value in the forward direction, then one with a minimum {a mathematical formula}gB-value in the backward direction, etc. [21], [26], [30], [43]. This method results in the number of node expansions in each search direction being within one of each other, but it also means they might expand a node in one search direction even though there is an open node with a strictly smaller g-value in the other direction. By contrast, Nicholson's algorithm [47] selects a node with the smallest g-value in either search direction and has no explicit policy (such as strictly alternating search direction) for changing search direction from time to time. Indeed, if all the edges leading to {a mathematical formula}goal from its predecessors cost more than {a mathematical formula}12C⁎, Nicholson's selection policy will result in {a mathematical formula}goal being the only node expanded in the backward direction, all the rest of the search will done in the forward direction. Nicholson's algorithm also differs from all other Bi-BS algorithms in that it expands all nodes with the minimum g-value in the chosen direction at once; other methods expand one node at a time.</paragraph><paragraph>For Bi-HS, Pohl (p. 96 [53]) proposed that the next node to be expanded be chosen in two steps—(1) choose a search direction, and then (2) expand a node with the minimum f-value in the chosen direction—and proved that a Bi-HS algorithm using his stopping condition would return optimal solutions no matter how the search direction is selected in step (1). He then defined the “cardinality criterion” for choosing a search direction: choose the search direction whose Open list is smaller. Pohl gave extensive arguments in favor of this criterion and almost all Bi-HS algorithms have used it. Auer and Kaindl's BiMax-BS{a mathematical formula}F⁎[2] applies the cardinality criterion to choose a direction, but then expands all nodes in that direction with the minimum f-value, even newly generated ones. Kowalski (p. 186 [38]) proposed a variation: choose the search direction having the smaller number of nodes in its open list with the minimum f-value. Similarly, Barker and Korf [3] expand an entire f-level in one direction and then choose the direction, for the next round of expansions, that did the fewest node expansions when it was last used.</paragraph><paragraph>Other selection policies are oblivious to the size and contents of the open lists and have simple switching policies such as strict alternation between the two directions [1], [51], [58] or switching to maintain a fixed ratio between the number of nodes expanded in the two directions [64]. An extreme policy of this form is perimeter search [13], [32], [41], [44], which begins by doing a fixed amount of search in the backward direction and then does all the remaining search in the forward direction.</paragraph><paragraph>The Bi-HS analog of Nicholson's selection policy is to select a node with the smallest f-value in either search direction. Despite its simplicity, it has been used in only one Bi-HS algorithm [52].</paragraph></section><section label="2.3"><section-title>Heuristics for bidirectional heuristic search</section-title><paragraph>The heuristics ({a mathematical formula}hF and {a mathematical formula}hB) used in Bi-HS can either be static or dynamic. A static heuristic (also called a front-to-end heuristic [32]) is the kind of heuristic used by Uni-HS: it directly estimates the distance from node n to the target of the search ({a mathematical formula}goal is the target of the forward search, {a mathematical formula}start is the target of the backward search). Most Bi-HS systems use static heuristics.</paragraph><paragraph>A dynamic heuristic also estimates the distance from a node n to the search target, but it takes into account information generated by search in the opposite direction, so its value for a state may change as search proceeds. The first dynamic heuristics were the “front-to-front” heuristics introduced by de Champeaux and Sint [10], [11] and later used by other “front-to-front” Bi-HS systems [1], [8], [9], [18], [54] and some perimeter search algorithms [13], [44]. These estimate the distance from n to the search target indirectly, using a function {a mathematical formula}h(n,m) that estimates the distance between any two nodes. Given {a mathematical formula}h(n,m), the front-to-front heuristic for forward search is {a mathematical formula}hF(n)=minm∈OpenB⁡{h(n,m)+gB(m)} ({a mathematical formula}hB is defined analogously).</paragraph><paragraph>Front-to-front heuristics are computationally expensive to use, leading Kaindl and Kainz [32] to develop two inexpensive alternatives, which they called Add and Max. These were developed in the context of perimeter search, where they were static adjustments only applicable to {a mathematical formula}hF. Later work showed that they could be applied in both search directions and no matter how the search direction is selected [2], [31], [51], [64].</paragraph><paragraph>Single-frontier Bidirectional Search (SFBDS) [19], [42], called BDS2 by Eckerle and Ottmann [18], also uses a heuristic {a mathematical formula}h(n,m) that estimates the distance between any two nodes, but in a different way than front-to-front search. In SFBDS each node in the search tree is a pair of states {a mathematical formula}(n,m) whose successors are either of the form {a mathematical formula}(n′,m) or of the form of {a mathematical formula}(n,m′), where {a mathematical formula}n′ is a successor of n in the forward direction and {a mathematical formula}m′ is a successor of m in the backward direction. The root node of SFBDS's search tree is {a mathematical formula}(start,goal) and any node of the form {a mathematical formula}(x,x) is a goal node. The g-value of node {a mathematical formula}(n,m) is {a mathematical formula}gF(n)+gB(m) and its h-value is {a mathematical formula}h(n,m). Bidirectional search in the original state space is simulated by applying any Uni-HS system to this expanded state space. The core idea underlying SFBDS was first described in 1966 in an unpublished report [14]. Complete pseudocode for a SFBDS-like system, BHFFA2, was published in 1975 [10] (with an incorrect stopping condition) but never implemented.</paragraph></section></section><section label="3"><section-title>Meeting in the middle</section-title><paragraph>Recall the definition of “meet in the middle” that we use throughout this paper.</paragraph><paragraph label="Definition 1">Meeting in the middleA bidirectional search algorithm meets in the middle if its forward search never expands a node n with {a mathematical formula}gF(n)&gt;{a mathematical formula}12C⁎ and its backward search never expands a node n with {a mathematical formula}gB(n)&gt;{a mathematical formula}12C⁎.</paragraph><section label="3.1"><section-title>Previous Bi-HS algorithms fail to meet in the middle</section-title><paragraph>We now show that previous Bi-HS algorithms fail to meet in the middle on the graph in Fig. 3. All edges cost 1 and the optimal path is {a mathematical formula}start,A,B,C,D,goal{a mathematical formula}(C⁎=5). Inside each node are its {a mathematical formula}hF-value (with a right-pointing arrow overhead) and its {a mathematical formula}hB-value (with a left-pointing arrow overhead). The heuristic values in each direction are consistent.</paragraph><paragraph>Any search algorithm that meets in the middle—Nicholson's algorithm [47], for example, or the MM algorithm we will define in Section 4—will expand {a mathematical formula}start, A, B and perhaps some (or all) of the {a mathematical formula}Xi in the forward direction and {a mathematical formula}C,D, and {a mathematical formula}goal in the backward direction. In particular, B will certainly not be expanded in the backward direction. We will now show that existing Bi-HS systems will expand B in the backward direction, and therefore do not meet in the middle according to our definition.</paragraph><section label="3.1.1"><section-title>Cardinality criterion variants</section-title><paragraph>Systems that use the cardinality criterion will only expand {a mathematical formula}start in the forward direction. After that {a mathematical formula}OpenF is always strictly larger than {a mathematical formula}OpenB so all further search will be in the backward direction. Kowalski's variation on the cardinality criterion does not change this behavior since all the nodes in {a mathematical formula}OpenF have the same {a mathematical formula}fF-value. With Barker and Korf's variation of the cardinality criterion {a mathematical formula}start will be expanded in the forward direction, then {a mathematical formula}goal,D,C, and B will be expanded in the backward direction because they all have the same {a mathematical formula}fB-value ({a mathematical formula}fB=3).</paragraph></section><section label="3.1.2">Alternation or smallest f<paragraph>Systems that strictly alternate search direction will expand B in their backward search because their forward search will be pre-occupied expanding the {a mathematical formula}Xi nodes. Systems that expand a node with the smallest f-value will expand B in the backward direction before expanding A or any of the {a mathematical formula}Xi in the forward direction because {a mathematical formula}fB(B)=3&lt;fF(A)=fF(Xi)=5.</paragraph></section><section label="3.1.3"><section-title>Front-to-front systems</section-title><paragraph>Front-to-front heuristics were introduced specifically to remedy the problem that Bi-HS systems using static heuristics were not meeting in the middle [10], [11].{sup:8} Some papers with front-to-front heuristics claim their searches meet in the middle [8], [10], [11] but none has a theorem to this effect. The example in Fig. 3 can be adapted to show that Bi-HS with a front-to-front heuristic will expand B in the backward direction if its policy for selecting nodes to expand is to alternate search directions and then to select a node with the minimum f-value in the chosen direction. This is the exact policy used by Arefin and Saha [1] and is a permissible policy for other front-to-front Bi-HS systems [8], [9], [10], [11], [18] since they are indifferent to how the search direction is chosen. The front-to-front heuristic for this example has {a mathematical formula}h(Xi,C)=h(Xi,D)=1 and {a mathematical formula}h(A,C)=h(A,D)=2 (the other values do not matter, they can be set in several ways to make h admissible and bi-monotone [17]). With this heuristic {a mathematical formula}fF(A) is strictly larger than {a mathematical formula}fF(Xi) when {a mathematical formula}OpenB={D} and when {a mathematical formula}OpenB={C}, so the search will be proceed exactly as described above for alternating search and B will expanded in the backward direction.</paragraph></section></section><section label="3.2"><section-title>Transforming heuristic search into brute-force search</section-title><paragraph>The only previously existing bidirectional search algorithm guaranteed to meet in the middle is therefore Nicholson's [47]. It is not a heuristic search algorithm, but, when {a mathematical formula}hF and {a mathematical formula}hB are consistent, it can be made to exactly simulate heuristic search by transforming the edge costs in the state space to take into account the heuristic values, as follows [30]. If u and v are nodes, with v a successor of u (in the forward direction), redefine the edge cost {a mathematical formula}cost(u,v) to be:{sup:9}{a mathematical formula} When Nicholson's algorithm is run using the transformed edge costs, it is a Bi-HS algorithm that is guaranteed to meet in the middle. Unfortunately, the “middle” at which it meets is with respect to the transformed edge costs not the original ones. For example, if this transformation is applied to Fig. 3, it leaves all the edges emanating from {a mathematical formula}start with a cost of 1 but changes the costs of edges {a mathematical formula}(B,C), {a mathematical formula}(C,D), and {a mathematical formula}(D,goal) to zero. When Nicholson's algorithm is run on this transformed graph it will expand D, C, and B in the backward direction before expanding A or any {a mathematical formula}Xi in the forward direction because the latter have {a mathematical formula}gF=1 while the former have {a mathematical formula}gB=0. As it must be, this is exactly the same as the behavior on Fig. 3, described above, of a Bi-HS system that selects nodes for expansion based on the minimum f-value.</paragraph></section><section label="3.3"><section-title>Why meet in the middle?</section-title><paragraph>Why is it important for a bidirectional search algorithm to “meet in middle” as we have defined it? There are several reasons:</paragraph><list><list-item label="1.">The original motivation for Bi-HS was to be able solve a problem whose optimal solution cost was {a mathematical formula}C⁎ doing only twice the work it would take {a mathematical formula}A⁎ to solve a problem whose solution cost was {a mathematical formula}12C⁎ (p. 108 [53]). Meeting in the middle is directly aimed at achieving this goal.</list-item><list-item label="2.">Meeting in the middle provides an upper bound on how many nodes a Bi-HS system will expand in the worst case and an upper bound on how much memory it will need. In state spaces where the number of states at distance d from {a mathematical formula}start or {a mathematical formula}goal grows exponentially with d, a system that ventures beyond d={a mathematical formula}12C⁎, whether it be bidirectional or unidirectional, is at risk of expanding exponentially more nodes than a system that meets in the middle.</list-item><list-item label="3.">Meeting in the middle provides a characterization of nodes that are guaranteed not to be expanded, analogous to the fact that A* is guaranteed not to expand nodes with {a mathematical formula}f(n)&gt;C⁎ (Lemma 4 [24]). This characterization enables us to partition the state space into disjoint regions and then provide an analytical comparison of bidirectional and unidirectional searches on a region-by-region basis (see Section 6). It also enables other analyses to be applied. For example, Barker and Korf's analysis [4] only applies to systems that meet in the middle.</list-item><list-item label="4.">Meeting in the middle guarantees that a state expanded in one direction will not be expanded in the other direction unless it is exactly distance {a mathematical formula}12C⁎ from both {a mathematical formula}start and {a mathematical formula}goal. In state spaces where no such states exist (e.g. unit-cost state spaces when {a mathematical formula}C⁎ is odd) a system that meets in the middle does not need consistent heuristics or special mechanisms, such as Kwa's “nipping” and “pruning” [40], to prevent states from being expanded in both directions.</list-item></list></section></section><section label="4">MM: a novel Bi-HS family of algorithms<paragraph>In this section we describe MM, our new Bi-HS algorithm. We first describe the original version [29], which we refer to as “basic MM”, and then describe an improved version, called MMe[59].</paragraph><section label="4.1">Basic MM<paragraph>Basic MM{sup:10} runs an A*-like search in both directions, except that MM orders nodes on the Open list in a novel way. The priority of node n on {a mathematical formula}OpenF, {a mathematical formula}prF(n), is defined to be:{a mathematical formula}{a mathematical formula}prB(n) is defined analogously. We use {a mathematical formula}prminF and {a mathematical formula}prminB for the minimum priority on {a mathematical formula}OpenF and {a mathematical formula}OpenB, respectively, and {a mathematical formula}C=min(prminF,prminB). On each iteration MM expands a node with priority C.</paragraph><paragraph>When a state s is generated in one direction MM checks whether s is in the Open list of the opposite direction. If it is, a solution (path from {a mathematical formula}start to {a mathematical formula}goal) has been found. MM maintains the cost of the cheapest solution found so far in the variable U. U is initially infinite and is updated whenever a better solution is found. MM stops when{a mathematical formula} where ϵ is the cost of the cheapest edge in the state space.</paragraph><paragraph>Each of the terms inside the max is a lower bound on the cost of any solution that might be found by continuing to search. Therefore, if U is smaller than or equal to any of them, its optimality is guaranteed and MM can safely stop.</paragraph><section label="4.1.1">Pseudocode for MM<paragraph>Algorithm 1 gives the pseudocode for MM. When {a mathematical formula}prminF=prminB any rule could be used to break the tie (e.g. Pohl's cardinality criterion [53]), it is not necessary to break such ties in favor of the forward direction as is done in line 9. In addition, tie breaking within a given direction should be performed in Line 11. Lines 5–23 are the usual best-first search expansion cycle. Duplicate detection is done in line 14. U is updated in line 21 and checked in line 7. Note that to determine if a better solution path has been found, MM only checks (line 20) if a newly generated node is in the Open list of the opposite search direction. That is all that is required by our proofs; it is not necessary to also check if a newly generated node is in the Closed list of the opposite search direction. We introduce the term solution detection for this check.</paragraph><paragraph>As presented, only the cost of the optimal path is returned (line 8). It is straightforward to add code to return the solution path.</paragraph></section><section label="4.1.2">Properties of MM<paragraph>When MM's heuristics are admissible, it has the following properties:</paragraph><list><list-item>MM's forward and backward searches meet in the middle, i.e. neither search expands a node whose distance from the search's origin ({a mathematical formula}gF(n) for forward search, {a mathematical formula}gB(n) for backward search) is larger than {a mathematical formula}12C⁎.</list-item><list-item>MM never expands a node whose f-value exceeds {a mathematical formula}C⁎.</list-item><list-item>MM returns {a mathematical formula}C⁎.</list-item></list><paragraph> These are formally stated and proven in Appendix A. Here we provide sketches of their proofs based on the following lemmas (L1–L3). For simplicity, in these sketches we assume MM stops if and only if {a mathematical formula}U≤C.{sup:11}</paragraph><paragraph>L1: If {a mathematical formula}d(start,s)&gt;{a mathematical formula}12C⁎, then {a mathematical formula}prF(s)&gt;C⁎ and if {a mathematical formula}d(s,goal)&gt;{a mathematical formula}12C⁎, then {a mathematical formula}prB(s)&gt;C⁎.</paragraph><paragraph label="Proof for the forward direction">{a mathematical formula}prF(s)≥2gF(s)≥2d(start,s). If {a mathematical formula}d(start,s)&gt;{a mathematical formula}12C⁎ then {a mathematical formula}prF(s)&gt;C⁎. □</paragraph><paragraph>For the next lemma we need the following definition.</paragraph><paragraph label="Definition 2">For any optimal path {a mathematical formula}P=s0,s1,…sn from {a mathematical formula}start(s0) to {a mathematical formula}goal(sn), let i be the largest index such that {a mathematical formula}sk∈ClosedF∀k∈[0,i−1], and let j be the smallest index such that {a mathematical formula}sk∈ClosedB∀k∈[j+1,n]. We say that P “has not been found” if {a mathematical formula}i&lt;j and that P “has been found” otherwise ({a mathematical formula}i≥j).</paragraph><paragraph>For example, if i and j, as defined in Definition 2, are as shown in Fig. 4 then this path has not been found because {a mathematical formula}i&lt;j ({a mathematical formula}si is to the left of {a mathematical formula}sj).</paragraph><paragraph>In the proof of the following lemma, i and j are as defined in Definition 2.</paragraph><paragraph>L2: If P is an optimal path from {a mathematical formula}start to {a mathematical formula}goal that has not been found, there will exist a node {a mathematical formula}n∈P such that either {a mathematical formula}n∈OpenF with {a mathematical formula}prF(n)≤C⁎ or {a mathematical formula}n∈OpenB with {a mathematical formula}prB(n)≤C⁎.</paragraph><paragraph label="Proof Sketch">Throughout MM's execution there will be a node in P, {a mathematical formula}si,{sup:12} in {a mathematical formula}OpenF with {a mathematical formula}gF(si)=d(start,si) and a node in P, {a mathematical formula}sj, in {a mathematical formula}OpenB with {a mathematical formula}gB(sj)=d(sj,goal). Since P has not yet been found there must exist a gap between {a mathematical formula}si and {a mathematical formula}sj, i.e. one or more edges from P that connect {a mathematical formula}si to {a mathematical formula}sj that MM has not traversed. This situation is depicted in Fig. 4, where the dashed line between {a mathematical formula}si and {a mathematical formula}sj is the gap consisting of one or more edges. In this situation, either {a mathematical formula}gF(si)≤{a mathematical formula}12{a mathematical formula}cost(P) or {a mathematical formula}gB(sj)≤{a mathematical formula}12{a mathematical formula}cost(P) (or both), where {a mathematical formula}cost(P) is the sum of the costs of P's edges. Therefore, {a mathematical formula}prF(si)≤cost(P)=C⁎ or {a mathematical formula}prB(sj)≤cost(P)=C⁎ (or both). □</paragraph><paragraph>L3:{a mathematical formula}U&gt;C⁎ until the first optimal path from {a mathematical formula}start to {a mathematical formula}goal is found, at which point {a mathematical formula}U=C⁎. This is a direct consequence of the process by which U is updated.</paragraph><paragraph>We now sketch the proofs that MM has properties P1–P3. L2 and L3 together ensure that MM will not terminate before an optimal path has been found (L2 implies that {a mathematical formula}C≤C⁎ until all optimal paths have been found and L3 says {a mathematical formula}U&gt;C⁎ until the first optimal path is found). P3 follows because {a mathematical formula}U=C⁎ once an optimal path has been found (L3).</paragraph><paragraph>L2 and L1 together ensure that MM will find an optimal path before MM expands any node in the forward direction with {a mathematical formula}prF(n)&gt;C⁎ or any node in the backward direction with {a mathematical formula}prB(n)&gt;C⁎. Together with L3 this implies that MM will terminate before it expands any node in the forward direction with {a mathematical formula}fF(n)&gt;C⁎ or {a mathematical formula}d(start,s)&gt;C⁎/2, or any node in backward direction with {a mathematical formula}fB(n)&gt;C⁎ or {a mathematical formula}d(s,goal)&gt;C⁎/2, thus proving P1 and P2.</paragraph></section></section><section label="4.2">MM0<paragraph>MM0 is the brute-force version of MM, i.e. MM when {a mathematical formula}h(n)=0∀n. Thus for MM0: {a mathematical formula}prF(n)=2gF(n) and {a mathematical formula}prB(n)=2gB(n). MM0 therefore selects for expansion a node on either open list with the smallest g-value, just as Nicholson's algorithm [47] does, and stops when {a mathematical formula}U≤gminF+gminB+ϵ. MM0's stopping condition is superior to Nicholson's in two ways: (1) Nicholson's does not have the +ϵ term, and (2) Nicholson's algorithm only updates U when a node is closed in both directions, MM0 updates U when a node becomes open in both directions. A third difference is that Nicholson's algorithm only checks the stopping condition after expanding all the nodes in the chosen search direction with the minimum g-value whereas MM0 checks the stopping condition after every node expansion. We return to this issue in Section 5, where we discuss immediate and delayed solution detection.</paragraph></section><section label="4.3">MMe<paragraph>We now present an enhanced version of MM, MMe[59], which is identical to basic MM except for a small change in how an open node's priority is defined. In all subsequent sections of the paper we will use the {a mathematical formula}prF and {a mathematical formula}prB to refer to MMe's definition of priority, but in this section, to clearly distinguish between basic MM's priority function and MMe's, we will use {a mathematical formula}prF and {a mathematical formula}prB for basic MM's priority function and use the special notation {a mathematical formula}prFϵ and {a mathematical formula}prBϵ for MMe's priority function.</paragraph><paragraph>In MMe the priority of {a mathematical formula}n∈OpenF is{a mathematical formula} where ϵ is the cost of the cheapest edge in the state space. {a mathematical formula}prBϵ(n) is defined analogously.</paragraph><paragraph>We now prove that MMe has properties P1–P3 by showing that lemmas L1–L3 hold for MMe. L1 is still true since {a mathematical formula}prFϵ(n)≥prF(n) and {a mathematical formula}prBϵ(n)≥prB(n). L3 is still true because it is not affected by the definition of a node's priority. To see that L2 is still true, note that {a mathematical formula}d(si,sj) is the cost of the gap illustrated in Fig. 4, i.e. {a mathematical formula}C⁎=gF(si)+d(si,sj)+gB(sj). Hence, at least one of {a mathematical formula}gF(si) and {a mathematical formula}gB(sj) must be less than or equal to {a mathematical formula}12{a mathematical formula}(C⁎−d(si,sj)).</paragraph><paragraph>The exact value of {a mathematical formula}d(si,sj) is not known, but it always holds that {a mathematical formula}ϵ≤d(si,sj).{sup:13} Therefore, either {a mathematical formula}gF(si)≤12(C⁎−ϵ) or {a mathematical formula}gB(sj)≤12(C⁎−ϵ). If {a mathematical formula}gF(si)≤12(C⁎−ϵ) then {a mathematical formula}2gF(si)+ϵ≤C⁎. Similar reasoning applies for {a mathematical formula}gB(sj). L2 follows because at least one of these must hold.</paragraph><section label="4.3.1">MM vs. MMe<paragraph>In this section we highlight the differences in behavior of basic MM and MMe with two examples. The first (Fig. 5) represents situations in which basic MM expands nodes that MMe does not expand. This is what is expected, given that {a mathematical formula}prF(n)≤prFϵ(n). The second example (Fig. 6) shows that the opposite behavior is also possible, there can be nodes that are expanded by MMe but not by basic MM.</paragraph><list><list-item label="•">Basic MM expands nodes that MMe does not expand.Consider the graph depicted in Fig. 5 with a specific focus on node X. Basic MM proceeds as follows. After {a mathematical formula}start is expanded {a mathematical formula}OpenF includes three nodes with the following priorities: {a mathematical formula}A(7), {a mathematical formula}X(4) and {a mathematical formula}goal(10). At this point U is set to 5. Now {a mathematical formula}goal{a mathematical formula}(prB=3) is expanded in the backward direction. Finally, X is the only node with {a mathematical formula}prF(X)=4≤U=5 so it is expanded. By contrast, for MMe, {a mathematical formula}prF(X)=6&gt;U=5 and it will not be expanded.</list-item><list-item label="•">MMe expands nodes that Basic MM does not expand.It is possible for MMe to expand more nodes than basic MM. Fig. 6 gives an example when both algorithms use all of MM's stopping conditions (line 7 in Algorithm 1). Both algorithms begin by expanding {a mathematical formula}start (forward) and {a mathematical formula}goal (backward). Node A will not be expanded in the forward direction by either algorithm because {a mathematical formula}gF(A)=8&gt;6={a mathematical formula}12C⁎ and both algorithms will halt as soon as A is expanded in the backward direction (when that happens {a mathematical formula}U=12≤gminF+gminB+ϵ=18. For both algorithms {a mathematical formula}prF(X)=fF(X)=9. For MM, {a mathematical formula}prB(A)=8 so MM will expand A before X and then halt without expanding X. For MMe, {a mathematical formula}prB(A)=12 so MM will expand X before A.</list-item></list><paragraph>Which of these two situations will occur more commonly? We expect the first situation (Fig. 5) will occur much more often than the second (Fig. 6), and therefore expect that MMe will usually expand fewer nodes than basic MM. Our reason is that the second situation requires the priority of at least one node on every optimal path to have its priority increased to be greater than the priority of nodes like X, a kind of collective conspiracy on the part of the optimal paths. By contrast, the first situation occurs when an individual node's priority increases beyond {a mathematical formula}C⁎.</paragraph></section></section></section><section label="5">Parallel external-memory MM<paragraph>In order to scale the size of the problems solvable by MM, we must either purchase the largest possible machine for solving problems or make better use of the resources on available hardware. The most common approach is to use external memory (disk) as storage [6], [36], [48], [39], [63] in place of RAM. While disks are much larger than RAM, the cost of random access to disk is high (high latency). The throughput of disk, however, is also high, so once the latency is overcome, disks have relatively high throughput. Thus, if we wish to modify MM, or any other bidirectional search algorithm, to use external memory, we must modify the algorithm to amortize high latency operations by grouping operations that access the same data on disk.</paragraph><paragraph>Looking at Algorithm 1, there are three places where MM performs random access to the {a mathematical formula}Open and {a mathematical formula}Closed lists. In lines 14 and 16 MM performs duplicate detection, checking if a newly generated state has already been generated or expanded. In line 20 MM performs solution detection, checking if a solution has been found.</paragraph><paragraph>The standard approach to avoiding the latency associated with duplicate detection in external memory is delayed duplicate detection (DDD) [35]. DDD avoids duplicate detection on individual states, instead performing it on many states at a time later in the search. In DDD successors are written to an open list on disk without duplicate detection. At a later time, such as when all successors have been written to a file, duplicate detection can be performed in batches, amortizing the disk latency.</paragraph><paragraph>Approaches for avoiding random access to disk for solution detection have not been previously been studied in external memory search beyond our own work on MM[61], which is expanded on slightly in this presentation. MM by default uses immediate solution detection (ISD), checking for solutions as soon as a state is generated. Delayed solution detection (DSD) refers to any approach that does not perform solution check immediately, but does perform the check before any state with larger f-cost is expanded.</paragraph><paragraph>We describe here DSD and our other modifications to MM to create PEMM, the parallel, external-memory version of MM.</paragraph><section label="5.1">Algorithmic changes to MM<paragraph>PEMM makes the following changes to MM to ensure the efficiency of the search. While these are all part of our implementation, the core change required for PEMM is the use of DDD, DSD, and the change of termination conditions. It is likely that other changes could be made to further increase the search efficiency without fundamentally changing the nature of PEMM:</paragraph><list><list-item label="1.">PEMM separates the states in {a mathematical formula}Open and {a mathematical formula}Closed, which are stored on disk, from the information about those states, which can be summarized in smaller {a mathematical formula}Open and {a mathematical formula}Closed data structures in RAM.</list-item><list-item label="2.">PEMM partitions {a mathematical formula}Open and {a mathematical formula}Closed on disk into buckets of states with similar properties for the efficiency of duplicate and solution detection.</list-item><list-item label="3.">PEMM expands states with the same priority from low to high g-cost.</list-item><list-item label="4.">PEMM uses delayed duplicate detection for finding duplicates.</list-item><list-item label="5.">PEMM uses delayed solution detection for finding solutions.</list-item><list-item label="6.">PEMM removes ϵ from the {a mathematical formula}gmin stopping condition and uses Basic MM's definition of {a mathematical formula}pr(n), because the ϵ-based variants do not work with delayed solution detection.</list-item><list-item label="7.">PEMM performs many operations in parallel.</list-item><list-item label="8.">PEMM assumes a consistent heuristic, an undirected search space, and unit-cost edges.</list-item></list><paragraph> These changes are now described in detail. One change not discussed or studied here is the performance of PEMM with inconsistent heuristics. The re-opening of nodes may or may not cause problems with the efficiency of external-memory search; we do not study this issue here, but leave it as a point for future research.</paragraph><section label="5.1.1">States are not stored in {a mathematical formula}Open and {a mathematical formula}Closed in RAM<paragraph>When using external memory, the assumption is that all states in the search will not fit in RAM at once. Thus, we maintain separate {a mathematical formula}Open and {a mathematical formula}Closed lists in memory and on disk. The lists in memory only contain the properties of the states; the actual states are kept on disk. While efficient data structures can be used to access and query this information in RAM, in practice a simple (resizable) array is sufficient, since the cost of iterating through {a mathematical formula}Open in RAM is dwarfed by the cost of loading and processing states on {a mathematical formula}Open from disk.</paragraph><paragraph>Individual states are stored unsorted on {a mathematical formula}Open and {a mathematical formula}Closed on disk, so writes can be efficiently performed at any time by appending to a file on disk. Writes are buffered both in memory and by the filesystem, which improves efficiency and eliminates latency concerns when writing.</paragraph></section><section label="5.1.2">States in {a mathematical formula}Open and {a mathematical formula}Closed are broken into buckets on disk<paragraph>Since we cannot load all of {a mathematical formula}Open into RAM at once, it must be subdivided into smaller buckets that will fit into RAM so that we can load these files when doing duplicate detection.</paragraph><paragraph>States are divided into buckets by (1) the priority of a state, (2) the g-cost of the state, (3) the search direction, and (4) the lower i bits of the state hash function. Every state in a given bucket will have the same values for each of these attributes. This means that all duplicates in the same direction will fall in the same bucket, and nodes found on opposite frontiers of the search will be found in similar buckets.</paragraph><paragraph>Our previous implementations also divided states into buckets by h-cost. We have removed this division because we sometimes found buckets with high g-cost (and thus high priority), but low h-cost. These buckets might only have a few dozen states, but required DSD to be performed against the opposite frontier, which might contain billions of states. Removing the h-cost division puts these states into larger buckets to make DSD more efficient, although we sometimes have to expand more nodes to find the solution as a result.</paragraph></section><section label="5.1.3">Tie-breaking from low to high g-cost<paragraph>As in MM, expansions are ordered by priority (low to high). But, in PEMM they are further ordered by search direction (forward then backward), g-cost (low to high), and then by the h-cost and hash function (low to high). The ordering by search direction, hash, and heuristic values does not influence the correctness or efficiency of search, but guarantees consistency between runs and problem instances. Ordering buckets by low to high g-cost may seem counter-intuitive, since it is the opposite of a typical A* ordering. This is important for PEMM, however, as it ensures that once we expand a bucket we will not generate any new states back into that bucket. Without this ordering we would be forced to process buckets multiple times as new states were re-added to the bucket.</paragraph><paragraph>For example, assume we process the bucket with g-cost 4 first, followed by the bucket with g-cost 3. Some of the successors of the g-cost 3 bucket will possibly have the same priority as their parents, and thus will get written back into the g-cost 4 bucket which has already been handled. Thus, the g-cost 4 bucket will then have to be processed a second time. This isn't a problem from the perspective of node expansions, but it can be quite inefficient for duplicate and solution detection where we want to limit the number of times we must access disk for these operations.</paragraph><paragraph>The consequence of this tie-breaking is that we may have the solution on the open list (with high g-cost) for a long time before it is found. In future work we will consider ways to improve DSD to find solutions earlier.</paragraph></section><section label="5.1.4"><section-title>Delayed duplicate detection (DDD)</section-title><paragraph>Hash-based delayed duplicate detection [35] is used to detect and remove duplicates when a bucket is loaded into RAM. Hash-based DDD works by loading all states into a hash table. Since duplicates will be mapped to the same entries, they are effectively removed in this process.</paragraph><paragraph>This is sufficient for removing duplicates within a bucket, but it will not remove duplicates between buckets, such as when a state is re-generated with larger g-cost. This happens, for example, when a state re-generates its parent or one of its siblings. In the general case we must look for duplicates in buckets on {a mathematical formula}Closed that could possibly contain a given state with lower g-cost. With unit edge costs this is more efficient, since a state from {a mathematical formula}Open with g-cost {a mathematical formula}gs can only be found in buckets with g-cost {a mathematical formula}gs−1 or {a mathematical formula}gs−2. When looking for duplicates on {a mathematical formula}Closed, we read the relevant {a mathematical formula}Closed files incrementally and check to see if any of their states are found in the hash table (bucket) in RAM. All such states are removed before expansions begin.</paragraph><paragraph>Our implementation does not include enhancements for handling arbitrary edge costs, something that is addressed in unidirectional search by PEDAL [25], and does not use frontier search [37], which uses additional domain-specific information to avoid generating duplicates.</paragraph></section><section label="5.1.5"><section-title>Delayed solution detection</section-title><paragraph>The primary idea of delayed solution detection is to check to see if a solution has been found when a state is being expanded as opposed to when the state is generated. We use a hash-based solution detection approach similar to hash-based DDD. When a bucket is loaded into RAM and DDD has been completed, all other buckets in the opposite frontier that could contain the same states as the current bucket are also loaded. These are not stored in RAM; instead we just check whether the states in them are found in the hash table that stores the current bucket. If a duplicate is found, U is updated according to the solution path through that state. For efficiency, only buckets in the opposite frontier that could lead to a better solution than U need to be checked.</paragraph></section><section label="5.1.6">Remove ϵ variants from MM<paragraph>MM uses a termination condition that allows the search to stop early based on the minimum edge cost (ϵ) and minimum g-cost in {a mathematical formula}Open in each direction. Unfortunately, this rule does not work in general with DSD. We illustrate this in Fig. 7, where states a and b are the start and goal. All edges are marked with their costs; no heuristic is used.</paragraph><paragraph>In this example the search begins with a on {a mathematical formula}OpenF and b on {a mathematical formula}OpenB. After each of these states are expanded, c will be on {a mathematical formula}OpenF and {a mathematical formula}OpenB. Although c is on both {a mathematical formula}Open lists, this will not be detected until c is expanded. Before it is expanded the minimum g-cost in the forward direction is 1 (c), and the minimum g-cost in the backward direction is 1 (also c). Using the ϵ termination condition we would conclude that we could terminate with an optimal solution cost 3. Thus, the search would terminate with the solution cost 2.5 between a and b, which is incorrect. The ϵ termination condition is justified with ISD because there are no paths through {a mathematical formula}Open that have not been discovered already. Thus, new paths can only be found by adding new edges to existing paths. With DSD there can be undiscovered solutions on {a mathematical formula}Open with cost {a mathematical formula}gminF+gminB such as through c in this example, hence the ϵ rule cannot be used.</paragraph><paragraph>As a result, the termination condition used for PEMM is:{a mathematical formula}</paragraph><paragraph>MMe introduces a new priority rule {a mathematical formula}prF(n)=max⁡(fF(n),2gF(n)+ϵ). We show here that the additional ϵ term cannot be used with PEMM. Consider again the graph in Fig. 7, where states a and b are the start and goal. All edges are marked with their costs; no heuristic is used.</paragraph><paragraph>In this example the search begins with a and b on {a mathematical formula}Open. After each of these states are expanded, c will be on {a mathematical formula}OpenF and {a mathematical formula}OpenB, but the solution will not yet be detected. The priority of c will be 3 in each direction. However, the search will have also found the path of cost 2.5 between the start and the goal. So, with DSD and the MMe priority rule, PEMM will terminate with the suboptimal solution cost 2.5.</paragraph><paragraph>As a result, the priority of a state in PEMM in the forward direction must be:{a mathematical formula} An analogous rule is used in the backward direction.</paragraph></section><section label="5.1.7"><section-title>Parallel search</section-title><paragraph>Because of the latency of disk, it makes sense to run as many PEMM operations in parallel as possible. PEMM performs three sets of operations in parallel. (1) The expansions for a given bucket can all be performed in parallel. This is where the greatest parallel efficiency is achieved. (2) Solution detection can be performed in parallel to expansion. If solution detection is not complete when expansion is complete, we wait until solution detection completes before moving to the next bucket. (3) The next bucket can be pre-loaded while the current bucket is being expanded.</paragraph></section></section><section label="5.2">Pseudocode for PEMM<paragraph>Algorithm 2 contains the pseudocode for PEMM. We begin by initializing the data structures (lines 1 through 4). The pseudocode refers to {a mathematical formula}Open as the data structure in RAM containing information about each state and the data on disk (line 11) as buckets. Adding a state to {a mathematical formula}Open implicit adds it both to the data structures in memory and disk.</paragraph><paragraph>The important differences between PEMM and MM are that PEMM must explicitly load buckets of states into RAM (line 13). It then performs all duplicate detection (lines 13 and 14) and solution detection (line 16) before expanding the states in a bucket (line 17). Finally, it uses a simpler termination condition (line 8) than MM, omitting the ϵ condition.</paragraph><paragraph>We haven't included the re-opening of states that are found with lower g-costs needed for inconsistent heuristics in the pseudocode because we have not tested these conditions to see if they are efficient in practice.</paragraph></section><section label="5.3"><section-title>Correctness of DSD</section-title><paragraph>We analyze the correctness of DSD here assuming the correctness of MM. While Nicholson [47] uses a variant of DSD, MM has only been proven to be correct with ISD. We show that delaying the solution detection cannot lead to termination with a suboptimal solution.</paragraph><paragraph>It is clear that for each direction of a bidirectional search, a state will pass monotonically through three phases: {a mathematical formula}ungenerated→open→closed. We assume that solution detection will be performed at some point during the {a mathematical formula}open phase or exactly at the point when a state is written to {a mathematical formula}closed, but do not distinguish when. In particular, we just need solution detection to be performed before states with higher f are expanded.</paragraph><paragraph label="Lemma 1">PEMMwith DSD will terminate with an optimal solution.</paragraph><paragraph label="Proof">Consider that MM finds an optimal solution through some state {a mathematical formula}s⁎ when it is generated and solution detection is performed. PEMM will not find the solution through {a mathematical formula}s⁎, but instead will put {a mathematical formula}s⁎ on {a mathematical formula}Open. At this point {a mathematical formula}s⁎ has optimal g-cost in both directions, {a mathematical formula}s⁎ is on an optimal path, and {a mathematical formula}s⁎ is found on {a mathematical formula}Open in both directions. We let {a mathematical formula}s⁎ be any state that has these three properties.We show that until {a mathematical formula}s⁎ is removed from {a mathematical formula}Open and a state with higher priority is expanded, PEMM cannot terminate with a suboptimal solution. Since we perform solution detection on {a mathematical formula}s⁎ before it is placed on {a mathematical formula}Closed, PEMM will terminate with the optimal solution when expanding and performing solution detection {a mathematical formula}s⁎.Recall the PEMM termination conditions:{a mathematical formula}Let {a mathematical formula}C⁎ be the cost of the optimal solution through {a mathematical formula}s⁎. As long as {a mathematical formula}s⁎ is on {a mathematical formula}Open in both directions, the search cannot terminate with a solution cost {a mathematical formula}&gt;C⁎. We examine the termination conditions one at a time and show that they will not be met.Since the heuristic is admissible and {a mathematical formula}s⁎ is on {a mathematical formula}Open, {a mathematical formula}fminF,fminB≤C⁎. This handles the second and third termination conditions. Given that {a mathematical formula}s⁎ has optimal g-cost in each direction, either {a mathematical formula}gF(s⁎)≤{a mathematical formula}12C⁎ or {a mathematical formula}gB(s⁎)≤{a mathematical formula}12C⁎, or both. Without loss of generality, assume {a mathematical formula}gF(s⁎)≤{a mathematical formula}12C⁎. In this case {a mathematical formula}s⁎'s bucket in the forward direction must have {a mathematical formula}prminF≤C⁎. (Because {a mathematical formula}2gF(s⁎)≤C⁎ and {a mathematical formula}fF(s⁎)≤C⁎.) This handles the first termination condition. Finally, since {a mathematical formula}s⁎ is on {a mathematical formula}Open in both directions, {a mathematical formula}gminF+gminB≤C⁎.Thus, until {a mathematical formula}s⁎ is removed from {a mathematical formula}Open, we cannot terminate with a suboptimal solution. Since we will perform solution detection when removing {a mathematical formula}s⁎ from {a mathematical formula}Open, we are guaranteed to find the optimal solution even when performing DSD. □</paragraph></section></section><section label="6"><section-title>Region based analysis</section-title><paragraph>In this section we analytically compare MM, MM0, and A*. “MM” here refers to the family of MM algorithms, including MMe.{sup:14} Whenever we illustrate a point with a specific example, we use MMe as the particular member of the MM family in the example. Even the examples involving MM0 use the MMe version of MM0, in which a node n's priority is {a mathematical formula}2g(n)+ϵ. We begin by providing a disjoint partitioning of the state space into regions.</paragraph><section label="6.1"><section-title>Dividing the state space into disjoint regions</section-title><paragraph>We say state s is “near to {a mathematical formula}start” if {a mathematical formula}d(start,s)≤{a mathematical formula}12C⁎, “far from {a mathematical formula}start” if {a mathematical formula}12C⁎{a mathematical formula}&lt;d(start,s)≤C⁎, and “remote” if {a mathematical formula}d(start,s)&gt;C⁎. “Near”, “far” and “remote” for {a mathematical formula}goal are defined analogously. Taken together, these categories divide the state space into the 9 disjoint regions shown in Fig. 8. We denote these regions by two letter acronyms. The first letter (N=near, F=far, R=remote) indicates the distance from {a mathematical formula}start, the second letter indicates the distance from {a mathematical formula}goal. For example, FN is the set of states that are far from {a mathematical formula}start and near to {a mathematical formula}goal. NN includes only those states at the exact midpoint of optimal solutions.</paragraph><paragraph>For the purpose of this paper we will only be interested in whether a node is near or not near to {a mathematical formula}goal since MM's backward search is guaranteed to expand no nodes that are far or remote from {a mathematical formula}goal. Therefore, we say that a node is “distant” from {a mathematical formula}goal if it is either far or remote from {a mathematical formula}goal. That is {a mathematical formula}D=F∪R. We thus use the following unified regions: {a mathematical formula}RD=RR∪RF, {a mathematical formula}FD=FR∪FF, and {a mathematical formula}ND=NR∪NF. This leaves only 6 regions depicted in Fig. 9. None of the search algorithms in this paper expands a state in RD, so only 5 regions will enter our discussions.</paragraph></section><section label="6.2"><section-title>Preliminaries</section-title><paragraph>In Subsections 6.3 , 6.4 , 6.5 , 6.6  we compare MM0, MM, Uni-BS, and A* based mainly on the nodes they expand in each region. In our analysis a region's name denotes both the set of states and the number of states in the region. We will use the names in equations and inequalities. An inequality involving two algorithms, e.g. A* &lt; MM, indicates that one algorithm (A* in this example) expands fewer nodes than the other. Since the region names denote both the number and set of states we will use one symbol, “+”, to indicate both adding the number of states in two regions and the set of states defined by their union. For example, the expression NN + FN + RN denotes both the union of those three regions and the number of states in their union.</paragraph><paragraph>The novelty of our analysis stems from the fact that we isolate the behavior of the different algorithms to each of the regions and then sum up these behaviors. As we will see, in all cases except Uni-HS vs. Uni-BS no algorithm-type is superior to any other in all regions. We will summarize these analyses with three general rules (GR1, GR2, and GR3). These are general expectations, not iron-clad guarantees. There are many factors in play in a given situation, some favoring one algorithm-type, some favoring another. It is the net sum of these factors that ultimately determines which algorithm-type outperforms another. Our general rules state what we expect will usually be the dominant forces.</paragraph><paragraph>Although MM and A* do not require their heuristics to be consistent, the analysis in this section, with the exception of Section 6.5.1, is directly applicable only when the heuristics are consistent, for two reasons. First, our analysis reasons about the number of distinct nodes that are expanded in each region, it does not take into account the number of times the same node is expanded. With a consistent heuristic, a node will be expanded at most once. With an inconsistent heuristic the same node can be expanded many times [20], [45]. For example, the number of nodes in FD could be much larger than the number of nodes in RN, but A* could do fewer node expansions in FD than MM does in RN if the heuristic is inconsistent. The second reason is that our analysis assumes that A* will expand every node with {a mathematical formula}f(n)&lt;C⁎. This is true when A*'s heuristic is consistent but it is not necessarily true when A*'s heuristic is inconsistent.</paragraph></section><section label="6.3">MM0 compared to Uni-BS<paragraph>We begin by analyzing the brute-force algorithms MM0 and Uni-BS since this lays the foundation for the subsequent comparisons.</paragraph><paragraph>Uni-BS only expands nodes that are near to or far from {a mathematical formula}start. We write this as the equation:{a mathematical formula} F′ here indicates that Uni-BS might not expand all the nodes that are far from {a mathematical formula}start. For example, Uni-BS will usually not expand all nodes that are exactly distance {a mathematical formula}C⁎ from {a mathematical formula}start. By contrast, Uni-BS must expand all nodes near to {a mathematical formula}start.</paragraph><paragraph>MM0 only expands nodes that are near to {a mathematical formula}start or to {a mathematical formula}goal as shown in the following equation:{a mathematical formula} N′ here indicates that MM0 might not expand all the nodes that are near to {a mathematical formula}start or {a mathematical formula}goal. For example, if {a mathematical formula}ϵ&gt;0, MM0 will not expand any node in NN. Moreover, MM0 can terminate before some nodes with {a mathematical formula}gF(n)&lt;(C⁎−ϵ)/2 or {a mathematical formula}gB(n)&lt;(C⁎−ϵ)/2 have been expanded. This is illustrated in Fig. 10. All edge costs in the figure are 1. The numbers in the nodes are discussed in Sections 6.4 and 6.6; they may be ignored for now. {a mathematical formula}Si ({a mathematical formula}Gi) is the layer of nodes at depth i in the tree rooted at {a mathematical formula}start ({a mathematical formula}goal) growing away from the optimal solution. After MM0 expands {a mathematical formula}start and {a mathematical formula}goal, A and {a mathematical formula}S1 will be in {a mathematical formula}OpenF, and C and {a mathematical formula}G1 will be in {a mathematical formula}OpenB, all with {a mathematical formula}g=1 ({a mathematical formula}pr=2g+ϵ=3). Assuming ties are broken in favor of the forward direction, MM0 will next expand A and {a mathematical formula}S1, generating B and {a mathematical formula}S2 with {a mathematical formula}gF=2 ({a mathematical formula}prF=5). It will then switch directions and expand C and {a mathematical formula}G1 in some order. As soon as C is expanded a solution costing {a mathematical formula}U=4 is found. Since {a mathematical formula}gminF+gminB+ϵ=2+1+1≥U, MM0 can stop. This may happen before some nodes in {a mathematical formula}G1 are expanded even though they are distance 1 from {a mathematical formula}goal and {a mathematical formula}(C⁎−ϵ)/2=1.5.</paragraph><paragraph>The difference between {a mathematical formula}F′N in Equation (5) and {a mathematical formula}FN′ in Equation (6) is the following. FN is the intersection of two sets, the set of states far from {a mathematical formula}start, denoted F*, and the set of states near to {a mathematical formula}goal, denoted *N. In {a mathematical formula}F′N, {a mathematical formula}F′ is a subset of F*, so {a mathematical formula}F′N is the intersection of *N and a subset of F*. By contrast, {a mathematical formula}FN′, is the intersection of F* and a subset of *N.</paragraph><paragraph>Uni-BS expands more nodes than MM0 iff (Eq. (5) &gt; Eq. (6)) as written in the following inequality:{a mathematical formula} To identify the core differences between the algorithms, i.e. regions explored by one algorithm but not the other, we ignore the difference between N and N′ and between F and F′, which simplifies Eq. (7) to:{a mathematical formula} We have identified two conditions that guarantee FD &gt; RN:</paragraph><list><list-item label="•">When {a mathematical formula}C⁎=D, the diameter of the space, there are no remote states, by definition, so RN is empty.</list-item><list-item label="•">When the number of states far from {a mathematical formula}start is larger than the number of states near to {a mathematical formula}goal, i.e. if FD + FN &gt; FN + NN + RN, or equivalently,{sup:15} FD &gt; NN + RN. We say a problem ({a mathematical formula}start,goal) is bi-friendly if it has this property.</list-item></list><paragraph>A special case of bi-friendly problems occurs when the following symmetry occurs: the number of states at any distance d from {a mathematical formula}start is the same as the number of states at distance d from {a mathematical formula}goal, for all {a mathematical formula}d≤C⁎. This occurs often in standard heuristic search testbeds, e.g. the Pancake Puzzle, Rubik's Cube, and the Sliding Tile Puzzle when the blank is in the same location type (e.g. a corner) in both {a mathematical formula}start and {a mathematical formula}goal. In such cases, a problem is bi-friendly if the number of states near to {a mathematical formula}start is less than the number of states far from {a mathematical formula}start, i.e. more than half the states at depths {a mathematical formula}d≤C⁎ occur after the solution midpoint. This is similar to the condition in BK1 with {a mathematical formula}h(s)=0∀s. In many testbeds this occurs because the number of states distance d from any state continues to grow as d increases until d is well past {a mathematical formula}12D. For example, Rubik's Cube has {a mathematical formula}D=20 and the number of states at distance d only begins to decrease when {a mathematical formula}d=19 (Table 5.1 in [57]).</paragraph><paragraph>Non-core differences (ND, NN, FN) can sometimes cause large performance differences. The example in Fig. 11 exploits the fact that Uni-BS always expands all nodes in NN but MM0 expands none when {a mathematical formula}ϵ&gt;0. All edges cost 1. {a mathematical formula}start and {a mathematical formula}goal each have one neighbor (s and g respectively) that are roots of depth d binary trees that share leaves (the middle layer, which is NN). {a mathematical formula}C⁎=2d+2 and all paths from {a mathematical formula}start to {a mathematical formula}goal are optimal. FD and RN are empty. The values on the figure's left may be ignored for now; they are used in Section 6.4. MM0 expands all the nodes except those in the middle layer, for a total of {a mathematical formula}2⋅2d nodes expanded. Uni-BS will expand all the nodes except {a mathematical formula}goal, for a total of {a mathematical formula}3⋅2d – 1 nodes, 1.5 times as many as MM0. This ratio can be made arbitrarily large by increasing the branching factor of the trees.</paragraph><paragraph>The general rule based on the analysis in this section is:</paragraph><paragraph>GR1: FD and RN usually determine whether MM0will expand fewer nodes than Uni-BS or more.</paragraph></section><section label="6.4">MM0 compared to A*<paragraph>A heuristic, h, splits each region into two parts, the states in the region that are pruned by h, and the states that are not pruned. For example, FNU is the unpruned part of FN. The set of states expanded by A* is therefore (modified Eq. (5)):{a mathematical formula} We first compare the first three terms to the corresponding terms in Eq. (5) for MM0 and then compare FDU to RN′.</paragraph><paragraph>Region ND: We expect A* to expand many nodes in ND. These nodes have {a mathematical formula}gF(n)≤{a mathematical formula}12C⁎ so A* would prune them only if {a mathematical formula}hF(n)&gt;{a mathematical formula}12C⁎. One might expect MM0's N′D to be larger than A*'s NDU because A* prunes ND with a heuristic. This underestimates the power of the {a mathematical formula}gminF+gminB+ϵ termination condition, which can cause N′D to be much smaller than NDU. In Fig. 10, a number inside node n with a right-pointing arrow over it is {a mathematical formula}hF(n). Not shown are {a mathematical formula}hF(C)=1 and {a mathematical formula}hF(s)=1∀s∈S3. Region ND contains {a mathematical formula}start, A, {a mathematical formula}S1 and {a mathematical formula}S2. The heuristic does no pruning in this region so these are all expanded by A*. MM0 will not expand any node n with {a mathematical formula}gF(n)={a mathematical formula}12C⁎ (e.g. {a mathematical formula}S2) so N′D is half the size of NDU. As a second example, on Rubik's Cube instances with {a mathematical formula}C⁎=20, MM0 only expands nodes with {a mathematical formula}gF(n)≤9 because of this termination condition. The heuristic used by Korf [34] to solve Rubik's Cube has a maximum value of 11, so A* with this heuristic will not prune any nodes in N′D. In general, we do not expect A* to have a large advantage over MM0 in ND unless its heuristic is very accurate.{sup:16}</paragraph><paragraph>Region NN: As discussed above, MM0 usually expands no nodes in NN. Nodes in NN have {a mathematical formula}gF(n)=gB(n)={a mathematical formula}12C⁎, so A*'s {a mathematical formula}f(n) cannot exceed {a mathematical formula}C⁎ on them. Therefore, even with an extremely accurate heuristic, A* may do little pruning in NN. For example, the heuristic values shown on the left side of Fig. 11 are consistent and “almost perfect” [28] yet they produce no pruning at all. A* behaves exactly the same on this example as Uni-BS and expands 1.5 times as many nodes as MM0.</paragraph><paragraph>Region FN: We expect A* to expand far fewer nodes than MM0 in FN. These nodes have {a mathematical formula}gF(n)&gt;{a mathematical formula}12C⁎ and are relatively close to {a mathematical formula}goal. It is common for heuristics to be very accurate near {a mathematical formula}goal so we expect the heuristic values for these nodes to be sufficiently large that many nodes in FN are pruned.</paragraph><paragraph>FDU vs RN′: RN′ certainly can be much smaller than FDU. In Fig. 10, RN ({a mathematical formula}G1+G2) is about the same size as FD ({a mathematical formula}S3), which is the same as FDU in this example. However, because MM0 will not expand any nodes with {a mathematical formula}gB(n)&gt;{a mathematical formula}12{a mathematical formula}(C⁎−ϵ) (= 1.5 in this example), RN′ is half the size of RN (RN′ contains {a mathematical formula}G1 but not {a mathematical formula}G2), so MM0 expands many fewer nodes in RN than A* does in FD. On the other hand, with a sufficiently accurate heuristic, FDU will certainly be the same size as or smaller than RN′. In the extreme case, when RN′ is empty, this requires a heuristic that prunes every node in FD. This is not impossible, since no optimal path passes through FD, but it does require an extremely accurate heuristic. Moreover, FD even without any pruning can be much smaller than RN′. Deleting {a mathematical formula}S3 from Fig. 10 makes FD empty, while RN′ can be made arbitrarily large.</paragraph><paragraph>The general rule based on the analysis in this section is:</paragraph><paragraph>GR2: When FD &gt; RN, A* will expand more nodes than MM0unless A*'s heuristic is very accurate.</paragraph></section><section label="6.5">MM compared to A*<paragraph>Modifying Eq. (6), the equation for MM is:{a mathematical formula} B has the same meaning as U, but is based on {a mathematical formula}hB, the heuristic of MM's backwards search. For example, FNB is the part of FN that is not pruned by {a mathematical formula}hB. In general, FNB will be different than FNU, the part of FN that is not pruned by {a mathematical formula}hF, the heuristic used by A*.</paragraph><paragraph>Regions ND and NN: By definition, N′DU ≤ NDU and N′N′U ≤ NN, so MM has an advantage over A* in ND and NN.</paragraph><paragraph>Region FN: Because A* and MM are searching in different directions in FN, FNU is almost certainly smaller than FN′B. In A*'s forward search nodes in FN have {a mathematical formula}gF(n)&gt;{a mathematical formula}12C⁎ and {a mathematical formula}hF is estimating a small distance (at most {a mathematical formula}12C⁎). By contrast, in MM's backwards search, nodes in FN have {a mathematical formula}gB(n)≤{a mathematical formula}12C⁎ and {a mathematical formula}hB would need to accurately estimate a distance larger than {a mathematical formula}12C⁎ to prune them. So, A* has an advantage over MM's backward search in FN. This is illustrated by the two ovals in Fig. 12. The left oval (solid) represents A* in the forward direction. The right oval (dotted) represents MM in the backward direction. All the nodes in FNU are also inside the dotted oval, i.e. are expanded by the backward search. In addition, FNB includes the striped areas marked “extra” in the figure. These are nodes expanded by MM's backward search but not by A*'s forward search.</paragraph><paragraph>FDU vs RNB: Not much pruning will usually occur during MM's backward search in RN because RN's {a mathematical formula}gB-values are small and the distances being estimated by {a mathematical formula}hB are large. The comparison of FDU and RNB therefore has the same general conclusion as the comparison in the previous subsection of FDU and RN′  namely, that FDU will certainly be the same size as or smaller than RNB with a sufficiently accurate heuristic.</paragraph><paragraph>The general rule based on this section's analysis is the same as GR2 with MM0 replaced by MM.</paragraph><section label="6.5.1">MM compared to A* with inconsistent heuristics<paragraph>Martelli [45] showed that A* could re-expand nodes an exponential number of times if its heuristic is inconsistent. This will also happen with MM when {a mathematical formula}pr(n)=f(n) for all nodes, since in that case MM's forward search will be identical to A*'s. However, if the heuristic is sufficiently weak that {a mathematical formula}pr(n)=2g(n)[+ϵ] for many nodes, MM can do exponentially fewer node re-expansions than A*. This is illustrated in Fig. 13, which is a slightly modified version of Martelli's 6-node graph. The optimal path is {a mathematical formula}start−A−B−C−D−goal, with {a mathematical formula}C⁎=23. The numbers inside the nodes are their {a mathematical formula}hF values. They are admissible but not consistent. {a mathematical formula}hB(goal) (not shown) is 23, which is larger than any {a mathematical formula}prminF-value that occurs during search after {a mathematical formula}start is expanded, so on this graph MM only searches in the forward direction, the same as A*. Just as in Martelli's original 6-node graph, A* re-expands nodes an exponential number of times on this graph. Nodes D, C and B are expanded a total of 8, 4 and 2 times, respectively, by A*. By contrast, MM expands each node only once.</paragraph><paragraph>We believe, but have not fully proven, that the opposite situation cannot occur, i.e. that if MM's forward search re-expands a node then A* must also re-expand it. It is easy to show that if node X is expanded by MM's forward search when {a mathematical formula}gF(X) is suboptimal it is because of the heuristic function, i.e. {a mathematical formula}prF(X)=fF(X) and, at the time X is expanded with a suboptimal {a mathematical formula}gF(X), there is a node N on the optimal path to X such that {a mathematical formula}prF(N)=fF(N)&gt;fF(X). This shows that if A* reaches X by a suboptimal path of the same cost, A* will expand X with this suboptimal {a mathematical formula}gF-value and later have to re-expand it. What we have not yet proven is that if MM's forward search reaches X by a suboptimal path then A* will also reach X by a suboptimal path of the same cost. Of course, node re-expansions by MM's backward search are not connected in any direct way to A*'s forward search.</paragraph></section></section><section label="6.6">MM0 compared to MM: an anomaly<paragraph>If {a mathematical formula}h1 and {a mathematical formula}h2 are admissible heuristics and {a mathematical formula}h1(s)&gt;h2(s) for all non-goal nodes, then every node expanded by A* using {a mathematical formula}h1 will also be expanded by A* using {a mathematical formula}h2 (RESULT 6, p. 81 [49]). In particular, A* with a consistent non-zero heuristic cannot expand more nodes than Uni-BS.</paragraph><paragraph>This is not necessarily true for MM or most Bi-HS algorithms. In Fig. 10 the value in a node is its h-value in the direction indicated by the arrow. All nodes in layer {a mathematical formula}S3 ({a mathematical formula}G3) have {a mathematical formula}hF(s)=1 ({a mathematical formula}hB(s)=1). The heuristic values in each direction are consistent. MMe expands all the nodes in {a mathematical formula}S1 and {a mathematical formula}G1 because they have {a mathematical formula}pr(s)=2g(s)+ϵ=3 while {a mathematical formula}prF(A)=prB(C)=4. By contrast, we saw (Section 6.3) that MM0 could stop before expanding all the nodes in {a mathematical formula}S1 and {a mathematical formula}G1. Thus we see that MM0 can expand strictly fewer nodes than MM with a consistent, non-zero heuristic. Bi-HS algorithms that strictly alternate search direction or use the cardinality criterion to choose the direction and then expand the node in the chosen direction with the smallest f-value will expand even more nodes—they will expand all the nodes in {a mathematical formula}S2 and {a mathematical formula}G2 ({a mathematical formula}f(n)=3) before expanding A and C ({a mathematical formula}fF(A)=fB(C)=4).</paragraph><paragraph>This example mimics behavior we report with the GAP-2 and GAP-3 heuristics in the Pancake puzzle experiments below. We believe it occurs commonly with heuristics that are very accurate near the goal but inaccurate elsewhere.</paragraph><paragraph>The largest excess of MMe over MM0 occurs when {a mathematical formula}fF(n)=fB(n)=C⁎ for all nodes n on all optimal paths, and {a mathematical formula}fF(n) and {a mathematical formula}fB(n) are both strictly less than {a mathematical formula}C⁎ for all other nodes. The situation for unit edge costs is different than that for non-unit edge costs, as we now discuss.</paragraph><list><list-item label="•">In state spaces where all edge costs are 1 there are two cases. If {a mathematical formula}C⁎ is odd, the difference between the two algorithms is entirely due to how they break ties among nodes with {a mathematical formula}gF(n)=(C⁎−1)/2 and {a mathematical formula}gB(n)=(C⁎−1)/2. If {a mathematical formula}C⁎ is even, there can be nodes with {a mathematical formula}gF(n)=C⁎/2−1 or {a mathematical formula}gB(n)=C⁎/2−1 that MMe must expand but that MM0 will only expand in its worst case ({a mathematical formula}G1,S1,A and C in Fig. 10).</list-item><list-item label="•">For state spaces with non-unit costs there is even greater scope for MMe to expand more nodes than MM0, even when there are no ties to break. This is illustrated in Fig. 14. MM0 will expand {a mathematical formula}start (forward), {a mathematical formula}goal (backward), D (backward) and B (forward) and then stop because it has found the path through C costing {a mathematical formula}U=31 and {a mathematical formula}gminF+gminB+ϵ=9+21+1=31. By contrast, once MMe has expanded {a mathematical formula}start (forward), {a mathematical formula}goal (backward), and D (backward), node A will have the lowest priority ({a mathematical formula}prF(A)=2gF(A)+ϵ=19, compared to {a mathematical formula}prF(B)=31 and {a mathematical formula}prB(C)=43), so MMe will expand A and the whole cloud of nodes below it before expanding B (forward) and stopping.</list-item></list><paragraph>The phenomenon we have just been discussing causes MM to expand more nodes than MM0, but MM's heuristic can result in nodes being pruned that MM0 would expand. If MM's heuristic is sufficiently accurate, the number of nodes its heuristic prunes will exceed the number of excess nodes MM expands because of the anomalous behavior described above, and MM will expand fewer nodes than MM0. This is the general conclusion we draw from this section's analysis.</paragraph><paragraph>GR3: Bi-HS with an inaccurate heuristic will expand more nodes than Bi-BS.</paragraph></section><section label="6.7"><section-title>Summary</section-title><paragraph>Many factors come into play in determining which algorithm—A*, MM0, or MM—will expand the fewest nodes. Our general rules express what we think will be the most common outcomes. GR1 says that MM0 will expand more nodes than Uni-BS if region FD is smaller than region RN. Adding a heuristic to both brute-force searches in this situation is not expected to reverse this conclusion, so we expect A* to expand fewer nodes than MM0 and MM (and of course, Uni-BS) when FD is smaller than RN.</paragraph><paragraph>When FD is larger than RN, the algorithm that expands the fewest nodes will depend on the accuracy of the heuristics. With sufficiently inaccurate heuristics, MM0 is expected to expand fewer nodes than A* (GR2) and MM (GR3). As the heuristics' accuracy increases, the advantage of MM0 over MM will diminish and eventually MM will expand fewer nodes than MM0. Because MMe is generally stronger than basic MM, MMe is expected to expand fewer nodes than MM0 with a less accurate heuristic than basic MM. Of course, A* will also be benefiting from the improved heuristic, but, contrary to Barker and Korf [4], we expect MM to expand fewer nodes than both MM0 and A* when the heuristics are moderately accurate. As the accuracy continues to increase, A* will eventually expand fewer nodes than either of the bidirectional searches (because FDU will become very small (GR2)).</paragraph></section></section><section label="7"><section-title>Experiments</section-title><paragraph>The purpose of the experiments in this section is to verify the correctness of our general rules (GR1–GR3) and the conjectures in Section 6.7 about which algorithm—A*, MM0, MM,{sup:17}or MMe—will expand the fewest nodes. Since some of the rules refer to the sizes of certain regions, they could only be tested in domains small enough to be fully enumerated. Likewise, since some of the rules and conjectures refer to a heuristic's relative accuracy, we used at least two heuristics of different accuracy in each domain. All heuristics used in these experiments were consistent, not just admissible. The three domains used in our study are the 10-Pancake Puzzle, grid pathfinding, and Rubik's Cube. In these domains all problems are bi-friendly. Because GR1–GR3 make predictions about the number of nodes expanded, that is the quantity we focus on in our experiments.</paragraph><section label="7.1"><section-title>10-Pancake puzzle</section-title><paragraph>In the 10-pancake puzzle a state is a vector with 10 numbers and the task is to sort them. Operators only allow the reversal of prefixes of the vector. We ran MM0, MM, MMe, Uni-BS, and A* on 30 random instances for each possible value of {a mathematical formula}C⁎ ({a mathematical formula}1≤C⁎≤11). We used the GAP heuristic [27]{sup:18} and derived less accurate heuristics from it, referred to as GAP-X, by not counting the gaps involving any of the X smallest pancakes. For example, GAP-2 does not count the gaps involving pancakes 0 or 1.</paragraph><section label="7.1.1">{a mathematical formula}C⁎=10<paragraph>Table 1 shows the number of nodes expanded in each region for each algorithm using each heuristic for {a mathematical formula}C⁎=10. The first row, “{a mathematical formula}|Region|” shows the number of states in each region. Column “Total” is the total of the five rightmost columns. The total for {a mathematical formula}|Region| is not the size of the entire state space because it does not include region RD (it is not in the table because none of the algorithms expand nodes in RD).</paragraph><paragraph>We see that RN is small (929) and FD is very large (3.5M). As a consequence, MM0 expands many fewer nodes than Uni-BS (GR1), as shown in the first two rows. ND is identical in size to FN+RN (= DN) because of the symmetry in this space. The asymmetry of MM0's expansions in ND and FN+RN is because, for {a mathematical formula}C⁎=10, MM0 must expand all the nodes with {a mathematical formula}g(s)=4 in one direction but not the other. MM's expansions in these regions are much more balanced.</paragraph><paragraph>For the more accurate heuristics (GAP and GAP-1), FDU is very small, and A*'s total is largely determined by ND. By contrast for the less accurate heuristics (GAP-2, GAP-3, and GAP-4) A*'s total is largely determined by FDU. Even with these less accurate heuristics FDU is less than 10% of FD but FDU is large enough to dominate the other regions.</paragraph><paragraph>The bold numbers show the best algorithm for a given heuristic. Depending on the heuristic, the algorithm expanding the fewest nodes is A* (GAP), MM/MMe (GAP-1 and GAP-2), or MM0 (GAP-3 and GAP-4). MM and MMe were identical except for GAP and GAP-4 where MMe had a slight advantage. We explain this behavior below (Section 7.1.3).</paragraph><paragraph>We also experimented with two Bi-HS algorithms that are not guaranteed to meet in the middle. The first was BS* [40] which uses Pohl's cardinality criterion for selecting the next node to expand. In addition, to examine the effect of the 2g term in MM's definition of a node's priority, we ran an altered version of MM, called MM-2g, which is identical to MM except it omits the 2g term in the definition of {a mathematical formula}pr(n), so node n's priority is the usual {a mathematical formula}f(n). We also added code to prevent MM-2g from expanding the same node in both directions. MM-2g is reminiscent of the algorithm by Piljs &amp; Post [52] in that both algorithms choose the node with the minimal f-value in either direction. BS* usually outperforms MM-2g, showing that the cardinality criterion is a better approach. Unlike MM, these algorithms expand many nodes in FD and many more nodes than MM in ND, NN, and FN. Therefore, these algorithms are much worse than MM, highlighting the importance of meeting in the middle. The GAP heuristic is an exception where BS* outperforms MM. The reason is that, similar to A*, when the heuristic is very accurate, using only f-values is beneficial.</paragraph><paragraph>GR1, GR2, and GR3 are all confirmed by this experiment.</paragraph><list><list-item label="•">GR1: For every instance for every value of {a mathematical formula}C⁎, FD &gt; RN and MM0 expanded fewer nodes than Uni-BS.</list-item><list-item label="•">GR2: A* expands more and more nodes in FD as the heuristic becomes less accurate, while MM and MM0 always expand a small fraction of the nodes in RN. With GAP-4 through GAP-1 A* expanded more nodes in FD than MM and MM0 expanded in RN. As expected by GR2, A* is inferior with these heuristics. With GAP, A* expanded fewer nodes in FD than MM0 expanded in RN. Thus, A* was the best with GAP. We note that A* expanded slightly more nodes in FD than MM in RN (3 compared to 0). Still, A* expanded fewer nodes than both MM0 and MM with GAP because of ND and FN (non-core regions).</list-item><list-item label="•">GR3: With the best heuristic, GAP, MM expands many fewer nodes than MM0. As the heuristic becomes less accurate, the difference between MM and MM0 steadily diminishes and eventually (GAP-2) turns into a steadily growing advantage for MM0.</list-item></list><paragraph>In summary, as expected, with the very accurate heuristic (GAP), A* was the best. With moderately accurate heuristics (GAP-1 and GAP-2) MM was the best. Finally, with the weaker heuristics (GAP-3 and GAP-4) MM0 was the best. We have performed similar experiments with {a mathematical formula}C⁎=11. Similar trends were observed with some exceptions, which we explain below in Section 7.1.3.</paragraph></section><section label="7.1.2">Does MM outperform A* because of tie-breaking?<paragraph>If MM's savings over A* all occurs on nodes with {a mathematical formula}f(n)=C⁎ then it could be argued that MM just has a better tie-breaking rule than A* and the gain does not really show that bidirectional search is inherently superior to unidirectional search (on our instances). On the other hand, we know that A*, with a consistent heuristic, must expand all nodes with {a mathematical formula}f(n)&lt;C⁎, but MM does not. So, if we see MM expanding fewer nodes with {a mathematical formula}f(n)&lt;C⁎ than A*, then we have shown that MM is fundamentally superior to unidirectional search on our test instances.</paragraph><paragraph>The {a mathematical formula}f&lt;C⁎ column in Table 1 presents the number of nodes with {a mathematical formula}f&lt;C⁎ expanded by the different algorithms. Clearly, MM expands fewer such nodes than A*, showing that its advantage is not due to better tie breaking.</paragraph></section><section label="7.1.3">Convergence to MM0<paragraph>As the heuristics get weaker the relative advantage of MM0 over MM increases due to the phenomenon underlying GR3. However, at some stage, this trend must stop as MM with the least accurate heuristic possible ({a mathematical formula}h(s)=0∀s) is equivalent to MM0. In order to validate this we varied the heuristics from GAP through GAP-9 (always returns 0) for A*, MM, MMe. Fig. 15, Fig. 16 present the number of nodes expanded (y-axis), averaged over 30 random instances, as a function of the heuristic (x-axis; {a mathematical formula}x=0 is GAP). The rightmost point for MM and MMe in each plot ({a mathematical formula}x=9 is GAP-9) is MM0. For {a mathematical formula}C⁎=10 (Fig. 15), the curves for MM and MMe are indistinguishable and are slightly inferior to MM0 for heuristics GAP-3 through GAP-8. For {a mathematical formula}C⁎=11 (Fig. 16), MMe never expands more nodes than MM0, but the advantage of MM0 over MM increases as we move from GAP through to GAP-3, decreases at GAP-4, and at GAP-5 the two algorithms expand the same number of nodes.</paragraph><paragraph>We note that, for {a mathematical formula}C⁎=10MM and MMe were close or identical in the number of nodes expanded. By contrast, for {a mathematical formula}C⁎=11MMe outperformed MM for the range of GAP-1 until GAP-4. The difference between {a mathematical formula}C⁎=11 and {a mathematical formula}C⁎=10 is explained as follows. As in Section 4.3, to clearly distinguish between basic MM's priority function and MMe's, in this explanation we will use {a mathematical formula}prF and {a mathematical formula}prB for basic MM's priority function and use the special notation {a mathematical formula}prϵF and {a mathematical formula}prBϵ for MMe's priority function. When {a mathematical formula}C⁎=11 all nodes u at depth 5 have {a mathematical formula}prF(u)=10&lt;C⁎ but they have {a mathematical formula}prFϵ(u)=11=C⁎. So, MM is likely to expand many more such nodes than MMe. Thus, for {a mathematical formula}C⁎=11MMe has a large advantage over MM due to such nodes. For {a mathematical formula}C⁎=10 nodes u at depth 4 will have {a mathematical formula}prF(u)=8&lt;C⁎ and {a mathematical formula}prFϵ(u)=9&lt;C⁎. Thus, MMe will have no advantage over MM for such nodes since for both systems these nodes have a priority smaller than {a mathematical formula}C⁎. Nodes u at depth 5 will have {a mathematical formula}prF(u)=10=C⁎ and {a mathematical formula}prFϵ(u)=11&gt;C⁎. MMe will never expand such nodes. However, only very few (or none) such nodes are expanded by MM, so the advantage of MMe over MM is very small (e.g. for GAP and GAP-4) or does not exist (e.g. for GAP-1, GAP-2 and GAP-3).</paragraph></section></section><section label="7.2"><section-title>Grid maps</section-title><paragraph>In this section we investigate grid maps because they have significantly different properties than the other domains that we study. First, they have non-unit edge costs. Second, the maps are irregular. Third, the size of the various regions in the maps can differ drastically between instances. To avoid averaging away too many differences we select a single map (brc203) from the game Dragon Age: Origins in the moving AI benchmark repository [60]. Agents can move in any of 8 directions on these maps, and the default heuristic is the octile heuristic, which is the perfect free-space heuristic for 8-way movement. This heuristic is relatively strong. Therefore, similar to the GAP heuristic for the Pancake puzzle, we weakened it artificially in order have a spectrum of heuristic strengths. For this we multiplied the octile heuristic by the following weights: {a mathematical formula}W={0.1,0.4,0.6,0.8,1}.</paragraph><paragraph>In Table 2 we report the primary results of this experiment on a variety of weights. Results are averages over 1320 problem instances that vary in length from 0 to 527. We focus on the A* and MMe algorithms, and report the average node expansions and region sizes across all problems on the map. We analyze GR1–GR3 in grid worlds and make the following observations:</paragraph><list><list-item label="•">GR1: On average, in this map FD &gt; RN and, as predicted, MM0 expanded fewer nodes than Uni-BS.</list-item><list-item label="•">GR2: Because FD &gt; RN we expect MMe to outperform A* unless the heuristic is accurate. As predicted, A* outperformed MMe with stronger heuristics, as it is able to perform significant pruning in FN. For weaker heuristics MMe was better than A*.</list-item><list-item label="•">GR3: With a heuristic weight of 0.1, MMe expands 0.1 more nodes than MM0 on average. This difference is minimal, suggesting that there are not a significant number of nodes that meet the conditions for GR3 in this state space.</list-item></list><paragraph>Thus, two of the three general rules are strongly confirmed, and the third is weakly confirmed. As we saw with the Pancake puzzle, tie breaking with f-cost &lt; C* does not play a significant role in node expansions, especially with weaker heuristics.</paragraph></section><section label="7.3"><section-title>Rubik's cube</section-title><paragraph>Rubik's cube is a well-studied domain with {a mathematical formula}4.3×1019 states; the maximum distance between any two states is twenty moves [57]. Optimal solutions to random Rubik's cube instances were first found in 1997 [34] using pattern databases (PDBs) [7]. The cube is made up of 8 corner cubes which have 3 possible orientations each and 12 edge cubes which have 2 possible orientations each.</paragraph><paragraph>We study PEMM on Rubik's cube with three different heuristics and with no heuristic (PEMM0). The first heuristic is {a mathematical formula}h1997, the heuristic used for the first optimal solutions. This heuristic uses an 8-corner PDB and two 6-edge PDBs. We use 4-bits per state for the PDBs. The 8-corner PDB has {a mathematical formula}8!37=88,179,840 entries{sup:19} and requires 42 MB of RAM. The 6-edge PDBs have {a mathematical formula}12!6!26=42,577,920 entries and require 20 MB of RAM. Problem symmetry is used for reverse heuristic lookups, so the same PDB can be used in both directions.</paragraph><paragraph>The second heuristic is {a mathematical formula}h888. This heuristic uses the 8-corner PDB and two 8-edge PDBs. The 8-edge PDBs have {a mathematical formula}12!4!28=5,109,350,400 entries and require 2.4 GB of space each. The final heuristic is {a mathematical formula}h8210. This uses the 8-corner PDB, a 2-edge PDB and a 10-edge PDB. The 10-edge PDB has {a mathematical formula}12!2!210=245,248,819,200 entries and requires 114.2 GB of RAM.</paragraph><paragraph>All experiments are run on a server with dual 2.4 GHz Intel Xeon E5 processors with 128 GB of RAM. Each processor has 8 cores, so a total of 16 compute cores are available, and 32 cores are available with hyperthreading. The machine has two 8TB SAS hard disk drives (HDD) combined in a RAID and two 1.6 TB SATA solid state drives (SSD). The experiments that we perform here use the HDDs for external memory and the SSDs for storing PDBs, but alternate configurations could provide higher performance.</paragraph><paragraph>In addition to running PEMM, we compare against IDA* using the standard operator pruning rules for Rubik's Cube [34]. To compare the implementations more fairly, we parallelized IDA* using the approach of AIDA* [56]. This parallel implementation is very efficient, achieving almost perfect speedups.</paragraph><paragraph>In our experimental results we solve the 10 instances from Korf [34] as well as the superflip (S) position [57], one of the positions that is maximally distant (20 moves) from the goal. This is the first time, to our knowledge, that a 20-move Rubik's cube position has been solved with general-purpose search algorithm.</paragraph><paragraph>For IDA* we report the total running time and total number of nodes expanded. Note that experiments with IDA* typically report node generations. We report expansions here to be comparable to the expansions performed by PEMM. In our previous results [29], we reported the number of nodes that would be expanded by a unidirectional search without parallelization. Here, we report the total number of nodes actually expanded by our parallel implementation. This might be more or less than a unidirectional implementation, depending on which thread finds the solution and how many solutions there are.</paragraph><paragraph>The efficiency of PEMM does depend, to some extent, on the number of buckets used during search. In all experiments here PEMM uses 9 bits of the hash in addition to the other bucket parameters, although using fewer bits is faster on easier problems. Using 9 bits is necessary for PEMM0 when solving the superflip position; in preliminary results increasing from 7 to 9 bits reduced the running time from 195,380 seconds to 102,893 seconds. A custom hash table and other enhancements further reduced this time to 59,743 seconds.</paragraph><paragraph>Summary results comparing IDA* and PEMM are found in Table 3, Table 4; more detailed results on the performance of PEMM follow.</paragraph><paragraph>We first look at node expansions in Tables 3. First, note that Rubik's cube is bi-friendly, so FD &gt; RN. We would then, according to GR2, expect that A* does more work than PEMM0, except with a very accurate heuristic. Indeed, with the less-accurate 1997 heuristic, PEMM dominates IDA*, except on the first (easiest) problem. PEMM0 also does fewer node expansions than IDA*, except on the first two problems. PEMM0 does worse than IDA* with the more-accurate 888 heuristic on all problems except the superflip problem; PEMM is only better on the hardest problems.</paragraph><paragraph>When we use the 114 GB 8210 PDB, which is extremely accurate, IDA* has better performance than PEMM across all problems except one. But, digging into the results, there is significant room for PEMM to improve. On the superflip position less than 1% of the states expanded by IDA* were expanded while searching at depth 20. PEMM0 also expands about 1% of the total states at depth 20, finding the solution in the first bucket. PEMM with the 888 heuristic expanded 3% of the total states at depth 20, but with the 8210 heuristic 65.8% of its node expansions are at depth 20. Here there is a conflict between the g-cost ordering needed for efficient search and the g-cost ordering needed for efficient tie-breaking. More research is needed to resolve this conflict.</paragraph><paragraph>In general, these results confirm GR2 and show the phase transition that occurs from PEMM0 to PEMM to IDA* as the problems get easier or the heuristic gets more accurate. PEMM still has room for optimization, as we will see clearly when looking at timing results.</paragraph><paragraph>In Table 4 we look at timing for each algorithm. Here we see a slightly different picture than node expansions. PEMM0 is faster than all other approaches by a wide margin on the superflip position, except when IDA* has a 114 GB pattern database. With the 1997 heuristic PEMM is generally faster than IDA*, but with the 888 heuristic IDA* is faster than PEMM.</paragraph><paragraph>One significant reason for the difference is that we expand states much slower when using a heuristic. The primary cost during search is the ranking function that converts states to and from integers for storage on disk. When using the heuristics we must perform additional ranking operations, which slows down the search. While our IDA* implementation is very efficient, there is still significant research that can be done on more efficient search for PEMM.</paragraph><paragraph>Looking into detail at the performance of PEMM and PEMM0 we find that PEMM spends from 95–99% of its time performing node expansions, with a negligible amount of time doing I/O. Before implementing a better hash table and removing the h-cost from the bucket computation PEMM was spending up to 40% of its time doing I/O, so these optimizations were effective in reducing I/O, which does not parallelize well. PEMM0 spends 40–50% of its time doing I/O, except on the easiest two problems. The majority of this time (30–40% overall) is spend on DSD. This suggest that PEMM0 needs different optimizations than PEMM moving forward, even though PEMM and PEMM0 are using exactly the same code.</paragraph><paragraph>Finally, we look at the disk usage required to solve each problem. These results are in Table 5. The superflip position consistently takes 5 TB of disk to solve, while the memory required for the other problems is significantly less, depending on the strength of the heuristic, which reduces storage, and the difficulty of the problem, which increases storage.</paragraph><paragraph>To summarize the results here, we see that GR2 is descriptive of performance on Rubik's cube. While our implementation of PEMM0 set a new milestone by solving a depth-20 problem without heuristic guidance, there is still room for improving the performance of PEMM and PEMM0 by reducing the overhead from DSD in PEMM0 and the cost of heuristic lookups in PEMM.</paragraph></section></section><section label="8"><section-title>Conclusions and future work</section-title><paragraph>In this paper we introduced MM, the first Bi-HS algorithm guaranteed to meet in the middle. We also introduced a framework that divides the state space into disjoint regions and allows a careful analysis of the behavior of the different algorithms in each of the regions. We studied the various types of algorithms and provided some general rules that were confirmed by our experiments.</paragraph><paragraph>This paper initiated this direction. Future work will continue as follows: (1) A deeper analysis on current and new MM variants may further deepen our knowledge in this issue. (2) A thorough experimental comparison should be done on more domains and with more bidirectional search algorithms. (3) Heuristics that are specifically designed for MM, i.e., that only return values larger than {a mathematical formula}C⁎/2 are needed [62].</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>Thanks to Joseph Barker for answering questions and providing extra data related to [4] and to Sandra Zilles and André Grahl Pereira for suggesting improvements in the theoretical analysis of MM. Financial support for this research was in part provided by the Natural Sciences and Engineering Research Council of Canada (NSERC) and by Israel Science Foundation (ISF) grants #417/3 and #212/17. Computational facilities for some of our experiments were provided by Compute Canada. This material is based upon work supported by the National Science Foundation under Grant No. 1551406.</paragraph></acknowledgements><appendices><section label="Appendix A">Proofs of MMe's properties<paragraph>In this appendix we prove that given an admissible heuristic (not necessarily consistent) MMe has various properties including:</paragraph><list><list-item>MMe's forward and backward searches meet in the middle; neither search expands a node whose distance from the search's origin ({a mathematical formula}gF(n) for forward search, {a mathematical formula}gB(n) for backward search) is larger than {a mathematical formula}12{a mathematical formula}(C⁎−ϵ) (Corollary 15).</list-item><list-item>MMe never expands a node whose f-value exceeds {a mathematical formula}C⁎ (Corollary 15).</list-item><list-item>MMe returns {a mathematical formula}C⁎ (Lemma 8 if there is no path from {a mathematical formula}start to {a mathematical formula}goal, Theorem 18 if there is).</list-item><list-item>If there exists a path from {a mathematical formula}start to {a mathematical formula}goalMMe never expands a state in both search directions (Theorem 16).</list-item></list><section label="A.1"><section-title>Terminology – nodes vs. states</section-title><paragraph>States and nodes are different kinds of entities. A state is an immutable element of a state space, with a fixed distance to {a mathematical formula}start and {a mathematical formula}goal. A node, by contrast, is an entity created and updated by a search algorithm representing a path (or set of paths) in the state space. At a minimum, node n stores the path's cost ({a mathematical formula}g(n)) and the last state on the path, which we call the state associated with n.</paragraph><paragraph>Rarely, if ever, is there ambiguity about which term should be used in a given context. Nodes are expanded, not states, because the process of expansion requires a g-value and states do not have g-values, only nodes do. Similarly, the regions defined in Section 6.1 (FD, NN, etc.) are regions of a state space—sets of states—because they are defined in terms of distances to {a mathematical formula}start and {a mathematical formula}goal and only states have such distances (nodes have g-values).</paragraph><paragraph>However, there are a few situations where the correct wording would be awkward. For example, it is technically incorrect to write “how many nodes are expanded in region FD?” The correct way to say this is “how many nodes are expanded whose associated state is in FD?”. We prefer the simpler expression even though it is not technically correct. As a second example, property P4 has the technically incorrect wording “never expands a state in both search directions”. What is meant is that if state s is associated with a node expanded in one direction s will not be associated with any of the nodes expanded in the other direction. Likewise, if we say a state s is open (or closed), we mean there is an open (or closed) node whose associated state is s.</paragraph></section><section label="A.2"><section-title>Formal definitions, theorems, and proofs</section-title><paragraph>In this appendix we will use the pseudocode in Algorithm 1 except for the stopping condition (line 7). For the moment, we will use a weaker stopping condition: MMe will terminate search as soon as {a mathematical formula}U≤C. This simplifies the proofs of MMe's key properties. In Section A.4 we will replace this stopping condition with the stronger stopping condition used in Algorithm 1 and show that MMe maintains all its key properties when the stronger stopping condition is used.</paragraph><paragraph>In this appendix we use the {a mathematical formula}prF and {a mathematical formula}prB (and related terms in the pseudocode such as {a mathematical formula}prminF and {a mathematical formula}prminB) to refer to MMe's definition of priority{sup:20}{a mathematical formula} and {a mathematical formula}prB(n) is defined analogously.</paragraph><paragraph>The core reasoning behind the proofs of P1–P3 is as follows. As long as there remains an optimal path that has not been “found” (Definition 2, page 239) C will be less than or equal to {a mathematical formula}C⁎ (Theorem 7), and as soon the first optimal path is found U will be set to {a mathematical formula}C⁎ (Lemma 13). Property P3 follows directly from these two facts (Theorem 18), since they show that MMe will not halt until after U has been set to {a mathematical formula}C⁎. These two facts also imply that MMe will terminate without expanding any node whose priority is greater than {a mathematical formula}C⁎ (Lemma 14), which immediately proves properties P1 and P2 (Corollary 15), since any node n with {a mathematical formula}gX(n)&gt;{a mathematical formula}12{a mathematical formula}(C⁎−ϵ) or {a mathematical formula}fX(n)&gt;C⁎ will have {a mathematical formula}prX(n)&gt;C⁎ (X here is a search direction, either F or B).</paragraph><paragraph>We assume that all edge costs ({a mathematical formula}cost(u,v)) are non-negative (zero-cost edges are allowed), that {a mathematical formula}start≠goal, and that the heuristic used by MMe in each search direction is admissible.</paragraph><paragraph>The following definition and Lemma 2, Lemma 3, Lemma 4 are closely based on Hart, Nilsson, and Raphael's Lemma 1 and its proof [23].</paragraph><paragraph label="Definition 3">Node n is “permanently closed” in the forward search direction if {a mathematical formula}n∈ClosedF and {a mathematical formula}gF(n)=d(start,n). Likewise, n is permanently closed in the backward search direction if {a mathematical formula}n∈ClosedB and {a mathematical formula}gB(n)=d(n,goal).</paragraph><paragraph>The name “permanently closed” is based on the following lemma.</paragraph><paragraph label="Lemma 2">If node n is permanently closed in a particular search direction at the start of some iteration, it will be permanently closed in that direction at the start of all subsequent iterations.</paragraph><paragraph label="Proof">This proof is for the forward search, the proof for the backward search is analogous. There is no code in Algorithm 1 to directly change {a mathematical formula}gF(n) while {a mathematical formula}n∈ClosedF, so n can only stop being permanently closed by being removed from {a mathematical formula}ClosedF. This is possible (line 17) but only if a strictly cheaper path to n is found (line 14). This is not possible since {a mathematical formula}gF(n)=d(start,n) for a node permanently closed in the forward direction. Therefore, once n is permanently closed in the forward direction it will remain so. □</paragraph><paragraph label="Lemma 3">Let{a mathematical formula}P=s0,s1,…snbe an optimal path from{a mathematical formula}start(s0)to any state{a mathematical formula}sn. If{a mathematical formula}snis not permanently closed in the forward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}sn−1is permanently closed in the forward direction, then{a mathematical formula}sn∈OpenFand{a mathematical formula}gF(sn)=d(start,sn). Analogously, let{a mathematical formula}P=s0,s1,…snbe an optimal path from any state{a mathematical formula}s0to{a mathematical formula}goal=sn. If{a mathematical formula}s0is not permanently closed in the backward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}s1is permanently closed in the backward direction, then{a mathematical formula}s0∈OpenBand{a mathematical formula}gB(s0)=d(s0,goal).</paragraph><paragraph label="Proof">This proof is for the forward search, the proof for the backward search is analogous. If {a mathematical formula}n=0, {a mathematical formula}s0=start has not been closed in the forward direction and the lemma is true because lines 1–2 put {a mathematical formula}start∈OpenF with {a mathematical formula}gF(start)=d(start,start)=0. Suppose {a mathematical formula}n&gt;0. When {a mathematical formula}sn−1 was expanded to become permanently closed in the forward direction {a mathematical formula}sn was generated via an optimal path (in lines 14 and 18, {a mathematical formula}gF(n)+cost(n,c)=d(start,sn−1)+cost(sn−1,sn)=d(start,sn)). {a mathematical formula}sn cannot have been permanently closed in the forward direction at that time because if it was, it still would be (Lemma 2). If {a mathematical formula}sn∈ClosedF∪OpenF at that time with a suboptimal g-value, then it would have been removed from {a mathematical formula}ClosedF∪OpenF (line 17) and added to {a mathematical formula}OpenF (line 19) with {a mathematical formula}gF=d(start,sn). If {a mathematical formula}sn∉ClosedF∪OpenF at that time, it would likewise have been added to {a mathematical formula}OpenF with {a mathematical formula}gF=d(start,sn) (line 19). Finally, if {a mathematical formula}sn∈OpenF at that time with {a mathematical formula}gF(sn)=d(start,sn) it would have remained so. Therefore, no matter what {a mathematical formula}sn's status was at the time {a mathematical formula}sn−1 was expanded to become permanently closed in the forward direction, at the end of that iteration {a mathematical formula}sn∈OpenF and {a mathematical formula}gF(sn)=d(start,sn). In subsequent iterations {a mathematical formula}gF(sn) cannot have changed, since that only happens if a strictly cheaper path to {a mathematical formula}sn is found (lines 14 and 15), which is impossible. It also cannot have been closed, since if that had happened it would now be permanently closed. □</paragraph><paragraph label="Lemma 4">Let{a mathematical formula}P=s0,s1,…snbe an optimal path from{a mathematical formula}start(s0)to any state{a mathematical formula}sn. If{a mathematical formula}snis not permanently closed in the forward direction then there exists an i ({a mathematical formula}0≤i≤n) such that{a mathematical formula}si∈OpenFand{a mathematical formula}gF(si)=d(start,si). Let{a mathematical formula}iminbe the smallest such i and define{a mathematical formula}nF(for path P) to be{a mathematical formula}simin. Analogously, let{a mathematical formula}P=s0,s1,…snbe an optimal path from any state{a mathematical formula}s0to{a mathematical formula}goal=sn. If{a mathematical formula}s0is not permanently closed in the backward direction then there exists a j ({a mathematical formula}0≤j≤n) such that{a mathematical formula}sj∈OpenBand{a mathematical formula}gB(sj)=d(sj,goal). Let{a mathematical formula}jmaxbe the largest such j and define{a mathematical formula}nB(for path P) to be{a mathematical formula}sjmax.</paragraph><paragraph label="Proof">This proof is for the forward search, the proof for the backward search is analogous. If {a mathematical formula}start∉ClosedF then {a mathematical formula}i=0 has the required properties ({a mathematical formula}start∈OpenF and {a mathematical formula}gF(start)=d(start,start)=0, because of lines 1–2). Suppose {a mathematical formula}start∈ClosedF. Let k ({a mathematical formula}0≤k&lt;n) be the largest index such that {a mathematical formula}sk is permanently closed. Such a k must exist because {a mathematical formula}start ({a mathematical formula}k=0) is permanently closed. By Lemma 3{a mathematical formula}sk+1∈OpenF and {a mathematical formula}g(sk+1)=d(start,sk+1). Therefore {a mathematical formula}i=k+1 has the required properties. □</paragraph><paragraph>The definition of {a mathematical formula}nF for path {a mathematical formula}s0(start),s1,…sn is illustrated in Fig. 17.</paragraph><paragraph>The following lemma shows that, for an optimal path P from {a mathematical formula}start to {a mathematical formula}goal, {a mathematical formula}nF and {a mathematical formula}nB for P are exactly the states {a mathematical formula}si and {a mathematical formula}sj for P defined in Definition 2 (page 239).</paragraph><paragraph label="Lemma 5">Let{a mathematical formula}P=s0,s1,…snbe an optimal path from{a mathematical formula}start(s0)to{a mathematical formula}goal(sn)that has not been found. Then{a mathematical formula}nFand{a mathematical formula}nB, as defined inLemma 4, both exist for P and{a mathematical formula}nF=siand{a mathematical formula}nB=sj, where{a mathematical formula}siand{a mathematical formula}sjare as defined inDefinition 2.</paragraph><paragraph label="Proof">Let i and j be as in Definition 2. For the forward search, {a mathematical formula}s0,s1,…si is an optimal path from {a mathematical formula}start to {a mathematical formula}si and {a mathematical formula}si∉ClosedF and therefore is not permanently closed in the forward direction. Therefore, {a mathematical formula}s0,s1,…si satisfies the conditions of Lemma 4 for the forward direction and {a mathematical formula}nF=si′ exists for path {a mathematical formula}s0,s1,…si. Because {a mathematical formula}s0,s1,…si−1 are all in {a mathematical formula}ClosedF, it must be that {a mathematical formula}i′=i. Since {a mathematical formula}i′ is the smallest index between 0 and i such that {a mathematical formula}si′∈OpenF and {a mathematical formula}gF(si′)=d(start,si′), it is also the smallest index between 0 and n with these properties, so {a mathematical formula}si′ is also {a mathematical formula}nF for path P. For the backward search, the reasoning is analogous. {a mathematical formula}sj,sj+1,…sn is an optimal path from {a mathematical formula}sj to {a mathematical formula}goal and {a mathematical formula}sj∉ClosedB and therefore is not permanently closed in the backward direction. Therefore, {a mathematical formula}sj,sj+1,…sn satisfies the conditions of Lemma 4 for the backward direction and {a mathematical formula}nB=sj′ exists for path {a mathematical formula}sj,sj+1,…sn. Because {a mathematical formula}sj+1,sj+2,…sn are all in {a mathematical formula}ClosedB, it must be that {a mathematical formula}j′=j. Since {a mathematical formula}j′ is the largest index between j and n such that {a mathematical formula}sj′∈OpenB and {a mathematical formula}gB(sj′)=d(sj′,goal), it is also the largest index between 0 and n with these properties, so {a mathematical formula}sj′ is also {a mathematical formula}nB for path P. □</paragraph><paragraph label="Lemma 6">If{a mathematical formula}P=s0,s1,…snis an optimal path from{a mathematical formula}start(s0)to{a mathematical formula}goal(sn)that has not been found, let{a mathematical formula}nF=siand{a mathematical formula}nB=sjbe as defined inLemma 4. Then{a mathematical formula}gF(nF)+gB(nB)≤C⁎−ϵ.</paragraph><paragraph label="Proof">Lemma 5 guarantees that {a mathematical formula}nF and {a mathematical formula}nB exist for P. Because {a mathematical formula}i&lt;j, {a mathematical formula}d(start,si)+d(si,sj)+d(sj,goal)=C⁎, the cost of the whole path P. Because edge costs are non-negative {a mathematical formula}d(si,sj)≥ϵ, and therefore {a mathematical formula}d(start,si)+d(sj,goal)≤C⁎−ϵ. The lemma follows because {a mathematical formula}gF(nF)=d(start,si) and {a mathematical formula}gB(nB)=d(sj,goal). □</paragraph><paragraph label="Theorem 7">If, at the beginning of anMMeiteration, there exists an optimal path P from{a mathematical formula}startto{a mathematical formula}goalthat has not been found, then{a mathematical formula}C≤C⁎.</paragraph><paragraph label="Proof">Let {a mathematical formula}nF and {a mathematical formula}nB on path P be as defined in Lemma 4. By Lemma 6, {a mathematical formula}gF(nF)+gB(nB)≤C⁎−ϵ, and therefore at least one of {a mathematical formula}gF(nF) and {a mathematical formula}gB(nB) must be less than or equal to {a mathematical formula}12{a mathematical formula}(C⁎−ϵ). Suppose, without loss of generality, that {a mathematical formula}gF(nF)≤12{a mathematical formula}(C⁎−ϵ). Then {a mathematical formula}prF(nF)≤C⁎ because {a mathematical formula}fF(nF)≤C⁎ (because the heuristic {a mathematical formula}hF is admissible and {a mathematical formula}gF(nF) is optimal) and {a mathematical formula}2gF(nF)+ϵ≤C⁎. Since C is the minimum priority of all the nodes in both Open lists and {a mathematical formula}nF∈OpenF, C cannot be larger than {a mathematical formula}prF(nF) and therefore {a mathematical formula}C≤C⁎. □</paragraph><paragraph>Much of the following proof is closely based on Pearl's proof that A* always terminates on finite graphs (Section 3.1.2 in [50]).</paragraph><paragraph label="Lemma 8">For any finite state space S with non-negative edge costsMMehalts for any{a mathematical formula}startand{a mathematical formula}goalstates in S. If there is no path from{a mathematical formula}startto{a mathematical formula}goal,MMereturns ∞.</paragraph><paragraph label="Proof">If the condition in line 7 is satisfied on some iteration, MMe will halt immediately. Suppose the condition in line 7 is never satisfied. Lines 14 and 15 ensure that MMe never expands a node via the same path twice and, because there are no negative-cost cycles{sup:21} (non-negative edge costs guarantee this), they also ensure that MMe never expands a node via a path containing a cycle. In a finite space there are a finite number of acyclic paths to each state. Therefore each state can only be expanded a finite number of times in each search direction before it becomes permanently closed in that direction, and once it becomes permanently closed in a direction it remains so (Lemma 2). Since each iteration expands a node in one of the search directions, after a finite number of iterations MMe will have permanently closed all the nodes reachable in one of the search directions, the Open list for that search direction will be empty, the condition in line 5 for continuing to iterate will not be satisfied, and MMe will halt (line 24).If there is no path from {a mathematical formula}start to {a mathematical formula}goal the condition in line 20 will never be satisfied, so U will always have its initial value of ∞. If C becomes infinite—for example because all {a mathematical formula}n∈OpenF have {a mathematical formula}hF(n)=∞ indicating that {a mathematical formula}goal cannot be reached from them and all {a mathematical formula}n∈OpenB have {a mathematical formula}hB(n)=∞ indicating that they cannot be reached from {a mathematical formula}start—then the condition in line 7 will be satisfied and MMe will return {a mathematical formula}U=∞. If C never becomes infinite, we have shown in the previous paragraph that, after a finite number of iterations, the condition in line 5 for continuing to iterate will not be satisfied, and MMe will return ∞ (line 24). □</paragraph><paragraph>Implementations of search algorithms often maintain “parent pointers” for each open and closed node. The parent pointer for an open node, n, points to the node p that is responsible for n being on Open with its current {a mathematical formula}g(n) value. If n is closed it keeps the parent pointer it had on Open at the time it was expanded. Our pseudocode for MMe does not contain parent pointers, but the proof of Lemma 12 makes use of properties of the “generating path” of an open node n, which is the sequence of nodes defined by the parent pointers from n back to {a mathematical formula}start (for forward search, back to {a mathematical formula}goal for backward search). Definition 4, Definition 5 are the formal definitions of “parent” and “generating path” and Lemmas 9 to 11 prove basic properties about them.</paragraph><paragraph label="Definition 4">If {a mathematical formula}s≠start and {a mathematical formula}s∈OpenF∪ClosedF at the start of iteration t with {a mathematical formula}gF(s)=g then {a mathematical formula}parentF(〈s,t,g〉) is defined to be the triple {a mathematical formula}〈s′,t′,g′〉 such that on iteration {a mathematical formula}t′, s was added to {a mathematical formula}OpenF with {a mathematical formula}gF(s)=g as a consequence of {a mathematical formula}s′ being expanded in the forward direction with {a mathematical formula}g′=gF(s′)=g−cost(s′,s). {a mathematical formula}parentF(〈start,t,g〉) is undefined. Similarly, if {a mathematical formula}s≠goal and {a mathematical formula}s∈OpenB∪ClosedB at the start of iteration t with {a mathematical formula}gB(s)=g then {a mathematical formula}parentB(〈s,t,g〉) is defined to be the triple {a mathematical formula}〈s′,t′,g′〉 such that on iteration {a mathematical formula}t′, s was added to {a mathematical formula}OpenB with {a mathematical formula}gB(s)=g as a consequence of {a mathematical formula}s′ being expanded in the backward direction with {a mathematical formula}g′=gB(s′)=g−cost(s,s′). {a mathematical formula}parentB(〈goal,t,g〉) is undefined.</paragraph><paragraph label="Lemma 9">Suppose{a mathematical formula}s≠startand{a mathematical formula}s∈OpenF∪ClosedFat the start of iteration t with{a mathematical formula}gF(s)=g. Then:</paragraph><list><list-item label="(a)">{a mathematical formula}parentF(〈s,t,g〉)exists,</list-item><list-item label="(b)">{a mathematical formula}parentF(〈s,t,g〉)is unique, and</list-item><list-item label="(c)">If{a mathematical formula}parentF(〈s,t,g〉)=〈s′,t′,g′〉then{a mathematical formula}t′&lt;t.</list-item></list><paragraph label="Proof">This proof is for the forward direction, the proof for the backward direction is analogous.(a) If {a mathematical formula}s≠start, the only way it can be added to {a mathematical formula}OpenF is by having been generated by some other node being expanded, and the only way it can be added to {a mathematical formula}ClosedF is to have first been added to {a mathematical formula}OpenF.(b) If a state is added to {a mathematical formula}OpenF multiple times, it must be with a different g-value each time. Therefore s and g together uniquely identify the state ({a mathematical formula}s′) that caused s to be added to {a mathematical formula}OpenF with {a mathematical formula}gF(s)=g.(c) A state cannot be on {a mathematical formula}OpenF or {a mathematical formula}ClosedF with {a mathematical formula}gF(s)=g until after it has been added to {a mathematical formula}OpenF with {a mathematical formula}gF(s)=g. □</paragraph><paragraph label="Lemma 10">Suppose{a mathematical formula}s≠startand{a mathematical formula}s∈OpenF∪ClosedFat the start of iteration t with{a mathematical formula}gF(s)=d(start,s). If{a mathematical formula}parentF(〈s,t,g〉)=〈s′,t′,g′〉, then{a mathematical formula}s′is permanently closed in the forward direction. Likewise, Suppose{a mathematical formula}s≠goaland{a mathematical formula}s∈OpenB∪ClosedBat the start of iteration t with{a mathematical formula}gB(s)=d(s,goal). If{a mathematical formula}parentB(〈s,t,g〉)=〈s′,t′,g′〉, then{a mathematical formula}s′is permanently closed in the backward direction.</paragraph><paragraph label="Proof">This proof is for the forward direction, the proof for the backward direction is analogous. {a mathematical formula}d(start,s)=gF(s)=g′+cost(s′,s)≥d(start,s′)+cost(s′,s)≥d(start,s). Therefore all these terms are equal. In particular {a mathematical formula}g′+cost(s′,s)=d(start,s′)+cost(s′,s), i.e. {a mathematical formula}g′=d(start,s′). Hence, {a mathematical formula}s′ became permanently closed on iteration {a mathematical formula}t′ and will remain so for all future iterations (Lemma 2). □</paragraph><paragraph label="Definition 5">If {a mathematical formula}s≠start and {a mathematical formula}s∈OpenF∪ClosedF at the start of iteration t with {a mathematical formula}gF(s)=g then the forward generating path for {a mathematical formula}〈s,t,g〉, {a mathematical formula}GenPathF(〈s,t,g〉), is defined recursively:{a mathematical formula}GenPathF(〈start,t,g〉)=∅if {a mathematical formula}s≠start, {a mathematical formula}GenPathF(〈s,t,g〉)=GenPathF(parentF(〈s,t,g〉))::parentF(〈s,t,g〉),where {a mathematical formula}X::Y adds element Y to the end of a sequence X. Likewise, if {a mathematical formula}s≠goal and {a mathematical formula}s∈OpenB∪ClosedB at the start of iteration t with {a mathematical formula}gB(s)=g then the backward generating path for {a mathematical formula}〈s,t,g〉, {a mathematical formula}GenPathB(〈s,t,g〉), is defined analogously.</paragraph><paragraph>The forward (backward) generating path for {a mathematical formula}〈s,t,g〉 is well-defined because the recursion must terminate (t strictly decreases as each recursive call is made (Lemma 9(c)), and t cannot be negative) and it cannot terminate at any state other than {a mathematical formula}start (for the forward direction, {a mathematical formula}goal for the backward direction) because {a mathematical formula}parentF(〈s,t,g〉) (for the forward direction, {a mathematical formula}parentB(〈s,t,g〉) for the backward direction) exists for all the {a mathematical formula}〈s,t,g〉 generated in this sequence of recursive calls unless {a mathematical formula}s=start (Lemma 9(a)).</paragraph><paragraph label="Lemma 11">Suppose{a mathematical formula}s≠startand{a mathematical formula}s∈OpenF∪ClosedFat the start of iteration t with{a mathematical formula}gF(s)=d(start,s). Then all the states in{a mathematical formula}GenPathF(〈s,t,g〉)are permanently closed in the forward direction. Likewise, suppose{a mathematical formula}s≠goaland{a mathematical formula}s∈OpenB∪ClosedBat the start of iteration t with{a mathematical formula}gB(s)=d(start,s). Then all the states in{a mathematical formula}GenPathB(〈s,t,g〉)are permanently closed in the backward direction.</paragraph><paragraph label="Proof">This proof is for the forward direction, the proof for the backward direction is analogous. By Lemma 10, if {a mathematical formula}parentF(〈s,t,g〉)=〈s′,t′,g′〉 then {a mathematical formula}s′ is permanently closed in the forward direction. The same lemma can therefore be applied to {a mathematical formula}〈s′,t′,g′〉 to show that {a mathematical formula}s″ is permanently closed, where {a mathematical formula}〈s″,t″,g″〉=parentF(s′,t′,g′). This process can be repeated backwards through the entire chain, showing that all states in {a mathematical formula}GenPathF(〈s,t,g〉) are permanently closed in the forward direction. □</paragraph><paragraph label="Lemma 12">If there exists a path from{a mathematical formula}startto{a mathematical formula}goal,MMewill not terminate until at least one optimal path from{a mathematical formula}startto{a mathematical formula}goalhas been found.</paragraph><paragraph label="Proof">Lemma 5 guarantees that {a mathematical formula}OpenF and {a mathematical formula}OpenB are both non-empty as long as there is any optimal path from {a mathematical formula}start to {a mathematical formula}goal that has not been found, so the termination condition in Line 5 cannot be satisfied until all optimal paths from {a mathematical formula}start to {a mathematical formula}goal have been found. The only other termination condition is {a mathematical formula}U≤C (the version of line 7 we are using in these proofs). Assume (for the purpose of contradiction) that this termination condition is satisfied before any optimal path from {a mathematical formula}start to {a mathematical formula}goal has been found. Theorem 7 shows that {a mathematical formula}C≤C⁎ until all optimal paths from {a mathematical formula}start to {a mathematical formula}goal have been found, so for {a mathematical formula}U≤C to hold if no optimal paths from {a mathematical formula}start to {a mathematical formula}goal have been found, U must be equal to {a mathematical formula}C⁎. We will now show that {a mathematical formula}U=C⁎ implies an optimal path from {a mathematical formula}start to {a mathematical formula}goal has been found, contradicting our assumption, thereby proving the lemma. U is set in line 21. On the iteration in which U was set to {a mathematical formula}C⁎, there must have been a child node generated, c, that satisfied the conditions of line 20, i.e. {a mathematical formula}c∈OpenB and {a mathematical formula}gF(c)+gB(c)=C⁎. The latter implies {a mathematical formula}gF(c)=d(start,c) and {a mathematical formula}gB(c)=d(c,goal), i.e. c is on an optimal path from {a mathematical formula}start to {a mathematical formula}goal with optimal g-values in both directions. This means Lemma 11 applies to c in both directions, i.e. that all the nodes on the forward and backward generating paths for c are permanently closed. The concatenation of these two paths, with c in between, is an optimal path from {a mathematical formula}start to {a mathematical formula}goal that was found on the iteration when U was set to {a mathematical formula}C⁎. □</paragraph><paragraph label="Lemma 13">If there exists a path from{a mathematical formula}startto{a mathematical formula}goal, let{a mathematical formula}P=s0,s1,…snbe the first optimal path from{a mathematical formula}start(s0)to{a mathematical formula}goal(sn)that is found duringMMe's execution, and let{a mathematical formula}nF=siand{a mathematical formula}nB=sjbe as defined inLemma 4at the beginning of the iteration on which P is found. Then during that iteration U will be set to{a mathematical formula}C⁎in line 21.</paragraph><paragraph label="Proof">Lemma 12 guarantees that P exists, and Lemma 5 guarantees that {a mathematical formula}nF and {a mathematical formula}nB exist for P at the beginning of the iteration on which it becomes found. One of them must be expanded on this iteration because P's status will not change from “not found” to “found” if {a mathematical formula}nF remains on {a mathematical formula}OpenF and {a mathematical formula}nB remains on {a mathematical formula}OpenB. We will complete the proof assuming that {a mathematical formula}nF is expanded. The proof in the case that {a mathematical formula}nB is expanded is analogous. We will prove the following before proving the lemma:<list>When {a mathematical formula}nF is expanded, {a mathematical formula}nB will be generated as one of its children;When the test in Line 14 is applied to {a mathematical formula}nB ({a mathematical formula}nB∈OpenF∪ClosedF and {a mathematical formula}gF(nB)≤gF(nF)+cost(nF,nB)) it will fail.Proof of (a): Suppose </list><paragraph>{a mathematical formula}nB is not generated as a child of {a mathematical formula}nF when it is expanded in the forward direction. Then there must exist one or more nodes between them, i.e. {a mathematical formula}P=start…nFt1…tknB…goal(k≥1). In order for P to be “found” at the end of this iteration, it must be the case that {a mathematical formula}ti∈ClosedF∀1≤i≤k. Since the path {a mathematical formula}start…nFt1 is an optimal path from {a mathematical formula}start to {a mathematical formula}t1, {a mathematical formula}t1∈ClosedF after being generated by {a mathematical formula}nF means that it had previously been generated via a different optimal path, which implies an optimal path had previously been found from {a mathematical formula}start to all the {a mathematical formula}ti and, indeed, to {a mathematical formula}nB. Combining this previously found optimal path from {a mathematical formula}start to {a mathematical formula}nB with the optimal path found by the backwards search from {a mathematical formula}nB to {a mathematical formula}goal creates an optimal path from {a mathematical formula}start to {a mathematical formula}goal that had been found prior to P. This contradicts the premise that P is the first optimal path found from {a mathematical formula}start to {a mathematical formula}goal.Proof of (b): The path {a mathematical formula}start…nFnB is optimal, i.e. {a mathematical formula}gF(nF)+cost(nF,nB)=d(start,nB). The test in line 14 can therefore only succeed if an optimal path from {a mathematical formula}start to {a mathematical formula}nB had previously been found, which contradicts the premise that P is the first optimal path found from {a mathematical formula}start to {a mathematical formula}goal.Proof of the lemma: Because of (b), the test in line 20 succeeds because {a mathematical formula}nB is a child of {a mathematical formula}nF (by (a)) and {a mathematical formula}nB∈OpenB by definition. Because of (b), {a mathematical formula}gF(nB)+gB(nB)=d(start,nB)+d(nB,goal)=C⁎, so U will be set to {a mathematical formula}C⁎ in line 21. □</paragraph></paragraph><paragraph label="Lemma 14">If there exists a path from{a mathematical formula}startto{a mathematical formula}goalandMMebegins an iteration with{a mathematical formula}C&gt;C⁎it will terminate immediately (i.e. without expanding a node on this iteration) and return{a mathematical formula}U=C⁎.</paragraph><paragraph label="Proof">By Theorem 7, {a mathematical formula}C&gt;C⁎ implies that all optimal solutions have been found, which implies (Lemma 13) {a mathematical formula}U=C⁎ so the termination criterion {a mathematical formula}U≤C in line 7 is satisfied (and it is tested before a node is expanded). □</paragraph><paragraph>The following establishes MMe's properties P1 and P2.</paragraph><paragraph label="Corollary 15">MMe's forward search never expands a node n with{a mathematical formula}fF(n)&gt;C⁎or{a mathematical formula}gF(n)&gt;{a mathematical formula}12{a mathematical formula}(C⁎−ϵ), andMMe's backward search never expands a node n with{a mathematical formula}fB(n)&gt;C⁎or{a mathematical formula}gB(n)&gt;{a mathematical formula}12{a mathematical formula}(C⁎−ϵ).</paragraph><paragraph label="Proof">If there does not exist a path from {a mathematical formula}start to {a mathematical formula}goal, {a mathematical formula}C⁎=∞ and nothing can be strictly larger than {a mathematical formula}C⁎. If there exists a path from {a mathematical formula}start to {a mathematical formula}goal, the proof for the forward search is as follows. The proof for the backward search is analogous. By Lemma 14, MMe's forward search never expands a node when {a mathematical formula}C&gt;C⁎, so if n was expanded in the forward search {a mathematical formula}prF(n)≤C⁎. Since {a mathematical formula}prF(n)=max⁡(fF(n),2gF(n)+ϵ) this means both {a mathematical formula}fF(n) and {a mathematical formula}2gF(n)+ϵ are less than or equal to {a mathematical formula}C⁎. □</paragraph><paragraph>The following establishes property P4.</paragraph><paragraph label="Theorem 16">IfMMe's heuristics are admissible and there exists a path from{a mathematical formula}startto{a mathematical formula}goalthenMMenever expands the same state in both search directions.</paragraph><paragraph label="Proof">Suppose (for the purpose of contradiction) that there is a state that is expanded in the forward direction and in the backward direction. Let n be the first state to be expanded in both directions. By Corollary 15, {a mathematical formula}gF(n)≤{a mathematical formula}12{a mathematical formula}(C⁎−ϵ) and {a mathematical formula}gB(n)≤{a mathematical formula}12{a mathematical formula}(C⁎−ϵ), and the path from {a mathematical formula}start to {a mathematical formula}goal via n would therefore cost {a mathematical formula}C⁎−ϵ or less. Because {a mathematical formula}C⁎ is finite and optimal, this implies {a mathematical formula}ϵ=0 and that {a mathematical formula}gF(n)=gB(n)={a mathematical formula}12C⁎, i.e. n is a state on an optimal solution path and {a mathematical formula}prF(n)=prB(n)=C⁎. On the iteration when n is about to be expanded for the second time (in direction {a mathematical formula}X∈{F,B}), {a mathematical formula}C=prX(n)=C⁎. We will prove in the next paragraph that U will be equal to {a mathematical formula}C⁎ before n is expanded for the second time. Thus, on the iteration when n is about to be expanded for the second time the test in line 7 ({a mathematical formula}U≤C) will succeed and MMe will terminate immediately, without expanding n for the second time. Therefore no state is expanded in both directions.To be expanded in both directions, n must first have been open, with its optimal g-value, in both directions. Suppose n first becomes open with its optimal g-value in the backward direction (an analogous argument holds if this happens first in the forward direction). There are two cases to consider:</paragraph><list><list-item>n is generated with its optimal{a mathematical formula}gF-value in the forward direction before it is expanded, with its optimal{a mathematical formula}gB-value, in the backward direction. In this case, the test in line 20 will succeed and U will be set to {a mathematical formula}gF(n)+gB(n)=C⁎.</list-item><list-item>n is not generated with its optimal{a mathematical formula}gF-value in the forward direction until after it is expanded in the backward direction with its optimal{a mathematical formula}gB-value. Let P be the path from {a mathematical formula}start to n that eventually results in n being added to {a mathematical formula}OpenF with its optimal {a mathematical formula}gF-value, and let p be the node on P that immediately precedes n, i.e. p is the node that is expanded in the forward direction, with its optimal {a mathematical formula}gF-value, to put n on {a mathematical formula}OpenF with its optimal {a mathematical formula}gF-value.{sup:22} There are two subcases to consider.</list-item></list><paragraph label="Lemma 17">IfMMe's heuristics are admissible and there exists a path from{a mathematical formula}startto{a mathematical formula}goalthen{a mathematical formula}OpenFand{a mathematical formula}OpenBare never empty.</paragraph><paragraph label="Proof">This is the proof for the forward direction. The proof for the backward direction is analogous. By Lemma 4, for {a mathematical formula}OpenF to be empty all states reachable from {a mathematical formula}start must be permanently closed in the forward direction. This is impossible because {a mathematical formula}goal is reachable from {a mathematical formula}start but, as we will now show, it will never be permanently closed in the forward direction.Suppose, for the sake of contradiction, that {a mathematical formula}goal becomes permanently closed in the forward direction on some iteration, t. This implies that {a mathematical formula}goal was added to {a mathematical formula}OpenF with {a mathematical formula}gF(goal)=C⁎ on some earlier iteration and, by Theorem 16, that {a mathematical formula}goal is never closed in the backward direction, i.e. that all search is in the forward direction. In particular, {a mathematical formula}goal∈OpenB on the iteration when {a mathematical formula}goal was added to {a mathematical formula}OpenF with {a mathematical formula}gF(goal)=C⁎ and therefore the test in line 20 would have succeeded and U would have been to set to {a mathematical formula}C⁎. At the beginning of iteration t we therefore would have {a mathematical formula}U=C⁎ and {a mathematical formula}C=prF(goal)=2C⁎+ϵ, so the test in line 7 ({a mathematical formula}U≤C) would have succeeded and MMe would have terminated immediately, without permanently closing {a mathematical formula}goal in the forward direction. □</paragraph><paragraph>The following establishes MMe's property P3.</paragraph><paragraph label="Theorem 18">If there exists a path from{a mathematical formula}startto{a mathematical formula}goalMMereturns{a mathematical formula}U=C⁎.</paragraph><paragraph label="Proof">Lemma 17 has shown that, if there is a path from {a mathematical formula}start to {a mathematical formula}goal, MMe will never terminate by {a mathematical formula}OpenF or {a mathematical formula}OpenB becoming empty, and MMe cannot terminate if {a mathematical formula}C&lt;C⁎, because U cannot be smaller than {a mathematical formula}C⁎. Therefore, MMe is certain to reach an iteration where {a mathematical formula}C≥C⁎. If MMe reaches an iteration where {a mathematical formula}C&gt;C⁎, Lemma 14 guarantees MMe will return {a mathematical formula}U=C⁎. The only reason it might not reach an iteration with {a mathematical formula}C&gt;C⁎ is that it might terminate on an iteration with {a mathematical formula}C=C⁎. If termination occurs on such an iteration then we have {a mathematical formula}U≤C=C⁎ and therefore {a mathematical formula}U=C⁎ is returned. □</paragraph></section><section label="A.3">MMe with consistent heuristics<paragraph>In this section we consider additional properties of MMe if its heuristics are consistent.</paragraph><paragraph>The following trivial lemma will be used in the proofs of Lemma 20, Lemma 24.</paragraph><paragraph label="Lemma 19">If{a mathematical formula}a1&gt;a2and{a mathematical formula}b1&gt;b2then{a mathematical formula}max⁡(a1,b1)&gt;max⁡(a2,b2).</paragraph><paragraph label="Proof">Suppose {a mathematical formula}max⁡(a1,b1)=a1. Then {a mathematical formula}a1≥b1&gt;b2. In addition, {a mathematical formula}a1&gt;a2 is a premise of the lemma. Together these imply {a mathematical formula}a1&gt;max⁡(a2,b2). Combining this with the symmetric argument when {a mathematical formula}max⁡(a1,b1)=b1 we have proven the lemma. □</paragraph><paragraph label="Lemma 20">IfMMe's heuristics are consistent and node c was added to{a mathematical formula}OpenFas the result of expanding node n, then{a mathematical formula}prF(c)≥prF(n). Likewise ifMMe's heuristics are consistent and node c was added to{a mathematical formula}OpenBas the result of expanding node n, then{a mathematical formula}prB(c)≥prB(n).</paragraph><paragraph label="Proof">This proof is for the forward search, the proof for the backward search is analogous. {a mathematical formula}fF(c)≥fF(n) because the heuristic is consistent, and {a mathematical formula}gF(c)=gF(n)+cost(n,c)≥gF(n) because edge costs are non-negative. Therefore, by Lemma 19, {a mathematical formula}prF(c)=max⁡(fF(c),2gF(c)+ϵ)≥prF(n)=max⁡(fF(n),2gF(n)+ϵ). □</paragraph><paragraph>The proof of Lemma 3 requires the ability to re-open closed nodes, and virtually all the results of the previous section depend on that lemma. With consistent heuristics we wish to remove the re-opening of closed nodes from the algorithm, so we now must re-prove the equivalent of Lemma 3 without re-opening closed nodes.</paragraph><paragraph label="Lemma 21">IfMMe's heuristics are consistent then C never decreases from one iteration ({a mathematical formula}n−1≥1) ofMMeto the next (n).</paragraph><paragraph label="Proof">Let {a mathematical formula}n≥2 be any iteration that MMe executed beyond line 8 in solving a given problem, and let {a mathematical formula}si denote the node chosen for expansion on iteration {a mathematical formula}i≤n and X the search direction (forward or backward) used for expanding {a mathematical formula}si, i.e. on iteration i{a mathematical formula}si was moved from {a mathematical formula}OpenX to {a mathematical formula}ClosedX and its children added to {a mathematical formula}OpenX with no changes being made to the open and closed lists in the other direction. Finally, let {a mathematical formula}Ci=prX(si) be MMe's C value as set in line 6 on iteration i.If {a mathematical formula}sn≠start and {a mathematical formula}sn≠goal then {a mathematical formula}sn was added to {a mathematical formula}OpenX with priority {a mathematical formula}prX(sn) on some previous iteration. Let {a mathematical formula}p&lt;n (p for “parent”) be the iteration that most recently added {a mathematical formula}sn to {a mathematical formula}OpenX with priority {a mathematical formula}prX(sn). If {a mathematical formula}p=n−1 then {a mathematical formula}Cn=prX(sn)≥prX(sn−1)=Cn−1 follows directly from Lemma 20. If {a mathematical formula}p&lt;n−1, then {a mathematical formula}sn has been on {a mathematical formula}OpenX with its current {a mathematical formula}prX-value ever since iteration {a mathematical formula}p+1, so it has been available for expansion, but not selected, on all iterations from {a mathematical formula}p+1 up to and including {a mathematical formula}n−1. In particular, it was on {a mathematical formula}OpenX with its current {a mathematical formula}prX-value in the most recent iteration {a mathematical formula}n−1, where MMe chose to expand a different node {a mathematical formula}sn−1 in a possibly different direction Y, instead of expanding {a mathematical formula}sn in direction X. Since MMe chooses a node with the smallest priority on either open list {a mathematical formula}Cn=prX(sn)≥Cn−1=prY(sn−1).Now consider {a mathematical formula}start and {a mathematical formula}goal. Before the first iteration begins {a mathematical formula}OpenF is initialized to contain {a mathematical formula}start and {a mathematical formula}OpenB is initialized to contain {a mathematical formula}goal. Because {a mathematical formula}gF(start)=gB(goal)=0, once these are expanded they will never be added to the open list in that direction again, since 0 is the shortest possible path to them. One of these was expanded on MMe's first iteration ({a mathematical formula}n=1). Suppose it was {a mathematical formula}start (analogous reasoning applies if {a mathematical formula}goal was expanded on the first iteration). If {a mathematical formula}goal was never expanded then our proof is complete since it plays no role in determining a C value for any of MMe's iterations. If {a mathematical formula}goal was first expanded in the backwards direction on the very next iteration ({a mathematical formula}n=2) then, because MMe chooses the node with the smallest priority on either open list we must have {a mathematical formula}C2=prB(goal)≥prF(start)=C1. If {a mathematical formula}goal was first expanded in the backwards direction on a subsequent iteration, {a mathematical formula}n&gt;2, then similar reasoning to {a mathematical formula}p&lt;n−1 case (above) applies, as follows. {a mathematical formula}goal has been on {a mathematical formula}OpenB with its current {a mathematical formula}prB value ever since the first iteration ({a mathematical formula}n=1), so it has been available for expansion, but not selected, on all iterations up to and including {a mathematical formula}n−1. In particular, it was on {a mathematical formula}OpenB with its initial {a mathematical formula}prB value in the most recent iteration {a mathematical formula}n−1, where MMe chose to expand a different node {a mathematical formula}sn−1 in a possibly different direction Y, instead of expanding {a mathematical formula}goal in the backwards direction. Since MMe chooses a node with the smallest priority on either open list {a mathematical formula}Cn=prB(goal)≥Cn−1=prY(sn−1). □</paragraph><paragraph label="Lemma 22">SupposeMMe's heuristics are consistent. Let{a mathematical formula}P=s0,s1,…snbe an optimal path from{a mathematical formula}start(s0)to any state{a mathematical formula}sn. If{a mathematical formula}snis not permanently closed in the forward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}sn−1is permanently closed in the forward direction, then on no iteration is{a mathematical formula}sn∈ClosedFwith{a mathematical formula}gF(sn)&gt;d(start,sn). Analogously, let{a mathematical formula}P=s0,s1,…snbe an optimal path from any state{a mathematical formula}s0to{a mathematical formula}goal=sn. If{a mathematical formula}s0is not permanently closed in the backward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}s1is permanently closed in the backward direction, then on no iteration is{a mathematical formula}s0∈ClosedBwith{a mathematical formula}gB(s0)&gt;d(s0,goal).</paragraph><paragraph label="Proof">This proof is for the forward search, the proof for the backward search is analogous. If {a mathematical formula}n=0, {a mathematical formula}s0=start has not been closed in the forward direction and the lemma is true because {a mathematical formula}ClosedF is initially empty and remains so until {a mathematical formula}start is closed, permanently, in the forward direction.Suppose {a mathematical formula}n&gt;0 and let {a mathematical formula}t1 be the iteration on which {a mathematical formula}sn−1 was expanded to become permanently closed in the forward direction. The value of {a mathematical formula}C=Ct1 during that iteration was {a mathematical formula}prFt1(sn−1). If {a mathematical formula}sn was added to {a mathematical formula}OpenF as a result of expanding {a mathematical formula}sn−1 on iteration {a mathematical formula}t1 then its priority {a mathematical formula}prFt1(sn)≥prFt1(sn−1) (by Lemma 20). If {a mathematical formula}sn was not been added to {a mathematical formula}OpenF as a result of expanding {a mathematical formula}sn−1 on iteration {a mathematical formula}t1, it must have already been on {a mathematical formula}OpenF with its optimal {a mathematical formula}gF(sn) value, and therefore have the same priority that it would have had if it had been added as a result of expanding {a mathematical formula}sn−1 on iteration {a mathematical formula}t1. In either case {a mathematical formula}prFt1(sn)≥prFt1(sn−1). If {a mathematical formula}sn had been on {a mathematical formula}ClosedF with a suboptimal {a mathematical formula}gF-value prior to iteration {a mathematical formula}t1 it must have been expanded on an earlier iteration {a mathematical formula}t0&lt;t1. The value of C on that iteration was {a mathematical formula}Ct0=prFt0(sn). Because {a mathematical formula}sn had only been reached via a suboptimal path, {a mathematical formula}prFt0(sn) (i.e. {a mathematical formula}Ct0) would be strictly greater than {a mathematical formula}prFt1(sn). Hence {a mathematical formula}Ct0&gt;Ct1 even though {a mathematical formula}t0&lt;t1, contradicting Lemma 21. So {a mathematical formula}sn cannot have been on {a mathematical formula}ClosedF with a suboptimal {a mathematical formula}gF-value prior to iteration {a mathematical formula}t1. On iteration {a mathematical formula}t1 it was added to {a mathematical formula}OpenF (if it was not already there) with an optimal {a mathematical formula}gF-value, so it will never subsequently be added to {a mathematical formula}OpenF with a suboptimal {a mathematical formula}gF-value, hence it will never subsequently be on {a mathematical formula}ClosedF with a suboptimal {a mathematical formula}gF-value. □</paragraph><paragraph>The following is the equivalent of Lemma 3 when MMe has consistent heuristics but does not have the ability to re-open closed nodes.{sup:23}</paragraph><paragraph label="Corollary 23">SupposeMMe's heuristics are consistent andMMedoes not re-open closed nodes. Let{a mathematical formula}P=s0,s1,…snbe an optimal path from{a mathematical formula}start(s0)to any state{a mathematical formula}sn. If{a mathematical formula}snis not permanently closed in the forward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}sn−1is permanently closed in the forward direction, then{a mathematical formula}sn∈OpenFand{a mathematical formula}gF(sn)=d(start,sn). Analogously, let{a mathematical formula}P=s0,s1,…snbe an optimal path from any state{a mathematical formula}s0to{a mathematical formula}goal=sn. If{a mathematical formula}s0is not permanently closed in the backward direction and either{a mathematical formula}n=0or{a mathematical formula}n&gt;0and{a mathematical formula}s1is permanently closed in the backward direction, then{a mathematical formula}s0∈OpenBand{a mathematical formula}gB(s0)=d(s0,goal).</paragraph><paragraph label="Proof">The proof of Lemma 3 applies directly since, by Lemma 22, there is no need to test if a newly generated node is on Closed with a suboptimal value. □</paragraph><paragraph>Since the proofs of the other lemmas and theorems in the previous section are all based on the conclusion of Lemma 3, because of Corollary 23 they continue to hold when MMe has consistent heuristics but does not have the ability to re-open closed nodes.</paragraph><paragraph label="Lemma 24">IfMMe's heuristics are consistent andMMedoes not re-open closed nodes, then whenMMeexpands a node its g-value is optimal.</paragraph><paragraph label="Proof">This is the proof for nodes expanded in MMe's forward search. The proof for its backward search is analogous. Suppose node n has just been added to {a mathematical formula}OpenF (line 19) with a suboptimal cost c, i.e. {a mathematical formula}n∈OpenF with {a mathematical formula}gF(n)=c&gt;d(start,n). Let P be an optimal path from {a mathematical formula}start to n. Since {a mathematical formula}n∉ClosedF, P satisfies the conditions of Lemma 4 and there exists a node {a mathematical formula}m=nF∈OpenF on P with {a mathematical formula}gF(m)=d(start,m). To prove the lemma, all that we need to show is that m will be expanded before n, i.e. that {a mathematical formula}prF(m)&lt;prF(n). By definition, {a mathematical formula}prF(m)=max⁡(d(start,m)+hF(m),2d(start,m)+ϵ) and {a mathematical formula}prF(n)=max⁡(c+hF(n),2c+ϵ). By Lemma 19, to show that {a mathematical formula}prF(m)&lt;prF(n) it suffices to show that {a mathematical formula}d(start,m)+hF(m)&lt;c+hF(n) and that {a mathematical formula}d(start,m)&lt;c. The latter follows because edge costs are non-negative, so {a mathematical formula}d(start,m)≤d(start,n)&lt;c. The former follows because the heuristic {a mathematical formula}hF is consistent, i.e. {a mathematical formula}hF(m)≤d(m,n)+hF(n). This implies {a mathematical formula}d(start,m)+hF(m)≤d(start,m)+d(m,n)+hF(n)=d(start,n)+hF(n)&lt;c+hF(n). □</paragraph><paragraph label="Theorem 25">SupposeMMe's heuristics are consistent andMMedoes not re-open closed nodes. If there exists a path from{a mathematical formula}startto{a mathematical formula}goalthenMMenever expands a state twice.</paragraph><paragraph label="Proof">MMe will not expand a state twice in the same search direction because Lemma 24 guarantees that the first time a state becomes closed it becomes permanently closed. The only remaining possibility for a state to be expanded twice is that it is expanded once in the forward direction and once in the backward direction. Theorem 16 shows that this cannot happen. □</paragraph></section><section label="A.4"><section-title>Using a stronger stopping condition</section-title><paragraph>We will now show that MMe maintains its four key properties (P1–P4) if it stops as soon as any of the following conditions is true:</paragraph><list><list-item label="1.">{a mathematical formula}U≤C (the stopping condition used above)</list-item><list-item label="2.">{a mathematical formula}U≤fminF</list-item><list-item label="3.">{a mathematical formula}U≤fminB</list-item><list-item label="4.">{a mathematical formula}U≤gminF+gminB+ϵ</list-item></list><paragraph> i.e. {a mathematical formula}U≤max⁡(C,fminF,fminB,gminF+gminB+ϵ).</paragraph><paragraph>P3 continues to hold because {a mathematical formula}fminF,fminB, and {a mathematical formula}gminF+gminB+ϵ are all lower bounds on the cost of any solution that might be found by continuing to search. The other properties continue to hold when MMe uses the stronger stopping condition because with a stronger stopping condition MMe will execute a subset of the iterations it executed with the stopping condition used to prove MMe's properties. Since those properties were true of every iteration done with MMe's original stopping condition, they are true of every iteration done with the stronger stopping condition.</paragraph></section></section></appendices><references><reference label="[1]"><authors>Kazi Shamsul Arefin,Aloke Kumar Saha</authors><title>A new approach of iterative deepening bi-directional heuristic front-to-front algorithm (IDBHFFA)</title><host>Int. J. Electr. Comput. Sci. (IJECS-IJENS)10 (2)(2010)</host></reference><reference label="[2]"><authors>Andreas Auer,Hermann Kaindl</authors><title>A case study of revisiting best-first vs. depth-first search</title><host>Proc. 16th European Conference on Artificial IntelligenceECAI(2004) pp.141-145</host></reference><reference label="[3]"><authors>Joseph K. Barker,Richard E. Korf</authors><title>Solving peg solitaire with bidirectional BFIDA*</title><host>Proc. 26th AAAI Conference on Artificial Intelligence(2012) pp.420-426</host></reference><reference label="[4]"><authors>Joseph Kelly Barker,Richard E. Korf</authors><title>Limitations of front-to-end bidirectional heuristic search</title><host>Proc. 29th AAAI Conference on Artificial Intelligence(2015) pp.1086-1092</host></reference><reference label="[5]">Claude Berge,Alain Ghouila-HouriProgramming, Games and Transportation Networks(1965)Methuenoriginally published in French in 1962</reference><reference label="[6]"><authors>Yi-Jen Chiang,Michael T. Goodrich,Edward F. Grove,Roberto Tamassia,Darren Erik Vengroff,Jeffrey Scott Vitter</authors><title>External-memory graph algorithms</title><host>SODAvol. 95 (1995) pp.139-149</host></reference><reference label="[7]"><authors>Joseph Culberson,Jonathan Schaeffer</authors><title>Pattern databases</title><host>Comput. Intell.14 (3)(1998) pp.318-334</host></reference><reference label="[8]"><authors>Henry W. Davis,Randy B. Pollack,Thomas Sudkamp</authors><title>Towards a better understanding of bidirectional search</title><host>Proc. National Conference on Artificial IntelligenceAAAI(1984) pp.68-72</host></reference><reference label="[9]"><authors>Dennis de Champeaux</authors><title>Bidirectional heuristic search again</title><host>J. ACM30 (1)(1983) pp.22-32</host></reference><reference label="[10]"><authors>Dennis de Champeaux,Lenie Sint</authors><title>An improved bi-directional heuristic search algorithm</title><host>Fourth International Joint Conference on Artificial Intelligence(1975) pp.309-314</host></reference><reference label="[11]"><authors>Dennis de Champeaux,Lenie Sint</authors><title>An improved bidirectional heuristic search algorithm</title><host>J. ACM24 (2)(1977) pp.177-191</host></reference><reference label="[12]"><authors>E.W. Dijkstra</authors><title>A note on two problems in connexion with graphs</title><host>Numer. Math.1 (1959) pp.269-271</host></reference><reference label="[13]"><authors>John F. Dillenburg,Peter C. Nelson</authors><title>Perimeter search</title><host>Artif. Intell.65 (1)(1994) pp.165-178</host></reference><reference label="[14]">James E. DoranDouble Tree Searching and the Graph TraverserTechnical Report Research Memo EPU-R-22<host>(1966)Dept. of Machine Intelligence and Perception, University of Edinburgh</host></reference><reference label="[15]"><authors>James E. Doran,D. Michie</authors><title>Experiments with the Graph Traverser Program</title><host>Proc. of the Royal Society, Ser. Avol. 294 (1966) pp.235-259</host></reference><reference label="[16]">Stuart E. DreyfusAn Appraisal of Some Shortest Path AlgorithmsTechnical Report RM-5433-PR<host>(October 1967)RAND CorporationSanta Monica, California</host></reference><reference label="[17]"><authors>Jürgen Eckerle</authors><title>An optimal bidirectional search algorithm</title><host>Proc. KI-94: Advances in Artificial Intelligence, 18th Annual German Conference on Artificial Intelligence(1994) pp.394-</host></reference><reference label="[18]"><authors>Jürgen Eckerle,Thomas Ottmann</authors><title>An efficient data structure for bidirectional heuristic search</title><host>ECAI(1994) pp.600-604</host></reference><reference label="[19]"><authors>Ariel Felner,Carsten Moldenhauer,Nathan R. Sturtevant,Jonathan Schaeffer</authors><title>Single-frontier bidirectional search</title><host>Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence(2010)</host></reference><reference label="[20]"><authors>Ariel Felner,Uzi Zahavi,Robert Holte,Jonathan Schaeffer,Nathan R. Sturtevant,Zhifu Zhang</authors><title>Inconsistent heuristics in theory and practice</title><host>Artif. Intell.175 (9–10)(2011) pp.1570-1603</host></reference><reference label="[21]"><authors>Andrew V. Goldberg,Chris Harrelson</authors><title>Computing the shortest path: A* search meets graph theory</title><host>Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete AlgorithmsSODA'05(2005) pp.156-165</host></reference><reference label="[22]"><authors>Patrick A.V. Hall</authors><title>Branch-and-bound and beyond</title><host>Proceedings of the 2nd International Joint Conference on Artificial IntelligenceIJCAI'71(1971) pp.641-650</host></reference><reference label="[23]"><authors>Peter E. Hart,Nils J. Nilsson,Bertram Raphael</authors><title>A formal basis for the heuristic determination of minimum cost paths</title><host>IEEE Trans. Syst. Sci. Cybern.4 (2)(1968) pp.100-107</host></reference><reference label="[24]"><authors>Peter E. Hart,Nils J. Nilsson,Bertram Raphael</authors><title>Correction to “A formal basis for the heuristic determination of minimum cost paths”</title><host>SIGART Newsl.37 (1972) pp.28-29</host></reference><reference label="[25]"><authors>Matthew Hatem,Ethan Burns,Wheeler Ruml</authors><title>Heuristic search for large problems with real costs</title><host>Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence(2011) pp.30-35</host></reference><reference label="[26]"><authors>Richard V. Helgason,Jeffery L. Kennington,B. Douglas Stewart</authors><title>The one-to-one shortest-path problem: an empirical analysis with the two-tree Dijkstra algorithm</title><host>Comput. Optim. Appl.2 (1)(1993) pp.47-75</host></reference><reference label="[27]"><authors>Malte Helmert</authors><title>Landmark heuristics for the pancake problem</title><host>Proc. 3rd Annual Symposium on Combinatorial SearchSoCS(2010)</host></reference><reference label="[28]"><authors>Malte Helmert,Gabriele Röger</authors><title>How good is almost perfect?</title><host>Proc. 23rd AAAI Conference on Artificial Intelligence(2008) pp.944-949</host></reference><reference label="[29]"><authors>Robert C. Holte,Ariel Felner,Guni Sharon,Nathan R. Sturtevant</authors><title>Bidirectional search that is guaranteed to meet in the middle</title><host>Proceedings of the AAAI Conference on Artificial Intelligence(2016)</host></reference><reference label="[30]"><authors>Takahiro Ikeda,Min-Yao Hsu,Hiroshi Imai,Shigeki Nishimura,Hiroshi Shimoura,Takeo Hashimoto,Kenji Tenmoku,Kunihiko Mitoh</authors><title>A fast algorithm for finding better routes by AI search techniques</title><host>Proc. Vehicle Navigation and Information Systems Conference(1994) pp.291-296</host></reference><reference label="[31]"><authors>Marcelo Johann,Andrew Caldwell,Andrew Kahng,Ricardo Reis</authors><title>A new bidirectional heuristic shortest path search algorithm</title><host>Symposium on Computational Intelligence in the International ICSC Congress on Intelligent Systems and Applications(2000)</host></reference><reference label="[32]"><authors>Hermann Kaindl,Gerhard Kainz</authors><title>Bidirectional heuristic search reconsidered</title><host>J. Artif. Intell. Res. (JAIR)7 (1997) pp.283-317</host></reference><reference label="[33]"><authors>Hermann Kaindl,Gerhard Kainz,Roland Steiner,Andreas Auer,Klaus Radda</authors><title>Switching from bidirectional to unidirectional search</title><host>Proc. 16th International Joint Conference on Artificial IntelligenceIJCAI(1999) pp.1178-1183</host></reference><reference label="[34]"><authors>Richard E. Korf</authors><title>Finding optimal solutions to Rubik's Cube using pattern databases</title><host>Proc. 14th National Conference on Artificial IntelligenceAAAI(1997) pp.700-705</host></reference><reference label="[35]"><authors>Richard E. Korf</authors><title>Best-first frontier search with delayed duplicate detection</title><host>Proc. 19th National Conference on Artificial IntelligenceAAAI(2004) pp.650-657</host></reference><reference label="[36]"><authors>Richard E. Korf,Peter Schultze</authors><title>Large-scale parallel breadth-first search</title><host>Proc. 20th National Conference on Artificial IntelligenceAAAI(2005) pp.1380-1385</host></reference><reference label="[37]"><authors>Richard E. Korf,Weixiong Zhang,Ignacio Thayer,Heath Hohwald</authors><title>Frontier search</title><host>J. ACM52 (5)(2005) pp.715-748</host></reference><reference label="[38]"><authors>Robert A. Kowalski</authors><title>AND/OR graphs, theorem-proving graphs, and bidirectional search</title><host>B. MeltzerD. MichieMach. Intell.vol. 7 (1972)Edinburgh University Press pp.167-194</host></reference><reference label="[39]"><authors>Daniel Kunkle,Gene Cooperman</authors><title>Solving Rubik's Cube: disk is the new RAM</title><host>Commun. ACM51 (4)(2008) pp.31-33</host></reference><reference label="[40]"><authors>James B.H. Kwa</authors><title>BS*: an admissible bidirectional staged heuristic search algorithm</title><host>Artif. Intell.38 (1)(1989) pp.95-109</host></reference><reference label="[41]"><authors>Carlos Linares López,Andreas Junghanns</authors><title>Perimeter search performance</title><host>Proc. 3rd International Conference on Computers and GamesCG(2002) pp.345-359</host></reference><reference label="[42]"><authors>Marco Lippi,Marco Ernandes,Ariel Felner</authors><title>Efficient single frontier bidirectional search</title><host>Proceedings of the Fifth Annual Symposium on Combinatorial SearchSoCS(2012)</host></reference><reference label="[43]"><authors>Michael Luby,Prabhakar Ragde</authors><title>A bidirectional shortest-path algorithm with good average-case behavior</title><host>Algorithmica4 (1)(1989) pp.551-567</host></reference><reference label="[44]"><authors>Giovanni Manzini</authors><title>BIDA*: an improved perimeter search algorithm</title><host>Artif. Intell.75 (2)(1995) pp.347-360</host></reference><reference label="[45]"><authors>Alberto Martelli</authors><title>On the complexity of admissible search algorithms</title><host>Artif. Intell.8 (1)(1977) pp.1-13</host></reference><reference label="[46]"><authors>Th. Mohr,C. Pasche</authors><title>A parallel shortest path algorithm</title><host>Computing40 (4)(1988) pp.281-292</host></reference><reference label="[47]"><authors>T.A.J. Nicholson</authors><title>Finding the shortest route between two points in a network</title><host>Comput. J.9 (3)(1966) pp.275-280</host></reference><reference label="[48]"><authors>Robert Niewiadomski,José Nelson Amaral,Robert C. Holte</authors><title>A parallel external-memory frontier breadth-first traversal algorithm for clusters of workstations</title><host>International Conference on Parallel Processing (ICPP)(2006) pp.531-538</host></reference><reference label="[49]"><authors>Nils J. Nilsson</authors><title>Principles of Artificial Intelligence</title><host>(1980)Tioga Press</host></reference><reference label="[50]"><authors>Judea Pearl</authors><title>Heuristics – Intelligent Search Strategies for Computer Problem Solving</title><host>(1984)Addison-Wesley</host></reference><reference label="[51]"><authors>Wim Pijls,Henk Post</authors><title>A new bidirectional algorithm for shortest paths</title><host>Eur. J. Oper. Res.198 (2009) pp.363-369</host></reference><reference label="[52]"><authors>Wim Pijls,Henk Post</authors><title>Note on “A new bidirectional algorithm for shortest paths”</title><host>Eur. J. Oper. Res.207 (2)(2010) pp.1140-1141</host></reference><reference label="[53]">Ira PohlBi-Directional and Heuristic Search in Path ProblemsTechnical Report 104<host>(1969)Stanford Linear Accelerator Center</host></reference><reference label="[54]"><authors>George Politowski,Ira Pohl</authors><title>D-node retargeting in bidirectional heuristic search</title><host>Proc. National Conference on Artificial IntelligenceAAAI(1984) pp.274-277</host></reference><reference label="[55]"><authors>Francisco Javier Pulido,Lawrence Mandow,José-Luis Pérez de la Cruz</authors><title>A two-phase bidirectional heuristic search algorithm</title><host>Proc. 6th Starting AI Researchers SymposiumSTAIRS(2012) pp.240-251</host></reference><reference label="[56]"><authors>Alexander Reinefeld,Volker Schnecke</authors><title>AIDA*-asynchronous parallel IDA*</title><host>Proceedings of the Biennial Conference-Canadian Society for Computational Studies of Intelligence(1994)Canadian Information Processing Society pp.295-302</host></reference><reference label="[57]"><authors>Tomas Rokicki,Herbert Kociemba,Morley Davidson,John Dethridge</authors><title>The diameter of the Rubik's Cube group is twenty</title><host>SIAM J. Discrete Math.27 (2)(2013) pp.1082-1105</host></reference><reference label="[58]"><authors>Samir K. Sadhukhan</authors><title>A new approach to bidirectional heuristic search using error functions</title><host>Proc. 1st International Conference on Intelligent Infrastructure at the 47th Annual National Convention COMPUTER SOCIETY of INDIACSI-2012(2012)</host></reference><reference label="[59]"><authors>Guni Sharon,Robert Holte,Nathan Sturtevant,Ariel Felner</authors><title>An improved priority function for MM</title><host>SoCS(2016)</host></reference><reference label="[60]"><authors>N. Sturtevant</authors><title>Benchmarks for grid-based pathfinding</title><host>IEEE Trans. Comput. Intell. AI Games4 (2)(2012) pp.144-148</host></reference><reference label="[61]"><authors>Nathan Sturtevant,Jingwei Chen</authors><title>External memory bidirectional search</title><host>International Joint Conference on Artificial IntelligenceIJCAI(2016)</host></reference><reference label="[62]"><authors>Nathan R. Sturtevant,Ariel Felner,Malte Helmert</authors><title>Value compression of pattern databases</title><host>AAAI Conference on Artificial Intelligence(2017)</host></reference><reference label="[63]"><authors>Nathan R. Sturtevant,Matthew J. Rutherford</authors><title>Minimizing writes in parallel external memory search</title><host>Proc. 23rd International Joint Conference on Artificial IntelligenceIJCAI(2013)</host></reference><reference label="[64]"><authors>Christopher Makoto Wilt,Wheeler Ruml</authors><title>Robust bidirectional search via heuristic improvement</title><host>Proc. 27th AAAI Conference on Artificial Intelligence(2013) pp.954-961</host></reference></references><footnote><note-para label="1">Meeting in the middle, according to our definition, is not so much about where the searches meet; it is about how far they venture from their starting point.</note-para><note-para label="2">Barker and Korf's theory was further discussed in our conference paper [29] where we compared it to our new theoretical findings. The interested reader is referred to that paper.</note-para><note-para label="3">Personal communication, Joseph Barker to R. Holte, July 3, 2015.</note-para><note-para label="4">A heuristic h is consistent if for any two states, n and m, {a mathematical formula}h(n)≤d(n,m)+h(m) where {a mathematical formula}d(n,m) is the cost of a least-cost path from n to m.</note-para><note-para label="5">We define “forward” to be the direction in which Uni-HS searches.</note-para><note-para label="6">Pohl (p. 13 [53]) gives a full description of this stopping condition and credits it to Dreyfus, citing the August 1967 version of Dreyfus's unpublished technical report, but it is not present in the October 1967 version [16], the earliest version we have been able to obtain.</note-para><note-para label="7">This had incorrectly be used as a stopping condition by Berge [5].</note-para><note-para label="8">It is not clear if de Champeaux and Sint [10], [11] were using “meet in the middle” in the sense we have defined or in some other sense.</note-para><note-para label="9">The justification of this formula is non-trivial. It is fully explained in the original paper [30].</note-para><note-para label="10">MM is used to denote both the entire family of algorithms using some variant of the priority function, stopping conditions, and tie-breaking described here as well as the specific basic variant. This is similar to A* which is used to denote both a general family of algorithms as well as the basic implementation.</note-para><note-para label="11">Additional stopping conditions may improve MM's running time but they do not affect these properties (see Section A.4 in Appendix A).</note-para><note-para label="12">Lemma 17 proves that {a mathematical formula}goal will never be closed in the forward direction and {a mathematical formula}start will never be closed in the backward direction, so i and j are both always legal indexes of nodes in P.</note-para><note-para label="13">Any lower bound on {a mathematical formula}d(si,sj) can be used where we are using ϵ. For example, if {a mathematical formula}ϵF(n) ({a mathematical formula}ϵB(n)) is the cost of the cheapest forward (reverse) operator applicable to n then {a mathematical formula}ϵF(si)≤d(si,sj) ({a mathematical formula}ϵB(sj)≤d(si,sj)) and can be used here instead of ϵ. Similarly, if {a mathematical formula}δ(x,y) is a lower bound on the number of edges between x and y, then {a mathematical formula}miny∈OpenB⁡ϵδ(si,y) can be used here instead of ϵ.</note-para><note-para label="14">In fact, our analysis applies to any Bi-HS that meets in the middle.</note-para><note-para label="15">Technically, it is not correct to cancel the occurrences of FN on either side of the inequality because FN is being searched in the forward direction by Uni-HS but in the backward direction by MM0. This can result in very different sets of nodes being expanded in FN. But just as we have ignored the difference between N and N′ and between F and F′ in order to identify core differences, here we are ignoring this difference.</note-para><note-para label="16">We recognize the imprecision in terms like “very accurate”, “inaccurate”, etc. We use these qualitative gradations to highlight that as the heuristic's accuracy increases or decreases, the advantage shifts from one algorithm to another. This is discussed in Section 6.7.</note-para><note-para label="17">In this section, “MM” refers to a specific implementation of basic MM, not to the general MM family.</note-para><note-para label="18">A gap occurs when two adjacent tokens in the current state are not consecutive numbers. The GAP heuristic counts the number of gaps in the entire state.</note-para><note-para label="19">Parity between the corners means that the last corner orientation is fixed given the rotation of the remaining corners.</note-para><note-para label="20">MM is a special case of MMe for {a mathematical formula}ϵ=0, so anything said about MMe also holds for MM.</note-para><note-para label="21">The observation that the proof only requires that there be no negative-cost cycles, as opposed to requiring all edge costs to be non-negative, is due to Gaojian Fan (University of Alberta).</note-para><note-para label="22">p is guaranteed to exist because n first becomes open with its optimal g-value in the backward direction and therefore {a mathematical formula}n≠start.</note-para><note-para label="23">The changes to Algorithm 1 are: (a) “{a mathematical formula}∪ClosedF” is removed from lines 16 and 17, and (b) the test in line 14 is changed to “{a mathematical formula}(c∈ClosedF) or {a mathematical formula}(c∈OpenF and gF(c)≤gF(n)+cost(n,c))”.</note-para></footnote></root>