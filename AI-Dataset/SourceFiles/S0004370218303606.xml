<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370218303606</url><title>Multi-robot inverse reinforcement learning under occlusion with estimation of state transitions</title><authors>Kenneth Bogert,Prashant Doshi</authors><abstract>Inverse reinforcement learning (IRL), analogously to RL, refers to both the problem and associated methods by which an agent passively observing another agent's actions over time, seeks to learn the latter's reward function. The learning agent is typically called the learner while the observed agent is often an expert in popular applications such as in learning from demonstrations. Some of the assumptions that underlie current IRL methods are impractical for many robotic applications. Specifically, they assume that the learner has full observability of the expert as it performs its task; that the learner has full knowledge of the expert's dynamics; and that there is always only one expert agent in the environment. For example, these assumptions are particularly restrictive in our application scenario where a subject robot is tasked with penetrating a perimeter patrol by two other robots after observing them from a vantage point. In our instance of this problem, the learner can observe at most 10% of the patrol. We relax these assumptions and systematically generalize a known IRL method, Maximum Entropy IRL, to enable the subject to learn the preferences of the patrolling robots, subsequently their behaviors, and predict their future positions well enough to plan a route to its goal state without being spotted. Challenged by occlusion, multiple interacting robots, and partially known dynamics we demonstrate empirically that the generalization improves significantly on several baselines in its ability to inversely learn in this application setting. Of note, it leads to significant improvement in the learner's overall success rate of penetrating the patrols. Our methods represent significant steps towards making IRL pragmatic and applicable to real-world contexts.</abstract><keywords>Inverse reinforcement learning;Robotics;Machine learning;Maximum entropy</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>We consider the problem of learning the behavior of multiple mobile robots executing cyclic trajectories in a common narrow space and possibly interacting with each other in their environment. A subject robot is tasked with learning their behaviors from passive observations in order to penetrate the patrol, as illustrated in Fig. 1. However, to avoid being detected the subject observes from a hidden vantage point from which it is able to observe a portion of their trajectories only. Consequently, the subject robot does not precisely know the trajectories of the patrollers (it does not know where each patroller turns around), nor its navigation and path planning capabilities, which further complicates the task. This problem has wide-ranging applications in robotics including forming an ad hoc team by coordinating with the other robots, executing a follow-the-leader behavior, and penetrating a simple perimeter patrol in surveillance and combat applications.</paragraph><paragraph>Reinforcement learning and planning for the subject robot may at first seem viable as solution approaches. However, both are infeasible here because very little is known about the patrollers' behaviors as mentioned above and observations are sparse due to occlusion. More specifically, in situ simulations are precluded due to the non-cooperative nature of the domain, and a faithful simulation of the environment is not possible as we are unaware of the patrollers' preferences, and consequently, how they patrol. Simple approximations of their behaviors, which would permit tractable planning for the subject robot, such as attributing a probability distribution to each patrollers' actions, are obviously inadequate in modeling the patrollers' trajectories. To illustrate, a patroller in Fig. 1 simply moves forward for large portions of its patrol. A fixed distribution that allocates some probability to turning is highly unlikely to model these portions of the patroller's behavior correctly. Due to this they cannot be immediately simulated or modeled (say, as noise) in the environment.</paragraph><paragraph>Inverse reinforcement learning (IRL) refers to both the problem and associated methods by which an agent passively observing another agent's actions over time, seeks to learn the latter's reward function [33]. It inverts RL with its focus on learning the reward function given information about optimal action trajectories. Usually, the other agent is assumed to be an expert in the task they are performing, having settled on an optimal policy before the subject begins observations.</paragraph><paragraph>IRL is applicable to the patrolling scenario as the task at hand is to passively learn the patrollers' preferences by observing their state-action trajectories over time. Learning the reward functions of the patrollers and how they interact offers a robust way of inferring the patrollers' paths. While paths may change slightly between runs due to motion noise, different start states, and interactions, the robots' preferences on the other hand are fixed. Furthermore, the reward function allows us to obtain the patrollers' policies, which is more useful in the learner's decision making than the realized trajectories because a policy informs the learner about the patroller's action from each state.</paragraph><paragraph>Formally, the expert is modeled as following an optimal policy resulting from solving a Markov decision process (MDP), whose reward function is unknown to the learner; other parameters of the MDP are assumed known to the learner. Furthermore, the expert uses its sensors to perfectly observe its location. The space of possible reward functions under which the demonstrated behavior is optimal is infinite. Therefore, to gain some tractability, it is common for the learner to assume that the reward function is a weighted linear combination of binary feature functions, which are known to the learner. This reduces the IRL problem to finding the associated weights that complete the MDP such that the demonstrated policy is optimal.</paragraph><paragraph>IRL therefore provides an appealing method to learn the behaviors of mobile robots from observations of their movement over time. A key challenge is that IRL is an inherently degenerate problem because its data are trajectories that cover just a small portion of the state-action space. More importantly, many IRL techniques make a number of assumptions that do not hold in our scenario. Specifically, the learner is commonly assumed to have full observability of the expert's trajectory; only a single expert is present; and the learner has full knowledge of the expert's transition function. In the patrolling problem, by contrast, the learner is limited to observing only a portion of the space covered by the patroller, which results in missing observations that are persistent in the data. There is more than one patroller (expert), and though each is nominally executing their own separate policies irrespective of each other, when in close proximity they may interact in a way that is not guided by their separate policies. Finally, the learner is not aware of each patroller's full transition function; for example it could be the case that an actuator of one of the robotic patrollers may be slightly damaged.</paragraph><paragraph>The first contribution of this article is to generalize IRL to the context of varying levels of occlusion of the observed trajectory. An IRL method apt for introducing occlusion is one that finds a probability distribution over candidate policies with the maximum entropy while satisfying the constraints imposed by the observed data [8]. We generalize this convex nonlinear optimization to account for parts of the trajectory being occluded from the learner. While the optimization remains convex, a challenge is that some reward features critical to explaining the behavior may be active entirely in the occluded space, which impacts the optimization technique.</paragraph><paragraph>In the presence of multiple experts, the second contribution of this article is to generalize IRL to allow for inverse learning from multiple robots that interact sparsely. While we may model the joint state of the experts in a single large MDP taking interactions into account in the transition function, the joint MDP becomes both prohibitively large and leads to a partially-observable state space when both experts are not observed simultaneously due to occlusion. As a consequence, we model each robot separately except when they interact. Interactions are modeled by an interaction game with the joint behavior being one of many possible Nash equilibria of the game. However, the optimization is complicated by the fact that interactions may not be observed and these may occur at variable points in the trajectory.</paragraph><paragraph>The third contribution of this article is to partially learn the transition function as well in the context of occlusion, to further relax IRL's model assumptions. In the absence of occlusion, the transition functions of each expert could be learned using several approaches such as learning a dynamic Bayesian network or possibly by supervised learning [25]. However, as occlusion results in receiving no data from certain states we require a method to transfer the transition information obtained from the visible states to the occluded ones. These contributions lead to a new method as shown in Fig. 2</paragraph><paragraph>Our approach is to limit consideration to a stochastic transition function for each expert that may be viewed as being composed of a deterministic core perturbed by transition error probabilities. We assume that for any given state-action pair the intended next state is known. Challenged by occlusion, we present a novel model that is based on the key insight that different transitions share common underlying component features, themselves not perfectly predictable. As an example, in physical agents such as robots the features are the physical components used to perform the transition, for example wheel rotations. The probability of successfully reaching an intended next state is the joint probability of success of all features involved in that action. On learning the probability of each feature's success, we can complete the full transition function as features may get reused in the unobservable portions of the state space. However, a complication arises as the observations received about transitions do not directly inform each feature but instead pertain to feature aggregates.</paragraph><paragraph>We comprehensively evaluate the methods in the application domain of multiple mobile robots engaged in a simple, cyclic perimeter patrolling while a subject robot observes them from a hidden vantage point. The subject must use the limited observations it receives to learn policies that explain the behavior of the patrollers. It then utilizes this information to predict their locations in the near future so that a route may be found that reaches a goal state while avoiding detection by the patrollers. Additionally, one of the patrolling robots may have a damaged wheel, which is not known to the learner. We evaluate the methods using both simulated robots as well as physical TurtleBots, and our results show a strong correlation in performance between them. We demonstrate a significantly increased prediction accuracy and a corresponding increase in the rate of successful penetrations by the subject robot when the generalized IRL is compared to standard IRL that does not model interactions or incorrectly assumes knowledge of the patrollers' transition functions.</paragraph><paragraph>The remainder of this article is organized as follows. We briefly review IRL and specifically summarize a method for inverse learning that maximizes the entropy of learned distributions, in Section 2. Our generalization of IRL given trajectories exhibiting varying levels of occlusion is presented in Section 3, and its generalization to contexts with multiple observed robots is discussed in Section 4. This is followed by methods in Section 5 that aim to relax model assumptions by learning the individual transition functions of multiple experts under occlusion. Section 6 provides a detailed report on our comprehensive experimentation in simulation and with physical robots in order to evaluate the new methods. We discuss related work in Section 7 and conclude with a direction for future work in Section 8.</paragraph></section><section label="2"><section-title>Background</section-title><paragraph>Informally, IRL refers to both the problem and method by which an agent learns preferences of another agent that explain the latter's observed behavior [33]. Usually considered an “expert” in the task that it is performing, the observed agent is modeled as executing the optimal policy of a MDP. The learning agent is assumed to perfectly know the parameters of the MDP with the exception of the reward function. Consequently, the learner's task may be viewed as finding a reward function under which the expert's observed behavior is optimal.</paragraph><paragraph>This problem in general is ill-posed because for any given behavior there are an infinite number of reward functions which align with the behavior. Ng and Russell [28] first formalized this problem as a linear program in which the reward function that maximizes the difference in value between the expert's policy and the next best policy is sought. This method required the expert to specify its complete behavior to the learner, which may not be possible in many domains. To make realistic progress, Abbeel and Ng [1] present an algorithm that allows the expert I to provide task demonstrations instead. The reward function is modeled as a linear combination of K binary features, each of which, {a mathematical formula}ϕk: {a mathematical formula}S×AI → {a mathematical formula}{0,1}, maps a state from the set of states S and an action from the set of I's actions {a mathematical formula}AI to either a 0 or 1. Throughout this article, we assume that these features are selected by the learner. These features could also be learned from data to mitigate feature engineering [23]. They need not be utilized by the expert in its true reward function. The reward function for expert I is then defined as {a mathematical formula}RI(s,a)=∑k=1Kθk⋅ϕk(s,a), where {a mathematical formula}θk are the weights. The learner's task is reduced to finding a vector of weights that complete the reward function, and subsequently the MDP, such that the demonstrated behavior is optimal.</paragraph><paragraph>To assist in finding the weights, feature expectations are calculated for the expert's demonstration and compared to those of possible policies. A demonstration is provided as one or more trajectories, which are a sequence of length T state-action pairs {a mathematical formula}(〈s,a〉1,〈s,a〉2,…〈s,a〉T), corresponding to an observation of the expert's behavior across T time steps. The feature expectations of the expert are obtained as an average over all observed trajectories, {a mathematical formula}ϕˆk=1|Traj|∑traj∈Traj∑〈s,a〉∈trajϕk(s,a), where {a mathematical formula}traj denotes a trajectory in the set of all observed trajectories, {a mathematical formula}Traj.</paragraph><paragraph>For some expert's policy {a mathematical formula}πI, the {a mathematical formula}kth feature expectation involves computing the {a mathematical formula}kth feature value for all states and corresponding actions weighted by the distribution over the state and action: {a mathematical formula}ϕkπI=∑s∈SμπI(s)ϕk(s,πI(s)). Here, {a mathematical formula}μπI(s) is the number of times state s is expected to be visited using the policy {a mathematical formula}πI and may be computed using dynamic programming:{a mathematical formula} where, {a mathematical formula}μπI0 is the initial state distribution, {a mathematical formula}0&lt;γ&lt;1 is the discount factor, {a mathematical formula}T:S×A×S→[0,1] is the transition function.</paragraph><paragraph>Given a set of reward weights the expert's MDP is completed and solved optimally to produce {a mathematical formula}πI⁎. The difference {a mathematical formula}ϕˆ−ϕπI⁎ provides a gradient with respect to the reward weights for a numerical solver. To resolve the degeneracy of this problem, Abbeel and Ng [1] again maximize the margin between the value of the optimal policy and the next best policy. The resulting program may be solved with a quadratic program solver such as a support vector machine.</paragraph><section label="2.1"><section-title>Maximum Entropy IRL</section-title><paragraph>While expected to be valid in some contexts, the max-margin approach introduces a bias into the learned reward function in general. To address this, Ziebart et. al. [37] utilize the principle of maximum entropy to find the distribution of maximum entropy over all trajectories that is constrained to match the observed feature expectations. The benefit is that this distribution makes no further assumption beyond those which are needed to match its constraints and is maximally noncommittal to any one trajectory. As such, it is most generalizable by being the least wrong most often of all alternative distributions.</paragraph><paragraph>A disadvantage of this approach is that it becomes intractable for long trajectories because the space of trajectories grows exponentially with the length. A related method [8] formulates the problem as one of finding a distribution over deterministic policies, {a mathematical formula}Pr(ΠI), which has the maximum entropy while matching the convex combination of feature expectations from all policies with those from the observed expert's trajectory:{a mathematical formula} Here, Δ is the space of all distributions, {a mathematical formula}Pr(ΠI); {a mathematical formula}ϕˆk is the expectation over the {a mathematical formula}kth feature from observations of the expert as defined previously; and the visitation frequency {a mathematical formula}μπI is computed as in Eq. 1.</paragraph><paragraph>Next, we discuss how the program of (2) may be solved to obtain the vector of feature weights, θ. In order to solve this nonlinear but convex optimization, we may apply the method of Lagrangian relaxation bringing both the constraints into the objective function. The relaxed Lagrangian objective becomes,{a mathematical formula} where η and θ are the Lagrange multipliers (also called the dual variables). Our aim now is to maximize Eq. 3 w.r.t. {a mathematical formula}Pr(πI), for all {a mathematical formula}πI∈ΠI, and minimize it w.r.t. η and θ. The function in Eq. 3 is a relaxation of the objective in (2) because the variable value that maximizes the Lagrangian is no smaller than the value that maximizes the latter. Subsequently, minimizing the dual variables gives us a tighter upper bound to the original optimal. To do this, we obtain its partial derivative w.r.t. unknowns {a mathematical formula}Pr(πI), θ, and η as:{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph><paragraph>The objective function in (2) may have its optimal solution as a saddle point in {a mathematical formula}L. However, many numerical optimization techniques such as hill climbing and gradient descent are instead designed to find local minima or maxima. Subsequently, we modify the objective function of the relaxed Lagrangian so that the optimal solution(s) resides at a local or global maximum or minimum. The revised objective function is {a mathematical formula}∂L∂Pr(πI)2+∂L∂θ2+∂L∂η2.</paragraph></section><section label="2.2"><section-title>Approximating Maximum Entropy</section-title><paragraph>As the maximum entropy optimization such as (2) is known to be convex, the solution resides where the partial derivatives equal zero. We therefore set Eq. 4 to zero to obtain a definition for {a mathematical formula}Pr(πI):{a mathematical formula} Here {a mathematical formula}Z(θ) is the partition function,{a mathematical formula}</paragraph><paragraph>Plugging Eq. 7 into Eq. 3 to obtain the remaining unknowns, we arrive at the dual that is minimized w.r.t. θ:{a mathematical formula}</paragraph><paragraph>Unfortunately, obtaining {a mathematical formula}Z(θ) is computationally intensive as it requires summing over all {a mathematical formula}|AI||S| policies. We will instead approximate {a mathematical formula}Pr(πI) and utilize this approximation to avoid calculating Z.</paragraph><paragraph>Notice that Eq. 7 states that the probability of a policy is proportional to its expected value {a mathematical formula}VπI as given by the exponent in its numerator. Therefore, the optimal policy {a mathematical formula}πI⁎ under the reward function with weights θ has the highest probability. We may thus replace {a mathematical formula}Pr(πI) as given in Eq. 7 with an approximation, first introduced by Ng and Russell (?) in the context of IRL, which quantifies {a mathematical formula}πI's distance from the expert's policy. We intend to use this in place of {a mathematical formula}∂L∂Pr(πI):{a mathematical formula} Here, {a mathematical formula}QπI(s,a;θ) denotes the expected value of performing action a at state s and following policy {a mathematical formula}πI thereafter. {a mathematical formula}VπI(s;θ)=QπI(s,πI(s);θ) is the value of {a mathematical formula}πI under a given θ. Notice that {a mathematical formula}QπI(s,a;θ)≤VπI(s;θ) and {a mathematical formula}QπI(s,a;θ)−VπI(s;θ) = 0 when {a mathematical formula}a=πI(s). In other words, the value difference is 0 when the action from the policy under consideration matches the observed (optimal) action of the expert, producing a probability of 1. This is analogous to Eq. 7 thereby motivating the approximation. As the policy's value increasingly diverges from the expert's its probability monotonically decreases and approaches zero asymptotically. Furthermore, {a mathematical formula}Pr(πI) as calculated by Eq. 9 stays normalized and no partition function is needed. As such, η becomes a free variable.</paragraph><paragraph>We now modify the partial derivatives given in the previous subsection to reflect our approximation and set {a mathematical formula}η=1:{a mathematical formula}</paragraph><paragraph>Next, we remove the summation over all policies from the derivative of Eq. 5, and retain {a mathematical formula}πI that is optimal for the weights θ in the current iteration of the optimization. It may change in the next iteration.{a mathematical formula} The revised objective function for minimization is,{a mathematical formula} As we seek to minimize w.r.t. θ, Eq. 10 can be relaxed to consider just the second term when each θ is constrained to be non-negative. Furthermore, as the partial derivative is squared in the objective, the negative sign is removed.{a mathematical formula}</paragraph><paragraph>Notice that we no longer have a summation over the entire space of policies (as in the partition function) and {a mathematical formula}πI changes according to the currently considered θ. The objective in Eq. 12 approximates the objective function at the end of Section 2.1 by retaining the key terms. {a mathematical formula}πI may be found using value iteration performed inline during each iteration of the objective function maximization procedure. The optimizing θ yields a reward function for the expert I.</paragraph></section><section label="2.3"><section-title>Learning Transitions</section-title><paragraph>An assumption common to many IRL methods is that the learner knows the expert's transition function. In cases when learning from real-world agents such as robots or humans, this assumption is difficult to satisfy. While the learner may not control the behavior of the expert it may be possible to estimate the expert's transition function given trajectory data that sufficiently explores the state-action space. A dynamic Bayesian network (DBN) [28], [11] provides a simple, graphical way for modeling the expert's transition function in this case. While several approaches exist for learning the network parameters from data, a popular method for learning from data with missing values is expectation-maximization (EM) [17]. In this method the transitions are modeled using a two-step state-action-state DBN whose parameters are learned from the observed portions of the trajectories as data using EM [22].</paragraph></section></section><section label="3"><section-title>IRL for Robots under Occlusion</section-title><paragraph>As introduced in Section 1, a motivating scenario involves a subject robot that must observe other mobile robots from a fixed vantage point. Its sensors allow it a limited observation area; within this area it can observe the other robots fully, outside this area it cannot observe at all. We denote this special case of partial observability where certain states are either fully observable or fully hidden as occlusion.</paragraph><paragraph>As a result of occlusion the trajectories received by the learner will contain missing data corresponding to time steps where the expert robot is in one of the occluded states. The empirical feature expectation of the expert {a mathematical formula}ϕˆk will therefore exclude the occluded states (and actions taken in those states). To ensure that the feature expectation constraint in (2) accounts for the missing data, we limit the calculation of feature expectations for policies to observable states only. Let {a mathematical formula}Obs(S)⊆S be the subset of all states of each robot that are observable. The traditional setting of full observability provides the limit, {a mathematical formula}Obs(S)=S. We revise the constraint in (2) to:{a mathematical formula} The distribution as given by {a mathematical formula}μπI over observed states flows from distributions over other states. Therefore, {a mathematical formula}μπI over all states is computed.</paragraph><paragraph>We label this generalization as IRL{sup:⁎}. This revision to the equation has the effect of further under-constraining the optimization problem compared to the original. Some features critical to the demonstrated behavior may be active entirely in the occluded portion of the state space, which will result in {a mathematical formula}ϕˆk=0 for this feature. As a result, we can no longer utilize the gradient of the feature expectation to find the reward weights. Thus, a numerical solving method that does not use the gradient function such as Nelder-Mead's simplex technique [27] is necessary when some states are occluded; these tend to converge slower in the absence of a guiding gradient. On the other hand, common methods such as Broyden-Fletcher-Goldfarb-Shanno (BFGS) [10] – a line search quasi-Newton technique – will simply return the initial weights it is given for these features.</paragraph></section><section label="4"><section-title>IRL for Multiple Mobile Robots</section-title><paragraph>In the motivating scenario, two robots patrol a hallway simultaneously and the subject robot is required to learn the patrol route of both. If there was no interaction between the two patrollers, it would be sufficient for the learner to simply solve two separate IRL problems. However, the patrollers are required to coordinate their actions to safely pass each other in a narrow hallway.</paragraph><paragraph>A straightforward approach would be for the subject to model the patrollers jointly in a single large MDP whose joint state includes the state of each robot and a variable to allow for distinguishing states where the interaction occurs. The action space of this joint MDP is the Cartesian product of the set of actions of each robot, which includes the actions each robot may perform during interactions. The transition and reward functions would be modified to account for the joint state and actions accordingly.</paragraph><paragraph>However, this approach suffers from a number of drawbacks:</paragraph><list><list-item label="1.">The joint state space may become very large making the solution of the joint MDP intractable. This is especially problematic as the approximation given in Eq. 9 requires repeatedly solving the MDP optimally for each new set of candidate reward weights.</list-item><list-item label="2.">Due to occlusion, one of the robots may be observed at a time step while the other is not, resulting in the state of the joint MDP being partially observed. The resulting problem may not be solved by traditional IRL algorithms. Choi and Kim [14] present a method for performing IRL with partial observability by modeling the expert as a POMDP. But, the technique requires a complete finite-state controller (policy) or the expert's belief trajectory as input, both of which are unrealistic to obtain.</list-item></list><paragraph>In light of the above limitations, we pursue an alternate approach which continues to model each robot using a separate, individual MDP. The behavior of the robots during an interaction is modeled separately and overrides the behavior prescribed by these MDPs for the duration of the interaction. A nonlinear program that combines the one in (2) for each mobile robot and is used for learning the rewards and policies of all robots is given by:{a mathematical formula}{a mathematical formula} We label this simple multi-agent extension of IRL{sup:⁎} as mIRL{sup:⁎}. The above program may also be decomposed into N similar programs for the N robots if there is no interaction between the robots.</paragraph><section label="4.1"><section-title>Modeling the Interaction</section-title><paragraph>We consider the situation when the interactions between the robots are sparse and ex-post do not impact the policy-guided actions that they perform separately. An example of such an interaction is the one that occurs in our scenario where the robots must coordinate their actions when they are in close proximity to minimize perturbations of their individual patrols. Notice that such interactions may occur in occluded regions as well. Modifications to our approach for those domains where interactions are pervasive are discussed in Section 8.</paragraph><paragraph>At interaction states such as the one in Fig. 3, the subject robot L models the patrollers as playing a single-shot game. Games are an established way of modeling interactions especially when the involved agents get differing payoffs. The strategies of this game correspond to the Cartesian product of the actions in each patroller's MDP. In Table 1, we show an example game in the context of our patrolling application domain. The game is single-shot because the sequence of moves available to robots to successfully pass each other in a narrow corridor can be satisfactorily seen as single (composite) actions. The payoffs are based on the expected outcome from performing the joint action. In particular, actions that avoid a collision and keep the patroller moving are more rewarding. Therefore, Sidestep obtains the highest payoff (of 5 in the example game of Table 1), whereas Turn around obtains the next highest payoff as it avoids a collision but does not allow the patroller to complete its patrol. On the other hand, both patrollers simply stopping while avoiding a collision fails to keep them moving, thereby incurring no payoff.</paragraph><paragraph>At interacting states, the patrollers are presumed to solve the game and pick a solution in the form of a Nash equilibrium. This equilibrium action profile prescribes each robot's behavior during the interaction. L utilizes this model both when the interaction occurs in occluded states and when L observes it. The Nash equilibria may be precomputed given the game formulation for the interaction scenario.</paragraph><paragraph>The solution of the game, in general, may admit multiple strategy profiles as Nash equilibria. The example game of Table 1 for the patrolling domain has five profiles in equilibria each representing a possible way of resolving the task of safely moving past each other when the robots approach each other from opposite directions. Multiple equilibria introduce an additional complication when the interaction is not observed. However, in the event L observes an interaction taking place it simply knows which equilibrium to pick.</paragraph><paragraph>For the sake of clarity, let us initially focus on the case where there is a single Nash equilibrium of an interaction game. As interactions alter the behavior of the robots, the equilibrium must be considered during IRL otherwise the learned reward and policy will likely be different from the actual policy of the observed mobile robot, as it may falsely attribute the interaction behavior to the MDP's reward function. Let {a mathematical formula}σe = {a mathematical formula}〈σ1,…,σN〉 be an equilibrium profile with {a mathematical formula}σI, I = {a mathematical formula}1,…,N denoting each robot's action that is in equilibrium, respectively. As interactions impact the state-visitation frequencies of the robots, we decompose Eq. 1 into a piecewise function for each robot,{a mathematical formula} The two cases in Eq. 16 differ by which action is used in the transition function. In particular, the action in an equilibrium is used if {a mathematical formula}s′ is an interacting state. We may then replace {a mathematical formula}μπI in Eq. 13 with the above equation.{a mathematical formula}</paragraph><paragraph>In the presence of occlusion, this equation may not be computable as the proportion of interacting states may be unknown to L. Instead, we empirically compute the state-visitation frequencies by projecting in the full environment the policy under consideration for each robot for a large number of time steps while utilizing the equilibrium behavior {a mathematical formula}σe when the robots interact. The state visitations in the projections are accumulated to obtain an approximation of the state-visitation frequency, {a mathematical formula}μπIe. Note that this technique renders the program non-convex, as the state-visitation frequency of a policy for one robot depends in part on the policy chosen for the other robot. The degree of non-convexity depends on the sparsity of interactions.</paragraph><paragraph>We refer to this extension of mIRL{sup:⁎} where the interaction behavior is known as mIRL{sup:⁎}+ne.</paragraph></section><section label="4.2"><section-title>Multiple Equilibria</section-title><paragraph>Note that while the interaction game modeled by L may admit multiple equilibria, the robots pick one to resolve their interaction. In the case where L is unable to determine the specific equilibrium used by the robots through observation or side information, how should L model the interactions?</paragraph><paragraph>Our approach is to retain each possible equilibrium behavior and weight it based on how much the state-visitation frequency resulting from using each behavior is supported by data. If {a mathematical formula}μπIe is the empirical state-visitation frequency when equilibrium {a mathematical formula}σe is used at each interaction and {a mathematical formula}πI elsewhere, and {a mathematical formula}μˆI is the state-visitation frequencies from the observed trajectories, then L weights the potential of this interaction behavior as inversely proportional to the difference,{a mathematical formula} This obtains ω for each equilibrium behavior, and the ω vector is then normalized to sum to 1. Finally, we modify the constraint in Eq. 17 with a convex combination of each equilibrium-based interaction. Let {a mathematical formula}E be the set of all equilibria, the constraint becomes:{a mathematical formula}</paragraph><paragraph>During each iteration of the numerical optimization, a new set of reward weights are generated and an optimum policy found for each patroller. {a mathematical formula}μπIe is found for each {a mathematical formula}σe and the normalized ω weights are then recomputed. We expect the initial weights to be nearly uniform because learned policies do not correctly model the observed behavior for the most part. Eventually, as the policies become accurate, projecting the true interaction behavior gains importance and the weights between equilibria diverge relatively. We explore the dynamics of the evolution of ω during optimization in Section 6.2.1. This new method for multiple interacting robots with unknown equilibrium in the presence of occlusion is labeled mIRL{sup:⁎}+Int.</paragraph></section></section><section label="5"><section-title>mIRL with Transition Function Estimation</section-title><paragraph>Key model assumptions of popular IRL methods are that the expert's stochastic transition function is completely known to the learner as in apprenticeship learning [1] and in Bayesian IRL [31]. Alternately, the transition function is effectively deterministic and thus is easily approximated from the observed trajectories as in entropy maximization [37] with the assumption that transition randomness has a limited effect on the final behavior. The prior knowledge requirement is often difficult to satisfy in practice, for example, in scenarios that are not cooperative such as the patrolling application. Alternately, the supposed impotency of transition errors is a strong assumption in the context of robots.</paragraph><paragraph>Learning the transition function of each expert from trajectory data is significantly complicated due to the following reasons:</paragraph><list><list-item label="•">As the learner is passive in IRL, it does not control the expert and cannot request transition demonstrations from the expert.</list-item><list-item label="•">Of course, the learner may learn the complete transition functions using supervised learning if it observes the experts fully and long enough. But, partial occlusion and a finite observation time motivate prudence and sophisticated methods.</list-item><list-item label="•">Furthermore, the expert performs its task optimally. Therefore, it may not perform all actions in all possible states, and the transition dynamics may not be obtained trivially.</list-item><list-item label="•">Real-world demonstrations consume time and energy resulting in limited trajectories available to the learner.</list-item></list><paragraph>An exception to knowing the expert's dynamics is model-free IRL [7]. However, it becomes untenable under occlusion because the required distributions over the full state-action space cannot be computed. Consequently, to inversely learn in our robotic scenario we must first estimate the transition function of each expert under these limiting circumstances.</paragraph><paragraph>Toward this objective, we tread a middle path and limit our method to those settings where a mobile robot's stochastic transition function may be viewed as composed of a deterministic core perturbed by transition error probabilities that make it stochastic. Given a state-action pair, the learner knows the intended next state of each expert. However, the transition error probabilities are unknown.</paragraph><section label="5.1"><section-title>Transition and Error Model</section-title><paragraph>Let ψ: {a mathematical formula}S×A→S map an observed robot's transition from state s given action a to a particular next state, {a mathematical formula}s′. The function ψ gives the intended outcome of each action from each state. We may view ψ as a deterministic transition function. In robotic scenarios, actions often lead to movement. Therefore, the patroller may reveal its intended next state given by {a mathematical formula}ψ(s,a) during this movement. For instance, a robot moving forward is obviously intending to reach the next state in front of it.</paragraph><paragraph>Actions taken by the agent may not always result in the intended outcome leading instead to some other state. We therefore focus on learning the probability of transitioning to the intended next state given a state-action pair for an observed robot I, {a mathematical formula}TI(s,a,ψ(s,a)), and this probability is estimated from the available trajectory data. The remaining probability mass, {a mathematical formula}1−TI(s,a,ψ(s,a)) is then distributed according to some error model, such as uniformly among all other states or perhaps to the intended outcomes of other actions.</paragraph><paragraph>This approach requires that ψ is known to the learning agent. An added bonus is if the precise error model is also known to the learner although this is challenging in the context of robotics. Despite this, not knowing the probability of the expert reaching its intended state is a significant relaxation of the requirement by existing IRL methods that the full stochastic transition function inclusive of all probabilities be known. Indeed, in domains involving mobile robots where the actions taken correspond to movement, {a mathematical formula}ψ(s,a) may be determined accurately and we follow this approach in our experimentation. Section 5.6 will explore the error model in more detail.</paragraph></section><section label="5.2"><section-title>Mapping from Transition to Feature Subset</section-title><paragraph>To learn transition probabilities robustly in the presence of occlusion we introduce an approach based on the following key observation:</paragraph><paragraph>Observation.  If transition probabilities are a function of some underlying component outcome probabilities the observed trajectory of the agent will inform these component probabilities. Then, if some of these components are shared with transitions in occluded states, information is implicitly transferred that facilitates obtaining the transition probabilities in occluded states.</paragraph><paragraph>Motivated by this observation, we begin by mapping all state-action pairs to some subset of lower-level transition features. Let {a mathematical formula}ξIs,a={τ1,…,τk} be the subset of independent features mapped to a state-action pair, {a mathematical formula}〈s,a〉. Each feature, {a mathematical formula}τ∈TI, is a binary random variable whose values are τ and {a mathematical formula}τ¯, and {a mathematical formula}⋃(s,a)ξIs,a=TI. Sets {a mathematical formula}TI and {a mathematical formula}ξIs,a could be obtained from observations of robot I or, more easily, from side information about the robot such as its type or model. Figure 4 illustrates the above observation.</paragraph><paragraph>Now, define for a transition {a mathematical formula}〈s,a,ψ(s,a)〉,{a mathematical formula} Because components may interact with each other – for instance a damaged wheel may also cause a navigation system to fail – the relationship between transition features is ideally represented using a Bayesian network. However, computing the joint of a Bayesian network takes exponential time in general. Consequently, we model the features as being mutually independent for tractability. This informs the selection of features and may require the use of additional correlation features for highly correlated components.</paragraph><paragraph>Equation 20 is loosely analogous to using feature functions in the reward function with the difference being that {a mathematical formula}TI is probabilistic due to which features are random variables. To illustrate, consider the move_forward action of a robot operating on a smooth surface in a grid. Movement is performed using some of its actuation components, for example, all of its wheels rotating with the same velocity for a period of time. A differential drive robot with 2 wheels could have the move_forward action from a state mapped to two features: left wheel rotating with some velocity on a smooth surface and right wheel rotating with the same velocity on the surface. Then, {a mathematical formula}TI(s,move_forward,ψ(s,move_forward))=Pr(left_wheel rotating on smooth sufrace) × Pr(right_wheel rotating on smooth surface). A damaged wheel decreases this probability. Other features involved in the transition may also be included, such as the state of the surface (smooth, rough, slippery) and a navigation system.</paragraph></section><section label="5.3"><section-title>Observed Probabilities</section-title><paragraph>Equation 20 casts the problem of learning the transition function as learning the probability distributions of the binary state-action features. However, a challenge faced by this technique is that we may not be able to attribute the transitions in the observed trajectory to the outcome of specific features. Instead, we obtain aggregated empirical probabilities attributable to multiple features. An observed trajectory of length T is a sequence of state-action pairs, {a mathematical formula}(〈s,a〉0,〈s,a〉1,…,〈s,∅〉T), where ∅ is the null action. From this, we obtain the probabilities of transitioning to the intended state given the previous state and action, denoted by {a mathematical formula}qψ(s,a), as simply the proportion of times the intended state is observed as the next state in the trajectory:{a mathematical formula} where {a mathematical formula}δ(⋅,⋅) is the indicator function that is equal to 1 when its two arguments are equal, otherwise 0.</paragraph><paragraph>To illustrate, consider a simple trajectory, {a mathematical formula}(〈s1,move_forward〉,〈s2,turn_around〉,〈s2,move_forward〉,〈s1,turn_around〉,〈s1,move_forward〉,〈s3,∅〉). If {a mathematical formula}s2 is the intended state from {a mathematical formula}s1 on performing action move_forward, then {a mathematical formula}qIψ(s1,move_forward)=1/2. Of course, a trajectory observed over a longer time leads to more accurate probabilities.</paragraph><paragraph>Notice that the probability {a mathematical formula}qIψ(s,a) calculated from a given trajectory is equivalent to {a mathematical formula}TI(s,a,ψ(s,a)). Therefore, we may utilize Eq. 20 to relate the observed transition probability to the transitions features:{a mathematical formula}</paragraph><paragraph>In the case where some state-action pairs are mapped to identical sets of features ({a mathematical formula}ξIs1,a1=ξIs2,a2) we need not distinguish between them and can arrive at a single transition probability by combining the transition counts from their respective {a mathematical formula}qIψ(s,a).</paragraph><paragraph>While it may be that one feature only is assigned to each transition, however, in general, we arrive at an ill-posed problem where there could be infinitely many feature probabilities satisfying the observed transition probabilities that serve as aggregates.</paragraph></section><section label="5.4"><section-title>Underconstrained Optimization</section-title><paragraph>To make progress, we take recourse to the principle of maximum entropy again because it makes the least assumptions beyond the problem formulation. More specifically, we seek the satisficing distribution over transition features with the maximum entropy; this distribution will be least wrong most often - an important criterion because we wish to transfer the transition information from observable states to occluded ones. In this context, we maximize the sum total entropy of probabilities of all features subject to the constraints given by Eq. 21. The novel nonlinear, non-convex program for finding the transition feature probabilities of N robots is:{a mathematical formula}</paragraph><paragraph>The Lagrangian relaxation of the above optimization problem introduces vectors of Lagrangian multipliers, v and λ, and the relaxed objective function is:{a mathematical formula} Vector v has as many elements as the number of state-action pairs and observed robots. We minimize {a mathematical formula}L by using the penalty approach with Broyden-Fletcher-Goldfarb-Shanno (BFGS) [10] unconstrained gradient descent technique used to solve each subproblem. We refer to this new method as mIRL{a mathematical formula}/T⁎+Int. To facilitate recall, the Appendix contains a listing of all methods and their extensions. These will be evaluated later in Section 6.</paragraph><paragraph>Unseen actions could have occurred in the occluded portion of the robot's trajectory. Nevertheless, these actions map to feature variables in {a mathematical formula}TI. As some of these features are factors in observed actions, we may obtain (partially) informed transition distributions for the unseen actions under the maximum entropy principle. This observation emphasizes the advantage of considering transition feature variables.</paragraph></section><section label="5.5"><section-title>Convex Approximation with Bernstein Polynomial</section-title><paragraph>The optimization program in (22) is obviously nonlinear and is also nonconvex due to the presence of monomial constraints. A convex approximation provides well-understood and much needed scalability as there are many well-known algorithms for quickly solving convex optimization problems.</paragraph><paragraph>With this benefit in mind, we aim for a convex approximation of (22). Our strategy for this is to find a posynomial approximation for the entropy based objective function, which will convert the program of (22) into a geometric program; these are log-convex. First, notice that the objective function is continuous on the variable {a mathematical formula}Pr(τ) that lies in {a mathematical formula}[0,1]. We may use a Bernstein polynomial of degree d to approximate this continuous function as:{a mathematical formula}</paragraph><paragraph>Bernstein polynomials have a number of properties that make them appealing for our approximation purposes. For instance, they are convex if the original function is convex, and are only-positive if the original function is only-positive. Importantly, they demonstrate uniform convergence to the original function as {a mathematical formula}d→∞. A challenge in our use of these polynomials is that all terms of the approximating polynomial should be positive in order for it to be a posynomial. However, we could not obtain such fitted approximations.{sup:1}</paragraph><paragraph>We hypothesized that the downward sloping portion of the binary entropy function caused the negative terms in the Bernstein approximation. By first negating the entropy function and adding on a constant term we arrive at a positive-only convex function. As a final step, we restricted the range of {a mathematical formula}Pr(τ) to {a mathematical formula}[0.5,1] to remove the downward sloping portion of this function from consideration – the approximation is piece-wise and is set to zero in the interval [0, 0.5). The program of (22) now becomes:{a mathematical formula}</paragraph><paragraph>The approximating Bernstein polynomial for {a mathematical formula}d=4 using the objective function over {a mathematical formula}Pr(τ) as f is:{a mathematical formula}</paragraph><paragraph>Then, the geometric program which approximates (24) is:{a mathematical formula}</paragraph><paragraph>Geometric programs such as the one above can be made convex through a well-known procedure involving change of variables [9]. We point out that this approximation is applicable when all {a mathematical formula}Pr(τ)≥0.5. This assumption is not limiting and is, in fact, easily satisfied because {a mathematical formula}Pr(τ) corresponds to the probability of success of the feature. We expect success rates of features to be well above 0.5, as is substantiated in our experiments. Indeed, components with very low success rates are unlikely to be used as they would be extremely unreliable.</paragraph></section><section label="5.6"><section-title>Estimating a Full Transition Function</section-title><paragraph>Unseen state-action pairs due to being in the occluded portion of the robot's state space may share transition features with state-action pairs that are observed. Thus, we obtain informed probabilities (perhaps partially) for these unseen transitions. This observation highlights an advantage of the use of transition features and in the case where the observed transitions engage all features, we may recover almost the complete transition function. This principled projection into the unobserved portion of the state and action spaces is justified by the maximum entropy method: over all possible distributions taking into account all available information the one with the maximum entropy is expected to be the least wrong.</paragraph><paragraph>mIRL{a mathematical formula}/T⁎+Int solves the nonlinear optimization to obtain feature probabilities that maximize the total entropy. We then compute {a mathematical formula}TI(s,a,ψ(s,a)) for each state-action pair using Eq. 20. In order to estimate the complete transition function, we additionally need the probability of reaching unintended states due to action errors.</paragraph><paragraph>The mass {a mathematical formula}1−TI(s,a,ψ(s,a)) could be distributed according to many possible models: uniformly across all next states excepting the intended state; to a dedicated error state; or among the intended states that would result due to performing actions other than a from s. While one could be chosen based on knowledge of the agent or robot being modeled, a general way is to choose the most likely model given the data on observed unintended transitions.</paragraph><paragraph>Let {a mathematical formula}Pr(st+1|st,at,m) be the distribution over next states {a mathematical formula}st+1 conditioned on error model m. The observed trajectories of the expert may yield several triples {a mathematical formula}〈st,at,st+1〉 where {a mathematical formula}st+1≠ψ(st,at). We wish to choose m that maximizes the likelihood of these observed triples:{a mathematical formula}</paragraph><paragraph>We limit our consideration to a finite number of error models that can be iterated. An extension to parametric models is straightforward as each model's parameters could be estimated from the available data first before selecting the maximum likelihood parameterized model.</paragraph><paragraph>Subsequent to choosing an error model, the full transition function of the other robot, I, is obtained as:{a mathematical formula} where {a mathematical formula}δ(⋅,⋅) is an indicator function as stated previously. In Eq 26 the probability of reaching {a mathematical formula}st+1 is due to both {a mathematical formula}TI(st,at,ψ(st,at)) (if {a mathematical formula}st+1 is the intended next state due to action {a mathematical formula}at from state {a mathematical formula}st) and due to error modeled by m. The transition probability due to m is weighted by the probability mass left over from transitioning to the intended state.</paragraph><paragraph>Other alternatives to the maximum likelihood error model include fitting a mixture of models, and learning a dynamic Bayesian network from the unintended transitions. However, given that errant transitions are expected to occur sparsely, there may not be sufficient data to learn these more complex error models.</paragraph></section></section><section label="6"><section-title>Experiments</section-title><paragraph>Comprehensive experimentation in a pragmatic robotic domain will empirically establish the efficacy of our methods in generalizing IRL{sup:⁎}. It will include a measure of how well the observed behavior of the expert(s) was learned, and demonstrate the benefit of having learned the behavior in the subject robot's planning toward achieving its goal.</paragraph><paragraph>Before we introduce the domain in detail, we briefly verify the validity of the approximation given in Section 2.2 on randomly-generated experiments. Specifically, single experts are simulated by running policies for 10 time steps obtained from exactly solving MDPs with 6 states, 4 actions, and random transition and reward functions. The learner L is tasked with assessing the reward function using the maximum entropy IRL as described in Section 2.1 along with the heuristic approximation of Section 2.2.</paragraph><paragraph>A naive test of IRL{sup:⁎} would be to simply compare the true reward function with the learned reward function for differences. But, this test may be overly stringent because several reward functions when plugged into the expert's MDP may generate the same policy. A more robust test is to minimize the difference between the value function of the optimal policy under the true rewards and the value function of the learned policy under the true rewards [15]. Intuitively, this inverse learning error (ILE) gives the loss of value if L uses the learned policy on the task instead of the expert's: {a mathematical formula}ILE=‖VπI⁎−VπIL‖1. Here, {a mathematical formula}VπI⁎ is the optimal value function of I's MDP and {a mathematical formula}VπIL is the value function due to utilizing the learned policy {a mathematical formula}πIL in I's MDP. Notice that when the learned reward function results in an identical optimal policy to I's optimal policy, {a mathematical formula}πI⁎=πIL, ILE will be zero; it increases monotonically as the two policies increasingly diverge in value.</paragraph><paragraph>An important test of the approximation is if ILE approaches zero as more trajectories (of length 10) are provided. As seen in Fig. 5, this is indeed the case. Each data point is the mean ILE measured across 1,000 trials grouped into blocks of size 10. Standard error is low ranging from 0.018 for a single trajectory to 0.0021 for 100 trajectories. As such, our method for performing maximum entropy IRL{sup:⁎} brings tractability to the optimization without negatively impacting its learning performance.</paragraph><section label="6.1"><section-title>Domain: Penetrating Perimeter Patrols by Multiple Robots</section-title><paragraph>Our primary evaluation domain involves a subject robot L aiming to penetrate continuous perimeter patrolling by two mobile robots, I and J, along a narrow hallway. The two patrollers each have a cyclical patrol route and operate independently for the most part until they approach each other from opposite directions. In order to pass each other and resume their patrol as quickly as possible the robots must coordinate their actions. The true interaction equilibrium (unknown to L) used in all our experiments is for J to stop while I sidesteps around it. This coordination occurs over a duration of 3 time steps. These events are a realization of the sparse interactions described in Section 4.1, and the joint behavior must be modeled by L.</paragraph><paragraph>The learner observes from a vantage point that is off the patrol route and must reach a designated goal state autonomously without being detected by either patroller. The two scenarios used in our experiments are shown in Fig. 6, with the scenario in (a) being significantly larger than the other in (b). In both scenarios, L is spotted if it is within 3 grid cells of any patroller while the patroller is facing it. These scenarios have obvious practical applications and the learner is tasked with a challenging end-to-end problem. It involves learning the behaviors of two patrolling robots from partial and passive observations, reasoning about those behaviors in its own decision making, followed by successfully executing the outcome of its decision making.</paragraph><section label="6.1.1"><section-title>MDP-based Models</section-title><paragraph>The patrolling behavior of I and J is modeled as being guided by the optimal policy of their own separate MDPs. The states of these MDPs are the cell decompositions of the patrolled area, {a mathematical formula}〈x,y〉, as well as the discretized orientation of each robot, o. Each cell maps to about 2.25 sq. mts of physical space in the hallways. A patroller's MDP for the larger environment has 96 states while the smaller environment exhibits 76 states. Figure 6(c) shows the cells for the smaller environment. The actions of the patroller allow it to move forward one cell, stop, turn right or left 90{sup:∘} at its place or turn around 180{sup:∘} at its place. As the actions involve motion, the intended next states of each state-action pair given by the function ψ are all easily determined. For the purpose of evaluation, each patroller's reward function is a weighted combination of feature functions, as described in Section 2, with each specific to the map used; these are described in the next subsection. In practice, the reward functions may be more abstract but are generally amenable to this type of modeling.</paragraph><paragraph>Subject robot L also uses a MDP to plan its route to the goal state. As it is operating in the same space as the patrollers its state and action spaces are similar to the patrollers' with one important exception: its state includes an additional variable, t, which is a time dimension discretized into steps as shown in Fig. 7. L's reward function assigns a positive reward to the goal states and a penalty (negative reward) to states in which L could be detected. However, modeling detections is challenging as the patrollers are mobile, which would make the reward function non-stationary. We may address this by considering the patrollers to be a part of the dynamic environment, and waiting until the patrollers have been observed and their policies learned. Subsequently, L projects joint trajectories forward in time from the last point it observed each patroller using their policies and interaction equilibrium. At each time step (i.e., at a state), the projected position of each patroller and a simple visibility model are used to identify the states at which L would be detected. The reward function is now completely defined. Completing the MDP, the transition function models the probability of any of its own actions failing to reach its intended next state at 2.5%. We obtained this value from observations of numerous pre-trials with a TurtleBot-based learner. The MDP may then be solved optimally using standard value iteration [30].</paragraph><paragraph>When should L start moving toward the goal? Obviously, L must inversely learn the reward function and policy of each patroller before it may solve its own MDP. Note that simply solving the MDP does not indicate that a path to the goal that avoids detection exists. L must determine if such a safe path exists before it leaves the vantage point, and we outline L's algorithm below:</paragraph><list><list-item label="1.">As the patrollers may start at any point in their trajectories, L waits for a specified amount of time gathering observations of the patrollers (called observation time in the charts). We assume that L is capable of distinguishing between the two patrollers and associating observations to the patroller generating them.</list-item><list-item label="2.">Utilize the observations received to learn each patroller's behavior using the IRL approach outlined in the previous sections.</list-item><list-item label="3.">Jointly project forward I and J's behavior from the last point each patroller was observed and use this information to complete L's reward function.</list-item><list-item label="4.">Solve the resulting MDP. If a path exists to the goal without likely being seen, then a positive value can be found at L's starting position at some time step in the future. If found, begin executing the optimal policy starting at this time step to attempt to reach the goal state.</list-item><list-item label="5.">If a positive value is not found, go back to step 1. Repeat until an attempt on the goal is made or the experiment times out.</list-item></list></section><section label="6.1.2"><section-title>Reward Feature Functions</section-title><paragraph>In the smaller environment, both {a mathematical formula}RI(s,a) and {a mathematical formula}RJ(s,a) are composed of the following two types of binary feature functions because their trajectories are simple cycles:</paragraph><list><list-item label="1.">Has moved, which returns 1 if the action causes the patroller to leave its current location, otherwise 0; and</list-item><list-item label="2.">Turn around at state s, which returns 1 if the robot turns around 180{sup:∘} at the location given by s, otherwise 0.</list-item></list><paragraph> The true vector of weights θ, not known to L, emphasizes moving in hallways and turning around after I or J has turned the corner. It penalizes turning around at other locations in hallways, thereby creating a cyclic patrolling pattern.</paragraph><paragraph>Reward functions of the patrollers in the larger environment additionally include the following binary feature functions as their trajectories may also go through rooms in the large hallways.</paragraph><list><list-item label="1.">Enter room, which returns 1 if the patroller enters a room from one of the hallways, otherwise 0;</list-item><list-item label="2.">In room, which returns 1 if the patroller is in any of the rooms in the map, otherwise 0; and</list-item><list-item label="3.">Leave room, which returns 1 if the patroller leaves a room to enter a hallway.</list-item></list><paragraph> Similar to the smaller environment, the true vector of weights penalizes entering any side rooms from the hallways. Consequently, the MDP-based policy of each patroller generates trajectories that move through the hallways turning around in proximity to the corners at the end of each hallway.</paragraph></section></section><section label="6.2"><section-title>Performance Evaluation in Simulation</section-title><paragraph>We promote a deeper understanding and evaluate the contributions of Sections 4 and 5 in the context of the robotic patrolling domain described previously both in simulation and on physical robots. While simulations were performed in both indoor environments, one larger than the other, physical robot experiments were performed in the smaller environment only.</paragraph><paragraph>Evaluation metrics.  Rewards learned through IRL are generally not directly comparable to the true preferences of the expert. This is because a demonstrated policy admits infinitely-many reward functions (for example affine transformations of the true reward function) that explain it. Consequently, we utilize two measures to evaluate the performance of our approaches. First, is the learned behavior accuracy that is the proportion of all states at which the actions prescribed by the inversely learned policies of both patrollers and the interaction model (if it applies) coincide with their actual actions. The latter are found by solving the patrollers' MDPs with the true rewards in conjunction with their true interaction behavior. This measure is a strong indicator of a successful learning algorithm, but does not incorporate the robotic aspects of the experiment. The measure also strongly correlates with ILE because if the learned behavior accuracy is high, then ILE is low, and analogously if the accuracy is low.</paragraph><paragraph>Success rate comprehensively measures all aspects of the experiment by reporting the proportion of runs L reached the designated goal state without being detected. This measure has practical implications – it assesses whether any approach and its implementation are actually feasible in the patrolling domain. Not only does it reflect the ability of L to accurately learn useful policies for the patrollers, but also that L can predict their future positions, and traverse a route around them following its own model of the world within an allotted time.</paragraph><paragraph>Robotic platform and software.  All robots are TurtleBots equipped with a Microsoft XBox 360 Kinect, both in simulation and in physical experiments. Thus, robots are equipped with a stereo camera and infrared laser for depth perception. TurtleBots are differential drive robots carrying a laptop running ROS Fuerte on Ubuntu 12.04. The robots identify each other by way of unique color tags using CMVision, which is ROS's blob finder. We use ROS's default localization and motion planning for Turtlebots, and the centers of the cells are translated into goal positions for the path planning algorithm. As such, the robots are self-localized and move autonomously. Trajectories of state-action pairs are generated by discretizing the observed feed of movement readings from the blob-finder into the most likely state and action taken during a time step. Specifically, we extracted 4 variables: blob color and centroid, distance to the blob, and the position on the current map. Each time step was calibrated to range for 2 seconds, which resulted in a feed of about 60 frames per time step. We utilized a simple voting protocol to resolve any conflict and arrive at a consensus state-action pair for each time step. We vary the starting locations of the patrollers across runs.</paragraph><paragraph>We begin with Section 6.2.1 that reports on the experiments involving two and more patrolling robots with varying levels of occlusion, followed by Section 6.2.2 that reports on results of additionally learning the transition function as well. While experiments in these sections are simulations, Section 6.3 presents the outcomes of evaluating the methods on actual physical TurtleBots conducting patrols in a building hallway.</paragraph><section label="6.2.1"><section-title>Multiple Experts Under Occlusion</section-title><section><section-title>Comparative approaches.</section-title><paragraph>In addition to the method mIRL{sup:⁎}+Int, which generalizes IRL to settings involving multiple robots under occlusion, we make use of three other approaches to provide baselines and benchmarks for evaluation.</paragraph><list><list-item label="•">mIRL{sup:⁎} extends IRL{sup:⁎} to multiple agents and accounts for occlusion but assumes no interaction between the robots. Consequently, mIRL{sup:⁎} involves independent use of the maximum entropy IRL method for each patroller, and provides a key benchmark by a previously known technique.</list-item><list-item label="•">KnownPolicy provides an upper bound on performance by revealing each patroller's policy and interaction behavior to L, removing the need to perform any learning.</list-item><list-item label="•">Finally, Random ignores all observations and simply waits a random amount of time before moving directly to the goal state.</list-item></list></section><section><section-title>Learned behavior accuracy.</section-title><paragraph>We start by evaluating the learned behavior accuracy of mIRL{sup:⁎}+Int and mIRL{sup:⁎} as a function of the degree of observability. The degree of observability is the proportion of all {a mathematical formula}(x,y) cells in the state space that are visible to L; its complement gives a measure of the occlusion. Observability in simulations is increased by letting the learner “see” larger contiguous portions of the trajectory from its vantage point for purposes of evaluation. Each data point shown in Fig. 8 is the average of 400 runs in the larger environment and 200 runs in the smaller one. Note that KnownPolicy and Random are not shown because these approaches do not perform any learning (KnownPolicy always has a learned behavior accuracy of 1 and Random exhibits 0).</paragraph><paragraph>mIRL{sup:⁎}+Int results in learning overall behavior of both I and J that is significantly more accurate in general compared to mIRL{sup:⁎} in both environments (Student's paired, two-tailed t-test, {a mathematical formula}p&lt;0.025 for both environments) and the learning difference between the two becomes larger with less occlusion. Clearly, modeling potential interactions in IRL provides a measurable benefit when learning others' behaviors. Notice also that while accuracy improves with the degree of observability for mIRL{sup:⁎}+Int, it surprisingly peaks for mIRL{sup:⁎} and gradually reduces as observability approaches 100%. In the larger map, it dropped to approximately the same level as when observability was at 10%. The reason for this phenomenon is that as interactions between I and J become increasingly visible to L, mIRL{sup:⁎}, which does not account for their interaction behavior, seeks to attribute this to their reward functions and is unable to learn individual policies that effectively reproduce the joint interaction behavior; likely actions in the interactions are seen as a persistent form of noise.</paragraph></section><section><section-title>Success rate.</section-title><paragraph>Figure 9 shows the success rates achieved in both environments as three factors are varied: the learned behavior accuracy, degree of observability, and the time L is given to observe the two patrolling robots.</paragraph><paragraph>As we show in Fig. 9(a), improved accuracy of the learned behaviors positively impacts the success rate for mIRL{sup:⁎}+Int in general. Between 0.75 and 0.85, the success rate remains mostly flat on the left chart as the additional accuracy does not lead to improved success rate in the smaller domain. But for the same level of learned behavior accuracy mIRL{sup:⁎} results in a reduced success rate that is attributed to the lack of interaction modeling in the forward projections by L.</paragraph><paragraph>We obtained the success rate with respect to learned behavior accuracy as follows: as the behavior accuracy is a continuous real number we begin by generating a histogram with bins of size 0.05. Individual runs are placed into their respective bin by their learned behavior accuracy. The success rate of each bin is calculated by grouping the runs into blocks of 30 within each bin, calculating the success rate of each block and finally computing the mean over blocks. The mean success rate and standard deviation of this mean for all bins is presented in Fig. 9(a). The final success rate peaks at 60%, which is indicative of the difficulty in penetrating the continuous robotic patrols.</paragraph><paragraph>Figures 9(b) and (c) respectively show the impact of varying L's degree of observability and its observation time on the success rate. Note that KnownPolicy and Random form the upper and lower bounds to the IRL-based approaches as expected. As observability and observation time increases mIRL{sup:⁎}+Int's performance improves significantly and begins to approach the performance of the upper bound. The two are almost identical at about 6 minutes of observation time. This shows the effectiveness of our new technique in this domain. Interestingly, the performance of KnownPolicy also improves with observation time for the larger environment, which appears to indicate that waiting in itself could be beneficial here. One reason why the passage of time helps somewhat is because the patrollers over time tend to become clustered rather than remain spaced apart from each other in the larger environment. This offers an improved chance of success to the learner. The data points are grouped into blocks so that each one comes from an identical distribution of parameters. On the other hand, the success rate of KnownPolicy comes close but does not exceed 80%. Multiple factors contribute to this result including the fact that despite knowing the policies and interaction behavior of the two patrollers, it cannot predict the exact motion of the robots due to the presence of some randomness in their motion and the fact that the subject robot (infrequently) loses its localization as it is moving toward its goal.</paragraph><paragraph>To illustrate L's performance we show a particularly interesting successful run in Fig. 10. L's inverse learning and subsequent MDP-based planning directs it to enter a side room and wait while J passes by it. As L lacks the option to stop, it moves toward the wall and turns around consuming the desired number of time steps. Here, the importance of accurate learning and prediction is evident: being wrong in the amount of time to wait would cause L to be caught by either I or J. Note that L simply follows the prescribed policy once it begins moving and subsequent observations are utilized in determining the state.</paragraph></section><section>Evolution of ω.<paragraph>As discussed in Section 4.2, we expect the vector that weights multiple Nash equilibria for interaction modeling, ω, to be relatively flat (high entropy) at the beginning of the optimization procedure because the reward weights are essentially random. Then, as the numerical optimization proceeds and the rewards begin to approach optimum the weights should converge toward the most likely equilibria that correctly model the interaction. We recorded the ω distribution during the numeric optimization of mIRL{sup:⁎}+Int for various levels of occlusion. The optimization was limited to 125 iterations of Nelder-Mead's Simplex, and at the end the reward weights as well as the most likely equilibrium attributed to the patrollers is obtained. We sampled 30 times for each level of occlusion and averaged the entropy at each iteration across the samples.</paragraph><paragraph>Interestingly, as seen in Fig. 11 for different degrees of observability, the ω distribution does not behave empirically as expected. Instead, it begins at low entropy and gradually rises to a moderate level. This surprising behavior is in part due to the fact that the reward function has not converged during the early iterations. We noticed that the distribution is almost deterministic with one equilibrium containing nearly all the probability mass. This did not persist, and in the next iteration a different equilibrium is assigned the bulk of the probability mass. This is likely because the numerical method is moving through the space of reward functions as well. Notwithstanding this, the standard deviation of the weights (the error bars) is higher in the initial iterations than in the later iterations as a satisficing reward function is found. The maximum entropy when all equilibria are equally probable is 1.6 in our experiments. As the optimization proceeds and the search becomes more informed, less and less equilibria are in play until the optimization narrows down to just a few reward functions, which allows few equilibria that can explain the observations. Most of the runs end with two or three equilibria having the bulk of the probability mass, for all levels of observability.</paragraph><paragraph>Notice also the difference in behavior between the low observability (0.14 and 0.29) runs and others. With few states available to compare the effect that each equilibrium has on the state visitation frequency, ω in these runs oscillates more and we see a jump to near maximum entropy around iteration 20, whereas runs with more visibility tend to gradually increase in entropy as the reward function converges.</paragraph></section><section><section-title>Scaling to more agents.</section-title><paragraph>A key question is how well mIRL{sup:⁎}+Int scales as the number of patrolling robots is increased. With more experts, the number of reward functions and the total number of associated feature weights (that form the variables in the IRL optimization) grows linearly. Furthermore, the number of possible action profiles that are candidates for Nash equilibria that resolve the interactions may grow exponentially in the number of experts.</paragraph><paragraph>At each iteration and for each equilibrium, we must sample repeatedly to estimate state-visitation frequency, {a mathematical formula}μπIei. Consequently, the solution time is expected to grow at most exponentially for more observed robots although less in general when interactions are sparse. Indeed, Fig. 12 illustrates this subexponential growth as we add more robotic patrollers to our simulated domain. Comparison with scenarios involving greater than 4 patrollers is difficult as the interactions begin to dominate the patrolling greatly, thereby violating the sparse interaction assumption critical to mIRL{sup:⁎}+Int. Each data point shown is the average of 10 runs, leading to standard error between 3.8 seconds for 2 agents to 65.16 seconds for 4 agents.</paragraph><paragraph>In summary, the extensive experimentation on a challenging domain demonstrates that our method for generalizing IRL to multiple observed robots whom may interact yields significant learning improvement on alternate methods. This holds true under a variety of observational conditions with observability falling to as low as just 10%. We show that this leads to a much better performance on the overall task under occlusion, clearly benefiting from the improved learning.</paragraph></section></section><section label="6.2.2"><section-title>Learning Transition Functions</section-title><paragraph>We evaluate mIRL{a mathematical formula}/T⁎+Int for learning the experts' transition functions as previously described in Section 5. mIRL{a mathematical formula}/T⁎+Int begins by estimating the transition functions of others, after which it collapses into mIRL{sup:⁎}+Int learning reward weights θ. Incorrectly identifying and labeling the state-action pairs of I and J may negatively impact the accuracy of the learned transition function, particularly in the presence of limited data due to occlusion. To help alleviate this, mIRL{a mathematical formula}/T⁎+Int uses an iterative approach that utilizes the learned policies of I and J from the previous iteration to help correct observation errors. To accomplish this, the observed trajectory of each robot is updated using the respective learned policies, replacing the perceived action with the one prescribed by the policy for the states in the trajectory. Then, mIRL{a mathematical formula}/T⁎+Int restarts with this newly revised trajectory. This procedure iterates until the labels for the observed state-action pairs do not change because the policies fixate.</paragraph><section><section-title>Transition error models.</section-title><paragraph>A remaining concern is which library of transition error models to use in our domain to complete the transition function. Following the procedure detailed in Section 5.6 we utilize several plausible error models and choose the one with the maximum likelihood given the erroneous transitions in the observed trajectories. The candidate models are:</paragraph><list><list-item label="•">Stop - probability mass not assigned to the intended state is all assigned to the agent's current state.</list-item><list-item label="•">Other Actions - remaining probability mass is divided equally among the intended states for the other actions (not including the one the agent performed).</list-item><list-item label="•">Nearby States - remaining probability mass is divided equally among all states whose center point is less than 1 meter from the starting state's center point.</list-item><list-item label="•">Uniform - remaining probability mass is divided equally among all states (except the intended).</list-item></list><paragraph>In all experiments, the Stop model is found to be exhibiting the maximum likelihood. This is because the most common error mode of the robots pertained to sometimes moving too slow to make it to the next state in the duration of a time step, thereby appearing to “stop” for a time step.</paragraph><paragraph>An additional challenge in the scenario is the possibility that a wheel of one of the patrollers is damaged, requiring L to learn the transition functions of each patroller individually, as illustrated in Fig. 13(a). The wheel was artificially damaged by reducing the requested rotation speed sent to the wheel by half without modifying the planning or navigation software (this involves modifying the velocity message sent to the wheel driver node in ROS from the navigation stack). This simulated a wheel that was internally damaged resulting in increased friction. The result was that any action which involved the “damaged” wheel was slowed, particularly moving forward as this caused the robot to veer to one side which needed repeated correction maneuvers from navigation and turning. A robust test of mIRL{a mathematical formula}/T⁎+Int is whether L can identify the patroller and which of its wheels is damaged.</paragraph></section><section><section-title>Comparative approaches.</section-title><paragraph>We modify the baselines to better evaluate the method.</paragraph><paragraph>• Known R, similar to KnownPolicy, is an upper bound on the performance of L in our scenario. Here, the reward function is known and the transition probability to the intended state is manually set to various probability values because the true transition probabilities of a TurtleBot are not known. Specifically, set {a mathematical formula}TI(s,a,ψ(s,a))=x for all states and actions where {a mathematical formula}x∈{0.7,0.8,0.9,0.95}. Remaining probability mass is assigned to the current state thereby utilizing the Stop error model.</paragraph><paragraph>• Known R{a mathematical formula}/T modifies this method by allowing the transition function to be learned using our approach.</paragraph><paragraph>• Similarly, mIRL{sup:⁎}+Int uses fixed transitions with the probability to the intended state set to various values, and mIRL{a mathematical formula}/T⁎+Int learns them.</paragraph><paragraph>• In DBN{a mathematical formula}EM we model each robot's transitions using a two-step state-action-state dynamic Bayesian network [11] and utilize expectation-maximization [17] to learn the parameters of the network subject to missing data (due to occlusion) [22]. This method represents a benchmark for quickly learning the transition function in the context of missing data.</paragraph><paragraph>• Finally, Random again ignores all observations and performs no learning. It arbitrarily picks a time within the given window to start moving toward the goal.</paragraph><paragraph>L utilizes the following independent binary random variables in ξ as part of {a mathematical formula}TI and {a mathematical formula}TJ:</paragraph><list><list-item label="1.">Rotate left wheel at specified speed, used at all states and for all actions except stop;</list-item><list-item label="2.">Rotate right wheel at specified speed, used at all states and for all actions except stop;</list-item><list-item label="3.">Navigation ability that models the robot's localization, low-level collision avoidance, and motion plan following capabilities in the absence of motion errors, used at all states and for all actions except stop.</list-item><list-item label="4.">Floor slip, used for all states and actions</list-item></list><paragraph>We evaluate the learned behavior accuracy of mIRL{a mathematical formula}/T⁎+Int as the degree of observability and observation time are increased in the ROS-based simulation of the smaller environment. As expected, Fig. 14 shows that the accuracy increases with more data – both more observable states and time – and Known R{a mathematical formula}/T provides an artificial upper bound. Each data point is calculated from at least 100 simulated runs by placing them in blocks of size 15 according to learned behavior accuracy grouped into bins of size 0.05, followed by averaging the success rate over the blocks. Standard error ranges from 0.09 (0.90, Known R{a mathematical formula}/T) to 0.0099 (1.0, Known R{a mathematical formula}/T). Furthermore, the success rate of L correlates strongly with the learned behavior accuracy, again demonstrating the importance of accurate inverse learning toward completion of the task. This property makes the patrolling problem an appealing evaluation testbed for IRL.</paragraph><paragraph>In scenarios where the left wheel of J is damaged (unknown to L) the robot's movement is slowed down as it must compensate in order to move in a straight line. Thus, the overall success rate of L should improve because we expect more opportunities for it to make it to the goal. However, incorrect transition functions, particularly those that overestimate the damaged robot's abilities, may prevent L from taking advantage of this situation. Figure 15(a) displays this trade off in success rates for Known R and mIRL{sup:⁎}+Int: increasing slightly as {a mathematical formula}TI(s,a,ψ(s,a)) increases, then peaking at 0.9 and falling again. The algorithms that learned T outperform the respective algorithms with fixed transition probabilities. We emphasize that {a mathematical formula}TI(s,a,ψ(s,a)) is not the true transition probability of patroller I (and, similarly for J). Therefore, its value does not impact those methods that learn {a mathematical formula}TI and {a mathematical formula}TJ. Also of note, DBN{a mathematical formula}EM performs much worse than Random! High occlusion presents a difficult challenge for this method: instead of randomly missing some data, much of the trajectory is missing, and furthermore the patrollers do not explore all states and actions resulting in the uniform probability distribution being assigned for most transitions. As a result, runs which used DBN{a mathematical formula}EM could not predict the patrollers accurately and most timed out from never finding a valid path to the goal state (100% under occlusion, 90% with no occlusion). In comparison, the timed-out failure rate is under 4% for the other techniques regardless of occlusion.</paragraph><paragraph>In Fig. 15(b), we show the average transition feature probabilities that were learned for both patrollers, {a mathematical formula}qIψ(s,a) and {a mathematical formula}qJψ(s,a), averaged over all state-action pairs, when the left wheel of J is damaged. Importantly, our method correctly found that the left wheel of J has a success probability that is significantly lower than that of the right wheel (Student's 2-tailed t-test, {a mathematical formula}p≪0.001), correctly indicating that the left wheel is the cause of J's reduced transition probabilities. Also visible is the possible correlation among the features, as for instance the learned probability for navigability is significantly lower for J than for I. This is especially pronounced when compared to Fig. 15(c), the probabilities learned when no wheels are damaged, as the transition probabilities for both wheels are nearly equal in this scenario.</paragraph></section><section><section-title>Bernstein polynomial approximation.</section-title><paragraph>We examine the efficacy of the approximation presented in Section 5.5 by comparing its performance to the exact formulation. Using trajectories generated from our patrolling scenario, we calculated the distribution {a mathematical formula}Pr(τ) using the exact and log-convex approximation techniques. The exact method utilized sequential least squares programming as implemented in the SciPy software package and the approximate method was solved with a primal-dual interior point method from the CVXOPT Python package. Approximation error is characterized as the average difference in the probabilities found by the two methods. Across 1,110 trajectories the median average difference was 0.047 with quartiles 0.026 and 0.075.</paragraph><paragraph>The primary advantage of using the convex approximation is scalability as the number of constraints increases. The patrolling scenario utilizes 4 feature random variables and a mapping resulting in 4 {a mathematical formula}q(s,a)I constraints (one per action). To demonstrate scalability, we increased the number of features and mapped them such that there is one {a mathematical formula}qI(s,a) constraint per state (9 features, 4 mapped to each state) and again such that each state-action pair had a unique constraint (12 features, 4 mapped to each state-action pair). This resulted in 124 and 496 constraints, respectively. In Fig. 16, we report the average time our exact and convex approximation algorithms consumed to solve these two larger problems. The approximation method took about 0.005 seconds to solve all problems on average while the exact method ranged from 0.0085 seconds (std. dev. 0.0062) with 4 constraints to 1.02 seconds (std. dev. 1.114 ) with 496 constraints. While all these run times are low in an absolute sense, the results point to a speed up that starts at 1.7 for the smaller problem to about a 200-fold speed up for the larger one.</paragraph><paragraph>Under the assumption that the intended state can be recognized from the observed robot's actions and previous state, our experiments conclusively show that the full transition model (dynamics) can be learned despite high degrees of occlusion. This is beneficial because the actual motion error of robots is often not known, and the learned transition model improves the accuracy of the inversely learned behavior of the expert. This method makes IRL more amenable to pragmatic applications.</paragraph></section></section></section><section label="6.3"><section-title>Evaluations on Physical Robots</section-title><paragraph>We evaluate the performance of mIRL{sup:⁎}+Int and mIRL{a mathematical formula}/T⁎+Int in the context of two TurtleBots physically patrolling the hallway of the smaller environment (Fig. 6(b)) while L, another TurtleBot, observes from a side room. L's field of view is approximately equivalent to a 10% degree of observability. The TurtleBot has a viewing range of 6 meters in the forward direction, beyond which another robot cannot be spotted. Algorithmic parameters matched those used in simulation, such as a run limit of 20 minutes after which the run is aborted.</paragraph><paragraph>As the degree of observability may not be changed in the physical environment without the risk of L being immediately spotted and due to the limited ranges of sensors, we instead vary the observation time and show the resulting success rate obtained by L using mIRL{sup:⁎}+Int. In Fig. 17, we compare the success rate obtained in the physical runs and compare it with those from simulations for the same environment with a 10% degree of observability. Each physical data point comes from 10 runs and as can be seen the success rate improves with more observation time – reaching as high as 60%. Importantly, the difference between the two is not statistically significant, indicating similar performances between our simulations and physical experiments. As such, the simulations in ROS were fairly representative of the physical world complexities.</paragraph><paragraph>Figure 18 shows snapshots of patrollers I and J in an interacting state. We also provide illustrations of our physical experiments with L exiting its vantage point and moving along a path that successfully results in reaching the goal state. An edited video of the physical runs is available at https://youtu.be/kuqd7BG093w.</paragraph><paragraph>Next, we evaluated the performance of mIRL{a mathematical formula}/T⁎+Int with the physical robots similarly to the previous evaluation. An important difference is that J's left wheel is artificially damaged in these physical runs. We compare the success rates of mIRL{a mathematical formula}/T⁎+Int, mIRL{sup:⁎}+Int with transition probabilities fixed, and Random. A transition success rate of 0.9 for mIRL{sup:⁎}+Int was used as this performed best in the simulations. As Table 2 demonstrates, this technique performed better than Random but was itself outperformed by mIRL{a mathematical formula}/T⁎+Int. When the left wheel of J is damaged we observe that the success rate increases across all methods as expected, and mIRL{a mathematical formula}/T⁎+Int again outperforms the other two methods. Each data point is calculated over 10 runs in the environment. While Table 2 represents a total of 60 physical runs, nevertheless, the small number of runs for each method and condition does not permit a statistical significance analyses. However, these results with physical robots are consistent with those in simulation where the differences in performance between the methods are significant. An edited video of the physical runs including the motion of the damaged patroller may be viewed at https://youtu.be/X0HymCtjYh0.</paragraph></section></section><section label="7"><section-title>Related Work</section-title><paragraph>Russell [33] introduced IRL as a way of learning the preferences of an agent by observing its state and actions. Subsequently, Ng and Russell [28] formalized it and suggested using a linear program in which a single expert agent collaboratively provided a complete policy to the learner who is then tasked with finding a reward function that produced the maximum margin of value between this policy and the second best one. Significant milestones in this rapidly emerging research area include modeling the reward function as a linear combination of features [1], utilizing the principle of maximum entropy to eliminate bias from the learned reward function [37], [8], using a Bayesian framework [32], and employing the structure of the MDP to help learn under noisy feature functions [8]. IRL lends itself naturally to a robot learning from demonstrations by a human teacher in controlled environments, and therefore finds application in robot learning from demonstration [3], [13] and imitation learning [29]. As Bayesian updating tends to be gradual and data intensive, we expect the Bayesian approach to IRL to be much less effective under occlusion, where observations are missing. Related to our use of IRL toward penetrating perimeter patrols, IRL has previously yielded good navigation capability for robots in environments shared with humans [19], [20] by first observing how humans navigate crowded environments.</paragraph><paragraph>However, inverse learning in contexts involving multiple agents has received limited attention. One such line of work involves a domain where a single human expert teaches a team of robots [12], [24] and interaction is between the learners, as compared to the experts in our context. In particular, the learners consider each other's actions to facilitate coordination. In Natarajan et al. [26], a centralized controller learns to coordinate vehicles moving through a number of intersections by controlling the traffic lights. The controller is tasked with learning the reward functions of individual agents controlling the traffic lights. Of interest, these agents do not coordinate or interact in any manner. Related to the context of multiple agents is the problem of multi-task IRL involving intertwined demonstrations from multiple experts or demonstrations of multiple tasks by a single expert. Choi and Kim [16] present a non-parametric approach for learning multiple reward functions simultaneously. Our contributions fill an important gap that involves contexts with multiple experts who could be interacting.</paragraph><paragraph>Modeling the interactions of robots has received some attention with Spaan and Melo [34] making use of game theory to model sparse interactions, similarly to our work, as a component of centralized planning for multiple robots to pass each other through a narrow corridor. The problem setting is that of multiagent planning instead of inverse learning, and our work may be viewed as its inverse where we seek to learn which Nash equilibrium was chosen based on observed actions. Valtazanos and Ramamoorthy [36] use demonstrations to learn a set of interactions, which may then be used by the learner robot when interacting with another. This allows it to predict the state that may result due to its actions, and utilize these to shape the actions of a second robot as a response to the learner's actions. Our work differs in that we focus on entire trajectories where the interaction is a (critical) part of the perceived motion; the learner cannot influence the actions of the experts in any way; and we seek to learn the behavior of multiple robots when they are not interacting as well.</paragraph><paragraph>Few other approaches investigate relaxing the prior knowledge of the expert's transition function requirement of IRL. Boularias et al. [7] propose model-free IRL by minimizing the relative entropy between distributions over trajectories generated by a baseline and target policies and utilizing importance sampling. In contrast, we explicitly learn the transition function first under occlusion making this a semi-model based method. Furthermore, to the best of our knowledge, this article is the first to consider IRL in the presence of occluded trajectories. Often a factor when the observed robot is mobile, occlusion significantly complicates methods that advance IRL.</paragraph><paragraph>Bard [5] used a maximum entropy approach to estimate state probabilities when the probabilities associated with aggregates of states are known. Our technique, mIRL{a mathematical formula}/T⁎+Int, is similar to Bard's approach but has several key differences: rather than discrete states we use random variables whose values are unknown during each transition, the presence of occlusion, and our novel extension to learning transition probabilities.</paragraph><paragraph>In general, IRL differs from plan and activity recognition [35], [4] by focusing on learning the underlying reward function of the observed expert, which is often a highly underconstrained problem leading to many solutions. In comparison, plan and activity recognition seek to recognize the plan or activity that is being executed based on a known library of action plans and activities. IRL is closer to intent recognition in its aim with intent being closer in semantics to preferences or rewards. Indeed, learning the preferences of the expert can allow us to recognize its intent and current plan. Therefore, IRL may be seen as an enabling method for plan, activity or intent recognition, and indeed Kitani et al. has utilized it for activity forecasting [21].</paragraph><paragraph>Finally, in the context of our application domain, related research has predominantly focused on generating robot patrolling trajectories that are theoretically difficult to learn from observations by relying on randomized behavior [2]. This direction is motivated by the aim of building robots that could be deployed for perimeter patrolling, which is complementary to our focus of exploring state-of-the-art techniques for building robots that may learn the simple patrolling behavior of multiple other robots or human agents.</paragraph></section><section label="8"><section-title>Concluding Remarks</section-title><paragraph>Occlusion is often unavoidable in scenarios involving mobile robots that are observed. We have shown that this cause of persistent hidden data must be accounted for when performing IRL, which otherwise severely impairs the use of common learning techniques such as expectation-maximization to learn a transition DBN. Limited data from observing physical agents performing their tasks further exacerbates this problem and motivated us to relax model assumptions and generalize IRL in this article. In adversarial settings such as the patrolling domain, an expert may not reveal its actions to the learner. However, in robotic scenarios actions often represent movement. Therefore, the patroller's chosen action is revealed by its movement. For instance, the action for a given time step is inferred from the learner's machine vision readings of a patroller over that time step, i.e., constant forward movement during observations would be interpreted as a move_forward action even if the robot did not arrive at the intended next state (the next grid location in front of it). Given our assumption that the intended next state is available while learning the transition function, one could hypothesize that this knowledge alone is sufficient to accurately model the patrollers' transitions. However, our empirical results do not lend support to this hypothesis. Notice in Fig. 15(a) the significant dip in success rates for both mIRL{sup:⁎}+Int and Known R when the probability of transitioning to the intended state is 0.95, which is very close to deterministic motion. Furthermore, we varied the transition success rate of the patrollers as used in mIRL{sup:⁎}+Int in our physical robot experiments all the way to 100% (intended state results with probability 1). Yet, we obtained the best performance of mIRL{sup:⁎}+Int with the physical robots when this success rate is 90%, as we mention in Section 6.3. Together, our comprehensive experimentation in both simulation and with actual platforms already contains evidence that a deterministic transition model is insufficient, as one expects from robot motion, especially in the context of low-cost mobile platforms such as TurtleBots.</paragraph><paragraph>Our experiments have shown that modeling each observed robot individually and accounting for the sparse interactions between them using an interaction game significantly improves the learning ability for IRL in the presence of multiple robots. Of course, the problem becomes a joint decentralized MDP [18] when interactions are pervasive and extended, and whose solution is usually highly intractable to the order of being doubly exponential in the number of time steps even for just two agents [6]. In comparison, modeling sparse interactions separately kept the explosion in complexity comparatively low at a level where physical experimentation was feasible. The methods in this article are applicable to many real-world applications where agents each have their own separate tasks but must interact briefly with others in close proximity occasionally.</paragraph><paragraph>Finally, we presented a method for inversely learning the partial transition functions of other agents in the presence of limited observations. Our model is applicable to contexts where the transitions of the agent may be attributed to independent underlying components or features that must all function properly for the agent to arrive at an intended state. As the interactions are sparse, the transition probabilities of the multiple observed robots are assumed to be conditionally independent of each other; we may assume locality. The locality can be relaxed by allowing joint actions in {a mathematical formula}TI and {a mathematical formula}ψI thereby mapping state and joint actions to features. Robust evaluations of the methods demonstrated that the dynamics may be learned accurately. In summary, this article contributes important and sophisticated methods that seek to generalize IRL to real-world pragmatics.</paragraph><paragraph>A line of future investigations may center around making more productive use of the observed data in imputing those portions of the trajectories that are occluded. For example, could we form an informed expectation over the various ways in which the hidden state-action pairs may be realized? Doing so has the potential to improve on the methods presented in this article although we expect a concomitant increase in the computational complexity because of the exponential combinations of hidden states and actions.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>We gratefully acknowledge support from NSF grant IIS-0845036, ONR grant N000141310870, and grants from the Toyota Research Institute of North America. Discussions with Brian Ziebart and feedback from seminar participants at the University of Illinois at Chicago, University of Michigan at Ann Arbor, University of Nebraska at Lincoln, Nanyang Technological University, and additionally, comments from several anonymous reviewers, all contributed toward improving this article.</paragraph></acknowledgements><appendices><section label="Appendix A"><paragraph>To conveniently recall the various methods that were introduced in this article and their corresponding labels, we compile them in the table below. The first three methods are used as baselines while the last two serve as upper bounds in the performance comparison.</paragraph><paragraph>{a mathematical formula}</paragraph></section></appendices><references><reference label="[1]">Abbeel, P., Ng, A., 2004. Apprenticeship learning via inverse reinforcement learning. In: ICML. p. 1.</reference><reference label="[2]">Agmon, N., Kraus, S., Kaminka, G. a., may 2008. Multi-robot perimeter patrol in adversarial settings. In: ICRA. pp. 2339–2345.</reference><reference label="[3]">Argall, B. D., Chernova, S., Veloso, M., Browning, B., may 2009. A survey of robot learning from demonstration. Robotics and Autonomous Systems 57 (5), 469–483.</reference><reference label="[4]">Banerjee, B., Lyle, J., Kraemer, L., 2015. The complexity of multi-agent plan recognition. Autonomous Agents and Multi-Agent Systems 29 (1), 40–72.</reference><reference label="[5]">Bard, Y., 1980. Estimation of state probabilities using the maximum entropy principle. IBM Journal of Research and Development 24 (5), 563–569.</reference><reference label="[6]">Bernstein, D. S., Givan, R., Immerman, N., Zilberstein, S., 2002. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research 27 (4), 819–840.</reference><reference label="[7]">Boularias, A., Kober, J., Peters, J., 2011. Relative entropy inverse reinforcement learning. In: AISTATS. pp. 182–189.</reference><reference label="[8]">Boularias, A., Kromer, O., Peters, J., 2012. Structured apprenticeship learning. Machine Learning and Knowledge Discovery in Databases 7524, 227–242.</reference><reference label="[9]">Boyd, S., Vandenberghe, L., 2004. Convex optimization. Cambridge university press.</reference><reference label="[10]">Byrd, R., Lu, P., Nocedal, J., Zhu, C., 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing 16 (5), 1190–1208.</reference><reference label="[11]">Charniak, E., 1991. Bayesian networks without tears. AI Magazine Winter, 51–63.</reference><reference label="[12]">Chernova, S., Veloso, M., 2008. Teaching multi-robot coordination using demonstration of communication and state sharing (short paper). In: AAMAS 2008. pp. 1183–1186.</reference><reference label="[13]">Chernova, S., Veloso, M., 2010. Confidence-based multi-robot learning from demonstration. International Journal of Social Robotics 2 (2), 195–215.</reference><reference label="[14]">Choi, J., Kim, K.-e., 2009. Inverse Reinforcement Learning in Partially Observable Environments. In: IJCAI. pp. 1028–1033.</reference><reference label="[15]">Choi, J., Kim, K.-e., 2011. Inverse reinforcement learning in partially observable environments. Machine Learning Research 12, 691–730.</reference><reference label="[16]">Choi, J., Kim, K.-e., 2012. Nonparametric Bayesian Inverse Reinforcement Learning for Multiple Reward Functions. In: Advances in Neural Information Processing Systems. pp. 314–322.</reference><reference label="[17]">Dempster, A. P., Laird, N. M., Rubin, D. B., 1977. Maximum likelihood from incomplete data via the EM algorithm. J. Roy. Statist. Soc. Ser. B 39 (1), 1–38, with discussion.</reference><reference label="[18]">Goldman, C., Zilberstein, S., 2008. Communication-based decomposition mechanisms for decentralized mdps. Journal of Artificial Intelligence Research 32, 169–202.</reference><reference label="[19]">Henry, P., Vollmer, C., Ferris, B., Fox, D., 2010. Learning to navigate through crowded environments. In: IEEE International Conference on Robotics and Automation (ICRA). pp. 981–986.</reference><reference label="[20]">Herman, M., Gindele, T., Wagner, J., Schmitt, F., Quignon, C., Burgard, W., 2016. Learning high-level navigation strategies via inverse reinforcement learning: A comparative analysis. In: AI 2016: Advances in Artificial Intelligence. pp. 525–534.</reference><reference label="[21]">Kitani, K. M., Ziebart, B. D., Bagnell, J. A., Hebert, M., 2012. Activity forecasting. ECCV, 201–214.</reference><reference label="[22]">Lauritzen, S. L., 1995. The em algorithm for graphical association models with missing data. Computational Statistics and Data Analysis 19, 191–201.</reference><reference label="[23]">Levine, S., Popovic, Z., Koltun, V., 2010. Feature construction for inverse reinforcement learning. Advances in Neural Information Processing Systems, 1342–1350.</reference><reference label="[24]">Martins, M. F., Demiris, Y., 2010. Learning multirobot joint action plans from simultaneous task execution demonstrations. In: 9th International Conference on Autonomous Agents and Multiagent Systems. pp. 931–938.</reference><reference label="[25]">Muller, K., Smola, A., Ratsch, G., 1997. Predicting time series with support vector machines. In: International Conference on Artificial Neural Networks (ICANN). p. 1327.</reference><reference label="[26]">Natarajan, S., Kunapuli, G., Judah, K., Tadepalli, P., Kersting, K., Shavlik, J., 2010. Multi-agent inverse reinforcement learning. In: Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on. IEEE, pp. 395–400.</reference><reference label="[27]">Nelder, J. A., Mead, R., 1965. A simplex method for function minimization. The Computer Journal 7 (4), 308–313.</reference><reference label="[28]">Ng, A., Russell, S., 2000. Algorithms for inverse reinforcement learning. In: ICML. pp. 663–670.</reference><reference label="[29]">Piot, B., Geist, M., Pietquin, O., 2017. Bridging the gap between imitation learning and inverse reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems 28 (8), 1814–1826.</reference><reference label="[30]">Puterman, M. L., 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming, 1st Edition. John Wiley &amp; Sons, Inc., New York, NY, USA.</reference><reference label="[31]">Ramachandran, D., 2007. Bayesian inverse reinforcement learning. In: IJCAI. pp. 2586–2591.</reference><reference label="[32]">Ramachandran, D., Amir, E., 2007. Bayesian inverse reinforcement learning. In: International Joint Conference on AI (IJCAI). pp. 2586–2591.</reference><reference label="[33]">Russell, S., 1998. Learning agents for uncertain environments (extended abstract). In: Eleventh Annual Conference on Computational Learning Theory. pp. 101–103.</reference><reference label="[34]">Spaan, M., Melo, F., 2008. Interaction-driven Markov games for decentralized multiagent planning under uncertainty. In: AAMAS. pp. 525–532.</reference><reference label="[35]">Sukthankar, G., Goldman, R. P., Geib, C., Pynadath, D. V., Bui, H. H. (Eds.), 2014. Plan, Activity, and Intent Recognition. Elsevier.</reference><reference label="[36]">Valtazanos, A., Ramamoorthy, S., 2013. Bayesian interaction shaping: learning to influence strategic interactions in mixed robotic domains. In: AAMAS. pp. 6–10.</reference><reference label="[37]">Ziebart, B. D., Maas, A., Bagnell, J. A., Dey, A. K., 2008. Maximum entropy inverse reinforcement learning. In: AAAI. pp. 1433–1438.</reference></references><footnote><note-para label="1">Polynomials of degree 4 provided the best fit but some of the coefficients were negative.</note-para></footnote></root>