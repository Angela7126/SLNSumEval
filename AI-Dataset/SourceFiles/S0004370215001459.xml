<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370215001459</url><title>A concept drift-tolerant case-base editing technique</title><authors>Ning Lu,Jie Lu,Guangquan Zhang,Ramon Lopez de Mantaras</authors><abstract>The evolving nature and accumulating volume of real-world data inevitably give rise to the so-called “concept drift” issue, causing many deployed Case-Based Reasoning (CBR) systems to require additional maintenance procedures. In Case-base Maintenance (CBM), case-base editing strategies to revise the case-base have proven to be effective instance selection approaches for handling concept drift. Motivated by current issues related to CBR techniques in handling concept drift, we present a two-stage case-base editing technique. In Stage 1, we propose a Noise-Enhanced Fast Context Switch (NEFCS) algorithm, which targets the removal of noise in a dynamic environment, and in Stage 2, we develop an innovative Stepwise Redundancy Removal (SRR) algorithm, which reduces the size of the case-base by eliminating redundancies while preserving the case-base coverage. Experimental evaluations on several public real-world datasets show that our case-base editing technique significantly improves accuracy compared to other case-base editing approaches on concept drift tasks, while preserving its effectiveness on static tasks.</abstract><keywords>Case-based reasoning;Case-base editing;Concept drift;Competence model</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Case-based Reasoning (CBR) is a problem-solving strategy that uses prior experience to understand and solve new problems. Unlike other model-based learning methods, which store past experience as generalized rules and objects, CBR systems store past experience as individual problem solving episodes [1] and delay generalization until problem-solving time. Despite several reported advantages of CBR in the literature [2] (e.g., CBR performs well with disjointed concepts, or CBR, as a lazy learner, is easy to update), large-scale and long-term CBR systems suffer from effectiveness and competence issues [3], which has led to increased awareness of the importance of Case-base Maintenance (CBM).</paragraph><paragraph>Case-base Maintenance refers to the process of revising the contents of a CBR system, thereby improving the system's efficiency and competence [4]. While CBM extends beyond the case-base to include all knowledge containers, in this paper, CBM is restricted to maintenance of the case-base only. In CBM, methods of revising the case-base (also called case-base editing) involve reducing the size of a case-base or training set while endeavoring to preserve or even improve generalization accuracy [5]. Two distinct areas in case-base editing have been identified and investigated: 1) competence enhancement, which aims to remove noisy cases, thereby increasing classifier accuracy; 2) competence preservation, which corresponds to redundancy reduction, i.e., removing superfluous cases that do not contribute to classification competence. Although several hybrid methods exist that search for a small subset of the training set, and simultaneously achieve the elimination of noisy and superfluous cases, competence enhancement and competence preservation methods can be combined to achieve the same objectives as hybrid methods [6].</paragraph><paragraph>The issue of concept drift refers to the change of distribution underlying the data [7], [8]. More formally, the issue of concept drift can be framed as follows: If we denote the feature vector as x and the class label as y, then a data stream will be an infinite sequence of {a mathematical formula}(x,y). If the concept drifts, it means that the distribution of {a mathematical formula}p(x,y) between the current data chunk and the yet-to-come data is changing. If we decompose {a mathematical formula}p(x,y) into the following two parts as {a mathematical formula}p(x,y)=p(x)×p(y|x), we can say that there are two sources of concept drift: one is {a mathematical formula}p(x), which evolves with time t, and can also be written as {a mathematical formula}p(x|t), and the other is {a mathematical formula}p(y|x), the conditional probability of feature y given x[9]. Learning under concept drift is considered to be one of the most challenging problems in machine learning research [10]; it is consequently the subject of increased attention [11].</paragraph><paragraph>At present, approaches for handling concept drift can be generally divided into three categories [12]: 1) instance selection (window-based), where the key idea is to select the instances that are most relevant to the current concept. Many case-base editing strategies in CBR – including this research – that delete noisy, irrelevant or redundant cases are also a form of instance selection. Since this research focuses on case-based techniques in dynamic environments where concept drift may occur, we will give a more detailed review of CBM methods in Section 2; 2) instance weighting, where each instance is assigned a weight to represent the decreasing relevance of existing training examples. These instances can be weighted according to their “age”, or their competence with regard to the current concept [13]. Research into this category [14], [15] mainly focuses on exploring a proper weighting schema; 3) ensemble learning (learning with multiple models) is reported to be the most popular and successful approach for dealing with concept drift [16]. It utilizes multiple models by voting [17], [18] or selecting the most relevant model to construct an effective predictive model [19]. Generally, there are two ensemble frameworks: 1) horizontal ensemble [20], [21], which builds on a set of buffered data chunks; 2) vertical ensemble [9], [22], which builds on the most recent data chunk only. More recently, an aggregate ensemble framework, which could be seen as a hybrid approach of the two, has been proposed [23]. There is also research that maintains ensembles with different diversity levels [24], [25]. The key idea behind this kind of research is that “before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent of the type of drift” [26].</paragraph><paragraph>Although concept drift has been an active research area in machine learning, little effort has been made in respect of CBM-related research. In real world applications, the case-base accumulates with usage over time, and the case distribution as well as the decision concepts underlying the cases may be subject to continuous change. This phenomenon poses additional challenges for existing case-base editing methods that implicitly assume that existing cases are drawn from a fixed yet unknown distribution, i.e., stationary assumption [22]. When concept drift occurs, past cases may not reflect current concepts; as a consequence, current case-base editing methods may preserve harmful cases. In addition, when class boundaries shift or novel concepts emerge, new cases representing novel concepts are more likely to be treated as noise and removed by competence enhancement algorithms, because they conflict with past concepts. This may seriously delay or even prohibit a case-based learner from learning new concepts. Finally, redundancy reduction is particularly challenging in domains with evolving decision boundaries. Most current competence preservation methods preserve only cases that lie on the decision boundaries. We argue that this is too aggressive and is inappropriate, particularly in the concept drift environment, for the following reasons: First, it will destroy the original case distribution, thus affecting the result of any change detection method which directly compares the case distribution. Second, it makes a learner too sensitive to noise, i.e., incorrectly retaining noisy cases as novel concepts, at the center of class definitions, will dramatically affect classification boundaries. Last, because “boundary cases distinguish one concept from another, while typical cases characterize the concept they belong to [27]”, preserving a certain number of typical cases may also help to improve a CBR system, e.g., by improving case explanation. Motivated by the above-mentioned issues, while recognizing that it is not possible to know in advance whether there will be concept drift, this research proposes a novel case-base editing method that addresses both competence enhancement and competence preservation, and works well in both static and changing environments.</paragraph><paragraph>In this paper, we present a new case-base editing technique which takes both competence enhancement and competence preservation into consideration. For competence enhancement, we develop a Noise-Enhanced Fast Context Switch (NEFCS) algorithm to prevent noise from being included during case retention and to speed the context switch process in the face of concept drift. By taking advantage of our previous research on concept drift detection [28], our NEFCS algorithm minimizes the risk of discarding novel concepts. For competence preservation, we invent a Stepwise Redundancy Removal (SRR) algorithm that uniformly removes superfluous cases without loss of case-base competence. Experimental evaluations based on public real-world datasets show that our case-base editing technique demonstrates significant improvements in time-varying tasks and exhibits good performance on static tasks compared with the most common case-base editing methods.</paragraph><paragraph>This paper is organized as follows. Section 2 reviews the established literature on case-base editing techniques, as well as the research in CBR for tackling concept drift. Section 3 presents our proposed two-stage case-base editing technique. Section 4 evaluates each proposed algorithm and the overall technique for handling concept drift, using real datasets of both static and concept drift tasks. Section 5 summarizes our conclusions.</paragraph></section><section label="2"><section-title>Literature review</section-title><paragraph>In this section, we first briefly explain some of the proposed competence models as a prerequisite, because many case-base editing solutions have taken a new direction in the sense that they are guided by explicit models of competence. We then review the established literature on case-base editing. Lastly, we describe the main developments in handling concept drift in CBR, and identify the limitations of the current research.</paragraph><section label="2.1"><section-title>Competence models</section-title><paragraph>Competence is a measurement of how well a CBR system fulfils its goals. As CBR is a problem-solving methodology, competence is usually taken to be the proportion of problems that it can solve successfully. In the CBR community, competence measurement has been given much attention and many competence models [5], [29], [30], [31] have been proposed. These proposed models have now become essential tools for use in all stages of system development and are reported to be particularly important for CBM, where knowledge is added, deleted and modified to affect system adaptation and improvement [32].</paragraph><paragraph label="Definition 3">The first study on competence measures was conducted by Smyth and Keane [29], in which the local competence of an individual case was characterized by its coverage and reachability. The coverage of a case is the set of target problems that this case can solve. The reachability of a target problem is the set of cases that can be used to provide a solution for the target problem. Since it is impossible to enumerate all possible future target problems, in practice the case-base itself is assumed to be a representative sample of the underlying distribution of target problems. As a result, the coverage of a case is estimated by the set of cases that can be solved by its retrieval and adaptation (Definition 1), and the reachability of a case is estimated by the set of cases that can bring about its solution (Definition 2). (See [29].) For a case-base {a mathematical formula}CB={c1,c2,⋯,cn}, given a case {a mathematical formula}c∈CB, {a mathematical formula}CoverageSet(c)={c′∈CB:Solves(c,c′)}, where {a mathematical formula}Solves(c,c′) means that c can be retrieved and adapted to solve {a mathematical formula}c′.(See [29].) {a mathematical formula}ReachabilitySet(c)={c′∈CB:Solves(c′,c)}, where {a mathematical formula}Solves(c′,c) means that {a mathematical formula}c′ can be retrieved and adapted to solve c.(See [30].) For {a mathematical formula}c1,c2∈CB,SharedCoverage(c1,c2) iff {a mathematical formula}[RelatedSet(c1)∩RelatedSet(c2)]≠ϕ, where {a mathematical formula}RelatedSet(c)=CoverageSet(c)∪ReachabilitySet(c).</paragraph><paragraph label="Definition 4">Smyth and Keane's competence models characterize what a case contributes to its local competence. However, arguing that it is these retrieved cases that should be blamed for misclassifications, Delany and Cunningham [5] extended Smyth and Keane's competence models by introducing a liability set (Definition 4) to measure the effectiveness or certainty of a case's competence contribution. (See [5].) {a mathematical formula}LiabilitySet(c)={c′∈CB:Misclassifies(c′,c)}, where {a mathematical formula}Misclassifies(c′,c) means that case c contributes in some way to the incorrect classification of target case {a mathematical formula}c′.</paragraph></section><section label="2.2"><section-title>Case-base editing methods</section-title><paragraph>This section presents a literature review on both competence enhancement and competence preservation aspects of case-base editing.</paragraph><section label="2.2.1"><section-title>Competence enhancement</section-title><paragraph>Competence enhancement aims to remove noisy instances to increase classifier accuracy. Some research that selects a set of appropriate cases and results in more suitable decision boundaries can also be viewed as a form of competence enhancement. One of the earliest noise removal techniques is Wilson's noise reduction technique [33]. In their proposed Edited Nearest Neighbor (ENN) algorithm [33], a case that is incorrectly classified by its nearest neighbors will be deleted from the original case-base as noise. To detect and delete small clusters of noisy cases, Tomek [34] made two modifications to ENN: the Repeated ENN (RENN) algorithm repeatedly applies ENN to the case-base until no more cases can be deleted, while the “all k-NN” algorithm increases the values of k during each cycle of RENN. These two modifications can improve classification accuracy; however, there is a risk of removing entire small clusters which represent genuine concepts [35]. Ferri and Vidal [36] integrated the ENN with cross-validation, which randomly splits the training set into n-folds and then iteratively removes cases from each fold that cannot be correctly classified by cases from other folds. According to the Structural Risk Minimization (SRM) principle, Karacali and Krim [37] proved that to construct an NN classifier on a reference set, one should aim for as few points as possible, while maintaining zero classification error on the training dataset. They developed a case-base editing method which simultaneously minimizes the noise incurred and the case-base redundancy. Inspired by the fact that most classification errors occur in the regions where the domains of the two classes are closest to each other, their method first considers the nearest pairs of points from different classes. Delany and Cunningham [5] criticized methods based on Wilson's noise reduction technique [33] because a misclassified case may not necessarily be a noisy case but could be classified incorrectly due to the retrieved cases which contribute to its classification. They extended Smyth's competence model [30] to include the liability set and proposed a Blame-Based Noise Reduction (BBNR) algorithm which reviews all cases that have contributed to misclassifications and removes a case if its deletion results in no loss of coverage. Pan et al. [38] invented a Kernel Greedy Case Mining (KGCM) algorithm which extracts a given number of most representative cases. This greedy algorithm operates in an extracted feature space where the intra-class scatter around target classes is minimized while the inter-class scatter is maximized. The selected cases aim to maximize a defined global diversity and therefore are widespread within the feature space. Another extension of the competence model is the complexity model [39], in which a case with complexity greater than a given threshold means that its neighborhoods contain a majority of different classes, therefore the case is treated as noise. Rather than proposing a novel case-base editing method, Cummins and Bridge [40] developed a framework which combines existing noise reduction algorithms and redundancy reduction algorithms in sequence or through a set of voting rules. By trying all possible combinations, they were able to create several highly effective composite case-base editing methods.</paragraph></section><section label="2.2.2"><section-title>Competence preservation</section-title><paragraph>Competence preservation corresponds to redundancy reduction which aims to remove superfluous cases that do not contribute to classification competence. Hart's Condensed Nearest Neighbor (CNN) approach [41] is probably the earliest and best-known redundancy reduction technique which incrementally adds cases that cannot be correctly classified by the current edited case-base to an empty case-base. CNN makes multiple passes through the original case-base until no more additions can be made. Although intuitive, CNN has been criticized for being sensitive to the order of cases examined and to noise [5]. To overcome these problems, several modifications to CNN have been made, such as the Reduced Nearest Neighbor (RNN) [42], the Selective Nearest Neighbor (SNN) [43], the Modified CNN (MCNN) [44], the Generalized CNN (GCNN) [45], the Fast CNN (FCNN) [6] and the Improved CNN (ICNN) [46]. Other approaches to case editing build a competence model of the training data and use the competence properties of the cases to determine which cases to include in the edited set. Smyth and Keane [29] developed a footprint deletion policy which divides cases into four categories by their competence contributions and then removes cases from a case-base, guided by the categories, until a limit on the case-base size is reached. McKenna and Smyth [47] proposed a family of competence-guided methods based on different combinations of four features: an ordering policy, an addition rule, a deletion rule and a competence update policy. Although they empirically showed that their method preserves the competency of a CBR system and outperforms a number of previous deletion-based strategies, their deletion policy was criticized by Zhu and Yang [48] for not always being able to guarantee the competence preserving property. Instead, Zhu and Yang suggested a case addition policy which recursively selects a case that leads to maximum coverage benefit if added into an initially empty case-base until a limit on the case-base size is reached. Besides the advantage of being able to achieve the maximized case-base coverage, they also proved that their algorithm can place a lower bound on the coverage of the resulting case-base. Another competence-based case-base editing method is the Iterative Case Filtering Algorithm (ICF) [49], in which a case with a coverage set size smaller than its reachability set size is believed to be far from the class borders and is removed. Because noisy cases were protected from removal by their deletion rule, Brighton and Mellish adopted Wilson's deletion rule [33] ahead of their ICF algorithm. Arguing that existing competence preservation techniques were very aggressive in their pruning of cases, Delany and Cunningham [5] presented the Conservative Redundancy Reduction (CRR) algorithm, which adopts a similar principle to ICF [49] but removes redundant cases by repeatedly selecting the case with the smallest coverage set which has not yet been covered.</paragraph></section></section><section label="2.3"><section-title>Case-based reasoning for handling concept drift</section-title><paragraph>The first attempt to handle concept drift with a case-based technique is the Instance-Based learning Algorithm 3 (IB3) [50], which monitors each case's accuracy and retrieval frequency. IB3 prevents noisy cases by deferring the inclusion of an instance in the case-base until it is proven reliable by means of a confidence interval test, where the number of successful classifications is assumed to be binomially distributed. A case already included in the case-base is permanently removed if its accuracy is significantly less than its class's observed frequency. However, IB3 has been criticized for being suitable only for gradual concept drift, and for its costly adaptation process [8].</paragraph><paragraph>The Locally Weighted Forgetting (LWF) algorithm [51], which reduces the weights of the k-nearest neighbors (k-NN) of a new case and discards a case if its weight falls below a threshold θ, was believed to be one of the best adaptive learning algorithms in its time. However, the LWF algorithm has lower asymptotic classification in non-varying conditions. Klinkenberg [52] also showed in his experiments that instance weighting techniques tend to over-fit the data, thus performing worse than analogous instance selection techniques.</paragraph><paragraph>Salganicoff [53] introduced the Prediction Error Context Switching (PECS) algorithm in his research, which achieved good performance in both time-varying and static tasks. The PECS algorithm was similar to IB3 in the sense that it also tracks each case's accuracy and adopts the same confidence interval test to determine noise, although PECS does not normalize its lower bound confidence interval with respect to the overall observed frequency of the class. Despite this, the PECS algorithm differs from IB3 in several ways: First, any new observation is immediately included in the case-base. Second, the PECS algorithm calculates a case's accuracy based only on its latest l predictions. Third, rather than permanently deleting a case as noise, the PECS algorithm deactivates it and tracks its accuracy for reactivation purposes. Experiments show that the PECS algorithm improves robustness over IB3 on time-varying tasks at the cost of increased storage requirements. However, PECS was originally designed to improve performance in concept drift problems, where noise along with incoming observations was not considered. As a result, all noisy observations are retained first and can only be removed with a deferment. PECS has also been criticized for its unlimited memory assumption [54], by only disabling cases but not deleting them.</paragraph><paragraph>Delany et al. [55] suggested a two-level learning for handling concept drift. In level-1, they used a Competence-Based Editing (CBE) method [5], which is a hybrid of Blame Based Noise Removal (BBNR) and Conservative Redundancy Reduction (CRR) to manage the case-base periodically. Specifically, BBNR analyzes all cases that have contributed to misclassification and removes cases if their deletion results in no coverage loss; CRR repeatedly selects a case with the smallest coverage set that cannot be correctly solved. The authors compared their method with full case-base without management, as well as window-based updating, and demonstrated several improvements. However, a hybrid of two CBM methods designed for a static environment does not guarantee effective learning under concept drift. In the worst case, novel concepts can be consistently discarded, especially with gradual concept drift. In addition, the BBNR algorithm has difficulties in removing small groups of noisy cases. For example, two noisy cases that have each other in their coverage sets can correctly classify each other but can cause the misclassification of all other nearby cases; these small groups of noisy cases can never be removed by the BBNR, even though they continue to provide incorrect classification results. This phenomenon can be caused by outdated cases when concept drift occurs. Last but not least, BBNR neglects the competence model maintenance issue. Referring to an ill-matched competence model may lead to the mistaken preservation of noisy cases, i.e., if we have previously removed a noisy case {a mathematical formula}c′ that happens to be a member of the coverage set c that we are currently considering. If we do not keep the competence model up-to-date, {a mathematical formula}c′ will still be considered even though we know it is a noisy case. In level-2 learning, Delany et al. periodically reselected features to completely rebuild a CBR system. Level-2 learning [55] is beyond the scope of a case-base editing method, but it is a model rebuilding strategy that can be plugged into any instance selection technique. Later studies conducted by Delany and Bridge [56] suggested a feature-free distance measure which showed further improvement in accuracy on the same datasets in their experiments.</paragraph><paragraph>Beringer and Hüllermeier [54] presented an Instance-Based Learning on Data Streams (IBL-DS) algorithm that autonomously controls the composition and size of the case-base. This IBL-DS algorithm is based on three modification rules: 1) when the size of the case-base exceeds its limit, the oldest instance will be removed; 2) when concept drift is reported using Gama's detection method [57], a large number of instances will be deleted in a spatially uniform but temporally skewed way. The number of cases to be removed depends on the discrepancy between the minimum error rate and the error rate of the 20 most recent classifications; 3) for every retained new case whose class dominates in a test range, all neighbors in a candidate range that belong to a different class will be removed. Although Beringer and Hüllermeier's method is instructive in that it differentiates its instance selection strategy based on whether concept drift is reported, their instance deletion strategy might be difficult to implement in problem domains such as spam filtering where all features may be binary. Removing cases temporally may also result in loss of case-base competence, e.g., deleting rare but correct cases.</paragraph><paragraph>Indrė Žliobaitė [58] invented a family of training set formation methods named FISH (uniFied Instance Selection algoritHm). The FISH family dynamically selects a set of relevant instances as the training set for the current target instance. The key concept of the FISH family is to linearly combine distances in time and feature space for training set selection. It includes three modifications: FISH1, FISH2 and FISH3. In FISH1, the size of a training set is fixed and set in advance. FISH2, which is considered to be central to the family, varies the size by selecting a training set that achieves the best accuracy based on leave-one-out cross validation. In FISH3, the relative importance of time and space distance is determined through an additional loop of cross validation. Although the FISH family is reported to be capable of cooperating with any base classifier, the validation set is chosen based on nearest neighbors to the current target instance.</paragraph><paragraph>Other research discusses the cost and availability of new labeled training data and proposes methods for handling concept drift with limited labeled instances [59], [60], [61]. There is a clear difference between the research in this paper and these researches, however. The present research primarily focuses on a case-base editing (instance selection) technique that continuously improves or at least preserves the effectiveness and efficiency of a classifier in unknown situations (e.g., whether concept drift occurs, the type of concept drift), when it is provided with newly labeled cases. Experiments to investigate the effect of the availability and the number or proportion of newly labeled cases are not performed.</paragraph></section></section><section label="3"><section-title>A new case-base editing approach</section-title><paragraph>Traditional case-base editing methods either omit the evolving nature of many real world scenarios by implicitly making a stationary assumption [9] or are incapable of simultaneously addressing effectiveness and competence issues. Motivated by these two problems, we present a two-stage CBM process. In Stage 1 learning (enhancement), we propose a NEFCS algorithm, which targets the removal of noise in a dynamic environment. In Stage 2 learning (preservation), we propose a SRR algorithm, which removes redundant cases in a recursively uniform way while preserving case-base coverage. We evaluate both proposed case-base editing methods respectively using real-world data on static tasks and time-varying tasks, as well as evaluating the overall learning performance.</paragraph><section label="3.1"><section-title>Problem description</section-title><paragraph>Our technique is related to both CBM and data stream classification under concept drift. We assume there is a case-based system monitoring a live stream of new instances, where new labeled instances can be retained for future reasoning. Since concept drift may occur and noise may persist, we want to endow this case-based system with the ability to learn and refine accordingly, while restricting its case-base size rather than allowing it to expand limitlessly. Our solution consists of three modules: 1) competence-based drift detection [28], which compares the case distribution between two sliding windows of recent cases. When concept drift is detected, it also identifies the competence area where the distribution changes most significantly; 2) Noise-Enhanced Fast Context Switch (NEFCS), which enhances the system's learning capacity under concept drift; 3) Stepwise Redundancy Removal (SRR) method which controls the size of the case-base to tackle the performance issue.</paragraph><paragraph>The overall CBM process is shown in Fig. 1. For each new labeled instance, Drift Detection performs a competence-based concept drift detection [28]. NEFCS adapts the case-base on the basis of concept drift detection, thereby continuously improving the system's classification accuracy during its use. When the case-based system exceeds its storage limit, SRR will be triggered to shrink the case-base while preserving the case distribution as much as possible.</paragraph></section><section label="3.2"><section-title>Competence-based drift detection</section-title><paragraph label="Theorem 1">Our CBM strategy is motivated by the idea that when there is no concept drift, old cases can help to identify noise and improve accuracy; on the other hand, when concept drift is evident, new instances are more representative of the novel concept, while old cases that conflict may be obsolete. A change detection module is therefore adopted in our proposed CBM approach. This section reviews and extends the theoretical foundation of the competence-based drift detection method [28], which not only detects a concept drift but also identifies competence regions in which the concept drifts most severely. (See [28].) Given a case base CB, and the case samples {a mathematical formula}Si⊆CB{a mathematical formula}(i=1,2). For any Related Set{a mathematical formula}r∈ℜCB(Si), denote {a mathematical formula}ℜ={r}, we define the density of r with regard to {a mathematical formula}Si as{a mathematical formula} where {a mathematical formula}ℜCB(Si) is the set of all related sets which contain at least one case {a mathematical formula}c∈Si, i.e., Related Closure of {a mathematical formula}Si.(See [28].) Given a case base CB, and the case sample sets {a mathematical formula}Si⊆CB{a mathematical formula}(i=1,2), denote the power set of {a mathematical formula}ℜCB(CB) as {a mathematical formula}℘(ℜCB(CB)). Considering {a mathematical formula}℘(ℜCB(CB)) as the measurable space {a mathematical formula}A, for {a mathematical formula}A∈A, we define the competence-based empirical weight of {a mathematical formula}Si with regard to A over CB as{a mathematical formula} In essence, the competence-based drift detection method partitions the problem space into a group of overlapping related sets, and then estimates the empirical probability over a competence area represented by A – a sub-set of {a mathematical formula}ℜCB(CB) through competence-based empirical weight. The higher the weight is, the larger is the proportion of cases in S that support the selected competence area −A.For two case sample sets {a mathematical formula}S1,S2⊆CB and a Related Set{a mathematical formula}r∈ℜCB(S1)∪ℜCB(S2), letting {a mathematical formula}ℜ={r}, we define the RelatedSet-based empirical distance of r between {a mathematical formula}S1 and {a mathematical formula}S2 as{a mathematical formula}With the definition of the RelatedSet-based empirical distance, the competence-based empirical distance between{a mathematical formula}S1and{a mathematical formula}S2,{a mathematical formula}dCB(S1,S2)=supA∈A⁡|S1CB(A)−S2CB(A)|can be computed as{a mathematical formula}where{a mathematical formula}</paragraph><paragraph label="Proof">For each {a mathematical formula}A∈A, we have{a mathematical formula} Thus, to obtain the supremum of {a mathematical formula}|S1CB(A)−S2CB(A)| with regard to A, we should let A be either{a mathematical formula} Correspondingly, the competence-based empirical distance between {a mathematical formula}S1 and {a mathematical formula}S2 is{a mathematical formula} On the other hand,{a mathematical formula} Thus {a mathematical formula}∑r∈PdrCB(S1,S2)=∑r∈QdrCB(S1,S2). □</paragraph><paragraph label="Definition 8">Compared with the competence-based empirical distance[28], which defines the distance between two sample sets, the Relatedset-based empirical distance defines the distance between two sample sets on a particular relatedset. Theorem 1 proves that the competence-based empirical distance is the sum of the relatedSet-based empirical distance over all partitions (related sets) that are in favor of either {a mathematical formula}S1 or {a mathematical formula}S2 (with higher weight). If a set of related sets{a mathematical formula}ℜp⊆{r∈ℜCB(S1)∪ℜCB(S2):drCB(S1,S2)&gt;0} meets the following three conditions{a mathematical formula} where {a mathematical formula}0≤p≤1, then each element in {a mathematical formula}ℜp is called top-p-competence area.</paragraph><paragraph>Since the largest RelatedSet-based empirical distance occurs in the top-p-competence areas, we treat these areas as the identified concept drift competence areas, where the most severe concept drift is believed to be occurring. To illustrate this, we visualize a simple moving circles dataset in Fig. 2, and depict the top-10%-competence areas identified in Fig. 3.</paragraph><paragraph>Clearly, we can see that the identified competence areas roughly highlight the exact problem space in which concept drift occurs. Also note that a small region has been selected in the top left corner, even though no real concept drift is taking place. This region has been identified because a significant increase in negative class has been detected, i.e., virtual concept drift. This phenomenon can be caused by bias in sampling. The results of both detection and description can be greatly improved if more representative samples are used for change detection. In addition, it worth pointing out that the smaller the p-value, the more confidence there will be in the identified areas, although fewer competence areas will be picked. Finally, in practice, areas undergoing real concept drift can be identified by translating the competence space back to the feature space and picking feature space where there is an emerging trend of positives in one sample set and an emerging trend of negatives in the other sample set.</paragraph><paragraph>Although any change detection technique can be adopted to accomplish this task, we choose the competence-based drift detection method [28], which uses the competence model as a space partition technique and compares the empirical distribution of two case windows. The main reason for adopting the competence-based change detection method is that it cannot only warn of possible concept drift but it can also highlight a small region of the problem space in terms of case-base competence, called the identified competence area (Definition 8), which is most likely to be affected by concept drift. This enhances one of the strengths of CBR for dealing with disjointed concepts, and also makes CBR more appropriate for dealing with local concept drift.</paragraph></section><section label="3.3"><section-title>Noise-enhanced fast context switching</section-title><paragraph>Our NEFCS method takes the results of competence-based change detection as input (whether there is a concept drift, and the identified competence area in which concept drift is detected), and aims to continuously improve the learning capability. NEFCS consists of three main processes: 1) Modified blame-based noise reduction (M-BBNR), 2) Context switching, and 3) Update competence model.</paragraph><section label="3.3.1"><section-title>Modified blame-based noise reduction</section-title><paragraph label="Definition 10">Modified blame-based noise reduction (M-BBNR) can prevent a novel concept from being removed as noise based on the result of whether and where there is concept drift, which solves the problem of distinguishing noise from novel concepts. Case {a mathematical formula}c′∈CB can be solved by CB (denoted as {a mathematical formula}Solves(CB,c′)) if there is a case {a mathematical formula}c∈CB, {a mathematical formula}c′≠c, so that {a mathematical formula}c′∈CoverageSet(c).Conditional BBNR ruleIf concept drift has been detected, a new case c will be safely deleted according to the BBNR rule, i.e.{a mathematical formula} only when c lies out of any identified concept drift competence area (i.e. {a mathematical formula}∀r∈ℜp{a mathematical formula}c∉r).</paragraph><paragraph>M-BBNR applies the conditional BBNR rule to examine every new case c and determine whether it is noise. If there is concept drift and c lies in the identified concept drift competence areas (Definition. 8), c will be not be deleted, because it may represent a novel concept. Otherwise, c will be removed from the case-base if it fulfills the BBNR rule [5]. This helps to differentiate between new instances and prevent the possible inclusion of noise. If c is removed, then the next process (Context switching) will be skipped, otherwise only existing cases which conflict with c will be checked (Definition 11), which accelerates the process of learning with new concepts.</paragraph><paragraph label="Theorem 2">Let us denote the set of new cases that are deleted according to the Conditional BBNR rule as ℵ. If a concept drift has been detected, the size of ℵ is non-increasing when increasing p value which is used to define the concept drift competence areas inDefinition 8.</paragraph><paragraph label="Proof">In addition, let NCB be the set of new cases that fulfill the BBNR rule, then it is trivially true that {a mathematical formula}ℵ=NCB when {a mathematical formula}p=0. Assume {a mathematical formula}{r∈ℜCB(S1)∪ℜCB(S2):drCB(S1,S2)&gt;0}={r1,r2,…,rn}, and sort its elements (related sets) in descending order as {a mathematical formula}ri1,ri2,…,rin according to their RelatedSet-based empirical distances. To discover all the top-p-competence areas that meet the three conditions shown in Definition 8, we successively put these sorted related sets into {a mathematical formula}ℜp starting with the first one {a mathematical formula}ri1 until the sum of RelatedSet-based empirical distances of all elements in {a mathematical formula}ℜp is greater than or equal to{a mathematical formula} Given {a mathematical formula}0≤p1&lt;p2≤1, we have{a mathematical formula} then clearly {a mathematical formula}ℜp2⊇ℜp1, that is, for any related set {a mathematical formula}r∈ℜp1, {a mathematical formula}r∈ℜp2 as well. For each case {a mathematical formula}c∈NCB, if it is in one identified concept drift competence area corresponding to {a mathematical formula}p1, that is, there is a certain {a mathematical formula}r∈ℜp1 so that {a mathematical formula}c∈r, then this case must be in one identified concept drift competence area corresponding to {a mathematical formula}p2 as well, since {a mathematical formula}r∈ℜp2. This means that if one case does not fulfill the Conditional BBNR rule when {a mathematical formula}p=p1, then it must also not fulfill it when {a mathematical formula}p=p2, so the size of ℵ is non-increasing when the p value is increased from {a mathematical formula}p1 to {a mathematical formula}p2.In particular, when {a mathematical formula}p=0, {a mathematical formula}p∑r∈ℜCB(S1)∪ℜCB(S2)drCB(S1,S2)=0. Because Condition 3 of Definition 8 cannot be met in any way. Thus {a mathematical formula}ℜp=∅ and none of the cases in NCB can lie in any area of {a mathematical formula}ℜp, so {a mathematical formula}ℵ=NCB. □</paragraph><paragraph label="Definition 11">The conditional BBNR rule imposes an additional condition on BBNR rule [5], i.e., any new case c will only be considered as noise and removed by the BBNR rule if c lies outside an identified concept drift competence area (does not represent a new concept). It is one of the reported strengths of the competence-based change detection method that it is able to identify small competence regions where concepts drift most severely. When working with other change detection methods which have no capacity to locate concept drifts, all new cases will be exempt from removal, which is also the case when {a mathematical formula}p=0 in Definition 8. Case {a mathematical formula}c′∈CB is defined as a conflicting case by a new case c which is not in ℵ, if {a mathematical formula}c∈LiabilitySet(c′).</paragraph><paragraph>To prevent unnecessary cost, the M-BBNR investigates only cases that conflict with c (Definition 11), instead of checking the entire case-base as the BBNR does. This largely improves the efficiency of the competence enhancement stage, because these retrieved cases have already been obtained when trying to solve c, therefore no additional case retrieval is required. Moreover, any case of the later window will be excluded from this list when there is concept drift if it lies in the identified competence area. There are two main reasons to exclude these cases: 1) there is still a chance that c may be noise, and the most recent cases in the same area can help to alleviate the effect; 2) to prevent the deletion of other recently retained cases which represent novel concepts. Lastly, every conflicting case will be examined for noise using the BBNR rule and will be removed if it fulfills this criterion.</paragraph><paragraph>The BBNR algorithm is given in Algorithm 1. We have made a minor modification to the original BBNR algorithm [5] in line-5 to ensure that no previously removed cases will be taken into coverage consideration.</paragraph></section><section label="3.3.2"><section-title>Context switching</section-title><paragraph>As BBNR has difficulty in removing groups of obsolete cases, NEFCS also refers to the effectiveness information to deal with concept drift. To track changes in accuracy due to concept drift without experiencing bias from a large number of historical observations derived from another concept, a shift register, which stores the latest l retrievals, is kept for each case as a form of effectiveness information. As a result, the shift register that keeps the latest l predictions of each retrievable case should be updated for each observed new case c, where a retrievable element {a mathematical formula}c′ to c means {a mathematical formula}sim(c,c′)≥sim(c,ck′), where {a mathematical formula}ck′ is the kth nearest neighbor to c.</paragraph><paragraph>NEFCS adopts the same confidence interval test used in the IB3 algorithm [50] and the PECS algorithm [53] to switch cases. This yields a confidence interval as calculated by (1). If the upper bound on the accuracy of a case c falls below the inactivation threshold {a mathematical formula}pmax, it is deactivated from future reasoning. However, this same example may eventually be moved back to the case-base if its lower bound accuracy rises above the agreement acceptance probability {a mathematical formula}pmin, as may the case when concepts drift cyclically.{a mathematical formula} where {a mathematical formula}pi is the calculated accuracy of {a mathematical formula}ci and n is the number of classification attempts of {a mathematical formula}ci. z is the confidence interval coefficient (either tabulated or computed).</paragraph></section><section label="3.3.3"><section-title>Update competence model</section-title><paragraph>This process is purely to ensure that the M-BBNR is referencing the correct competence model, i.e., the competence model is updated for case-addition [62] and removed cases will be purged from competence models.</paragraph></section></section><section label="3.4"><section-title>Stepwise redundancy removal</section-title><paragraph>We adopt a similar schema to CNN [41] to guide our redundancy removal algorithm for competence preservation, that is, to obtain a sub-set of the original case-base that can successfully solve all removed cases. In this section, we will explain the SRR algorithm with the k-nearest neighbors (k-NN) rule, which makes it easier to apply to the nearest neighbor (NN) rule.</paragraph><paragraph>A major difference between SRR and other deletion-based competence preservation methods is that SRR recursively removes redundant cases in a competently uniform way, which we believe is more suitable for the concept drift problem. A similar approach used to handle the concept drift problem can be found in IBL-DS [54], which removes cases in a spatially uniform way. However, deleting cases uniformly in the feature space could be difficult to implement in high-dimensional data, such as spam filtering. In addition, IBL-DS suffers from the problem of losing case-base competence. Therefore, we suggest the uniform removal of redundant cases in the competence space. In addition, by taking advantage of the competence models, our method does not require multiple passes through the whole case-base, which saves the time incurred as a result of repeated case retrievals, as required in many competence preservation algorithms.</paragraph><paragraph>SRR starts by ordering the cases decreasingly by their reachability set, where a large reachability set indicates that a case is far from the class border [49] and works in a recursive manner. While running, SRR maintains three structures:</paragraph><list><list-item label="1.">a preserved list – pL, which stores cases that cannot be removed;</list-item><list-item label="2.">a locked list – oL, which prevents a case from being deleted in the current round and will be cleared at the beginning of the next round;</list-item><list-item label="3.">a linked list for each case – kL, which links several previously removed cases to a case that solves them.</list-item></list><paragraph> Stepwise Redundancy Removal works in a recursive manner. During each round, SRR continuously examines an unlocked case c with the largest reachability set until all cases are locked or preserved. A case c will be removed if c and all cases in the linked list (kL) of c can still be correctly solved without c. When c is removed, the closest {a mathematical formula}(k+1)/2 cases that solve c are locked and linked to c. {a mathematical formula}(k+1)/2 is chosen as the number of cases to lock because it is the minimum number of cases required to secure a correct classification of c, therefore no case-base competence will be lost by discarding c. In addition, each case {a mathematical formula}ci′ in the kL of c will be added to the kL of the closest {a mathematical formula}(k+1)/2 cases that solve {a mathematical formula}ci′. Fig. 4 depicts an example in which case A is removed by the nearest neighbor rule. First, cases in the remaining case-base that can solve case A as well as cases {a mathematical formula}c1,c2 are retrieved. As a result, cases B, C, and D are retrieved. Such retrievals are instant because of the competence model, i.e., ReachabilitySet. Successful retrieval means that case A and all cases to which case A links can still be solved by the remaining case-base without case A; therefore A can be safely removed (no competence loss) and case B will be locked temporarily (added to the oL list). Then, any case in the linked list of case A will be added to the kL of the corresponding case that solves it. Failure to retrieve any case of B, C, or D means that case A cannot be removed; therefore case A will be preserved. SRR will move to the next round when there are no more unlocked cases, and will stop automatically when all cases have been preserved. However, SRR can be stopped at any time since it ensures that all removed cases can be correctly solved at any time (no coverage loss). SRR is presented in Algorithm 2.</paragraph><paragraph>Clearly, the preserved list – pL ‘preserves’ essential cases whose removal will cause competence loss, i.e., lead to failure to solve removed cases. In addition, pL helps to speed up the redundancy removal process, because cases in pL will be excluded from inspection in further rounds which avoids the cost of repeatedly examining the same essential case in every round. The locked list – oL – ensures that redundant cases will be deleted in a competently uniform way. For each case deleted, a list of cases that solve the deleted case will be locked to protect them from being removed during the current round. This prevents cases in a particular region from being cleaned quickly, even if they are at the center of class definitions. The linked list – kL – is the key to ensuring that there is no loss in competence for any deletion during multiple rounds. Since SRR removes cases gradually, having a subset of the case-base that correctly solves all cases at the beginning of each round cannot guarantee that this subset will solve all cases of the original case-base. For example, during the first round, case A is removed, and case B, which solves case A, is locked. During the next round, if case B is removed simply because case C, which solves case B, can be retrieved, there will be a risk of failing to solve case A. Therefore, for any case c, SRR maintains a kL to store cases for which c is retrieved to solve. To claim a case c can be removed, it must be possible to correctly solve c and all cases to which c links by the remaining case-base.</paragraph><paragraph>The differences between SRR and existing competence preservation methods are dramatic. First, existing methods take either a case addition or case deletion approach, while the approach in SRR is more like a hybrid of the two, because SRR can also “add” a case and prevent it from being deleted through the preserved list. Second, SRR uniformly removes redundancy through its locking mechanism. This not only facilitates case explanation [27] but also makes the learning of novel concepts smoother, without leaving a large blank in the feature space. Third, for each deletion, SRR guarantees that there will be no case coverage loss. As a result, SRR can be stopped at any time when the size limit is fulfilled, which gives the decision maker much more control over the size of the edited case-base.</paragraph><paragraph>We illustrate the redundancy reduction process of SRR during each loop with simple circle data [63] in Fig. 5, where star and circle indicate two different classes. We assume the 3-NN rule is used for classification.</paragraph><paragraph>We compare the deletion process with another iterative redundancy reduction algorithm, ICF [49], in Fig. 6 using the same dataset. Note that in Fig. 5, Fig. 6, only three rounds were performed and visualized; more cases can be removed if the process is continued.</paragraph><paragraph>It can be seen that both methods remove redundancy gently and gradually. However, SRR operates in a uniform way, without leaving a large gap in the case of concept drift that may seriously affect the classification boundary.</paragraph><paragraph>To illustrate that SRR preserves its effectiveness for static tasks, we also compare the results with NUN-CNN, which sorts cases in ascending order of the nearest unlike neighbor (NUN) distance [64], ICF [49] and CRR [5]. Fig. 7 shows the results of minimized case-bases, i.e., no further cases can be deleted. It can be seen that when the case-base size is minimized for static tasks, SRR also focuses on defining decision boundaries because SRR shares the same principle as CNN [41]. However, the competently uniform approach taken by SRR exhibits a significant advantage (particularly where concept drift takes place) with better coverage while cases are being deleted. In addition, SRR can be stopped at any time, while the process of addition-based algorithms like NUN-CNN [64] and CRR [5] is completely uncontrollable. This means that SRR also exhibits the potential to intelligently adjust the case-base size in the face of concept drift.</paragraph></section></section><section label="4"><section-title>Experimental evaluation</section-title><paragraph>This section provides a comprehensive experimental evaluation of the proposed two-stage CBM approach. In this section, we first evaluate the NEFCS algorithm and the SRR algorithm separately as independent CBM methods targeting competence enhancement and competence preservation respectively. We then evaluate the proposed two stage CBM approach for handling concept drift. All evaluations are based on real-world datasets.</paragraph><section label="4.1"><section-title>Evaluating NEFCS</section-title><paragraph label="Experiment 1">Competence enhancement algorithms designed for static tasks, such as BBNR and ENN, can be applied directly to each training set. However, for NEFCS, PECS and IB3, which remove noise during their classification processes, we perform leave-one-out-classification to edit the training set. That is, for each case in the training set, we first remove it from the training set or de-activated list and then try to classify it with each algorithm's successive learning mechanism. By doing this, we are able to initialize the de-activated list and classification accuracy for each case in advance.To construct the competence model, a case c is considered to solve a case {a mathematical formula}c′ if {a mathematical formula}sim(c,c′)&gt;sim(c′,cnun′), where {a mathematical formula}cnun′ is the nearest unlike neighbor (NUN) of {a mathematical formula}c′. This is the same method of constructing the competence model as described in ICF [49]. To compare case distribution, the window size selected is half the training set size, that is, 450, so that change detection can be started from the outset. We build a separate competence model using leave-one-out-classification with cases in the two windows for change detection purposes, where a case c is considered to solve a case {a mathematical formula}c′ if {a mathematical formula}sim(c,c′)&gt;sim(c′,cnun′), where {a mathematical formula}cnun′ is the NUN of {a mathematical formula}c′, and c is one of the retrieved cases for solving {a mathematical formula}c′. There are three reasons for us to use a different way of constructing the competence model for change detection purposes: 1) While performing change detection, the competence model should reflect the current competence contribution of each case. While performing case-base editing, the competence model should reflect the maximum contribution of each case, since cases may be removed at any time. 2) We want to restrict the region of SharedCoverage (Definition 3) and focus each case's contribution on its closest and most related competence region. 3) It avoids re-searching the whole case-base to determine local competence. Using the actual retrieved cases can speed up the process of updating the competence model, thus facilitating online change detection.As the problem space is very sparse, the confidence level chosen for confidence intervals is 80% for IB3, PECS and NEFCS. Other parameters required by PECS and NEFCS are set empirically, with {a mathematical formula}l=10, {a mathematical formula}pmax=pmin=0.5. The reason for this choice is that if a case provides more correct classifications than incorrect classifications, it will be added to the case-base, otherwise it will be de-activated. We did not test different parameter choices since NEFCS chooses the same parameters as PECS, and we believe this makes the comparison sufficiently fair.We used the same classifier as Delany et al. [5] for all implemented techniques; that is, k-NN with {a mathematical formula}k=3 using unanimous voting, where an email is classified as spam if and only if all three retrieved emails are spam.Finally, we force the FISH2 algorithm to start with a training set of 200 instances. Because the problem space is very sparse, starting with an extremely small training size {a mathematical formula}N=k makes the original FISH2 quite unsuitable for the spam filtering domain. Changing the starting size from {a mathematical formula}N=3 to {a mathematical formula}N=200 significantly improves its performance across all datasets; however a starting training size that is too large will contravene the original idea of FISH2, therefore 200 is chosen.The results of comparing NEFCS with BBNR, PECS, FISH2, ENN, IB3, Full CB with and without update across the five datasets, and the overall average results of all datasets, are shown in Fig. 8 and Fig. 9 respectively. We also show percentage values for Err, FPR and FNR in both figures.The results can be summarized as follows:</paragraph><list>In general, competence-based methods (NEFCS, BBNR) need extra time for training, mainly because of building competence models. By contrast, on-line learning methods (NEFCS, PECS, IB3) need more time for classification, because they embed logic during the classification process to continuously improve effectiveness. However, it is rather unfair to make a straight comparison, because the training process is completely different and NEFCS is used in the reasoning process rather than being run once on the training set. In real world applications, the execution time is the actual time required to classify and learn incrementally for each new case. The extra execution time required by NEFCS implies a possible limitation of our proposed algorithm in application domains that require very fast performance. However, because the execution time of case-based algorithms depends heavily on the size of the case-base, performing redundancy reduction can help to improve the overall efficiency.</list><paragraph label="Experiment 2">Spam filtering, concept driftTo evaluate how NEFCS behaves in real-world time-varying tasks, we choose to conduct studies on two concept drift spam filtering datasets [55] that can be downloaded from http://www.comp.dit.ie/sjdelany/Dataset.htm. Each dataset consists of more than 10,000 emails (spam or legitimate) collected over a period of approximately one year. A training set of 1000 cases (500 spam/500 legitimate), is given for each dataset. The remaining data is used to test our algorithm over time. Table 2 shows a summary of the test data across each month for both datasets.Similar to Spam Filtering – static, each email {a mathematical formula}ei is represented by a vector of features {a mathematical formula}ei=〈x1,x2,⋯,xn〉, where each feature is binary, and we use the same similarity as described in Experiment 1.Since error rate is not a good metric for skewed datasets, the most common effectiveness metrics [66] for spam filtering are used to evaluate effectiveness, where Legitimate Recall (LR) is defined as {a mathematical formula}LR=TNTN+FP=TNN, and Legitimate Precision (LP) is defined as {a mathematical formula}LP=TNTN+FN, where FP means legitimate emails that are incorrectly classified as spam, and FN means spam emails that are incorrectly classified as legitimate.In this experiment, we again compare our algorithm with two closely related concept handling methods, IB3 [50] and PECS [53], as well as other well-known competence enhancement methods designed for static tasks, ENN [33] and BBNR [5], and the Full Base which performs no noise removal at all. Similar to Experiment 1, all the algorithms compared in this experiment, with the exception of IB3 and NEFCS, immediately retain a new case after classification, which leads to significantly better results. ENN and BBNR are applied monthly to remove any resulting noisy cases, because it is not feasible to check the whole case-base after every single case retention. We do not make a comparison with the original work of Delany et al. [55], since their proposed CBE algorithm is a hybrid method of both competence enhancement and competence preservation. We intentionally leave this comparison for Section 4.3.We construct the competence model in the same way as described in Experiment 1. To detect concept drift, we adopt two sliding windows. Each window contains all the emails received during the last 30 days. As a result, we do not detect concept drift in the first two months and assume there is no concept drift. Lastly, we use the same classifier and parameters as described in Experiment 1. The classification results for each month for both datasets are shown respectively in Fig. 10 and Fig. 11.From these results, we conclude that:</paragraph><list><list-item label="•">NEFCS improves the overall results on both LR and LP for dataset 1 over all the algorithms compared.</list-item><list-item label="•">To compare multiple algorithms over multiple datasets, we perform the same statistical tests as in Experiment 1. The result indicates a significant difference in LP at a confidence level of 99%, as well as a significant difference in LR at a confidence level of 95% across all compared algorithms. Again, by setting NEFCS as the control classifier, the Bonferroni–Dunn test reports a significant improvement in LP over ENN {a mathematical formula}(z&gt;10), IB3 {a mathematical formula}(z&gt;10), Full CB {a mathematical formula}(z=7.9), and PECS {a mathematical formula}(z=3.7) at a confidence level of 95%, and a significant improvement in LR over IB3 {a mathematical formula}(z=4.6) at a confidence level of 95%.</list-item><list-item label="•">Although the Bonferroni–Dunn post-hoc test is not able to pick up the difference between NEFCS and BBNR, a separate Wilcoxon signed-rank test comparing NEFCS and BBNR reports a significant improvement in LP at a confidence level of 95%, with no loss in LR (the difference is not statistically significant).</list-item><list-item label="•">NEFCS mainly improves LP. This is because we are using k-NN with unanimous voting. As a result, mistakenly retaining one or two noisy spam emails in the same competence area will not affect the classification result for a legitimate email; however, mistakenly retaining a legitimate email will dramatically affect the classification result for a spam email.</list-item><list-item label="•">NEFCS is less affected by unknown concept drift and can quickly react to it. The proof is that there is a decreasing proportion of legitimate email from May to July for dataset 1, and a sudden fall in the proportion of legitimate email in July for dataset 2. As a result, all algorithms suffer from a loss of LP during these periods. NEFCS catches this trend and performs the best of all the algorithms compared during these periods, even improving the LP in July for dataset 1.</list-item><list-item label="•">NEFCS lies between PECS and IB3 by selectively retaining new cases. Experimental evaluation reveals that this strategy is better when coping with concept drift than either retaining or de-activating all new cases.</list-item></list><paragraph label="Experiment 3">In this experiment, Learn++.NSE achieves the best overall accuracy, followed by NEFCS and BBNR; Since there is no significant difference between the compared algorithms, and on the other hand the dataset bears no clear information about when concept drift occurs, only the overall accuracy is given for this experiment.</paragraph></section><section label="4.2"><section-title>Evaluating SRR</section-title><paragraph label="Experiment 4">From the above table, we conclude that competence-based redundancy removal algorithms exhibit a visible advantage over NUN-CNN because they avoid the repeated cost of searching the case-base by referring to competence models. The execution time grows exponentially as the size of the case-base grows, which is mainly the result of the increasing number of case comparisons required to generate competence models. In addition, ICF is much slower than CRR and SRR, because as an iterative algorithm, ICF constantly needs to rebuild competence models. By contrast, SRR avoids this cost by maintaining its three unique lists. Finally, although CRR is slightly faster than SRR, the two algorithms are very close in execution time.</paragraph></section><section label="4.3"><section-title>Evaluating overall approach</section-title><paragraph>In this section, we will first evaluate our proposed NEFCS-SRR approach on two public online artificial concept drift datasets (http://www.win.tue.nl/~mpechen/data/DriftSets/) to reveal its behavior towards different types of concept drift (e.g., sudden or gradual). We then conduct experiments on real world data and compare our NEFCS-SRR approach with the most popular CBM algorithms as well as a recent ensemble approach [18].</paragraph><paragraph label="Experiment 5">SEA concepts, sudden concept driftThere are four blocks of data with different concepts. Each block contains 2500 random points of three-dimensional feature space. The three features have values randomly generated in the range {a mathematical formula}[0;10), and only the first two features are relevant. In each block, a data point belongs to class 1, if {a mathematical formula}f1+f2≤θ, where {a mathematical formula}f1 and {a mathematical formula}f2 represent the first two features, and θ is a threshold value for the two classes. Threshold values for the four data blocks are 8, 9, 7 and 9.5 in sequence. 10% class noise is introduced into each block of data by randomly changing the class value of 10% of instances.The experiment is set up as follows: The first 500 data points from the first block are used as the training set. The remaining 2000 points, together with the other three blocks, are classified and learnt incrementally. The window size is chosen as 500 points. The case-base size limit is set to 1000, except for FISH2 and Learn++.NSE which do not perform redundancy removal. For CBE [55] and ICF [49], new instances are learnt immediately, while a noise removal process is performed every 500 points. With the k-NN rule, where {a mathematical formula}k=5, we record classification accuracy for every 500 points, therefore a sudden concept drift will occur at the 5th, 10th and 15th records respectively. Fig. 12 reports the accuracy of the different algorithms evaluated in this experiment.From these results, we conclude that:</paragraph><list><list-item label="•">There is a significant difference by Friedman test {a mathematical formula}(p&lt;0.01) in classification accuracy across all the algorithms compared. By setting NEFCS-SRR as the control classifier, we conduct the Bonferroni–Dunn test as a post-hoc test. The result reveals a significant improvement over NEFCS at a confidence level of 95% ({a mathematical formula}z=2.74). Therefore, we claim that the proposed SRR algorithm helps to improve the learning capability for sudden concept drift.</list-item><list-item label="•">Although there is no statistical difference between NEFCS-SRR, FISH2 and CBE, visually we notice that just as sudden concept drift occurs (on the 5th, 10th and 15th records), NEFCS-SRR experiences a sharper drop in accuracy than FISH2 and CBE. This is probably because NEFCS-SRR adopts a concept drift detection algorithm and re-acts differently whether or not this is a reported concept drift. Therefore, before a concept drift can be detected, NEFCS-SRR will try to modify the case-base to preserve the previous learnt concept. However, NEFCS-SRR recovers after the concept drift with a higher slope than CBE and eventually achieves the best overall accuracy.</list-item></list><paragraph label="Experiment 7">Comparing NEFCS-SRR, CBE and ICF, we find that although ICF also adopts a competence-guided approach to the removal of redundant cases, the way in which it works may generate competence holes when concept drift occurs. Therefore, a conservative competence preservation method is recommended.</paragraph></section></section><section label="5"><section-title>Conclusion and further study</section-title><paragraph>Case-base Editing is an important aspect of CBM research that adopts an instance selection approach for handling concept drift. Editing the case-base properly can continuously tune a case-based learner according to concept drift.</paragraph><paragraph>In this paper, we first present a competence enhancement method – NEFCS, to prevent noise from being included and to quickly adapt the case-base according to concept drift by the safe removal of obsolete cases. Instead of blindly trusting all the most recent cases, NEFCS incorporates a concept drift detection method to differentiate the treatment on new cases based on whether there is concept drift. In addition, as an on-line learning algorithm, NEFCS can take immediate advantage of any system feedback, enabling a fast response to possible concept drift. Since it is almost impossible to increase the case-base size without limit, we have also proposed a competence preservation method – SRR – to restrict the storage requirement. Two main features of SRR are that: First, SRR tries to preserve the case distribution during each iteration, which is very important for concept drift problems, e.g., to prevent competence holes, to aid concept drift detection. Second, as a competence-guided method, SRR preserves the competence of the case-base, i.e., it ensures that any removed instance can still be solved by the remaining case-base.</paragraph><paragraph>Several interesting findings have been revealed through our experiments: 1) FISH2 is not suitable for static tasks, because it forces to take into account the time factor whether or not concept drift occurs; 2) When sudden concept drift occurs, a certain degree of redundancy removal can help the learner to adapt more quickly to a novel concept; 3) Before concept drift can be detected, our proposed NEFCS-SRR tries to preserve previously learnt concepts; therefore compared to sudden concept drift it is believed to be more suitable for gradual concept drift; 4) In the spam filtering domain, where the problem space is very sparse, a competence-based instance selection technique has a clear advantage over other instance selection techniques; 5) Although each method behaves very differently based on the dataset, our proposed NEFCS-SRR approach constantly reports good overall accuracy across the datasets compared, and we therefore believe it is more general than the other methods compared; 6) Finally, in term of efficiency, all competence-based methods require extra time for maintaining the competence models; this makes a competence-based method inappropriate for applications that require real time decisions, such as network intrusion detection where an extremely high volume of packages arrives every second. However, for spam filtering, the effect of a little extra time is trivial.</paragraph><paragraph>From this research, we conclude that: 1) differentiating new cases according to drift detection is better than retaining or discarding all new cases indiscriminately; 2) a competence-guided, controllable, and conservative method is preferable to a non-competence-guided, uncontrollable, and unbalanced method of conducting competence preservation for time-varying tasks.</paragraph><paragraph>Instead of either retaining or removing a case completely, our next attempt will target the discovery of a fuzzy case weighting schema which will better balance the decision as to whether a case is noisy or whether it represents a novel concept. In addition, it would be interesting to consider our work in relation to the machine learning field, and to discover whether it is possible to plug our instance selection technique into other learning models.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>The work presented in this paper is supported by the Australian Research Council (ARC) under Discovery Project DP150101645. We thank Weigang Zhao, Fang Dong and Anjin Liu for assistance with experiments. We would also like to show our gratitude to the editors and anonymous reviewers for comments that have greatly improved the manuscript.</paragraph></acknowledgements><references><reference label="[1]"><authors>R. Lopez de Mantaras,D. McSherry,D. Bridge,D. Leake,B. Smyth,S. Craw,B. Faltings,M.L. Maher,M.T. Cox,K. Forbus,M. Keane,A. Aamodt,I. Watson</authors><title>Retrieval, reuse, revision and retention in case-based reasoning</title><host>Knowl. Eng. Rev.20 (3)(2005) pp.215-240</host></reference><reference label="[2]"><authors>P. Cunningham,N. Nowlan,S.J. Delany,M. Haahr</authors><title>A case-based approach to spam filtering that can track concept drift</title><host>ICCBR'03 Workshop on Long-Lived CBR SystemsTrondheim, Norway, Jun 24(2003)</host></reference><reference label="[3]"><authors>M.K. Haouchine,B. Chebel-Morello,N. Zerhouni</authors><title>Competence-preserving case-deletion strategy for case-base maintenance</title><host>9th European Conference on Case-Based ReasoningECCBR '08, Trier, Germany, Sep. 1–4(2008) pp.171-183</host></reference><reference label="[4]"><authors>D. Wilson,D.B. Leake</authors><title>Maintaining case-based reasoners: dimensions and directions</title><host>Comput. Intell.17 (2)(2001) pp.196-213</host></reference><reference label="[5]"><authors>S.J. Delany,P. Cunningham</authors><title>An analysis of case-base editing in a spam filtering system</title><host>7th European Conference on Case Based ReasoningECCBR '04, Madrid, Spain, Aug. 30–Sep. 2(2004) pp.128-141</host></reference><reference label="[6]"><authors>F. Angiulli</authors><title>Fast nearest neighbor condensation for large data sets classification</title><host>IEEE Trans. Knowl. Data Eng.19 (11)(2007) pp.1450-1464</host></reference><reference label="[7]"><authors>G. Widmer,M. Kubat</authors><title>Effective learning in dynamic environments by explicit context tracking</title><host>6th European Conference on Machine LearningECML '93, Vienna, Austria, Apr. 5–7(1993) pp.227-243</host></reference><reference label="[8]"><authors>G. Widmer,M. Kubat</authors><title>Learning in the presence of concept drift and hidden contexts</title><host>Mach. Learn.23 (1)(1996) pp.69-101</host></reference><reference label="[9]"><authors>P. Zhang,X. Zhu,Y. Shi</authors><title>Categorizing and mining concept drifting data streams</title><host>14th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningKDD '08, Las Vegas, NV, USA, Aug. 24–27(2008) pp.812-820</host></reference><reference label="[10]"><authors>Q. Yang,X. Wu</authors><title>10 challenging problems in data mining research</title><host>Int. J. Inf. Technol. Decis. Mak.5 (4)(2006) pp.597-604</host></reference><reference label="[11]"><authors>M. Scholz,R. Klinkenberg</authors><title>Boosting classifiers for drifting concepts</title><host>Intell. Data Anal.11 (1)(2007) pp.3-28</host></reference><reference label="[12]"><authors>C.-J. Tsai,C.-I. Lee,W.-P. Yang</authors><title>Mining decision rules on data streams in the presence of concept drifts</title><host>Expert Syst. Appl.36 (2)(2009) pp.1164-1178</host></reference><reference label="[13]">A. TsymbalThe problem of concept drift: definitions and related workTechnical report TCD-CS-2004-15<host>(2004)Trinity College DublinIreland</host></reference><reference label="[14]"><authors>Y. Li,Y. Wei,A. Kolesnikova,W.D. L</authors><title>A new gradual forgetting approach for mining data stream with concept drift</title><host>International Symposium on Information Science and EngineeringISISE '08, Shanghai, China, Dec. 20–22(2008) pp.556-559</host></reference><reference label="[15]"><authors>Y. Koren</authors><title>Collaborative filtering with temporal dynamics</title><host>15th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningKDD '09, Paris, France, June 28–July 1(2009) pp.447-456</host></reference><reference label="[16]"><authors>W. Qu,Y. Zhang,J. Zhu,Q. Qiu</authors><title>Mining multi-label concept-drifting data streams using dynamic classifier ensemble</title><host>1st Asian Conference on Machine LearningACML '09, Nanjing, China, Nov. 2–4(2009) pp.308-321</host></reference><reference label="[17]"><authors>A. Bifet,G. Holmes,B. Pfahringer,R. Kirkby,R. Gavaldà</authors><title>New ensemble methods for evolving data streams</title><host>15th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningKDD '09, Paris, France, June 28–July 1(2009) pp.139-148</host></reference><reference label="[18]"><authors>R. Elwell,R. Polikar</authors><title>Incremental learning of concept drift in nonstationary environments</title><host>IEEE Trans. Neural Netw.22 (10)(2011) pp.1517-1531</host></reference><reference label="[19]"><authors>R. Klinkenberg,T. Joachims</authors><title>Detecting concept drift with support vector machines</title><host>17th International Conference on Machine LearningICML '00, Stanford, CA, USA, June 29–July 2(2000) pp.487-494</host></reference><reference label="[20]"><authors>N. Street,Y. Kim</authors><title>A streaming ensemble algorithm (SEA) for large-scale classification</title><host>7th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningKDD '01, San Francisco, CA, USA, Aug. 26–29(2001) pp.377-382</host></reference><reference label="[21]"><authors>J.Z. Kolter,M.A. Maloof</authors><title>Using additive expert ensembles to cope with concept drift</title><host>22nd International Conference on Machine LearningICML '05, Bonn, Germany, Aug. 7–11(2005) pp.449-456</host></reference><reference label="[22]"><authors>J. Gao,W. Fan,J. Han</authors><title>On appropriate assumptions to mine data streams: analysis and practice</title><host>7th IEEE International Conference on Data MiningICDM '07, Omaha, NE, USA, Oct. 28–31(2007) pp.143-152</host></reference><reference label="[23]"><authors>P. Zhang,X. Zhu,Y. Shi,X. Wu</authors><title>An aggregate ensemble for mining concept drifting data streams with noise</title><host>13th Pacific-Asia Conference on Knowledge Discovery and Data MiningPAKDD '09, Bangkok, Thailand, Apr. 27–30(2009) pp.1021-1029</host></reference><reference label="[24]"><authors>L.L. Minku,X. Yao</authors><title>DDD: a new ensemble approach for dealing with concept drift</title><host>IEEE Trans. Knowl. Data Eng.24 (4)(2012) pp.619-633</host></reference><reference label="[25]"><authors>V. Attar,P. Chaudhary,S. Rahagude,G. Chaudhari,P. Sinha</authors><title>An instance-window based classification algorithm for handling gradual concept drifts</title><host>7th International Workshop on Agents and Data Mining InteractionAMDI '11, Taipei, Taiwan, May 2–6(2012) pp.156-172</host></reference><reference label="[26]"><authors>L.L. Minku,A.P. White,X. Yao</authors><title>The impact of diversity on online ensemble learning in the presence of concept drift</title><host>IEEE Trans. Knowl. Data Eng.22 (5)(2010) pp.730-742</host></reference><reference label="[27]"><authors>J. Zhang</authors><title>Selecting typical instances in instance-based learning</title><host>9th International Workshop on Machine LearningML '92, Aberdeen, Scotland, UK, July 1–3(1992) pp.470-479</host></reference><reference label="[28]"><authors>N. Lu,G. Zhang,J. Lu</authors><title>Concept drift detection via competence models</title><host>Artif. Intell.209 (2014) pp.11-28</host></reference><reference label="[29]"><authors>B. Smyth,M.T. Keane</authors><title>Remembering to forget: a competence-preserving case deletion policy for case-based reasoning systems</title><host>14th International Joint Conference on Artificial IntelligenceIJCAI '95, Montreal, Quebec, Canada, Aug. 20–25(1995) pp.377-382</host></reference><reference label="[30]"><authors>B. Smyth,E. McKenna</authors><title>Competence models and the maintenance problem</title><host>Comput. Intell.17 (2)(2001) pp.235-249</host></reference><reference label="[31]"><authors>R. Pan,Q. Yang,S.J. Pan</authors><title>Mining competent case bases for case-based reasoning</title><host>Artif. Intell.171 (16–17)(2007) pp.1039-1068</host></reference><reference label="[32]"><authors>A. Smiti,Z. Elouedi</authors><title>Overview of maintenance for case based reasoning systems</title><host>Int. J. Comput. Appl.32 (2)(2011) pp.49-56</host></reference><reference label="[33]"><authors>D.L. Wilson</authors><title>Asymptotic properties of nearest neighbor rules using edited data</title><host>IEEE Trans. Syst. Man Cybern.2 (3)(1972) pp.408-421</host></reference><reference label="[34]"><authors>I. Tomek</authors><title>Two modifications of CNN</title><host>IEEE Trans. Syst. Man Cybern.6 (11)(1976) pp.769-772</host></reference><reference label="[35]"><authors>D.R. Wilson,T.R. Martinez</authors><title>Reduction techniques for instance-based learning algorithms</title><host>Mach. Learn.38 (3)(2000) pp.257-286</host></reference><reference label="[36]"><authors>F.J. Ferri,E. Vidal</authors><title>Small sample size effects in the use of editing techniques</title><host>11th IAPR International Conference on Pattern RecognitionICPR '92, Hague, Netherlands, Aug. 30–Sep. 3(1992) pp.607-610</host></reference><reference label="[37]"><authors>B. Karaçalı,H. Krim</authors><title>Fast minimization of structural risk by nearest neighbor rule</title><host>IEEE Trans. Neural Netw.14 (1)(2003) pp.127-137</host></reference><reference label="[38]"><authors>R. Pan,Q. Yang,J.J. Pan,L. Li</authors><title>Competence driven case-base mining</title><host>20th National Conference on Artificial IntelligenceAAAI'05, Pennsylvania, USA(2005) pp.228-233</host></reference><reference label="[39]"><authors>S. Craw,S. Massie,N. Wiratunga</authors><title>Informed case base maintenance: a complexity profiling approach</title><host>22nd AAAI Conference on Artificial IntelligenceAAAI-07, Vancouver, British Columbia, Canada, July 22–26(2007) pp.1618-1621</host></reference><reference label="[40]"><authors>L. Cummins,D. Bridge</authors><title>Maintenance by a committee of experts: the MACE approach to case-base maintenance</title><host>8th International Conference on Case-Based ReasoningICCBR'09, Seattle, USA, July 20–23(2009) pp.120-134</host></reference><reference label="[41]"><authors>P.E. Hart</authors><title>The condensed nearest neighbor rule</title><host>IEEE Trans. Inf. Theory14 (3)(1968) pp.515-516</host></reference><reference label="[42]"><authors>G.W. Gates</authors><title>The reduced nearest neighbor rule</title><host>IEEE Trans. Inf. Theory18 (3)(1972) pp.431-433</host></reference><reference label="[43]"><authors>G. Ritter,H. Woodruff,S. Lowry,T. Isenhour</authors><title>An algorithm for a selective nearest neighbor decision rule</title><host>IEEE Trans. Inf. Theory21 (6)(1975) pp.665-669</host></reference><reference label="[44]"><authors>V.S. Devi,M.N. Murty</authors><title>An incremental prototype set building technique</title><host>Pattern Recognit.35 (2)(2002) pp.505-513</host></reference><reference label="[45]"><authors>C.-H. Chou,B.-H. Kuo,F. Chang</authors><title>The generalized condensed nearest neighbor rule as a data reduction method</title><host>18th International Conference on Pattern RecognitionICPR '06, Hong Kong, China, Aug. 20–24(2006) pp.556-559</host></reference><reference label="[46]"><authors>X. Hao,C. Zhang,H. Xu,X. Tao,S. Wang,Y. Hu</authors><title>An improved condensing algorithm</title><host>7th IEEE/ACIS International Conference on Computer and Information ScienceICIS'08, Portland, OR, USA, May 14–16(2008) pp.316-321</host></reference><reference label="[47]"><authors>E. McKenna,B. Smyth</authors><title>Competence-guided case-base editing techniques</title><host>5th European Workshop on Case-Based ReasoningEWCBR '00, Trento, Italy, Sep. 6–9(2000) pp.235-257</host></reference><reference label="[48]"><authors>J. Zhu,Q. Yang</authors><title>Remembering to add: competence-preserving case-addition policies for case-base maintenance</title><host>16th International Joint Conference on Artificial IntelligenceIJCAI '99, Stockholm, Sweden, July 31–August 6(1999) pp.234-239</host></reference><reference label="[49]"><authors>H. Brighton,C. Mellish</authors><title>Advances in instance selection for instance-based learning algorithms</title><host>Data Min. Knowl. Discov.6 (2)(2002) pp.153-172</host></reference><reference label="[50]"><authors>D.W. Aha,D. Kibler,M.K. Albert</authors><title>Instance-based learning algorithms</title><host>Mach. Learn.6 (1)(1991) pp.37-66</host></reference><reference label="[51]"><authors>M. Salganicoff</authors><title>Density-adaptive learning and forgetting</title><host>10th International Conference on Machine LearningICML '93, Amherst, MA, USA, June 27–29(1993) pp.276-283</host></reference><reference label="[52]"><authors>R. Klinkenberg</authors><title>Learning drifting concepts: example selection vs. example weighting</title><host>Intell. Data Anal.8 (3)(2004) pp.281-300</host></reference><reference label="[53]"><authors>M. Salganicoff</authors><title>Tolerating concept and sampling shift in lazy learning using prediction error context switching</title><host>Artif. Intell. Rev.11 (1)(1997) pp.133-155</host></reference><reference label="[54]"><authors>J. Beringer,E. Hüllermeier</authors><title>Efficient instance-based learning on data streams</title><host>Intell. Data Anal.11 (6)(2007) pp.627-650</host></reference><reference label="[55]"><authors>S.J. Delany,P. Cunningham,A. Tsymbal,L. Coyle</authors><title>A case-based technique for tracking concept drift in spam filtering</title><host>Knowl.-Based Syst.18 (4–5)(2005) pp.187-195</host></reference><reference label="[56]"><authors>S.J. Delany,D. Bridge</authors><title>Catching the drift: using feature-free case-based reasoning for spam filtering</title><host>7th International Conference on Case-Based ReasoningICCBR'07, Northern Ireland, UK, August 13–16(2007) pp.314-328</host></reference><reference label="[57]"><authors>J. Gama,P. Medas,G. Castillo,P. Rodrigues</authors><title>Learning with drift detection</title><host>17th Brazilian Symposium on Artificial IntelligenceSBIA '04, Sao Luis, Maranhao, Brazil, Sep. 29–Oct. 1(2004) pp.286-295</host></reference><reference label="[58]"><authors>I. Žliobaitė</authors><title>Combining similarity in time and space for training set formation under concept drift</title><host>Intell. Data Anal.15 (4)(2011) pp.589-611</host></reference><reference label="[59]"><authors>S. Huang,Y. Dong</authors><title>An active learning system for mining time-changing data streams</title><host>Intell. Data Anal.11 (4)(2007) pp.401-419</host></reference><reference label="[60]"><authors>P. Lindstrom,S.J. Delany,B.M. Namee</authors><title>Handling concept drift in text data stream constrained by high labelling cost</title><host>23rd International Florida Artificial Intelligence Research Society ConferenceFLAIRS'10, Florida, USA, May 19–21(2010) pp.26-31</host></reference><reference label="[61]"><authors>P. Lindstrom,B.M. Namee,S.J. Delany</authors><title>Drift detection using uncertainty distribution divergence</title><host>Evol. Syst.4 (1)(2013) pp.13-25</host></reference><reference label="[62]"><authors>B. Smyth,E. McKenna</authors><title>An efficient and effective procedure for updating a competence model for case-based reasoners</title><host>11th European Conference on Machine LearningECML '00, Barcelona, Catalonia, Spain, May 31–June 2(2000) pp.357-368</host></reference><reference label="[63]"><authors>K. Nishida,K. Yamauchi</authors><title>Detecting concept drift using statistical testing</title><host>10th International Conference on Discovery ScienceDS '07, Sendai, Japan, Oct. 1–4(2007) pp.264-269</host></reference><reference label="[64]"><authors>B. Smyth,E. McKenna</authors><title>Building compact competent case-bases</title><host>3rd International Conference on Case-Based ReasoningICCBR '99, Seeon Monastery, Germany, July 27–30(1999) pp.329-342</host></reference><reference label="[65]"><authors>J. Demšar</authors><title>Statistical comparisons of classifiers over multiple data sets</title><host>J. Mach. Learn. Res.7 (2006) pp.1-30</host></reference><reference label="[66]"><authors>K.R. Gee</authors><title>Using latent semantic indexing to filter spam</title><host>18th Annual ACM Symposium on Applied ComputingSAC '03, Melbourne, FL, USA, Mar. 9–12(2003) pp.460-464</host></reference><reference label="[67]"><authors>G. Ditzler,R. Polikar</authors><title>Incremental learning of concept drift from streaming imbalanced data</title><host>IEEE Trans. Knowl. Data Eng.25 (10)(2013) pp.2283-2301</host></reference></references><footnote/></root>