<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370215001022</url><title>Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiability</title><authors>Jeremias Berg,Matti Järvisalo</authors><abstract>Integration of the fields of constraint solving and data mining and machine learning has recently been identified within the AI community as an important research direction with high potential. This work contributes to this direction by providing a first study on the applicability of state-of-the-art Boolean optimization procedures to cost-optimal correlation clustering under constraints in a general similarity-based setting. We develop exact formulations of the correlation clustering task as Maximum Satisfiability (MaxSAT), the optimization version of the Boolean satisfiability (SAT) problem. For obtaining cost-optimal clusterings, we apply a state-of-the-art MaxSAT solver for solving the resulting MaxSAT instances optimally, resulting in cost-optimal clusterings. We experimentally evaluate the MaxSAT-based approaches to cost-optimal correlation clustering, both on the scalability of our method and the quality of the clusterings obtained. Furthermore, we show how the approach extends to constrained correlation clustering, where additional user knowledge is imposed as constraints on the optimal clusterings of interest. We show experimentally that added user knowledge allows clustering larger datasets, and at the same time tends to decrease the running time of our approach. We also investigate the effects of MaxSAT-level preprocessing, symmetry breaking, and the choice of the MaxSAT solver on the efficiency of the approach.</abstract><keywords>Boolean optimization;Boolean satisfiability;Maximum satisfiability;Correlation clustering;Cost-optimal clustering;Constrained clustering</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Integration of the fields of constraint solving and data mining and machine learning has recently been identified within the AI community as an important research direction with high potential. This work contributes to this direction by studying the applicability of Boolean optimization to cost-optimal correlation clustering under constraints.</paragraph><paragraph>A common problem setting in data analysis is a set of data points together with some information regarding their pairwise similarities from which some interesting underlying structure needs to be discovered. One way of approaching the problem is to attempt to divide the data into subgroups in a meaningful way, for example, so that data points in the same group are more similar to each other than to data points in other groups [2]. Discovering an optimal way of making such a division is in most settings computationally challenging and an active area of research [3]. A general term for problems of this kind is clustering: the groups the data is partitioned into are called clusters, and a partitioning of the dataset is called a clustering of the data.</paragraph><paragraph>In this work, we study the correlation clustering paradigm [4] in a general similarity-based setting. Correlation clustering is a well-studied [5], [6], [7], [8], [9] NP-hard problem. Given a labeled undirected graph with each edge labeled with either a positive or a negative label, the objective of correlation clustering is to cluster the nodes of the graph in a way which minimizes the number of positive edges between different clusters and negative edges within clusters. Taking a more general view to correlation clustering, we study the problem setting of weighted correlation clustering, in which each edge is associated with a weight (instead of merely a negative or positive label), indicating our confidence in that label. In the more general weighted case, the objective of correlation clustering is to minimize the sum of the weights of the positive edges between different clusters and the negative edges within clusters.</paragraph><paragraph>The correlation clustering paradigm is geared towards classifying data based on qualitative similarity information—as opposed to quantitative information—of pairs of data points. In contrast to other typical clustering paradigms, correlation clustering does not require the number of clusters as input. This makes it especially well-suited for settings in which the true number of clusters is unknown—which is often the case when dealing with real-world data. As a concrete example, consider the problem of clustering documents by topic without any prior knowledge on what those topics might be, based only on similarity information (edges) between pairs of different documents [4], [10]. Indeed, correlation clustering has various applications in biosciences [11], social network analysis and information retrieval [12], [13], [14]. Furthermore, the related problem of consensus clustering[15], with recent applications in bioinformatics and in particular microarray data analysis [16], [17], [18], [19], can also be naturally cast as correlation clustering.</paragraph><paragraph>Due to NP-hardness of correlation clustering [4], most algorithmic work on the problem has been heuristic, focusing on local search and approximative algorithms. While strong approximation algorithms have been proposed [4], [5], [6], [9]—providing up to constant-factor approximations in restricted settings—these algorithms are unable to provide actual cost-optimal solutions in general. In this work, we take a different approach: we study the applicability of state-of-the-art Boolean optimization techniques to cost-optimally solving real-world instances of the correlation clustering problem. A baseline motivation for this work are the recent advances in applying constraint programming for developing generic approaches to common data analysis problems [20], [21], [22], [23], [24], [25]. In a constraint programming based approach, the data analysis problem is stated in a declarative fashion within some constraint language, and then a generic solver for that language is used for solving the resulting instance.</paragraph><paragraph>Harnessing constraint solving for data analysis tasks has two key motivations. Firstly, declarative optimization systems allow for finding provably cost-optimal solutions. While heuristic approaches allow for scaling to very large datasets, quickly obtaining some hopefully meaningful clustering, the provably cost-optimal solutions obtained by the declarative approach can result in notably better clusterings which provide better insights into the data. This can be valuable especially when working on smaller scientific datasets which have taken years to collect [26]. Secondly, the declarative approach allows for easily integrating various types of additional constraints over the solution space at hand. This way, a user (domain data expert) may specify properties of solutions that are of interest to the user, without needing to extend available specialized algorithms in a non-trivial way to cope with such additional constraints. A constraint-based framework for clustering problems is well-suited for problem instances where some form of domain specific knowledge might be required in order to obtain meaningful clusterings. The paradigm for clustering problems of this type is known as constrained clustering[27], [28], [29]. Recently, Boolean satisfiability (SAT) [30] based approaches to solving constrained clustering within other clustering problems have been proposed [23], [31], However, to the best of our knowledge the only work done on constrained correlation clustering is the linear programming based approach of [10]; this work is the first study on the applicability of Maximum Satisfiability (MaxSAT) [32], a well-known optimization version of SAT, to correlation clustering under constraints. The problem definition we study covers correlation clustering with additional constraints that, e.g., either force or forbid a pair of points from being assigned to the same cluster; known as must-link and cannot-link constraints [27].</paragraph><section label="1.1"><section-title>Contributions</section-title><paragraph>We present a novel and extensible MaxSAT-based approach to optimal correlation clustering. Using propositional logic as the declarative language, we formulate the correlation clustering task in an exact fashion as weighted partial MaxSAT [32] and apply a state-of-the-art MaxSAT solver to solve the resulting MaxSAT instance optimally. To our best knowledge this is the first practical approach to exactly solving correlation clustering for finding cost-optimal clusterings, i.e., optimal clusterings w.r.t. the actual objective function of the problem, for real-world datasets with hundreds of elements. In contrast, most of the previous work on correlation clustering has mainly focused on approximation algorithms and greedy local-search techniques which cannot in general find optimal clusterings.</paragraph><paragraph>At the core of the approach, we present three different MaxSAT formulations of correlation clustering, and provide formal proofs for their correctness. We experimentally evaluate our approach on real-world datasets and compare the approach to both two alternative exact approaches, based on linear and quadratic integer programming [5], [33], and two approximation algorithms [5], [34]. The results show that our approach can provide cost-optimal solutions and scales better than competing exact integer and quadratic programming formulations. Furthermore, our approach performs especially well in terms of solution cost on sparse datasets (with many missing similarity entries), outperforming approximative methods even when the approximative methods are given full similarity information. Our approach easily extends to the task of constrained correlation clustering, which allows for the user to specify the clusterings of one's interest by imposing hard user-defined constraints over the search space of clusterings. We explain how different types of constraints can be handled within a MaxSAT-based approach to cost-optimally solving constrained correlation clustering instances. While approaches to constrained clustering have been proposed previously for different clustering paradigms [27], [28], [29], [35], [36], [23], the only previous work on constrained correlation clustering that we know of is [10]. However, their approach is approximative and the experiments are done on smaller datasets. In contrast, we show experimentally that added user knowledge allows clustering on larger datasets as it tends to notably decrease the running time of the approach. We also provide experimental results on MaxSAT-specific aspects of solving the correlation clustering instances, considering the effects of MaxSAT-level preprocessing, symmetry breaking, as well as the choice of the MaxSAT solver used on the efficiency of the approach.</paragraph></section><section label="1.2"><section-title>Paper organization</section-title><paragraph>In Section 2 we provide a generic problem definition for correlation clustering that is used throughout this article. Our definition covers both correlation clustering and constrained correlation clustering. We also demonstrate how a symmetric similarity measure simplifies the objective function of the clustering problem and show that a similarity measure can always be assumed to be symmetric. In Sections 3 and 4 we overview previously proposed linear and quadratic integer programs for solving correlation clustering exactly. In Section 5 we provide necessary background on Maximum Satisfiability. The MaxSAT encodings of correlation clustering are detailed in Sections 6, 7 and 8, respectively. Extensive experimental results are provided in Section 9. Finally, we present a short survey on related work in Section 10 and give some concluding remarks in Section 11. Formal proofs of the theorems presented in the paper are given in Appendix A.</paragraph></section></section><section label="2"><section-title>Problem setting</section-title><paragraph>In this section, we present the general similarity-based problem setting under which we study correlation clustering in both unconstrained and constrained settings.</paragraph><section label="2.1"><section-title>Problem definition</section-title><paragraph label="Example 1">Let {a mathematical formula}R‾=R∪{∞,−∞}, {a mathematical formula}V={v1,…,vN} a set of N data points that we wish to cluster, and {a mathematical formula}W∈R‾N×N a similarity matrix. We denote the element on row i column j in W by {a mathematical formula}W(i,j). This input can be viewed as a weighted graph, as demonstrated by the following example. Let {a mathematical formula}V={v1,v2,v3,v4} be a set of data points and consider the similarity matrix W given in Fig. 1 on the left. We can view this input as a directed graph {a mathematical formula}G=(V,E) where {a mathematical formula}(vi,vj)∈E if {a mathematical formula}W(i,j)≠0, and the weight of each edge {a mathematical formula}(vi,vj) is equal to {a mathematical formula}W(i,j). Fig. 1(right) illustrates the graph corresponding to W. In case the similarity matrix is symmetric, i.e., {a mathematical formula}W(i,j)=W(j,i) for all i and j, the graph underlying W is essentially undirected.</paragraph><paragraph>The intuition behind the similarity matrix is that it expresses preferences on whether or not two points {a mathematical formula}vi and {a mathematical formula}vj should be assigned to the same cluster; a positive value indicates that {a mathematical formula}vi and {a mathematical formula}vj should be assigned to the same cluster, a negative value that they should not. We say that points {a mathematical formula}vi and {a mathematical formula}vj are similar if {a mathematical formula}W(i,j)∈R and {a mathematical formula}W(i,j)&gt;0. If {a mathematical formula}W(i,j)&lt;0 and {a mathematical formula}W(i,j)∈R, we say that {a mathematical formula}vi and {a mathematical formula}vj are dissimilar. In the most general setting, neither the requirement of assigning pairs of points to the same (different) cluster(s) nor the notion of pairs of points being (dis)similar are required to be symmetric relations.</paragraph><paragraph>Any function {a mathematical formula}cl:V→N is a solution to the clustering problem, representing a clustering of the data points into clusters indexed with natural numbers. We say that two points {a mathematical formula}vi and {a mathematical formula}vj are co-clustered if {a mathematical formula}cl(vi)=cl(vj). Note that our formulation allows forcing two points to the same or different clusters. If {a mathematical formula}W(i,j)=∞ for some i and j, then {a mathematical formula}vi and {a mathematical formula}vj have to be co-clustered. Analogously, if {a mathematical formula}W(i,j)=−∞, then {a mathematical formula}vi and {a mathematical formula}vj are not allowed to be co-clustered. If the infinite values are in conflict with each other, the problem instance is infeasible. These additional hard constraints are commonly referred to as must-link (ML) and cannot-link (CL) constraints [27]. We will use the following definition to incorporate the intended semantics of the infinite values onto the possible clusterings. Given a similarity matrix W, we say that a clustering cl respects the infinite values of W, if {a mathematical formula}cl(vi)=cl(vj) whenever {a mathematical formula}W(i,j)=∞ and {a mathematical formula}cl(vi)≠cl(vj) whenever {a mathematical formula}W(i,j)=−∞.</paragraph><paragraph>Given a cost function G such that {a mathematical formula}G(W,cl)∈R for every solution cl, we say that a clustering cl (of V) is optimal under W as measured by G, if cl respects the infinite values of W and {a mathematical formula}G(W,cl)≤G(W,cl′) holds for any clustering {a mathematical formula}cl′ (of V) that respects the infinite values of W. The definition is sufficient for all purposes as we can always turn a function G we wish to maximize into a minimization problem by considering the function −G. For a given similarity matrix W we use {a mathematical formula}argmincl(G(W,cl)) to denote the set of optimal clusterings under W as measured by G.</paragraph><paragraph>In this work we focus on the cost function of correlation clustering with additional must-link and cannot-link constraints. In correlation clustering [4], [9] we are given a pairwise similarity measure over a set of data points. The task is then to cluster the nodes in a way that maximizes the number of similar points co-clustered and minimizes the number of dissimilar points co-clustered. More formally, given a symmetric similarity matrix W, the task is to find a clustering which minimizes the cost function{a mathematical formula} where {a mathematical formula}I[b] is an indicator function which takes the value 1 if the condition b is true, else {a mathematical formula}I[b]=0. Fig. 2 gives a precise formulation of constrained correlation clustering used throughout this work.</paragraph><paragraph>This definition covers all variants of correlation clustering that we are aware of. For example, the definition of [4] where the input consists of a complete graph with each edge labeled by a + or − is equivalent to restricting the input similarity matrix to only contain values from {a mathematical formula}{−1,1} and specifically not to contain infinite values. Furthermore, the assumption of symmetric input can be made without loss of generality, as detailed in Section 2.2.</paragraph><paragraph label="Example 2">Let {a mathematical formula}V={v1,v2,v3,v4} be a set of data points and consider the similarity matrix given in Fig. 3 on the left. Fig. 3(right) illustrates one possible solution cl to the correlation clustering problem for this input data. In described solution, {a mathematical formula}cl(v1)=cl(v2)=cl(v3)≠cl(v4). The cost of cl is{a mathematical formula}</paragraph><paragraph>In contrast to many other clustering problems, deciding the number of clusters is in the most general case part of the correlation clustering problem. However, as every point is assigned to exactly one cluster, in practice it is enough to search over all functions {a mathematical formula}cl:V→{1,…,N}.</paragraph></section><section label="2.2"><section-title>On the assumption of symmetric similarities</section-title><paragraph label="Theorem 1">We will now show that the assumption of symmetric similarity matrices in our problem definition (Fig. 2) can be done without loss of generality. Correlation clustering is often defined with 2 positive weights {a mathematical formula}wij+ and {a mathematical formula}wij− for each pair of data points {a mathematical formula}vi, {a mathematical formula}vj as the input [5]. The intuition behind these weights is that they give a separate measure for the costs of not assigning {a mathematical formula}vi and {a mathematical formula}vj to the same ({a mathematical formula}wij+) and to different ({a mathematical formula}wij−) cluster(s). A straightforward method of modeling this in terms of our clustering setting would be to use a cost function such as{a mathematical formula} and letting {a mathematical formula}W(i,j)=wij+ and {a mathematical formula}W(j,i)=−wij− for all {a mathematical formula}i&lt;j. However, this turns out to be unnecessary. Let{a mathematical formula}V={v1…vN}be a set of data points and W an asymmetric similarity matrix over V. Assume that for all i and j,{a mathematical formula}W(i,j)=∞implies{a mathematical formula}W(j,i)≠−∞(from which it also follows that{a mathematical formula}W(i,j)=−∞implies{a mathematical formula}W(j,i)≠∞). Then there is a symmetric similarity matrix{a mathematical formula}WSsuch that{a mathematical formula}</paragraph><paragraph>We note that the assumption in the theorem is minor. The condition can be checked in polynomial time, and if it does not hold, there are no feasible solutions to the constrained problem. A detailed proof of Theorem 1 is provided in Appendix A. The simpler objective function H simplifies the exact declarative formulations considered in this work.</paragraph></section><section label="2.3"><section-title>Constrained clustering</section-title><paragraph>In the clustering domain, the concept of a constraint is fairly abstract and the exact types of constraints that are feasible depend on the particular domain. A typical categorization of different types of constraints are instance-level constraints and cluster-level constraints[37]. Cluster-level constraints [36] deal with relationships between clusters. Examples of cluster-level constraints include constraints which enforce a predefined lower bounds for the similarity (distance) over clusters or a predefined upper bound on the dissimilarity of points within the clusters, as well as constraints requiring that the clustering contains at most/exactly/at least some fixed number of clusters or that all clusters contain at least a certain number of data points. Instance-level constraints deal with relationships between points. Two very well known examples are the already discussed ML and CL constraints. ML and CL constraints are have been shown to be flexible in the sense that many different types of constraints can be expressed in terms of them [37].</paragraph></section><section label="2.4"><section-title>Consensus clustering</section-title><paragraph>As detailed in [15], another problem closely related to correlation clustering is consensus clustering. In consensus clustering we are given a set V of data points and K different clusterings of V. The task is then to find a single consensus clustering which agrees as well as possible with the input clusterings. Consensus clustering fits into our problem definition by the following construction. For each pair of points {a mathematical formula}vi,vj∈V let {a mathematical formula}sij be the number of clusterings in which {a mathematical formula}vi and {a mathematical formula}vj are co clustered and {a mathematical formula}dij=K−sij be the number of clusterings in which they are not. Now construct a similarity matrix W by assigning {a mathematical formula}W(i,j)=sij and {a mathematical formula}W(j,i)=−dij for each pair of data points and apply Theorem 1 to obtain the equivalent (in terms of correlation clustering) symmetric similarity matrix. Then an optimal solution to the resulting correlation clustering problem corresponds to an optimal solution to the consensus clustering problem. Consensus clustering is indeed also NP-hard [38]. Recently the problem has received more attention due to applications in bioinformatics and in particular microarray data analysis [16], [17], [18], [19].</paragraph></section></section><section label="3"><section-title>Correlation clustering as integer linear programming</section-title><paragraph>An exact integer linear programming (ILP) formulation of correlation clustering has been proposed in [5], [10]. We will now restate this integer linear programming formulation in terms of our generic problem setting.</paragraph><paragraph>Given a set {a mathematical formula}V={v1,…,vN} of N data points and a symmetric similarity matrix W, the integer program involves binary variables {a mathematical formula}xij∈{0,1}, where {a mathematical formula}1≤i&lt;j≤N. The intended interpretation of these variables is that {a mathematical formula}xij=1 iff {a mathematical formula}vi and {a mathematical formula}vj are co-clustered in any clustering. We note that the variables are only required whenever {a mathematical formula}i&lt;j. However, for notational convenience, we use {a mathematical formula}xij and {a mathematical formula}xji to denote the same variable. Using these variables, the set of optimal solutions to the following integer linear program represents the set of optimal clusterings of V under W[5].{a mathematical formula}</paragraph><paragraph>The purpose of the transitivity constraint{a mathematical formula}xij+xjk≤1+xik is to ensure a well-defined clustering; for any {a mathematical formula}(vi,vj,vk)∈V×V×V, each of the points {a mathematical formula}vi,vj,vk must belong to exactly one cluster, and hence it follows that if points {a mathematical formula}vi,vj are assigned to the same cluster and points {a mathematical formula}vj,vk are assigned to the same cluster, by transitivity then points {a mathematical formula}vi,vk should also be assigned to the same cluster. Stated as a linear constraint we require that if {a mathematical formula}xij+xjk=2 then {a mathematical formula}xik=1, which is exactly what the transitivity constraint in the integer program demands. The purpose of the two other constraints is to ensure that the solution clustering respects the infinite values of W. Whenever {a mathematical formula}W(i,j)=∞, {a mathematical formula}vi and {a mathematical formula}vj have to be co-clustered, which in terms of the integer program is equivalent to {a mathematical formula}xij=1. Analogously, {a mathematical formula}W(i,j)=−∞ is equivalent to {a mathematical formula}xij=0. This formulation consists of {a mathematical formula}O(N2) variables and {a mathematical formula}O(N3) constraints. In terms of practical considerations, this suggests poor scalability for larger datasets.</paragraph></section><section label="4"><section-title>Correlation clustering as quadratic integer programming</section-title><paragraph>A quadratic integer programming formulation of correlation clustering was proposed in [33]. In addition to the number of data points N, the quadratic integer programming (QIP) formulation requires one additional parameter K, an upper limit for the number of clusters that the solution clustering should contain. The formulation allows {a mathematical formula}K=N in which case the set of possible solutions to the quadratic program exactly matches the set of possible solutions to the integer linear programming formulation of correlation clustering and our general definition of correlation clustering (recall Fig. 2). We next restate the quadratic program in terms of our generic problem setting and the parameter K.</paragraph><paragraph>Given a set {a mathematical formula}V={v1,…,vN} of N data points, an upper bound on the number of clusters K, and a symmetric similarity matrix W, the quadratic program involves binary variables {a mathematical formula}yik∈{0,1}, where {a mathematical formula}1≤i≤N and {a mathematical formula}1≤k≤K. The intended interpretation of the variables is that {a mathematical formula}yik=1 iff data point {a mathematical formula}vi is assigned to cluster k. Using these variables, the set of optimal solutions to the following quadratic integer program represents the set of optimal clusterings of V under W[33].{a mathematical formula}</paragraph><paragraph>For some intuition, note that the sum {a mathematical formula}∑k=1K(yikyjk) is equal to 1 only if points {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster in the solution clustering. The purpose of the {a mathematical formula}∑k=1Kyik=1 for all i constraint is to ensure that the solution to the quadratic program corresponds to a well-defined clustering of the data. As all the variables used are binary, the constraint forces exactly one of the variables {a mathematical formula}yi1,…,yiK to 1 for all i, which in turn ensures that the corresponding data point {a mathematical formula}vi is assigned to exactly one cluster, as required for a well-defined clustering. This non-convex QIP consists of {a mathematical formula}O(NK) variables and {a mathematical formula}O(N+I) constraints where I is the number of infinite values in the input similarity matrix. We note that the non-convexity of the quadratic program can follow both from the integrality constraints as well as the similarity values themselves, as demonstrated by the following example.</paragraph><paragraph label="Example 3">Consider the set {a mathematical formula}V={v1,v2,v3} of data points and the following similarity matrix over V:{a mathematical formula} For this similarity matrix and {a mathematical formula}K=N=3, the QIP in matrix form is{a mathematical formula} where{a mathematical formula} with A and b chosen to fit the constraints {a mathematical formula}∑k=1Kyik=1 for all i. For this instance of correlation clustering, the matrix {a mathematical formula}W is indefinite. To see this, observe that it has both negative and positive eigenvalues, for example 10 and {a mathematical formula}−5−33. Hence the objective function of the quadratic program in itself is not convex.</paragraph></section><section label="5"><section-title>Maximum Satisfiability</section-title><paragraph>Before describing our MaxSAT formulations of correlation clustering, we review necessary basic concepts related to Maximum Satisfiability.</paragraph><section label="5.1"><section-title>Syntax and semantics</section-title><paragraph>For a Boolean variable x, there are two literals, x and ¬x. A clause is a disjunction (∨, logical OR) of literals and a truth assignment is a function from Boolean variables to {a mathematical formula}{0,1}. A clause C is satisfied by a truth assignment τ ({a mathematical formula}τ(C)=1) if {a mathematical formula}τ(x)=1 for a literal x in C, or {a mathematical formula}τ(x)=0 for a literal ¬x in C. A set F of clauses is satisfiable if there is an assignment τ satisfying all clauses in F ({a mathematical formula}τ(F)=1), and unsatisfiable ({a mathematical formula}τ(F)=0 for any assignment τ) otherwise.</paragraph><paragraph>An instance {a mathematical formula}F=(Fh,Fs,c) of the weighted partial MaxSAT problem consists of two sets of clauses, a set {a mathematical formula}Fh of hard clauses and a set {a mathematical formula}Fs of soft clauses, and a function {a mathematical formula}c:Fs→R+ that associates a non-negative cost with each of the soft clauses.{sup:1} Any truth assignment τ that satisfies {a mathematical formula}Fh is a solution to F. The cost{a mathematical formula}cost(F,τ) of a solution τ to F is defined as{a mathematical formula} i.e., as the sum of the costs of the soft clauses not satisfied by τ. A solution τ is (globally) optimal for F if {a mathematical formula}cost(F,τ)≤cost(F,τ′) holds for any solution {a mathematical formula}τ′ to F. The cost of the optimal solutions of F is denoted by {a mathematical formula}opt(F). Given a weighted partial MaxSAT instance F, the weighted partial MaxSAT problem asks to find an optimal solution to F. For simplicity, we will from here on drop the term “weighted partial” when referring to weighted partial MaxSAT instances, and simply refer to them as MaxSAT instances.</paragraph><paragraph label="Example 4">As an example of modeling problems with MaxSAT, consider the 3-coloring problem for the graph in Fig. 4. The coloring problem can be modeled with MaxSAT by forming a MaxSAT instance {a mathematical formula}F=(Fh,Fs,c) using a set of 15 boolean variables, {a mathematical formula}{ri,bi,gi|i=1..5}. The intended semantics of a variable {a mathematical formula}rx is that the node x is colored red, similarly for {a mathematical formula}gx (green) and {a mathematical formula}bx (blue). The hard clauses in F restrict each node to be colored with exactly one color and the soft clauses represent the constraints forcing each pair of nodes sharing an edge to be colored with different colors. As clauses, this corresponds to{a mathematical formula} and{a mathematical formula} with {a mathematical formula}c(w)=1 for all {a mathematical formula}w∈Fs. An optimal solution τ to F is {a mathematical formula}τ(r1)=τ(r3)=τ(b5)=τ(b2)=τ(g4)=1 and {a mathematical formula}τ(x)=0 for all other variables. The cost of this solution is 1, proving that any 3-coloring of the graph in Fig. 4 has to assign the same color to at least one pair of nodes sharing an edge.</paragraph></section><section label="5.2"><section-title>Solving MaxSAT</section-title><paragraph>Recent advances in MaxSAT solvers make MaxSAT a viable approach to finding globally (cost-)optimal solutions to various optimization problems with successful real-world applications such as hardware design debugging [39], post-silicon and C-code fault localization [40], [41], reasoning over biological networks [42], and optimal Bayesian network structure learning [43]. As both SAT solvers and MaxSAT solvers continue improving, it is becoming commonly accepted that large problems can be solved in practice [44] and that the computational time is very much an empirical question and often not dominated by theoretical worst-case complexity. Indeed, MaxSAT is an active area of research [45], [46], [47], [48], [49]. We next provide a short overview of MaxSAT solvers. For a more comprehensive discussion, we refer the reader to [50], [51].</paragraph><paragraph>Many of the state-of-the-art MaxSAT solvers aimed at efficiently solving real-world instances in practice make use of a SAT solver as a subroutine. By relaxing the soft clauses in the input formula, the MaxSAT solver can linearly search for the optimal solution to the instance by querying the SAT solver for the existence of a truth assignment (not) satisfying at least (at most) k soft clauses for different values of k. Intuitively, k can then either be an upper [46] or a lower bound [52], [53] for the optimal solution. Another often used search strategy is binary search [49], [47]. This basic idea of the algorithm has been improved by exploiting the fact that whenever invoked on an unsatisfiable set of clauses, a modern SAT-solver can produce proof of unsatisfiability in the form of a (small) subset of the input clauses that in itself is unsatisfiable. These subsets are commonly referred to as unsatisfiable cores[52], [47], [54]. By using the information provided by the cores, MaxSAT solver can relax soft cores on demand, instead of having to relax all of them upfront. Solvers following this strategy are referred to as core-guided solvers. Other proposed methods for MaxSAT solving include incorporating integer linear programming techniques, either as one part of the solving algorithm [55] or by directly encoding the MaxSAT instance as an instance of integer linear programming [56].</paragraph><paragraph>In this work, we extend the application domains of MaxSAT to correlation clustering by presenting three different encodings for finding optimal solutions to the correlation clustering problem. Given a symmetric similarity matrix W over a set V of data points (recall Section 2.1), the basic idea behind all of our MaxSAT formulations of correlation clustering is that hard clauses are used to enforce that any solution to the MaxSAT instance represents a well-defined clustering (i.e., a mapping {a mathematical formula}cl:V→N). The soft clauses are used to encode the cost function in a faithful way, so that each solution to the MaxSAT instance can be mapped into a clustering with exactly the same cost. In this way the optimal solution of the created MaxSAT instance can be mapped into the optimal clustering of the correlation clustering problem. Next we will present all three encodings in detail.</paragraph></section></section><section label="6"><section-title>A MaxSAT formulation of correlation clustering: transitive encoding</section-title><paragraph>Our first MaxSAT formulation, the transitive encoding, of correlation clustering can be viewed as a simple reformulation of the integer linear programming formulation (recall Section 3) in terms of MaxSAT.</paragraph><paragraph>Similarly as in the ILP formulation, we use boolean variables {a mathematical formula}xij, where {a mathematical formula}1≤i&lt;j≤N, with the interpretation that {a mathematical formula}xij=1 iff points {a mathematical formula}vi and {a mathematical formula}vj are co-clustered.{sup:2} We again adopt the notational convenience {a mathematical formula}xij=xji. Now the transitive encoding forms the MaxSAT instance {a mathematical formula}F1=(Fh1,Fs1,c) summarized in Fig. 5.</paragraph><paragraph>We next describe the different parts of {a mathematical formula}F1 in detail.</paragraph><section label="6.1"><section-title>Hard clauses</section-title><paragraph>The hard clauses {a mathematical formula}Fh1 of the transitive encoding are a clausal formulation of the transitivity constraints ({a mathematical formula}xij+xjk≤1+xik for all distinct i,j,k) of the ILP formulation. In terms of propositional logic, these can be stated as {a mathematical formula}(xij∧xjk)→xik, which in clausal form corresponds to{a mathematical formula}</paragraph></section><section label="6.2"><section-title>Soft clauses</section-title><paragraph>The soft clauses {a mathematical formula}Fs1 encode the cost function. Each dissimilar pair of points {a mathematical formula}vi and {a mathematical formula}vj ({a mathematical formula}−∞&lt;W(i,j)&lt;0) that are co-clustered corresponds to exactly one unsatisfied soft clause with weight {a mathematical formula}−W(i,j), and similarly, each similar pair of points {a mathematical formula}vi and {a mathematical formula}vj ({a mathematical formula}∞&gt;W(i,j)&gt;0) that are assigned to different clusters corresponds to one unsatisfied soft clause with weight {a mathematical formula}W(i,j). These conditions are captured by the unit soft clauses {a mathematical formula}(¬xij) and {a mathematical formula}(xij), respectively, with weights set to {a mathematical formula}|W(i,j)|.</paragraph></section><section label="6.3"><section-title>Encoding constrained clustering</section-title><paragraph>The transitive encoding extends naturally to constrained correlation clustering with ML and CL constraints. For each {a mathematical formula}W(i,j)=∞, {a mathematical formula}vi and {a mathematical formula}vj are forced to be co-clustered. This is achieved with the hard clause {a mathematical formula}(xij). Similarly, for each {a mathematical formula}W(i,j)=−∞, points {a mathematical formula}vi and {a mathematical formula}vj are forced to different clusters, which is achieved by the hard clause {a mathematical formula}(¬xij). In addition to ML and CL, various types of other constraints can be expressed.</paragraph><paragraph label="Example 5">Running example of further constraintsWe will use a running example of encoding additional constraints under the three MaxSAT encodings considered in the work, highlighting some of the differences between the encodings. As an example, consider the constraint {a mathematical formula}NotCoClustered(i,j,t) forbidding a triple of points {a mathematical formula}vi, {a mathematical formula}vj and {a mathematical formula}vt from being co-clustered. Under the transitive encoding, this constraint can be encoded as a single clause {a mathematical formula}NotCoClustered(i,j,t):=(¬xij∨¬xjt∨¬xit). As another example, consider the cluster-level constraint {a mathematical formula}AtMostInAll(k) requiring each cluster to contain at most k data points. This constraint can be reformulated as requiring that each data point {a mathematical formula}vi is co-clustered with at most {a mathematical formula}k−1 other data points. For a fixed data point {a mathematical formula}vi the latter formulation can be encoded as a cardinality constraint{a mathematical formula}∑j={1,…,N}∖{i}xij≤(k−1) requiring at most {a mathematical formula}k−1 of the variables {a mathematical formula}xi1,…xiN to be set to true, which can further be encoded with hard clauses using one of the several compact cardinality constraints; see e.g. [57], [58]. The whole {a mathematical formula}AtMostInAll(k) constraint decomposes in to a conjunction of such cardinality constraints over i.</paragraph></section><section label="6.4"><section-title>Constructing a clustering from a MaxSAT solution to the transitive encoding</section-title><paragraph>Any solution τ to {a mathematical formula}F1 represents a valid clustering {a mathematical formula}clτ of V, constructed in an iterative manner as follows.</paragraph><paragraph>While there still are unassigned points left:</paragraph><list><list-item label="1.">Let i be the smallest index for which {a mathematical formula}clτ(vi) is not defined yet and let j be the iteration number ({a mathematical formula}j=1...).</list-item><list-item label="2.">Assign {a mathematical formula}clτ(vi)=j.</list-item><list-item label="3.">Assign {a mathematical formula}clτ(vk)=j for all still unassigned {a mathematical formula}vk for which {a mathematical formula}τ(xik)=1.</list-item></list><paragraph label="Theorem 2"> The fact that {a mathematical formula}clτ is well-defined follows from the observation that each point gets assigned to at most one cluster and each iteration of the procedure assigns at least one point to a cluster. Furthermore, the hard transitivity constraints in {a mathematical formula}F1 ensure that the intended semantics of the {a mathematical formula}xij variables hold in {a mathematical formula}clτ. Hence it follows that the optimal solutions of {a mathematical formula}F1 correspond to the optimal clusterings of V. The correctness of the transitive encoding can be formalized as follows. Given a set V of data points and a symmetric similarity matrix W over V, let{a mathematical formula}F1be the MaxSAT instance produced by the transitive encoding on W. The clustering{a mathematical formula}clτ⁎:V→Nconstructed from an optimal solution{a mathematical formula}τ⁎to{a mathematical formula}F1is an optimal clustering of V. A detailed proof of the theorem is given in Appendix A.</paragraph><paragraph>We note that the transitive encoding does not require a predefined number of clusters. This is avoided by the definition of the {a mathematical formula}xij variables, interpreted as pairwise indicator variables for two data points {a mathematical formula}vi,vj being assigned to the same cluster. However, the encoding is not very compact. Its size is similar to the ILP presented earlier, {a mathematical formula}O(N2) variables and {a mathematical formula}O(N3) clauses, suggesting that also this encoding does not scale well. Next we will present a unary encoding of correlation clustering into MaxSAT, which to some extent addresses the compactness issue of the transitive encoding.</paragraph></section></section><section label="7"><section-title>An unary encoding of correlation clustering into MaxSAT</section-title><paragraph>We now consider a more compact unary encoding, which to some extent resembles the quadratic integer programming formulation presented in Section 4. Similarly to the QIP, the unary encoding allows an upper bound K on the number of available clusters. By letting {a mathematical formula}K=N, the set of clusterings produced by the unary encoding is exactly the same as for the transitive encoding. The size of the unary encoding is {a mathematical formula}O(E⋅K+N⋅K) variables and {a mathematical formula}O(E⋅K) clauses where E is the number of non-zero values in the input similarity matrix W. Due to the dependence on E, in practice the unary encoding is more compact than the transitive encoding whenever the input matrix contains 0-entries or {a mathematical formula}K&lt;N.</paragraph><paragraph>The unary encoding involves {a mathematical formula}N⋅K boolean variables {a mathematical formula}yik, where {a mathematical formula}i=1..N (the number of data points) and {a mathematical formula}k=1..K (the number of clusters). The intended interpretation of these variables is that {a mathematical formula}yik=1 iff point {a mathematical formula}vi belongs to cluster k. Furthermore, the encoding employs two types of auxiliary variables.</paragraph><list><list-item label="•">{a mathematical formula}Aijk, where {a mathematical formula}i=1..N, {a mathematical formula}j=2..N, {a mathematical formula}i&lt;j, {a mathematical formula}W(i,j)&gt;0, and {a mathematical formula}k=1..K, with the interpretation {a mathematical formula}Aijk=1 iff points {a mathematical formula}vi and {a mathematical formula}vj are both assigned to cluster k.</list-item><list-item label="•">{a mathematical formula}Dij, where {a mathematical formula}i=1..N, {a mathematical formula}j=2..N, {a mathematical formula}i&lt;j, and {a mathematical formula}W(i,j)&lt;0, with the interpretation that if {a mathematical formula}Dij=0, then points {a mathematical formula}vi and {a mathematical formula}vj are assigned to different clusters.</list-item></list><paragraph> These variables are used for compactly encoding the similarity and dissimilarity constraints. We will next present details on the clauses used in the unary encoding. As with the transitive encoding, the hard clauses limit the set of solutions to well-defined clusterings, and the soft clauses encode the cost function in a faithful way. However, the hard and soft clauses differ significantly from the clauses in the transitive encoding. Most notably, both hard and soft clauses are included in the unary encoding for encoding the similarity and dissimilarity constraints.</paragraph><paragraph>Concretely, the unary encoding forms the MaxSAT instance {a mathematical formula}F2=(Fh2,Fs2,c) summarized in Fig. 6.</paragraph><paragraph>We next describe the different parts of {a mathematical formula}F2 in detail.</paragraph><section label="7.1"><section-title>Ensuring well-defined clusterings</section-title><paragraph>The hard constraints {a mathematical formula}ExactlyOne(i) constrain the search to well-defined clusterings by enforcing that each data point {a mathematical formula}vi is assigned into exactly one cluster k. In terms of the variables in the encoding this means that, for each i, exactly one of the variables {a mathematical formula}yi1,…,yiK should be assigned to 1, i.e.,{a mathematical formula} A number of different encodings of this cardinality constraint as clauses have been previously developed [59]. In our experiments, we used the so-called sequential encoding[60] which is linear, or more precisely, introduces {a mathematical formula}3K−4 clauses and {a mathematical formula}K−1 auxiliary variables for each i. We refer the interested reader to [60] for a detailed description of this encoding.</paragraph></section><section label="7.2"><section-title>Encoding similarity</section-title><paragraph>For a similar pair of data points {a mathematical formula}vi and {a mathematical formula}vj, the constraints {a mathematical formula}HardSimilar(i,j,k) for each {a mathematical formula}k=1..K and {a mathematical formula}SoftSimilar(i,j) together enforce the requirement that {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster whenever the soft constraint {a mathematical formula}SoftSimilar(i,j) is satisfied. In terms of propositional logic, this requirement can be expressed as the formula{a mathematical formula} In order to the express this propositional formula as clauses, we employ the auxiliary variables {a mathematical formula}Aijk and define the semantics of these to be {a mathematical formula}τ(Aijk)=1 iff {a mathematical formula}τ(yik∧yjk)=1. In terms of propositional logic, the defining constraint is {a mathematical formula}Aijk↔(yik∧yjk), which can be expressed as{a mathematical formula} We note that the definitions of the auxiliary variables do not yet enforce points {a mathematical formula}vi and {a mathematical formula}vj to be assigned to cluster k. Instead, the clauses {a mathematical formula}HardSimilar(i,j,k) state that the variable {a mathematical formula}Aijk is set to true if and only if points {a mathematical formula}vi and {a mathematical formula}vj are both assigned to cluster k. This must hold in every solution to {a mathematical formula}F2, hence the clauses are hard.</paragraph><paragraph>Using the auxiliary variables, the soft constraint expressing that the points {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster can be encoded as the clause{a mathematical formula} For some intuition, we note that if this clause is satisfied in a solution τ, then for some k, {a mathematical formula}τ(Aijk)=1. Since all hard clauses are satisfied in any solution, it follows that points {a mathematical formula}vi and {a mathematical formula}vj will be assigned to cluster k, exactly as required. Similarly, if points {a mathematical formula}vi and {a mathematical formula}vj are not assigned to the same cluster, then due to the hard constraints we have {a mathematical formula}τ(Aijk)=0 for all k, and the soft clause will not be satisfied. Each unsatisfied clause must increase the cost of a MaxSAT solution according to the similarity values of the corresponding points, which is why the weight of the clause is set to {a mathematical formula}W(i,j).</paragraph></section><section label="7.3"><section-title>Encoding dissimilarity</section-title><paragraph>For a dissimilar pair of data points {a mathematical formula}vi and {a mathematical formula}vj, the clauses {a mathematical formula}HardDissimilar(i,j,k) for each {a mathematical formula}k=1..K and {a mathematical formula}SoftDissimilar(i,j) together enforce the requirement that {a mathematical formula}vi and {a mathematical formula}vj are assigned to different clusters. This can be expressed by requiring for each cluster that at least one of {a mathematical formula}vi and {a mathematical formula}vj should not be assigned to that cluster, which in clausal form is expressed by {a mathematical formula}(¬yik∨¬yjk) for a cluster k. The whole constraint enforcing {a mathematical formula}vi and {a mathematical formula}vj to be assigned to different clusters is hence{a mathematical formula}</paragraph><paragraph>Equation (5) is already in clausal form. However, we want to make sure that breaking any of the individual clauses corresponds to a cost of {a mathematical formula}|W(i,j)|. To achieve this, we use the auxiliary variables {a mathematical formula}Dij, and define them in terms of propositional logic as {a mathematical formula}¬Dij→(¬yik∨¬yjk) for each cluster {a mathematical formula}k=1..K. That is, if {a mathematical formula}τ(Dij)=0 for some solution τ to {a mathematical formula}F2, then {a mathematical formula}vi and {a mathematical formula}vj are not assigned to the same cluster.{sup:3} The defining constraint can be expressed as the hard clauses{a mathematical formula} The auxiliary variable {a mathematical formula}Dij makes it possible to express the soft constraint requiring {a mathematical formula}vi and {a mathematical formula}vj to not be co-clustered simply as{a mathematical formula} For some intuition, we have that if the clause {a mathematical formula}(¬Dij) is satisfied in a solution to {a mathematical formula}F2, then the clauses {a mathematical formula}(¬yik∨¬yjk) also have to be satisfied for all k. Hence points {a mathematical formula}vi and {a mathematical formula}vj are not assigned to the same cluster. On the other hand, if {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster k, then the solution has to assign {a mathematical formula}Dij=1 in order to satisfy the hard clause {a mathematical formula}(Dij∨¬yik∨¬yjk), resulting in one unsatisfied clause with weight {a mathematical formula}|W(i,j)|, exactly as required for representing the correlation clustering cost function faithfully.</paragraph></section><section label="7.4"><section-title>Encoding constrained clustering</section-title><paragraph>By noticing that for each {a mathematical formula}k=1..K we need to enforce that {a mathematical formula}cl(vi)=k iff {a mathematical formula}cl(vj)=k, the must-link constraint over {a mathematical formula}vi and {a mathematical formula}Vj can be encoded under the unary encoding as{a mathematical formula} where the clauses {a mathematical formula}(¬yik∨yjk) and {a mathematical formula}(yik∨¬yjk) correspond to {a mathematical formula}yik↔yjk. For some intuition, in any solution τ we have that, whenever {a mathematical formula}τ(yik)=1, the solution has to assign {a mathematical formula}τ(yjk)=1 in order to satisfy {a mathematical formula}(¬yik∨yjk). Furthermore, based on the other hard clauses, we know that there exists exactly one {a mathematical formula}k=1..K for which {a mathematical formula}τ(yik)=1, and hence {a mathematical formula}τ(yik′)=0 for all {a mathematical formula}k′≠k. Thus τ has to assign {a mathematical formula}τ(yjk′)=0 in order to satisfy the clause {a mathematical formula}(yik′∨¬yjk′); hence the points are assigned to the same cluster.</paragraph><paragraph>The benefit of encoding the must-link constraint in this way compared to the similarity constraint presented earlier is the elimination of the auxiliary variables {a mathematical formula}Aijk and hence a decrease in the number of clauses generated. On the other hand, similarity constraints cannot be encoded directly in this way since they are soft. Furthermore, whenever a similarity constraint is not satisfied, the cost added to a MaxSAT solution should be exactly the corresponding similarity value, which is controlled in a simple way with the {a mathematical formula}Aijk variables.</paragraph><paragraph>Cannot-link constraints in the unary encoding can also be encoded more compactly than the dissimilarity constraints. The variable {a mathematical formula}Dij in the encoding is used to ensure that an unsatisfied dissimilarity constraint corresponds exactly to cost {a mathematical formula}|W(i,j)|. If we know that the dissimilarity constraint has to be satisfied (making it a hard cannot-link constraint), we can simply leave out the extra variable. The intuition between the cannot-link clauses is that for each {a mathematical formula}k=1..K and any solution τ, either {a mathematical formula}τ(yik)=0 or {a mathematical formula}τ(yjk)=0. Stated as clauses, we have{a mathematical formula}</paragraph><paragraph label="Example 6">Running example of further constraints continuedUnder the unary encoding, the {a mathematical formula}NotCoClustered(i,j,t) constraint, forbidding all three of the points {a mathematical formula}vi, {a mathematical formula}vj and {a mathematical formula}vt from being co-clustered, can be encoded with a set of K clauses{a mathematical formula} The constraint includes one clause for each cluster {a mathematical formula}s=1..K, each forbidding all three points from being assigned to cluster s. The constraint {a mathematical formula}AtMostInAll(k), requiring each cluster to contain at most k data points, can be encoded as a conjunction of K cardinality constraints, namely, by enforcing {a mathematical formula}∑i=1Nyij≤k over each cluster index j.</paragraph></section><section label="7.5"><section-title>Constructing a clustering from a MaxSAT solution to the unary encoding</section-title><paragraph>Given a solution τ to {a mathematical formula}F2, we can easily construct a corresponding well-defined clustering {a mathematical formula}clτ of the data points by assigning each point {a mathematical formula}vi into the cluster k for which {a mathematical formula}τ(yik)=1. Due to the hard constraints {a mathematical formula}Fh2, in any solution τ there is exactly one such k for every i. Especially, the clustering constructed from an optimal solution to {a mathematical formula}F2 will be an optimal clustering of the data, minimizing the correlation clustering objective function. This correctness of the unary encoding can be formalized as follows.</paragraph><paragraph label="Theorem 3">Given a set V of data points with{a mathematical formula}|V|=N, a symmetric similarity matrix W over V, and an upper limit K on the available clusters such that{a mathematical formula}1≤K≤N, let{a mathematical formula}F2be the MaxSAT instance produced by the unary encoding on W. The clustering{a mathematical formula}clτ⁎:V→{1,…K}constructed from an optimal solution{a mathematical formula}τ⁎to{a mathematical formula}F2is an optimal clustering of V over all clusterings{a mathematical formula}cl:V→{1,…K}. In other words,{a mathematical formula}clτ⁎is optimal over all clusterings of V that use at most K clusters.</paragraph><paragraph>Intuitively, the theorem follows from the already discussed connections between cost incurred by a clustering and the weight of unsatisfied soft clauses in the unary encoding. A proof of Theorem 3 is provided in Appendix A.</paragraph></section></section><section label="8"><section-title>A binary encoding of correlation clustering into MaxSAT</section-title><paragraph>As the third encoding, we describe a binary encoding of correlation clustering as MaxSAT, which is essentially a bitwise reformulation of the unary encoding. Similarly to the unary encoding, the binary encoding allows an upper limit K on the available clusters. As is often the case with SAT and MaxSAT encodings, the binary encoding is more compact than both the unary and the transitive encoding, regardless of the input similarity matrix or the value of K. An instance formed by the binary encoding contains {a mathematical formula}O(E+N⋅log2⁡K) variables and {a mathematical formula}O(E⋅log2⁡K) clauses, where E is the number of non-zero values in the input similarity matrix W.</paragraph><paragraph>For simplicity, we first assume that K is a power of 2, more precisely {a mathematical formula}K=2a for some {a mathematical formula}a∈N. From this it follows that {a mathematical formula}log2⁡K=a is an integer. The encoding also works if this is not the case; we will describe the required adaptations in Section 8.5. The encoding uses a variables {a mathematical formula}bik where {a mathematical formula}1≤k≤a for each point {a mathematical formula}vi. The intended semantics of these variables is that point {a mathematical formula}vi is assigned to cluster index {a mathematical formula}bia..bi1, interpreted as a binary number with the least significant bit to the right. Additionally, we employ two types of auxiliary variables.</paragraph><list><list-item label="•">{a mathematical formula}EQijk, where {a mathematical formula}1≤i&lt;j≤N, {a mathematical formula}W(i,j)∈R∖{0}, and {a mathematical formula}1≤k≤a. The intended semantics of {a mathematical formula}EQijk is {a mathematical formula}EQijk=1 iff {a mathematical formula}bik=bjk.</list-item><list-item label="•">{a mathematical formula}Sij, where {a mathematical formula}1≤i&lt;j≤N and {a mathematical formula}W(i,j)∈R∖{0}. {a mathematical formula}Sij=1 iff points {a mathematical formula}vi and {a mathematical formula}vj are co-clustered. (Note the equivalence: also, if {a mathematical formula}Sij=0, then points {a mathematical formula}vi and {a mathematical formula}vj are not assigned to the same cluster.)</list-item></list><paragraph>An instance {a mathematical formula}F3 produced by the binary encoding is summarized in Fig. 7. This time, the only hard clauses required are the clauses defining the auxiliary variables. This is due to the fact that any MaxSAT solution has to assign all the variables {a mathematical formula}bik in some unique way, and hence any solution will represent a well-defined clustering. We next describe the binary encoding in more detail.</paragraph><section label="8.1"><section-title>Hard clauses</section-title><paragraph>As the {a mathematical formula}bik variables form the bit-representation of the cluster index of point {a mathematical formula}vi, the question of whether two points {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster is equivalent to whether the values of {a mathematical formula}bik and {a mathematical formula}bjk are equal for all {a mathematical formula}1≤k≤a. In order to reason about the equality of individual bits, the binary encoding uses a “equality” variables {a mathematical formula}EQijk for each pair of points {a mathematical formula}vi and {a mathematical formula}vj for which {a mathematical formula}i&lt;j and {a mathematical formula}W(i,j)∈R∖{0}. These variables are defined to be equivalent to {a mathematical formula}τ(bik)=τ(bjk) when τ is a solution to {a mathematical formula}F3. In terms of propositional logic, the defining constraint is {a mathematical formula}EQijk↔(bik↔bjk), which corresponds to the set of clauses{a mathematical formula}</paragraph><paragraph>Encoding the semantics of the {a mathematical formula}Sij variables is straightforward using the equality variables. Two points {a mathematical formula}vi and {a mathematical formula}vj are assigned to the same cluster iff the values at each bit-position in the bit representation of their cluster indices are the same. Stated in propositional logic, we have {a mathematical formula}Sij↔(EQij1∧…∧EQija), which corresponds to{a mathematical formula}</paragraph></section><section label="8.2"><section-title>Soft clauses</section-title><paragraph>As the variable {a mathematical formula}Sij has the exact same semantics as the variable {a mathematical formula}xij in the transitive encoding, it can be used to formulate the soft clauses of the binary encoding in a very similar manner as the soft clauses in the transitive encoding. For every similar pair of points {a mathematical formula}vi and {a mathematical formula}vj, the cost of the clustering should increase by {a mathematical formula}W(i,j) whenever the points are not assigned to the same cluster. This condition is encoded by the unit soft clause {a mathematical formula}(Sij) with weight {a mathematical formula}c((Sij))=W(i,j). Analogously, for every dissimilar pair the instance includes the soft clause {a mathematical formula}(¬Sij) with weight {a mathematical formula}c((¬Sij))=|W(i,j)|.</paragraph></section><section label="8.3"><section-title>Encoding constrained clustering</section-title><paragraph>For compactly encoding the must-link constraint in the binary encoding, we simplify the similarity constraint. We need to ensure that {a mathematical formula}τ(bik)=τ(bjk) for all bits {a mathematical formula}k=1..a and all MaxSAT solutions τ. For a fixed k, this can be stated as {a mathematical formula}(bik↔bjk), which as clauses is expressed by {a mathematical formula}(¬bik∨bjk),(bik∨¬bjk). Hence the whole must-link constraint is{a mathematical formula} The cannot-link constraint can be seen as a simplified dissimilarity constraint. The variable {a mathematical formula}EQijk and the clauses defining it are still required for all bits. However, the cannot-link constraint can be stated as a single clause: we simply require that there exists a bit-position k such that the values {a mathematical formula}bik and {a mathematical formula}bjk differ. The whole cannot-link constraint is{a mathematical formula}</paragraph><paragraph label="Example 7">Running example of further constraints continuedDue to the similar semantics of the {a mathematical formula}Sij variables of the binary encoding and the {a mathematical formula}xij variables of the transitive encoding, both of our example constraints can be encoded very similarly to the transitive encoding. The {a mathematical formula}NotCoClustered(i,j,t) constraint, forbidding all three of the points {a mathematical formula}vi, {a mathematical formula}vj and {a mathematical formula}vt from being co-clustered, can be encoded by a single clause {a mathematical formula}NotCoClustered(i,j,t):=(¬Sij∨¬Sit∨¬Sjt), i.e., more compactly than in the unary encoding directly. Also, the {a mathematical formula}AtMostInAll(k) constraint can be encoded similarly to the transitive encoding by using for each {a mathematical formula}vi, a cardinality constraint forbidding {a mathematical formula}vi from being co-clustered with mode than {a mathematical formula}k−1 other points: {a mathematical formula}∑j={1…N}∖{i}Sij≤k−1.</paragraph></section><section label="8.4"><section-title>Constructing a clustering from a MaxSAT solution to the binary encoding</section-title><paragraph label="Theorem 4">Given a solution τ to {a mathematical formula}F3, there is again a very natural way of constructing a clustering of V. For each data point {a mathematical formula}vi, let {a mathematical formula}τ(bia)τ(bia−1)…τ(bi1)=c, where the left hand side is interpreted as a binary number, and assign {a mathematical formula}clτ(vi)=c+1. Since the number of available bits is {a mathematical formula}log2⁡K, it follows that {a mathematical formula}0≤c≤K−1, and hence {a mathematical formula}1≤clτ(vi)≤K holds for all {a mathematical formula}vi. The clustering constructed from an optimal solution to {a mathematical formula}F3 is optimal amongst all clusterings using at most K clusters. Given a set V of data points with{a mathematical formula}|V|=N, a symmetric similarity matrix W over V, and an upper limit K on the available clusters such that{a mathematical formula}1≤K≤N, let{a mathematical formula}F3be the MaxSAT instance produced by the binary encoding on W. The clustering{a mathematical formula}clτ⁎:V→{1,…K}constructed from an optimal solution{a mathematical formula}τ⁎to{a mathematical formula}F3is an optimal clustering of V under W over all clusterings{a mathematical formula}cl:V→{1,…K}. In other words,{a mathematical formula}clτ⁎is optimal over all clusterings of W that use at most K clusters. A proof of this theorem is provided in Appendix A.</paragraph></section><section label="8.5">The binary encoding for general K<paragraph>So far we have assumed that the upper limit on the available clusters is a power of 2, or, more precisely, that {a mathematical formula}K=2a for some a. This assumption simplifies the binary encoding since the values representable in binary with a bits are exactly 0 to {a mathematical formula}2a−1. It is also possible to constraint K to an arbitrary value. A simple approach would be to encode a separate constraint for each point {a mathematical formula}vi and each value {a mathematical formula}j∈{K,K+1,…,2a−1} forbidding the value of the bit variables {a mathematical formula}bia,…,bi1 (interpreted as a binary number) from being equal to j. However, this would result in {a mathematical formula}O(N2⋅log2⁡N) clauses, the same as the worst-case size of the whole encoding.</paragraph><paragraph>A more compact formulation can be obtained by observing that, for each data point we only need to encode a single constraint stating that the value of its assigned cluster index should be less than K. For a given K, let {a mathematical formula}Kj denote the value of the jth bit in the binary representation of K. Note that as {a mathematical formula}2a−1&lt;K≤2a, there are exactly a bits in the binary representation of K. For any set of bit variables {a mathematical formula}bia,…,bi1, denote the value represented by these variables in binary by {a mathematical formula}(bia…bi1)2. For a given datapoint {a mathematical formula}vi we can encode the constraint {a mathematical formula}(bia…bi1)2&lt;K recursively using the observation that a binary number {a mathematical formula}(bia…bi1)2 is less than another binary number {a mathematical formula}(Ka…K1)2 iff</paragraph><list><list-item label="•">{a mathematical formula}Ka=1 and {a mathematical formula}bia=0, or</list-item><list-item label="•">{a mathematical formula}Ka=bia and {a mathematical formula}(bia−1…bi1)2&lt;(Ka−1…K1)2.</list-item></list><paragraph> This formulation of inequality between binary numbers follows directly from the properties of binary numbers. We encode it as MaxSAT by introducing a fresh variables {a mathematical formula}Bij, {a mathematical formula}1≤j≤a, and adding clauses defining them recursively as{a mathematical formula} As the value of K is known, we can simplify the definition accordingly when adding the clauses to the encoding. Using these variables, the whole constraint limiting the number of clusters is enforced by the clauses defining the semantics of the {a mathematical formula}Bij variables, together with N unit clauses, one for each data point:{a mathematical formula} The size of this formulation is {a mathematical formula}O(N⋅log2⁡(N)).</paragraph></section></section><section label="9"><section-title>Experimental evaluation</section-title><paragraph>We will now describe an experimental evaluation of our MaxSAT-based approach to correlation clustering.</paragraph><section label="9.1"><section-title>Benchmarks</section-title><paragraph>We experiment on real-world datasets consisting of similarity values between amino-acid sequences of different proteins [62], as well as similarity matrices we obtained from standard UCI benchmark datasets. For each of the obtained similarity matrices, we normalized the matrix entries to the range {a mathematical formula}[−0.5,0.5].</paragraph><section label="9.1.1"><section-title>Protein sequence datasets</section-title><paragraph>We obtained four protein sequence datasets from http://www.paccanarolab.org/scps. The data consists of similarity values between amino-acid sequences, originally computed using BLAST [63]. All values were originally in the range {a mathematical formula}[0,1.0]. Normalization of the similarity information to the range {a mathematical formula}[−0.5,0.5] was done by subtracting 0.5 from each entry. Table 1 shows the number of data points for each data set.</paragraph></section><section label="9.1.2"><section-title>UCI datasets</section-title><paragraph>In addition to the protein sequence datasets, we produced similarity matrices based on the following UCI datasets.</paragraph><list><list-item label="•">ORL: the AT&amp;T ORL database of images of faces, each of size {a mathematical formula}92×112. Obtained from http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html.</list-item><list-item label="•">Ionosphere: the UCI ionosphere dataset, for classification of radar returns from the ionosphere, originally with 34 attributes. Obtained from http://archive.ics.uci.edu/ml/.</list-item><list-item label="•">Umist: the Sheffield (previously UMIST) Face Database, each face image of size {a mathematical formula}92×112. Obtained from http://www.sheffield.ac.uk/eee/research/iel/research/face.</list-item><list-item label="•">Breastcancer: the LIBSVM breast-cancer dataset, originally named “Wisconsin Breast Cancer in UCI”. The set contains 10 features. Obtained from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.</list-item><list-item label="•">Diabetes: the LIBSVM diabetes dataset, originally from UCI, containing 8 features. Obtained from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.</list-item><list-item label="•">Ecoli: the UCI Ecoli dataset, containing protein localization sites, with 8 features. Obtained from http://archive.ics.uci.edu/ml/.</list-item><list-item label="•">Vowel: the LIBSVM Vowel dataset, originally from UCI, with 10 features. Obtained from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.</list-item></list><paragraph> For these datasets, we first calculated the normalized Euclidean distance between each pair of points, and directly interpreted the distances as similarity values by linear inverse mapping to the range {a mathematical formula}[−0.5,0.5]. In order to simulate incomplete similarity information, we finally modified all similarity values in the range {a mathematical formula}[−0.25,0.25] to be 0. The size of each dataset is reported in Table 1.</paragraph></section><section label="9.1.3"><section-title>Setup</section-title><paragraph>For solving the MaxSAT instances resulting from our encodings, we used the academic off-the-shelf MaxSAT solver MaxHS [64], [55], [65] (MaxSAT evaluation 2013 version) obtained from the authors. MaxHS implements a hybrid approach to MaxSAT solving, combining the logical reasoning power of a SAT solver with the arithmetic reasoning power of an integer linear programming solver. During its execution, MaxHS maintains a set of unsatisfiable cores (recall Section 5.2). At each iteration, the ILP solver is used for finding a minimum-cost hitting set over the soft clauses in the current set of cores. Clauses in the hitting set are then temporarily removed from the instance and the SAT solver is invoked again. MaxHS terminates when the working formula is satisfiable, at which point the assignment returned by the SAT solver is an optimal solution to the MaxSAT instance. We note that MaxHS is by no means the only possible choice for a MaxSAT solver to use. We also report on a comparison of different state-of-the-art MaxSAT solvers in Section 9.4.3, the results of which motivate the use of MaxHS.</paragraph><paragraph>We compare the MaxSAT-based approach with exactly solving the integer linear programming and the quadratic integer programming formulations of correlation clustering (recall Sections 3 and 4, respectively). We used the commercial state-of-the-art integer programming solvers IBM CPLEX (version 12.6) and Gurobi Optimizer (version 6.0) for solving the integer linear programs, and additionally, the non-commercial SCIP [66] framework for solving the quadratic integer programs. Furthermore, we also compare to two approximative algorithms in terms of the cost of solutions obtained: the approximation algorithm KwickCluster (KC) proposed in [5] and further considered in [67], and the SDPC approach based on a semi-definite relaxation of the quadratic integer programming formulation, proposed in [34]. More details on these algorithms are provided in Section 10.1. For solving the semi-definite programs, we used the Matlab package SeDuMi 1.3 [68].</paragraph><paragraph>On the protein data we also experimented with the algorithms described in [62], available from http://www.paccanarolab.org/scps, which are specialized algorithms for correlation clustering protein sequences. The authors provide two algorithms that allow an unrestricted number of clusters by default. One is based on spectral clustering (SCPS) and the other on connected component analysis (CCA).</paragraph><paragraph>In addition to the comparative results, we also report on MaxSAT-specific experiments on the effect of MaxSAT-level preprocessing (in Section 9.4.1) and symmetry breaking (in Section 9.4.2) on solving times. We employed MaxSAT preprocessing in all experiments due to its positive impact on solving times. As for symmetry breaking on the MaxSAT-level in the other experiments, we only applied partial symmetry breaking to all formulas by enforcing the point with the lowest index to always be assigned to the first cluster.</paragraph><paragraph>A timeout of 8 hours and a memory limit of 30 GB were enforced on each individual run of a solver. The experiments were run under Linux on eight-core Intel Xeon E5440 2.8-GHz cluster nodes each with 32 GB of RAM. In order to ensure repeatable results, only a single algorithm on a single benchmark instance was executed on each cluster node at each time.</paragraph></section></section><section label="9.2"><section-title>Experiments on unconstrained correlation clustering</section-title><paragraph>We first focus on unconstrained correlation clustering, i.e., correlation clustering under the assumption that there are no infinite values in the input similarity matrices.</paragraph><section label="9.2.1"><section-title>Comparison of algorithms providing optimal solutions</section-title><paragraph>We start with a comparison of the exact approaches to correlation clustering: our three MaxSAT encodings, the integer linear programming formulation (ILP), and the quadratic integer programming formulation (QIP). As the size of the transitive encoding and the integer linear program does not depend on the number of non-zero elements in the similarity matrix, for these experiments we created instances by varying the number of points {a mathematical formula}n≥50 in the four protein datasets (Prot1, Prot2, Prot3 and Prot4) by considering only the n first rows and columns of the original similarity matrix of each data set.</paragraph><paragraph>The results are shown in Fig. 8. The reason for the absence of the QIP approach from the plot is that neither CPLEX, Gurobi, nor SCIP was able to solve any of the quadratic programs exactly within the time limit. For example, SCIP was able to solve the instances when using 20 points within seconds, but was unable to solve 50 points within 8 hours. While we do not have a definitive explanation for this poor behavior, one possible explanation may deal with the non-convexity (recall Section 4) of the QIP formulation of correlation clustering.{sup:4} The transitive and the unary MaxSAT encodings, as well as the ILP approach, are competitive with the binary encoding only when the number of points is small. However, all three MaxSAT encodings scale better than ILP and QIP. Both CPLEX and Gurobi ran out of memory on the ILP formulation for instances larger than 300 points, suggesting it will fail to solve larger instance irrespective of the timeout. Furthermore, the encodings for which the size of the instance is not dependent on the number of non-zero entries in the similarity matrix cannot benefit from any sort of pruning that one might be able to do on the similarity values of the input data.</paragraph><paragraph>Based on these observations, for the MaxSAT-based approach we focus on the binary encoding in the rest of the experiments.</paragraph></section><section label="9.2.2"><section-title>Performance on sparse data</section-title><paragraph>Next we simulate a setting in which the input data is sparse, that is, situations in which the similarity information available is incomplete. For {a mathematical formula}p∈{0.05,0.10,…,1}, we created instances from a given similarity matrix W by independently setting each non-zero element {a mathematical formula}W(i,j) to 0 with a probability {a mathematical formula}1−p. This results in a matrix {a mathematical formula}W′ where the expected number of non-zero entries is {a mathematical formula}p⋅100% of the number of non-zero entries in W.</paragraph><paragraph>We ran each of the approximative algorithms 100 times on each instance, and report the best values returned by them. We note that a single run of any of the approximative algorithms is very short, at most one minute for SDPC and within 10 seconds for the others.</paragraph><paragraph>Fig. 9, Fig. 10, Fig. 11 summarize the result of solving the sparse instances. A sparser matrix results in MaxSAT instances which are faster to solve. More importantly, however, we notice that MaxSAT is fairly robust when it comes to dealing with sparse data and the cost of the solution clustering. We experimentally compare the robustness of the different algorithms by calculating the cost {a mathematical formula}H(W,cl) for clusterings cl which were obtained with {a mathematical formula}W′ as input. This simulates a setting where there is some “true” objective function value that we would like the algorithms to optimize, but the amount of information available to the algorithms is limited/noisy. The cost of clusterings produced by the binary encoding is significantly lower than the other generic correlation clustering algorithms KwickCluster and SDPC for all values of p solvable by MaxSAT. For {a mathematical formula}p&gt;0.4, the solutions obtained with MaxSAT have a clearly lower cost than the solutions provided by two algorithms specialized for clustering protein sequences. Perhaps the most significant observation here is that, whenever the dataset was not solvable within the timeout, the clusterings obtained by MaxSAT on the highest value of p still solvable had in general a lower cost than any of the approximative algorithms at {a mathematical formula}p=1.0. This suggests that one can prune away a significant number of the non-zero entries in a matrix, hence speeding up MaxSAT solving, and still obtain clusterings of lower cost than those obtained with the approximative algorithms. We hypothesize that a more sophisticated method of pruning, perhaps taking into account the structure of the input matrix, could further improve the results. Comparing KwickCluster with SDPC, we observe that semi-definite programming performs slightly better on extremely sparse instances. However, when the density of the underlying graph increases, the performance of KwickCluster improves while the performance of SDPC remains fairly constant. One possible explanation for this could be that the relaxation of a quadratic program into a semi-definite program (see Section 10.1 for details) has a similar effect to the quality of the obtained clustering as pruning similarity information from the matrix.</paragraph></section></section><section label="9.3"><section-title>Constrained correlation clustering</section-title><paragraph>We now turn our attention to MaxSAT-based constrained correlation clustering.</paragraph><section label="9.3.1"><section-title>Instance-level constraints</section-title><paragraph>We first consider a situation in which an oracle, for example a domain expert, provides domain specific knowledge in the form of a set of must-link and cannot-link constraints the solution clusterings are expected to satisfy. By running several tests with an increasing number of constraints, we simulate a setting in which the current solution clustering is shown to the oracle, who is then allowed to add more constraints to the clustering algorithm in order to further restrict the set of acceptable clusterings. An iterative setting like this has previously been studied for example in [70], [71] and has been shown to greatly increase the clustering accuracy in other clustering problems [70], [71], [27].</paragraph><paragraph>We simulate this setting with the help of a (human created) “golden” clustering supplied with each of the datasets. Given the similarity matrix W based on a dataset, the golden clustering can be seen as a symmetric similarity matrix {a mathematical formula}GW of the same dimension where each element is either ∞ or −∞. To simulate this iterative setting, we sampled an increasing number of pairs of indices {a mathematical formula}i&lt;j, and modify W by assigning {a mathematical formula}W(i,j)=GW(i,j). Added {a mathematical formula}x% user knowledge (UK) means that {a mathematical formula}x% of available pairs of indices {a mathematical formula}i&lt;j were sampled. This results in a setting in which at each iteration the MaxSAT algorithm has an increasing amount of information on the golden clustering. We note that, to the best of our knowledge, the considered approximative algorithms cannot handle such a constrained correlation clustering setting directly. Even though additional constraints could be included into the semi-definite program solved with SDPC, there are no guarantees that the clustering obtained after the rounding procedure within SDPC respects the added constraints (see Section 10.1 for more details). This is why all the values reported for those are for {a mathematical formula}0% added UK.</paragraph><paragraph label="Definition 1">In these tests the performance of our encoding is evaluated using the well-known rand index[72] designed for measuring the similarity of two clusterings. Given a dataset {a mathematical formula}V={v1…vN}, a clustering cl of V, and an example clustering g, let{a mathematical formula} denote the number of pairs of points {a mathematical formula}i&lt;j that are co-clustered in both cl and g (true positives). Let{a mathematical formula} denote the number of pairs of points {a mathematical formula}i&lt;j that are assigned to different clusters in both cl and g (true negatives). The rand index of cl and g is then{a mathematical formula} Note that the denominator is the total number of unordered pairs of points over N data points.</paragraph><paragraph>As discussed in Section 2.3, ML and CL constraints are a non-trivial addition to the correlation clustering problem. Local search style clustering algorithms tend to suffer from over-constrainment in the sense that adding too many constraints can prevent such algorithms from converging. In contrast, the running time of the MaxSAT solver decreases with added constraints, see Fig. 12.{sup:5} As a consequence, using UK several additional datasets could be fully clustered with MaxSAT.</paragraph><paragraph>Finally, we demonstrate that added UK constraints steer the clusterings produced by our MaxSAT encoding effectively towards the golden clustering. Fig. 13 shows how the rand index increases as ML and CL constraints are added to the original similarity matrix. The number of extra constraints required for our algorithm to achieve rand indexes over 0.95 is for most datasets fairly small. The results suggest that extra constraints are highly beneficial and user knowledge should be taken advantage of whenever available.</paragraph></section><section label="9.3.2"><section-title>Cluster-level constraints</section-title><paragraph>To illustrate that the MaxSAT-based approach also enables obtaining optimal solutions which are guaranteed to satisfy cluster-level constraints, we consider a Cluster Dissimilarity constraint {a mathematical formula}CL-DIS(k), closely related to constraints previously studied in distance-based clustering. Informally, a clustering cl satisfies the {a mathematical formula}CL-DIS(k) constraint if no pair of points that are “more similar” than the threshold k are assigned to different clusters. More precisely, we require that {a mathematical formula}W(i,j)&lt;k whenever {a mathematical formula}cl(vi)≠cl(vj). This constraint is similar to the δ constraint in [37], where it was enforced by observing that the constraint can be decomposed into a set of ML constraints: whenever {a mathematical formula}W(i,j)&gt;k, we add a ML constraint over {a mathematical formula}vi and {a mathematical formula}vj.</paragraph><paragraph>Fig. 14 demonstrates the running time of MaxHS on the four protein datasets (without any pruning) and an added {a mathematical formula}CL-DIS(k) constraint for different threshold values k. Recall that all values in the benchmark similarity matrices were normalized to be between −0.5 and 0.5, which explains why we experimented with the threshold values {a mathematical formula}0.02,0.04,…,0.5. Note that {a mathematical formula}k=0.5 means that we are solving the original instance. All in all, the results show that the running time of MaxHS decreased drastically already for values only slightly below 0.5, all such instances being solvable in under a second.</paragraph></section></section><section label="9.4"><section-title>MaxSAT-specific experiments</section-title><paragraph>For the rest of this section, we will focus more MaxSAT-specific questions. We will consider the effects of MaxSAT-level preprocessing and symmetry breaking, as well as the performance of different state-of-the-art MaxSAT solvers, under the best-performing binary encoding. For these experiments we used the same set of benchmarks as in Section 9.2.2. We begin by considering preprocessing.</paragraph><section label="9.4.1"><section-title>Effects of MaxSAT preprocessing</section-title><paragraph>Preprocessing is today an essential part of SAT solving. However, to date there has been few studies on MaxSAT preprocessing [73], [74]. As such, MaxSAT preprocessing is a relatively recent area of research, possibly due to the fact that not all popular SAT preprocessing techniques can be directly applied in the context of MaxSAT [73]. However, one recently proposed way of using SAT preprocessing on MaxSAT instances is through the so-called labeled-CNF framework [75], [73] which we also apply here.</paragraph><paragraph>We preprocess a given MaxSAT instance {a mathematical formula}F=(Fh,Fs,c) with N soft clauses in the following way.</paragraph><list><list-item label="1.">Form the CNF formula {a mathematical formula}FSAT=Fh∪Fr, where {a mathematical formula}Fr={(wi∨¬ri)|wi∈Fs,i=1..N}, with each of the variables {a mathematical formula}ri not appearing anywhere in {a mathematical formula}FSAT except the clause {a mathematical formula}(wi∨¬ri), thus obtaining the so-called labeled-CNF[75].</list-item><list-item label="2.">Apply the Coprocessor 2.0 [76] SAT preprocessor on {a mathematical formula}FSAT, obtaining the CNF formula {a mathematical formula}F′SAT. Coprocessor implements a large range of modern SAT preprocessing techniques, including unit propagation, variable elimination [77], clause elimination [78], [79], binary clause reasoning [80], etc. We used the white-listing option of Coprocessor to disable the preprocessor from removing any occurrences of any of the {a mathematical formula}ri variables. This is critical for ensuring correctness on the MaxSAT-level [75].</list-item><list-item label="3.">Finally, we constructed the MaxSAT instance {a mathematical formula}FPRE=(FhPRE,FsPRE,cPRE) where {a mathematical formula}FhPRE=F′SAT, {a mathematical formula}FsPRE={(ri)|i=1..N} and {a mathematical formula}cPRE(ri)=c(wi). Now {a mathematical formula}opt(F)=opt(FPRE) and any solution τ to {a mathematical formula}FPRE can be extended into a solution for F of equal cost in polynomial (negligible) time, similarly as when applying SAT preprocessing on the SAT-level [81].</list-item></list><paragraph> The preprocessing time of MaxSAT instances resulting from the binary encoding was negligible, less than 10 seconds for each instance.</paragraph><paragraph>Fig. 15 demonstrates the difference in running time with and without preprocessing on instances consisting of {a mathematical formula}50% and {a mathematical formula}100% of the non-zero values of the datasets. On a majority of 94 out of 140 instances, preprocessing lowered the running time of the MaxSAT solver enough to compensate for the extra time spent on preprocessing. Furthermore, all instances that were solved without preprocessing were also solved after applying preprocessing. However, we did also observe instances on which preprocessing had a negative impact on the running time, exemplified in Fig. 15 by the Prot1 dataset.</paragraph></section><section label="9.4.2"><section-title>Effect of symmetry breaking</section-title><paragraph>The solution space of correlation clustering is highly symmetric: given any clustering {a mathematical formula}cl:V→{1…|V|} of a set of data points, the cluster indices can be arbitrarily permuted without affecting the actual partitioning of the data points and hence its cost. This leads to the question of whether MaxSAT solving could be sped-up by breaking some of these symmetries on the MaxSAT-level.</paragraph><paragraph>Full symmetry breaking seems unlikely to be beneficial, due to the fact that a very large number of clauses—going far beyond the size of the binary encoding—would be needed. More formally, define a relation ≡ over the set of possible clusterings of V by {a mathematical formula}cl≡cl′ if {a mathematical formula}cl=σ∘cl′ for some permutation σ. It is simple to see that the relation ≡ is an equivalence relation. Full symmetry breaking corresponds to adding constraints that forbid all but one member out of each equivalence class of ≡. A straightforward approach to achieve this would require clauses that identify some representative point (e.g., the smallest) assigned to each cluster in a clustering and enforce an ordering over these points. This could be done by encoding constraints stating If{a mathematical formula}viis not co-clustered with{a mathematical formula}vjfor any{a mathematical formula}j&lt;i, then{a mathematical formula}cl(vi)&gt;cl(vk)for all{a mathematical formula}k&lt;i. Such a constraint can be encoded into MaxSAT by techniques similar to the ones presented in Section 8. However, as there can be up to N clusters, this would introduce {a mathematical formula}O(N3) new clauses, which would evidently deteriorate the performance of a MaxSAT solver. As a related observation, note that the cubic transitive MaxSAT encoding breaks all symmetries, but does not perform as well as the binary encoding (without symmetry breaking).</paragraph><paragraph>Even though full symmetry breaking seems infeasible, we might still be able to boost solver performance by applying partial symmetry breaking to our encoding. Partial symmetry breaking refers to including clauses that remove (only) some symmetric solutions. As a simple example and the baseline in our experiments, we have the already mentioned the very simple first point into the first cluster constraint that can be enforced with {a mathematical formula}log2⁡N unit clauses of the form {a mathematical formula}(¬b1i), {a mathematical formula}1≤i≤log2⁡N.</paragraph><paragraph>A more involved symmetry breaking constraint we consider is the {a mathematical formula}ClustersLessThan(i,N) presented in Section 8.5. Without enforcing a limit on the number of clusters, the constraints {a mathematical formula}ClustersLessThan(i,N) are not required in order for the encoding to be correct. However, including them does prune away a significant number of symmetric solutions. Furthermore, the constraints are relatively compact, in total only {a mathematical formula}O(N⋅log2⁡(N)) new clauses need to be added. In our experiments we call this type of symmetry breaking RemoveSlack.</paragraph><paragraph>A further form of symmetry breaking deals with symmetries induced by the possibility of empty clusters. As an instance created by the binary encoding allows (at least) N different cluster indices for every point, the placement of empty clusters can potentially introduce symmetries into the solution space. Assume for example that some optimal clustering cl contains C clusters. Then cl is equivalent to at least {a mathematical formula}(NN−C) other clusterings, depending on which of the N cluster indices are empty. We can remove some of these symmetries by forcing the empty clusters to occupy indices {a mathematical formula}C+1,…N, or, more generally, the largest (or equivalently, the smallest) available indices. We next describe the encoding of this constraint in terms of the binary encoding.</paragraph><paragraph>Assume that we are using a bits to represent the cluster indices in the binary encoding. We introduce new variables {a mathematical formula}E1…E(2a), with the intended interpretation {a mathematical formula}τ(Ej)=1 iff {a mathematical formula}c(v)≠j for all {a mathematical formula}v∈V, that is, cluster j is empty. Using these variables, the empty cluster indices are propagated with constraints of the form {a mathematical formula}Ei→Ei+1 which inductively require that the clusters of higher index than an empty cluster are also empty, and that all clusters of lower index than a non-empty cluster are also non-empty. To define the {a mathematical formula}Ej variables for a given cluster j and data point {a mathematical formula}vi, let {a mathematical formula}bit⁎ denote the literal corresponding to the value of bit t of j, i.e., {a mathematical formula}bit if bit t of j is 1, and {a mathematical formula}¬bit otherwise. Now the {a mathematical formula}Ej variable can be defined as{a mathematical formula} which can be compactly represented as clauses by introducing N new auxiliary variables {a mathematical formula}C1j,…,CNj and defining them as {a mathematical formula}Cij↔(bi1⁎∧…∧bia⁎). Similar constraints are introduced for all of the {a mathematical formula}Ei variables. We call this type of symmetry breaking PropagateEmpty. Compared to RemoveSlack, the PropagateEmpty constraint breaks more symmetries. In particular, symmetries broken by PropagateEmpty include all of the symmetries broken by RemoveSlack. However, PropagateEmpty is more costly in terms of encoding size. In total, the constraints introduce {a mathematical formula}O(N2⋅log2⁡N) new clauses. Recall that the total size of the binary encoding is {a mathematical formula}O(E⋅log2⁡N) where E is at most of order {a mathematical formula}N2, which means that enforcing the PropagateEmpty constraint might significantly increase the number of clauses on sparse instances.</paragraph><paragraph>Fig. 16 demonstrates the effect of the RemoveSlack constraint compared to the baseline. The PropagateEmpty constraint is missing from the figure due to the fact that, when enforcing it, no instances could be solved within the timeout. We hypothesize that the reason for this is the significant number of clauses required for encoding it. As a concrete example, the preprocessed Prot1 dataset with {a mathematical formula}100% of the non-zero values present contains 323 301 clauses without PropagateEmpty and 6 427 796 clauses when enforcing PropagateEmpty. However, as can be seen in Fig. 16, the RemoveSlack constraint actually does improve solver performance on most instances.{sup:6}</paragraph></section><section label="9.4.3"><section-title>A comparison of MaxSAT solvers</section-title><paragraph>In the main experiments reported in this work, we used the MaxHS MaxSAT solver, which has shown very good performance especially in the “crafted” category of the recent MaxSAT Evaluations.{sup:7} Here we report on the performance of other state-of-the-art MaxSAT solvers, using the following solvers.</paragraph><list><list-item label="•">Eva500 solver [82], obtained from http://www.maxsat.udl.cat/14/solvers/.</list-item><list-item label="•">MsUnCore bcd2 version [49], [48] obtained from http://www.csi.ucd.ie/staff/jpms/soft/soft.php.</list-item><list-item label="•">OpenWBO [83], [84] version 1.1.1. obtained from http://sat.inesc-id.pt/open-wbo/.</list-item><list-item label="•">The ILP2013 solver [56]. We implemented the conversion to an integer program ourselves and used CPLEX to solve the resulting instances.</list-item></list><paragraph> The first three in the list are core guided solvers (recall Section 5.2). Eva500 uses the identified cores and a restricted form of MaxSAT resolution [85] to relax the MaxSAT instance in a controlled way. MsUnCore performs binary search over the cost function and also maintains a set of already identified disjoint cores and relaxes each core separately whenever a new one is found. OpenWBO uses an incremental approach that allows it to pertain the state of the internal SAT solver more efficiently between the iterations. ILP2013 encodes the whole MaxSAT instances as an integer linear program and then calls an ILP solver. Since MsUnCore, OpenWBO and Eva500 accept only integral weights, for running these solvers we multiplied all similarity values by 10{sup:13} (the highest possible multiplier with which the trivial cost upper bound required as input by the solvers still stays within the 2{sup:63} range) and rounded afterwards to integers. Table 2 gives a performance comparison of the solvers. MaxHS scales significantly better than the other solvers. All in all, MaxHS solved 121 instances within the timeout while the second-best performing Eva500 solved 65. The other solvers in the comparison timed out on most instances. Note that, apart from the first point into the first cluster constraint, symmetry breaking was not applied in this experiment.</paragraph></section></section></section><section label="10"><section-title>Related work</section-title><paragraph>We continue with a survey on related work.</paragraph><section label="10.1"><section-title>Correlation clustering</section-title><paragraph>While the notion of producing good clusterings under inconsistent advice first appeared in [11], the formal definition of correlation clustering was proposed in [4] and shown to be NP-hard on complete graphs with each edge labeled with + or −; or, in terms of the general problem definition considered in this work, on symmetric similarity matrices W where {a mathematical formula}W(i,j)={−1,1} for all i and j. NP-hardness motivated early work on approximative algorithms for the problem. Approximation algorithms for correlation clustering typically address one of three different objectives for the problem: minimizing disagreements, maximizing agreements, or maximizing correlation. Given a similarity matrix W over a set of data points {a mathematical formula}V={v1,…,vN}, minimizing disagreements refers to minimizing the number of point pairs {a mathematical formula}vi, {a mathematical formula}vj whose cluster assignment does not agree with their similarity value {a mathematical formula}W(i,j), or more precisely, to finding a clustering cl minimizing{a mathematical formula} Maximizing agreements refers to maximizing the number of pairs of points {a mathematical formula}vi, {a mathematical formula}vj whose cluster assignment agrees with their similarity value {a mathematical formula}W(i,j), or more precisely, to finding a clustering cl maximizing{a mathematical formula} Maximizing correlation refers to maximizing the difference between agreements and disagreements, i.e., using the objective function obtained by substracting Equation (7) from Equation (8).</paragraph><paragraph>A polynomial-time approximation scheme for maximizing agreements on complete symmetric matrices with {a mathematical formula}{−1,1} similarity values was presented in [4]. No such scheme is likely to exist for minimizing disagreements as the problem is APX-hard [86]. On general matrices, maximizing agreements is also APX-hard [6], except for when the ratio between the smallest and largest absolute value in the matrix is bounded by a constant [33]. To the best of our knowledge, the SDPC algorithm proposed in [34] and detailed below is the best known approximation algorithm for maximizing correlation.</paragraph><paragraph>The two approximative algorithms for correlation clustering we experimented with in this work are based on different techniques. KwickCluster [5] is a greedy combinatorial approximation algorithm that at each iteration picks one of the still unassigned nodes to be the pivot node, and forms a new cluster containing the pivot node and all still unassigned nodes that are similar to the pivot node (recall that nodes {a mathematical formula}vi and {a mathematical formula}vj are similar if {a mathematical formula}W(i,j)&gt;0 in the similarity matrix under consideration). The algorithm terminates when all points have been assigned to some cluster. The same algorithm also appears in [67] under the name PivotAlg. As shown in [5], [67], KwickCluster is a factor-3 approximation algorithm for minimizing disagreements under the assumption that {a mathematical formula}W(i,j)∈{−1,1} for all i and j, and a factor-5 approximation algorithm under the assumption that {a mathematical formula}−1≤W(i,j)≤1 for all i, j.{sup:8}</paragraph><paragraph>SDPC [34] is based on rounding solutions to a semi-definite program that itself is a relaxation of the quadratic programming formulation of correlation clustering restricted to two clusters. Restricting the correlation clustering problem search space to clusterings only containing two clusters, the quadratic program in Equation (4) (recall Section 4) can be formulated equivalently as{a mathematical formula} where N is the number of data points, and the value in the solution of the variable {a mathematical formula}zi indicates whether point {a mathematical formula}vi is assigned to cluster 1 or −1. This quadratic program can be relaxed into the semi-definite program{a mathematical formula} where each {a mathematical formula}zi binary variable from Equation (9) is represented by a vector {a mathematical formula}ui on the unit sphere in {a mathematical formula}RN. The relaxation of the quadratic program (Equation (9)) into the semi-definite program (Equation (10)) is standard, being similar to the SDP relaxation for MaxCut presented in [87].</paragraph><paragraph>In [34] an algorithm that rounds a solution obtained to Equation (10) into a well-defined clustering is presented and shown to achieve an {a mathematical formula}Ω(log⁡(N)−1) approximation factor for maximizing correlation. The algorithm compares clusterings obtained from rounding the semi-definite program with the (unique) cost of the trivial clustering in which all data points are assigned to different clusters, and returns the better solution out of these two.</paragraph><paragraph>In [33] the authors develop a PTAS for maximizing agreements on general matrices under the assumption that the ratios between weights in the input matrices are bounded by a constant, which implies that the matrices cannot contain 0-entries, i.e. the available similarity information has to be complete. The PTAS is developed by using the smooth polynomial programming technique on the QIP formulation, which results in strong approximation bounds for the maximization problem on matrices satisfying the assumption.</paragraph><paragraph>In [88] a more restricted version of the approximative correlation clustering algorithm of [4] is presented, with experiments on identifying and resolving noun co-reference in texts. In [89] a greedy randomized adaptive search procedure (GRASP) based approximative algorithm is presented, with the motivation that the obtained solutions can be used as a criterion for determining the balance in social networks. Also, in [10] correlation clustering is used for crosslingual link detection between google news groups. The authors build on the results of [86] and present an algorithm based on relaxing the ILP formulation of correlation clustering into a linear program and then using region growing techniques for rounding of the solution of the linear program. As such, their algorithm is also approximative in nature and, in contrast to our approach, cannot provide optimality guarantees. The authors also provide some results on exactly solving the ILP with added must-link and cannot-link constraints.{sup:9} As noted in [10] and supported by our experiments, the ILP-based approach to correlation clustering suffers from the fact that the number of constraints is cubic in the number of data points, leading to memory problems in practice. The authors of [10] approach the issue by splitting the LP into smaller chunks and processing the chunks separately. In contrast, our experimental results suggest that using MaxSAT for solving correlation clustering is more memory-efficient without extra tuning. There has also been some work done on a variant of correlation clustering in which the search is further restricted to {a mathematical formula}cl:V→{1,…K} for some {a mathematical formula}K&lt;N[8]. As explained in Sections 7 and 8, both the binary and the unary encoding can be used in this setting as well.</paragraph><paragraph>A few generalizations of correlation clustering have been proposed. In [12] the authors experiment with correlation clustering allowing overlapping clusters. The proposed solution to overlapping correlation clustering is a local search algorithm that locally adjusts the solution clustering as long as the cost function decreases. Out of the MaxSAT encodings presented in this work, the unary encoding extends naturally to overlapping clustering by changing the cardinality constraint {a mathematical formula}ExactlyOne(i) of each point to a more general {a mathematical formula}∑k=1Kyik≤p, where p is the maximum number of clusters a single point can be assigned to. The resulting encoding can be shown to produce globally optimal solutions to the overlapping clustering problem. Another proposed generalization to correlation clustering is chromatic correlation clustering [13]. In the basic form of correlation clustering, there are two possible relationships between pairs of data points. A pair of data points can either be similar, dissimilar (or neither). Chromatic correlation clustering generalizes this by allowing more than two different categories of relationships. This can be visualized as an undirected graph in which each edge is colored. The task is then to find a clustering that maximizes color purity of edges within clusters. Our MaxSAT encodings can be extended to cover Chromatic Correlation Clustering by introducing variables which represent the principal color of each cluster.</paragraph></section><section label="10.2"><section-title>Constrained clustering</section-title><paragraph>As exemplified in Section 9.3, the MaxSAT-based approach allows for obtaining solution which are guaranteed to satisfy additional hard constraints on the clusterings of interest. This includes both instance-level and, as exemplified in Section 9.3.2, even some distance-based cluster-level constraints which have been previously studied in the context of constrained clustering [90], [23]. The idea of adding constraints to the clustering problem was first introduced in [27], [28]. The introduction of constraints to the clustering problem allows the addition of domain knowledge to the problem and has been shown to increase clustering accuracy [27]. Much of the early work on constrained clustering concentrated on modifying existing heuristics and clustering algorithms in order to allow the addition of constraints. Examples include k-means and COB-WEB [27], [28], EM [91], hierarchical [92] and spectral clustering [93]. The problem of deciding if there exists a clustering satisfying a given set of must and cannot-link constraints was shown to be NP hard in [37]. In fact, many of the modified approximative algorithms are not even guaranteed to return a clustering satisfying all user constraints. The algorithms also have difficulties in handling too many extra constraints, they are easily over-constrained, preventing the algorithms from converging at all [37].</paragraph><paragraph>An alternative approach to constrained clustering is to cast the task as a constraint optimization problem, allowing for a very natural incorporation of added constraints. This is the approach which we employ in this work. A similar idea was proposed in [23] in a different clustering setting. The authors show that a satisfiability-based framework is well-suited for constrained clustering in the sense that constraints are easily added, the solutions returned are guaranteed to be globally optimal and satisfy all given constraints, and the search algorithm is not as easily over-constrained. Our approach to solving constrained correlation clustering is similar, but more generic as we do not restrict ourselves to only allowing two distinct clusters, which is a polynomial time special case of the general clustering problem. Furthermore, our encoding are on the MaxSAT-level (optimization instead of pure SAT), and employ a MaxSAT solver instead of a pure SAT solver.</paragraph><paragraph>Constrained clustering has also been approached via integer programming. In [94], [95] a variety of different possible constraints and optimization functions are considered. However, in practice their approach might be difficult to use as it requires a predetermined set of candidate clusters from which the algorithm searches for the best subset. In [90], [96] the authors use an integer programming and column generation based approach in order to exactly solve the minimum sum of squares clustering problem. Constrained clustering has also been approached, again in a different clustering setting, by constraint programming (CP) [97], [24].{sup:10} In [97] different optimization criteria for clustering are studied and solved by casting the clustering problems as constraint programming problems. A SAT-based framework of constrained clustering has also been proposed for example in [31], but optimization criteria are not applied in their experiments. In [24] a general framework for K-pattern set mining under constraints is introduced. The authors present a general framework and explore the strengths and limitations of using constraint programming. Yet another recent example of using declarative programming in the context of clustering is [26], in which an ILP formulation of hierarchical clustering, with an explicit objective function that is globally optimized, was presented; that approach would similarly allow for satisfying hard constraints over the solution space.</paragraph></section></section><section label="11"><section-title>Conclusions</section-title><paragraph>This work contributes to the research direction of harnessing constraint solving for developing novel types of generic data analysis techniques. The focus of our study is the applicability of state-of-the-art Boolean optimization procedures to cost-optimal correlation clustering in both unconstrained and constrained settings. To this end, we presented a novel MaxSAT-based framework for solving correlation clustering. Our approach is based on casting the clustering problem declaratively as weighted partial maximum satisfiability, and using a generic MaxSAT solver for finding cost-optimal clusterings. We studied three different encodings of correlation clustering as MaxSAT, and reported on an experimental evaluation, comparing both the time required to solve the resulting MaxSAT instances, and the quality of the clusterings obtained. We compared the MaxSAT-based approach to previously proposed both exact (integer linear and quadratic programming based) and approximative (specialized local search and approximation algorithms and semi-definite programming) approaches on real-world datasets. The MaxSAT approach scales better than the exact integer linear and quadratic programming approaches, and provides clusterings of significantly lower cost than the approximative algorithms, especially when the input data is sparse. Due to the intrinsic computational hardness of correlation clustering, we acknowledge that a potential issue with our approach is scalability, especially scaling the MaxSAT-based approach to very large datasets (with tens of thousands of data points). Nevertheless, the approach can provide cost-optimal clusterings on real-world datasets with close to a thousand points. The approach is also flexible when it comes to satisfying user-specified constraints, i.e., in constrained correlation clustering. The running times of the approach can notably decrease in a constrained setting, allowing for solving larger datasets faster compared to the non-constrained setting. This is in stark contrast with local search algorithms which easily suffer from over-constraining in constrained settings. It is conceivable that our approach can be improved also by foreseeable improvements to generic MaxSAT solvers and by developing domain-specific parallelization schemes, as well as by specialized constraint optimization techniques and heuristics for the problem domain. Yet another interesting direction would be to study the applicability of Large Neighborhood Search which combine local search strategies for fixing a subspace of the search space to which to apply exact search techniques.</paragraph></section></content><appendices><section label="Appendix A"><section-title>Proofs</section-title><paragraph>We provide detailed proofs of the fact that any similarity matrix can be symmetrized without affecting the set of optimal clusterings, as discussed in Section 2.2, and the correctness of the three encodings of correlation clustering as MaxSAT, presented in Sections 6, 7 and 8.</paragraph><section label="A.1">Proof of Theorem 1<paragraph>Assume that {a mathematical formula}V={v1…vN} is a set of N data points and {a mathematical formula}W∈R‾N×N is an asymmetric similarity matrix. Let {a mathematical formula}H′ be the non-simplified cost function of correlation clustering (Equation (2)) and H the simplified cost function (Equation (1)). We will assume w.l.o.g. that none of the considered matrices include contradicting infinite values.</paragraph><paragraph>The proof of Theorem 1 consists of considering the two different possible sources of asymmetries. The first are pairs of indices i and j for which {a mathematical formula}W(i,j)&lt;0&lt;W(j,i). Any such pair will always incur a cost of at least {a mathematical formula}min⁡(|W(i,j)|,W(j,i)) to any clustering. Thus the absolute value of both {a mathematical formula}W(i,j) and {a mathematical formula}W(j,i) can be decreased by this minimum without affecting the set of optimal clusterings. Notice that after this either {a mathematical formula}W(i,j)=0 or {a mathematical formula}W(j,i)=0. This observation is formalized in Lemma 1.</paragraph><paragraph>Based on the above, we can assume that all pairs {a mathematical formula}W(i,j) and {a mathematical formula}W(j,i) have the same sign. Now the existence of the symmetric {a mathematical formula}WS follows from the following observations. If both {a mathematical formula}W(i,j) and {a mathematical formula}W(j,i) are non-positive, the points {a mathematical formula}vi and {a mathematical formula}vj either incur a cost of {a mathematical formula}|W(i,j)|+|W(j,i)| or 0 to {a mathematical formula}H′(W,cl) under any clustering cl. Analogously, if both are non-negative, then the points either incur a cost of {a mathematical formula}W(i,j)+W(j,i) or 0. Hence, by letting {a mathematical formula}WS(i,j)=W(i,j)+W(j,i), cl will incur the same cost under {a mathematical formula}WS (as measured by H) as under W (as measured by {a mathematical formula}H′). This discussion is formalized in the proof of Theorem 1 given after the proof of Lemma 1.</paragraph><paragraph label="Proof">There is a similarity matrix{a mathematical formula}WTsuch that{a mathematical formula}WT(i,j)⋅WT(j,i)≥0(i.e., both have the same sign) for all i and j and{a mathematical formula}argmincl(H′(W,cl))=argmincl(H′(WT,cl)).{a mathematical formula}WT can be constructed by repeatedly applying Lemma 2 to each pair of indices corresponding to elements of opposing signs in W.  □</paragraph><paragraph label="Proof">Let i and j be any pair of indices for which{a mathematical formula}W(i,j)&lt;0&lt;W(j,i). There exists a similarity matrix{a mathematical formula}Wtfor which{a mathematical formula}Wt(i,j)⋅Wt(j,i)=0and{a mathematical formula}argmincl(H′(W,cl))=argmincl(H′(Wt,cl)).Construct {a mathematical formula}Wt as{a mathematical formula}{a mathematical formula}{a mathematical formula} Now either {a mathematical formula}Wt(i,j)=0 or {a mathematical formula}Wt(j,i)=0, and hence {a mathematical formula}Wt(i,j)⋅Wt(j,i)=0. Notice also that {a mathematical formula}Wt includes exactly the same infinite values as W. This means that the set of feasible clusterings is the same for both matrices. We prove the second part of the lemma by showing that{a mathematical formula} for any feasible clustering cl of V. The fact that the set of optimal clusterings under W is the same as under {a mathematical formula}Wt follows from {a mathematical formula}min⁡(|W(i,j)|,W(j,i)) being independent of cl.First, if either {a mathematical formula}W(i,j) or {a mathematical formula}W(j,i) is infinite, then it is infinite in {a mathematical formula}Wt. Furthermore, the other element is 0 in {a mathematical formula}Wt. Hence the pair i, j will incur cost {a mathematical formula}min⁡(|W(i,j)|,W(j,i)) under W and 0 under {a mathematical formula}Wt. As all other elements are equal in both matrices, we have {a mathematical formula}H′(W,cl)=H′(Wt,cl)+min⁡(|W(i,j)|,W(j,i)).Assume now that both {a mathematical formula}W(i,j) and {a mathematical formula}W(j,i) are finite. As the transformation from W to {a mathematical formula}Wt maintains signs of all elements, Equation (A.1) is equivalent to{a mathematical formula} This can be verified by considering the possible cases separately.  □</paragraph><paragraph label="Proof of Theorem 1">By Lemma 1 we can assume that {a mathematical formula}W(i,j)⋅W(j,i)≥0 for all i and j. Let {a mathematical formula}WS(i,j)=W(i,j)+W(j,i). It is clear that {a mathematical formula}WS is symmetric. It remains to be shown that {a mathematical formula}argmincl(H(WS,cl))=argmincl(H′(W,cl)). First note that {a mathematical formula}WS(i,j)=±∞ iff either {a mathematical formula}W(i,j)=±∞ or {a mathematical formula}W(j,i)=±∞, so the set of feasible clusterings is the same for both matrices.Let {a mathematical formula}i&lt;j and cl be any feasible clustering of V. We show that {a mathematical formula}H(WS,cl)=H′(W,cl). By decomposing both H and {a mathematical formula}H′ as in the proof of Lemma 1, is enough to show that{a mathematical formula} and{a mathematical formula} corresponding to the two possible scenarios, {a mathematical formula}cl(vi)=cl(vj) and {a mathematical formula}cl(vi)≠cl(vj), respectively. Both equations follow from the fact that the transformation from W to {a mathematical formula}WS preserves the signs of all elements. Thus {a mathematical formula}|WS(i,j)|=|W(i,j)+W(j,i)|.  □</paragraph></section><section label="A.2"><section-title>Correctness of the MaxSAT encodings</section-title><paragraph label="Proposition 1">Next we move on to prove the correctness of the three MaxSAT encodings presented in this work, in other words, we prove Theorem 2, Theorem 3, Theorem 4. Again, let {a mathematical formula}V={v1,…vN} be a set of data points, {a mathematical formula}W∈R‾N×N a symmetric similarity matrix, and K an upper bound on the available clusters. Note that we allow {a mathematical formula}K=N, so the proofs presented here cover the problem definition of [4], [12] and [9] as well as [8]. We first consider general conditions for correct MaxSAT encodings of correlation clustering. Recall that H is the cost function, Equation (1), of correlation clustering under minimization. Let F be a MaxSAT instance and assume that a clustering{a mathematical formula}clτ:V→{1…K}can be constructed from any solution τ to F. Further assume the following.</paragraph><list><list-item label="1.">{a mathematical formula}clτis well-defined for all solutions τ to F.</list-item><list-item label="2.">For each solution τ to F,{a mathematical formula}clτrespects the infinite values of W.</list-item><list-item label="3.">For each clustering cl that respects the infinite values of W, there exists some solution τ to F for which{a mathematical formula}H(W,cl)=H(W,clτ).</list-item><list-item label="4.">{a mathematical formula}cost(F,τ)=H(W,clτ)for any solution τ to F.</list-item></list><paragraph label="Proof">First note that Condition 1 ensures that {a mathematical formula}clτ⁎ is well-defined and Condition 2 ensures that {a mathematical formula}clτ⁎ is indeed a solution to the constrained problem. Now let cl be any clustering that respects the infinite values of W. Then by Condition 3 there exists a solution τ to F such that {a mathematical formula}H(W,clτ)=H(W,cl). By the optimality of {a mathematical formula}τ⁎ and condition Condition 4 it follows that{a mathematical formula} and hence {a mathematical formula}clτ⁎ is optimal.  □</paragraph><paragraph>Next we prove Theorem 2, Theorem 3, Theorem 4 by showing that the instances generated with the transitive, unary and binary encodings fulfill the assumptions of Proposition 1.</paragraph><section label="A.2.1"><section-title>Correctness of the transitive encoding</section-title><paragraph>Let {a mathematical formula}F1=(Fh1,Fs1,c) be a MaxSAT instance generated by the transitive encoding, and {a mathematical formula}clτ be the clustering constructed from a solution τ to {a mathematical formula}F1 by the procedure described in Section 6.4. The proof of Theorem 2, i.e., the fact that the transitive encoding produces optimal clusterings, follows from the following lemmas.</paragraph><paragraph label="Proof">For any solution τ to{a mathematical formula}F1and any{a mathematical formula}i&lt;j, we have{a mathematical formula}τ(xij)=1⇔clτ(vi)=clτ(vj).Assume {a mathematical formula}clτ(vi)=k. The lemma follows from the two possible scenarios that can occur when constructing {a mathematical formula}clτ at iteration k.(i) i is the smallest not yet assigned index. Then clearly {a mathematical formula}τ(xij)=1⇔clτ(vi)=k=clτ(vj).(ii) Some other index {a mathematical formula}t&lt;i for which {a mathematical formula}τ(xti)=1 is the smallest non-assigned index. Now {a mathematical formula}τ(xij)=1⇔τ(xtj)=1⇔clτ(vi)=k=clτ(vj). The first equivalence follows from τ being a solution to {a mathematical formula}F1. Thus {a mathematical formula}τ((¬xij∨¬xti∨xtj))=1, implying {a mathematical formula}τ(xij)=1⇒τ(xtj)=1, and {a mathematical formula}τ((¬xti∨¬xtj∨xij))=1, implying {a mathematical formula}τ(xij)=0⇒τ(xtj)=0.  □Condition 1 of Proposition 1{a mathematical formula}clτis well-defined for all solutions τ to{a mathematical formula}F1.Trivial, as each point is assigned to at most one cluster by the procedure in Section 6.4 and the procedure only terminates after all points have been assigned to a cluster.  □Condition 2 of Proposition 1{a mathematical formula}clτrespects the infinite values of W for all solutions τ to{a mathematical formula}F1.First notice that, due to the hard unit clauses {a mathematical formula}(xij) and {a mathematical formula}(¬xij), {a mathematical formula}τ(xij)=1 for all {a mathematical formula}W(i,j)=∞, and {a mathematical formula}τ(xij)=0 for all {a mathematical formula}W(i,j)=−∞. The rest follows from Lemma 3.  □Condition 3 of Proposition 1For each clustering cl that respects the infinite values of W there exists some solution τ to F for which{a mathematical formula}H(W,cl)=H(W,clτ).We construct such a τ as follows:{a mathematical formula} Notice that τ satisfies all hard transitivity clauses since cl is well-defined. Furthermore, τ satisfies all unit hard clauses since cl respects the infinite values of W. Finally, the claim {a mathematical formula}H(W,cl)=H(W,clτ) follows from Lemma 3 and the construction of τ as {a mathematical formula}cl(vi)=cl(vj)⇔τ(xij)=1⇔clτ(vi)=clτ(vj).  □Condition 4 of Proposition 1{a mathematical formula}cost(F1,τ)=H(W,clτ)holds for any solution τ to{a mathematical formula}F1.We consider the part {a mathematical formula}H(W,clτ)≤cost(F1,τ). The other direction is almost identical. A similar pair of points {a mathematical formula}vi and {a mathematical formula}vj incurs a cost {a mathematical formula}W(i,j) to {a mathematical formula}H(W,clτ) iff {a mathematical formula}clτ(vi)≠clτ(vj). By Lemma 3, {a mathematical formula}τ(xij)=0, and hence τ does not satisfy the unit soft clause {a mathematical formula}(xij) of weight {a mathematical formula}W(i,j). Similarly, a dissimilar pair of points {a mathematical formula}vi, {a mathematical formula}vj incurring a cost {a mathematical formula}W(i,j) to {a mathematical formula}H(W,clτ) corresponds to one unsatisfied soft clause {a mathematical formula}(¬xij) of the same weight.  □</paragraph></section><section label="A.2.2"><section-title>Correctness of the unary encoding</section-title><paragraph label="Proof">Let {a mathematical formula}F2 be a MaxSAT instance generated with the unary encoding and, given a solution τ to {a mathematical formula}F2, let {a mathematical formula}clτ be the clustering constructed form τ by the procedure described in Section 7.5. The proof of Theorem 3 follows from the following lemmas. Condition 1 of Proposition 1{a mathematical formula}clτis a well-defined clustering.Follows directly from the fact that, for any point {a mathematical formula}vi, {a mathematical formula}τ(ExactlyOne(i))=1, and hence there exists exactly one {a mathematical formula}1≤k≤K for which {a mathematical formula}τ(yik)=1.  □Condition 2 of Proposition 1{a mathematical formula}clτrespects the infinite values of W for all solutions τ to{a mathematical formula}F2.Let {a mathematical formula}vi be an arbitrary data point. Assume {a mathematical formula}clτ(vi)=k. It follows that {a mathematical formula}τ(yik)=1. The hard clause {a mathematical formula}(¬yik∨yjk) for each j s.t. {a mathematical formula}W(i,j)=∞ implies {a mathematical formula}τ(yjk)=1 and {a mathematical formula}clτ(vj)=k=clτ(vi). The hard clause {a mathematical formula}(¬yik∨¬yjk) for each j s.t. {a mathematical formula}W(i,j)=−∞ implies {a mathematical formula}τ(yjk)=0 and {a mathematical formula}clτ(vj)≠k=clτ(vi).  □</paragraph><paragraph label="Proof">Condition 3 of Proposition 1Let{a mathematical formula}cl:V→{1,2,…,K}be any clustering of V that respects the infinite values of W. There is a solution τ to{a mathematical formula}F2such that{a mathematical formula}cl=clτ.We construct such a τ. For each {a mathematical formula}1≤i≤N and {a mathematical formula}1≤k≤K, let{a mathematical formula}{a mathematical formula} Clearly {a mathematical formula}cl=clτ as long as τ is a solution to {a mathematical formula}F2. We show that it is by considering the different types of hard constraints present in {a mathematical formula}F2.</paragraph><list><list-item label="1.">Since cl is well-defined, there is exactly one k for which {a mathematical formula}cl(vi)=k for each {a mathematical formula}vi. Hence {a mathematical formula}τ(ExactlyOne(i))=1 for all {a mathematical formula}vi∈V.</list-item><list-item label="2.">By construction {a mathematical formula}τ(Aijk)=τ(yik∧yjk) for all similar {a mathematical formula}vi, {a mathematical formula}vj and k. Hence {a mathematical formula}τ(HardSimilar(i,j,k))=1.</list-item><list-item label="3.">If {a mathematical formula}τ(yik)=0 or {a mathematical formula}τ(yjk)=0 for a dissimilar pair of points {a mathematical formula}vi, {a mathematical formula}vj, then {a mathematical formula}τ(¬yik∨¬yjk∨Dij)=1. If {a mathematical formula}τ(yik)=τ(yjk)=1, then {a mathematical formula}cl(vi)=cl(vj). Hence {a mathematical formula}τ(Dij)=1 and {a mathematical formula}τ(¬yik∨¬yjk∨Dij)=1. Thus {a mathematical formula}τ(HardDissimilar(i,j,k))=1 for all dissimilar {a mathematical formula}vi,vj and k.</list-item><list-item label="4.">For all {a mathematical formula}W(i,j)=∞, we have that {a mathematical formula}cl(vi)=cl(vj). Hence there exists a k for which {a mathematical formula}τ(yik)=τ(yjk)=1. Since {a mathematical formula}τ(ExactlyOne(vi))=τ(ExactlyOne(vj))=1, {a mathematical formula}τ(yik′)=τ(yjk′)=0 for all other {a mathematical formula}k′. Hence {a mathematical formula}τ(yik↔yjk)=1 holds for all k and {a mathematical formula}τ(MLU(vi,vj))=1.</list-item><list-item label="5.">For all {a mathematical formula}W(i,j)=−∞ we have that {a mathematical formula}cl(vi)≠cl(vj). Hence either {a mathematical formula}cl(vi)≠k or {a mathematical formula}cl(vj)≠k for all k. By the construction of τ it follows that {a mathematical formula}τ(¬yik∨¬yjk)=1 and {a mathematical formula}τ(CLU(vi,vj))=1.</list-item></list><paragraph label="Proof">Condition 4 of Proposition 1{a mathematical formula}cost(F2,τ)=H(W,clτ)for any solution τ to{a mathematical formula}F2.We consider the part {a mathematical formula}H(W,clτ)≤cost(F2,τ). The other direction is almost identical. A similar pair of points {a mathematical formula}vi,vj incurs a cost {a mathematical formula}W(i,j) to {a mathematical formula}H(W,clτ) iff {a mathematical formula}clτ(vi)≠clτ(vj). Either {a mathematical formula}τ(yik)=0 or {a mathematical formula}τ(yjk)=0 (or both) for all k, and hence {a mathematical formula}τ(Aijk)=0 for all k. Thus τ does not satisfy the soft clause {a mathematical formula}SoftSimilar(i,j) with weight {a mathematical formula}W(i,j). Similarly a dissimilar pair of points {a mathematical formula}vi{a mathematical formula}vj incurs cost {a mathematical formula}|W(i,j)| to {a mathematical formula}H(W,clτ) iff {a mathematical formula}clτ(vi)=clτ(vj). There is a k for which {a mathematical formula}τ(yik∧yjk)=1. Thus τ does not satisfy the unit soft clause {a mathematical formula}(¬Dij) with weight {a mathematical formula}|W(i,j)|.  □</paragraph></section><section label="A.2.3"><section-title>Correctness of the binary encoding</section-title><paragraph label="Proof">Let {a mathematical formula}F3 be a MaxSAT instance generated with the binary encoding and, given a solution τ to {a mathematical formula}F3, let {a mathematical formula}clτ be the clustering constructed from τ by the procedure described in Section 8.4. We prove the correctness of the binary encoding for an arbitrary K. Let {a mathematical formula}k=⌈log2⁡K⌉ and assume that the encoding contains k bit variables for each data point. For any number {a mathematical formula}a∈N, let {a mathematical formula}an denote the nth bit in the bit representation of a. For any set of bits {a mathematical formula}bk,…,b1, denote by {a mathematical formula}(bk…b1)2 the value of the bit vector interpreted as a binary number, least significant bit to the right. Finally, let {a mathematical formula}bin⁎=bin if {a mathematical formula}Kn=1 and {a mathematical formula}bin⁎=¬bin if {a mathematical formula}Kn=0. The proof of Theorem 4, i.e., of the fact that the binary encoding produces optimal clusterings, follows from the following lemmas. Condition 1 of Proposition 1{a mathematical formula}clτis a well-defined clustering.Follows from the fact that for any point {a mathematical formula}vi, τ has to assign all the values {a mathematical formula}τ(bik),…,τ(bi1) in some unique way. Hence the value {a mathematical formula}clτ(vi) is uniquely defined. What remains to be shown is that {a mathematical formula}1≤clτ(vi)≤K. Assume for contradiction that {a mathematical formula}clτ(vi)=A for some {a mathematical formula}A&gt;K. Then {a mathematical formula}K−1&lt;A−1=(τ(bik),…,τ(bi1))2. Based on the properties of binary numbers, we know that {a mathematical formula}(A−1)j=1 and {a mathematical formula}(K−1)j=0 at the most significant bit j where the values differ. As {a mathematical formula}τ(DefB(i,j′))=1, we have {a mathematical formula}τ(Bik)=0, a contradiction.  □Condition 2 of Proposition 1{a mathematical formula}clτrespects the infinite values of W for all solutions τ to{a mathematical formula}F3.If {a mathematical formula}W(i,j)=∞, then τ has to assign {a mathematical formula}τ(bin)=τ(bjn) for each {a mathematical formula}n=1..k in order to satisfy the hard clauses corresponding to {a mathematical formula}bin↔bjn. Hence {a mathematical formula}τ(bin)=τ(bjn) for all bits and {a mathematical formula}clτ(vi)=clτ(vj). If {a mathematical formula}W(i,j)=−∞, then {a mathematical formula}τ(EQijn)=0 for some {a mathematical formula}n=1..k due to the hard clause {a mathematical formula}(¬EQij1∨…∨¬EQijk). It follows that {a mathematical formula}τ(bin↔bjn)=0. Thus {a mathematical formula}τ(bin)≠τ(bjn) and {a mathematical formula}clτ(vi)≠clτ(vj).  □</paragraph><paragraph label="Proof">Condition 3 of Proposition 1Let{a mathematical formula}cl:V→{1,2,…,K}be any clustering of V that respects the infinite values of W. There is a solution τ to{a mathematical formula}F3such that{a mathematical formula}cl=clτ.Construct such τ as{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula} Clearly {a mathematical formula}clτ=cl as long as τ is a solution to {a mathematical formula}F3, so it remains to be shown that {a mathematical formula}τ(Fh3)=1. Consider the different types of hard constraints present in {a mathematical formula}F3.</paragraph><list><list-item label="1.">For any {a mathematical formula}vi∈V and any bit n, the fact that {a mathematical formula}τ(DefB(i,n))=1 follows directly from the definition given in Equation (6), recalling that {a mathematical formula}(cl(vi)−1)n=τ(bin). Furthermore, {a mathematical formula}cl(vi)≤K⇔cl(vi)−1&lt;K. Hence there is a bit position n for which {a mathematical formula}Kn=1, {a mathematical formula}(cl(vi)−1)n=0 and {a mathematical formula}(cl(vi)−1)m=Km for all {a mathematical formula}n&lt;m≤k. Thus {a mathematical formula}τ(Bin)=1 and {a mathematical formula}τ(ClustersLessThan(i,K))=1.</list-item><list-item label="2.">For any {a mathematical formula}W(i,j)&lt;∞, {a mathematical formula}W(i,j)≠0, and any bit n position, it holds that{a mathematical formula} Hence {a mathematical formula}τ(Equality(i,j,m))=1.</list-item><list-item label="3.">For any {a mathematical formula}W(i,j)≠0 and {a mathematical formula}W(i,j)∈R, it holds that{a mathematical formula} Hence {a mathematical formula}τ(SameCluster(i,j))=1.</list-item><list-item label="4.">For all {a mathematical formula}W(i,j)=∞, {a mathematical formula}cl(vi)=cl(vj), and hence {a mathematical formula}cl(vi)−1=cl(vj)−1. By the construction of τ, we have {a mathematical formula}τ(bin)=τ(bjn) for all n. Thus {a mathematical formula}τ(yin↔yjn)=1 for all bit positions, and {a mathematical formula}τ(MLB(vi,vj))=1.</list-item><list-item label="5.">For all {a mathematical formula}W(i,j)=−∞, {a mathematical formula}cl(vi)≠cl(vj) and {a mathematical formula}cl(vi)−1≠cl(vj)−1. Hence there is a bit position n for which {a mathematical formula}(cl(vi)−1)n≠(cl(vj)−1)n. By the construction of τ, {a mathematical formula}τ(EQijn)=0 and {a mathematical formula}τ(¬EQij1∨…∨¬EQijk)=1. As we already demonstrated that {a mathematical formula}τ(Equality(i,j,m))=1 holds for all m, we conclude that {a mathematical formula}τ(CLB(vi,vj))=1.</list-item></list><paragraph label="Proof">Condition 4 of Proposition 1{a mathematical formula}cost(F3,τ)=H(W,clτ)for any solution τ to{a mathematical formula}F3.As the semantics of the {a mathematical formula}Sij variables exactly match the {a mathematical formula}xij variables from the transitive encoding, the proof of this lemma is almost identical to the proof of the corresponding result for the transitive encoding. The key observation is that any pair of points {a mathematical formula}vi and {a mathematical formula}vj increases the cost of a MaxSAT solution by {a mathematical formula}|W(i,j)| iff it also increases the cost of {a mathematical formula}clτ by {a mathematical formula}|W(i,j)|.  □</paragraph></section></section></section></appendices><references><reference label="[1]"><authors>J. Berg,M. Järvisalo</authors><title>Optimal correlation clustering via MaxSAT</title><host>Proc. 2013 IEEE ICDM Workshops(2013)IEEE Press pp.750-757</host></reference><reference label="[2]"><authors>D.H. Fisher</authors><title>Knowledge acquisition via incremental conceptual clustering</title><host>Mach. Learn.2 (2)(1987) pp.139-172</host></reference><reference label="[3]"><authors>A.K. Jain,M.N. Murty,P.J. Flynn</authors><title>Data clustering: a review</title><host>ACM Comput. Surv.31 (3)(1999) pp.264-323</host></reference><reference label="[4]"><authors>N. Bansal,A. Blum,S. Chawla</authors><title>Correlation clustering</title><host>Mach. Learn.56 (1–3)(2004) pp.89-113</host></reference><reference label="[5]">N. Ailon, M. Charikar, A. Newman, Aggregating inconsistent information: ranking and clustering, J. ACM 55 (5), Article No. 23.</reference><reference label="[6]"><authors>M. Charikar,V. Guruswami,A. Wirth</authors><title>Clustering with qualitative information</title><host>J. Comput. Syst. Sci.71 (3)(2005) pp.360-383</host></reference><reference label="[7]"><authors>R. Shamir,R. Sharan,D. Tsur</authors><title>Cluster graph modification problems</title><host>Discrete Appl. Math.144 (1–2)(2004) pp.173-182</host></reference><reference label="[8]"><authors>I. Giotis,V. Guruswami</authors><title>Correlation clustering with a fixed number of clusters</title><host>Theory Comput.2 (1)(2006) pp.249-266</host></reference><reference label="[9]"><authors>E.D. Demaine,D. Emanuel,A. Fiat,N. Immorlica</authors><title>Correlation clustering in general weighted graphs</title><host>Theor. Comput. Sci.361 (2–3)(2006) pp.172-187</host></reference><reference label="[10]"><authors>J.V. Gael,X. Zhu</authors><title>Correlation clustering for crosslingual link detection</title><host>Proc. IJCAI(2007) pp.1744-1749</host></reference><reference label="[11]"><authors>A. Ben-Dor,R. Shamir,Z. Yakhini</authors><title>Clustering gene expression patterns</title><host>J. Comput. Biol.6 (3/4)(1999) pp.281-297</host></reference><reference label="[12]"><authors>F. Bonchi,A. Gionis,A. Ukkonen</authors><title>Overlapping correlation clustering</title><host>Proc. ICDM(2011)IEEE pp.51-60</host></reference><reference label="[13]"><authors>F. Bonchi,A. Gionis,F. Gullo,A. Ukkonen</authors><title>Chromatic correlation clustering</title><host>Proc. KDD(2012)ACM pp.1321-1329</host></reference><reference label="[14]"><authors>N. Cesa-Bianchi,C. Gentile,F. Vitale,G. Zappella</authors><title>A correlation clustering approach to link classification in signed networks</title><host>Proc. COLTJ. Mach. Learn. Res. Workshop Conf. Proc.vol. 23 (2012)JMLR.org pp.34.1-34.20</host></reference><reference label="[15]"><authors>P. Bonizzoni,G.D. Vedova,R. Dondi,T. Jiang</authors><title>Correlation clustering and consensus clustering</title><host>Proc. ISAACLecture Notes in Computer Sciencevol. 3827 (2005)Springer pp.226-235</host></reference><reference label="[16]"><authors>V. Filkov,S. Skiena</authors><title>Integrating microarray data by consensus clustering</title><host>Int. J. Artif. Intell. Tools13 (4)(2004) pp.863-880</host></reference><reference label="[17]"><authors>V. Filkov,S. Skiena</authors><title>Heterogeneous data integration with the consensus clustering formalism</title><host>Proc. DILSLecture Notes in Computer Sciencevol. 2994 (2004)Springer pp.110-123</host></reference><reference label="[18]"><authors>R. Giancarlo,F. Utro</authors><title>Speeding up the consensus clustering methodology for microarray data analysis</title><host>Algorithms Mol. Biol.6 (2011) pp.1-</host></reference><reference label="[19]"><authors>Z. Yu,H.-S. Wong,H.-Q. Wang</authors><title>Graph-based consensus clustering for class discovery from gene expression data</title><host>Bioinformatics23 (21)(2007) pp.2888-2896</host></reference><reference label="[20]"><authors>T. Guns,S. Nijssen,L.D. Raedt</authors><title>Itemset mining: a constraint programming perspective</title><host>Artif. Intell.175 (12–13)(2011) pp.1951-1983</host></reference><reference label="[21]"><authors>S. Nijssen,T. Guns,L.D. Raedt</authors><title>Correlated itemset mining in ROC space: a constraint programming approach</title><host>Proc. KDD(2009)ACM pp.647-656</host></reference><reference label="[22]"><authors>L.D. Raedt,T. Guns,S. Nijssen</authors><title>Constraint programming for data mining and machine learning</title><host>Proc. AAAI(2010)AAAI Press</host></reference><reference label="[23]"><authors>I. Davidson,S.S. Ravi,L. Shamis</authors><title>A SAT-based framework for efficient constrained clustering</title><host>Proc. SDM(2010)SIAM pp.94-105</host></reference><reference label="[24]"><authors>T. Guns,S. Nijssen,L.D. Raedt</authors><title>K-pattern set mining under constraints</title><host>IEEE Trans. Knowl. Data Eng.25 (2)(2013) pp.402-418</host></reference><reference label="[25]"><authors>B. Négrevergne,A. Dries,T. Guns,S. Nijssen</authors><title>Dominance programming for itemset mining</title><host>Proc. ICDM(2013)IEEE pp.557-566</host></reference><reference label="[26]"><authors>S. Gilpin,S. Nijssen,I.N. Davidson</authors><title>Formalizing hierarchical clustering as integer linear programming</title><host>Proc. AAAI(2013)AAAI Press</host></reference><reference label="[27]"><authors>K. Wagstaff,C. Cardie</authors><title>Clustering with instance-level constraints</title><host>Proc. ICML(2000)Morgan Kaufmann pp.1103-1110</host></reference><reference label="[28]"><authors>K. Wagstaff,C. Cardie,S. Rogers,S. Schrödl</authors><title>Constrained K-means clustering with background knowledge</title><host>Proc. ICML(2001)Morgan Kaufmann pp.577-584</host></reference><reference label="[29]"><authors>I. Davidson,S.S. Ravi</authors><title>Intractability and clustering with constraints</title><host>Proc. ICML(2007)ACM pp.201-208</host></reference><reference label="[30]"><host>A. BiereM.J.H. HeuleH. van MaarenT. WalshHandbook of Satisfiability(2009)IOS Press</host></reference><reference label="[31]"><authors>J.-P. Métivier,P. Boizumault,B. Crémilleux,M. Khiari,S. Loudni</authors><title>Constrained clustering using SAT</title><host>Proc. IDALecture Notes in Computer Sciencevol. 7619 (2012)Springer pp.207-218</host></reference><reference label="[32]"><authors>C.M. Li,F. Manyà</authors><title>MaxSAT, hard and soft constraints</title><host>Handbook of Satisfiability(2009)IOS Press pp.613-631</host></reference><reference label="[33]"><authors>P. Bonizzoni,G.D. Vedova,R. Dondi,T. Jiang</authors><title>On the approximation of correlation clustering and consensus clustering</title><host>J. Comput. Syst. Sci.74 (5)(2008) pp.671-696</host></reference><reference label="[34]"><authors>M. Charikar,A. Wirth</authors><title>Maximizing quadratic programs: extending Grothendieck's inequality</title><host>Proc. FOCS(2004)IEEE Computer Society pp.54-60</host></reference><reference label="[35]"><authors>D. Klein,S.D. Kamvar,C.D. Manning</authors><title>From instance-level constraints to space-level constraints: making the most of prior knowledge in data clustering</title><host>Proc. ICML(2002)Morgan Kaufmann pp.307-314</host></reference><reference label="[36]"><authors>I. Davidson,S.S. Ravi</authors><title>Clustering with constraints: feasibility issues and the k-means algorithm</title><host>Proc. SDM(2005)SIAM pp.138-149</host></reference><reference label="[37]"><authors>I. Davidson,S.S. Ravi</authors><title>The complexity of non-hierarchical clustering with instance and cluster level constraints</title><host>Data Min. Knowl. Discov.14 (1)(2007) pp.25-61</host></reference><reference label="[38]"><authors>M. Kr̆ivánek,J. Morávek</authors><title>NP-hard problems in hierarchical-tree clustering</title><host>Acta Inform.23 (3)(1986) pp.311-323</host></reference><reference label="[39]"><authors>Y. Chen,S. Safarpour,J. Marques-Silva,A.G. Veneris</authors><title>Automated design debugging with maximum satisfiability</title><host>IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.29 (11)(2010) pp.1804-1817</host></reference><reference label="[40]"><authors>C.S. Zhu,G. Weissenbacher,S. Malik</authors><title>Post-silicon fault localisation using maximum satisfiability and backbones</title><host>Proc. FMCAD(2011)FMCAD Inc. pp.63-66</host></reference><reference label="[41]"><authors>M. Jose,R. Majumdar</authors><title>Cause clue clauses: error localization using maximum satisfiability</title><host>Proc. PLDI(2011)ACM pp.437-446</host></reference><reference label="[42]"><authors>J. Guerra,I. Lynce</authors><title>Reasoning over biological networks using maximum satisfiability</title><host>Proc. CPLecture Notes in Computer Sciencevol. 7514 (2012)Springer pp.941-956</host></reference><reference label="[43]"><authors>J. Berg,M. Järvisalo,B. Malone</authors><title>Learning optimal bounded treewidth Bayesian networks via maximum satisfiability</title><host>Proc. AISTATSvol. 33 (2014)JMLR pp.86-95</host></reference><reference label="[44]"><authors>M. Järvisalo,D. Le Berre,O. Roussel,L. Simon</authors><title>The international SAT solver competitions</title><host>AI Mag.33 (1)(2012) pp.89-92</host></reference><reference label="[45]"><authors>C.M. Li,F. Manyà,N.O. Mohamedou,J. Planes</authors><title>Exploiting cycle structures in Max-SAT</title><host>Proc. SATLecture Notes in Computer Sciencevol. 5584 (2009)Springer pp.467-480</host></reference><reference label="[46]"><authors>M. Koshimura,T. Zhang,H. Fujita,R. Hasegawa</authors><title>QMaxSAT: a partial Max-SAT solver</title><host>J. Satisf. Boolean Model. Comput.8 (1/2)(2012) pp.95-100</host></reference><reference label="[47]"><authors>J. Marques-Silva,J. Planes</authors><title>Algorithms for maximum satisfiability using unsatisfiable cores</title><host>Proc. DATE(2008)IEEE pp.408-413</host></reference><reference label="[48]"><authors>A. Morgado,F. Heras,J. Marques-Silva</authors><title>Improvements to core-guided binary search for MaxSAT</title><host>Proc. SATLecture Notes in Computer Sciencevol. 7317 (2012)Springer pp.284-297</host></reference><reference label="[49]"><authors>F. Heras,A. Morgado,J. Marques-Silva</authors><title>Core-guided binary search algorithms for maximum satisfiability</title><host>Proc. AAAI(2011)AAAI Press</host></reference><reference label="[50]"><authors>C. Ansótegui,M.L. Bonet,J. Levy</authors><title>SAT-based MaxSAT algorithms</title><host>Artif. Intell.196 (2013) pp.77-105</host></reference><reference label="[51]"><authors>A. Morgado,F. Heras,M.H. Liffiton,J. Planes,J. Marques-Silva</authors><title>Iterative and core-guided MaxSAT solving: a survey and assessment</title><host>Constraints18 (4)(2013) pp.478-534</host></reference><reference label="[52]"><authors>Z. Fu,S. Malik</authors><title>On solving the partial MaxSAT problem</title><host>Proc. SATLecture Notes in Computer Sciencevol. 4121 (2006)Springer pp.252-265</host></reference><reference label="[53]"><authors>V.M. Manquinho,J.P.M. Silva,J. Planes</authors><title>Algorithms for weighted boolean optimization</title><host>Proc. SATLecture Notes in Computer Sciencevol. 5584 (2009)Springer pp.495-508</host></reference><reference label="[54]"><authors>A. Morgado,C. Dodaro,J. Marques-Silva</authors><title>Core-guided MaxSAT with soft cardinality constraints</title><host>Proc. CPLecture Notes in Computer Sciencevol. 8656 (2014)Springer pp.564-573</host></reference><reference label="[55]"><authors>J. Davies,F. Bacchus</authors><title>Exploiting the power of MIPs solvers in MaxSAT</title><host>Proc. SATLecture Notes in Computer Sciencevol. 7962 (2013)Springer pp.166-181</host></reference><reference label="[56]"><authors>C. Ansótegui,J. Gabàs</authors><title>Solving (weighted) partial MaxSAT with ILP</title><host>Proc. CPAIORLecture Notes in Computer Sciencevol. 7874 (2013)Springer pp.403-409</host></reference><reference label="[57]"><authors>J.P. Marques-Silva,I. Lynce</authors><title>Towards robust CNF encodings of cardinality constraints</title><host>Proc. CPLecture Notes in Computer Sciencevol. 4741 (2007)Springer pp.483-497</host></reference><reference label="[58]"><authors>I. Abío,R. Nieuwenhuis,A. Oliveras,E. Rodríguez-Carbonell</authors><title>A parametric approach for smaller and better encodings of cardinality constraints</title><host>Proc. CPvol. 8124 (2013)Springer pp.80-96</host></reference><reference label="[59]">S. PrestwichCNF encodingsHandbook of Satisfiability(2009)IOS Press pp.75-97Ch. 2</reference><reference label="[60]"><authors>C. Sinz</authors><title>Towards an optimal CNF encoding of boolean cardinality constraints</title><host>Proc. CPLecture Notes in Computer Sciencevol. 3709 (2005) pp.827-831</host></reference><reference label="[61]"><authors>F. Heras,A. Morgado,J. Marques-Silva</authors><title>An empirical study of encodings for group MaxSAT</title><host>Proc. Canadian Conference on AILecture Notes in Computer Sciencevol. 7310 (2012)Springer pp.85-96</host></reference><reference label="[62]"><authors>T. Nepusz,R. Sasidharan,A. Paccanaro</authors><title>SCPS: a fast implementation of a spectral method for detecting protein families on a genome-wide scale</title><host>BMC Bioinform.11 (2010) pp.120-</host></reference><reference label="[63]"><authors>S. Altschul,W. Gish,W. Miller,E. Myers,D. Lipman</authors><title>Basic local alignment search tool</title><host>J. Mol. Biol.215 (3)(1990) pp.403-410</host></reference><reference label="[64]"><authors>J. Davies,F. Bacchus</authors><title>Solving MaxSAT by solving a sequence of simpler SAT instances</title><host>Proc. CPLecture Notes in Computer Sciencevol. 6876 (2011)Springer pp.225-239</host></reference><reference label="[65]"><authors>J. Davies,F. Bacchus</authors><title>Postponing optimization to speed up MAXSAT solving</title><host>Proc. CPLecture Notes in Computer Sciencevol. 8124 (2013)Springer pp.247-262</host></reference><reference label="[66]"><authors>T. Achterberg,T. Berthold,T. Koch,K. Wolter</authors><title>Constraint integer programming: a new approach to integrate CP and MIP</title><host>Proc. CPAIORLecture Notes in Computer Sciencevol. 5015 (2008)Springer pp.6-20</host></reference><reference label="[67]"><authors>A. Wirth</authors><title>Correlation clustering</title><host>Encyclopedia of Machine Learning(2010)Springer pp.227-231</host></reference><reference label="[68]">J. SturmUsing SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric conesOptim. Methods Softw.11–12 (1999) pp.625-653version 1.05 available from<host>http://fewcal.kub.nl/sturm</host></reference><reference label="[69]"><authors>C. Buchheim,M.D. Santis,L. Palagi</authors><title>A fast branch-and-bound algorithm for non-convex quadratic integer optimization subject to linear constraints using ellipsoidal relaxations</title><host>Oper. Res. Lett.43 (4)(2015) pp.384-388</host></reference><reference label="[70]">D. Cohn,R. Caruana,A. McCallumSemi-supervised clustering with user feedbackTech. rep.<host>(2003)</host></reference><reference label="[71]"><authors>I. Davidson,S.S. Ravi,M. Ester</authors><title>Efficient incremental constrained clustering</title><host>P. BerkhinR. CaruanaX. WuKDD(2007)ACM pp.240-249</host></reference><reference label="[72]"><authors>W. Rand</authors><title>Objective criteria for the evaluation of clustering methods</title><host>J. Am. Stat. Assoc.66 (336)(1971) pp.846-850</host></reference><reference label="[73]"><authors>A. Belov,A. Morgado,J. Marques-Silva</authors><title>SAT-based preprocessing for MaxSAT</title><host>Proc. LPARLecture Notes in Computer Sciencevol. 8312 (2013)Springer pp.96-111</host></reference><reference label="[74]"><authors>J. Berg,P. Saikko,M. Järvisalo</authors><title>Improving the effectiveness of SAT-based preprocessing for MaxSAT</title><host>Proceedings of the 24th International Joint Conference on Artificial IntelligenceIJCAI 2015(2015)AAAI Press</host></reference><reference label="[75]"><authors>A. Belov,M. Järvisalo,J. Marques-Silva</authors><title>Formula preprocessing in MUS extraction</title><host>Proc. TACASLecture Notes in Computer Sciencevol. 7795 (2013)Springer pp.108-123</host></reference><reference label="[76]"><authors>N. Manthey</authors><title>Coprocessor 2.0 – a flexible CNF simplifier (tool presentation)</title><host>Proc. SATLecture Notes in Computer Sciencevol. 7317 (2012)Springer pp.436-441</host></reference><reference label="[77]"><authors>N. Eén,A. Biere</authors><title>Effective preprocessing in SAT through variable and clause elimination</title><host>Proc. SATLecture Notes in Computer Sciencevol. 3569 (2005)Springer pp.61-75</host></reference><reference label="[78]"><authors>M. Järvisalo,A. Biere,M. Heule</authors><title>Blocked clause elimination</title><host>Proc. TACASLecture Notes in Computer Sciencevol. 6015 (2010)Springer pp.129-144</host></reference><reference label="[79]"><authors>M. Heule,M. Järvisalo,A. Biere</authors><title>Clause elimination procedures for CNF formulas</title><host>Proc. LPARLecture Notes in Computer Sciencevol. 6397 (2010)Springer pp.357-371</host></reference><reference label="[80]"><authors>M. Heule,M. Järvisalo,A. Biere</authors><title>Efficient CNF simplification based on binary implication graphs</title><host>Proc. SATLecture Notes in Computer Sciencevol. 6695 (2011) pp.201-215</host></reference><reference label="[81]"><authors>M. Järvisalo,A. Biere</authors><title>Reconstructing solutions after blocked clause elimination</title><host>Proc. SATLecture Notes in Computer Sciencevol. 6175 (2010)Springer pp.340-345</host></reference><reference label="[82]"><authors>N. Narodytska,F. Bacchus</authors><title>Maximum satisfiability using core-guided MaxSAT resolution</title><host>Proc. AAAI(2014)AAAI Press pp.2717-2723</host></reference><reference label="[83]"><authors>R. Martins,V.M. Manquinho,I. Lynce</authors><title>Open-WBO: a modular MaxSAT solver</title><host>Proc. SATLecture Notes in Computer Sciencevol. 8561 (2014)Springer pp.438-445</host></reference><reference label="[84]"><authors>R. Martins,S. Joshi,V.M. Manquinho,I. Lynce</authors><title>Incremental cardinality constraints for MaxSAT</title><host>Proc. CPLecture Notes in Computer Sciencevol. 8656 (2014)Springer pp.531-548</host></reference><reference label="[85]"><authors>J. Larrosa,F. Heras</authors><title>Resolution in Max-SAT and its relation to local consistency in weighted CSPs</title><host>Proc. IJCAI(2005)Professional Book Center pp.193-198</host></reference><reference label="[86]"><authors>E.D. Demaine,N. Immorlica</authors><title>Correlation clustering with partial information</title><host>Proc. RANDOM-APPROXLecture Notes in Computer Sciencevol. 2764 (2003)Springer pp.1-13</host></reference><reference label="[87]"><authors>M.X. Goemans,D.P. Williamson</authors><title>Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming</title><host>J. ACM42 (6)(1995) pp.1115-1145</host></reference><reference label="[88]"><authors>A. McCallum,B. Wellner</authors><title>Conditional models of identity uncertainty with application to noun coreference</title><host>Proc. NIPS(2004) pp.905-912</host></reference><reference label="[89]"><authors>L. Drummond,R. Figueiredo,Y. Frota,M. Levorato</authors><title>Efficient solution of the correlation clustering problem: an application to structural balance</title><host>Proc. OTM WorkshopsLecture Notes in Computer Sciencevol. 8186 (2013)Springer pp.674-683</host></reference><reference label="[90]"><authors>B. Babaki,T. Guns,S. Nijssen</authors><title>Constrained clustering using column generation</title><host>Proc. CPAIORLecture Notes in Computer Sciencevol. 8451 (2014)Springer pp.438-454</host></reference><reference label="[91]"><authors>A. Bar-Hillel,T. Hertz,N. Shental,D. Weinshall</authors><title>Learning distance functions using equivalence relations</title><host>Proc. ICML(2003)AAAI Press pp.11-18</host></reference><reference label="[92]"><authors>I. Davidson,S.S. Ravi</authors><title>Using instance-level constraints in agglomerative hierarchical clustering: theoretical and empirical results</title><host>Data Min. Knowl. Discov.18 (2)(2009) pp.257-282</host></reference><reference label="[93]"><authors>T. Coleman,J. Saunderson,A. Wirth</authors><title>Spectral clustering with inconsistent advice</title><host>Proc. ICML(2008)ACMNew York, NY, USA pp.152-159</host></reference><reference label="[94]"><authors>M. Mueller,S. Kramer</authors><title>Integer linear programming models for constrained clustering</title><host>Proc. DSLecture Notes in Computer Sciencevol. 6332 (2010)Springer pp.159-173</host></reference><reference label="[95]"><authors>J. Schmidt,E.M. Brändle,S. Kramer</authors><title>Clustering with attribute-level constraints</title><host>Proc. ICDM(2011)IEEE pp.1206-1211</host></reference><reference label="[96]"><authors>D. Aloise,P. Hansen,L. Liberti</authors><title>An improved column generation algorithm for minimum sum-of-squares clustering</title><host>Math. Program.131 (1–2)(2012) pp.195-220</host></reference><reference label="[97]"><authors>T.-B.-H. Dao,K.-C. Duong,C. Vrain</authors><title>A declarative framework for constrained clustering</title><host>Proc. ECML-PKDD(2013) pp.419-434</host></reference></references><footnote><note-para label="1">Our definition for the function c is more general than the standard {a mathematical formula}c:Fs→N+, which restricts the costs of soft clauses to be integral.</note-para><note-para label="2">Unlike the two other MaxSAT encodings considered in this work, the transitive encoding does not directly allow for enforcing an upper bounds of less than N on the number of clusters.</note-para><note-para label="3">The formalism behind grouped soft clauses like the ones in Equation (5) is known as group-MaxSAT. An exact treatment of group-MaxSAT is beyond the scope of this work, we refer the interested reader to [61].</note-para><note-para label="4">This would be inline with behavior observed in other problem domains as well, see e.g. [69].</note-para><note-para label="5">Note that, especially when using a MaxSAT solver which searches bottom-up in the cost function (such as MaxHS), adding more constraints could also have a negative effect on the running times of the solver.</note-para><note-para label="6">We remind the reader that, apart from the first point into the first cluster constraint, symmetry breaking was not applied in the other experiments reported on in this article.</note-para><note-para label="7">http://www.maxsat.udl.cat/14/results/index.html#wpms-crafted.</note-para><note-para label="8">Recall that a factor α approximation algorithm on a minimization (maximization) problem is guaranteed to return a solution of cost lower (higher) than α times the cost of the optimal solution.</note-para><note-para label="9">Unfortunately, the authors were unable to provide an implementation of their algorithm.</note-para><note-para label="10">The term constraint programming refers here to the declarative language as opposed to a general term of the paradigm.</note-para></footnote></root>