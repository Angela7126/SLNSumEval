<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370216300789</url><title>Norm-based mechanism design</title><authors>Nils Bulling,Mehdi Dastani</authors><abstract>The increasing presence of autonomous (software) systems in open environments in general, and the complex interactions taking place among them in particular, require flexible control and coordination mechanisms to guarantee desirable overall system level properties without limiting the autonomy of the involved systems. In artificial intelligence, and in particular in the multi-agent systems research field, social laws, norms, and sanctions have been widely proposed as flexible means for coordinating the behaviour of autonomous agents in multi-agent settings. Recently, many languages have been proposed to specify and implement norm-based environments where the behaviour of autonomous agents is monitored, evaluated based on norms, and possibly sanctioned if norms are violated. In this paper, we first introduce a formal setting of multi-agent environments based on concurrent game structures which abstracts from concrete specification languages. We extend this formal setting with norms and sanctions, and show how concepts from mechanism design can be used to formally analyse and verify whether a specific behaviour can be enforced (or implemented) if agents follow their subjective preferences. We relate concepts from mechanism design to our setting, where agents' preferences are modelled by linear time temporal logic (LTL) formulae. This proposal bridges the gap between norms and mechanism design allowing us to formally study and analyse the effect of norms and sanctions on the behaviour of rational agents. The proposed machinery can be used to check whether specific norms and sanctions have the designer's expected effect on the rational agents' behaviour or if a set of norms and sanctions that realise the effect exists at all. We investigate the computational complexity of our framework, focusing on its implementation in Nash equilibria and we show that it is located at the second and third level of the polynomial hierarchy. Despite this high complexity, on the positive side, these results are in line with existing complexity results of related problems. Finally, we propose a concrete executable specification language that can be used to implement multi-agent environments. We show that the proposed specification language generates specific concurrent game structures and that the abstract multi-agent environment setting can be applied to study and analyse the behaviour of multi-agent programs with and without norms.</abstract><keywords>Norms;Multi-agent systems;Mechanism design</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>The emergence of autonomous software systems and their increasing number of interactions with open environments such as the Internet, financial markets, large ICT systems, socio-technical systems and industrial platforms, urgently requires flexible control and coordination mechanisms in order to guarantee desirable overall system level properties. This urgency became painfully clear in 2010 by the so-called “Flash Crash”, where the uncontrolled and uncoordinated interactions between high frequency algorithmic trading systems in financial environments have led to extraordinary upheaval of U.S. equity markets [63]. Similar urgency is experienced in large ICT systems of organisations such as banks or insurance companies, where due to competition and innovation business processes have to be modified in rapid tempo. Such continuous changes constitute a potential threat for business processes to become non-compliant with the companies' business rules and policies. There is, therefore, an increasing need for flexible control and supervision mechanisms that could ensure the compliance of business processes in such dynamic environments without limiting the functionality and performance of the business processes [47], [25], [49]. In addition to these existing cases, the rapid development of autonomous cars strongly suggests that future traffic will be populated by intelligent autonomous cars that interact in shared (physical) environments called smart roads. In order to guarantee the safety and throughput of such smart road environments, the behaviour of autonomous cars has to be regulated by intelligent monitoring and coordination mechanisms without directly steering their behaviour [40], [38]. These and other applications show the urgency of tools and techniques to design, develop and analyse intelligent flexible control and coordination mechanisms.</paragraph><paragraph>These and many other applications can be best modelled as multi-agent systems. A multi-agent system consists of interacting computer systems that are situated in some environment and are capable of autonomous actions in the environment in order to meet their delegated objectives [68]. The individual agents are generally assumed to be heterogeneous in the sense that they may be designed and developed by various parties, using different technologies, and pursuing different objectives. It is exactly the autonomous and heterogeneous character of the computer agents in multi-agent applications that require intelligent flexible control and coordination mechanisms. In general, any control and coordination mechanism used in a multi-agent application should strike a balance between the autonomy of the agents on the one hand and the desirable global properties of the multi-agent system on the other hand, i.e., while the autonomy of agents should be respected, the global properties of multi-agent systems should be ensured.</paragraph><paragraph>Existing coordination techniques in computer science, such as synchronization techniques or interaction protocols, can be used to ensure the overall desirable properties of the interacting systems. However, these techniques will severely limit the autonomy and intelligence of the involved systems [29]. In artificial intelligence, norms and norm enforcement have been widely proposed as flexible and effective means for coordinating the behaviour of autonomous agents in multi-agent systems [50], [45], [14]. Singh et al. [62] provide an overview of various uses and applications of norms in multi-agent systems, and Criado et al. [26] discuss some challenges and open issues concerning representation, reasoning, creation and implementation of norms in multi-agent systems. A multi-agent system that uses norms to coordinate the behaviour of agents is often called norm-based multi-agent system or normative multi-agent system.</paragraph><paragraph>In general, there are two approaches to exploit norms for coordination purposes in multi-agent systems. Norms can be either endogenous to agents in the sense that they form an integral part of the agents' specifications [59], [60], or exogenous to agents in the sense that they are enforced by some external regulatory mechanism [2], [27]. Each approach comes with specific assumptions and applications. For example, the endogenous approach assumes that norms are internalised by the agents in the sense that the agents' decision making mechanisms are designed and developed based on a given set of norms. This assumption implies that norms are available at design time and enforced on agents by the agents' developers at design time. The endogenous approach can, for example, be used to examine the (emergent) effects of norms in agent-based simulations [59], [60], [48], [6]. In contrast, the exogenous approach is agnostic about norm internalisation, but assumes an authority that monitors agents' behaviour and enforces norms by means of preventing norm violations or imposing sanctions on violating behaviour [65], [2], [44], [57]. Following the exogenous approach, autonomous systems may respect norms or decide to violate them in which case they may incur sanctions. It is exactly the incursion of sanctions that may incentivize, but not restrict, agents to behave in a particular way. The exogenous approach is also conceived as a way to make the engineering of multi-agent systems easier to manage as it supports the general principle of ‘separation of concerns’, i.e., it supports the encapsulation of coordination and control concerns in a system entity that is separable from the agents' internals [10], [17], [70]. In addition, the external authority can be conceived as a mechanism that is designed to implement norms and the norm enforcement process. This perspective on norms and norm enforcement allows us to apply formal tools from game theory and to study and analyse norms and norm enforcement from a mechanism design perspective [18], [19], [30], [69].</paragraph><paragraph>This paper follows the exogenous approach and provides a mechanism design perspective on norms and norm enforcement. A multi-agent environment is modelled as a concurrent game structure where possible paths in the game structure denote possible execution traces of the corresponding multi-agent system. By considering the states of the game structure as the states of the environment in which agents operate, the concurrent game structure becomes the representation of a mechanism (game form) that specifies the effect of the agents' actions on the multi-agent environment. The enforcement of norms on a multi-agent system may change the effect of the agents' actions on the environment and thereby its underlying concurrent game structure and the set of possible execution traces. We call the modification of concurrent game structure by norm enforcement norm-based update. Clearly, various sets of norms can be used to update a concurrent game structure. The decision regarding which set of norms to use depends on the behaviour of the agents which in turn is a result of the agents' preferences, and, of course, the intended objective of the multi-agent system. We introduce norm-based mechanism design as a formal methodology to analyse norms and norm-based updates, and to find suitable norms taking into consideration agents' rational behaviour.</paragraph><paragraph>We aim at bridging the gap between norms and mechanism design by focusing on the relation between multi-agent systems, norm enforcement, and concurrent game structures. This relation sets the stage for studying formal properties of norm enforcement such as whether enforcing a set of norms implements a specific social choice function, which describes the desired system executions in relation to the agents' preferences, in specific equilibria. This also allows us, for example, to analyse whether a group of agents is willing to obey some norms. The formal analysis is closely related to work presented in [1] and [65], where the enforcement of norms is modelled by the deactivation of transitions. Adding norms to environments in our model may, however, either deactivate transitions in the case of regimenting norms or change the effect of actions in the case of sanctioning norms. The latter is achieved by (re)labelling states with new propositions that encode, for example, sanctions or incentives which can affect the behaviour of agents. Our work is also motivated by [59] and [60], where social laws were proposed to be used in computer science to control agents' behaviour.</paragraph><paragraph>The proposed norm-based mechanism design is an abstract methodology as it assumes concurrent game structures as models of multi-agent environments. In order to ground this methodology, we design an executable specification language to support the development of norm-based multi-agent systems. The aim of the executable specification language is to facilitate the implementation of multi-agent environments that are governed by norms. Multi-agent environments without norms are often implemented using a computational specification language that provides constructs to specify (initial) environment states and actions. The execution of such an environment specification is a process that continuously observes agents' actions and updates the environment state based on the specification of the observed actions. The implementation of multi-agent environments with norms requires additional constructs to specify norms and sanctions. The execution of an environment specification with norms includes two additional steps through which the observed agents' actions are further evaluated based on the given set of specified norms after which norm violations are either prevented or sanctioned. Our abstract norm-based mechanism design methodology can be applied to norm-based multi-agent environment programs in order to analyse whether the enforcement of a set of norms by a norm-based environment program can implement a specific social choice function. We also investigate the complexity of verifying whether a given norm-based multi-agent system implements a given system specification. Besides the verification problem we analyse the decision problem whether a norm-based multi-agent system with desirable properties exists at all. In terms of complexity our results are negative, in the sense that the problems are intractable. On the positive side however, the results are in line with existing complexity results of related problems.</paragraph><paragraph>The novel contribution of this work is a formal methodology for analysing norms and norm enforcement used for coordinating the behaviour of rational agents. This is realised by applying game theory/mechanism design techniques to study the effects of logic-based norms on multi-agent systems. We ground the theoretical analysis in an executable multi-agent setting to allow the development of norm-based coordination mechanisms for multi-agent systems. This paper extends and revises the work presented in [19]. The idea of norm-based mechanism design was first proposed in the extended abstract [18]. In comparison with [19] we significantly revise the formal setting, add new complexity results to new decision problems (weak and strong implementability) and give full proofs. In addition, the executable specification language as well as the related work section and a running example are new. The design of the executable specification language is inspired by the programming languages proposed in [28] and [27] for which an interpreter, called 2OPL (Organisation-Oriented Programming Language), has been developed.{sup:1} One of the main differences between these languages and our proposed executable specification language is the representation of norms. While norms in [28] and [27] are state-based (i.e., norms represent prohibited states), the proposed specification language in this paper considers conditional action-based norms (i.e., norms represent prohibited actions in specific states).</paragraph><paragraph>The structure of this paper is as follows. First, Section 2 presents concurrent game structures as models for multi-agent environments and connects it to game theory. Section 3 introduces a specification language for norm-based multi-agent environments, extends the formal setting of environments with norms and sanctions, and introduces the concept of norm-based mechanism design. Section 4 investigates the complexity of two implementation problems. Section 5 grounds the proposed approach by linking it to a norm-based multi-agent environment programming language. Finally, related work is discussed and some conclusions are drawn. The formal proofs about the computational complexity can be found in the appendix.</paragraph></section><section label="2"><section-title>Multi-agent environment model</section-title><paragraph>In order to illustrate our proposal, we shall use the following example throughout the paper. The example is closely related to the well-known train-gate controller example presented by [9]. Though, we slightly modify it to highlight the rational, decentralized decision making aspect of the car drivers.</paragraph><paragraph label="Example 1">Narrow road exampleThe scenario consists of a narrowed road and two cars at the opposite ends of the road. The cars cannot simultaneously pass through the narrowed road. In this scenario, both cars can wait for each other forever, both can move simultaneously causing a congestion/crash at the middle of the road, or one can move while the other is waiting. There are two alternatives for the latter case: either the first car moves while the second is waiting or vice versa. The system designer prefers one of these two alternatives. In order to realise this behaviour, the system designer aims at synthesizing a set of norms and corresponding sanctions such that when they are enforced the behaviour of the cars satisfies the given (system) preference assuming that the drivers' behaviours are driven by their own individual preferences. This scenario is illustrated in Fig. 1, where proposition {a mathematical formula}pix indicates that car i is at position {a mathematical formula}x∈{s,m,e} (s: start position, m: middle position, e: end position). We assume the following states in this scenario:</paragraph><list><list-item label="•">{a mathematical formula}q0: the cars are at their starting positions, i.e., {a mathematical formula}p1s∧p2s holds</list-item><list-item label="•">{a mathematical formula}q1: car 1 is at its ending and car 2 is at its starting position, i.e., {a mathematical formula}p1e∧p2s holds</list-item><list-item label="•">{a mathematical formula}q2: car 1 is at its starting and car 2 is at its ending position, i.e., {a mathematical formula}p1s∧p2e holds</list-item><list-item label="•">{a mathematical formula}q3: the cars are congested in the middle of the road, i.e., {a mathematical formula}p1m∧p2m holds</list-item><list-item label="•">{a mathematical formula}q4: the cars are at their ending positions, i.e., {a mathematical formula}p1e∧p2e holds</list-item></list><section label="2.1"><section-title>Concurrent structures and strategies</section-title><paragraph>In the following we introduce concurrent game structures (CGSs) from [9] (modulo minor modifications). They serve as models for our formal analysis of the environment in multi-agent systems. An environment in a multi-agent system is assumed to be specified in terms of a set of states, possibly including an initial state, and a set of (synchronized and concurrent) transitions. Informally speaking, a CGS is given by a labelled transition system where transitions are activated by action profiles.</paragraph><paragraph label="Definition 1">CGS, pointedA concurrent game structure ( CGS) is a tuple {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o), comprising a nonempty finite set of all agents {a mathematical formula}Agt={1,…,k}, a nonempty finite set of states Q, a nonempty finite set of atomic propositions Π (also called propositional symbols) and their valuation {a mathematical formula}π:Q→P(Π), and a nonempty finite set of (atomic) actions Act. Function {a mathematical formula}d:Agt×Q→P(Act)\{∅} defines nonempty sets of actions available to agents at each state, and o is a (deterministic) transition function that assigns the outcome state {a mathematical formula}q′=o(q,(α1,…,αk)) to state q and a tuple of actions {a mathematical formula}(α1,…,αk) with {a mathematical formula}αi∈d(i,q) and {a mathematical formula}1≤i≤k, that can be executed by {a mathematical formula}Agt in q. A pointed CGS is given by {a mathematical formula}(M,q) where {a mathematical formula}M is a CGS and q is a state in it.</paragraph><paragraph>In the following, we write {a mathematical formula}di(q) instead of {a mathematical formula}d(i,q) and {a mathematical formula}o(q,α→) instead of {a mathematical formula}o(q,(α1,…,αk)) for {a mathematical formula}α→=(α1,…,αk). In CGSs it is assumed that all the agents execute their actions synchronously.{sup:2} The combination of actions together with the current state determines the next transition of the system.</paragraph><paragraph label="Example 2">CGSOur scenario from Example 1 is formally modelled by CGS {a mathematical formula}M1=(Agt,Q,Π,π,Act,d,o), shown in Fig. 2, where</paragraph><list><list-item label="•">{a mathematical formula}Agt={1,2},</list-item><list-item label="•">{a mathematical formula}Q={q0,…,q4},</list-item><list-item label="•">{a mathematical formula}Π={pix|i∈{1,2} and x∈{s,m,e}},</list-item><list-item label="•">{a mathematical formula}Act={M,W},</list-item><list-item label="•">the function d is defined as{a mathematical formula}</list-item><list-item label="•">π and o are defined as illustrated in Fig. 2, e.g. {a mathematical formula}o(q0,(M,M))=q3.</list-item></list><paragraph>In the following we assume that a CGS {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) is given, if not said otherwise. A strategy of agent i is a conditional plan that specifies what i is going to do in each situation. It makes sense, from a conceptual and computational point of view, to distinguish between two types of “situations” (and hence strategies): an agent might base its decision only on the current state or on the whole history of events that have happened. In this paper we only consider the former type of strategies which are often called memoryless or positional. In general, memoryless strategies enjoy better computational properties than the second type of strategies called perfect-recall strategies. Note that ‘memoryless’ sounds more severe than it actually is: an agent is still able to base decisions on the current state; thus, finite histories up to an arbitrary but fixed length could be encoded within states.</paragraph><paragraph label="Definition 2">StrategyA (memoryless) strategy for agent i is a function {a mathematical formula}si:Q→Act such that {a mathematical formula}si(q)∈di(q). The set of such strategies is denoted by {a mathematical formula}Σi. A collective strategy for a group of agents {a mathematical formula}A={i1,…,ir}⊆Agt is a tuple{sup:3}{a mathematical formula}sA=(si1,…,sir) where each {a mathematical formula}sij, {a mathematical formula}1≤j≤r, is a strategy for agent {a mathematical formula}ij. The set of A's collective strategies is given by {a mathematical formula}ΣA=∏i∈AΣi. The set of all (complete) strategy profiles is defined as {a mathematical formula}Σ=ΣAgt.</paragraph><paragraph>Given the notation above, {a mathematical formula}(s1,s2) is a collective strategy of agents 1 and 2. A path{a mathematical formula}λ=q0q1…∈Qω is an infinite sequence of states such that for each {a mathematical formula}j=0,1,… there is an action tuple {a mathematical formula}α→∈dAgt(qj) with {a mathematical formula}o(qj,α→)=qj+1. The set of all paths starting in state q is denoted by {a mathematical formula}ΛM(q). We write {a mathematical formula}λ[j] to refer to the j-th state on path λ, and {a mathematical formula}λ[j,∞] to refer to the (sub)path {a mathematical formula}qjqj+1… of λ. Function {a mathematical formula}outM(q,s) returns the unique path that occurs when the agents execute (the complete) strategy profile s from state q onward.</paragraph><paragraph label="Definition 3">OutcomeThe outcome{a mathematical formula}outM(q,s)of a complete strategy profile{a mathematical formula}s=(s1,…,sk)from state q in model{a mathematical formula}M is the path {a mathematical formula}λ=q0q1q2… such that {a mathematical formula}q0=q and for each {a mathematical formula}j=0,1,2,… there is an action tuple {a mathematical formula}α→j=(α1j,…,αkj) with {a mathematical formula}αij=si(qj) for every {a mathematical formula}i∈Agt, and {a mathematical formula}o(qj,α→j)=qj+1. Often, we will omit subscript “{a mathematical formula}M” from {a mathematical formula}outM(q,s) if clear from context.</paragraph><paragraph>The following example illustrates how strategies in our example scenario can be represented.</paragraph><paragraph label="Example 3">StrategiesIn the CGS from Example 2, we encode individual strategies by a 5-tuple prescribing an action for each state. However, as agent 1 (resp. agent 2) has no choice other than to wait (W) in states {a mathematical formula}q1, {a mathematical formula}q3, and {a mathematical formula}q4 (resp. {a mathematical formula}q2, {a mathematical formula}q3, and {a mathematical formula}q4), we encode strategies by a tuple xy, indicating that agent 1 (resp. agent 2) executes actions x in {a mathematical formula}q0 and y in {a mathematical formula}q2 (resp. x in {a mathematical formula}q0 and y in {a mathematical formula}q1). The strategy MW for agent 1 selects M in state {a mathematical formula}q0 and W in state {a mathematical formula}q2 (and also W for states {a mathematical formula}q1, {a mathematical formula}q3, and {a mathematical formula}q4). Each agent has therefore 4 strategies resulting in a total of 16 different strategy profiles. Note that a strategy defines an action for all possible states even those not reachable if the current strategy is implemented. Therefore, some strategy profiles have identical outcome paths. For example, all strategies profiles in the set {a mathematical formula}{(M⋆1,M⋆2)|⋆1,⋆2∈{W,M}} result in the path {a mathematical formula}q0q3ω.</paragraph></section><section label="2.2"><section-title>Agents' preferences</section-title><paragraph>In the behavioural analysis of multi-agent systems, preferences of agents are often of utmost importance. They are the driving force of agents' behaviour. In the models we are considering, the agents' preferences are defined on the executions of the environment and thus paths in the corresponding CGS. Hence, we assume that agents prefer some executions over others. Therefore, we use temporal formulae to describe sets of paths. This idea was already followed in several pieces of work for similar purposes, e.g. [1] used CTL to represent agents' preferences and [23], [21] used ATL for the same purpose, where [65] used ATL for the representation of the objective of the social law. In this paper we use linear temporal logic LTL, first proposed by [56] for the verification of programs, for modelling preferences of agents. The logic extends propositional logic with operators that allow to express temporal patterns over infinite sequences of sets of propositions. It allows to express natural properties related to safety, liveness and fairness properties and combinations thereof. As we aim at evaluating system paths, which are infinite sequences of states, we need a logic which allows to compare paths rather than a logic the formulae of which are evaluated in states. This makes LTL a more natural choice than, e.g., CTL and ATL. The basic temporal operators are {a mathematical formula}U (until), □ (always), ◇ (eventually) and ◯ (in the next state). As before, propositions, drawn from a finite and non-empty set Π, are used to describe properties of states.</paragraph><paragraph label="Definition 4">Language LTLLet Π be a finite, non-empty set of propositions. The formulae of{a mathematical formula}LTLΠ are generated by the following grammar, where {a mathematical formula}p∈Π is a proposition: {a mathematical formula}φ::=p|¬φ|φ∧φ|φUφ|◯φ. For convenience, we define the two temporal operators ◇φ and □φ as macros {a mathematical formula}⊤Uφ and {a mathematical formula}¬◇¬φ, respectively, where {a mathematical formula}⊤≡p∨¬p denotes universal truth. We will omit the set of propositions “Π” as subscript of {a mathematical formula}LTLΠ if clear from context.</paragraph><paragraph>LTL-formulae are interpreted over ω-sequences{sup:4} (infinite words) w over sets of propositions, i.e. {a mathematical formula}w∈P(Π)ω. We use the same notation for words w as introduced for paths λ, e.g. {a mathematical formula}w[i] and {a mathematical formula}w[i,∞]. We note that a model {a mathematical formula}M and a path λ in {a mathematical formula}M—more precisely the valuation function included in the model—induce such an ω-sequence. However, we give the semantics in more general terms because it shall later prove convenient when relating the setting to mechanism design.</paragraph><paragraph label="Definition 5">Semantics {a mathematical formula}⊨LTLLet Π be a finite, non-empty set of propositions and {a mathematical formula}w∈P(Π)ω. The semantics of {a mathematical formula}LTLΠ-formulae is given by the satisfaction relation {a mathematical formula}⊨LTL defined by the following cases:</paragraph><list><list-item>{a mathematical formula}w⊨LTLp iff {a mathematical formula}p∈w[0] and {a mathematical formula}p∈Π;</list-item><list-item>{a mathematical formula}w⊨LTL¬φ iff not {a mathematical formula}w⊨LTLφ (we write {a mathematical formula}w⊭LTLφ);</list-item><list-item>{a mathematical formula}w⊨LTLφ∧ψ iff {a mathematical formula}w⊨LTLφ and {a mathematical formula}w⊨LTLψ;</list-item><list-item>{a mathematical formula}w⊨LTL◯φ iff {a mathematical formula}w[1,∞]⊨LTLφ; and</list-item><list-item>{a mathematical formula}w⊨LTLφUψ iff there is an {a mathematical formula}i∈N0 such that {a mathematical formula}w[i,∞]⊨ψ and {a mathematical formula}w[j,∞]⊨LTLφ for all {a mathematical formula}0≤j&lt;i.</list-item></list><paragraph label="Example 4">PreferenceIn our scenario, we assume that car 1 has the preference {a mathematical formula}p1s∧p2s→◯p1e modelling that it wants the next state to be its ending state (i.e., it prefers to pass through the narrow passage as the first car). The path {a mathematical formula}q0q2ω violates the preference, we have: {a mathematical formula}π(q0q2ω)⊭p1s∧p2s→◯p1e for π being the valuation function of {a mathematical formula}M1 from Example 2.</paragraph><paragraph>We use preference lists to define preferences of agents [23]. Such a list consists of a sequence of LTL-formulae each coupled with a natural number. The formula is a binary classifier of paths in the model—considering the induced ω-words over propositions. The natural number assigns a utility to the paths which satisfy the respective formula. Thus, a preference list assigns a utility value to all paths in a model.</paragraph><paragraph label="Definition 6">Preference list, preference profileA preference list over a set of propositions Π of an agent {a mathematical formula}i∈Agt={1,…,k} is given by a finite sequence {a mathematical formula}γi=((φ1,u1),…,(φl−1,ul−1),(φl,ul)) where each {a mathematical formula}φj∈LTLΠ, each {a mathematical formula}uj∈N for {a mathematical formula}j=1,…,l−1, {a mathematical formula}φl=⊤, and {a mathematical formula}ul=0. A preference profile over Π and {a mathematical formula}Agt is given by {a mathematical formula}γ→=(γ1,…,γk) containing a preference list over Π for each agent in {a mathematical formula}Agt. We typically use {a mathematical formula}Prefs to denote a non-empty set of such preference profiles. We omit mentioning the set of propositions Π and the set of agents {a mathematical formula}Agt, respectively, if clear from context.</paragraph><paragraph>A preference list of an agent may be interpreted as the agent's goals, denoting the behaviours that the agent wants to realise. Given an agent {a mathematical formula}i∈Agt with preference list {a mathematical formula}γ=((φ1,u1),…,(φl,ul)), a word w is assigned utility {a mathematical formula}uj (to denote the utility of outcome path λ with {a mathematical formula}π(λ)=w for agent i) if {a mathematical formula}φj is the formula with the smallest index j in γ that is satisfied on w, i.e., {a mathematical formula}w⊨φj and for all {a mathematical formula}l&lt;j we have that {a mathematical formula}w⊭φl. Note that there is always a formula in γ which is satisfied on w since the last formula in a preference list is required to be ⊤.</paragraph><paragraph label="Example 5">Preference profilesIn our scenario, we consider the following two preference profiles {a mathematical formula}γ→1 and {a mathematical formula}γ→2 of the cars where proposition {a mathematical formula}finei should be read as agent i has received a fine.{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula} The first preference profile is egoistic in the sense that the cars first prefer to reach their end positions directly without any sanction, then eventually to get to their end positions without any sanction, and finally to get no sanction. The second preference profile is more social because cars now first prefer that they both get to their end positions directly.We note that technically {a mathematical formula}fine1 and {a mathematical formula}fine2 are simply two fresh propositional symbols. As before states in the model can be labelled with them. Thus, it should be intuitive that the preferences can be used to classify paths according to the preferences. Later, in Section 3.1.4 we formally introduce the formal machinery to update a model by such new propositional symbols; in particular, in Example 11 we shall return to a formal treatment in the context of this example.</paragraph></section><section label="2.3"><section-title>From CGS to game theory</section-title><paragraph>In order to analyse the behaviour of agents and the emerging system behaviour, we use the machinery of game theory (cf. [54]). A strategic game form is a tuple {a mathematical formula}Gˆ=(Agt,(Aci)i∈Agt,O,g) where {a mathematical formula}Agt is the set of agents, {a mathematical formula}Aci is the set of actions available to agent {a mathematical formula}i∈Agt, O is a set of outcomes, and {a mathematical formula}g:Ac→O with {a mathematical formula}Ac=×i∈AgtAci is an outcome function that associates an outcome with every action profile. For the sake of readability, we use the same notation for agents in strategic game forms and CGSs. A strategic game{a mathematical formula}G=(Gˆ,(⪰i)i∈Agt) extends a strategic game form with a preference relation {a mathematical formula}⪰i on O for each agent {a mathematical formula}i∈Agt. The preference relation of agent i induces a preference relation on action profiles: {a mathematical formula}a1⪰ia2 iff g(a1)⪰ig(a2) for any {a mathematical formula}a1,a2∈Ac. We also use payoff functions {a mathematical formula}μi:O→R to represent preference relations where higher payoff values correspond to more preferred outcomes.</paragraph><paragraph>It is well known how an extensive form game can be transformed to a strategic game without changing specific sets of strategy profiles, e.g. the set of Nash equilibria. The next definition connects CGSs to strategic games following the notation of [23]. The actions of the strategic game correspond to the (memoryless) strategies in the CGS. A payoff function is obtained from preference profiles and the outcome paths resulting from strategy profiles.</paragraph><paragraph label="Definition 7">CGS ↝ strategic game form, strategic gameLet {a mathematical formula}(M,q) be a pointed CGS with {a mathematical formula}Agt={1,…,k} and {a mathematical formula}γ→=(γ1,…,γk)∈Prefs be a preference profile. We define {a mathematical formula}Γ(M,q) as the strategic game form {a mathematical formula}(Agt,(Σi)i∈Agt,ΛM(q),g) associated with {a mathematical formula}(M,q) where {a mathematical formula}Agt is the set of agents in {a mathematical formula}M, {a mathematical formula}Σi is the set of strategies of agent i (cf. Definition 2), {a mathematical formula}ΛM(q) the set of all paths in {a mathematical formula}M starting in state q, and {a mathematical formula}g(s)=outM(q,s) for {a mathematical formula}s∈ΣAgt. Moreover, we define {a mathematical formula}Γ(M,q,γ→) as the strategic game {a mathematical formula}(Γ(M,q),(μi)i∈Agt) associated with {a mathematical formula}(M,q) and {a mathematical formula}γ→, where for all {a mathematical formula}s∈ΣAgt the payoff function {a mathematical formula}μi is defined as follows: {a mathematical formula}μi(s)=uj where {a mathematical formula}γi=((φ1,u1),…,(φl−1,ul−1),(φl,ul)) and j is the minimal index such that {a mathematical formula}π(outM(q,s))⊨LTLφj.</paragraph><paragraph>In general, not all paths from a CGS can be obtained by memoryless strategies, i.e. {a mathematical formula}⋃s∈ΣAgtoutM(q,s)≠ΛM(q). As mentioned before, we use memoryless strategies because of their better computational properties. Note that the obtained strategic game form is always finite as the set of strategies of each agent is finite. Note also that the just defined strategic game is well-defined, especially because for the last entry {a mathematical formula}(φ,u) of each preference list we have that {a mathematical formula}φ=⊤.</paragraph><paragraph label="Example 6">Strategic game formFor our car scenario, the strategic game form {a mathematical formula}Γ(M1,q0) associated with {a mathematical formula}(M1,q0) is illustrated in Fig. 3. Strategies are represented according to the conventions of Example 3.</paragraph><paragraph>A game theoretic solution concept, e.g. Nash equilibria or dominant strategy equilibrium, can be considered as a function {a mathematical formula}S the domain of which is the set of strategic games and the image of which is a set of strategy profiles [54]. That is, for each strategic game G with strategy profiles Σ we have that {a mathematical formula}S(G)⊆Σ. We assume that the reader is familiar with the notion of solution concept and refer to [54] for further details. In the present work we are mainly concerned with the concept of Nash equilibrium and also discuss dominant strategy equilibrium. Therefore, we give the basic definitions. An action profile {a mathematical formula}(a1,…,ak), containing an action for each agent, is a Nash equilibrium if no agent can unilaterally deviate to get a better payoff; that is, for each agent i it must be the case that for all actions {a mathematical formula}ai′ of that agent it holds that {a mathematical formula}(a1,…,ak)⪰i(a1,…,ai−1,ai′,ai+1,…,ak). We use {a mathematical formula}NE(G) to refer to the Nash equilibria in a given game G. A dominant strategy equilibrium is a stronger notion. Formally, a profile {a mathematical formula}(a1,…,ak) is a (weakly) dominant strategy equilibrium if for each agent i, the action {a mathematical formula}ai is the best action independent of the choices of the opponents. Such an action {a mathematical formula}ai is called (weakly) dominant strategy.{sup:5} We refer to [58] for more details. Furthermore, we use {a mathematical formula}DOE(G) to refer to the set of dominant strategy equilibria in a given game G. We lift the notion to tuples consisting of a CGS{a mathematical formula}M and a preference profile {a mathematical formula}γ→ by {a mathematical formula}S(M,q,γ→):=S(Γ(M,q,γ→)).</paragraph><paragraph label="Example 7">Example 6 contd.: strategic gamesWe include the preference profiles {a mathematical formula}γ→1 and {a mathematical formula}γ→1, respectively, from Example 5 in the strategic game form of Example 6. We obtain the strategic games {a mathematical formula}Γ(M1,q0,γ→1) and {a mathematical formula}Γ(M1,q0,γ→2) which are shown in Fig. 4. Bold entries are used to identify Nash equilibria. For example, we have that {a mathematical formula}NE(M1,q0,γ→1)={(WM,M⋆2)|⋆2∈{M,W}}∪{(M⋆1,WM)|⋆1∈{M,W}}. The game {a mathematical formula}Γ(M1,q0,γ→1) has no dominant strategy equilibria whereas {a mathematical formula}DOE(M1,q0,γ→2)={(M⋆1,M⋆2)|⋆1,⋆2∈{M,W}}.</paragraph><paragraph>It is worth to note that pure Nash equilibria as well as dominant strategy equilibria may not exist. The matching pennies game{sup:6} is a classical, strictly-competitive game without Nash equilibria [54].</paragraph></section><section><section-title>Concluding remarks</section-title><paragraph>In this section we introduced the formal setting in which we shall study the effects of imposing norms on a multi-agent system. We used LTL to define agent's preferences. In combination with concurrent game structures they allow us to relate the multi-agent setting to normal form games, which are a well-studied mathematical model to investigate the outcome of interactions among rational agents. We presented the concept of Nash equilibrium and dominant strategy equilibrium as examples to capture agents' rational behaviour. The general idea of normative mechanism design, introduced in the next section, however, does not depend on a specific solution concept.</paragraph></section></section><section label="3"><section-title>Norm-based mechanism design</section-title><paragraph>So far we have considered a concurrent game structure as a formal tool to model a multi-agent system. We also assumed that agents have preferences over the system behaviour. As the actual agents' preferences may not be known in advance, we may only assume that a set of possible preference profiles over the system behaviour is given—containing those preferences which seem sensible to the system designer. In the previous section we explained how these ingredients set the stage to analyse the “optimal” behaviours of the multi-agent systems by considering the equilibria of the game constructed from the concurrent game structure and the preference profiles. With “optimal” we refer to the system behaviours that correspond to the outcome of Nash equilibria. In this section, we further assume that a social choice function is given that models the multi-agent system designer's preferred system behaviour, also called the social perspective. A social choice function indicates the system behaviour that is socially preferred (or preferred by the system designer) when the agents act according to a specific preference profile.</paragraph><paragraph>The problem that we consider in this section is the implementation problem that can be formulated as follows. Suppose that for a given set of agents' preference profiles the socially preferred system behaviour is not aligned with the agents' preferred system behaviour, i.e. for the given agents' preference profiles the optimal system behaviour from the agents' perspective (represented by Nash equilibria) is not aligned/contradicts with the optimal system behaviour from the system designer's perspective (represented by the social choice function). The question is whether the enforcement of some norms (e.g., by means of regimentation and sanctions) on the agents' behaviour can align the preferred system behaviour from both perspectives, i.e., whether the enforcement of some norms on the agents' behaviour can change the agents' behaviour toward the socially preferred system behaviour. A second more elaborate question is about the existence of a set of norms whose enforcement aligns the preferred system behaviour from the agents' as well as system designer's perspectives. We coin the design of a set of norms to change the agents' behaviour, which is inspired by mechanism design, norm-based mechanism design.</paragraph><paragraph>In order to ease the reading of the rest of this paper, we list some of the concepts used in our formalisation and their notation in Fig. 5.</paragraph><section label="3.1"><section-title>Preliminaries</section-title><paragraph>We start with reviewing concepts from classical mechanism design and introducing corresponding concepts for norm-based mechanism design. In particular, we define the concept of normative behaviour function which is the counterpart of the concept of social choice function in classical mechanism design. We then introduce the concept of norms and norm-based multi-agent system, and explain how they can influence the agents' behaviour. We define the notion of norm enforcement formally as an update function that changes the specification of the multi-agent system based on a given set of norms. These concepts set the stage for norm-based mechanism design.</paragraph><section label="3.1.1"><section-title>Classical mechanism design</section-title><paragraph>In our exposition of classical mechanism design we mostly follow the presentation of [54], in particular we often use their notation which allows us to clearly relate the classical concepts with the corresponding ones of norm-based mechanism design. In social choice theory a social choice rule{a mathematical formula}f:P→P(O) assigns a subset of outcomes from the set of outcomes O to a preference profile {a mathematical formula}⪰=(⪰i)i∈Agt∈P, consisting of a preference relation {a mathematical formula}⪰i over O for each agent from {a mathematical formula}Agt. Here, we consider P to be a set of such preference profiles. In contrast, a social choice function picks exactly one element of the outcome rather than a set of outcomes. For example, a profile {a mathematical formula}(⪰1fast,⪰2fast) could express that both agents have a preference for driving fast. In that case a social choice function can be used to map the preference profile to the outcome {a mathematical formula}oh representing “build highway”, i.e. {a mathematical formula}f((⪰1fast,⪰2fast))=oh.</paragraph><paragraph>In the following discussion we focus on social choice rules. The outcome {a mathematical formula}f(⪰) is the social outcome defined by the social choice rule f. Mechanism design is concerned with the problem of constructing a mechanism—a strategic game form—which implements a social choice rule assuming a specific rational behaviour of the agents. The mechanism is constructed over a structure fixing the set of agents {a mathematical formula}Agt, the set of possible outcomes O, the set of possible preference profiles P over outcomes O, and a set of strategic game forms {a mathematical formula}G with outcomes in O. The mechanism is drawn from {a mathematical formula}G. This fixed structure has different names in the literature. Osborne and Rubinstein [54] refer to it as environment while Shoham and Leyton-Brown [58] use a related formalisation in terms of Bayesian game settings. We follow [54] but, in order to avoid confusion with our notation, refer to the structure as implementation setting. Now, a strategic game form {a mathematical formula}G∈G together with a preference profile {a mathematical formula}⪰∈P defines a strategic game{a mathematical formula}(G,⪰). Given a solution concept {a mathematical formula}S (e.g., Nash equilibrium) for strategic games, i.e. a mapping from {a mathematical formula}(G,⪰) to a set of action profiles, we can formulate the {a mathematical formula}S-implementation problem over an implementation setting as follows. A game form {a mathematical formula}G∈G{a mathematical formula}S-implements the social choice rule f over P if, and only if, for all preference profiles {a mathematical formula}⪰∈P and all equilibria {a mathematical formula}(a1,…,a|Agt|)∈S(G,⪰) we have that the outcome obtained by {a mathematical formula}(a1,…,a|Agt|) is contained{sup:7} in {a mathematical formula}f(⪰). We emphasize that {a mathematical formula}S(G,⪰) contains the strategy profiles in the game {a mathematical formula}(G,⪰) that satisfy the solution concept {a mathematical formula}S.</paragraph></section><section label="3.1.2"><section-title>Normative behaviour function</section-title><paragraph>As discussed above, in social choice theory a social choice rule assigns outcomes to given profiles of preferences. In our setting, the social choice rule represents the preference of the system designer and is defined as a function{sup:8} that assigns an LTL-formula—describing a set of paths—to each preference profile (a sequence of sequences of LTL-formulae) of the agents. From now on, when considering norm-based mechanism design concepts we use our own notation that has been introduced in previous sections.</paragraph><paragraph label="Definition 8">Normative behaviour functionLet {a mathematical formula}Prefs be a set of preference profiles over Π. A normative behaviour function{sup:8}f is a mapping {a mathematical formula}f:Prefs→LTLΠ.</paragraph><paragraph>Similar to classical mechanism design, the preference of the system designer describes the designer's desired outcome if the agents had the given preference profile as their true profile. Thus, representing the preference of the system designer by a (normative behaviour) function allows us to model the system designer's uncertainty about the true preferences of the agents. The following is a simple example where the preference of the system designer is independent of the agents' preferences such that it maps possible preference profiles to an LTL-formula. We choose deliberately to keep the example simple, but it should be clear that the preference of the system designer is not always a single formula and often depends on the agents' preferences.</paragraph><paragraph label="Example 8">Normative behaviour functionWe assume that the preference of the system designer is represented by the following normative behaviour function, which indicates that in all cases the first car should reach its end position directly: {a mathematical formula}f(γ→i)=◯p1e.</paragraph><paragraph>We refer to the outcome as the normative outcome wrt. a given preference profile. In our view, the aim of norm-based mechanism design is to come up with a norm-based mechanism (i.e., an environment specified in terms of actions, norms and sanctions) such that the agents—again following some rationality criterion according to their preferences—behave in such a way that the possible environment executions stay within the normative outcome. The idea is that norms and sanctions will (de)motivate agents to perform specific actions.</paragraph><paragraph label="Example 9">Non-aligned preferencesFollowing Example 7, for {a mathematical formula}γ→1, the path {a mathematical formula}q0q2q4ω (yielded by strategy profile {a mathematical formula}(WM,MW), which is a Nash equilibrium) in {a mathematical formula}M1 does not satisfy the preference {a mathematical formula}f(γ→1) of the system designer.</paragraph></section><section label="3.1.3"><section-title>Normative system, hard and soft facts</section-title><paragraph>In the remainder of this section, let Π again be a finite and non-empty set of propositions. We assume that Π can be partitioned into two types of propositions: the set {a mathematical formula}Πh denotes the hard facts and {a mathematical formula}Πs the soft facts. Hard facts describe the (physical/brute) properties of the multi-agent environment which in turn are used to define the action structure of a CGS. For example, typical hard facts describe that a car is at a certain position or that the traffic light is red. Soft facts are used to model properties which do not affect the action structure but can affect the agents' preferences. For example, they model that a car has violated a traffic norm, a car received a fine, or a car arrived late at its destination (where we use car as a shorthand for “the driver of a car” etc.). In other words, the modification of soft facts does not change the available actions nor their executability but they can affect the evaluation of the agents' preferences.</paragraph><paragraph>The classification into hard and soft facts depends on the specific modelling. In the following we assume that we are given a set of hard facts{a mathematical formula}Πh and a set of soft facts{a mathematical formula}Πs, where sets are finite, non-empty and disjoint. Therefore, we assume that a CGS {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) is given with {a mathematical formula}Π=Πh∪Πs. In this paper, we also distinguish between two types of norms. Sanctioning norms are enforced through sanctions imposed on the execution of actions from specific states. Regimenting norms are enforced by modifying the effect of the execution of actions from a specific state; they make the actions effectless [27]. In the following, we use {a mathematical formula}PLX for a set X of propositional atoms to refer to all propositional formulae over X and denote by {a mathematical formula}⊨PL the satisfaction relation of propositional logic.</paragraph><paragraph label="Definition 9">Norms and normative systemA norm (over {a mathematical formula}M) is an element from {a mathematical formula}PLΠh×P(Actk)\{∅}×(P(Πs)\{∅}∪{⊥}). A norm of type {a mathematical formula}(φ,A,⊥) is a regimenting norm and one of type {a mathematical formula}(φ,A,S) with non-empty {a mathematical formula}S⊆Πs a sanctioning norm. A set of such norms {a mathematical formula}N is called a normative system (over {a mathematical formula}M) with a typical norm denoted by η. {a mathematical formula}Nϵ=∅ is the empty normative system, and {a mathematical formula}Nrs={N1,N2,…} is the set of all normative systems. We define {a mathematical formula}Ns and {a mathematical formula}Nr as the set of all normative systems consisting of only sanctioning and regimenting norms, respectively. We often write {a mathematical formula}(φ,A,⋅) to refer to a sanctioning or regimenting norm.</paragraph><paragraph>A norm {a mathematical formula}(φ,A,⊥) should be read as: “it is forbidden to perform action profiles from {a mathematical formula}A in φ-states (i.e. states in which φ holds) and that the enforcement of this norm prevents action profiles in {a mathematical formula}A to be performed”. A norm {a mathematical formula}(φ,A,S) should be read as: “it is forbidden to perform action profiles from {a mathematical formula}A in φ-states and that the enforcement of this norm imposes sanctions S on the states resulting by performing the very action profile from {a mathematical formula}A in a φ-state”. The basic idea is that sanctioning norms enforced on a model change the valuation of states by soft facts only; that is, the underlying physical structure represented by hard facts remains intact and thereby the action structure of the model remains intact. As we will see later in this paper, the agents' preferences are also defined on soft facts such that any changes in valuations of states by soft facts may affect agents' rational behaviour. Regimenting norms affect the physical structure as they directly affect the action structure of the model.</paragraph><paragraph>We note that a norm {a mathematical formula}(φ,A,⋅) can be seen as a prohibition (“it is prohibited that any action profile in {a mathematical formula}A is executed in states satisfying φ”) but just as well as a obligation to perform an action profile not in {a mathematical formula}A in any state satisfying φ. This also corresponds to the duality of obligations and prohibitions: a prohibition of doing an action, is the same as being obliged to not doing the action.</paragraph><paragraph label="Example 10">NormsIn order to avoid cars to congest in the narrow part of the road we introduce two sanctioning norms. The first norm prohibits the first car to wait in the start position {a mathematical formula}p1s∧p2s when the second car waits as well. The violation of this norm imposes sanction {a mathematical formula}fine1 (i.e., car 1 is sanctioned). This norm is represented as {a mathematical formula}(p1s∧p2s,{(W,W)},{fine1}). The second norm prohibits the second car to move in the start position. This norm is represented as {a mathematical formula}(p1s∧p2s,{(⋆,M)|⋆∈{M,W}},{fine2}). Note that these norms implement a priority for passage in the narrowed road by obliging car 1 to move and car 2 to wait.</paragraph><paragraph>It is important to note that sanctioning norms can directly influence the (quantitative) utility that an agent obtains as specific elements of a preference list may no longer be satisfiable. To illustrate this, consider the preference profile {a mathematical formula}γ→1 introduced in Example 5:{a mathematical formula} In general, there can be an outcome of the system which ensures agent 1 a payoff of 3, i.e. an outcome that satisfies {a mathematical formula}◯p1e∧□¬fine1. The enforcement of a norm by sanctioning norms of type {a mathematical formula}(φ,A,{fine1}) for an appropriate formula φ and set of action profiles {a mathematical formula}A, however, can make it impossible to yield outcomes on which {a mathematical formula}¬□fine1 holds. In such a case, it would be impossible for the agent to obtain a utility of 3. The technical details of this procedure are introduced in the next section.</paragraph></section><section label="3.1.4"><section-title>Norm-based update</section-title><paragraph>In order to examine the impact of the enforcement of a set of norms on multi-agent systems, we need to determine applicable norms and their sanctions. Therefore, we need to decide which norms are applicable in a concurrent game structure and what are the sanctions that should be imposed on the concurrent game structure. A norm {a mathematical formula}η=(φ,A,⋅) is applicable in a state which satisfies φ and in which an action tuple from {a mathematical formula}A is being performed.</paragraph><paragraph label="Definition 10">Applicable norms and sanctionsLet {a mathematical formula}N∈Nrs be a normative system, {a mathematical formula}X⊆Π be a set of facts, and {a mathematical formula}α→ be an action profile. The set of norms from {a mathematical formula}Napplicable wrt. X and {a mathematical formula}α→, denoted by {a mathematical formula}AppN(X,α→), is defined as follows:{a mathematical formula} The set of sanctions from {a mathematical formula}N that should be imposed based on X and {a mathematical formula}α→, denoted as {a mathematical formula}SanN(X,α→), is computed as follows:{a mathematical formula}</paragraph><paragraph>We note that the evaluation of {a mathematical formula}X⊨PLφ can be done in polynomial time as it corresponds to evaluating a propositional formula with respect to a given truth assignment represented by the set X. Note that we use a set of facts {a mathematical formula}X⊆Π (rather than a state {a mathematical formula}q∈Q) that triggers norms and sanctions. We do this in order to reuse this definition later on in this paper. In the following, we use {a mathematical formula}SanN(π(q),α→) to determine the sanctions wrt. {a mathematical formula}N, q and {a mathematical formula}α→. We emphasize that the computed sanctions in a state q are meant to be imposed on the state which is reached when executing {a mathematical formula}α→, i.e., the computed sanctions are imposed on state {a mathematical formula}o(q,α→).</paragraph><paragraph>The enforcement of a set of norms on a multi-agent system can be modelled as updating the concurrent game structure of the multi-agent system by a normative system, i.e. by regimenting or sanctioning behaviour depending on the type of norm.</paragraph><paragraph label="Definition 11">Update by normsLet {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) be a concurrent game structure and {a mathematical formula}N∈Nrs be a normative system over {a mathematical formula}M. The update of{a mathematical formula}Mwith{a mathematical formula}N, denoted by {a mathematical formula}M↾N, is the CGS {a mathematical formula}(Agt,Q′,Π,π′,Act,d′,o′) where</paragraph><list><list-item label="1.">{a mathematical formula}Q′:=Q∪{(q,S)|∃q′∈Q,α→∈Actk:o(q′,α→)=q, S=SanN(π(q′),α→) and {⊥}≠S≠∅}</list-item><list-item label="2.">{a mathematical formula}π′(x)={π(x)if x∈Qπ(q)∪Sif x=(q,S)</list-item><list-item label="3.">{a mathematical formula}di′(x)=di(q) where {a mathematical formula}x=q or {a mathematical formula}x=(q,S), and {a mathematical formula}i∈Agt</list-item><list-item label="4.">{a mathematical formula}o′(x,α→)={(o(q,α→),S) if S=SanN(π(q),α→)and{⊥}≠S≠∅o(q,α→) if SanN(π(q),α→)=∅x otherwise (i.e. {⊥}=SanN(π(q),α→)) </list-item><list-item>for either {a mathematical formula}x=(q,S′) with {a mathematical formula}S′⊆Πs, or {a mathematical formula}x=q.</list-item></list><paragraph>The first item shows how to duplicate states in Q that are sanctioned but not regimented. The second item defines the updated evaluation of (sanctioned) states. The third item ensures that the new (duplicated) states have the same options as their original counterparts. This is due to the fact that sanctions are solely defined on soft propositions not affecting the transition structure. Finally, the fourth item ensures that the outcome of actions from either the states in Q or their (new) duplicates are in accordance with the original model whenever the actions in those states are not regimented; otherwise when regimented, the actions have no effect and loop in the same state. It is important to notice that looping of a regimented action in a state models a situation where the action cannot be performed and should therefore be interpreted as non-performance of the action. In this case the system remains in the same state. The following example illustrates how the effect of actions is determined in an updated model.</paragraph><paragraph label="Example 11">Norm-based updateThe norm-based update of the environment model {a mathematical formula}M1, as illustrated in Fig. 2, based on the normative system {a mathematical formula}N1={(p1s∧p2s,{(W,W)},{fine1}),(p1s∧p2s,{(⋆,M)|⋆∈{M,W}},{fine2})}, denoted by {a mathematical formula}M2:=M1↾N1, results in a new CGS shown in Fig. 6. In particular, the effects of action profiles {a mathematical formula}(M,M),(M,W), and {a mathematical formula}(W,W) in the duplicated state {a mathematical formula}(q0,{fine1}) from {a mathematical formula}M1↾N1 are defined as follows:{a mathematical formula}{a mathematical formula}{a mathematical formula} A different norm-based update of the environment model {a mathematical formula}M1 by the normative system that consists of only regimenting norms {a mathematical formula}N2={(p1s∧p2s,{(M,M)},⊥),(p1s∧p2s,{(W,M)},⊥)} is illustrated in Fig. 7. It can be observed that the regimentation of these norms does not allow car 2 to move at the starting position. Note that in our system only action profiles can be the subject of norm regimentation. However, this does not mean that individual agents cannot be the subject to norm regimentation. For example, the resulting transition system in Fig. 7 allows car 1 to do a move action in the starting position while it prevents the move action of car 2.</paragraph><paragraph>It is important to notice that sanctions do not accumulate through state transitions. This is reflected by the following proposition which expresses that states carry the sanctions caused by the most recent action only, or the most recent action has been regimented.</paragraph><paragraph label="Proof">Non-accumulation of sanctionsLet{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o)and{a mathematical formula}M↾N=(Agt,Q′,Π,π′,Act,d′,o′),{a mathematical formula}q∈Qand either{a mathematical formula}x=qor{a mathematical formula}x=(q,S′)withS′⊆Πs. We have{a mathematical formula}Directly follows from Definition 11 because {a mathematical formula}o′ ensures that {a mathematical formula}S′ is ignored after {a mathematical formula}α→.  □</paragraph><paragraph>The non-accumulation of sanctions holds for state transitions that take place based on non-regimented actions. A regimented action that loops in a state with sanctions causes state transitions through which sanctions are not removed (consecutive states through regimentation actions are identical). Although this may suggest that sanctions are accumulated through consecutive states, the persistence of the sanctions in state transitions caused by regimented actions should not be interpreted as receiving sanctions repeatedly. Note that this is consistent with our interpretation that regimented actions cannot be performed, i.e., consecutive states through regimented actions should be considered as remaining in one and the same state. However, we also agree that an alternative solution could be the one where the regimented norm causes a state transition to a fresh copy of the current state with all sanctions removed. This is a design choice. We also note that in a previous version of this work [19] we completely removed specific transitions. As a result the selection of individual actions by agents could yield invalid action profiles. The selection of such invalid action profiles was then essentially avoided by assigning to them a negative utility.</paragraph><paragraph>We also observe that updating a CGS with only regimenting norms does not duplicate any state and may only modify the accessibility relation between the state. However, updating a CGS with sanctioning norms can duplicate and introduce new states. This update is essentially different from norm update as introduced by [1]. In case of regimentation we remove accessibility relation between states (by enforcing a looping transition) and in case of sanctioning we add new copies of states and extend the accessibility relation. In [1] an update results in restricting the accessibility relation by removing transitions from the model. Another characteristic of our model update is that different orders of updates with regimenting norms result in the same outcome, which in turn is the same as the single update with the union of regimenting norms. That is, we have that:{a mathematical formula}</paragraph><paragraph>Observe that this is not true for sanctioning norms, i.e., different orders of updates with sanctioning norms do not necessarily result in the same CGS. This means that for some CGS {a mathematical formula}M and non-empty sets {a mathematical formula}SN1 and {a mathematical formula}SN2 we may have{a mathematical formula} This is due to the fact that updates of sanctioning norms may duplicate states such that subsequent updates generate different states. One obvious reason different orders of updates by sanctioning norms result in different models is of a syntactic nature, interpreting = as strict/syntactic equality. We illustrate this with a small example. Suppose that a new state {a mathematical formula}(q,S1) is the result of a norm update. Now, if the model is updated by another normative system, this may yield states of type {a mathematical formula}((q,S1),S2). Clearly, reversing the order in which the update is performed yields states of type {a mathematical formula}((q,S2),S1) which are different from the previous types of states whenever {a mathematical formula}S1≠S2. Next we consider an example which shows that subsequent updates with sanctioning norms may generate more states than updating with the union of the sanctioning norms at once as illustrated in the example below. Conceptually, all norm violations occurring at the same time should also be sanctioned immediately. This is reflected in our definition of norm update and illustrated in Example 12 and Fig. 8.</paragraph><paragraph label="Example 12">Update order with sanctioning normsWe consider the CSG {a mathematical formula}M shown in Fig. 8 and update it with {a mathematical formula}SN1={(qˆ,{a},{s1})} and {a mathematical formula}SN2={(qˆ,{a},{s2})} in different orders, and at once ({a mathematical formula}qˆ is a propositional formula that is true only in state q), respectively.</paragraph><paragraph>The example showed that the models are different. Interestingly, however, in Example 12 if the models {a mathematical formula}(M↾SN1)↾SN2, {a mathematical formula}(M↾SN2)↾SN1 and {a mathematical formula}M↾(SN1∪SN2) are restricted to states reachable from q, then these models are identical apart from the names of their states. Essentially, given a pointed CGS, different orders of updates with sanctioning norms result in similar CGSs if we consider the parts of the updated CGSs that are reachable from the initial state only. Indeed, it can be shown that the models {a mathematical formula}(M↾SN1)↾SN2 and {a mathematical formula}(M↾SN2)↾SN1 are identical apart from the names of states{sup:9} if restricted to the part reachable from a given state. One may be tempted to conclude that in the same sense these models are similar to {a mathematical formula}M↾(SN1∪SN2); however, this is not always the case as the reachable part of the latter model may have strictly less states than the previous models. It is our conjecture, however, that the reachable part of these models are bisimilar to each other. As these results are not directly relevant for the exposition of this paper, we omit a formal treatment of this matter.</paragraph><paragraph>The order of updates for some non-empty sets of sanctioning and regimenting norms is important as well, i.e., updating first with sanctioning norms and then with regimenting norms may result in different outcomes than updating first with regimenting norms and then with sanctioning norms. Moreover, subsequent updates with regimenting and sanctioning norms do not necessarily result in the same outcome as the single update with the union of regimentation and sanctioning norms. The reason for this is that two consecutive updates by {a mathematical formula}RN and {a mathematical formula}SN result in different outcomes as one single update by {a mathematical formula}RN∪SN. The basic intuition is that regimenting norms have priority and if an action has been regimented it does not make sense to sanction the action as it can by the nature of regimentation no longer be executed. This means that for some CGS {a mathematical formula}M and non-empty sets {a mathematical formula}SN and {a mathematical formula}RN, we have{a mathematical formula}</paragraph><paragraph>These observations are illustrated in Example 13 and Fig. 9, where {a mathematical formula}(M↾SN)↾RN is different from {a mathematical formula}(M↾RN)↾SN, which is again different from {a mathematical formula}M↾RN, which is in this particular case the same as {a mathematical formula}M↾(RN∪SN).</paragraph><paragraph label="Example 13">Update order with sanctioning and regimenting normsWe consider the CGS {a mathematical formula}M shown in Fig. 9 and update it with {a mathematical formula}SN={(qˆ,{a},{s})} and {a mathematical formula}RN={(qˆ,{a},⊥)} in different orders, and at once ({a mathematical formula}qˆ is a propositional formula that is true only in state q), respectively.</paragraph><paragraph>We summarize the results in the following proposition.</paragraph><paragraph label="Proposition 2">Properties of norm updateLet{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o)be a CGS,{a mathematical formula}RN,RN′∈Nrbe sets of regimenting norms, and{a mathematical formula}SN∈Nsbe a set of sanctioning norms over{a mathematical formula}M. Also let{a mathematical formula}N=RN∪SN. Then, we have</paragraph><list><list-item label="1.">{a mathematical formula}M↾Nϵ=M</list-item><list-item label="2.">{a mathematical formula}(M↾RN)↾RN′=(M↾RN′)↾RN=M↾(RN∪RN′)</list-item><list-item label="3.">In general, we have:{a mathematical formula}(M↾SN)↾RN≠M↾N=M↾(RN∪SN)≠(M↾RN)↾SN</list-item><list-item label="4.">If forall{a mathematical formula}q∈Qand for all{a mathematical formula}α→∈dAgt(q)we have that{a mathematical formula}AppRN(π(q),α→)=∅or{a mathematical formula}AppSN(π(q),α→)=∅then{a mathematical formula}M↾N=(M↾RN)↾SN.</list-item></list><paragraph label="Proof">(1) Obvious. (2) Regimenting norms may introduce loops. Applying a regimenting norm on an already regimented action has no effect. (3) Follows from Example 13. (4) If in no state a regimenting norm and a sanctioning norm are applicable, then the updates cannot interfere. The result follows trivially.  □</paragraph><paragraph>The properties above concern the iterated applications of the update operation. In the rest of this paper, we focus on single updates.</paragraph></section></section><section label="3.2"><section-title>Implementation setting</section-title><paragraph>In this section we introduce norm-based mechanism design by considering a mechanism using norms to influence the agents' behaviour. We use a normative behaviour function to assign a set of “desired” environment executions (desired from the system designer's perspective), represented by an LTL-formula, to each preference profile of the agents. Thus, based on the preferences of the agents the system designer prefers specific outcomes. The aim of norm-based mechanism design is to yield a normative system (i.e., a set of regimenting and sanctioning norms) for a given environment such that the enforcement of this normative system in the environment motivates the agents to behave in such a way that the outcomes preferred by the system designer are realised. The idea is that the enforcement of norms (de)motivates or prevents agents to perform specific actions. In order to predict how agents act we assume that they act rationally. What playing rationally means shall be specified by game theoretic solution concepts.</paragraph><paragraph>Below we introduce the norm-based implementation setting, where mechanism design is interpreted in terms of norms. The basic structure consists of a CGS, a set of preference profiles, and a set of normative systems which if applied on the CGS change the underlying CGS. A tuple consisting of the CGS and such a normative system is called norm-based mechanism. In this sense a norm-based mechanism relates to game forms which are mechanisms in classical mechanism design. Outcomes are defined as paths and preferences over those paths are specified by preference profiles based on LTL-formulae. In Fig. 10 we summarize the correspondence between (classical) mechanism design and norm-based mechanism design.</paragraph><paragraph label="Definition 12">Norm-based implementation settingA (norm-based) implementation setting (NIS) is given by a tuple {a mathematical formula}I=(M,Prefs,N) where</paragraph><list><list-item label="•">{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) is a CGS with {a mathematical formula}Π=Πh∪Πs partitioned into hard and soft facts.</list-item><list-item label="•">{a mathematical formula}Prefs is a set of preference profiles over {a mathematical formula}Agt and Π.</list-item><list-item label="•">{a mathematical formula}N={N1,N2,…} is a set of normative systems over Π with {a mathematical formula}Ni=SNi∪RNi, where {a mathematical formula}SNi∈Ns and {a mathematical formula}RNi∈Nr.</list-item></list><paragraph>For the remainder of this section we assume that {a mathematical formula}I=(M,Prefs,N) is a fixed implementation setting where {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) and q is a state in {a mathematical formula}M.</paragraph><paragraph label="Definition 13">Norm-based mechanismLet {a mathematical formula}N∈Nrs be a normative system (over {a mathematical formula}M). The tuple {a mathematical formula}(M,N) is called norm-based mechanism.</paragraph><paragraph>A norm-based mechanism {a mathematical formula}(M,N) gives rise to a CGS by updating {a mathematical formula}M with {a mathematical formula}N as introduced in Definition 11.</paragraph><paragraph label="Example 14">Norm-based mechanismThe environment model {a mathematical formula}M1, as illustrated in Fig. 2, together with the normative system {a mathematical formula}N1, as defined in Example 11, constitute a norm-based mechanism {a mathematical formula}(M1,N1), which gives rise to CGS {a mathematical formula}M2 from Example 11 and Fig. 6. The CGS {a mathematical formula}M2 yields in turn the strategic game form {a mathematical formula}Γ(M1↾N1,q0) as illustrated in Fig. 11. For simplicity reasons, we encode strategies as triples {a mathematical formula}(x,y,z). For agent 1 (resp. agent 2) the triple encodes that the agent executes action x at {a mathematical formula}q0, y at {a mathematical formula}q0′, and z at {a mathematical formula}q2′ (resp. x at {a mathematical formula}q0, y at {a mathematical formula}q0′, and z at {a mathematical formula}q1).</paragraph><paragraph>A norm-based mechanism defines regimenting and sanctioning norms that influence agents' behaviour in certain directions. Agents are assumed to act rationally, in particular in accordance to their preferences. We use the concept of Nash equilibrium as rationality criterion. As the fulfilment of agents' preferences depends on the valuation of states on a path, agents' behaviour may be affected if imposing norms and sanctions would cause a modification of states' valuations. Thus, norms can provide an incentive (e.g. in the case of sanctioning norms) to agents to change their behaviour. Therefore, a key question is how to engineer good incentives. In [19], we required that for a successful implementation of a normative behaviour function, all resulting Nash equilibria have to agree with the behaviour specified by the normative behaviour function, which is essentially the case in the classical game theoretic implementation setting given by [54]. Following [69] and [30] we also introduce a weaker notion of implementation where the existence of some good behaviour is sufficient. We use the notation of [69] and [30] and introduce weak and strong implementation.</paragraph><paragraph label="Definition 14">{a mathematical formula}S-implementationLet {a mathematical formula}I=(M,Prefs,N) and {a mathematical formula}N∈N. We say that a norm-based mechanism {a mathematical formula}(M,N)(strongly){a mathematical formula}S-implements the normative behaviour function f over{a mathematical formula}(I,q) iff for all {a mathematical formula}γ→∈Prefs, {a mathematical formula}S(Γ(M↾N,q,γ→))≠∅ and for all {a mathematical formula}s∈S(Γ(M↾N,q,γ→)):outM↾N(q,s)⊨LTLf(γ→). We say that {a mathematical formula}(M,N) weakly {a mathematical formula}S-implements normative behaviour function f over {a mathematical formula}(I,q) iff for all {a mathematical formula}γ→∈Prefs there is an {a mathematical formula}s∈S(Γ(M↾N,q,γ→)) such that {a mathematical formula}outM↾N(q,s)⊨LTLf(γ→). We define the following corresponding sets:{a mathematical formula}{a mathematical formula} If {a mathematical formula}SIS(I,q)≠∅ (resp. {a mathematical formula}WIS(I,q)≠∅), we say that f is strongly (resp. weakly) {a mathematical formula}S-implementable over {a mathematical formula}(I,q).</paragraph><paragraph>The next example gives a norm-based mechanism which strongly {a mathematical formula}NE-implements a normative behaviour function.</paragraph><paragraph label="Example 15">Norm implementationAdding the preference profiles {a mathematical formula}γ→1 and {a mathematical formula}γ→2 to the game form {a mathematical formula}Γ(M1↾N1,q0) from of Example 14 results in the strategic games shown in Fig. 12. Bold payoffs indicate the Nash equilibria. The equilibria paths do now satisfy the system designer's preference.</paragraph></section><section><section-title>Concluding remarks</section-title><paragraph>This concludes our formal model of norm-based mechanism design. The complexity results in the following section are based on Nash equilibrium implementability. In general, other solution concepts, like dominant strategy equilibria, may be considered, depending on the problem at stake. A natural question is why we should be interested in strong or weak implementability. The short answer is: to obtain a stable and desired system behaviour. For illustration let us consider smart energy grids involving actors such as producers, consumers and regulation authorities. For the sake of the argument we simply assume producers and consumers to act rationally according to the concept of Nash equilibrium. Clearly, this is an idealised and abstracted view,{sup:10} but we believe it delivers the main ideas behind our framework. The regulation authority may have information about the producers' and consumers' preferences. The information might be obtained based on common knowledge (power should always be available), previous behaviour of the producers and consumers, or market research techniques. The regulation authority has some objectives such as flattening peak energy demand by consumers or efficient energy production by producers that it wants to have met. As all actors are self interested, the system (1) may not show stable behaviour in the sense that there is no optimal (social) solution, or (2) may not satisfy the objectives of the regulation authority. In order to overcome these problems, the authority can introduce and enforce norms in order to restrict or incentivize consumers and producers to change their behaviour. For example, additional fees{sup:11} may be charged on consumers if energy is used at specific hours, or fines are imposed on producers for overproduction. In this context, strong implementability means that all stable behaviours satisfy the authority's objectives, i.e., if all actors act rationally and play equilibrium strategies the objectives of the regulation authority are guaranteed. On the other hand, weak implementability indicates that there are some stable behaviour that satisfy the objectives of the regulation authority, but additional means are needed to coordinate on such a behaviour. Weak implementability can therefore be seen as a first step. Obviously, there are many applications for which our formal methodology can be used to analyse and improve the overall system behaviour, e.g., transportation systems, traffic, financial markets and business processes. In general, our formal methodology can be applied to applications where the behaviour of involved actors/processes need to be monitored and controlled. In the next paragraph we shall investigate the complexity of weak and strong implementation.</paragraph></section></section><section label="4"><section-title>Verification and complexity issues</section-title><paragraph>In this section we consider the complexity of the problem whether some normative system implements a normative behaviour function and whether such a normative system exists at all. We focus on implementation in Nash equilibria. These results are important in order to check whether a norm-governed environment ensures that the agents' rational behaviour satisfies the system specification. We present the proofs of the main results and moved technical details to the appendix.</paragraph><paragraph>For the complexity results we first need to be clear about how the size of the input is measured. The size of a CGS is defined as the product of the number of states and the number of transitions, which is denoted by {a mathematical formula}|M|. The size of a finite normative system {a mathematical formula}N is given by the number of norms it is comprised of times the maximal length propositional formula contained in a norm of {a mathematical formula}N (i.e. a norm is only measured in terms of the size of its precondition, as it can be assumed that the other elements are bound by the size of the model and the agents' preferences). The size of a set of normative systems is measured by {a mathematical formula}|N|⋅|Nmax| where {a mathematical formula}Nmax∈N is a normative system of maximal size contained in {a mathematical formula}N. Moreover, we assume that any considered normative behaviour function f is polynomial-time computable. The first result investigates the complexity of performing a norm-based update.</paragraph><paragraph label="Proof">Let{a mathematical formula}I=(M,Prefs,N)be given. For any{a mathematical formula}N∈N,{a mathematical formula}M↾Ncan be computed in polynomial time wrt. the size of{a mathematical formula}Mand{a mathematical formula}N. The size of{a mathematical formula}M↾Nis bounded by{a mathematical formula}O(|M|4).Let {a mathematical formula}M′:=M↾N, and n and t denote the number of states and transitions in {a mathematical formula}M, respectively. The set {a mathematical formula}AppN(π(q),α→) can be computed in polynomial time in the size of {a mathematical formula}N: for each {a mathematical formula}η=(φ,A,⋅)∈N it has to be checked whether {a mathematical formula}π(q)⊨PLφ. To determine the size of {a mathematical formula}M′ we inspect Definition 11. The number of states of {a mathematical formula}M′ is bounded by {a mathematical formula}n+n⋅n⋅t=O(n2t) as the number of tuples {a mathematical formula}(π(q′),α→) is bounded by {a mathematical formula}n⋅t and by Proposition 1 sanctions are not accumulated, i.e. there are at most {a mathematical formula}n2⋅t many new states of the form {a mathematical formula}(q,S). In each new state {a mathematical formula}(q,S) the choices are the same as in q, thus also the number of outgoing transitions. The number of transitions in {a mathematical formula}M′ is bounded by {a mathematical formula}O(n2t)⋅t=O(n2t2). Thus, the size of {a mathematical formula}M′ is bounded by {a mathematical formula}O(n4⋅t3)≤O(|M|4).  □</paragraph><paragraph>The next result is concerned with the special case of the weak implementation problem. We investigate the complexity of verifying whether a given normative system weakly implements a given normative behaviour function. Hardness is shown by a reduction of QSAT2 which refers to the satisfiability problem of Quantified Boolean Formulae (cf. Appendix A). In the appendix we show how to construct a model {a mathematical formula}Mϕ and a preference profile from a QSAT2-formula ϕ such that there is a Nash equilibrium iff ϕ is satisfiable. The construction of {a mathematical formula}Mϕ is based on a construction proposed by [22].</paragraph><paragraph label="Proof">Weak implementation: verificationLet{a mathematical formula}(I,q)be a pointed NIS, where{a mathematical formula}I=(M,Prefs,N)and{a mathematical formula}N∈Nbe a normative system. Let also f be a normative behaviour function. The problem whether{a mathematical formula}N∈WINE(I,q,f)is{a mathematical formula}Σ2P-complete in the size of{a mathematical formula}M,{a mathematical formula}Nand{a mathematical formula}Prefs.Membership: We construct a non-deterministic polynomial time oracle Turing machine M which can make calls to a non-deterministic polynomial time Turing machine {a mathematical formula}M′. We first describe the machine {a mathematical formula}M′. It takes as input a strategy profile s and returns true if the profile is not a Nash equilibrium. Therefore, for each agent i, the oracle machine {a mathematical formula}M′ guesses an individual strategy {a mathematical formula}si′ for i and checks whether {a mathematical formula}(s−i,si′) yields a better payoff than s. If the profile yields a better payoff the machine returns true. The machine M works as follows. Firstly, M computes {a mathematical formula}M↾N. Then, for each profile {a mathematical formula}γ→∈Prefs the machine guesses a strategy profile s and checks whether {a mathematical formula}π(out(q,s))⊨LTLf(γ→). The latter check can be done in polynomial deterministic time as s determines a cyclic path in {a mathematical formula}M on which the truth of a linear-time temporal formula can easily be determined. If successful the machine checks whether s is a Nash equilibrium in {a mathematical formula}Γ(M,q,γ→) by calling {a mathematical formula}M′ and reverting the answer. This shows that the problem whether {a mathematical formula}N∈WINE(I,q,f) is in {a mathematical formula}Σ2P=NPNP.Sketch of hardness: The hardness result is proven in Theorem 18 in the appendix. Here, we only give the high level idea of the reduction of QSAT2 to the weak implementability problem. Given a QSAT2 formula ϕ we construct a two player CGS {a mathematical formula}Mϕ consisting of the refuter playerr and the verifier playerv. Essentially, the players decide on the value of universally and existentially controlled variables, respectively. Once they decide on a truth assignment, they play a game to evaluate the Boolean formula contained in ϕ following the game theoretical semantics of propositional logic. That is, the refuter playerr and the verifier playerv control conjunctions and disjunctions, respectively. Using this result, we construct a preference profile {a mathematical formula}(γv,γr) such that {a mathematical formula}ϕ is satisfiable iff {a mathematical formula}∃s∈NE(Mϕ,q0,(γv,γr)) such that {a mathematical formula}out(q0,s) satisfies a constructed formula {a mathematical formula}f(γv,γr), the state {a mathematical formula}q0 is a distinguished initial state in {a mathematical formula}Mϕ. Now, this is the case if the empty normative systems {a mathematical formula}Nϵ∈WINE(I,q0,f).  □</paragraph><paragraph>Analogously to the previous result, we next investigate whether a given normative system strongly implements a normative behaviour function. In the following, we show that the verification version of the strong implementation problem is in {a mathematical formula}P‖Σ2P[2]. This complexity class consists of all problems which can be solved by a polynomial-time deterministic oracle Turing machine which can make two non-adaptive queries to a problem in {a mathematical formula}Σ2P, cf. [67]. Non-adaptive means that queries must be independent from each other; in other words, it must be possible to perform them in parallel.</paragraph><paragraph label="Proof">Strong implementation: verificationLet{a mathematical formula}(I,q)be a pointed NIS, where{a mathematical formula}I=(M,Prefs,N)and{a mathematical formula}N∈Nbe a normative system. Let also f be a normative behaviour function. The problem whether{a mathematical formula}N∈SINE(I,q,f)is in{a mathematical formula}P‖Σ2P[2]. The problem is{a mathematical formula}Σ2P-hard as well as{a mathematical formula}Π2P-hard in the size of{a mathematical formula}M,{a mathematical formula}Nand{a mathematical formula}Prefs.Membership: We construct a deterministic polynomial time Turing machine (TM) N with an {a mathematical formula}Σ2P-oracle, implemented by a non-deterministic TM {a mathematical formula}N′ with NP-oracle. The TM {a mathematical formula}N′ works similar to the TM M of Theorem 4. N uses {a mathematical formula}N′ twice: to check whether {a mathematical formula}NE(Γ(M↾N,q,γ→))≠∅ and whether for all {a mathematical formula}s∈NE(Γ(M↾N,q,γ→)) it holds that {a mathematical formula}π(outM↾N(q,s))⊨LTLf(γ→). The first part is done as in the proof of Theorem 4. For the second part, N calls {a mathematical formula}N′ with the additional input {a mathematical formula}¬f(γ→) and reverts the answer of {a mathematical formula}N′. This shows that the problem is in {a mathematical formula}P‖Σ2P[2].Sketch of hardness. The hardness results are proven in Theorem 21. The intuition is similar to the one given in Theorem 4 where the same two-player model {a mathematical formula}Mϕ is used, but the preference profiles of both players are changed. This case is technically more difficult as the verification problem consists of two parts: (i) ensuring that the set of Nash equilibria is non-empty; and (ii) ensuring that all Nash equilibria satisfy the outcome of the normative behavior function. For the {a mathematical formula}Π2P-hardness we also need to slightly modify the model {a mathematical formula}Mϕ by updating the roles of the verifier and the refuter.  □</paragraph><paragraph>The next lemma shows that normative systems enjoy a small representation property: any update of a model by a normative systems can be obtained by a “small” normative system which is polynomial in the size of the model.</paragraph><paragraph label="Proof">Let{a mathematical formula}I=(M,Prefs,N)be given where{a mathematical formula}N∈{Nr,Ns,Nrs}. For each{a mathematical formula}N∈Nthere is an{a mathematical formula}N′∈Nwith{a mathematical formula}|N′|≤2⋅|Q|⋅|Actk|such that{a mathematical formula}M↾N=M↾N′, where k is the number of agents in{a mathematical formula}M.Let {a mathematical formula}N be given, comprised of sanctioning norms {a mathematical formula}SN and regimenting norms {a mathematical formula}RN. For each {a mathematical formula}q∈Q and {a mathematical formula}α→∈Actk we define {a mathematical formula}Sq,α→=SanN(π(q),α→) and {a mathematical formula}π⁎(q) as the formula {a mathematical formula}⋀p∈π(q)p∧⋀p∉π(q)¬p. We add the sanctioning norm {a mathematical formula}(π⁎(q),{α→},Sq,α→) to {a mathematical formula}N′ if {a mathematical formula}⊥∉Sq,α→, and {a mathematical formula}(π⁎(q),{α→},⊥) otherwise. We observe that {a mathematical formula}|N′|≤|Q|⋅|Actk|. It remains to show that {a mathematical formula}M↾N=M↾N′. By Proposition 2.4 and the properties of {a mathematical formula}N′ we have that {a mathematical formula}M↾N′=(M↾RN′)↾SN′. Thus, we can consider sanctioning and regimenting norms separately.Firstly, we show that{a mathematical formula}M↾RN=M↾RN′. Let q and {a mathematical formula}α→ be a state and action profile in {a mathematical formula}M, respectively. It suffices to show that {a mathematical formula}AppRN(π(q),α→)=∅ iff {a mathematical formula}AppRN′(π(q),α→)=∅. “⇐”: {a mathematical formula}AppRN(π(q),α→)≠∅ implies that there is an {a mathematical formula}η=(φ,A,⊥)∈RN with {a mathematical formula}π(q)⊨φ and {a mathematical formula}α→∈A. This implies {a mathematical formula}(π⁎(q),{α→},⊥)∈RN′ which implies {a mathematical formula}AppRN′(π(q),α→)≠∅. “⇒”: {a mathematical formula}AppRN′(π(q),α→)≠∅ implies {a mathematical formula}∃η=(π⁎(q′),{α→},⊥)∈RN′ with {a mathematical formula}π(q)⊨π⁎(q′). This implies {a mathematical formula}∃η′=(φ,A,⊥)∈RN with {a mathematical formula}a→∈A and {a mathematical formula}π(q′)⊨φ. But then also {a mathematical formula}π(q)⊨φ which implies {a mathematical formula}AppRN(π(q),α→)≠∅.Secondly, we show that{a mathematical formula}SanSN′(π(q),α→)=SanSN(π(q),α→)for all q and{a mathematical formula}α→in{a mathematical formula}Mwhenever{a mathematical formula}AppRN(π(q),α→)=∅:{a mathematical formula} This implies that the updates yield identical models.  □</paragraph><paragraph>Now we consider the problem whether there is a normative systems that weakly implements a normative behaviour function.</paragraph><paragraph label="Proof">Weak implementation: existenceLet{a mathematical formula}(I,q)be a pointed NIS,{a mathematical formula}N∈{Nrs,Nr,Ns},{a mathematical formula}N∈N, and f be a normative behaviour function. The problem whether{a mathematical formula}WINE(I,q,f)≠∅is{a mathematical formula}Σ2P-complete.Membership. By Lemma 6, if {a mathematical formula}N∈WINE(I,q,f) then there is also a “small” normative system {a mathematical formula}N′∈WINE(I,q,f). We extend the algorithm described in the proof of Theorem 4 in such a way that the TM M guesses, in addition to the strategy profile, a “small” normative system {a mathematical formula}N, and then works as before. Hardness follows from Theorem 4.  □</paragraph><paragraph>For strong implementation it is no longer possible to guess a strategy profile but the normative behaviour function must be satisfied on all Nash equilibria. Thus, we can use the result of Theorem 5, but first a small normative system is guessed.</paragraph><paragraph label="Proof">Strong implementation: existenceLet{a mathematical formula}(I,q)be a pointed NIS and{a mathematical formula}N∈{Nrs,Ns,Nr},{a mathematical formula}N∈N, and f be a normative behaviour function. The problem whether{a mathematical formula}SINE(I,q,f)≠∅is{a mathematical formula}Σ3P-complete.Membership. By Lemma 6, if {a mathematical formula}N∈SINE(I,q,f) then there is also a “small” normative system {a mathematical formula}N′∈SINE(I,q,f). We construct a non-deterministic TM M which uses {a mathematical formula}N′ from Theorem 5 as oracle TM. M works as N from Theorem 5 but additionally guesses a small normative system {a mathematical formula}N, and then works as N. This shows that the problem is in {a mathematical formula}NPΔ3P=Σ3P.Sketch of hardness. The hardness proof for sanctioning norms and for regimentation norms is given in Theorems 25 and 29, respectively, where the basic intuition is similar to the one given in Theorems 4 and 5, the construction is slightly more sophisticated. As we now reduce QSAT3 to the strong implementation problem, we need to include the additional existential quantification in QSAT3 in the construction. For this purpose we encode a truth assignment of variables as a normative system. Guessing such a normative system corresponds to guessing a truth assignment of the newly existentially quantified variables. The difficulty in the construction is to ensure that the guessed normative system does not affect “good parts” of the model used in the construction of the two previous hardness results (Theorems 4 and 5) because after a normative system has been guessed the two players should essentially simulate the semantics of ϕ by guessing truth assignments of the variables under the scope of the other two quantifiers in the given QSAT3-formula, followed by playing the game theoretic game to evaluate the resulting propositional formula.  □</paragraph><section><section-title>Concluding remarks</section-title><paragraph>In this section we analysed the complexity of the weak and strong implementation problem. The computational complexity of the membership problems, i.e. whether a given normative system weakly or strongly implements a normative behaviour function, respectively, were shown to be essentially located on the second level of the polynomial hierarchy [55]. In [37] it was shown that checking the existence of a pure Nash equilibrium is NP-complete: a strategy profile must be guessed and then verified, in polynomial time, whether it is a Nash equilibrium. The latter check cannot be done in polynomial time in the setting considered here, as the number of strategies of each player is exponential in the size of the model; there are {a mathematical formula}|Act||Q| many of these. Given this observation, a lower bound of {a mathematical formula}Σ2P=NPNP seems intuitive for weak implementation (an existential problem: guess a strategy profile with specific properties and verify it against all deviations) and a lower bound of {a mathematical formula}Π2P=coNPNP for strong implementation (a universal problem). Our results show that the complexities of the decision problems considered here are in line with these intuitive bounds: in Theorem 5 we show {a mathematical formula}Σ2P as well as {a mathematical formula}Π2P hardness and prove an {a mathematical formula}P‖Σ2P[2] upper bound. Moreover, we show that the weak implementation problem is no more difficult than the verification of checking that a given normative system weakly implements a normative behaviour function. The complexity of the strong implementation problem is located one level up in the polynomial hierarchy. The results do also not appear that bad when compared with the complexity results of [69] in the context of Boolean games. The authors show that the problem whether there is an taxation scheme which ensures the existence of a pure Nash equilibrium with desirable properties is already {a mathematical formula}Σ2P-complete in the pure setting of Boolean games. Thus, we cannot hope for any better results for our weak implementation problem. That our strong implementation problem is more difficult than its weak version, {a mathematical formula}Σ3P-complete, stems from the fact that an additional normative system has to be guessed such that all Nash equilibria satisfy a specific property.</paragraph></section></section><section label="5"><section-title>Multi-agent environment programs</section-title><paragraph>The model of a multi-agent environment, as proposed in Section 2, is abstract in the sense that it assumes that the set of states and the state transitions are without an internal structure. In this section, we propose a specification language to program multi-agent environments. The introduction of this specification language allows us to program mechanisms and apply our formal methodology, as proposed in Section 3, to analyse the behaviour of such programs. In this way, one can check whether an environment program can ensure the system designer's objectives given that the participating agents behave according to their preferences. The proposed language allows the specification of an initial environment state as well as a set of synchronized actions. In environment programming, we are agnostic about individual agents and how they are programmed. We assume that a set of agents performing synchronized actions in the environment is given. These actions form the input of an environment program. The structural operational semantics of the language specifies the execution of programs. We show that the proposed language can be used to program a broad class of multi-agent environments as defined in Section 2. We extend the specification language with norms, as defined in Section 3, and present its operational semantics. We show that the extended language is expressive enough to program norm-governed multi-agent environments. In particular, we show that adding a set of norms to the program of a given multi-agent environment specifies the multi-agent environment updated with the set of norms.</paragraph><paragraph>In the rest of this section, we follow our abstract model of norm-based multi-agent systems and distinguish hard and soft facts. We assume that the states of norm-based environment programs is represented by hard and soft facts. We use {a mathematical formula}Π=Πh∪Πs to denote the union of disjoint and finite sets of hard and soft facts (i.e. {a mathematical formula}Πh∩Πs=∅), {a mathematical formula}Πhl={p,¬p|p∈Πh} be the set of hard literals, {a mathematical formula}Πsl={p,¬p|p∈Πs} be the set of soft literals, and {a mathematical formula}Πl=Πhl∪Πsl.</paragraph><section label="5.1"><section-title>Programming multi-agent environments</section-title><paragraph>A multi-agent environment can be programmed by specifying its initial state and a set of action profiles. The initial state of an environment program is specified by a set of atomic propositions and the action profiles are specified in terms of pre- and postconditions. The precondition of an action profile is a set of literals that specify the states of the environment programs in which the performance of the action profile results in a state transition. The resulting state is determined by adding the positive atoms of the action's postcondition to the state in which the action is performed and removing the negative atoms of the postcondition from it. The pre- and postconditions of action profiles are assumed to consist of hard facts such that action profiles are activated by hard facts and change only those facts of the program states. The performance of an action profile by individual agents in a state that does not satisfy its precondition is assumed to have no effect on the state and considered as looping in that state. Please note that in environment programming we are agnostic about individual agents and their internals. We assume that the agents decide and perform synchronized actions, and that the environment program realises the effect of the actions according to their specified pre- and postconditions. So, it is possible that the performance of actions by individual agents do not change the environment state. This is also the case with the performance of any action profile that is not specified by the environment program.</paragraph><paragraph label="Definition 15">Multi-agent environment programLet {a mathematical formula}Agt={1,…,k} be a set of agents. A multi-agent environment program (over a finite set of propositional symbols Π as introduced above) is a tuple {a mathematical formula}(F0,(Act1,…,Actk),Aspec), where</paragraph><list><list-item label="•">{a mathematical formula}F0⊆Π is the initial state of the environment program,</list-item><list-item label="•">{a mathematical formula}Acti is the set of actions of agent {a mathematical formula}i∈Agt,</list-item><list-item label="•">{a mathematical formula}Aspec⊆{(Pre,α→,Post)|α→∈Act1×…×ActkandPre,Post⊆Πhl} is a subset of action profile specifications, where we assume that each action tuple {a mathematical formula}α→ can be included in at most one action profile specification.</list-item></list><paragraph>We assume that k agents operate in a multi-agent environment such that {a mathematical formula}α→ is an action profile of the form {a mathematical formula}(α1,…,αk), where {a mathematical formula}αi∈Acti is the name of the action performed by agent {a mathematical formula}1≤i≤k. It is important to note that pre- and postconditions are not assigned to the actions of individual agents, but to action profiles. This implies that some possible action profiles from {a mathematical formula}Act1×…×Actk may not be specified in {a mathematical formula}Aspec in which case we assume that their executions will not change the state of the multi-agent environment program and loop in those states. Finally, we follow the convention, similar to the programming language Prolog [13], to use “_” as a place-holder for any action in an action profile specification.{sup:12} For example, we use {a mathematical formula}(Pre,(M,_),Post) to indicate that the performance of action profiles {a mathematical formula}(M,M) and {a mathematical formula}(M,W) in states that satisfy the precondition Pre results in states that satisfies the postcondition Post.</paragraph><paragraph label="Example 16">Environment program for the narrowed roadLet {a mathematical formula}Agt={1,2}, {a mathematical formula}Acti={M,W} for {a mathematical formula}i∈{1,2} be the actions that can be performed by the cars in our running example, and propositional symbol {a mathematical formula}piX be interpreted as before. The running example can be implemented by the multi-agent environment program {a mathematical formula}({p1s,p2s},(Act1,Act2),{a1,…,a5}), where{a mathematical formula}</paragraph><paragraph>Note that {a mathematical formula}ai is used to denote an action profile specification, while {a mathematical formula}α→i denotes an action profile. Starting from the initial state of a multi-agent environment program, an execution of the program changes the program state depending on the agents' actions (the input of the environment program). An arbitrary state of an environment program is specified by a set of atomic propositions {a mathematical formula}F⊆Π. The structural operational semantics of the multi-agent environment programs are specified by a set of transition rules, each specifies how the environment program state changes when agents perform actions. Therefore, in the sequel, we use {a mathematical formula}(F,(Act1,…,Actk),Aspec) to denote an environment program in state F. Since the agents' action repertoire and the action specifications do not change during the execution of the program, we only use the state of the environment program F (instead of {a mathematical formula}(F,(Act1,…,Actk),Aspec)) in the transition rules. We note that not every state F is necessarily reachable from the initial state.</paragraph><paragraph label="Definition 16">Structural operational semanticsLet {a mathematical formula}(F,(Act1,…,Actk),Aspec) be an environment program in state {a mathematical formula}F⊆Π and {a mathematical formula}α→∈Act1×…×Actk. For {a mathematical formula}Pre,Post⊆Πhl we write {a mathematical formula}F⊢PreiffPre∩Π⊆F and (Pre∖Π)∩F=∅ (the precondition Pre is derivable from the facts F iff all positive atoms in Pre are in F and all negative atoms in Pre are not in F). Further, we define {a mathematical formula}F⊕Post=(F∖{p|¬p∈Post})∪{p|p∈Post} (updating F with postcondition Post removes negative atoms in Post from F and adds positive atoms in Post to F). The following three transition rules specify possible execution steps of the environment program in state F.{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph><paragraph>The first transition rule specifies the execution step of the environment program based on the action profile {a mathematical formula}α→ when its precondition is satisfied. Such an execution step updates the set of facts F with the postcondition of the action profile {a mathematical formula}α→. The second transition rule is the same except that it applies when the precondition of {a mathematical formula}α→ is not satisfied. Such an execution step does not change the state of the environment program. Finally, the third rule specifies the execution step of the environment program based on an unspecified action profile {a mathematical formula}α→. Such an execution step does not change the state of the environment program.</paragraph><paragraph>In the following, we use {a mathematical formula}Tbasic to denote the set of transition rules from Definition 16, {a mathematical formula}trans(F,Act1,…,Actk,Aspec,Tbasic) to denote the set of all transitions that are derivable from transition rules {a mathematical formula}Tbasic based on the environment program {a mathematical formula}(F,(Act1,…,Actk),Aspec), and {a mathematical formula}trans(Act1,…,Actk,Aspec,Tbasic) to denote the set of all transitions that are derivable from transition rules {a mathematical formula}Tbasic based on environment programs {a mathematical formula}(F,(Act1,…,Actk),Aspec) for any {a mathematical formula}F⊆Π. The latter is the set of all possible transitions based on transition rules {a mathematical formula}Tbasic, actions {a mathematical formula}Act1,…,Actk, action specifications {a mathematical formula}Aspec, and all sets of facts {a mathematical formula}F⊆Π.</paragraph><paragraph>An execution of an environment program consists of subsequent execution steps resulting in a sequence of program states. In order to define the set of all possible executions of an environment program, we first define the set of all possible states that can be generated (reached) from an arbitrary state by applying subsequent transitions.</paragraph><paragraph label="Definition 17">Program statesLet {a mathematical formula}F⊆Π be a set of facts and τ be a set of transitions. The set of states reached from F by subsequent transitions from τ, denoted by {a mathematical formula}gen(τ,F), is defined as follows:{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph><paragraph>Observe that {a mathematical formula}gen(τ,F) is finite when τ is finite. Given an environment program and the set of transition rules {a mathematical formula}Tbasic, the set of possible executions of the environment program generates a concurrent game structure (as specified in Section 2).</paragraph><paragraph label="Definition 18">Programs ↝ CGSLet {a mathematical formula}(F0,(Act1,…,Actk),Aspec) be an environment program, {a mathematical formula}Tbasic be the set of transition rules as defined in Definition 16 and {a mathematical formula}τb=trans(Act1,…,Actk,Aspec,Tbasic). The environment program {a mathematical formula}Tbasic-generates a pointed CGS {a mathematical formula}(M,F0) with {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) as follows:</paragraph><list><list-item label="•">{a mathematical formula}Agt={1,…,k}</list-item><list-item label="•">{a mathematical formula}Q=gen(τb,F0)</list-item><list-item label="•">{a mathematical formula}Π=F0∪{p|(Pre,α→,Post)∈Aspecandp∈Post}</list-item><list-item label="•">{a mathematical formula}π(F)=F for {a mathematical formula}F∈Q</list-item><list-item label="•">{a mathematical formula}Act=Act1∪…∪Actk</list-item><list-item label="•">{a mathematical formula}di(F)=Acti for {a mathematical formula}i∈Agt and F∈Q</list-item><list-item label="•">{a mathematical formula}o(F,α→)=F′ for {a mathematical formula}F,F′∈Q, {a mathematical formula}α→∈d1(F)×…×dk(F), and {a mathematical formula}F→α→F′∈τb</list-item></list><paragraph>The translation between environment programs and concurrent game structures is restricted to specific classes of concurrent game structures as specified in the next definition. In the following, we use also variables {a mathematical formula}q0,q1,… to range over the set of states Q.</paragraph><paragraph label="Definition 19">Finite, distinguished, generated CGSLet {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) be a concurrent game structure. We introduce the following notation:</paragraph><list><list-item label="•">{a mathematical formula}M is called finite if Q is finite,</list-item><list-item label="•">{a mathematical formula}M is called distinguished if all states differ in their valuations, i.e., for all {a mathematical formula}q,q′∈Q with {a mathematical formula}q≠q′ we have that {a mathematical formula}π(q)≠π(q′), and</list-item><list-item label="•">{a mathematical formula}M is called uniform if for all {a mathematical formula}i∈Agt and all {a mathematical formula}q,q′∈Q we have that {a mathematical formula}d(i,q)=d(i,q′), i.e., the agents have the same options in every state.</list-item></list><paragraph>The following proposition formulates the relation between environment programs and their corresponding CGSs.</paragraph><paragraph label="Proof">Let Π be the set of propositional symbols,{a mathematical formula}(F0,(Act1,…,Actk),Aspec)be an environment program and{a mathematical formula}(M,F0)be the pointed CGS that is{a mathematical formula}Tbasic-generated by the environment program. If Π is finite, then{a mathematical formula}Mis finite. Moreover,{a mathematical formula}Mis distinguished and uniform.The sets of atoms in {a mathematical formula}gen(trans(Act1,…,Actk,Aspec,Tbasic),F0), which determine the set of states in Q, are subsets of Π such there can be only a finite number of them as Π is finite. Moreover, that {a mathematical formula}M is distinguished follows directly from the fact that all sets in {a mathematical formula}gen(trans(Act1,…,Actk,Aspec,Tbasic),F0) are different. Finally, that {a mathematical formula}M is uniform follows directly from the fact that for each {a mathematical formula}i∈Agt it holds: {a mathematical formula}d(i,q)=Acti for all states {a mathematical formula}q∈Q, i.e., each agent has one and the same set of options in every state.  □</paragraph></section><section label="5.2"><section-title>Multi-agent environment programs with norms</section-title><paragraph>The environment programs as defined in the previous subsection do not account for norms and norm enforcement. In order to add norms to multi-agent environment programs and to enforce them during program executions, we use (sanctioning and regimenting) norms as introduced in Definition 9. A norm is thus represented as a triple {a mathematical formula}(φ,A,S), where φ is a propositional formula, {a mathematical formula}A is a set of action profiles, and either {a mathematical formula}S⊆Πs (sanctioning norm) or {a mathematical formula}S=⊥ (regimenting norm). In the rest of this paper, we just use the term norm when the distinction between sanctioning and regimenting norms is not relevant. Like before, the pre- and postconditions of action profiles are assumed to consist of hard facts only such that adding norms does not change the activation and effect of action profiles. As explained before, throughout this section we assume that {a mathematical formula}Π=Πh∪Πs is a finite set of propositional symbols.</paragraph><paragraph label="Definition 20">Norm-based multi-agent environment programLet {a mathematical formula}Agt={1,…,k} be a set of agents. A norm-based multi-agent environment program is a tuple {a mathematical formula}(F0,(Act1,…Actk),Aspec,N), where</paragraph><list><list-item label="•">{a mathematical formula}F0⊆Π,</list-item><list-item label="•">{a mathematical formula}Acti and {a mathematical formula}Aspec are as introduced in Definition 15, and</list-item><list-item label="•">{a mathematical formula}N is a set of (sanctioning and regimenting) norms as introduced in Definition 9.</list-item></list><paragraph>Following the narrowed road environment program as specified in Example 16, the behaviour of cars can be regulated by adding norms to the environment program.</paragraph><paragraph label="Example 17">Norm-based environment program for the narrowed roadLet {a mathematical formula}({p1s,p2s},(Act1,Act2),{a1,…,a5}) be an environment program as specified in Example 16, and {a mathematical formula}N={(p1s∧p2s,{(W,W)},{fine1}),(p1s∧p2s,{(_,M)},{fine2})} be a set of norms as explained in Example 10. The norm-based environment program {a mathematical formula}({p1s,p2s},(Act1,Act2),{a1,…,a5},N) implements the narrowed road example where norms {a mathematical formula}N are enforced.</paragraph><paragraph>The executions of norm-based multi-agent environment programs are similar to the executions of multi-agent environment programs (without norms) in the sense that both update the program states with the effects of action profiles. However, the execution steps of a norm-based multi-agent environment program proceed by enforcing norms to the resulting program states. The enforcement of norms on a program state consists of updating the state with the consequences of applicable norms. So, in order to define the execution steps of norm-based multi-agent environment programs, we need to determine the norms that are applicable in a program state and their consequences. For this purpose, we use the function {a mathematical formula}SanN(X,α→), as defined in Definition 10, to determine the sanction set that should be imposed when the agents perform action profile {a mathematical formula}α→ in state X. Note that the sanction set may be {a mathematical formula}{⊥}, which means that a regimenting norm should be enforced. In this case, we have to ensure that the performance of {a mathematical formula}α→ does not have any effect on state X.</paragraph><paragraph>We distinguish two general cases depending on whether the sanction set is {a mathematical formula}{⊥} or not. When applicable norms are sanctioning norms (i.e., resulting in a set of soft facts), we update the resulting program state with the sanctions. We define the update of a program state with sanctions as being non-cumulative in the sense that previous sanctions are removed before new sanctions are added. We use the update function ⊗ to update a state with sanctions. Note that in Definition 21 ⊗ removes all sanctions (soft facts) before adding new ones. This ensures that sanctions become non-cumulative in transitions. This update function should not be confused with ⊕, which is used to update program states with the postcondition of action profiles.</paragraph><paragraph>The effect of an action profile on an environment program state in the context of some existing norms is specified by distinguishing four cases: 1) the action specification is given and its precondition is satisfied, 2) the action specification is given but its precondition is not satisfied, 3) the action specification is not given, and 4) the action profile is regimented by some norms. In the first case, we update the program state with the postcondition of the action profile and then with possible sanctions, while in the second and third case the program state is updated only with possible sanctions. These two cases indicate that the performance of an action profile in a program state will be sanctioned (if there is a sanctioning norms applicable) even when it has a specification but its precondition is not satisfied or when the action profile has no specification. These two cases capture the intuition that any unsuccessful attempt to violate norms will be sanctioned as well. Note that the sanction set in the first three cases can be an empty set if no norm is applicable. Finally, in the fourth case, when an action is regimented by some norms, the action will have no effect on the environment state.</paragraph><paragraph label="Definition 21">Structural operational semanticsLet {a mathematical formula}(F,(Act1,…,Actk),Aspec,N) be a norm-based environment program in an arbitrary state {a mathematical formula}F⊆Π, {a mathematical formula}α→∈Act1×…×Actk, and {a mathematical formula}N be a set of norms as defined in Definition 9. Let also ⊕ and ⊢ be defined as in Definition 16. Finally, let {a mathematical formula}F⊗S=(F∖Πs)∪S for {a mathematical formula}S⊆Πs. The following four transition rules specify possible execution steps of the norm-based environment program.{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph><paragraph>The first transition rule applies when action profile {a mathematical formula}α→ is performed, its precondition holds, and no regimenting norms are triggered. The second transition rule is the same except that the precondition of {a mathematical formula}α→ is not satisfied. In this case applicable sanctioning norms are enforced without realising the effect (i.e., postcondition) of {a mathematical formula}α→. The third transition rule applies when an unspecified action profile {a mathematical formula}α→ is performed and no regimenting norms are triggered. In this case, applicable sanctioning norms are enforced. Note that the environment is assumed to be exogenous to agents in the sense that agents decide on which actions to perform independently of the environment specification. This allows agents to decide and perform actions for which there is no environment specification. It is also important to note that a norm specifies that the performance of an action profile in a state should be sanctioned, regardless of the specification of the action profile. Thus, agents can perform an unspecified action profile for which a sanction may occur. The first three transition rules ensure that sanctioning norms are enforced regardless of the specification of action profiles. These transition rules capture the idea that any (successful or unsuccessful) attempt to violate norms is sanctioned. Note also that in the first three transition rules no sanctioning norm needs to be applicable in state X, i.e., {a mathematical formula}SanN(X,α→) can be an empty set indicating that action profile α does not violate any norm in state X. Finally, the fourth transition rule applies when the performance of an action profile triggers regimenting norms, again regardless of the specification of action profiles. It is important to notice that sanctions do not accumulate through consecutive states. The following proposition shows that the sanctions imposed on a state (propositional symbols from {a mathematical formula}Πs) are only those caused by the action performed in the previous state.</paragraph><paragraph>In the following, we use {a mathematical formula}Tnorm for the set of transition rules from Definition 21, {a mathematical formula}trans(F,Act1,…,Actk,Aspec,N,Tnorm) to denote the set of all transitions that are derivable from transition rules {a mathematical formula}Tnorm based on the norm-based environment program {a mathematical formula}(F,(Act1,…,Actk),Aspec,N), and {a mathematical formula}trans(Act1,…,Actk,Aspec,N,Tnorm) to denote the set of all transitions that are derivable from transition rules {a mathematical formula}Tnorm based on norm-based environment programs {a mathematical formula}(F,(Act1,…,Actk),Aspec,N) for all {a mathematical formula}F⊆Π.</paragraph><paragraph label="Proof">Let{a mathematical formula}(F,(Act1,…,Actk),Aspec,N)be a norm-based environment program in an arbitrary program state F,{a mathematical formula}F→α→F′∈trans(Act1,…,Actk,Aspec,N,Tnorm)and{a mathematical formula}S=SanN(F,α→). Then, we have{a mathematical formula}Directly follows from the definition of ⊗ and the transition rules in Definition 21.  □</paragraph><paragraph>For a given norm-based environment program, the set of transition rules {a mathematical formula}Tnorm generates a concurrent game structure. We use Definition 18 to define the concurrent game structure, which is said to be generated by the norm-based environment program. Note that Definition 18 can be applied as norm-based environment programs have the required ingredients such as an initial state {a mathematical formula}F0, a sets of actions for each agent, and a set of action profile specifications. There is, however, one difference between environment programs and norm-based environment programs which requires a slight modification of Definition 18. The difference is that we now assume the set of transitions is {a mathematical formula}trans(Act1,…,Actk,Aspec,N,Tnorm), instead of {a mathematical formula}trans(Act1,…,Actk,Aspec,Tbasic). This means that we use {a mathematical formula}gen(trans(Act1,…,Actk,Aspec,N,Tnorm),F0) to generate the set of reachable states, and {a mathematical formula}F→α→F′∈trans(Act1,…,Actk,Aspec,N,Tnorm) to indicate that the transition {a mathematical formula}F→α→F′ is derivable based on {a mathematical formula}Tnorm. The formal definitions are as before.</paragraph><paragraph label="Definition 22">Norm-based programs ↝ CGSLet {a mathematical formula}(F0,(Act1,…,Actk),Aspec,N) be a norm-based environment program and {a mathematical formula}τn=trans(Act1,…,Actk,Aspec,N,Tnorm). The program is said to {a mathematical formula}Tnorm-generate a pointed CGS {a mathematical formula}(M,F0), where {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) is defined as follows:</paragraph><list><list-item label="•">{a mathematical formula}Agt,Π,π,Act,d are specified as in Definition 18,</list-item><list-item label="•">{a mathematical formula}Q=gen(τn,F0), and</list-item><list-item label="•">{a mathematical formula}o(F,α→)=F′ for {a mathematical formula}F,F′∈Q,α→∈d1(F)×…×dk(F) and {a mathematical formula}F→α→F′∈τn.</list-item></list><paragraph>The class of concurrent game structures generated by norm-based environment programs is characterized by the following proposition.</paragraph><paragraph label="Proof">Let Π be the set of propositional symbols,{a mathematical formula}(F0,(Act1,…,Actk),Aspec,N)be a norm-based environment program, and{a mathematical formula}(M,F0)be the pointed CGS that is{a mathematical formula}Tnorm-generated by it. If Π is finite, then{a mathematical formula}Mis finite. Moreover,{a mathematical formula}Mis distinguished and uniform.Similar to the proof of Proposition 9. Observe that the elements of {a mathematical formula}Q=gen(trans(Act1,…,Actk,Aspec,N,Tnorm),F0) are subsets of Π and there are only finitely many of them when Π is finite. Observe also that {a mathematical formula}M is distinguished as the elements of Q are distinct and that {a mathematical formula}M is uniform as {a mathematical formula}di(q)=Acti for all {a mathematical formula}i∈Agt and {a mathematical formula}q∈Q.  □</paragraph><paragraph>The generated CGS does not accumulate sanctions through consecutive states.</paragraph><paragraph label="Proof">Let{a mathematical formula}(F0,(Act1,…,Actk),Aspec,N)be a norm-based environment program that{a mathematical formula}Tnorm-generates{a mathematical formula}(M,F0), where{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o). For{a mathematical formula}F∈Q,{a mathematical formula}α→∈Act1×…×Actkand{a mathematical formula}S=SanN(F,α→), we have{a mathematical formula}From Definition 22 we have {a mathematical formula}o(F,α→)=F′ iff {a mathematical formula}F→α→F′∈trans(Act1,…,Actk,Aspec,N,Tnorm) and Proposition 10 we know that sanctions are not accumulated through transitions.  □</paragraph><paragraph>Note the correspondence between Proposition 12 and Proposition 1 (on page 109). The execution of a norm-based environment program may generate a different set of states than the execution of the corresponding environment program without norms does. This is due to the fact that the performance of unspecified action profiles or the performance of specified action profiles for which the precondition does not hold can now be sanctioned resulting in new states. Note that the performance of such an action profile in an environment program without norms results in the same state. Also, the application of regimenting norms may prevents reaching new states. These observations are formulated in the following proposition.</paragraph><paragraph label="Proposition 13">Let{a mathematical formula}(F0,(Act1,…,Actk),Aspec)be an environment program,{a mathematical formula}(F0,(Act1,…,Actk),Aspec,RN∪SN)be the environment program with regimenting norms{a mathematical formula}RNand sanctioning norms{a mathematical formula}SN. Let also{a mathematical formula}τb=trans(Act1,…,Actk,Aspec,Tbasic)and{a mathematical formula}τn=trans(Act1,…,Actk,Aspec,RN∪SN,Tnorm). Then, we have:</paragraph><list><list-item label="•">When the sets of regimentation and sanctioning norms are empty, i.e.,{a mathematical formula}RN∪SN=∅, we have{a mathematical formula}gen(τn,F0)=gen(τb,F0).</list-item><list-item label="•">When there are only regimentation norms and no sanctioning norms, i.e.,{a mathematical formula}RN≠∅and{a mathematical formula}SN=∅, we have{a mathematical formula}gen(τn,F0)⊆gen(τb,F0).</list-item><list-item label="•">When there are only sanctioning norms and no regimentation norms, i.e.,{a mathematical formula}RN=∅and{a mathematical formula}SN≠∅, we have{a mathematical formula}|gen(τn,F0)|≥|gen(τb,F0))|.</list-item></list><paragraph label="Proof"><list><list-item label="•">Since {a mathematical formula}RN∪SN=∅, we have for all {a mathematical formula}F⊆Π and all {a mathematical formula}α→∈Act1×…×Actk:SanN(F,α→)=∅. This makes the transition rules {a mathematical formula}Tbasic and {a mathematical formula}Tnorm identical.</list-item><list-item label="•">Let {a mathematical formula}RN≠∅ and {a mathematical formula}SN=∅. Following Definition 17, we have that {a mathematical formula}gen(τ,F):={F}∪⋃i=1∞sucτi({F}). We show that if {a mathematical formula}F∈{F0}∪⋃i=1lsucτni({F0}) then also {a mathematical formula}F∈{F0}∪⋃i=1lsucτbi({F0}) by induction on l. The base case, {a mathematical formula}l=1, is trivial. Suppose the claim holds for {a mathematical formula}l&gt;1. Let {a mathematical formula}F∈{F0}∪⋃i=1l+1sucτni({F0}). Then, there must be an {a mathematical formula}F′∈{F0}∪⋃i=1lsucτni({F0}) with {a mathematical formula}F′→α→F∈τn. If the latter transition is obtained by a regimentation of {a mathematical formula}α→ in {a mathematical formula}F′ then {a mathematical formula}F=F′ and by induction hypothesis we obtain that {a mathematical formula}F∈{F0}∪⋃i=1l+1sucτbi({F0}). If the action is not regimented, then we also have that {a mathematical formula}F′→α→F∈τb and thus {a mathematical formula}F∈sucτb({F′}). Then, as by induction hypothesis {a mathematical formula}F′∈{F0}∪⋃i=1lsucτbi({F0}), we also obtain that {a mathematical formula}F∈{F0}∪⋃i=1l+1sucτbi({F0}). The claim follows.</list-item><list-item label="•">Let {a mathematical formula}F⊆Π, {a mathematical formula}F→α→F∈τb for some {a mathematical formula}α→∈Act1×…×Actk, and {a mathematical formula}SanSN(F,α→)≠∅ (note that {a mathematical formula}⊥∉SanSN(F,α→)). Then, we have {a mathematical formula}F→α→F⊕SanSN(F,α→)∈τn. This implies that {a mathematical formula}F⊕SanSN(F,α→)∈gen(τn,F0) but {a mathematical formula}F⊕SanSN(F,α→)∉gen(τb,F0). □</list-item></list></paragraph></section><section label="5.3"><section-title>Relation between norm-based update and norm-based environment</section-title><paragraph>We now investigate the relation between the concurrent game structure {a mathematical formula}Mn that is generated by a norm-based environment program, and the concurrent game structure {a mathematical formula}M′ that is generated by the corresponding environment program without norms (i.e., the same initial state, actions, and action specifications) which is then updated with the same norms. We first show that if states {a mathematical formula}qn and {a mathematical formula}q′ from {a mathematical formula}Mn and {a mathematical formula}M′, respectively, have the same valuation, then the states reached from {a mathematical formula}qn and {a mathematical formula}q′ by the same action profile have the same valuation as well. This is formulated by the following lemma.</paragraph><paragraph label="Lemma 14">Let{a mathematical formula}(F0,(Act1,…,Actk),Aspec)be an environment program that{a mathematical formula}Tbasic-generates the concurrent game structure{a mathematical formula}(M,F0)where{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o). Let{a mathematical formula}(F0,(Act1,…,Actk),Aspec,N)be a norm-based environment program that{a mathematical formula}Tnorm-generates the concurrent game structure{a mathematical formula}(Mn,F0)where{a mathematical formula}Mn=(Agt,Qn,Π,πn,Act,dn,on). Let{a mathematical formula}M′=M↾N, where{a mathematical formula}M′=(Agt,Q′,Π,π′,Act,d′,o′). For any{a mathematical formula}Fn∈Qn,{a mathematical formula}F′∈Q′and{a mathematical formula}α→∈Act1×…×Actk, we have that if{a mathematical formula}πn(Fn)=π′(F′)then{a mathematical formula}πn(on(Fn,α→))=π′(o′(F′,α→)).</paragraph><paragraph label="Proof">First, we note that by Proposition 11 we have that {a mathematical formula}M and {a mathematical formula}Mn are uniform. Therefore, by the definition of norm update given in Definition 11 model {a mathematical formula}M′ is uniform as well. As a consequence, in all states of all three models all action tuples from {a mathematical formula}Act1×…×Actk are available. Let {a mathematical formula}Fn∈Qn, {a mathematical formula}F′∈Q′ with {a mathematical formula}πn(Fn)=π′(F′). We note that {a mathematical formula}F′,Fn⊆Π, or {a mathematical formula}Fn⊆Π and {a mathematical formula}F′=(F,S) with {a mathematical formula}F⊆Πh, {a mathematical formula}S⊆Πs and {a mathematical formula}π′(F′)=π′(F)∪S=F∪S. Therefore, we define:{a mathematical formula}Furthermore, we note that for all {a mathematical formula}α→∈Act1×…×Actk we have that: {a mathematical formula}S⁎:=SanN(πn(Fn),α→)=SanN(π′(F′),α→) and that in {a mathematical formula}Fn and {a mathematical formula}F′ the same action profiles are enabled. We show the claim by considering the three cases how a transition can occur according to Definition 11. Case{a mathematical formula}S⁎=∅.In that case we have that {a mathematical formula}o′(F′,α→)=o(G,α→), where {a mathematical formula}o′(F′,α→)⊆Πh which is generated by {a mathematical formula}G→α→o′(F′,α→)∈τb. As the precondition of {a mathematical formula}α→ is independent of soft facts and the first three transition rules of Definitions 16 and 21 are identical for {a mathematical formula}S⁎=∅, we obtain that {a mathematical formula}on(Fn,α→)=o′(F′,α→). The claim follows.Case{a mathematical formula}S⁎={⊥}.In this case {a mathematical formula}Fn→α→Fn∈τn and {a mathematical formula}on(Fn,α→)=Fn as well as {a mathematical formula}o′(F′,α→)=F′. The claim follows because {a mathematical formula}π′(F′)=πn(Fn).Case{a mathematical formula}{⊥}≠S⁎≠∅.We further divide this case according to the first three transition rules of Definitions 16 and 21.</paragraph><list><list-item label="•">First suppose that {a mathematical formula}(Pre,α→,Post)∈Aspec and {a mathematical formula}Fn⊢Pre (and thus also {a mathematical formula}G⊢Pre). We have that {a mathematical formula}on(Fn,α→)=(Fn⊕Post)⊗S⁎. Analogously, we have that {a mathematical formula}o′(F′,α→)=(G⊕Post,S⁎). We obtain that: {a mathematical formula}πn(on(Fn,α→))=πn((Fn⊕Post)⊗S⁎)=πn(((Fn⊕Post)\Πs)∪S⁎)=πn(((Fn⊕Post)\Πs))∪S⁎=πn((G⊕Post))\Πs)∪S⁎=πn(G⊕Post)∪S⁎=π′((G⊕Post,S⁎))=π′(o′(F′,α→)).</list-item><list-item label="•">The second case, i.e. {a mathematical formula}Fn⊬Pre, follows analogously.</list-item><list-item label="•">Also the final case, i.e. {a mathematical formula}(Pre,α→,Post)∉Aspec for any {a mathematical formula}Pre,Post⊆Πh, follows analogously. □</list-item></list><paragraph>As the final result of this section, we show that the concurrent game structures generated by an environment program with and without norms are closely related. In particular, we show that the concurrent game structure generated by a norm-based environment program is isomorphic to the reachable part of the concurrent game structure, which is generated by the corresponding environment program without norm, and updated with the same set of norms. We formally introduce the notions of reachable states and isomorphism before showing the correspondence result.</paragraph><paragraph label="Definition 23">Reachable states, reachable CGSLet {a mathematical formula}M=(Agt,Q,Π,π,Act,d,o) be a CGS and {a mathematical formula}q0∈Q. A state {a mathematical formula}q∈Q is said to be reachable from {a mathematical formula}q0 if there is a path starting in {a mathematical formula}q0 which also contains q. The set of all states reachable from {a mathematical formula}q0 in {a mathematical formula}M is denoted as {a mathematical formula} Reachable(M,q0). The reachable part of {a mathematical formula}M from {a mathematical formula}q0, denoted as {a mathematical formula}Mq0, is the CGS {a mathematical formula}(Agt,Q′,Π,π′,Act,d′,o′) where {a mathematical formula}Q′= Reachable(M,q0), {a mathematical formula}π′(q)=π(q), {a mathematical formula}di′(q)=di(q), and {a mathematical formula}o′(q,α→)=o(q,α→) for all {a mathematical formula}α→∈dAgt′(q), {a mathematical formula}q∈Q′ and {a mathematical formula}i∈Agt.</paragraph><paragraph>A model updated by sanctioning norms can yield states of type {a mathematical formula}(q,S). States in norm generated CGSs, on the other hand, have no internal structure; they are plain sets of propositional symbols. We are less interested in such purely syntactic differences and need a way to compare models from a semantic perspective. Therefore, we say that two models {a mathematical formula}M1 and {a mathematical formula}M2 are isomorphic if they are identical besides the names of the states. The next definition captures this formally.</paragraph><paragraph label="Definition 24">Isomorphic modelsLet {a mathematical formula}Mi=(Agti,Qi,Πi,πi,Acti,di,oi) for {a mathematical formula}i∈{1,2} be two CGSs. {a mathematical formula}M1 and {a mathematical formula}M2 are isomorphic, written as {a mathematical formula}M1≅M2, if the following conditions hold: {a mathematical formula}Agt1=Agt2, {a mathematical formula}Π1=Π2, {a mathematical formula}Act1=Act2 and there is a bijection {a mathematical formula}f:Q1→Q2 such that:</paragraph><list><list-item label="1.">{a mathematical formula}π1(q)=π2(f(q)) for all {a mathematical formula}q∈Q1.</list-item><list-item label="2.">{a mathematical formula}d1(q)=d2(f(q)) for all {a mathematical formula}q∈Q1.</list-item><list-item label="3.">{a mathematical formula}f(o1(q,α→))=o2(f(q),α→) for all {a mathematical formula}q∈Q1 and {a mathematical formula}α→∈d1(q).</list-item></list><paragraph>Note that we focus on the part of the generated CGS that is reachable from the initial state since the application of regimenting norms in the operational semantics of norm-based environment programs blocks transitions and thus causes some states to become unreachable. Thus, having an environment program {a mathematical formula}(F0,(Act1,…,Actk),Aspec) and a set of norms {a mathematical formula}N, we can now show that the concurrent game structure {a mathematical formula}Mn generated by {a mathematical formula}(F0,(Act1,…,Actk),Aspec,N) is isomorphic to {a mathematical formula}(M↾N)F0, which is the part of the concurrent game structure generated by {a mathematical formula}(F0,(Act1,…,Actk),Aspec) (i.e., environment program without norms) and updated with {a mathematical formula}N, and is reachable from {a mathematical formula}F0. This relation is illustrated in the diagram shown in Fig. 13 and formulated in the following theorem.</paragraph><paragraph label="Theorem 15">Let{a mathematical formula}(F0,(Act1,…,Actk),Aspec)be an environment program that{a mathematical formula}Tbasic-generates the pointed concurrent game structure{a mathematical formula}(M,F0)with{a mathematical formula}M=(Agt,Q,Π,π,Act,d,o), and{a mathematical formula}(F0,(Act1,…,Actk),Aspec,N)be a norm-based environment program that{a mathematical formula}Tnorm-generates the pointed concurrent game structure{a mathematical formula}(Mn,F0)with{a mathematical formula}Mn=(Agt,Qn,Π,πn,Act,dn,on). We define{a mathematical formula}M′=M↾Nwith{a mathematical formula}M′=(Agt,Q′,Π,π′,Act,d′,o′). Then, we have that{a mathematical formula}Mn≅(M↾N)F0.</paragraph><paragraph label="Proof">We define a (partial) function {a mathematical formula}f:Qn→Q′ by {a mathematical formula}f(Fn)=F′ if and only if {a mathematical formula}πn(Fn)=π′(F′). That f is well defined follows from the fact that for all {a mathematical formula}F1,F2∈Q′ we have that {a mathematical formula}π′(F1)≠π′(F2) whenever {a mathematical formula}F1≠F2. This is because a norm update is only performed once, causing states to be of the form {a mathematical formula}F⊆Πh or {a mathematical formula}(F,S) with {a mathematical formula}F⊆Πh and {a mathematical formula}∅≠S⊆Πs. We show that f constitutes an isomorphism according to Definition 24 between {a mathematical formula}Mn and {a mathematical formula}(M↾N)F0. Therefore, we have to show that f is a bijection (and total) and that the three conditions of Definition 24 are satisfied. Condition 1 is true by the definition of f. Condition 2 is true because both models are uniform by Proposition 9 and Definition 11, and by Proposition 11, respectively. Thus, only the last of these three conditions, i.e. that {a mathematical formula}f(on(q,α→))=o′(f(q),α→) for all {a mathematical formula}q∈Qn and {a mathematical formula}α→∈dn(q), remains to be shown. By definition, both sets of states {a mathematical formula}Q′ and {a mathematical formula}Qn consist of all states which are reachable by some sequence of actions from {a mathematical formula}F0. Therefore, we show that the claim holds by induction on the length of an action sequence.Base case. We have that {a mathematical formula}F0∈Qn∩Q′, thus, {a mathematical formula}f(F0)=F0 is well defined. Moreover, {a mathematical formula}F0 is reached by the empty action sequence. By Lemma 14 and as {a mathematical formula}f(F0)=F0 we obtain that {a mathematical formula}π′(o′(F0,α→))=πn(on(F0,α→)) for any {a mathematical formula}α→∈Act1×…×Actk. This implies that {a mathematical formula}f(on(F0,α→))=o′(F0,α→) and thus also {a mathematical formula}f(on(F0,α→))=o′(f(F0),α→) as desired.Induction step. We assume that the claim holds for all action sequences of length i. Thus, let {a mathematical formula}Fn∈Qn and {a mathematical formula}F′∈Q′ be two states reached after the same action sequence of length i. By induction, {a mathematical formula}f(Fn)=F′ and Condition 3 is satisfied for these states. Let {a mathematical formula}α→∈Act1×…×Actk be an arbitrary action profile. Again, by Lemma 14 we obtain that {a mathematical formula}π′(o′(F′,α→))=πn(on(Fn,α→)). Consequently, {a mathematical formula}f(on(Fn,α→))=o′(F′,α→) is well defined. To establish Condition 3 we consider another arbitrary action profile {a mathematical formula}β→∈Act1×…×Actk. Because both states are in relation by f we can once more apply Lemma 14 and obtain {a mathematical formula}πn(on(on(Fn,α→),β→))=π′(o′(o′(F′,α→),β→)) which implies that {a mathematical formula}f(on(on(Fn,α→),β→))=o′(o′(F′,α→),β→)=o′(f(on(Fn,α→)),β→). Which shows that Condition 3 is satisfied.In each inductive step we also showed that f is well defined and total. This implies that f is indeed a bijection as we considered arbitrary action sequences and thus made sure that each state from {a mathematical formula}Q′ is reached.  □</paragraph><paragraph>This theorem shows that the operational semantics of the proposed executable specification languages for environments with and without norms are aligned with the semantics of norms and norm update, as presented in the first part of the paper. This result allows us to apply the proposed abstract mechanism design methodology to analyse the enforcement effect of norms in executable environment programs.</paragraph></section><section><section-title>Concluding remarks</section-title><paragraph>In this section, an executable specification language for the implementation of multi-agent environments was presented. The language includes constructs to specify the initial state of multi-agent environments as well as the specification of action profiles in terms of pre- and postconditions. The operational semantics of the proposed specification language was presented and its relation with concurrent game structures was established. An execution of an environment specification initializes a multi-agent environment, which is subsequently modified by the performance of the agents' actions and according to the action specifications. Subsequently, the specification language was extended with norms and sanctions, its operational semantics was presented, and its relation with concurrent game structures that are updated with norms was established. The operational semantics ensures that the norms are enforced by means of regimentation or sanctions. An execution of a norm-based environment specification initializes a multi-agent environment and effectuates agents' actions in the environment based on the action specifications, the norms, and their corresponding sanctions. The agents are assumed to be aware of the norms and their enforcement. They are also assumed to autonomously decide whether to comply with the norms, or to violate them and accept the consequences. The presented executable specification language with norms can be used in various application domains such as traffic or business process management, where the behaviour of autonomous agents should be externally controlled and coordinated to be aligned with some laws, rules, policies, or requirements. The next section reports on an application of the extended specification language in traffic simulation, where traffic laws (norms) are enforced to reduce traffic congestion in a ramp merging scenario. We argue that although the executable specification language is useful for the implementation of traffic simulations, there are specific issues that should be resolved before we can apply the proposed norm-based mechanism design methodology to such applications.</paragraph></section></section><section label="6"><section-title>Applying norm-based specification language in traffic simulation</section-title><paragraph>A characteristic feature of the proposed specification language for norm-based multi-agent environments is the modularity of norms in the sense that norms can be programmed as a separate module isolated from the specification of actions and (initial) states of multi-agent environments. This feature allows us to implement different sets of norms and to compare their enforcement effects in one and the same multi-agent environment. The use of the proposed specification language is already illustrated by the running example that is presented in previous sections (see Example 17). In this section we present a more complex and realistic application of the proposed specification language.</paragraph><paragraph>This application concerns the development of norm-based traffic environments for SUMO (Simulation of Urban MObility) [12]. SUMO [46] is a traffic simulation platform that supports the modelling of traffic, including cars, public transport and pedestrians. In this application, SUMO is extended to simulate traffic scenarios, where norms and traffic laws are explicitly specified as input and enforced during the simulation runs. In particular, SUMO is extended with a norm-based traffic controller module that monitors the simulated traffic by continuously extracting relevant traffic information such as the position and speed of the simulated cars from the SUMO platform, instantiates the given input set of traffic norms to generate traffic directives for the observed cars, communicates traffic directives to the cars, and imposes payment sanctions on the cars that violate their traffic directives. In addition to the traffic controller module, the standard SUMO car model that is responsible for the actual behaviour of individual cars, is extended to allow individual cars to incorporate norms in their driving behaviour.</paragraph><paragraph>The SUMO extension [12] is used to simulate a ramp merging traffic scenario. A schematic representation of the ramp merging scenario is illustrated in Fig. 14. In this figure, triangles represent cars that drive from left to right and rectangles {a mathematical formula}s1 to {a mathematical formula}s5 are sensors that observe the position and speed of cars at various points of the roads. There are two important points on the road: m is the point where the roads merge and e is the ending point of the traffic scenario. In order to manage traffic at the merging point m, observed cars at sensor {a mathematical formula}s1, {a mathematical formula}s2 and {a mathematical formula}s3 receive traffic directives from the traffic controller. The directive for a car is generated based on the given set of traffic norms and consists of a velocity and a fine that will be imposed if the directive is not followed by the car. In this figure, white cars have not received their directives from the traffic controller, while grey cars have received their directives.</paragraph><paragraph>An example of a traffic norm used in this scenario is {a mathematical formula}(x∧y,A(vx,vy),{finez}) to be read as “cars x and y, observed simultaneously by sensor {a mathematical formula}s2 and {a mathematical formula}s3, are prohibited to have velocities other than respectively {a mathematical formula}vx and {a mathematical formula}vy at the merging point m to avoid a fine of z Euro”. For this traffic norm, the set of prohibited velocities {a mathematical formula}A(vx,vy)={(v1,v2)|v1≠vx and v2≠vy}{sup:13} represents the obligation that cars x and y should have velocities {a mathematical formula}vx and {a mathematical formula}vy, respectively, at the merging point m. The traffic controller instantiates the input norm by determining x, y, {a mathematical formula}vx, {a mathematical formula}vy, and {a mathematical formula}finez based on the observed cars and the properties of the current traffic state such as traffic density on the roads. The velocities of the observed cars are determined in such a way that cars arrives at m with a safe distance {a mathematical formula}dsafe from each other given the current traffic density. The fine of z Euro will be imposed on car x (respectively y) if its velocity {a mathematical formula}vx (respectively {a mathematical formula}vy) at m is not realised. Based on the instantiated norm, the traffic controller sends to each observed cars a corresponding directive. For example, the traffic controller sends to the observed car x the directive {a mathematical formula}(vx,finez), which should be read as “car x should have velocity {a mathematical formula}vx at the merge point m to avoid the fine of z Euro”. For this simulation scenario, the traffic controller generates also directives for the cars that are observed by one of the two sensors {a mathematical formula}s2 or {a mathematical formula}s3, even if there is no car simultaneously observed at the other sensor.</paragraph><paragraph>In this traffic simulation, a car decides which action (e.g., which velocity on which lane) to perform and whether to follow or ignore the received directives based on its information and preferences. In particular, the car model is designed by means of an action selection function that selects an action that maximizes the car's utility. The action selection function is defined in terms of the car's internal state (including its current velocity, position, and the received directive), an expected arrival time function that, given the current traffic situation, determines the impact of a velocity action on the arrival time at the car's destination (for simplicity it is assumed that all cars have the same destination), an action effect function (that determines the expected consequence of a velocity action on its internal state), a sanction grading function (mapping fines to real values reflecting the utility of a fine), and an arrival grading function (mapping an arrival time to real values reflecting the utility of the given arrival time at the car's destination). The internal state of the car together with the expected arrival time function and the action effect function constitute an agent's information about themselves and the traffic situation including other cars. The sanction grading function and the arrival grading function constitute the preference of the cars. The further details of the car model can be found in [12].</paragraph><paragraph>The objective of this simulation was to investigate the impact of the enforcement of various norm sets on traffic situation in the ramp merging scenario for a different population of cars. Two types of cars were distinguished: leisure and business cars. These two types were implemented by the sanction and arrival time grading functions. The sanction grading function for a leisure car evaluates a sanction as having a higher impact on the car's utility compared to the same function for a business car. Conversely, the arrival time grading function for a leisure car evaluates a late arrival time as having less impact on the car's utility compared to the same function for a business car. The simulation results show that the number of norm violations, and therefore traffic congestion, decreases as the severity of fines increases. In particular, they show that norm violations do not occur when proper fines (i.e., fines that match the cars' preferences) are imposed.</paragraph><paragraph>The formal connection between the proposed abstract mechanism design methodology and the specification language, as established in Proposition 14, may suggest that the results of the mechanism design analysis of norms can be related to the results of the implementation of norms in multi-agent simulations. Such a relation can be used to justify the simulation results by means of mechanism design explanations, or vice versa, to verify the results of the mechanism design analysis by means of simulations. For example, if a mechanism design analysis of the ramp merging scenario shows that a set of traffic norms implements the system designer's objective to avoid traffic congestion (i.e., to avoid simultaneous arrival of cars at the merging point m) assuming that cars follow their Nash equilibrium strategies, then one can use the specification language to simulate the scenario in order to verify whether the enforcement of the traffic norms avoids traffic congestion.</paragraph><paragraph>However, connecting theoretical results obtained by a mechanism design analyses and by experimental results obtained by running agent-based simulations is not straightforward and requires further considerations. For example, one could argue that the reported experimental results from the traffic simulation in SUMO can be used to claim that an observed reduction of traffic congestion is due to the optimality of norms in the sense that the norms implement the objective of avoiding traffic congestion in Nash equilibrium. However, such claims could only be justified if the simulated cars were capable of strategic reasoning, which is not the case in the reported traffic simulation experiment. The ability of strategic reasoning for cars is not supported by our extension of the SUMO platform and requires further extensions. This is due to the fact that the preferences of the cars are not accessible to each other such that the cars do not share the structure of the game and are thus unable to reason strategically. This implies that the simulations setting in SUMO is not yet rich enough to establish a connection to our mechanism design setting.</paragraph><paragraph>It should also be noted that our formal mechanism design methodology can now be applied to analyse only one snapshot of the traffic simulation, i.e., to analyse the behaviour of cars that arrive simultaneously at the sensor positions. Such a snapshot of the ramp merging scenario constitutes a game setting that is quite comparable with the setting of our running example (the narrow part of the road in the running example is the merging point of the ramp merging scenario). In order to connect the mechanism design methodology and the experimental results of the simulations, which consists of a continuous stream of cars, one could model the simulation as a sequence of games. Although this may be a reasonable suggestion, one needs to investigate how to model and analyse the change and development of the traffic state in consecutive game settings. For example, a high stream of cars may necessarily cause the creation of traffic congestion at the merging point, which changes the state of the traffic and therefore the structure of the consecutive games. We believe that a profound connection between mechanism design and simulation settings is a challenging future research direction.</paragraph></section><section label="7"><section-title>Related work</section-title><paragraph>There have been many proposals on abstract and executable models for norms and norm enforcement in multi-agent systems. However, unlike our work, these proposals focus either on abstract models of norm enforcement or on executable models, ignoring the connection between them. Despite this key difference, our abstract and executable model differ from existing abstract and executable models, respectively. In the following, we first compare our abstract model for norm and norm enforcement to existing abstract models, and then compare our executable specification language with existing executable models for norms and norm enforcement.</paragraph><paragraph>Our abstract model of norms and norm enforcement is closely related to [1] and [65], though they consider a norm from a semantic perspective as a set of transitions instead of a syntactic expression as in our case. In particular, they use labelled Kripke structures as multi-agent models, supposing that each agent controls a specific number of transitions. The enforcement of a norm is then considered as the deactivation of specific transitions. In our proposal, we distinguish between regimenting and sanctioning norms, which is also one of the main conceptual differences besides the mechanism design perspective we follow. The enforcement of the regimenting norms is similar to the deactivation of transitions as in [1], [65], but the enforcement of sanctioning norms may create new states resulting from a relabelling by soft facts and thereby new transitions. In this sense, our approach is different as the enforcement of norms may change the underlying multi-agent model with new states and transitions. From a conceptual point of view the agent can still perform the action, the physical structure encoded by means of hard facts remains intact, but may have to bear the imposed sanctions depending on the agents' preferences. Note also, the change in the underlying transition system may only affect some agents, namely those which use relevant soft facts in their preferences. Another difference to our work is that the outcome of norm enforcement is assumed to be independent of the preferences where we consider a more general setting captured in terms of normative behaviour functions. It is also important to recall that our focus is of a more practical nature. We try to implement and to analyse mechanisms from a practical point of view, i.e., how to implement and verify norm-based multi-agent environment by means of executable specifications. The work presented in [3] also assumes that the designer has multiple objectives the value of which is determined by the (de)activated transitions in the transition system. The authors show how to compactly represent the designer's objectives by (a set of) CTL formulae each assigned a feature value. Then, the utility of a social law is the sum of all feature values of the satisfied CTL formulae minus the costs to implement the social law. The authors give an algorithm to compute optimal social laws as well as complexity results. The focus of the work is on computing a good social law from the designer's perspective, not considering agents' objectives at first place (which is a key concern in our setting).</paragraph><paragraph>Fitoussi and Tennenholtz [34] put forward a formal framework to engineer and study social laws in multi-agent systems. A social law restricts the set of agents' actions. The authors investigate two properties of social laws: minimality and simplicity. Minimality refers to the number of restrictions imposed on the system, where simplicity refers to the capabilities of agents. Simpler laws can be followed more easily by simpler agents. The authors consider complexity issues about the existence of appropriate norms and study their properties. The setting is quite abstract, using strategies similar to those known from normal form games, whereas we start from a transition system usually requiring multiple step strategies. Also, the authors consider two specific types of goals, liveness and safety goals, whereas we allow arbitrary LTL-formulae to specify agents' preferences.</paragraph><paragraph>Endriss et al. [30] and Wooldridge et al. [69] propose taxation schemes to incentivize agents to change their behaviour such that the emerging behaviour is stable and shows desirable properties in line with the system specification. In particular, the computational complexity of the weak and strong implementation problems is investigated. We have drawn inspiration from these problems in the present article. A difference with our work is that they study these problems in the context of Boolean games, where we consider a strategic setting in which agents act in a temporal setting. Instead of taxation schemes we follow a norm-based approach, and use techniques of mechanisms design to specify the system designer's objectives and analyse their implementability. We believe that this approach is quite flexible and allows to model more realistic settings in which the system designer may not know the agents' preferences and should therefore consider a set of possible preferences.</paragraph><paragraph>A different avenue of work in this area focuses on norm monitoring in the context of imperfect monitors, e.g., [20], [8]. In the present paper, we have assumed that monitors are perfect in the sense that the norm enforcement mechanism can detect and respond to all norm violating behaviours. It should be clear that any work on norm enforcement either implicitly or explicitly assumes that the behaviour of agents is monitored and evaluated with respect to a given set of norms. Our assumption that monitors are perfect is reflected by the fact that updating a multi-agent model (i.e. CGS) with a set of norms covers possible (violating) behaviour. Moreover, although [20] and [8] consider norms syntactically like us, they use LTL-formulae as norm representation to refer to good/bad behaviour. We have considered less expressive norms in order not to complicate the main message of our approach unnecessarily. We believe that our general approach can be instantiated with more expressive norms as well.</paragraph><paragraph>Another line of related research concerns the issue of norm synthesis. Shoham and Tennenholtz [59], [60], have discussed the problem of off-line design of a set of norms in order to constrain the behaviour of agents and to ensure the overall objectives of the multi-agent system. In this work and similar to our approach, the structure of multi-agent systems is required and the norms are generated at design time. In line with this tradition, Morales et al. [53], [52] consider the problem of on-line design of a set of norms to constrain and steer the behaviour of agents. In both off-line and on-line approaches, the overall objectives of multi-agent systems are guaranteed by assuming that agents are norm-aware and comply with the generated norms. In this sense norms are considered as being regimented in multi-agent systems. Moreover, in contrast to our work, these approaches neither provide a game theoretic analysis of norms nor consider the generation of norms with sanctions in the context of agents' preferences. However, we believe that the concepts such as effective, necessary, and concise norms as introduced by [53], [52] can also be used in off-line norm synthesis approaches such as ours. Following the tradition of Shoham and Tennenholtz, Boella and van der Torre [15] consider the problem of norm enforcement by distinguishing the choice of off-line designed stable social laws from the choice of control systems. A control system is explained to be responsible for monitoring and sanctioning of norm violations. Although the functionality of their proposed notion of control system is similar to the functionality of our notion of norm-based mechanism, there are some fundamental differences between these approaches. For example, in our approach the behaviour of a multi-agent system is modelled by a concurrent game structure while they consider a multi-agent system in an abstract way as a one-shot game, norms in our approach are explicit and enforced by an update mechanism while they consider norms as integrated in the structure of the games and enforced by a special agent called normative system that selects which game is going to be played, and finally they consider the notion of (quasi-)stable social laws while we consider norms from a mechanism design perspective as implementing a social choice function in Nash equilibria.</paragraph><paragraph>Our work differs also from verification approaches to norm-based systems or protocols [33], [7], [27], [5]. Of course, one can consider and exploit our work as an approach to verify the impact of norm enforcement on agents' behaviour in the sense that our approach can be used to check the influence of norm enforcement on agents' behaviour. However, in contrast to our approach, the mentioned work focuses on different types of norms and norm enforcement mechanisms, and does not provide any game theoretic tool to analyse the impact of norms on the behaviour of rational agents. In particular, in [33] norms are defined in terms of communication actions and enforced by means of regimentation, while [7] and [27] consider norms as state-based obligations/prohibitions enforced by means of both regimentation and sanctions. The approach presented by [5] focuses mainly on the protocol verification and aims at providing a mechanism that can be used, for example by the agents, to decide whether following a protocol guarantees their objectives without any norm violation.</paragraph><paragraph>In the literature of multi-agent systems various proposals focus on the issue of the practical implementation of norms and norm enforcement. They consider norms and norm enforcement mechanisms in broader contexts such as institutions, organisations, or coordination environments. Examples are electronic institutions such as ISLANDER/AMELI [33], [32], organisational models such as {a mathematical formula}MOISE{sup:+}/{a mathematical formula}S–{a mathematical formula}MOISE{sup:+}[42], [43] and OperettA [4], coordination models such as ORG4MAS [41], the norm-based framework proposed by [24], the law-governed interaction proposed by [51], and the norm enforcement mechanism proposed by [35]. The focus on these proposals is primarily on the development of norm-based multi-agent system rather than devising special purpose programming languages to implement norms and norm enforcement. Moreover, the lack of explicit formal syntax and (operational) semantics for norm-related concepts in these approaches makes it difficult, if not impossible, to relate them to the existing abstract models for norms and norm enforcement. These approaches are concerned with agents' interaction, use different norm types, or focus on norm regimentation only. In the following, we discuss further details of some of these approaches.</paragraph><paragraph>ISLANDER [31] is one of the early modelling languages for specifying institutions in terms of institutional rules and norms. In order to interpret institution specifications, an execution platform, called AMELI, has been developed [32]. This platform implements an infrastructure that, on the one hand, facilitates agent participation within the institutional environment supporting their communication and, on the other hand, enforces the institutional rules and norms. The distinguishing feature of ISLANDER/AMELI is that it does not allow any agent to violate norms, i.e., norms are regimenting. Moreover, norms in [32], but also in [36] and [61], are action-based and prescribe actions that should or should not be performed by agents.</paragraph><paragraph>Another related approach is {a mathematical formula}MOISE{sup:+}[43], where a modelling language is proposed to specify multi-agent systems through three organisational dimensions: structural, functional, and deontic. The relevant dimension for our work is the deontic dimension that concerns concepts such as obligations and prohibitions. Different computational frameworks have been proposed to implement and execute {a mathematical formula}MOISE{sup:+} specifications, e.g., {a mathematical formula}S–{a mathematical formula}MOISE{sup:+}[42] and its artifact-based version ORG4MAS [41]. These frameworks are concerned with norms prescribing states that should be achieved/avoided. {a mathematical formula}S–{a mathematical formula}MOISE{sup:+} is an organisational middleware that provides agents access to the communication layer and the current state of the organisation. This middleware allows agents to change the organisation and its specification, as long as such changes do not violate organisational constraints. In this sense, norms in {a mathematical formula}S–{a mathematical formula}MOISE{sup:+} can be considered as regimenting norms. ORG4MAS uses organisational artifacts to implement specific components of an organisation such as group and goal schema. In this framework, a special artifact, called reputation artifact, is introduced to manage the enforcement of norms.</paragraph><paragraph>There are two proposals that specifically aim at implementing norms and norm enforcement. The first proposal, presented in [64] and [27], is a norm-based executable specification language designed to facilitate the implementation of software entities that exogenously control and coordinate the behaviour of agents by means of norms. Similar to our approach, norms in this proposal can be either sanctioning or regimenting norms. Also, similar to our approach, they come with an operational semantics such that executable specifications can be formally analysed by means of verification techniques [11]. However, in contrast to the approach presented in the present paper, they consider state-based norms such as obligation or prohibition of states and ignore action-based norms. This proposal comes with an interpreter, called 2OPL, which initiates a process that continuously monitors agents' actions (i.e., communication and environment actions), determines the effect of the observed actions based on the action specifications, and enforces norms when necessary. We plan to extend 2OPL such that it can interpret and execute action-based norms as presented in the present paper. The second proposal, called JaCaMo [16], aims at supporting the implementation of organisational artifacts, which are responsible for the management and enactment of the organisation. An organisational artifact is implemented by a program which implements a {a mathematical formula}MOISE{sup:+} specification. A translation of {a mathematical formula}MOISE{sup:+} specifications into norm-based programs is described by [44]. In contrast to our approach, the sanctions in JaCaMo are actions that are delegated to some agents and there is no guarantee that the agents will eventually perform the actions. In particular, the violation of norms is detected by organisational artifacts after which organisational agents have to deal with those violations.</paragraph><paragraph>We conclude this section by the following observation. We have assumed that agents are norm aware in the sense that agents follow their preferences and choose optimal behaviours in order to maximize their utilities. The norm awareness is reflected in our approach by 1) defining agents' preferences in terms of specific behaviour and whether the agents incur sanctions, and 2) by applying the equilibrium analysis to characterize the behaviour of rational agents under norm enforcement. However, we did not focus on how individual agents reason to choose their optimal behaviours as studied, for example, by [66] and [6]. In contrast to [66] and [6], we abstract over the specific reasoning schemes of individual agents and assume that whatever reasoning schemes individual agents use, they will always act rationally according to game theoretic concepts, more precisely according to Nash equilibria.</paragraph></section><section label="8"><section-title>Conclusions, discussion, and future work</section-title><paragraph>Our work focuses on norms and norm enforcement in multi-agent systems. We proposed a formal methodology that allows analysing norms and norm enforcement from a mechanism design perspective, and a programming model for implementing them. Using game theoretic tools we showed that the enforcement of norms can change the behaviour of rational agents using regimentation and sanctions. It is also shown that our presented programming model is aligned with the abstract model such that our developed game theoretical tools can be applied to analyse norm-based environment programs.</paragraph><paragraph>Specifically, we proposed norm-based mechanism design as a formal methodology for analysing norm-based environment programs. We showed how to abstract from a particular environment specification language and how to apply methods from mechanism design to verify whether the enforcement of a set of norms on a multi-agent environment agrees with the behaviour of rational agents that the system designer expects. More precisely, we introduced normative behaviour functions for representing the “ideal” behaviour of multi-agent environments with respect to different sets of agents' preferences. The latter enabled us to apply concepts from game theory to identify agents' rational behaviour. These formal ideas can now be used to verify whether the enforcement of a set of norms is sufficient to motivate rational agents to act in such a way that their behaviour become aligned with that described by the normative behaviour function.</paragraph><paragraph>We defined a normative system in such a way that it can modify (soft) facts of the environment states. As the language used for modelling agents' preferences and the facts in normative systems are based on the same set of propositional symbols, a norm-based mechanism can steer the behaviour of each agent in flexible ways. This notion of mechanism is powerful. A first refinement could be to identify a subset {a mathematical formula}ΠM⊆Π of soft facts and assume that a normative system can only modify state valuations with respect to this set. Such a mechanism can be much weaker but also more natural.</paragraph><paragraph>Another direction for future research is to consider robustness against group deviation. Our approach can be extended such that each agent a has its “own” set {a mathematical formula}Πa of propositional symbols which is used to specify its preference. If we now want that some agents are not sensitive to norms and sanctions we simply define the set {a mathematical formula}ΠNF of facts that are used in a normative system such that {a mathematical formula}ΠNF∩Πa=∅. Another alternative is to take on a more game theoretic point of view in the line with [1] and [23]. For example, one may consider partial strategies which assume that only subgroups of agents play rationally. Then, the outcome is usually not a single path any more, but rather a set of paths. This gives rise to a notion of {a mathematical formula}(S,A)-implementability.</paragraph><paragraph>We investigated the problem, given a CGS {a mathematical formula}M and a set of agents' preferences {a mathematical formula}Prefs, and a normative behaviour function f, whether there is a normative system {a mathematical formula}N which {a mathematical formula}NE-implements f over {a mathematical formula}M, q and {a mathematical formula}Prefs. In future work it would be interesting to identify settings in which such normative systems can be constructed efficiently. We also plan to extend our analysis to other implementability notions apart from Nash equilibria in more detail, e.g. dominant strategy equilibrium implementability.</paragraph><paragraph>Finally, yet another interesting direction for future research is to investigate core properties of classical mechanism design in our norm-based setting, including budget balanced and individual rational mechanisms. We note that already the interpretation of these properties in our setting is interesting. For example, in the case of individual rationality it is not clear what it means for an agent “not to take part” in the mechanism/in the multi-agent systems. This may require a shift to an open MAS where agents can leave an join the system.</paragraph><section-title>Acknowledgement</section-title></section></content><acknowledgements><paragraph>We thank the anonymous reviewers for their extensive and valuable comments which significantly improved the paper.</paragraph></acknowledgements><appendices><section label="Appendix A"><section-title>Quantified Boolean satisfiability problem</section-title><paragraph>Our hardness proofs reduce validity of Boolean quantified formulae to the implementation problems. We consider fragments of the Quantified Boolean Satisfiability problem (QSAT), a canonical PSPACE-complete problem. Restricting the number of quantifier alternations of QSAT yields subproblems which are complete for different levels of the polynomial hierarchy. The problem class QSATi starts with an existential quantifier and allows for {a mathematical formula}i−1 quantifier alternations; similarly, ∀QSATi-formulae begin with an universal quantifier, {a mathematical formula}i=1,2,… . The previous problems are {a mathematical formula}ΣiP and {a mathematical formula}ΠiP-complete, respectively. In the formal definition we write QX for {a mathematical formula}Qx1…Qxn for a set {a mathematical formula}X={x1,…,xn} of propositional variables and {a mathematical formula}Q∈{∀,∃}.</paragraph><paragraph label="Definition 25">QSATi[55]The QSATi problem is defined as follows.Input: A quantified Boolean formula (QBF) {a mathematical formula}ϕ=∃X1∀X2…QiXiφ where φ is a Boolean formula in negation normal form (nnf) (i.e., negations occur only at the propositional level) over disjoint sets of propositional variables {a mathematical formula}X1,…,Xi and {a mathematical formula}i≥1 where {a mathematical formula}Qi=∀ if i is even, and {a mathematical formula}Qi=∃ if i is odd. ϕ does not contain any free variables.Output: True if {a mathematical formula}∃X1∀X2…QiXiφ is valid, and false otherwise.The problem ∀QSATi is defined analogously but formulae ϕ start with a universal quantifier and then alternate between quantifier types.By abuse of notation, we refer to a QBFϕ that satisfies the structural properties required by the problem class QSATi and ∀QSATi simply as QSATi-formula and ∀QSATi-formula, respectively.</paragraph><paragraph>A truth assignment or valuation for a set of variables X is a mapping {a mathematical formula}v:X→{t,f}. Given a Boolean formula φ over variables X and a truth assignment v over {a mathematical formula}Y⊆X we denote by {a mathematical formula}φ[v] the formula obtained from φ where each {a mathematical formula}y∈Y is replaced by ⊥ (falsum) and ⊤ (verum) if {a mathematical formula}v(y)=f and {a mathematical formula}v(y)=t, respectively. For two truth assignments {a mathematical formula}v1 and {a mathematical formula}v2 over X and Y, respectively, with {a mathematical formula}X∩Y=∅ we use the notation {a mathematical formula}v1∘v2 to refer to the induced truth assignment over {a mathematical formula}X∪Y; analogously for more than two truth assignments. Using this notation a formula {a mathematical formula}∃X1∀X2…QiXiφ is true iff there is a truth assignment {a mathematical formula}v1 over {a mathematical formula}X1 such that for all truth assignments {a mathematical formula}v2 over {a mathematical formula}X2 etc. the Boolean formula {a mathematical formula}φ[v1∘…∘vi] is valid. For further details, we refer to [55].</paragraph></section><section label="Appendix B"><section-title>Proofs to implementation problems: verification</section-title><section label="B.1">Hardness of weak implementation problem: Proposition 4<paragraph>We show that the membership problem “{a mathematical formula}N∈WINE(I,q,f)” is {a mathematical formula}Σ2P-hard by reducing QSAT2 to it. In the following we consider an instance of QSAT2 of the form{a mathematical formula} where {a mathematical formula}X1={x1,…,xm} and {a mathematical formula}X2={xm+1,…,xn}. The reduction consists of three main steps:</paragraph><list><list-item label="1.">We encode ϕ as a two-player CGS{a mathematical formula}Mϕ and show that ϕ is satisfiable if, and only if, player one has a winning strategy in {a mathematical formula}Mϕ to achieve a given property (Lemma 16).We use results from [22] where it was shown that the satisfiability of a QBFϕ can be reduced to model checking a two-player CGS such that one of the players, the verifierv, has a winning strategy to enforce a (fixed) formula which is constructed from ϕ, against all strategies of the other player, the refuterr, if and only if, ϕ holds.</list-item><list-item label="2.">We construct a preference profile {a mathematical formula}γ→=(γv,γr) such that a winning strategy of the verifier in {a mathematical formula}Mϕ is part of a Nash equilibria in {a mathematical formula}Γ(Mϕ,q,(γv,γr)) if, and only if, ϕ is satisfiable.</list-item><list-item label="3.">We show that the existence of such a specific Nash equilibria can be answered by testing membership in the weak implementation problem.</list-item></list><paragraph> In the following we assume that the QSAT2-formula ϕ given above is fixed, including the indexes m and n and that it is in negated normal form. Moreover, in the following it is important that {a mathematical formula}X1 and {a mathematical formula}X2 are non-empty. This can be assumed without loss of generality.</paragraph><section label="B.1.1">The model {a mathematical formula}Mϕ<paragraph>We describe the construction the CGS {a mathematical formula}Mϕ from formula ϕ. The idea of the construction in [22] is that the verifierv (controlling existential variables and disjunctions) and the refuterr (controlling existential variables and conjunctions) firstly choose truth values of the variables they control. This is illustrated in Fig. B.15.</paragraph><paragraph>For example, if v plays ⊤ in {a mathematical formula}q2 a state labelled {a mathematical formula}x2 is reached; otherwise, a state labelled {a mathematical formula}notx2. This represents that variable {a mathematical formula}x2 is assigned true or false, respectively. This part of the model is called value choice section and consists of states{a mathematical formula} States {a mathematical formula}qi with {a mathematical formula}1≤i≤m are controlled by v, states with {a mathematical formula}m+1≤i≤n are controlled by r. Afterwards, both agents simulate the game theoretic semantics of propositional logic. Player v tries to make the formula true (thus controls disjunctions) and r tries to falsify the formula (thus controls conjunctions). This part of the model corresponds to the parse tree of a formula, see Fig. B.16. For the formal definition we need additional notation. First, we use {a mathematical formula}sf(ϕ) to denote the set of subformulae of ϕ. For every formula {a mathematical formula}ψ=ψ1∘ψ2 with {a mathematical formula}∘∈{∧,∨} we use {a mathematical formula}L(ψ)=ψ1 and {a mathematical formula}R(ψ)=ψ2 to refer to the left and right subformula of ϕ, respectively. If {a mathematical formula}ψ=¬ψ′ and ψ is not a literal{sup:14} we define {a mathematical formula}L(ψ)=R(ψ)=ψ′. This allows to use a sequence of L's and R's, i.e. an element from {a mathematical formula}{L,R}⁎, to uniquely refer to a subformula where ϵ refers to ϕ itself. We refer to such a sequence as index. For a formula ϕ we use {a mathematical formula}ind(ϕ)⊆{L,R}⁎ to denote the set of all possible indexes wrt. ϕ. By slight abuse of notation, we denote the subformula referred to by such an index also by {a mathematical formula}sf(i) where {a mathematical formula}i∈{L,R}⁎. Note, that different indexes can refer to the same subformula. For example, given the formula {a mathematical formula}(x1∧x2)∨(x2∧¬x1) we have that {a mathematical formula}sf(LR)=sf(RL)=x2. Note, as we stop at the literal level, the indexes RRL and RRR are not contained in {a mathematical formula}ind((x1∧x2)∨(x2∧¬x1)).</paragraph><paragraph>Given this notation, the formula structure section of the model consists of states{a mathematical formula} For every index ι where {a mathematical formula}sf(ι)=ψ=ψ1∘ψ2 with {a mathematical formula}∘∈{∧,∨} one of the players chooses {a mathematical formula}L(ψ)=ψ1 or {a mathematical formula}R(ψ)=ψ2. If {a mathematical formula}∘=∨ the verifier v chooses the subformulae; otherwise the refuter does. The “semantic game” between both players ends up in a literal, modelled by a literal state. The section of literals is built over states{a mathematical formula} for each index ι corresponding to a literal l in Φ, we have that the state {a mathematical formula}qι is controlled by the owner of the Boolean variable {a mathematical formula}xi in l (i.e. {a mathematical formula}l=xi or {a mathematical formula}l=¬xi). As in the value choice section, the owner of that state chooses a value (⊤ or ⊥) for the underlying variable (not for the literal!) which leads to a new state of the evaluation section. These states are denoted by{a mathematical formula} A state {a mathematical formula}qιv with {a mathematical formula}sf(ι)=xi is labelled with the proposition {a mathematical formula}xi if {a mathematical formula}v=⊤ and with {a mathematical formula}notxi if {a mathematical formula}v=⊥; similarly, {a mathematical formula}qιv with {a mathematical formula}sf(ι)=¬xi is labelled with the proposition {a mathematical formula}xi if {a mathematical formula}v=⊥ and with {a mathematical formula}notxi if {a mathematical formula}v=⊤. That is, the label {a mathematical formula}v∈{⊤,⊥} models the evaluation of the literal and not of the underlying variable. These states shall be used to ensure that a strategy induces a truth assignment, as further explained below. Then, the system proceeds to the winning state {a mathematical formula}q⊤ (labelled with the proposition {a mathematical formula}winv) if the valuation of {a mathematical formula}xi makes the literal {a mathematical formula}sf(ι) true, and to the losing state {a mathematical formula}q⊥ (labelled with the proposition {a mathematical formula}winr) otherwise – see Fig. B.17 for details. Finally, we need two special gadgets to ensure that the reduction works. First, we connect a state {a mathematical formula}qd to the initial state {a mathematical formula}q0. This state will be used to ensure the existence of some Nash equilibrium. Secondly, we need to give the players a possibility to mark specific strategies as inconsistent, for reasons which will become clear below. Therefore, we insert a small substructure as shown in Fig. B.18 in-between the start state {a mathematical formula}q0 and {a mathematical formula}q1, the beginning of the value-choice section. The whole construction is illustrated in Fig. B.19. We refer to the CGS just constructed as {a mathematical formula}Mϕ. The formal definition is given next.</paragraph><paragraph label="Definition 26">Model {a mathematical formula}MϕLet QSAT2-formula {a mathematical formula}ϕ≡∃{x1,…,xm}∀{xm+1,…,xn}φ be given and in negated normal form. We define {a mathematical formula}Xv={x1,…,xm} and {a mathematical formula}Xr={xm+1,…,xn}. The CGS {a mathematical formula}Mϕ=(Agt,Q,Π,Act,π,d,o) is defined as follows:</paragraph><list><list-item label="•">{a mathematical formula}Agt={v,r},</list-item><list-item label="•">{a mathematical formula}Q={q0,q0′,qd}∪{qv,qv′,qr,qr′}∪Q1∪Q2∪Q3∪Q4∪Q5,</list-item><list-item label="•">{a mathematical formula}Π={xi|i=1,…,n}∪{notxi|i=1,…,n}∪{winv,winr,start,d,ir,iv},</list-item><list-item label="•">{a mathematical formula}π(q0)={start}, {a mathematical formula}π(qv′)={iv}, {a mathematical formula}π(q0′)=π(qr′)={ir}, {a mathematical formula}π(qi⊤)={xi} for all {a mathematical formula}qi⊤∈Q1, {a mathematical formula}π(qi⊥)={notxi} for all {a mathematical formula}qi⊤∈Q1, {a mathematical formula}π(qxi⊤)={xi} for all {a mathematical formula}qxi⊤∈Q4, {a mathematical formula}π(qxi⊥)={notxi} for all {a mathematical formula}qxi⊤∈Q5, {a mathematical formula}π(q¬xi⊥)={xi} for all {a mathematical formula}q¬xi⊤∈Q4, {a mathematical formula}π(q¬xi⊤)={notxi} for all {a mathematical formula}q¬xi⊤∈Q5, {a mathematical formula}π(q⊤)={winv}, {a mathematical formula}π(qd)={d} and {a mathematical formula}π(q⊥)={winr}.</list-item><list-item label="•">{a mathematical formula}Act={L,R,⊤,⊥,−}</list-item><list-item label="•">the function d is defined as</list-item><list-item label="•">and o is defined as:</list-item></list><paragraph>Each state of {a mathematical formula}Mϕ has at most two outgoing transitions, with the exception of state {a mathematical formula}q0 which has four. Hence, the number of transitions is polynomial in the number of states. In the next section we describe how to ensure that only strategies of the players are taken into consideration which correspond to truth assignments.</paragraph></section><section label="B.1.2"><section-title>Strategies and assignments</section-title><paragraph>If a path in the model goes through a state labelled {a mathematical formula}xi and {a mathematical formula}notxi this can be interpreted as setting variable {a mathematical formula}xi true and false, respectively. We observe that the states which have as children an {a mathematical formula}xi-state or a {a mathematical formula}notxi-state the transitions to a successor is determined by a single player only. Thus, a strategy of a player completely determines which {a mathematical formula}xi-states and {a mathematical formula}notxi-states, where {a mathematical formula}xi is a variable of the very player, are visited. It may happen that a path contains an {a mathematical formula}xi-state as well as a {a mathematical formula}notxi-state, by performing contrary actions in a state {a mathematical formula}qi of the value-choice section and in {a mathematical formula}qι with {a mathematical formula}sf(ι)=xi in the literal section. If this happens we call the responsible strategy of the player inconsistent as it does not encode a truth assignment of the variables that the player controls. Thus, it has to be ensured that choices are made consistently: the same variable x is always assigned true, or always assigned false. For this purpose the following consistency constraints—temporal formulae—are introduced for each {a mathematical formula}i∈{1,…,n}:{a mathematical formula} It is easy to see that if {a mathematical formula}Ti is true along a path then this path—or the associated strategy of the player that controls {a mathematical formula}xi—represents a truth assignment of {a mathematical formula}xi. We set{a mathematical formula} where the propositions {a mathematical formula}ir and {a mathematical formula}iv are used to indicate that the current strategy of the refuter and verifier, respectively, are invalid. The latter is needed to prevent specific strategy profiles to constitute a Nash equilibrium. We call a strategy of the verifier v (resp. refuter r) consistent if for all strategies of the refuter (resp. the verifier) the induced outcome path satisfies {a mathematical formula}Tv (resp. {a mathematical formula}Tr). In the case of consistent choices we observe that the formula structure section together with the sections of literals implement the game theoretical semantics of Boolean formula [39].</paragraph><paragraph>Next we recall from [22] (with small modifications) the following lemma which says that ϕ is satisfiable iff there is a consistent strategy of player v such that for all consistent strategies of player r—again, consistent means that truth values are assigned consistently to variables and that it is not invalid—such that eventually {a mathematical formula}winv holds. We emphasize that our construction does not assume perfect-recall strategies due to the fact the we are only considering one quantifier alternation. Also, note that if {a mathematical formula}q⊥ or {a mathematical formula}q⊤ is reached, each agent can switch from a consistent strategy to an inconsistent one without changing the outcome paths with the possible exception of the transitions between {a mathematical formula}qv and {a mathematical formula}q1.</paragraph><paragraph label="Proof">Let a QSAT2-formula{a mathematical formula}ϕ≡∃{x1,…,xm}∀{xm+1,…,xn}φ(x1,…,xn)in negated normal form be given. The model{a mathematical formula}Mϕcan be constructed in polynomial time. We have that ϕ is satisfiable iff there is a strategy{a mathematical formula}svof the verifiervwith{a mathematical formula}sv(q0)=⊥such that for all strategies{a mathematical formula}srof the refuterrit holds that{a mathematical formula}outMϕ(q0,(sv,sr))⊨LTLTv∧(Tr→◇winv).“⇒”: Suppose ϕ is true and let {a mathematical formula}sv be the strategy induced by a truth assignment v of {a mathematical formula}{x1,…,xm} witnessing the truth of ϕ (i.e. {a mathematical formula}φ[v] is valid) and by the simulation of the game theoretic semantics which witnesses the truth of {a mathematical formula}φ[v]. Moreover, let {a mathematical formula}sv(q0)=⊥ (this is needed to avoid that the refuter can reach the state {a mathematical formula}qd) and let {a mathematical formula}sr be any consistent strategy of r. Note that the strategy {a mathematical formula}(sv,sr) induces a truth assignment {a mathematical formula}v1∘v2 of {a mathematical formula}Xv∪Xr. Then, {a mathematical formula}◇winv must be true on {a mathematical formula}outMϕ(q0,(sv,sr)) otherwise the refuter had a strategy {a mathematical formula}sr which induces a truth assignment v with {a mathematical formula}φ[v1∘v] is false. Which would yield a contradiction.“⇐”: Let {a mathematical formula}sv be a strategy of the verifier v with {a mathematical formula}sv(q0)=⊥ such that for all strategies {a mathematical formula}sr of the refuter r it holds that {a mathematical formula}outMϕ(q0,(sv,sr))⊨LTLTv∧(Tr→◇winv). {a mathematical formula}sv induces a truth assignment v. For any consistent strategy {a mathematical formula}sr we have that {a mathematical formula}◇winv is true. It is straight-forward to check that the truth assignment v induced by {a mathematical formula}sv encodes a truth assignment witnessing the truth of ϕ due to the encoded game theoretic semantics in the model. That is, {a mathematical formula}φ[v] is valid and thus ϕ is true.  □</paragraph></section><section label="B.1.3"><section-title>Preferences and Nash equilibria</section-title><paragraph>Lemma 16 showed that the satisfiability problem of a given QSAT2-formula ϕ can be reduced to a strategy existence and model checking problem of a rather simple formula in {a mathematical formula}Mϕ. What we need for the reduction to the weak implementability problem is that a winning strategy of the verifier is part of a Nash equilibrium iff the formula ϕ is satisfiable. For this purpose, we define the following preference lists for player v and r, respectively:{a mathematical formula}{a mathematical formula}</paragraph><paragraph>Now we can relate the satisfiability of ϕ to the existence of Nash equilibria which contains a winning strategy of the verifier according to Lemma 16.</paragraph><paragraph label="Proposition 17">Nash equilibria in {a mathematical formula}MϕLet ϕ be a QSAT2-formula in nnf and{a mathematical formula}(γv,γr)be the preference profile defined above.</paragraph><list><list-item label="(a)">We have that{a mathematical formula}(sd,sd′)∈NE(Mϕ,q0,(γv,γr))for any two strategies{a mathematical formula}sdand{a mathematical formula}sd′with{a mathematical formula}sd(q0)=sd′(q0)=⊤.</list-item><list-item label="(b)">ϕ is satisfiable iff there is an{a mathematical formula}s∈NE(Mϕ,q0,(γv,γr))with{a mathematical formula}outMϕ(q0,s)⊨LTL◇winv.</list-item><list-item label="(c)">Let{a mathematical formula}γv′and{a mathematical formula}γr′equal{a mathematical formula}γvand{a mathematical formula}γrbut with the first list entry{a mathematical formula}(◯d,4)being removed, respectively. Then, it holds that ϕ is satisfiable iff{a mathematical formula}NE(Mϕ,q0,(γv′,γr′))≠∅.</list-item></list><paragraph label="Proof"><list><list-item label="(a)">The strategy profile {a mathematical formula}(sd,sd′) yields the path {a mathematical formula}q0qdω. For that path the payoff for both players is four. No player can deviate to improve its payoff. It is a Nash equilibrium.</list-item><list-item label="(b)">“⇒”: If ϕ holds then, according to Lemma 16, there is a strategy {a mathematical formula}sv such that {a mathematical formula}sv(q0)=⊥ and for all strategies {a mathematical formula}sr it holds that {a mathematical formula}outMϕ(q0,(sv,sr))⊨Tv∧(Tr→◇winv) (⋆). We claim that the profile {a mathematical formula}(sv,sr′) for any consistent strategy {a mathematical formula}sr′ of the refuter is a Nash equilibrium. First, observe that {a mathematical formula}sr(q0)=⊥, otherwise {a mathematical formula}Tr could not be true on the outcome path. Second, note that such a strategy {a mathematical formula}sr′ must exist as r always has a consistent strategy. Given these strategies players v and r get a payoff of 3 and 1, respectively. As {a mathematical formula}sr(q0)=sv(q0)=⊥ no player can deviate to reach state {a mathematical formula}qd. Moreover, player r cannot deviate to a consistent strategy making {a mathematical formula}¬◇winv true by Lemma 16. This shows that no player can unilaterally deviate to increase its payoff.“⇐”: Suppose ϕ is false. That is, (⋆) for each truth assignment {a mathematical formula}v1 of {a mathematical formula}{x1,…,xm} there is a truth assignment {a mathematical formula}v2 of {a mathematical formula}{xm+1,…,xn} such that {a mathematical formula}φ[v1∘v2] is false. We consider a strategy profile {a mathematical formula}s=(sv,sr) with {a mathematical formula}outMϕ(q1,s)⊨LTL◇winv and show that it cannot be a Nash equilibrium. For the sake of contradiction, suppose s were a Nash equilibrium. Then, first, it must be the case that {a mathematical formula}Tv is true on the path; otherwise, player v can increase its utility by deviating to a consistent strategy that still satisfies {a mathematical formula}◇winv (note that the strategy of r is fixed and memoryless). Second, we show that player r can always deviate to increase its payoff. If {a mathematical formula}Tr holds then r gets a payoff of 1. However, by {a mathematical formula}(⋆) the refuter can deviate to a consistent strategy such that the resulting path satisfies {a mathematical formula}¬◇winv. By doing so it can increase its payoff to 3. On the other hand, if {a mathematical formula}Tr does not hold, then the refuter gets a payoff of 0 and can again deviate to increase its payoff by {a mathematical formula}(⋆), i.e. by deviating to a consistent strategy that satisfies {a mathematical formula}¬◇winv.</list-item><list-item label="(c)">We consider the modified preference list. If ϕ is satisfiable then {a mathematical formula}NE(Mϕ,q0,(γv′,γr′))≠∅ by (b). Now suppose ϕ is false. That is, {a mathematical formula}(⋆) holds. We consider a strategy profile {a mathematical formula}s=(sv,sr) and show that it cannot be a Nash equilibrium. Firstly, suppose {a mathematical formula}outMϕ(q0,(sv,sr))⊨◇winv. Then, it has already been shown in (b) that s cannot be a Nash equilibrium. Thus, we consider the case that {a mathematical formula}λ:=outMϕ(q0,(sv,sr))⊨¬◇winv. We distinguish the four cases which result from players playing consistent or inconsistent strategies. (i) Suppose {a mathematical formula}Tv∧Tr holds on path λ. Then, v is better off by playing an inconsistent strategy resulting in a payoff of 1 rather than 0. (ii) If {a mathematical formula}Tv∧¬Tr is true then r is better of playing a consistent strategy which remains to satisfy {a mathematical formula}¬◇winv. This is possible by {a mathematical formula}(⋆). (iii) {a mathematical formula}¬Tv∧Tr is true on λ. Then, player r can deviate to an inconsistent strategy that still makes {a mathematical formula}¬◇winv true (this can be achieved by playing ⊤ in state {a mathematical formula}q0). This increases the player's payoff from 0 to 2. (iv) Suppose the path satisfies {a mathematical formula}¬Tv∧¬Tr. Then, player v is better off to deviate to a consistent strategy. This shows that there cannot be any Nash equilibrium. □</list-item></list></paragraph><paragraph label="Proof">Hardness of weak implementation: verificationLet{a mathematical formula}(I,q)be a pointed NIS and{a mathematical formula}Na normative system included in{a mathematical formula}I. The problem whether{a mathematical formula}N∈WINE(I,q,f)is{a mathematical formula}Σ2P-hard in the size of{a mathematical formula}M,{a mathematical formula}Nand{a mathematical formula}Prefs.{a mathematical formula}Σ2P-hardness is shown by a reduction to the weak implementability problem of {a mathematical formula}f((γv,γr))=◇winv. Given a QSAT2-formula ϕ in nnf we construct the model {a mathematical formula}Mϕ, which can be done in polynomial time according to Lemma 16, and define {a mathematical formula}I=(Mϕ,{(γv,γr)},{Nϵ}). Now, by Proposition 17(b) we have that: {a mathematical formula}ϕ is satisfiable iff {a mathematical formula}∃s∈NE(Mϕ,q0,(γv,γr)) such that {a mathematical formula}out(q0,s)⊨◇winv. This equivalence, however, is equivalent to {a mathematical formula}Nϵ∈WINE(I,q0,f).  □</paragraph></section></section><section label="B.2">Hardness of strong implementability: Theorem 5<paragraph>The strong implementation problem requires checking two parts: that the set of Nash equilibria is non-empty and that all Nash equilibria satisfy specific properties. We show that the latter one gives rise to {a mathematical formula}Π2P-hardness. In order to be able to use our model {a mathematical formula}Mϕ for a reduction of ∀QSAT2, we need to switch the roles of the verifier and refuter. That is, the refuter controls variables in {a mathematical formula}{x1,…,xm} and the verifier in {a mathematical formula}{xm+1,…,xn}. This revised model denoted by {a mathematical formula}Mˆϕ and will later also be used for the problem of the existence of an appropriate normative system. This reduction requires an additional labelling of some states, modelling the guessing of a normative system.</paragraph><paragraph label="Definition 27">The model {a mathematical formula}MˆϕGiven a ∀QSAT2-formula {a mathematical formula}ϕ≡∀{x1,…,xm}∃{xm+1,…,xn}φ(x1,…,xn) in nnf we define the CGS {a mathematical formula}Mˆϕ analogously to {a mathematical formula}Mϕ but {a mathematical formula}Xr={x1,…,xm} and {a mathematical formula}Xv={xm+1,…,xn}. We further label {a mathematical formula}q0 and each state reachable in an even number of transitions from {a mathematical formula}q0 with a new proposition {a mathematical formula}t.</paragraph><paragraph>The labelling {a mathematical formula}t will later be used for ensuring that the structure of {a mathematical formula}Mˆϕ is not affected by a norm-based update, more precisely that no transitions are regimented making for instance the winning state of the verifier unreachable. The proof of the following lemma is done analogously to the one of Lemma 16.</paragraph><paragraph label="Lemma 19">Given a ∀QSAT2-formula{a mathematical formula}ϕ≡∀{x1,…,xm}∃{xm+1,…,xn}φ(x1,…,xn)in nnf we have that ϕ is satisfiable iff for all strategies{a mathematical formula}srof refuterrwith{a mathematical formula}sr(q0)=⊥there is a strategy{a mathematical formula}svof verifiervsuch that{a mathematical formula}outMˆϕ(q0,(sv,sr))⊨LTLTr→(Tv∧◇winv).</paragraph><paragraph label="Proof">[Sketch] “⇒”: Suppose ϕ is true. For each truth assignment v of {a mathematical formula}Xr let {a mathematical formula}wv denote a truth assignment of {a mathematical formula}Xv such that {a mathematical formula}φ[v∘wv] is valid. Let {a mathematical formula}sr denote the strategy induced by v and let {a mathematical formula}svsr be a consistent strategy induced by {a mathematical formula}wv which witnesses the truth of ϕ. Thus, {a mathematical formula}◇winv must be true on {a mathematical formula}outMˆϕ(q0,(svsr,sr)) otherwise the refuter had a consistent strategy for which there is no consistent strategy {a mathematical formula}svsr such that {a mathematical formula}outMˆϕ(q0,(svsr,sr))⊭LTL◇winv. This would imply that there is an v such that for all {a mathematical formula}wv, {a mathematical formula}φ[v∘wv] is false. Contradiction.“⇐”: Let {a mathematical formula}svsr be a consistent strategy of the verifier v which witnesses the truth of the formulae when the refuter plays the consistent strategy {a mathematical formula}sr, i.e. {a mathematical formula}outMˆϕ(q0,(svsr,sr))⊨LTL◇winv. The strategies induce the truth assignments v and {a mathematical formula}wv, respectively (using the same notation as above). It is straight-forward to check that {a mathematical formula}φ[v∘wv] is true.  □</paragraph><paragraph>The next lemma shows that we can use the previous result to reduce the truth of a ∀QSAT2 instance to a property over all Nash equilibria in {a mathematical formula}Mˆϕ using a slightly modified variant of the preference lists of player v used before. We define{a mathematical formula} and {a mathematical formula}γˆr=γr.</paragraph><paragraph label="Proposition 20">Nash equilibria in {a mathematical formula}MˆϕLet ϕ be a ∀QSAT2-formula in nnf.</paragraph><list><list-item>We have that{a mathematical formula}(sd,sd′)∈NE(Mˆϕ,q0,(γˆv,γˆr))for any strategies{a mathematical formula}sdand{a mathematical formula}sd′with{a mathematical formula}sd(q0)=sd′(q0)=⊤.</list-item><list-item>ϕ is satisfiable iff for all{a mathematical formula}s∈NE(Mˆϕ,q0,(γˆv,γˆr))it holds that{a mathematical formula}out(q0,s)⊨LTL◇winv∨◯d.</list-item></list><paragraph label="Proof"><list><list-item label="(a)">Analogously to Proposition 17(a).</list-item><list-item label="(b)">“⇒”: If ϕ holds then, according to Lemma 19, for all strategies {a mathematical formula}sr with {a mathematical formula}sr(q0)=⊥ there is a strategy {a mathematical formula}sv such that {a mathematical formula}outMˆϕ(q0,(sv,sr)))⊨Tr→(Tv∧◇winv) (⋆). Now, suppose there was a Nash equilibrium {a mathematical formula}s=(sv,sr) with {a mathematical formula}outMˆϕ(q0,(sv,sr))⊨¬◇winv∧¬◯d. We consider the following cases. First, assume that {a mathematical formula}outMˆϕ(q0,(sv,sr))⊨Tr. Then, by {a mathematical formula}(⋆) the verifier could deviate from {a mathematical formula}sv to obtain a path satisfying {a mathematical formula}Tv∧◇winv. This shows that {a mathematical formula}(sv,sr) is not a Nash equilibrium. Second, assume that {a mathematical formula}outMˆϕ(q0,(sv,sr))⊨¬Tr∧Tv. In that case, the refuter would be better off playing a consistent strategy. Thirdly, assume that {a mathematical formula}outMˆϕ(q0,(sv,sr))⊨¬Tr∧¬Tv. In that case the verifier would be better off switching to a consistent strategy. Also note that not both players can play ⊤ in {a mathematical formula}q0 as {a mathematical formula}¬◯d holds. This shows that for all {a mathematical formula}s∈NE(Mˆϕ,q0,(γˆv,γˆr)), {a mathematical formula}outMˆϕ(q0,s)⊨LTL◇winv∨◯d.“⇐”: Suppose ϕ is false. We need to show that there is a Nash equilibrium s such that {a mathematical formula}outMˆϕ(q0,s)⊨LTL¬◇winv∧¬◯d. By Lemma 19 there is a strategy {a mathematical formula}sr with {a mathematical formula}sr(q0)=⊥ such that for all strategies {a mathematical formula}sv we have {a mathematical formula}outMˆϕ(q0,(sv,sr))⊨LTLTr∧(Tv→¬◇winv) (by contraposition). Now, let {a mathematical formula}sv′ be any strategy such that {a mathematical formula}outMˆϕ(q0,(sv′,sr))⊨LTLTv and {a mathematical formula}sv′(q0)=⊥. We show that {a mathematical formula}(sr,sv′)∈NE(Mˆϕ,q0,(γˆv,γˆr)). By Lemma 19, the verifier cannot change its strategy to ensure {a mathematical formula}◇winv. Moreover, as {a mathematical formula}sr(q0)=⊥ the verifier v cannot deviate to increase its payoff. Analogously, as {a mathematical formula}sv′(q0)=⊥, the refuter cannot deviate from {a mathematical formula}sr to increase its payoff. This shows that {a mathematical formula}(sr,sv′) is indeed a Nash equilibrium. Thus we have that there is an {a mathematical formula}s∈NE(Mˆϕ,q0,(γˆv,γˆr)) with {a mathematical formula}outMˆϕ(q0,s)⊨LTL¬◇winv∧¬◯d. □</list-item></list></paragraph><paragraph label="Proof">Hardness of strong implementation: verificationLet{a mathematical formula}(I,q)be a pointed NIS and{a mathematical formula}Na normative system included in{a mathematical formula}I. The problem whether{a mathematical formula}N∈SINE(I,q,f)is{a mathematical formula}Σ2P-hard as well as{a mathematical formula}Π2P-hard in the size of{a mathematical formula}M,{a mathematical formula}Nand{a mathematical formula}Prefs.{a mathematical formula}Σ2P-hardness. By Lemma 16 and Proposition 17(c) we can construct a model {a mathematical formula}Mϕ from a QSAT2-formula ϕ in nnf in polynomial time such that: {a mathematical formula}ϕ is satisfiable iff NE(Mϕ,q0,(γˆv,γˆr))≠∅. This is equivalent to{a mathematical formula} for {a mathematical formula}f((γˆv,γˆr))=⊤.{a mathematical formula}Π2P-hardness. We reduce ∀QSAT2 to the strong implementation problem for {a mathematical formula}f((γˆv,γˆr))=◇winv∨◯d. By Lemma 19 and Proposition 20 we can construct a model {a mathematical formula}Mˆϕ from a ∀QSAT2-formula ϕ in polynomial time such that:{a mathematical formula} □</paragraph></section></section><section label="Appendix C"><section-title>Proofs of implementation problems: existence</section-title><paragraph>In the following we consider the hardness proof of the existence problem stated in Proposition 8. The proof that the problem is {a mathematical formula}Σ3P-hard requires a further technical sophistication. The basic idea is that the normative system is used to simulate the first existential quantification in a QSAT3-formula. Firstly, we show how this can be achieved by using sanctioning norms and afterwards by using regimenting norms.</paragraph><section label="C.1"><section-title>Sanctioning and regimentation norms</section-title><paragraph>For the hardness part we reduce QSAT3. Therefore, we consider the QSAT3-formula{a mathematical formula} in nnf and show that ϕ is true iff {a mathematical formula}SINE(I,q,f)=∅ for appropriate {a mathematical formula}I, q, and f. The idea of the reduction extends the reduction given in the previous section. Essentially, we use the construction {a mathematical formula}Mˆϕˆ from the previous section where {a mathematical formula}ϕˆ is a modified version of the QSAT3-formula ϕ. The refuter r additionally controls the literal states referring to the new variables in {a mathematical formula}{x1,…,xr}. In order to use our previous construction, we define {a mathematical formula}ϕˆ as the formula obtained from ϕ as follows:{a mathematical formula} That is, the first existentially quantified variable are moved to the universal part controlled by the refuter. A sanctioning norm is used to define a truth assignment of the variables {a mathematical formula}x1,…,xr. For this purpose, a sanctioning norm labels states {a mathematical formula}q0′ and {a mathematical formula}qv with new propositions representing the truth assignment. For illustration assume that the sanctioning norm should encode a truth assignment which assigns true to {a mathematical formula}x1,…,xg and false to {a mathematical formula}xg+1,…,xr with {a mathematical formula}1≤g≤r. This can be modelled by the normative system:{a mathematical formula} Then, in the model {a mathematical formula}Mˆϕˆ↾N, states {a mathematical formula}q0′ and {a mathematical formula}qv are additionally labelled with {a mathematical formula}{x1,…,xg,notxg+1,…,notxr}. Given that we consider {a mathematical formula}ϕˆ the consistency constraint {a mathematical formula}Tr of r in {a mathematical formula}Mˆϕˆ has the form:{a mathematical formula} We also define the formula{a mathematical formula} expressing that the next state is either {a mathematical formula}qd, or {a mathematical formula}q0′ or {a mathematical formula}qv each of the two labelled with a representation of a truth assignment for the variables {a mathematical formula}{x1,…,xr}, assuming that the current state is {a mathematical formula}q0. These formulae suffice if we were only given sanctioning norms. In the presence of regimenting norms, however, it has also to be ensured that regimenting norms do not regiment transitions invalidating the structure of the model. For example, the transitions leading to state {a mathematical formula}q⊤ may simply be regimented which makes it impossible for the verifier to win. The idea is to introduce a formula which “forbids” such undesirable normative systems. Therefore, we define the formula{a mathematical formula}</paragraph><paragraph>It describes that a path does not loop in states other than the already looping states {a mathematical formula}qd, {a mathematical formula}q⊤, and {a mathematical formula}q⊥. Now, we can prove the following result:</paragraph><paragraph label="Proof">Let{a mathematical formula}N∈Nrsbe a normative system such that for all paths{a mathematical formula}λ∈ΛMˆϕˆ↾N(q0)it holds that{a mathematical formula}λ⊨tick. Then,{a mathematical formula}Ncannot contain a regimentation norm which is applicable in a state{a mathematical formula}Q\{qd,q⊥,q⊤}.Suppose that an outgoing transition in a state {a mathematical formula}q∈Q\{qd,q⊥,q⊤} is regimented. Then, there is a loop from q to q. Let λ denote the path from {a mathematical formula}q0 to q followed by {a mathematical formula}qω. Clearly, the formula {a mathematical formula}tick is violated on that path which contradicts the assumption.  □</paragraph><paragraph label="Proof">Given a QSAT3-formula{a mathematical formula}ϕ≡∃{x1,…,xr}∀{xr+1,…,xr+m}∃{xr+m+1,…,xn}φ(x1,…,xn)in nnf we have that ϕ is satisfiable iff there is an{a mathematical formula}N∈Nrssuch that for all strategies{a mathematical formula}srofrwith{a mathematical formula}sr(q0)=⊥there is a strategy{a mathematical formula}svofvsuch that{a mathematical formula}outMˆϕˆ↾N(q0,(sv,sr))⊨LTL(Tr→(Tv∧◇winv))and for all paths{a mathematical formula}λ∈ΛMˆϕˆ↾N(q0)we have that{a mathematical formula}λ⊨ass∧tick. Moreover, for the direction “⇒” we can always find a normative system in{a mathematical formula}Ns.“⇒”: Suppose ϕ holds. Let v be a witnessing truth assignment of the variables {a mathematical formula}{x1,…,xr}. Then, {a mathematical formula}ϕ′≡∀{xr+1,…,xr+m}∃{xr+m+1,…xn}φ(x1,…,xn)[v] is satisfiable. By Lemma 19, we have that for all {a mathematical formula}sr with {a mathematical formula}sr(q0)=⊥ there is an {a mathematical formula}sv such that{a mathematical formula} Now let{a mathematical formula} such that {a mathematical formula}vi1=…=vig=t and {a mathematical formula}vig+1=…=vir=f where {a mathematical formula}(i1,…,ir) is a permutation of {a mathematical formula}(1,…,r). Then, we have that for all paths {a mathematical formula}λ∈ΛMˆϕˆ↾N(q0), {a mathematical formula}π(λ)⊨LTLass∧tick. The claim follows by {a mathematical formula}(⋆) applied on {a mathematical formula}φ[v] as the normative system essentially fixes the choices of the refuter for all variables {a mathematical formula}{x1,…,xr}; every deviation of these induced truth values of the refuter would result in an inconsistent strategy.“⇐”: Let {a mathematical formula}N∈Nrs such that for all strategies {a mathematical formula}sr with {a mathematical formula}sr(q0)=⊥ of r there is a strategy {a mathematical formula}sv of v such that {a mathematical formula}outMˆϕˆ↾N(q0,(sv,sr))⊨LTL(Tr→(Tv∧◇winv)) and for all paths {a mathematical formula}λ∈ΛMˆϕˆ↾N(q0) we have that {a mathematical formula}λ⊨ass∧tick. By Lemma 22 no transitions can be regimented apart from those starting in {a mathematical formula}qd, {a mathematical formula}q⊥, and {a mathematical formula}q⊤. The valuation of {a mathematical formula}q0′ and of {a mathematical formula}qv induce truth assignments {a mathematical formula}v1 and {a mathematical formula}v2 of {a mathematical formula}{x1,…,xr}, respectively. We can choose {a mathematical formula}N in such a way that {a mathematical formula}v1=v2. This is the case because for all strategies {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ there is a strategies {a mathematical formula}sv of v such that {a mathematical formula}outMˆϕˆ↾N(q0,(sv,sr))⊨LTL(Tr→(Tv∧◇winv)), we also have that {a mathematical formula}∀{xr+1,…xr+m}∃{xr+m+1,…,xn}φ[vi] is true for {a mathematical formula}i∈{1,2} (cf. Lemma 19). The claim follows.  □</paragraph><paragraph>Now, we can present our reduction to the implementation problem. Again, we need to slightly modify the players' preference lists:{a mathematical formula}</paragraph><paragraph label="Proposition 24">Let ϕ be a QSAT3-formula in nnf.</paragraph><list><list-item>For any{a mathematical formula}N∈Nrs, we have that{a mathematical formula}(sd,sd′)∈NE(Mˆϕˆ↾N,q0,(γˆˆv,γˆˆr))for any strategy{a mathematical formula}sdand{a mathematical formula}sd′with{a mathematical formula}sd(q0)=sd′(q0)=⊤.</list-item><list-item>ϕ is satisfiable iff there is an{a mathematical formula}N∈Nrssuch that for all{a mathematical formula}s∈NE(Mˆϕˆ↾N,q0,(γˆˆv,γˆˆr))with{a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTL(◇winv∨◯d)∧ass∧tick.</list-item><list-item>ϕ is satisfiable iff there is an{a mathematical formula}N∈Nssuch that for all{a mathematical formula}s∈NE(Mˆϕˆ↾N,q0,(γˆˆv,γˆˆr))with{a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTL(◇winv∨◯d)∧ass∧tick.</list-item></list><paragraph label="Proof"><list><list-item label="(a)">The action profile {a mathematical formula}(sd,sd′) yields the path {a mathematical formula}q0qdω which satisfies {a mathematical formula}◯d if the transition {a mathematical formula}(⊤,⊤) is not regimented, and the path {a mathematical formula}q0ω which satisfies {a mathematical formula}¬tick if it is regimented. On both paths the payoff for both players is maximal. No player can improve its payoff.</list-item><list-item label="(b)">“⇒”: Suppose ϕ holds. By Lemma 23 there is an {a mathematical formula}N∈Ns such that for all strategies {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ there is a strategy {a mathematical formula}sv of v such that {a mathematical formula}outMˆϕˆ↾N(q0,(sv,sr))⊨LTLTr→(Tv∧◇winv) and for all paths {a mathematical formula}λ∈ΛMˆϕˆ↾N(q0) we have that {a mathematical formula}λ⊨ass∧tick. Now suppose that there was a Nash equilibrium {a mathematical formula}(sv,sr) such that {a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTLass∧tick→(¬◇winv∧¬◯d). As {a mathematical formula}ass∧tick is true on all paths, this means that {a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTL(¬◇winv∧¬◯d) for this Nash equilibrium. We can use the same reasoning as in the proof of Proposition 20(b) to obtain a contradiction. Hence, such a Nash equilibrium cannot exist.“⇐”: Suppose ϕ does not hold. We have to show that for all {a mathematical formula}N∈Nrs there is an {a mathematical formula}s∈NE(Mˆϕˆ↾N,q0,(γˆˆv,γˆˆr)) such that {a mathematical formula}(⋆){a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTL(ass∧tick)→(¬◇winv∧¬◯d).Firstly, suppose that {a mathematical formula}Mˆϕˆ↾N contains a path {a mathematical formula}λ∈ΛMˆϕˆ↾N(q0) with {a mathematical formula}λ⊨¬tick. This path can be generated by some strategy profile s which satisfies {a mathematical formula}(⋆), as the antecedent of {a mathematical formula}(⋆) will be false. As this strategy profile gives maximal utility it is a Nash equilibrium. Thus, from now on we can assume that all paths in the norm updated model satisfy {a mathematical formula}tick. Completely analogous, we can also assume that {a mathematical formula}ass is satisfied in the updated models.Secondly, consider {a mathematical formula}Mˆϕˆ↾N such that all paths {a mathematical formula}λ∈ΛMˆϕˆ↾N(q0) satisfy {a mathematical formula}tick∧ass. As ϕ does not hold, by Lemma 23 and our assumption about the updated models we have that for all {a mathematical formula}N∈Nrs there is a strategy {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ such that for all strategies {a mathematical formula}sv of v, {a mathematical formula}outMˆϕˆ↾N(q0,(sv,sr))⊨LTLTr∧(Tv→¬◇winv). The rest of the proof follows analogously to Proposition 20(b): let {a mathematical formula}sv′ be any strategy such that {a mathematical formula}outMˆϕˆ↾N(q0,(sv′,sr))⊨LTLTv and {a mathematical formula}sv′(q0)=⊥. By Lemma 23, the verifier cannot change its strategy to ensure {a mathematical formula}◇winv. Moreover, as {a mathematical formula}sr(q0)=⊥ the verifier v cannot deviate to increase its payoff. Analogously, as {a mathematical formula}sv′(q0)=⊥, the refuter cannot deviate from {a mathematical formula}sr to increase its payoff. This shows that {a mathematical formula}(sr,sv′) is indeed a Nash equilibrium. Thus we have that there is an {a mathematical formula}s∈NE(Mˆϕˆ↾N,q0,(γˆˆv,γˆˆr)) with {a mathematical formula}outMˆϕˆ↾N(q0,s)⊨LTL¬◇winv∧¬◯d.</list-item><list-item label="(c)">Follows immediately from (b) and Lemma 23. □</list-item></list></paragraph><paragraph label="Proof">Hardness strong implementation: existence, sanctioningLet{a mathematical formula}(I,q)be a pointed NIS and{a mathematical formula}N∈{Nrs,Ns}. The problem whether{a mathematical formula}SINE(I,q,f)≠∅is{a mathematical formula}Σ3P-hard.First, let {a mathematical formula}N=Nrs and ϕ be a QSAT3 formula in nnf. We have for {a mathematical formula}f(γˆˆv,γˆˆr)=(◇winv∨◯d)∧ass∧tick:{a mathematical formula} The case {a mathematical formula}N=Ns follows analogously using Proposition 24(c) in the first step.  □</paragraph></section><section label="C.2"><section-title>Regimentation norms</section-title><paragraph>Finally, we consider the case in which {a mathematical formula}N∈Nr. We have already seen how to ensure that regimenting norms do not regiment specific transitions, by means of the formula {a mathematical formula}tick. However, we can no longer use the previous construction, based on sanctioning norms, to encode a truth assignment. We have to find a way to achieve this with regimenting norms. The idea of this construction consists of two parts:</paragraph><list><list-item label="1.">Use regimenting norms to simulate truth assignments of variables {a mathematical formula}x1 to {a mathematical formula}xr by removing some of the outgoing transitions of states in {a mathematical formula}{q1,…,qr}.</list-item><list-item label="2.">Ensure that no other transition is regimented by using a formula which characterizes the structure of {a mathematical formula}Mˆϕˆ.</list-item></list><paragraph label="Definition 28">The model {a mathematical formula}MˆrϕˆLet {a mathematical formula}ϕ≡∃{x1,…,xr}∀{xr+1,…,xr+m}∃{xr+m+1,…,xn}φ(x1,…,xn) in nnf be given. We define {a mathematical formula}Mˆrϕˆ as {a mathematical formula}Mˆϕˆ but each state from {a mathematical formula}{q1,q1⊥,q1⊤,…,qr,qr⊥,qr⊤} is additionally labelled by a fresh proposition {a mathematical formula}rr.</paragraph><paragraph>For part 1, we observe that the model contains only the three looping states {a mathematical formula}qd, {a mathematical formula}q⊤, and {a mathematical formula}q⊥. We introduce the formulae{a mathematical formula}</paragraph><paragraph>As before we use {a mathematical formula}tickr to ensure that no regimentation norm is imposed on any state except from possibly {a mathematical formula}{q1,…,qr}. The idea is that some of these transitions in the set may be regimented. A second formula {a mathematical formula}validr is true on a path on which no transition from states in {a mathematical formula}{q1,…,qr} which are also contained on the path are regimented. Finally, {a mathematical formula}tick represents that no transition on a path is regimented.</paragraph><paragraph label="Lemma 26">Let{a mathematical formula}N∈Nrsbe a normative system such that for all paths{a mathematical formula}λ∈ΛMˆϕˆ↾N(q0)it holds that{a mathematical formula}λ⊨tickr. Then,{a mathematical formula}Ncannot contain a regimentation norm which is applicable in a state of{a mathematical formula}Q\{qd,q⊥,q⊤,q1,q1⊥,q1⊤,…,qr,qr⊥,qr⊤}.</paragraph><paragraph label="Proof">Given a QSAT3-formula{a mathematical formula}ϕ≡∃{x1,…,xr}∀{xr+1,…,xr+m}∃{xr+m+1,…,xn}φ(x1,…,xn)in nnf we have that ϕ is satisfiable iff there is an{a mathematical formula}N∈Nrsuch that in{a mathematical formula}Mˆrϕˆ↾Nthere is a path from{a mathematical formula}q0to{a mathematical formula}qr+1(or to{a mathematical formula}qϵif{a mathematical formula}r=n), such that for all strategies{a mathematical formula}srofrwith{a mathematical formula}sr(q0)=⊥there is a strategy{a mathematical formula}svofvsuch that{a mathematical formula}outMˆrϕˆ↾N(q0,(sv,sr))⊨LTL(Tr∧validr)→(Tv∧◇winv), and for all paths{a mathematical formula}λ∈ΛMˆrϕˆ↾N(q0)we have that{a mathematical formula}λ⊨tickr.[Sketch] “⇒”: Suppose ϕ is satisfiable. Let v be a witnessing valuation of the variables {a mathematical formula}{x1,…,xr}. We consider the normative system {a mathematical formula}N which regiments from {a mathematical formula}qi, {a mathematical formula}1≤i≤r, the transition {a mathematical formula}(−,⊤) (resp. {a mathematical formula}(−,⊥)) if {a mathematical formula}v(xi)=f (resp. {a mathematical formula}v(xi)=t). Now the truth assignment of {a mathematical formula}{xr+1,…,xr+m,…,xn} induces witnessing strategies following the same reasoning as in Lemma 19. It is also the case that {a mathematical formula}tickr is true on all paths starting in {a mathematical formula}q0.“⇐”: Suppose there is an {a mathematical formula}N∈Nr such that for all strategies {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ there is a strategy {a mathematical formula}sv of v such that {a mathematical formula}outMˆrϕˆ↾N(q0,(sv,sr))⊨LTL(Tr∧validr)→(Tv∧◇winv), and for all paths {a mathematical formula}λ∈ΛMˆrϕˆ↾N(q0) we have that {a mathematical formula}λ⊨tickr and there is a path from {a mathematical formula}q0 to {a mathematical formula}qr+1 (or to {a mathematical formula}qϵ if {a mathematical formula}r=n).By Lemma 26 the transitions of all states in which {a mathematical formula}rr does not hold are not affected by the update by the normative system. We can assume wlog that {a mathematical formula}N regiments exactly one outgoing transition for each of the states in {a mathematical formula}{q1,…,qr} (it cannot regiment both transitions due to the connectivity property). This is so, because otherwise we move the variable {a mathematical formula}xi, {a mathematical formula}1≤i≤r, of which no transition is regimented to the universally quantified part of ϕ. Thus, the normative system {a mathematical formula}N induces a truth assignment v of the variables {a mathematical formula}{x1,…,xr} as follows: {a mathematical formula}v(xi)=t (resp. {a mathematical formula}v(xi)=f) iff transition {a mathematical formula}(−,⊥) (resp. {a mathematical formula}(−,⊤)) is regimented in {a mathematical formula}qi. Now, we can apply a reasoning similar to that of Lemma 19 wrt. {a mathematical formula}ϕˆ[v] and the model {a mathematical formula}Mˆrϕˆ[v]↾N. We get that {a mathematical formula}ϕˆ[v] is satisfiable. The normative systems gives a witnessing truth assignment v of the variables {a mathematical formula}{x1,…,xr} showing that ϕ is satisfiable.  □</paragraph><paragraph>In the previous result it was crucial to assume that the normative system does not regiment both of the transitions outgoing of some state {a mathematical formula}q1,…,qr. In the following we have to ensure that normative systems which do not respect this condition, yield some “bad” Nash equilibrium. Therefore, we define the following preference lists:{a mathematical formula} Now we are able to show the following result:</paragraph><paragraph label="Proposition 28">Let ϕ be a QSAT3-formula in nnf and{a mathematical formula}N∈Nr.</paragraph><list><list-item>We have that{a mathematical formula}(sd,sd′)∈NE(Mˆrϕˆ↾N,q0,(γv⁎,γr⁎))for any strategy{a mathematical formula}sdand{a mathematical formula}sd′with{a mathematical formula}sd(q0)=sd′(q0)=⊤.</list-item><list-item>ϕ is satisfiable iff there is an{a mathematical formula}N∈Nrsuch that for all{a mathematical formula}s∈NE(Mˆrϕˆ↾N,q0,(γv⁎,γr⁎))we have that{a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTL(◇winv∨◯d)∧tick.</list-item></list><paragraph label="Proof"><list><list-item label="(a)">Either we end up in path {a mathematical formula}q0qdω or in {a mathematical formula}q0ω. The former satisfies {a mathematical formula}tick∧◯d, the latter satisfying {a mathematical formula}¬tick.</list-item><list-item label="(b)">“⇒”: Suppose ϕ is satisfiable. Then, we can apply Lemma 27. That is, let {a mathematical formula}N∈Nr such that in {a mathematical formula}Mˆrϕˆ↾N there is a path from {a mathematical formula}q0 to {a mathematical formula}qr+1 (or to {a mathematical formula}qϵ if {a mathematical formula}r=n), for all strategies {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ there is a strategy {a mathematical formula}sv of v such that {a mathematical formula}outMˆrϕˆ↾N(q0,(sv,sr)⊨LTL(Tr∧validr)→(Tv∧◇winv) and for all paths {a mathematical formula}λ∈ΛMˆrϕˆ↾N(q0) we have that {a mathematical formula}λ⊨tickr. Now suppose there were a Nash equilibrium {a mathematical formula}s=(sv,sr) with {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTLtick→(¬◇winv∧¬◯d).First, assume that {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊭LTLtick. Then, we also have {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊭LTLtickr by Lemma 27 and the fact that {a mathematical formula}validr∧tickr→tick holds on the path. Thus, the refuter would be better of to deviate to a strategy {a mathematical formula}sr′ such that {a mathematical formula}outMˆrϕˆ↾N(q0,(sv,sr′))⊨Tr∧validr. Second, assume that {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTLtick. Then, also {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTLvalidr. If {a mathematical formula}sv(q0)=⊤ then the refuter can increase its payoff by also deviating to {a mathematical formula}sr(q0)=⊤; so, let us suppose that {a mathematical formula}sv(q0)=⊥. We can also assume that the verifier plays a consistent strategy, otherwise it would deviate to one. In that case we can also assume that {a mathematical formula}sr is consistent; otherwise, the refuter can again deviate to a consistent strategy to increase its payoff. Then, however, by Lemma 27 the verifier can deviate to a better strategy that satisfies {a mathematical formula}◇winv. If {a mathematical formula}sr(q0)=⊤ then we can also assume that {a mathematical formula}sv(q0)=⊤; otherwise, the verifier would deviate to such a strategy. This contradicts the existence of a Nash equilibrium with the above stated properties.“⇐”: Suppose ϕ does not hold. We show that for all {a mathematical formula}N∈Nr there is an {a mathematical formula}s∈NE(Mˆrϕˆ↾N,q0,(γv⁎,γr⁎)) with {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTLtick→(¬◇winv∧¬◯d). We suppose, for the sake of contradiction, that this is not the case, i.e. that {a mathematical formula}(⋆) for all {a mathematical formula}s∈NE(Mˆrϕˆ↾N,q0,(γv⁎,γr⁎)) we have that {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTL(◇winv∨◯d)∧tick.As ϕ does not hold, we have according to Lemma 27 that for all {a mathematical formula}N∈Nr it holds that:<list>there is no path from {a mathematical formula}q0 to {a mathematical formula}qr+1 (or to {a mathematical formula}qϵ if {a mathematical formula}r=n) in {a mathematical formula}Mˆrϕˆ↾N;there is a path {a mathematical formula}λ∈ΛMˆrϕˆ↾N(q0) such that {a mathematical formula}λ⊭tickr; orthere is a strategy {a mathematical formula}sr of r with {a mathematical formula}sr(q0)=⊥ such that for all strategies {a mathematical formula}sv of v it holds that {a mathematical formula}(⋆){a mathematical formula}outMˆrϕˆ↾N(q0,(sv,sr))⊨LTL(Tr∧validr)∧(Tv→¬◇winv).First, suppose that (i) holds. Consider the strategies </list><paragraph>{a mathematical formula}sr and {a mathematical formula}sv with {a mathematical formula}sr(q0)=sv(q0)=⊥ which end in the looping state in-between {a mathematical formula}q0 and {a mathematical formula}qr+1 (or to {a mathematical formula}qϵ if {a mathematical formula}r=n) which has to exist by assumption. We have that {a mathematical formula}outMˆrϕˆ↾N(q0,s)⊨LTL¬tick∧¬validr. Thus, {a mathematical formula}(sv,sr) is a Nash equilibrium which contradicts {a mathematical formula}(⋆).Secondly, suppose that (ii). As for (i) suppose both players play {a mathematical formula}sr and {a mathematical formula}sv with {a mathematical formula}sr(q0)=sv(q0)=⊥ which yield the very path λ with {a mathematical formula}λ⊭tickr. On this path it holds that {a mathematical formula}¬tick and {a mathematical formula}¬tickr, thus no player has an incentive to deviate from it.Thirdly, suppose that (iii) holds. Let {a mathematical formula}sr be a strategy as defined above, and {a mathematical formula}sv such that it is consistent and {a mathematical formula}sv(q0)=⊥. By Lemma 27 the verifier cannot change its strategy to a consistent one which ensures {a mathematical formula}◇winv. The player would also not deviate to an inconsistent one. Moreover, no player can deviate to ensure a path with {a mathematical formula}◯d. We can also assume that (i) and (ii) do not hold. Thus, neither the verifier nor the refuter can deviate to a strategy such that the outcome path satisfies {a mathematical formula}¬tick nor {a mathematical formula}¬tickr, respectively. Thus, this strategy profile is a Nash equilibrium, contradicting {a mathematical formula}(⋆). □</paragraph></list-item></list></paragraph><paragraph label="Proof">Hardness of strong implementation: existence, regimentation normsLet{a mathematical formula}(I,q)be a pointed NIS and{a mathematical formula}N∈{Nr}. The problem whether{a mathematical formula}SINE(I,q,f)≠∅is{a mathematical formula}Σ3P-hard.Let ϕ be a QSAT3 formula in nnf. We have for {a mathematical formula}f(γv⁎,γr⁎)=(◇winv∨◯d)∧tick:{a mathematical formula} □</paragraph></section></section></appendices><references><reference label="[1]"><authors>T. Ågotnes,W. van der Hoek,M. Wooldridge</authors><title>Normative system games</title><host>Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent SystemsAAMAS '07(2007)ACMNew York, NY, USA pp.1-8</host></reference><reference label="[2]"><authors>T. Ågotnes,W. van der Hoek,M. Woolridge</authors><title>Robust normative systems and a logic of norm compliance</title><host>Log. J. IGPL18 (2010) pp.4-30</host></reference><reference label="[3]"><authors>T. Ågotnes,M. Wooldridge</authors><title>Optimal social laws</title><host>Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1AAMAS '10(2010)International Foundation for Autonomous Agents and Multiagent SystemsRichland, SC pp.667-674</host><host>http://dl.acm.org/citation.cfm?id=1838206.1838294</host></reference><reference label="[4]">H. Aldewereld,V. DignumOperetta: organization-oriented development environmentLanguages, Methodologies, and Development Tools for Multi-Agent Systems – Third International WorkshopLADS 2010, Lyon, France, August 30–September 1(2010) pp.1-18revised selected papers</reference><reference label="[5]"><authors>H. Aldewereld,J. Vázquez-Salceda,F. Dignum,J.C. Meyer</authors><title>Verifying norm compliancy of protocols</title><host>O. BoissierJ.A. PadgetV. DignumG. LindemannE.T. MatsonS. OssowskiJ.S. SichmanJ. Vázquez-SalcedaCoordination, Organizations, Institutions, and Norms in Multi-Agent SystemsLecture Notes in Computer Sciencevol. 3913 (2006)Springer pp.231-24510.1007/11775331_16</host></reference><reference label="[6]"><authors>N. Alechina,M. Dastani,B. Logan</authors><title>Programming norm-aware agents</title><host>Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 12, vol. 2(2012) pp.1057-1064</host></reference><reference label="[7]"><authors>N. Alechina,M. Dastani,B. Logan</authors><title>Reasoning about normative update</title><host>Proceedings of the Twenty Third International Joint Conference on Artificial IntelligenceIJCAI 2013(2013)AAAI Press pp.20-26</host></reference><reference label="[8]"><authors>N. Alechina,M. Dastani,B. Logan</authors><title>Norm approximation for imperfect monitors</title><host>Proceedings of the 13th International Conference on Autonomous Agents and Multiagent SystemsAAMAS 2014(2014)</host></reference><reference label="[9]"><authors>R. Alur,T.A. Henzinger,O. Kupferman</authors><title>Alternating-time temporal logic</title><host>J. ACM49 (2002) pp.672-713</host></reference><reference label="[10]"><authors>F. Arbab</authors><title>Abstract behavior types: a foundation model for components and their composition</title><host>F. de BoerM. BonsangueS. GrafW.-P. de RoeverFormal Methods for Components and ObjectsLNCSvol. 2852 (2003)Springer-Verlag pp.33-70</host></reference><reference label="[11]"><authors>L. Astefanoaei,M. Dastani,J.-J.C. Meyer,F. Boer</authors><title>On the semantics and verification of normative multi-agent systems</title><host>Int. J. Univers. Comput. Sci.15 (13)(2009) pp.2629-2652</host></reference><reference label="[12]"><authors>J. Baumfalk,B. Poot,B. Testerink,M. Dastani</authors><title>A sumo extension for norm based traffic control systems</title><host>Proceedings of the SUMO2015 Intermodal Simulation for Intermodal Transport(2015)DLRBerlin, Adlershof pp.63-83</host></reference><reference label="[13]"><authors>P. Blackburn,J. Bos,K. Striegnitz</authors><title>Learn Prolog Now!</title><host>Texts in Computingvol. 7 (2006)College Publications</host></reference><reference label="[14]"><authors>G. Boella,L. van der Torre</authors><title>Regulative and constitutive norms in normative multiagent systems</title><host>Proceedings of the Ninth International Conference on Principles of Knowledge Representation and ReasoningKR'04(2004) pp.255-266</host></reference><reference label="[15]"><authors>G. Boella,L.W.N. van der Torre</authors><title>Enforceable social laws</title><host>The Fourth International Joint Conference on Autonomous Agents and Multiagent SystemsAMAAS 2005(2005) pp.682-689</host></reference><reference label="[16]"><authors>O. Boissier,R. Bordini,J. Hübner,A. Ricci,A. Santi</authors><title>Multi-agent oriented programming with jacamo</title><host>Sci. Comput. Program78 (6)(2011) pp.747-761</host></reference><reference label="[17]"><authors>F. Brazier,C. Jonker,J. Treur</authors><title>Compositional design and reuse of a generic agent model</title><host>Appl. Artif. Intell. J.14 (2000) pp.491-538</host></reference><reference label="[18]"><authors>N. Bulling,M. Dastani</authors><title>Normative mechanism design (extended abstract)</title><host>Proceedings of the 10th International Conference on Autonomous Agents and Multi-Agent SystemsAAMAS 2011(May 2011)ACM PressTaipei, Taiwan pp.1187-1188</host></reference><reference label="[19]"><authors>N. Bulling,M. Dastani</authors><title>Verification and implementation of normative behaviours in multi-agent systems</title><host>Proc. of the 22nd Int. Joint Conf. on Artificial IntelligenceIJCAI, Barcelona, Spain(July 2011) pp.103-108</host></reference><reference label="[20]"><authors>N. Bulling,M. Dastani,M. Knobbout</authors><title>Monitoring norm violations in multi-agent systems</title><host>Twelfth International Conference on Autonomous Agents and Multi-Agent SystemsAAMAS'13(2013) pp.491-498</host></reference><reference label="[21]"><authors>N. Bulling,J. Dix</authors><title>Modelling and verifying coalitions using argumentation and ATL</title><host>Intel. Artif.14 (46)(March 2010) pp.45-73</host></reference><reference label="[22]"><authors>N. Bulling,W. Jamroga</authors><title>Verifying agents with memory is harder than it seemed</title><host>AI Commun.23 (4)(December 2010) pp.389-403</host></reference><reference label="[23]"><authors>N. Bulling,W. Jamroga,J. Dix</authors><title>Reasoning about temporal properties of rational play</title><host>Ann. Math. Artif. Intell.53 (1–4)(2009) pp.51-114</host></reference><reference label="[24]"><authors>H.L. Cardoso,E. Oliveira</authors><title>Electronic institutions for b2b: dynamic normative environments</title><host>Artif. Intell. Law16 (1)(2008) pp.107-128</host></reference><reference label="[25]"><authors>M. Comuzzi,I. Vanderfeesten,T. Wang</authors><title>Optimized cross-organizational business process monitoring: design and enactment</title><host>Inf. Sci.244 (2013) pp.107-118</host></reference><reference label="[26]"><authors>N. Criado,E. Argente,V. Botti</authors><title>Open issues for normative multi-agent systems</title><host>AI Commun.24 (3)(2011) pp.233-264</host></reference><reference label="[27]"><authors>M. Dastani,J.-J.C. Meyer,D. Grossi</authors><title>A logic for normative multi-agent programs</title><host>J. Log. Comput.23 (2)(2013) pp.335-354</host></reference><reference label="[28]"><authors>M. Dastani,N. Tinnemeier,J.-C. Meyer</authors><title>A programming language for normative multi-agent systems</title><host>V. DignumMulti-Agent Systems: Semantics and Dynamics of Organizational Models(2009)Information Science Reference</host></reference><reference label="[29]"><authors>F. Dignum</authors><title>Autonomous agents with norms</title><host>Artif. Intell. Law7 (1)(1999) pp.69-7910.1023/A%3A1008315530323</host></reference><reference label="[30]"><authors>U. Endriss,S. Kraus,J. Lang,M. Wooldridge</authors><title>Designing incentives for boolean games</title><host>AAMAS(2011) pp.79-86</host></reference><reference label="[31]"><authors>M. Esteva,D. de la Cruz,C. Sierra</authors><title>ISLANDER: an electronic institutions editor</title><host>Proceedings of the First International Joint Conference on Autonomous Agents and MultiAgent SystemsAAMAS 2002, Bologna, Italy(2002) pp.1045-1052</host></reference><reference label="[32]"><authors>M. Esteva,J. Rodríguez-Aguilar,B. Rosell,J. Arcos</authors><title>AMELI: an agent-based middleware for electronic institutions</title><host>Proceedings of AAMAS 2004New York, US(July 2004) pp.236-243</host></reference><reference label="[33]">M. Esteva,J. Rodriguez-Aguilar,C. Sierra,W. VasconcelosVerifying norm consistency in electronic institutionsV. DignumD. CorkillC. JonkerF. DignumProceedings of the AAAI-04 Workshop on Agent Organizations: Theory and PracticeAAAI(July 2004)AAAI PressSan JoseTechnical Report WS-04-02</reference><reference label="[34]"><authors>D. Fitoussi,M. Tennenholtz</authors><title>Choosing social laws for multi-agent systems: minimality and simplicity</title><host>Artif. Intell.119 (1)(2000) pp.61-101</host></reference><reference label="[35]"><authors>N. Fornara,M. Colombetti</authors><title>Specifying and enforcing norms in artificial institutions</title><host>Proc. of DALT'08(2009)</host></reference><reference label="[36]"><authors>A. Garcia-Camino,P. Noriega,J.A. Rodriguez-Aguilar</authors><title>Implementing norms in electronic institutions</title><host>Proceedings of the Fourth International Joint Conference on Autonomous Agents and MultiAgent SystemsAAMAS'05, New York, NY, USA(2005) pp.667-673</host></reference><reference label="[37]"><authors>G. Gottlob,G. Greco,F. Scarcello</authors><title>Pure Nash equilibria: hard and easy games</title><host>J. Artif. Intell. Res. (2003) pp.215-230</host></reference><reference label="[38]"><authors>H. Guo</authors><title>Automotive Informatics and Communicative Systems: Principles in Vehicular Networks and Data Exchange</title><host>(2009)Information Science Reference – Imprint of IGI PublishingHershey, PA</host></reference><reference label="[39]"><authors>J. Hintikka</authors><title>Logic, Language Games and Information</title><host>(1973)Clarendon PressOxford</host></reference><reference label="[40]"><authors>R. Horowitz,P. Varaiya</authors><title>Control design of an automated highway system</title><host>Special Issue on Hybrid SystemsProc. IEEE88 (7)(2000) pp.913-925</host></reference><reference label="[41]"><authors>J. Hübner,O. Boissier,R. Kitio,A. Ricci</authors><title>Instrumenting multi-agent organisations with organisational artifacts and agents: giving the organisational power back to the agents</title><host>Int. J. Auton. Agents Multi-Agent Syst.20 (2010) pp.369-400</host></reference><reference label="[42]"><authors>J. Hübner,J. Sichman,O. Boissier</authors><title>S–MOISE+: a middleware for developing organised multi-agent systems</title><host>Proceedings of the International Workshop on Coordination, Organizations, Institutions, and Norms in Multi-Agent SystemsLNCSvol. 3913 (2006)Springer pp.64-78</host></reference><reference label="[43]"><authors>J. Hübner,J. Sichman,O. Boissier</authors><title>Developing organised multiagent systems using the MOISE+ model: programming issues at the system and agent levels</title><host>Int. J. Agent-Oriented Softw. Eng.1 (3/4)(2007) pp.370-395</host></reference><reference label="[44]"><authors>J.F. Hübner,O. Boissier,R.H. Bordini</authors><title>From organisation specification to normative programming in multi-agent organisations</title><host>J. DixJ. LeiteG. GovernatoriW. JamrogaProceedings of 11th International Workshop on Computational Logic in Multi-Agent SystemsCLIMA XI, Lisbon, Portugal, August 16–17, 2010Lecture Notes in Computer Sciencevol. 6245 (2010)Springer pp.117-134</host></reference><reference label="[45]"><authors>A.J.I. Jones,M. Sergot</authors><title>On the characterization of law and computer systems</title><host>J.-J.C. MeyerR. WieringaDeontic Logic in Computer Science: Normative System Specification(1993)John Wiley &amp; Sons pp.275-307</host></reference><reference label="[46]"><authors>D. Krajzewicz,J. Erdmann,M. Behrisch,L. Bieker</authors><title>Recent development and applications of sumo-simulation of urban mobility</title><host>Int. J. Adv. Syst. Meas.5 (3&amp;4)(2012)</host></reference><reference label="[47]"><authors>K. Mahbub,G. Spanoudakis</authors><title>A framework for requirements monitoring of service based systems</title><host>Proceedings of the 2nd International Conference on Service Oriented ComputingICSOC '04(2004)ACMNew York, NY, USA pp.84-93</host></reference><reference label="[48]"><authors>F.R. Meneguzzi,M. Luck</authors><title>Norm-based behaviour modification in BDI agents</title><host>C. SierraC. CastelfranchiK.S. DeckerJ.S. Sichman8th International Joint Conference on Autonomous Agents and Multiagent SystemsAAMAS 2009(2009)IFAAMAS pp.177-184</host></reference><reference label="[49]"><authors>A. Metzger,P. Leitner,D. Ivanovic,E. Schmieders,R. Franklin,M. Carro,S. Dustdar,K. Pohl</authors><title>Comparing and combining predictive business process monitoring techniques</title><host>IEEE Trans. Syst. Man Cybern.45 (2)(2014) pp.276-290</host></reference><reference label="[50]"><authors>J. Meyer,R. Wieringa</authors><title>Deontic Logic in Computer Science: Normative System Specification</title><host>Wiley Professional Computing (1993)J. Wiley</host></reference><reference label="[51]"><authors>N.H. Minsky,V. Ungureanu</authors><title>Law-governed interaction: a coordination and control mechanism for heterogeneous distributed systems</title><host>ACM Trans. Softw. Eng. Methodol.9 (3)(2000)</host></reference><reference label="[52]"><authors>J. Morales,M. Lopez-Sanchez,J.A. Rodriguez-Aguilar,M. Wooldridge,W. Vasconcelos</authors><title>Minimality and simplicity in the on-line automated synthesis of normative systems</title><host>Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent SystemsAAMAS '14(2014)International Foundation for Autonomous Agents and Multiagent SystemsRichland, SC pp.109-116</host></reference><reference label="[53]"><authors>J. Morales,M. López-Sánchez,J.A. Rodríguez-Aguilar,M. Wooldridge,W.W. Vasconcelos</authors><title>Automated synthesis of normative systems</title><host>International Conference on Autonomous Agents and Multi-Agent SystemsAAMAS '13, Saint Paul, MN, USA, May 6–10(2013) pp.483-490</host></reference><reference label="[54]"><authors>M. Osborne,A. Rubinstein</authors><title>A Course in Game Theory</title><host>(1994)MIT Press</host></reference><reference label="[55]"><authors>C. Papadimitriou</authors><title>Computational Complexity</title><host>(1994)Addison WesleyReading</host></reference><reference label="[56]"><authors>A. Pnueli</authors><title>The temporal logic of programs</title><host>Proceedings of Foundations of Computer ScienceFOCS(1977) pp.46-57</host></reference><reference label="[57]"><authors>A. Ricci,M. Viroli,A. Omicini</authors><title>Give agents their artifacts: the A&amp;A approach for engineering working environments in MAS</title><host>E.H. DurfeeM. YokooM.N. HuhnsO. Shehory6th International Joint Conference on Autonomous Agents and Multiagent SystemsAAMAS 2007(2007)IFAAMAS</host></reference><reference label="[58]"><authors>Y. Shoham,K. Leyton-Brown</authors><title>Multiagent Systems – Algorithmic, Game-Theoretic, and Logical Foundations</title><host>(2009)Cambridge University Press</host></reference><reference label="[59]"><authors>Y. Shoham,M. Tennenholtz</authors><title>On the synthesis of useful social laws for artificial agent societies</title><host>Proceedings of the Tenth National Conference on Artificial IntelligenceAAAI-92, San Diego, CA(1992)</host></reference><reference label="[60]"><authors>Y. Shoham,M. Tennenholtz</authors><title>On social laws for artificial agent societies: off-line design</title><host>Artif. Intell.73 (1–2)(1995) pp.231-252</host></reference><reference label="[61]"><authors>V.T. Silva</authors><title>From the specification to the implementation of norms: an automatic approach to generate rules from norms to govern the behavior of agents</title><host>Int. J. Auton. Agents Multiagent Syst.17 (1)(2008) pp.113-155</host></reference><reference label="[62]"><authors>M.P. Singh,M. Arrott,T. Balke,A.K. Chopra,R. Christiaanse,S. Cranefield,F. Dignum,D. Eynard,E. Farcas,N. Fornara,F. Gandon,G. Governatori,H.K. Dam,J. Hulstijn,I. Krueger,H.-P. Lam,M. Meisinger,P. Noriega,B.T.R. Savarimuthu,K. Tadanki,H. Verhagen,S. Villata</authors><title>The uses of norms</title><host>G. AndrighettoG. GovernatoriP. NoriegaL.W.N. van der TorreNormative Multi-Agent SystemsDagstuhl Follow-Upsvol. 4 (2013)Schloss Dagstuhl–Leibniz-Zentrum fuer InformatikDagstuhl, Germany pp.191-229</host></reference><reference label="[63]"><authors>I. Sommerville,D. Cliff,R. Calinescu,J. Keen,T. Kelly,M. Kwiatkowska,J. Mcdermid,R. Paige</authors><title>Large-scale complex it systems</title><host>Commun. ACM55 (7)(2012) pp.71-77</host><host>http://doi.acm.org/10.1145/2209249.2209268</host></reference><reference label="[64]"><authors>N. Tinnemeier,M. Dastani,J.-J.C. Meyer,L. van der Torre</authors><title>Programming normative artifacts with declarative obligations and prohibitions</title><host>Proceedings of IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology(2009)IEEE Computer Society pp.145-152</host></reference><reference label="[65]"><authors>W. van der Hoek,M. Roberts,M. Wooldridge</authors><title>Social laws in alternating time: effectiveness, feasibility, and synthesis</title><host>Synthese156 (1)(2007) pp.1-19</host></reference><reference label="[66]"><authors>M.B. van Riemsdijk,K.V. Hindriks,C.M. Jonker</authors><title>Programming organization-aware agents: a research agenda</title><host>Proceedings of the Tenth International Workshop on Engineering Societies in the Agents' WorldESAW'09LNAIvol. 5881 (2009)Springer pp.98-112</host></reference><reference label="[67]"><authors>K.W. Wagner</authors><title>Bounded query classes</title><host>SIAM J. Comput.19 (5)(1990) pp.833-846</host></reference><reference label="[68]"><authors>M. Wooldridge</authors><title>An Introduction to Multiagent Systems</title><host>2nd edition(2009)WileyChichester, UK</host></reference><reference label="[69]"><authors>M. Wooldridge,U. Endriss,S. Kraus,J. Lang</authors><title>Incentive engineering for boolean games</title><host>Artif. Intell.195 (2013) pp.418-439</host><host>http://www.sciencedirect.com/science/article/pii/S0004370212001518</host></reference><reference label="[70]"><authors>F. Zambonelli,N. Jennings,M. Wooldridge</authors><title>Organizational abstractions in the analysis and design of multi-agent systems</title><host>First International Workshop on Agent-Oriented Software Engineering at ICSE(2000)</host></reference></references><footnote><note-para label="1">http://oopluu.sourceforge.net/.</note-para><note-para label="2">We note that the framework allows to model turn-based games as a special case.</note-para><note-para label="3">We assume some implicit ordering among the agents to obtain a unique representative of a strategy profile.</note-para><note-para label="4">We could also give a semantics over paths in a given model. We use ω-sequences over sets of propositions as it makes the semantics independent of the structure of a specific model which shall prove useful when relating the setting to mechanism design in Section 3.</note-para><note-para label="5">We note that this is a rather weak definition of dominance, other definitions use a strict preference relation {a mathematical formula}a≻ia′, defined as {a mathematical formula}a⪰ia′ and not {a mathematical formula}a′⪰ia.</note-para><note-para label="6">Each of two agents shows one side of a coin. One agent wins if both coins show the same side. The other wins if this is not the case. The matching pennies game can, e.g., also be found in [54].</note-para><note-para label="7">Sometimes, it is assumed that all outcomes specified by {a mathematical formula}f(⪰) can be obtained by some equilibrium. It is also common to consider a social choice function that maps to single outcomes rather than a social choice rule.</note-para><note-para label="8">We emphasize its definition as a function. When considering the set of paths satisfying a LTL-formula rather than the formula itself, however, it shows the characteristics of a social choice rule.</note-para><note-para label="9">More formally, they are isomorphic in the sense of Definition 24. Reachability is formally introduced in Definition 23.</note-para><note-para label="10">In particular, we assume that the amount of energy demanded and supplied can be measured qualitatively expressing, e.g., ‘low’, ‘medium’, and ‘high’ demand.</note-para><note-para label="11">We note that fines and incentives can be measured quantitatively, assuming that the set of possible numerical values is finite and fixed in advance. We stress that the purpose of this example is to illustrate the conceptual idea of this approach rather than giving a full-fledged real-world modelling of smart grids.</note-para><note-para label="12">Thus, _ plays the same role as ⋆ in the context of CGS. We use Prolog's notation here to emphasize that we are working in a program setting.</note-para><note-para label="13">We assume that {a mathematical formula}v1 and {a mathematical formula}v2 are taken from some sensible finite set of velocity values.</note-para><note-para label="14">We stop at the literal level as it simplifies our construction.</note-para></footnote></root>