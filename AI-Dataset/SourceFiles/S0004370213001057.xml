<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370213001057</url><title>CROSS cyclic resource-constrained scheduling solver</title><authors>Alessio Bonfietti,Michele Lombardi,Luca Benini,Michela Milano</authors><abstract>Cyclic scheduling problems consist in ordering a set of activities executed indefinitely over time in a periodic fashion, subject to precedence and resource constraints. This class of problems has many applications in manufacturing, embedded systems and compiler design, production and chemical systems. This paper proposes a Constraint Programming approach for cyclic scheduling problems, based on modular arithmetic: in particular, we introduce a modular precedence constraint and a global cumulative constraint along with their filtering algorithms. We discuss two possible formulations. The first one (referred to as CROSS) models a pure cyclic scheduling problem and makes use of both our novel constraints. The second formulation (referred to as CROSS⁎) introduces a restrictive assumption to enable the use of classical resources constraints, but may incur a loss of solution quality. Many traditional approaches to cyclic scheduling operate by fixing the period value and then solving a linear problem in a generate-and-test fashion. Conversely, our technique is based on a non-linear model and tackles the problem as a whole: the period value is inferred from the scheduling decisions. Our approach has been tested on a number of non-trivial synthetic instances and on a set of realistic industrial instances. The method proved to effective in finding high quality solutions in a very short amount of time.</abstract><keywords>Cyclic scheduling problem;Cumulative constraint;Filtering algorithm;Constraint programming</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Cyclic scheduling problems arise in a number of application areas, such as in hoist scheduling [10], mass production [19], [13], flow shop [30], [31], cyclic job-shop [8], [9], compiler design (optimizing loops on parallel architectures) [27], [19], software pipelining [35], and data-flow computing [22]. Optimal cyclic schedulers are lately in great demand, as stream computing paradigms are gaining momentum across a wide spectrum of computing platforms, ranging from multi-media encoders and decoders in mobile and consumer devices, to advanced packet processing in network appliances, and high-quality rendering in game consoles. In stream computing, an application can be abstracted as a set of tasks that have to be performed on incoming items (called coding units, packets, pixels, depending on the context) of a data stream. A typical example is video decoding, where a compressed video stream has to be expanded and rendered. Since video compression exploits temporal correlation between successive frames, decoding is not a pure process-and-forward activity (because the computation on the current frame depends on the previously decoded frame). These dependencies must be taken into account in the scheduling model. In embedded computing contexts, resource constraints (computational units and buffer storage) imposed by the underlying hardware platforms are of great importance. In addition, the computational effort which can be spent to compute an optimal schedule is often limited by cost and time-to-market considerations.</paragraph><paragraph>From a combinatorial optimization standpoint, cyclic scheduling is the problem of assigning start times to a set of activities (indefinitely repeated over time) such that the average repetition time for the overall set is minimal, i.e. the execution frequency is maximal. From a practical point of view, one is typically interested in finding a periodic schedule, where the same pattern of start times is repeated every λ time units. The value λ is the schedule period.</paragraph><paragraph>Traditional constraint-based scheduling techniques have achieved a good level of maturity in the last decade [3]. They can be applied to cyclic scheduling problems, but the adaptation techniques defined so far have failed to produce state-of-the-art results. In particular, the related literature is focused on two approaches:</paragraph><list><list-item label="•">The blocked scheduling approach [5] builds a schedule for a single repetition of the set of activities. The schedule is then repeated after the last activity is over, so that consecutive repetitions do not overlap. As a side effect, however, blocked scheduling may fail to make optimal use of the available resources, with possibly large decrease of solution quality.</list-item><list-item label="•">The unfolding approach [33] schedules a number of consecutive repetitions of the application. The schedule is then repeated after the last activity is over, like in blocked scheduling. Unfolding allows to exploit inter-repetition overlaps, but at the cost of an increased instance size. Moreover, determining the right number of repetitions is a non-trivial problem.</list-item></list><paragraph> On the other hand, a number of approaches investigate the so-called modulo scheduling. This technique builds a schedule for a single repetition, which is repeated every λ time units, where λ is called modulus and is in fact the schedule period. Hence, the modulo scheduling approach builds a periodic schedule, with no loss of optimality. Existing modulo scheduling techniques operate by fixing the modulus and searching for a feasible schedule. In this process, inter-repetition overlaps are taken into account, so that dealing with resource constraint becomes more complicated. The smallest modulus with a feasible schedule is the optimal solution.</paragraph><paragraph>In this paper we introduce a novel Constraint Programming approach based on modular arithmetic to compute resource-constrained cyclic schedules with minimum period. In our approach, the modulus is a decision variable, linked via global constraints to the other decision variables in the model (i.e. the activity start times). As a consequence, we gain efficiency with respect to other modulo scheduling approaches and solution quality with respect to the blocked and the unfolding approaches. To obtain these gains however, we cannot reuse constraint based techniques, but we have to define extensions to cope with modularity and periodic schedules.</paragraph><paragraph>In particular, we have developed novel constraints to model temporal and resource restrictions in a cyclic context. Namely, we have introduced the Modular Precedence Constraint (MPC) and the Global Cyclic Cumulative Constraint (GCCC), and we have devised efficient filtering algorithms for both of them. The new constraints are the foundation for the constraint model employed within CROSS (Cyclic Resource cOnstrained Scheduling Solver). The solver makes use of a novel random restart search strategy, where we set the upper bound of the modulus variable while the lower bound is inferred from the scheduling decisions. This is in contrast with classical modular approaches that fix the modulus and solve the corresponding scheduling sub-problem.</paragraph><paragraph>As another contribution, we have proposed a restricted approach (called {a mathematical formula}CROSS⁎), where we simplify the model by using traditional resource constraints instead of the GCCC. This comes at a cost of a limitation: namely, we make the assumption that the end times of all the activities should be assigned within the modulus. As a consequence, the approach may lose optimality.</paragraph><paragraph>We have performed an extensive experimental evaluation on a number of non-trivial synthetic instances and on a set of industrial instances. We have compared our approach with a state-of-the art ILP (Integer Linear Programming) scheduler and the Swing Modulo Scheduling heuristic technique (SMS). SMS is a non-constraint-based (heuristic) modular approach adopted in the gcc compiler [18]. CROSS (and its restricted version {a mathematical formula}CROSS⁎) proved to be able to deal effectively with temporal and resource constraints and to compute very high quality solutions in a short time. The experiments also show that, for some instance classes, our technique outperforms both the blocked and the unfolding approaches in terms of search speed and solution quality.</paragraph><paragraph>The paper is structured as follows: in Section 2 we formally define the considered problem, while in Section 3 we describe all related works highlighting the differences w.r.t. our approach. Section 4 formally describes the modular formulation adopted in this work. In Sections 5 and 6 we respectively describe the model with the novel constraints and their filtering algorithms. Section 7 discusses two search strategies and the restricted version of the solver, and formally defines a dominance rule used to narrow the search space. Experimental results are reported in Section 8 and Section 9 concludes the paper.</paragraph></section><section label="2"><section-title>The problem</section-title><paragraph>The problem we consider is the Cyclic Resource-Constrained Scheduling Problem (CRCSP). As a main difference with traditional scheduling, the set of activities is repeated indefinitely (in practice, a very large number of times). In the following, we formally define the concepts needed to address the CRCSP and we provide some intuitions. The problem is defined over a Project Graph:</paragraph><paragraph label="Definition 1">A Project Graph {a mathematical formula}G is a directed (possibly cyclic) graph consisting of a pair {a mathematical formula}〈V,A〉, where:</paragraph><list><list-item label="•">elements in {a mathematical formula}V ({a mathematical formula}‖V‖=n) are nodes that represent activities;</list-item><list-item label="•">elements in {a mathematical formula}A ({a mathematical formula}‖A‖=m) are arcs (formally described in Section 2.2). An arc {a mathematical formula}(i,j) represents a temporal dependency between activities i and j.</list-item></list><paragraph>We assume that every activity {a mathematical formula}i∈V in the graph {a mathematical formula}G has a fixed duration {a mathematical formula}di. We refer to {a mathematical formula}(i,ω) as the ω-th execution of activity {a mathematical formula}i∈V, where {a mathematical formula}ω∈Z is called execution number. A set of executions {a mathematical formula}(i,ω) of all the activities in {a mathematical formula}V with the same ω value is referred to as a repetition. A schedule is defined as an assignment of start times to all executions {a mathematical formula}(i,ω). We refer to {a mathematical formula}start(i,ω) as the starting time of activity i at execution ω. Without loss of generality, we also assume that {a mathematical formula}start(i,ω)⩾start(i,ω′) if {a mathematical formula}ω⩾ω′ and that {a mathematical formula}start(i,ω)⩾0.</paragraph><paragraph>The most important performance metric of a cyclic schedule is the average inter-execution distance, which is strictly related to the concept of execution frequency (i.e. the throughput).</paragraph><paragraph label="Definition 2">The average inter-execution distance {a mathematical formula}λ(i) of an activity i is defined as the following limit:{a mathematical formula}</paragraph><paragraph>Note that, since the activities are repeated indefinitely, the sum can start for {a mathematical formula}ω=0 without loss of generality, even if ω is in {a mathematical formula}Z as from the problem definition.</paragraph><paragraph label="Definition 3">The average inter-execution distance λ of a set of activities {a mathematical formula}V is the worst case {a mathematical formula}λ(i).{a mathematical formula}</paragraph><paragraph>The throughput of an activity i is defined as the average number of executions of i per time unit, and corresponds to the inverse of the average distance:{a mathematical formula} Analogously, the throughput of a set of activities is the inverse of λ. Lower period (i.e. higher throughput) values are to be preferred since they correspond to more efficient schedules.</paragraph><paragraph>Note that a schedule has infinite size in principle, because each activity executes an infinite number of times. However, since building an infinite schedule is impossible in practice, we should find a way to compute a more compact problem solution. Typically, one wants to build a periodic schedule, i.e. a schedule where activities are executed regularly with a fixed period. Such an approach requires to specify only a start time for each activity, plus the period value. Periodic schedules will be formally defined in Section 2.3.</paragraph><paragraph>In practical cases, activities are subject to several restrictions. In particular, there may be resource constraints and temporal dependencies, described in the following sections.</paragraph><section label="2.1"><section-title>Resource constraints</section-title><paragraph>In this work, we consider activities requiring different amounts of a set R of limited capacity resources. Each resource {a mathematical formula}k∈R has maximum capacity {a mathematical formula}CAPk and each activity {a mathematical formula}i∈V has a set of non-negative requirements {a mathematical formula}ri,k, one per resource. A zero requirement denotes a non-required resource. A schedule is feasible iff, for each {a mathematical formula}k∈R, at any point in time t the sum of the requirements {a mathematical formula}ri,k does not exceed the capacity {a mathematical formula}CAPk of the resource. Formally:{a mathematical formula}</paragraph></section><section label="2.2"><section-title>Time constraints</section-title><paragraph>Temporal dependencies between activities corresponds to arcs in the project graph and may connect executions with different ω number.</paragraph><paragraph label="Definition 4">Each arc {a mathematical formula}(i,j)∈A is a tuple {a mathematical formula}〈i,j,θ(i,j),δ(i,j)〉, where:</paragraph><list><list-item label="•">i is the source activity;</list-item><list-item label="•">j is the sink activity;</list-item><list-item label="•">{a mathematical formula}θ(i,j)∈R is called minimum time lag;</list-item><list-item label="•">{a mathematical formula}δ(i,j)∈Z is called the repetition distance;</list-item></list><paragraph>Observe that, in a cyclic problem, a temporal dependency connects an infinite number of distinct pairs of executions of i and j. The value {a mathematical formula}δ(i,j) acts as a repetition offset: it declares the distance in terms of repetitions between the executions of the connected activities. If {a mathematical formula}δ(i,j)=0, the edge {a mathematical formula}〈i,j,θ(i,j),0〉 is called intra-repetition edge while if {a mathematical formula}δ(i,j)≠0, then {a mathematical formula}〈i,j,θ(i,j),δ(i,j)〉 is an inter-repetition edge. The time lag {a mathematical formula}θ(i,j) specifies the minimal temporal distance between the start of execution {a mathematical formula}(j,ω) and the end of execution {a mathematical formula}(i,ω−δ(i,j)).</paragraph><paragraph>The presence of inter-repetition dependencies may create feasible cycles in the project graph. Consider the following simple example about building a skyscraper, depicted in Fig. 1. The project graph contains the two activities floor and pillars (having respectively duration 5 and 2) and two arcs ({a mathematical formula}〈floor,pillars,2,0〉 and {a mathematical formula}〈pillars,floor,1,1〉). Both arcs have a positive time lag. Activity floor represents the act of building the floor frame and activity {a mathematical formula}pillars represents the edification of the pillars for the next floor. The first temporal dependency ensures that the pillars are built after the floor completion, while the second one conveys that without the pillars of the previous floor a new floor frame cannot be built. Note that {a mathematical formula}〈pillars,floor,1,1〉 is an inter-repetition dependency.</paragraph><paragraph>Based on Eq. (4), we can formally define both the dependencies:{a mathematical formula}{a mathematical formula} We now proceed by providing a numeric example to clarify the mechanics of the temporal dependencies. Since a cyclic schedule is infinite, we can choose for convenience a reference repetition and start time. Fixing the start time of one activity does not compromise completeness since {a mathematical formula}ω∈Z. Specifically, let us assume that {a mathematical formula}start(floor,0)=0 (i.e. the workers immediately start to build the ground floor). The following equations and Fig. 2 show how the start times of different executions are (lower) bounded by temporal dependencies:{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula}</paragraph></section><section label="2.3"><section-title>Periodic schedule</section-title><paragraph>As stated in Section 2, a way to compute a compact solution to the problem is to consider schedules where the activities are executed regularly with a fixed period (i.e. periodic schedules). Note that in this case the fixed period (called iteration period) is the same as the maximum average inter-execution distance λ from Definition 3. Formally:</paragraph><paragraph label="Definition 5">A periodic schedule is pair {a mathematical formula}〈L,λ〉 where λ is the iteration period and {a mathematical formula}L is a vector containing the start times for the execution 0 of all the activities, i.e.:{a mathematical formula} A periodic schedule obeys the following restriction:{a mathematical formula}</paragraph><paragraph>The iteration period of the schedule is defined as the distance between the start times of two consecutive executions of the same activity. We use the notation λ, since for a periodic schedule the iteration period is the same as the maximum average inter-execution distance. This can be checked by combining Eqs. (1) and (2) from Section 2 with Eq. (7). Finally, the makespan for a periodic schedule is defined as the distance between the start of the first activity and the end of the last activity in every repetition. For this reason, it will be referred in the following as schedule length. To avoid confusion, we will use the same terminology also for other, non-strictly periodic, approaches.</paragraph><section label="2.3.1"><section-title>Iteration bound</section-title><paragraph>Let {a mathematical formula}C be the set of all cycles of a project graph {a mathematical formula}G, where a cycle is defined in the usual fashion as a sequence of precedence connected activities whose last element is also connected to the first one. Let {a mathematical formula}V(c) and {a mathematical formula}A(c) respectively be the set of activities and arcs that belong to a cycle {a mathematical formula}c∈C.</paragraph><paragraph label="Definition 6">The cycle bound {a mathematical formula}CB(c) of a cycle c is the minimum time required in a periodic schedule to execute all the activities in c. This is equal to:{a mathematical formula} where:</paragraph><list><list-item label="•">{a mathematical formula}T(c)=∑i∈V(c)di is the sum of the durations of the activities in {a mathematical formula}V(c).</list-item><list-item label="•">{a mathematical formula}D(c)=∑(i,j)∈A(c)δ(i,j) is the sum of the repetition distances of arcs in {a mathematical formula}A(c).</list-item></list><paragraph>Note that the sum of the repetition distances must be strictly positive {a mathematical formula}D(c)&gt;0, otherwise no feasible schedule exists. This can be intuitively checked since {a mathematical formula}CB(c)→∞ as {a mathematical formula}D(c)→0: more details and formal proofs can be found in [17]. As a consequence, at least an arc in the cycle must have a positive δ. A graph with {a mathematical formula}D(c)&gt;0∀c∈C is called deadlock-free.</paragraph><paragraph label="Definition 7">The Iteration Bound IB of a graph is the maximum of the cycle bounds:{a mathematical formula}</paragraph><paragraph>The iteration bound is related to the concept of critical path in traditional scheduling and can be computed in polynomial time with the algorithm presented in [21]. The iteration bound IB is an intrinsic lower bound on the iteration period: we can never achieve a λ less than IB, even if unlimited resources are available. Periodic schedules are said to be periodically optimal if the iteration period λ is the same as the iteration bound IB. If there are no resource restrictions, then a periodically optimal schedule is guaranteed to exist. This is not true once we add resource constraints to the problem. Further details can be found in [12], [16].</paragraph></section></section><section label="2.4"><section-title>Problem definition</section-title><paragraph>We can now define the CRCSP as follows. Given:</paragraph><list><list-item label="•">a Project Graph {a mathematical formula}G=〈V,A〉 with:</list-item><list-item label="•">A set R of limited capacity resources, where each resource {a mathematical formula}k∈R has capacity {a mathematical formula}CAPk.</list-item><list-item label="•">A fixed duration {a mathematical formula}di for each activity.</list-item><list-item label="•">A resource requirement {a mathematical formula}ri,k⩾0 for each activity i and resource k.</list-item></list><paragraph> The CRCSP consists in finding a periodic schedule: i.e. an iteration period λ and a feasible assignment of {a mathematical formula}start(i,0){a mathematical formula}∀i∈V such that all dependencies are consistent, no resource capacity is exceeded and the iteration period λ is minimized.</paragraph></section><section label="2.5"><section-title>Overlapped schedules</section-title><paragraph>We end this section with a couple of useful definitions. Cyclic schedules can be either non-overlapped or overlapped.</paragraph><list><list-item label="•">A schedule is said to be non-overlapped if repetition {a mathematical formula}ω+1 starts only once repetition ω is over. Formally:{a mathematical formula}</list-item><list-item label="•">A schedule is overlapped if Eq. (8) does not hold, i.e.:{a mathematical formula}</list-item></list></section></section><section label="3"><section-title>Related works</section-title><paragraph>The cyclic scheduling literature mainly arises in industrial and computing contexts. While there is a considerable body of work on cyclic scheduling in the OR literature [1], the problem has not received much attention from the AI community ([13] is one of the few related papers).</paragraph><paragraph>Three main approaches have been proposed for the CRCSP. The first belongs to the non-overlapped class, while the other two exploit inter-repetition overlaps:</paragraph><list><list-item label="•">The so-called blocked scheduling approach (see Section 3.1) builds a schedule for a single repetition and assumes that the period is equal to the schedule length. As a consequence, consecutive repetitions are not allowed to overlap, with an obvious loss in terms of solution quality.</list-item><list-item label="•">The unfolding approach (see Section 3.2) schedules a number of consecutive repetitions and then repeats the block similarly to the previous technique. Unfolding often leads to higher quality schedules, but it also requires to solve problem instances with artificially increased size.</list-item><list-item label="•">The modulo scheduling approach (see Section 3.3) schedules a single repetition, which is however repeated every λ time units. The value λ is called the modulus and it is the same as the iteration period. By exploiting repetition overlaps, the modulus can be made much smaller than the schedule length, obtaining considerable speed-ups.</list-item></list><paragraph>Fig. 3 depicts a simple example of a CRCSP. All minimum time lags {a mathematical formula}θi,j are assumed to be 0 and the repetition distance {a mathematical formula}δi,j is 0 whenever not explicitly mentioned. In the following subsections we use this instance to show how the three approaches described above work.</paragraph><section label="3.1"><section-title>Blocked scheduling</section-title><paragraph>Traditional static resource-constrained scheduling techniques for cyclic problems are non-overlapped [24], [7]. These methods optimize the performance of a single repetition of the project graph and then repeat the schedule periodically.</paragraph><paragraph>Fig. 4 shows the optimal blocked schedule for the simple problem described in Fig. 3. The output of the solution approach is the schedule (or block) identified by the black arrows, which is then repeated every iteration period {a mathematical formula}λ=27. Hence, the throughput is one over the length of the schedule {a mathematical formula}Thp=1λ=0.037. The horizontal dotted line represents the resource capacity: note that this schedule leaves most of the resource idle. With blocked schedules, this may happen quite frequently.</paragraph></section><section label="3.2"><section-title>Unfolded scheduling</section-title><paragraph>Overlapped schedules exploit the repetitive nature of periodic schedules to achieve higher throughput. On this purpose, they take into account inter-repetition dependencies in addition to intra-repetition ones. The unfolding technique, presented in [32] and [33], consists in scheduling u consecutive repetitions of the graph, where u is referred to as unfolding factor (or as blocking factor). Then, the resulting schedule (say with total length L) is treated as a normal blocked schedule and repeated every L time units. Note that the schedule length acts as a sort of period, but since u repetitions are considered, the actual average inter-repetition distance is {a mathematical formula}λ=Lu. In fact, u executions are completed in L time units.</paragraph><paragraph>Fig. 5, Fig. 6 show the optimal unfolded schedule for the simple problem depicted in Fig. 3 with unfolding factor {a mathematical formula}u=2 and {a mathematical formula}u=3 respectively (with {a mathematical formula}u=1 we obtain the blocked schedule from Fig. 4). The schedule is restarted after L time units: for {a mathematical formula}u=2 the optimal L is 35 and {a mathematical formula}λ=17.5, while for {a mathematical formula}u=3, {a mathematical formula}L=47 and {a mathematical formula}λ=15.6. The throughput is higher than in the blocked schedule, but the problem size (and consequently the search space) is bigger because it consists of u different repetitions. Because the problem is NP-hard [25], multiplying the number of activities by the unfolding factor leads to an exponential increase in the solution time.</paragraph><paragraph>Since the group of repetitions in the schedule is repeated in a blocked fashion, the unfolding approach may be incapable to optimally exploit inter-repetition overlaps to make the best use of the available resources. Moreover, despite increasing u from 2 to 3 led to a lower λ value in our example, it is not true in general that increasing the unfolding factor leads to better schedules. The optimal unfolding factor cannot be trivially computed.</paragraph><paragraph>On the other hand, it is known [19] that periodic schedules are dominated by K-periodic schedules (i.e. periodic schedules for sequences of K iterations) in the presence of finite capacity resources. Since the unfolding technique takes into account multiple iterations, in some cases an unfolded schedule may be better than a (single-iteration) periodic schedule. Hence, no strict dominance exists between the two approaches. An in-depth survey of traditional and unfolding cyclic scheduling techniques can be found in [5].</paragraph></section><section label="3.3"><section-title>Modulo scheduling</section-title><paragraph>The modulo scheduling method consists in finding a schedule for a single repetition, plus a modulus value λ (usually lower than the whole schedule length). The schedule is repeated every λ time units and the activities of consecutive repetitions, like in a pipelined system, overlap over time. This technique is the best in exploiting the available resources. Fig. 7 shows the optimal modular schedule for the graph of Fig. 3. The figure shows that, after an initial transient phase, the execution reaches a periodic phase where a repetition of the graph is completed every {a mathematical formula}λ=10 (corresponding to a throughput {a mathematical formula}1λ=0.1).</paragraph><paragraph>Note that a time window of length λ (evidenced with the black arrows) contains the start time of exactly one execution of each activity. The activities may however appear with different execution numbers (i.e. ω values), following a well defined pattern. Such a collection of activity executions is called an iteration and is a fundamental concept for our method.</paragraph><paragraph>Fig. 8a depicts the schedule subpart corresponding to a specific iteration starting at a multiple of λ (in the periodic phase). In the picture, with {a mathematical formula}xω we refer to the ω-th execution of activity x, i.e. to {a mathematical formula}(x,ω). Note that an iteration contains a single execution of each activity and that not all the activities appear with the same ω values. For example, {a mathematical formula}(C,ω+1) and {a mathematical formula}(A,ω+2) are executing concurrently with {a mathematical formula}(D,ω), i.e. the first execution of activity D runs together with the second execution of C and the third execution of A.</paragraph><paragraph>One of the key ideas of this paper is to focus on scheduling a single iteration (Fig. 8a) instead of a single repetition with a large horizon (Fig. 8b). In this context, it is convenient to restrict to iterations starting at a multiple of the period λ, so that we can refer as the ω-th iteration to the iteration starting at {a mathematical formula}ω⋅λ. Further details will be given in Section 5.</paragraph><section label="3.3.1"><section-title>Specific modulo scheduling techniques</section-title><paragraph>OR approaches An advanced ILP formulation for the modulo scheduling approach has been proposed in [14] by Dupont de Dinechin and is based on a time-indexed model. A recurring issue for this kind of approach is that, in periodic problems, the schedule length (which determines the number of problem variables) may be fairly big compared to the period. In an attempt to circumvent this difficulty, a second formulation was proposed in [15] by Eichenberger and Davidson (by exploiting a decomposition of start times) at the cost of a reduction in the quality of the LP bound. An excellent overview of state-of-the-art ILP formulations is given in [2], where the authors present also a new model based on the Danzig-Wolfe Decomposition. Another good overview of complete methods can be found in [19] while McCormick et al., in [31], study the complexity in cyclic scheduling. All the mentioned approaches are based on iteratively solving resource constrained subproblems obtained by fixing the period value. To the best of our knowledge, this is a common trait of all the state-of-the-art approaches in the OR field. The main reason is that fixing λ allows to solve the Resource Constrained Cyclic Scheduling Problem via an integer linear program, while modeling λ as an explicit decision variable yields non-linear models.</paragraph><paragraph>A constraint programming approach In [13] the authors present a formulation for solving cyclic job shop scheduling as a Constraint Satisfaction Problem. They also describe how their formulation could be generalized to face problems with cumulative resources. The approach they propose is again based on fixing the period and solving the derived subproblems. The obvious drawback is that a resource constrained scheduling problem needs to be repeatedly solved for different λ values to obtain the feasible/optimal solution. Our method is also based on Constraint Programming, but it does not require to fix a λ value, thanks to the use of a global constraint to model resource restrictions.</paragraph><paragraph>Incomplete approaches Several heuristic approaches have been proposed to find the smallest possible λ. These are usually instruction scheduling techniques that are used by several current compilers.{sup:1} The Iterative Modulo Scheduling algorithm is presented in [35]: the main feature of the algorithm is its iterative nature, in the sense that each activity can be scheduled and unscheduled (therefore considering backtrack operations) several times before a suitable slot is found. Another heuristic approach (called SCAN) is presented in [6]. SCAN is built on some of the main ideas behind Iterative Modulo Scheduling, but it is based on an ILP model. In [20] Huff presented the slack modulo scheduling algorithm. Key of this method is the use of a bidirectional scheduling approach{sup:2} (top-down and bottom-up) and the use of slacks. The slack of an unbound activity is a measure of the freedom that the activity would have if it was scheduled in the partial solution. Another interesting approach is described in [8], where Brucker and Kampmeyer apply tabu-search to cyclic job-shop scheduling problems. The state of the art for incomplete methods is probably Swing Modulo Scheduling, described in [26], [27]. The approach is implemented in the optimization chain of the gcc compiler [18]. The method produces effective schedules with a low computational cost (see [11] for a well structured comparison between most of these modulo scheduling approaches). Key for the efficiency is a valid variable selection function considering both the period bound imposed by the current iteration and the criticality of the path to which the activity to be scheduled belongs to.</paragraph><paragraph>Those heuristic approaches compute a schedule for a single repetition of the application. The schedule is characterized by its length and by an initiation interval, which is the same as the iteration bound and determines the throughput. However, the schedule length can be extremely large, with implications on the size of the model in case of ILP based approaches (e.g. SCAN). Our model is considerably more compact, since we schedule a single iteration as opposed to a repetition, so that we restrict to a time window with length λ.</paragraph></section></section></section><section label="4"><section-title>Modular representation for cyclic schedules</section-title><paragraph>In this section, we recall some modular arithmetic notions that are the foundations of the cyclic scheduling solver we propose. The main underlying idea is to focus on a λ-width time window in the periodic phase (see Fig. 7). First, we present a start/end time decomposition similar to that in [15]. The start time of execution 0 of activity i (i.e. {a mathematical formula}start(i,0)) can be expressed as:{a mathematical formula} where {a mathematical formula}si is a value in the half-open interval {a mathematical formula}[0,λ[ and {a mathematical formula}βi is an integer number. In practice, {a mathematical formula}βi identifies the iteration when activity i is first scheduled (see Section 3.3) and {a mathematical formula}si is its relative start time within the corresponding λ-width time window, i.e. its modular start time. Analogously, the end time {a mathematical formula}end(i,0) can be decomposed into a modular end time{a mathematical formula}ei and an iteration number {a mathematical formula}ηi.{a mathematical formula}</paragraph><paragraph>Fig. 9 shows the optimal modulo scheduling related to the graph from Fig. 3 described in Section 2. Note that activity C is longer than the period λ, hence the activity begins in a period and ends in the following one. For instance, the third execution of C (i.e. {a mathematical formula}C″) starts at instant {a mathematical formula}start(i,3)=30 and terminates at 42. Using the modular representation the activity has {a mathematical formula}si=0, {a mathematical formula}βi=3 and {a mathematical formula}ei=2, {a mathematical formula}ηi=4. Hence it starts at {a mathematical formula}start(i,3)=0+3⋅λ and ends at {a mathematical formula}2+4⋅λ. Since {a mathematical formula}λ=10 we have {a mathematical formula}si+βi⋅λ=30 and {a mathematical formula}ei+ηi⋅λ=42.</paragraph><paragraph>Note that there is a strong correlation between iteration numbers and schedule length. In particular, a larger difference between the highest and smallest {a mathematical formula}βi in an iteration corresponds to a larger schedule length. Moreover, start and end times are constrained by the relation {a mathematical formula}end(i,0)=start(i,0)+di, hence we have {a mathematical formula}ei+ηi⋅λ=si+βi⋅λ+di and hence:{a mathematical formula}</paragraph><paragraph>Moreover, since {a mathematical formula}ei−si is strictly less than λ, we have {a mathematical formula}ηi−βi=⌊diλ⌋, which means that {a mathematical formula}ηi is unambiguously determined once {a mathematical formula}βi and λ are known. We can also rewrite the temporal dependencies using the modular formulation. In particular, the relation:{a mathematical formula} is rewritten as:{a mathematical formula}{a mathematical formula} performing the usual eliminations we have the following inequality that no longer depends on ω:{a mathematical formula} This is a very important result and it is equivalent to say that in a periodic schedule, if the (modular) precedence constraints are satisfied for iteration 0, then they are satisfied for every other iteration.</paragraph><section label="4.1"><section-title>Resource modeling</section-title><paragraph>Before introducing our constraint model it is necessary to get a better insight into the resource usage profile of a periodic schedule. The most unusual peculiarity of this case is that the value of λ may modify the resource usage over time, as depicted in Fig. 10. The picture presents the profiles corresponding to schedules with the same start times, but with 4 different period values:</paragraph><list><list-item label="•">case a) represents a classic non-overlapped schedule.</list-item><list-item label="•">In b) the modular start time s is equal to the modular end time e, since the period is equal to the length of the activity.</list-item><list-item label="•">In case c) the period is shorter than the activity, hence the schedule is overlapped and the resource profile exhibits a “pulse” in the middle.</list-item><list-item label="•">In d) the period is slightly larger than the half of the activity duration. In this profile the modular end time e precedes the modular start time s, which is a bit counterintuitive.</list-item></list><paragraph> Note that changes in the modulus affect not only the resource usage but also the modular end time {a mathematical formula}ei. In general, in [28] we showed that the amount of resource k required by activity i in a time window having width λ is given by the following expression:</paragraph><paragraph>{a mathematical formula} In other words, the resource usage is given by a constant factor {a mathematical formula}ri,k⋅⌊diλ⌋, plus an additional {a mathematical formula}ri,k in case the considered time point lies “between” the modular start and the modular end (the quotes are used since we may have {a mathematical formula}ei&lt;si). In Fig. 10, the constant usage factor is always 1 except for a), where it is 0. As a particular case, if {a mathematical formula}βi=ηi the constant usage factor is zero and {a mathematical formula}rqi,k(si,di,t,λ) becomes a classical resource usage function. This suggests that forcing the end times to be within the modulus allows the use of classical resource constraints (see Section 5).</paragraph></section></section><section label="5"><section-title>Constraint-based model</section-title><paragraph>In this section we propose a complete constraint-based approach for the cyclic scheduling problem, based on modular arithmetic. Our CP model features three classes of variables, representing the (modular) start times, the corresponding iteration numbers and the modulus. The modular start time variables have domain {a mathematical formula}[0,MAX_TIME[, the iteration numbers are in {a mathematical formula}{−‖V‖..+‖V‖} and the modulus is in {a mathematical formula}]IB,MAX_TIME].{sup:3} We recall that {a mathematical formula}‖V‖ is the number of nodes in the graph. The value {a mathematical formula}MAX_TIME instead is given by the sum of the durations of the activities and the sum of the time lags of the edges.</paragraph><paragraph>For sake of brevity, in the rest of the paper we will use the notation {a mathematical formula}si,{a mathematical formula}ei, {a mathematical formula}βi, {a mathematical formula}ηi and λ to refer to variables rather than values. Moreover, we will speak of “start/end times”, implicitly meaning their modular counterparts: non-modular start/end times are never used in the model. In the rest of the paper we also assume that:</paragraph><list><list-item label="•">{a mathematical formula}ESTi is the earliest start time of i: the minimum in the domain of {a mathematical formula}si;</list-item><list-item label="•">{a mathematical formula}LSTi is the latest start time of i: the maximum in the domain of {a mathematical formula}si;</list-item><list-item label="•">{a mathematical formula}EETi is the earliest end time of i: the minimum in the domain of {a mathematical formula}ei;</list-item><list-item label="•">{a mathematical formula}LETi is the latest end time of i: the maximum in the domain of {a mathematical formula}ei.</list-item></list><paragraph> Hence the domain of a start time variable {a mathematical formula}si is actually {a mathematical formula}[ESTi,LSTi], with {a mathematical formula}LSTi&lt;λ. Similarly, the domain of {a mathematical formula}ei is {a mathematical formula}[EETi,LETi]). Furthermore, we use the notation {a mathematical formula}x¯ and {a mathematical formula}x̲ to refer to the highest and the lowest values (i.e. the upper and the lower bounds) of the domain of a generic variable x. Obviously the lower/upper bound of a time variable is the same as its earliest/latest time, e.g. {a mathematical formula}s̲i=ESTi and {a mathematical formula}s¯i=LSTi. Finally, as stated in Section 4, the {a mathematical formula}ei and {a mathematical formula}ηi values are equal to {a mathematical formula}si+(dimodλ) and {a mathematical formula}βi+⌊diλ⌋ respectively. Therefore the model is actually based only on the {a mathematical formula}si and {a mathematical formula}βi variables.</paragraph><paragraph>These variables are subject to temporal and resource constraints. In order to model a temporal dependency {a mathematical formula}〈i,j,θ(i,j),δ(i,j)〉 we have implemented a modular precedence constraint (ModPC), with the following signature:{a mathematical formula} where {a mathematical formula}ei,sj,ηi,βj,λ are the variables representing the end time of activity i, the start time of activity j, their respective iteration numbers and the period. The parameters {a mathematical formula}θ(i,j) and {a mathematical formula}δ(i,j) are assumed to have fixed values. The filtering algorithm for the constraint is described in Section 6.1.</paragraph><paragraph>In the CRCSP all activities are subject to resource constraints. Unlike in traditional scheduling, the resource profile depends on the period value (see Section 4.1). For this reason we devised a global cumulative constraint based on a modular time table, that ensures a consistent resource usage (the GCCC, discussed in Section 6.3). The signature is as follows:{a mathematical formula} where {a mathematical formula}[si] is a vector of start time variables, {a mathematical formula}[di] is the vector of corresponding durations and {a mathematical formula}[ri] are the requirements for the modeled resource. The cap value is the resource capacity and λ is the period variable.</paragraph><paragraph>As an alternative, it is possible to use traditional resource constraints by making the assumption that {a mathematical formula}βi=ηi, i.e. that the end and the start time of an execution {a mathematical formula}(i,ω) must be within to the same period. Formally:{a mathematical formula} Since no activity is scheduled across different periods, the classical cumulative constraint can be used. As a main drawback, we may incur a quality loss due to the presence of an unnecessary restriction, which can be substantial for large resource capacities and large activity durations. In such situations, if we make no special assumption and we allow {a mathematical formula}ηi⩾βi and {a mathematical formula}ei&lt;si, we may obtain much better schedules. We refer to the solution approach making use of both the modular precedence constraint and the GCCC as CROSS, with no restricting assumption. Conversely, in the {a mathematical formula}CROSS⁎ approach we force the end times to be within the modulus and replace the GCCC with traditional cumulative constraints. The two approaches differ also for the adopted search strategy, discussed in Section 7.</paragraph><section label="5.1"><section-title>Buffer constraints</section-title><paragraph>In a real world context, a precedence constraint often implies an exchange of intermediate products between activities that should be stored in buffers. For example, in the context of embedded systems two activities may exchange data packets that should be stored in memory.</paragraph><paragraph>Every time the activity i ends, its product is accumulated in a buffer and whenever the activity j starts, a product in the buffer is consumed. It is common to have on each buffer a size limit, which can be modeled through the following constraint:{a mathematical formula} where {a mathematical formula}B(i,j) is the size limit and the reified constraint {a mathematical formula}(ei⩽sj) evaluates to one if the condition is satisfied. Inequality (15) limits the number of executions of activity i (the producer) before the first execution of j (the consumer): this ensures that the buffer capacity is always respected. Obviously {a mathematical formula}B(i,j)⩾δ(i,j), otherwise the problem is infeasible. In fact, the value {a mathematical formula}δ(i,j) can be thought as the number of products already accumulated in the buffer{a mathematical formula}〈i,j,θ(i,j),δ(i,j)〉 when the project starts for the first time.</paragraph></section></section><section label="6"><section-title>Filtering algorithms</section-title><paragraph>This section presents the filtering algorithms of the Modular Precedence Constraint, the Buffer Constraint and the Global Cyclic Cumulative Constraint, used respectively to model temporal dependencies, buffer and resource constraints.</paragraph><section label="6.1"><section-title>Filtering of the Modular Precedence Constraint ModPC</section-title><paragraph>We recall that the Modular Precedence Constraint (ModPC) constraint has the following signature:{a mathematical formula} where {a mathematical formula}ei,sj,ηi,βj,λ are variables representing respectively the end time of activity i, the start time of activity j, their respective iteration numbers and the modulus, and {a mathematical formula}θ(i,j), {a mathematical formula}δ(i,j) are constant values representing the minimum time lag and the iteration distance associated to the arc. For sake of simplicity, in the following we will omit the subscript when referring to θ and δ. We recall that the value {a mathematical formula}θ(i,j) must be non-negative, i.e. {a mathematical formula}θ⩾0.</paragraph><paragraph>The filtering algorithm of the Modular Precedence Constraint has three fundamental components:</paragraph><list><list-item label="•">The filtering rules for the iteration variables, which updates the bounds of the {a mathematical formula}ηi and {a mathematical formula}βj variables so that a proper distance (in terms of number of iterations) exists between activity i and j.</list-item><list-item label="•">The filtering rules for the start time variables, which modify the start times of the involved activities to avoid infeasible overlaps.</list-item><list-item label="•">The filtering rules for the modulus variable, which compute a lower bound on the modulus.</list-item></list><paragraph> The algorithm is executed whenever the domain bounds of any involve variable change. Filtering a single precedence relation achieves bound consistency and takes constant time.</paragraph><section label="6.1.1"><section-title>Filtering the iteration variables</section-title><paragraph>With reference to the temporal model proposed in Section 5 we can rewrite Inequality (12) as:{a mathematical formula} Starting from the equation above, we have{a mathematical formula} with {a mathematical formula}−λ&lt;sj−ei⩽λ and {a mathematical formula}θ⩾0. Eq. (17) can be used to obtain bounds over the {a mathematical formula}βj (and {a mathematical formula}ηi) variables, in particular:{a mathematical formula} As an example, suppose that during search two activities i and j connected by a temporal dependency {a mathematical formula}〈i,j,0,0〉 are overlapping and have {a mathematical formula}sj=0,ei=3: then Inequality (18) appears as follows:{a mathematical formula} which implies that {a mathematical formula}βj&gt;ηi. In fact, two connected activities can overlap iff they have different iteration values: in particular {a mathematical formula}βsinkNode&gt;ηsourceNode.</paragraph></section><section label="6.1.2"><section-title>Filtering the start time variables</section-title><paragraph>Let {a mathematical formula}Δω be {a mathematical formula}ηi−βj−δ and hence {a mathematical formula}Δω̲ be {a mathematical formula}ηi̲−βj¯−δ. Constraint (16) can now be written as:{a mathematical formula} Note that we must have {a mathematical formula}Δω⩽0: in fact {a mathematical formula}Δω&gt;0 would imply that {a mathematical formula}βj&lt;ηi−δ which is, by definition, impossible. From {a mathematical formula}Δω⩽0, we can deduce the two inequalities, that lead to bounds on the start/end variables:{a mathematical formula}{a mathematical formula} Note that if {a mathematical formula}Δω=0 the modular constraint boils down to a classical precedence constraint, i.e. {a mathematical formula}sj⩾ei+θ. In fact, two connected activities with the same iteration value cannot overlap and Inequality (21) “pushes” the destination activity j after the end time of i plus the time lag θ.</paragraph></section><section label="6.1.3"><section-title>Filtering the modulus variable</section-title><paragraph>The domain of the modulus variable can be pruned only when {a mathematical formula}Δω is strictly negative, i.e. {a mathematical formula}Δω&lt;0. In this situation, we can derive from Eq. (20) the following inequality, resulting in a lower bound on the modulus variable:{a mathematical formula}</paragraph></section></section><section label="6.2"><section-title>Filtering for the buffer constraints</section-title><paragraph>From Inequality (15) we can derive two expressions to compute bounds over the β variables:{a mathematical formula}{a mathematical formula} where {a mathematical formula}B(i,j) is the size limit of the buffer and the reified constraint {a mathematical formula}(e̲i⩽s¯j) equals one if the condition is satisfied. In other words Inequality (15) limits the iteration distance between the activities involved.</paragraph></section><section label="6.3"><section-title>Filtering of the Global Cyclic Cumulative Constraint GCCC</section-title><paragraph>The Global Cyclic Cumulative Constraint for resource k ensures consistency in the use of a single resource. The notion of consistency is formally given by Eq. (3). In our approach we make use of an equivalent formulation, which can be obtained in two steps. First, observe that in a periodic schedule, the resource constraints are satisfied if and only if they are satisfied in any λ-long interval. Therefore, without loss of generality we can restrict to {a mathematical formula}[0,λ[. Then, the resource usage due to a single activity in such an interval is given by the {a mathematical formula}rqi(si,di,t,λ) function from Eq. (13) and we can rewrite Eq. (3) as:{a mathematical formula}</paragraph><paragraph>Since the GCCC refers to a single resource, for the sake of readability we remove the k index from the requirement functions. Hence {a mathematical formula}ri,k becomes {a mathematical formula}ri and {a mathematical formula}CAPk becomes CAP.</paragraph><paragraph>The filtering is inspired by the timetable filtering for the cumulative constraint [3]. Similarly to timetable filtering, our algorithm works with the compulsory parts of activities.</paragraph><paragraph>Activity i has a compulsory part if and only if there exists a time span where the activity is necessarily executing. This happens if {a mathematical formula}ESTi+di&gt;LSTi. To take into account the resource usage corresponding to compulsory parts, we introduce a generalized version of Eq. (13) from Section 4.1:{a mathematical formula} Where {a mathematical formula}si is a start time variable, rather than a value as in the non-generalized form of the function. Basically, the generalized {a mathematical formula}rqˆi(si,di,t,λ) represents the resource usage of an unbound activity, corresponding in practice to that of its compulsory part. If the {a mathematical formula}si variable is bound, then {a mathematical formula}rqˆi(si,di,t,λ)=rqi(si,di,t,λ). The shape of the function is the same as before, namely a constant factor plus a “pulse”. Note that if the activity duration is longer than the period (i.e. {a mathematical formula}di⩾λ), then there exists a compulsory part at least as wide as the period itself. The GCCC constraint guarantees that:</paragraph><list><list-item label="1.">the start time of each activity is not lower than the minimum instant where enough resources are available.</list-item><list-item label="2.">The modulus is not lower than the minimum value such that the cumulative usage due to the compulsory parts does not exceed the resource capacity.</list-item></list><paragraph> The filtering algorithm exploits incremental computation and consists of three procedures:</paragraph><list><list-item label="•">Trigger: this procedure is executed whenever any variable bound changes. The aim of this algorithm is to update the time tabling data structure.</list-item><list-item label="•">Core: this algorithm is executed at the end of all trigger procedures and it is structured in two independent phases:</list-item><list-item label="•">Coherence: the procedure is executed whenever the modulus upper bound changes. The procedure modifies the data structure to guarantee the coherence with the new λ bound.{sup:5}</list-item></list><paragraph> In the rest of this section we focus on the two phases of the Core procedure.</paragraph><section label="6.3.1"><section-title>Core phase 1: start time filtering algorithm</section-title><paragraph>The filtering algorithm guarantees that the start time of each activity is not lower than the minimum instant where enough resources are available, i.e.:{a mathematical formula} where {a mathematical formula}di⁎=di mod λ is the length of the pulse and is referred to as modular duration.{sup:6} Similarly to the timetable approach, we adopt a data structure to store the minimum resource usage, given the current scheduling decisions. For every {a mathematical formula}t∈[0,λ), this is given by the following expression:{a mathematical formula} Note that changes in the expression value occur at the {a mathematical formula}LSTi and {a mathematical formula}EETi of all the activities: this is a direct consequence of the {a mathematical formula}rqˆi definition in Eq. (26).</paragraph><paragraph>Intuitively the algorithm proceeds as follows: for each unbound activity i the algorithm scans the resource profile, starting from {a mathematical formula}ESTi, to search for a schedulability window. A schedulability window is a time slice large enough and with enough resources to allow the activity execution. The process stops when a window is found or the search goes beyond the Latest Start Time ({a mathematical formula}LSTi). Since the solver is based on modular arithmetic, the procedure follows a modular time wheel and the {a mathematical formula}LSTi and {a mathematical formula}EETi values (i.e. the time points when a profile change may occur) are stored in a circular queue. Recall that, since {a mathematical formula}LSTi and {a mathematical formula}EETi are modular values, they are guaranteed to be lower than {a mathematical formula}λ¯. The asymptotic complexity of the algorithm is discussed at the end of the section.</paragraph><paragraph>Data structure As stated in Section 5, each activity {a mathematical formula}i∈V has four relevant time points: two of them are related to the start time, namely {a mathematical formula}ESTi (or {a mathematical formula}s̲i) and {a mathematical formula}LSTi (or {a mathematical formula}s¯i), and two related to the end time, i.e. {a mathematical formula}EETi, {a mathematical formula}LETi.</paragraph><paragraph>The constraint relies on an a circular queue {a mathematical formula}Ω[0,(‖V‖⋅2)] where each activity {a mathematical formula}i∈V is represented via two queue items, respectively corresponding to the {a mathematical formula}LSTi and its {a mathematical formula}EETi. Each item {a mathematical formula}Ω[idx] stores three values:</paragraph><list><list-item label="•">{a mathematical formula}Ω[idx].activity: the activity corresponding to item {a mathematical formula}Ω[idx].</list-item><list-item label="•">{a mathematical formula}Ω[idx].time: the time value, either the {a mathematical formula}LSTi or the {a mathematical formula}EETi.</list-item><list-item label="•">{a mathematical formula}Ω[idx].res: the total resource usage at instant {a mathematical formula}Ω[idx].time; formally{a mathematical formula}</list-item></list><paragraph> The items are sorted by increasing {a mathematical formula}Ω[idx].time.</paragraph><paragraph>The algorithm The pseudo-code is reported in Algorithm 1 where {a mathematical formula}S is the set of unscheduled activities. Lines 3–7 contain the algorithm initialization: the variable canStart represents the candidate start time of the schedulability window and initially it assumes the value of the Earliest Start Time of the selected activity i. The feasible flag is used to remember if enough resource is available at the current canStart value. The {a mathematical formula}t0 and {a mathematical formula}idx0 values are respectively the time instant and the Ω index currently being processed. Initially, they are respectively equal to {a mathematical formula}ESTi and to the largest index in Ω corresponding to an item with {a mathematical formula}Ω[idx].time⩽t0. Note that since profile changes always correspond to queue items, the resource usage at {a mathematical formula}t0 is the same as at {a mathematical formula}Ω[idx0].time. The startidx variable stores the first index in Ω examined by the algorithm, i.e. the first {a mathematical formula}idx0. The {a mathematical formula}stop flag is used for loop termination.</paragraph><paragraph>The algorithm searches for a schedulability window by scanning the queue Ω, item by item. The window is assumed to start at the current {a mathematical formula}canStart value and to end at {a mathematical formula}t0. Note that, with the exception of the first while-loop iteration (where {a mathematical formula}Ω[idx0].time may be strictly less than {a mathematical formula}t0), the current time point under examination (i.e. {a mathematical formula}t0) always matches the time value of the queue index under examination (i.e. {a mathematical formula}Ω[idx0].time).</paragraph><paragraph>From line 9 to 14 the algorithms checks the resource availability at {a mathematical formula}t0. The value avRes computed at line 9 is the resource usage, adjusted by subtracting the contribution of the compulsory part of activity i. Line 10 checks if scheduling i at time canStart would cause an over-usage at time {a mathematical formula}t0. Note that {a mathematical formula}rqi(canStart,di,t0,λ¯) corresponds to the resource consumption of activity i at time {a mathematical formula}t0, assuming it is scheduled at time canStart. Depending on the result of the check, the feasible flag is updated. A transition between {a mathematical formula}feasible=false to {a mathematical formula}feasible=true means that a new candidate start time for the schedulability window has been found and that the {a mathematical formula}canStart value must be updated.</paragraph><paragraph>Since Ω is circular, at some point the process may reach the end of the queue and re-start from the first element: the filtering algorithm makes use of an offset value to distinguish between items encountered before and after crossing the end of the queue. In particular, for the latter ones it holds {a mathematical formula}idx0&lt;startidx and the offset is set to {a mathematical formula}λ¯ (lines 10–11). We recall that {a mathematical formula}λ¯ (i.e. the maximum value in the domain of λ) is the modulus value corresponding to the minimum resource usage profile (see Fig. 11).</paragraph><paragraph>At lines 17–19 the algorithm checks if the current schedulability window, from {a mathematical formula}canStart to {a mathematical formula}t0: 1) has enough available resource and 2) it is long enough to contain activity i. The time window length is adjusted by adding the value offset. If both the conditions hold, then a valid schedulability window has been found and the start time is filtered (line 18). If this is not the case, then the algorithm checks if the current {a mathematical formula}t0 (adjusted with the offset value) has become larger than {a mathematical formula}LSTi (line 20–21): in this case there is no way to schedule activity i and the algorithm fails. If neither of the two situations occurs, at line 22 the process moves to the next item in Ω by updating {a mathematical formula}t0 and {a mathematical formula}idx0.</paragraph><paragraph>Note that the filtering procedure guarantees that an activity can be scheduled at its earliest start time (i.e. has sufficient resources), as formalized in Expression (27).</paragraph><paragraph>Traditional cumulative constraint With the assumption that the activities are scheduled within the period (i.e. {a mathematical formula}ei&lt;λ), as in the {a mathematical formula}CROSS⁎ approach, the algorithm proposed achieves the same filtering of a traditional cumulative constraint, i.e. obtained through the timetabling or the sweep algorithm (respectively in [3] and [4]). In case this hypothesis is relaxed, the traditional cumulative constrained cannot be applied.</paragraph><paragraph>Time complexity The filtering algorithm has two nested cycles, respectively over the set of non-scheduled activities {a mathematical formula}S and over the items of the circular queue Ω. The corresponding maximum numbers of iterations are n and {a mathematical formula}2⋅n. Hence the asymptotic complexity is {a mathematical formula}O(n2).</paragraph></section><section label="6.3.2"><section-title>Core 2: modulus filtering algorithm</section-title><paragraph>In cyclic scheduling, it is possible to reduce the cumulative usage at time t by increasing the modulus. As a consequence, unlike in classical scheduling, the compulsory parts in the current schedule may enforce a non-trivial lower bound on the feasible λ. The goal of lambda filtering is to find the minimum period where sufficient resources are available for the current schedule. Formally:{a mathematical formula}</paragraph><paragraph>The algorithm makes again use of a circular queue Ω. However, the value {a mathematical formula}Ω[idx].time of the items corresponding to the Earliest End Time is computed assuming that {a mathematical formula}λ=λ̲. In other words, they are the Earliest End Time for the scenario where the period takes its minimum value, corresponding to the most constrained resource profile.</paragraph><paragraph>Fig. 11 shows two different resource usage profiles for the same activity start times, but with different period value. The former corresponds to the maximum λ value (i.e. {a mathematical formula}λ¯) and the latter corresponds to the minimum λ value (i.e. {a mathematical formula}λ̲). Note that with {a mathematical formula}λ̲ the activities A and B now cross the modulus, increasing the resource consumption at time 0. This causes a resource over-usage, represented by the shaded area. The effect of the period λ on the usage profile is a direct consequence of how the resource usage function {a mathematical formula}rq(si,di,t,λ) is defined (see Section 4.1). The goal of the modulus filtering algorithm is to get rid of the over-usage by increasing the lower bound {a mathematical formula}λ̲. This is done in an iterative fashion, by repeatedly computing the over-usage integral and pushing the λ lower bound ahead.</paragraph><paragraph>The filtering algorithm The pseudo-code for the filtering procedure is reported in Algorithm 2. It is an iterative process, repeated until the resource over-usage becomes 0.</paragraph><paragraph>The cumulative resource over-usage at each iteration is referred to as En. At the beginning of each iteration (lines 3–5), the algorithm updates and reorders the data structure Ω: this is necessary since {a mathematical formula}λ̲ is changed at each iteration, causing a modification of all the modular end times. Moreover, the procedure resets the values lastres and lasttime, respectively referring to the last processed resource consumption and to the corresponding time point.</paragraph><paragraph>Then the items in the queue are processed one by one. At lines 9–12 the resource consumption is increased or decreased depending on whether the item corresponds to a start or an end time. At line 13, the procedure checks if the resource consumption of the last processed time point (i.e. lastres) exceeds the resource. In this case, the cumulative resource over-usage on the time window {a mathematical formula}[lasttime,Ω(idx).time[ is summed to the current En quantity. At line 15 the algorithm updates the lastres and lasttime values, before starting to process the next item. The period lower bound is updated at the end of each iteration (line 16), by dividing the cumulative over-usage by the capacity of the resource and summing the quantity to the current {a mathematical formula}λ̲.</paragraph><paragraph>In the following we provide the correctness and termination proofs. Three important reminders for the proofs are:</paragraph><list><list-item label="•">the resource consumption of a compulsory part is a decreasing function of the period (see Eq. (26)).</list-item><list-item label="•">The algorithm works with partial solutions as not all activities have been scheduled.</list-item><list-item label="•">The resource profile is defined as the cumulative usage due to the compulsory parts of all the activities. We recall that the profile depends on the value of λ and on the start time variable domains (see Eq. (28)).</list-item></list><paragraph>Correctness proof Let us assume that λ is a real number (i.e. {a mathematical formula}λ∈R). At each algorithm step a new bound is computed: {a mathematical formula}λ̲←λ̲+Encap.</paragraph><paragraph>The algorithm is correct if the last computed bound is valid. A bound {a mathematical formula}λ̲ is valid if for any lower λ value the resource profile exceeds the resource capacity (i.e. {a mathematical formula}λ&lt;λ̲⇒infeasible profile).</paragraph><paragraph>The algorithm is therefore correct if at each step the computed offset (i.e. {a mathematical formula}Encap) is the minimum bound shifting that might get rid of the resource over-usage.</paragraph><paragraph>By shifting the bound {a mathematical formula}λ̲ by an offset Λ, a new resource area E is introduced in the schedule: formally {a mathematical formula}E=Λ⋅cap. This shifting might reduce the over-usage energy En and obviously the maximal reduction is E. Note that if {a mathematical formula}E⩽En, then {a mathematical formula}En⩾Λ⋅cap. We can therefore compute the minimum offset which corresponds to {a mathematical formula}Λ=Encap (from {a mathematical formula}En=Λ⋅cap).</paragraph><paragraph>Finally, as λ is actually an integer variable, the ceiling function {a mathematical formula}⌈Encap⌉ guarantees to find the minimum integer valid bound. The algorithm is therefore correct.</paragraph><paragraph>Termination proof First, observe that the offset Λ computed at step k is never greater that the offset at step {a mathematical formula}k+1.</paragraph><paragraph>Now, the Algorithm terminates if (1) it finds a valid bound or (2) it proves an infeasibility. Note that an infeasibility means that no valid bound exists in the domain of λ, i.e. that even the value {a mathematical formula}λ¯ corresponds to an infeasible resource profile. However, this condition is already checked by Algorithm 1, meaning that, when Algorithm 2 executes, the profile for {a mathematical formula}λ=λ¯ does not exceed the resource usage. We can therefore safely disregard termination condition 2. Moreover, we know that the computation will converge to some bound value {a mathematical formula}λ⁎⩽λ¯.</paragraph><paragraph>However, assuming a real λ variable, it may happen that the computation takes an infinite number of steps. This may occur in case the computed offset Λ tends to zero but never becomes null. Now, it is sufficient to assume a finite precision for the computation to converge. Since in practice we model λ with an integer variable the termination proof holds.</paragraph><paragraph>Time complexity The algorithm inner loop has two main steps:</paragraph><list><list-item label="•">Updating and sorting the modular end time for items in Ω, with asymptotic complexity {a mathematical formula}O(n⋅logn).</list-item><list-item label="•">Computing the cumulative over-usage. This takes up to {a mathematical formula}2⋅n steps, and thus has asymptotic complexity {a mathematical formula}O(n).</list-item></list><paragraph> It is difficult to obtain a tight bound on the number of iterations of the main loop (lines 2–16). As λ is an integer, then {a mathematical formula}λ¯−λ̲ provides a trivial bound and the overall asymptotic complexity is {a mathematical formula}O((λ¯−λ̲)⋅n⋅logn). In practice, however, the number of iterations of the main loop is very small.</paragraph></section></section></section><section label="7"><section-title>Search</section-title><paragraph>In this section, we introduce two search strategies tailored for two models of the problem. The first (called Path-based method) works for the restricted case where the end time are assumed to be within the modulus, {a mathematical formula}βi=ηi∀i∈V. The second (called random restart method) is used in the generic version and it is based on restarts, although in this case they are employed to ensure correctness rather than to speed up search. As discussed in Section 5, by constraining the end times to be within the modulus, we can model the resource constraints with traditional cumulative constraints.</paragraph><paragraph>In this section we present both the search strategies, together with their variable and value selection heuristics. Finally we also present a dominance rule on β variables used to narrow the search space.</paragraph><section label="7.1"><section-title>Path-based method</section-title><paragraph>This is the search strategy employed in {a mathematical formula}CROSS⁎. Since our approach is designed to build periodic schedules, the start time value of each activity can be decided with respect to another, arbitrarily chosen, reference activity. The {a mathematical formula}si and {a mathematical formula}βi variable of this reference activity (called The reference node) can be fixed to zero. Such a restriction does not compromise the method completeness, as long as the {a mathematical formula}βi variable domains are large enough. We always choose as a reference activity a node with no in-going arc having {a mathematical formula}δ(i,j)=0. Formally, for a node src to be a candidate, it must hold {a mathematical formula}∄〈i,src,θ(i,src),δ(i,src)〉∈A such that {a mathematical formula}δ(i,src)=0. Note that there always exists at least one node with this property, otherwise the graph is in deadlock and the problem is infeasible. A formal proof can be found in [5].</paragraph><paragraph>The reference activity (let its index be src) is always immediately scheduled, by posting {a mathematical formula}ssrc=0,βsrc=0, with no backtrack possible. Then, at any search node the next activity j to be used for branching is chosen among those connected by an arc {a mathematical formula}(i,j) to one of the activities i already scheduled. Those candidates are ranked according to the number of outgoing arcs (the more, the better). This selection criterion seems to lead to better filtering of the Modular Precedence Constraint and for the dominance rule described in Section 7.3. In case of ties, the algorithm prefers activities with longer duration, and finally with smaller index.</paragraph><paragraph>Then we assign a value to the start time and to the iteration number of the selected activity, i.e. to variables {a mathematical formula}sj and {a mathematical formula}βj. This is done in two successive search steps, rather than by posting the conjunction of the two assignments as a single constraint.</paragraph><paragraph>In particular, the algorithm attempts to schedule the activity at its earliest start time, i.e. it assigns {a mathematical formula}sj to its minimum value. On backtrack, the activity is postponed, i.e. marked as non-selectable until its earliest start time is modified by propagation. This is analogous to what it done in the classical schedule-or-postpone strategy for the RCPSP [23] and can be done since in {a mathematical formula}CROSS⁎ we enforce resource restrictions by means of traditional cumulative constraints.</paragraph><paragraph>The {a mathematical formula}βj variables are assigned via labeling, with value order given by increasing {a mathematical formula}|v| (where v is the value to be assigned). In case of ties, the positive v is chosen. This criterion is based on the existing correlation between iteration numbers and schedule length (see Section 4) and is designed to produce schedules as short as possible. A deeper discussion on the values that the iteration variables could assume can be found in Section 7.3). On backtrack, the previously assigned value is simply removed from the domain of {a mathematical formula}βj.</paragraph><paragraph>Unlike other modulo scheduling approaches, our solution method does not require to a-priori fix the period. When all the activities have been scheduled, we simply assign to the λ variable the minimum in its domain. This value is guaranteed to be feasible by the propagation of precedence and resource constraints. Optimization is performed in a traditional CP fashion by posting a permanent upper bound on λ whenever a feasible solution is found.</paragraph></section><section label="7.2"><section-title>Random Restart method</section-title><paragraph>If the activities are allowed to cross different iterations, like in our generic CROSS approach, the modular starting times depend non-monotonically on λ. As a major consequence, updating the upper bound of λ may cause some values that were previously inconsistent to become consistent (or vice versa).</paragraph><paragraph>Consider the following example (see Fig. 12): we have two activities A and B whose duration is 9 and 3 respectively. There is a precedence constraint between A and B with {a mathematical formula}δ=0 and a precedence constraint between B and A with {a mathematical formula}δ=1. A is scheduled with {a mathematical formula}sA=0 and {a mathematical formula}βA=0. Suppose also that {a mathematical formula}λ¯=6, which implies that the modular end time of A is 3. Then the feasible values for {a mathematical formula}sB are {a mathematical formula}3,4,5. Now suppose we update {a mathematical formula}λ¯=4. The modular end time of A becomes 1 and therefore, we have that values 1 and 2, that were previously infeasible for the modular start time of B, are now feasible.</paragraph><paragraph>For overcoming this issue, we have decided to restart the search from scratch any time the upper bound of λ is updated.</paragraph><paragraph>The structure of the search strategy is the same as before: an activity j is selected, then a value is assigned to its start time and iteration variables (i.e. {a mathematical formula}sj and {a mathematical formula}βj) in two successive search nodes. Once all the activities have been scheduled, the λ variable is fixed to {a mathematical formula}λ̲. The iteration numbers are assigned as in {a mathematical formula}CROSS⁎, giving priority to values v in the domain having the minimum absolute value {a mathematical formula}|v| and posting {a mathematical formula}βj≠v on backtrack.</paragraph><paragraph>As a first difference, we do not fix a reference activity. At every search node, the activity j to be scheduled is chosen at random among those connected to at least one already scheduled activity i. At the root node (where no scheduling decision has been taken) the choice is simply made at random.</paragraph><paragraph>Second, the selected activity is not scheduled at {a mathematical formula}ESTi. Instead, the algorithm keeps a list of candidate start times, corresponding to the values {a mathematical formula}si+(dimodλ¯) for all the already scheduled activities.{sup:7} The value 0 is always added to the list in case it is not present. Then the selected activity j is scheduled at the smallest candidate start time t falling in the interval {a mathematical formula}[ESTi,LSTi]. If there is no such t, the algorithm backtracks without even opening a choice point. Otherwise, the activity j is scheduled at time t and on backtrack the constraint {a mathematical formula}si&gt;t is posted.</paragraph></section><section label="7.3"><section-title>Dominance rules</section-title><paragraph>One important observation is that assigning different iteration values to activities connected by an arc {a mathematical formula}(i,j) allows to place activity j before activity i in the λ-long window, apparently violating the precedence constraint. In particular, this requires to {a mathematical formula}βj to be greater than {a mathematical formula}βi by a minimum amount: any further increase obtains the same effect. This observation serves as the basis to devise a dominance rule that filters redundant values for the β domains.</paragraph><paragraph>Suppose we want to schedule two activities i and j, connected by a temporal dependency {a mathematical formula}〈i,j,θ(i,j),δ(i,j)〉. From (12) we derive:{a mathematical formula} and let {a mathematical formula}Δβ=si+di−sj+θ(i,j)λ. Letʼs focus on the successor j. We analyze the dependency in Eq. (29) in two different cases:</paragraph><list><list-item label="1.">Activity j follows activity i: in this case {a mathematical formula}sj⩾si+di+θ(i,j). Since {a mathematical formula}sj&lt;λ, {a mathematical formula}Δβ is in the interval {a mathematical formula}]−1,0]. Hence, {a mathematical formula}⌈Δβ⌉=0 and then {a mathematical formula}βj⩾βi−δ(i,j).</list-item><list-item label="2.">The two activities overlap in the λ-long window, or activity j precedes activity i: in this case {a mathematical formula}sj&lt;si+di+θ(i,j) and {a mathematical formula}Δβ is strictly positive. Therefore, the lowest possible value that {a mathematical formula}⌈Δβ⌉ can assume is 1, which implies {a mathematical formula}βj&gt;βi−δ(i,j).</list-item></list><paragraph> Note that {a mathematical formula}βj must be at least equal to {a mathematical formula}βi−δ(i,j), and must be strictly greater if we want activity j to overlap with or precede activity i in the time window. In particular, the smallest value that allows the apparent violation of the precedence constraint is {a mathematical formula}βi−δ(i,j)+⌈Δβ⌉: any higher value provides no benefit from a schedule building perspective. This information can be used to devise a dominance constraint that removes dominated (but feasible) values from the domain of β variables. In detail, we can post:{a mathematical formula} where {a mathematical formula}Precj={i∈V|〈i,j,θ(i,j),δ(i,j)〉∈A} is the set of the predecessors of j. Note that, as a consequence of the combination of the modular precedence constraints (i.e. Eq. (29)) and the dominance rule, {a mathematical formula}βj is strongly constrained by the {a mathematical formula}βi of the predecessor activities: this is the reason why scheduling activities according to their topological order may result in a much better propagation (see Section 7.1).</paragraph><paragraph>We can obtain a second dominance rule by reasoning in a similar fashion for the successors of an activity i:{a mathematical formula} where {a mathematical formula}Nexti={j∈V|〈i,j,θ(i,j),δ(i,j)〉∈A} is the set of the successors of i.</paragraph><paragraph>To make the rules clearer, we make a simple example. Assume we have a graph with three activities {a mathematical formula}A,B,C connected with two arcs: {a mathematical formula}〈A,B,0,0〉 and {a mathematical formula}〈B,C,0,0〉. Assume that each variable is initially unbound, with {a mathematical formula}si∈[0,λ[ and {a mathematical formula}βi∈{−3..+3} for every activity. If we schedule activity A by posting {a mathematical formula}sA=0 and {a mathematical formula}βA=0, then the {a mathematical formula}βi domain for all other activities becomes {a mathematical formula}{0..+3}. The application of the dominance rule (30) results in a further reduction of the domains, in particular {a mathematical formula}βB∈{0,+1} and {a mathematical formula}βC∈{0..+2}. Note that the values {a mathematical formula}βB=2 and {a mathematical formula}βB=3 are feasible but redundant. At this point, if the activity B is scheduled so that it overlaps with A (i.e. {a mathematical formula}sB&lt;sA+dA), its iteration variable is be forced to {a mathematical formula}βB=1.</paragraph><paragraph>From an implementation perspective, the direct formulation of the rules may be difficult to model and heavy to propagate. It is therefore convenient to modify the right hand side expression in Inequality (30) with an upper bound, obtaining the following, updated rule:{a mathematical formula}{a mathematical formula} For a fixed {a mathematical formula}λ¯ value, the expression {a mathematical formula}⌈di+θ(i,j)λ⌉ can be replaced with a fixed value to obtain a (much simpler to compute) upper bound on {a mathematical formula}βj. This technique is employed in the context of the CROSS search strategy, which works by restarting the search whenever the λ upper bound is modified.</paragraph></section></section><section label="8"><section-title>Experimental results</section-title><paragraph>We have evaluated the effectiveness and scalability of both the approaches discussed in Section 5 (CROSS and {a mathematical formula}CROSS⁎) on various sets of instances. Moreover, we have compared them against a state-of-the-art ILP method and a state-of-the-art heuristic.</paragraph><paragraph>In the first part of this section we will focus on the {a mathematical formula}CROSS⁎ approach: we show that the solver is able to compute good quality solutions in terms of seconds in the context of a real world problem. In the second part of the section we present a comparison between CROSS and {a mathematical formula}CROSS⁎, plus a blocked and an unfolded approach. More in detail, this section contains:</paragraph><list><list-item label="1.">An experimentation on industrial instances: the main purpose of this experimental evaluation is to show the our {a mathematical formula}CROSS⁎ approach to cyclic scheduling is viable in a practical setting. In particular, the experimentation will focus on instruction scheduling benchmarks, namely an industrial set of 36 instances for the ST200 processor by STmicroelectronics (also employed in [2]). We compare {a mathematical formula}CROSS⁎ with two state-of-the-art approaches: the complete ILP method presented in [2] and Swing Modulo Scheduling (SMS), a heuristic used by the GCC compiler [18].</list-item><list-item label="2.">An evaluation of the solution quality obtained by the {a mathematical formula}CROSS⁎ approach: the second group of tests is performed on a set of synthetic instances and compares the best solution obtained by {a mathematical formula}CROSS⁎ (within 300 s) with a lower bound on the instance period (namely, the iteration bound from Section 2.3.1).</list-item><list-item label="3.">A comparison between CROSS/{a mathematical formula}CROSS⁎ and blocked/unfolded scheduling: this experimentation is performed on a set of synthetic instances, for which we compare the solution quality of both the approaches with respect to classic blocked and unfolded scheduling.</list-item><list-item label="4.">A comparison between CROSS and {a mathematical formula}CROSS⁎, w.r.t. the Throughput / Resource trade-off: the fourth group of results compares our two approaches in terms of Throughput / Resource-usage trade-off.</list-item></list><paragraph> All our solvers have been implemented in IBM ILOG Solver and Scheduler 6.7. All the experiments are performed on an Intel Core 2 Duo 3.3 GHz with 8 GB of RAM. A 300 s time limit was set on each run. The synthetic instances were built by means of an internally developed task-graph generator, designed to produce graphs with realistic structure and parameters.</paragraph><section label="8.1">Evaluation of {a mathematical formula}CROSS⁎ on industrial instances<paragraph>The first set of 36 instances is obtained from a compiler for a VLIW (Very-Long Instruction Word) architecture. In this setting, cyclic scheduling problems arise when optimizing inner loops at instruction level: in this context the activities represent instructions, the precedence constraints model data and control dependencies and the resources are the hardware units required to execute the instructions. In the considered instances, all the resources have unary capacity and all the instructions have unary duration. With the objective to evaluate the approaches on a more diversified setting, in [2] the authors have obtained an additional set of modified instances, by replacing the original resource capacities and consumptions with randomly generated integer values.</paragraph><paragraph>The smallest instance features 10 nodes and 42 arcs, while the largest one has 214 nodes and 1063 arcs. In [2] the authors present two ILP formulations for the resource-constrained modulo scheduling problem (RCMSP), which is equivalent to the CRCSP discussed in this paper, with the (easy to enforce) exception that the period must be integer. As described in [2], both the ILP approaches adopt a dual process by iteratively increasing an infeasible lower bound; as a consequence, the method does not provide any feasible solution before the optimum is reached. Given a large time limit (604 800 s) their solvers found the optimal solution for almost all the instances: our experiments compare the optimal value with the solution found by our method within a 300 s time limit.{sup:8} We also compare our approach with an incomplete method, namely a state of the art heuristic approach called Swing Modulo Scheduling (SMS), presented in [26] and used by the gcc compiler [18].</paragraph><paragraph>Note that the ILP and the SMS approaches were developed to tackle instruction scheduling problems; hence both work with activities with unary duration.</paragraph><paragraph>As we mentioned, our benchmarks consists of two subsets of instances: the original problems extracted by the compiler and the modified ones (which tend to be more challenging). The results of the experiments performed with both the sets are reported respectively in Table 1 and Table 2. In the tables the first three columns describe the instance (name, number of nodes and arcs), the third shows the run-time of the ILP approach in [2], the fourth and the fifth respectively report the solution time and the quality of our solutions, evaluated by means of the gap w.r.t. the optimal solution.{sup:9} The last two columns present the solution time and the quality gap for the SMS heuristic approach. For some instances (see the missing time information) the ILP approach was not able to find a solution: in such cases we report the best solution found by {a mathematical formula}CROSS⁎, which is used as a reference for computing the SMS gap.</paragraph><paragraph>On the industrial set, {a mathematical formula}CROSS⁎ is able to find the optimal solution within one second for all but one instance (adpcm-st231.2, whose optimality gap is 2.44%). We also found a solution for the gsm-st231.18 instance that was previously unsolved. The SMS heuristic method obtains an average optimality gap of 14.58%. Its average run time is less than few seconds, with a maximum of 58.36 s, corresponding to instance gsm-st231.18. For the previously unsolved instance (i.e. gsm-st231.18), the gap between SMS and {a mathematical formula}CROSS⁎ is 3.8%.</paragraph><paragraph>Table 2 report the experimental results on the modified set of instances. Our approach finds the optimal solution within a second for all of the instances but two (for which the average gap is 0.61%). SMS achieves an average gap of 3.03%: again, the highest computation time corresponds to instance gsm-st231.18, but it is considerably larger compared to that of the industrial set (i.e. 429.36 s).</paragraph><paragraph>Finally, note that for the instances gsm-st231.25 and gsm-st231.33 no optimal solution is available, since in [2] the authors were only able to find a suboptimal schedule within the time limit of 604 800 s (no details are given on how the dual process they propose converges to sub-optimal solutions). On the instance gsm-st231.25 both the modular and the SMS approaches find the same solution as [2]. On gsm-st231.33 the best period value found in [2] has value 52 and the SMS solver finds the same solution; our method however finds in one second a solution with value 47 and within 56 s a solution of value 46.</paragraph><paragraph>Discussion on the optimality proof The results of our experimentation show that {a mathematical formula}CROSS⁎ is able to find very good solutions in a very short time (a few seconds). The method appears however much less effective in proving optimality: despite the optimal solution was found on most instances, an optimality proof was achieved only for 12.5% of them within the time limit.</paragraph><paragraph>This is mainly due to the fact that cyclic instances are frequently characterized by a large number of precedences having high repetition distance δ values. Unfortunately this makes the precedence constraints propagation less effective (see Section 6.1).</paragraph><paragraph>As formulated in Section 2.3.1 a cycle c with a high {a mathematical formula}D(c) (we recall that {a mathematical formula}D(c)=∑(i,j)∈A(c)δ(i,j)) has less impact on the iteration bound than a cycle with a lower {a mathematical formula}D(c).</paragraph><paragraph>A simple example is considering two activities {a mathematical formula}i,j connected in a cycle with: {a mathematical formula}D(c)=1, or {a mathematical formula}D(c)&gt;1 or without precedences. Let also the activity i be scheduled. In the first case (where {a mathematical formula}D(c)=1) the precedence constraint propagation has a strong impact forcing the activity j to be scheduled after the end of i. In the other two cases (where {a mathematical formula}D(c)&gt;1 or without precedences) the precedence constraint cannot filter on the start time domain of activity j. In fact the two activities might overlap their executions with different iteration values (i.e. β).</paragraph><paragraph>To conclude, high {a mathematical formula}D(c) values mean that most of activities of c can overlap their executions. There is therefore a strong analogy with those instances and acyclic scheduling problems with few precedence constraints (or even no precedences). It is well known that CP approaches based on start time variables and cumulative constraints have difficulties in proving optimality for this class of problem instances.</paragraph><paragraph>We are currently investigating how to improve the efficiency of the proof of optimality through no-good learning. Note however that the optimality gap is so small that the method is very appealing even if used as a heuristic.</paragraph></section><section label="8.2">Evaluating {a mathematical formula}CROSS⁎ solution quality on synthetic benchmarks<paragraph>The second set of experiments targets a task scheduling problem over a multi-processor platform. The benchmark in this case contains 1200 synthetic instances with 20 to 100 activities, cycles in the graph and high concurrency (i.e. the precedence constraints allow many activities to run in parallel). In a preliminary experimentation, this kind of graph structure appears to be the toughest to solve for our approach. The set of processors on the platform is represented in the generated instances as a cumulative resource with capacity 6 (representing the number of parallel threads). All activities have unary resource consumption.</paragraph><paragraph>Table 3 shows the average, best and worst gap over time between the {a mathematical formula}CROSS⁎ solution and a lower bound, computed as:{a mathematical formula} that is the maximum between the iteration bound IB (see Section 2.3.1) and the ratio between the sum of the execution times and the total capacity.</paragraph><paragraph>As one can see in the table, {a mathematical formula}CROSS⁎ finds a solution which is about 3.7% distant from the lower bound value within one second. After 300 s the gap has decreased to 2.9%. Hence, even on this new instance set the solver is able to find a high quality solution very quickly. After that, improvements are obtained much more slowly. Fig. 13 provides a more detailed view of the progress of the gap value. Once again, we can conclude that our approach converges very quickly to period values close to the lower bound. The actual optimum lies somewhere in-between the two values and therefore is even closer.</paragraph></section><section label="8.3">CROSS and {a mathematical formula}CROSS⁎ vs blocked and unfolding scheduling<paragraph>Our third experimentation contains in first place a comparison between our approach, producing overlapped schedules, and the blocked approach that takes into account a single iteration. Since the problem is periodic and the schedule is iterated indefinitely over time, the latter method pays a penalty in the quality of the schedule obtained. A technique often employed to address this limitation and to allow inter-iteration parallelism is unfolding (see Section 3). Unfolding often leads to improved blocked schedules, at the cost of an increase of the instance size. In detail, we report the results for two separate sets of experiments, respectively evaluating the {a mathematical formula}CROSS⁎ and CROSS method.</paragraph><paragraph>First set of experiments: The first experimentation is performed on a set of 220 instances, divided into three classes: small instances (14 to 24 activities), medium-size instances (25 to 44 activities) and big instances (45 to 65 activities). We compared our approach with the blocked one and with unfolded scheduling, using seven different unfolding factors (referred to as UnfoldX, where X is the number of unrolled iterations).</paragraph><paragraph>Table 4 shows the average gap between the above mentioned configurations and {a mathematical formula}CROSS⁎. The last column presents the average optimality gap over the whole experimental set. As expected, the worst gap is relative to the blocked schedule. Less obviously, the UnfoldX configurations tend to have an oscillatory behavior. Fig. 14 depicts the relation between the gap (Y axis) and the blocking factor (X axis) for a selected instance with 30 activities. With an unfolding factor {a mathematical formula}u=11 the solver finds a solution with the same period as the overlapped one. Increasing the unfolded factor to 12 deteriorates, rather than improving, the solution quality. In general (see Section 3.2), it is false that increasing u necessarily results in better solutions, making the determination of the optimal unfolding factor a non-trivial problem.</paragraph><paragraph>Furthermore, there are cases where finding a schedule with the same quality of a periodic approach is not possible at all: such an example is provided in [33]. On the other hand, we recall that periodic schedules are dominated by K-periodic schedules, i.e. periodic schedules for a sequence of consecutive iterations. K-periodic schedules are known to dominate 1-periodic schedule in the presence of finite capacity resources [19]. Therefore, since the unfolding technique produces a restricted class of K-periodic schedules, there may be cases where an unfolded schedule is strictly better than any possible 1-periodic schedule. In summary: no strict dominance exists between the two approaches. Our experimentation shows however that, on the considered benchmarks, periodic schedules tend to be much better in practice.</paragraph><paragraph>Second set of experiments: The second experimentation is performed on a group of 200 synthetically generated project graphs with 5 to 25 activities. The resource capacity is fixed to 5 times the average consumption. The durations are randomly generated so that around 10% of the activities in each graph are much longer than the remaining 90%. We compare the CROSS approach with blocked and the unfolded scheduling, for unfolding factors up to 5 (this value is sufficient to see the overall trend). We use the classical (and very effective) schedule or postpone search strategy for the blocked and the unfolded approach. For the CROSS one, we couple the Random &amp; Restart method from Section 7.2 with a binary search on λ (using a time limit of 2 s on each iteration).</paragraph><paragraph>Table 5 shows the result of the comparison, grouped by the number of activities in the graph. In particular, the whole set of 200 instances has been partitioned into two subsets, respectively with 5–14 and 15–25 activities. The Time column shows the average time taken by the CROSS approach to reach the best solution found within 300 s. The table then reports, for all the considered approaches, the (average) quality gap w.r.t. the CROSS solution (i.e. {a mathematical formula}λ−λCROSSλCROSS) and an (average) {a mathematical formula}Δtime value, representing the difference between the time taken by the considered approach and by CROSS to reach the best solution found within 300 s.</paragraph><paragraph>The results show the existence of a relevant trade-off between solution time and quality. The blocked scheduling approach represents one of the extremes, providing its best solution in a fraction of second, but with a quality gap as large as 216%. Increasing the unfolding factor leads to better λ values, at the cost of increased solution time. The Unfold5 configuration eventually manages to produce better solutions compared to CROSS (1.3% improvement), but the process is up to 6 times longer. Our approach seems to provide a very good compromise, providing high quality solutions in a short amount of time.</paragraph></section><section label="8.4"><section-title>Throughput/resource trade-off investigation</section-title><paragraph>Cyclic scheduling allows to improve the resource efficiency by partially overlapping different schedule iterations. In particular, it is possible to exploit the available resources to reduce the period, even when the makespan cannot be further minimized (e.g. due to precedence constraints). Loops in the project graph limit the degree of such an improvement (see Section 2.3.1): if the graph is acyclic, however, the throughput can be arbitrarily increased by augmenting the resource capacity. A large number of practical problems (e.g. in VLIW compilation or stream computing) are described by project graphs with a few cycles or no cycle at all. In such a case, identifying the optimal throughput/resource-usage trade-off is the primary optimization problem.</paragraph><paragraph>We performed an experimentation on two groups of 20 synthetically generated instances, respectively consisting of cyclic and acyclic graphs. Durations are again unevenly distributed, as described in Section 8.3. In order to investigate the throughput/resource trade-off, we solved a set of period minimization problems with different resource availability levels. In detail, we considered a single resource and activities in each graph were labeled with random resource requirements, following a normal distribution. The resource capacity (referred to as CAP) ranges between 4 times and 14 times the average consumption level: the minimum value is chosen so as to guarantee the problem feasibility, while the maximum one is chosen to assess the solution quality in case of abundant resources.</paragraph><paragraph>Fig. 15 shows the average period after 300 s (the time limit was hit in all cases) obtained for different resource capacity values. The vertical bars report the corresponding standard deviation. The solid line corresponds to the CROSS approach and the dashed one to {a mathematical formula}CROSS⁎. Note that CROSS obtains considerably better results for higher capacity values, i.e. the scenario when we expected the highest benefit from allowing activities to cross iterations. The gap is larger for acyclic graphs, where the lack of loops enables to fully exploit the available resources.</paragraph><paragraph>The iteration period difference corresponds to a much larger gap in terms of “wasted” resource capacity, which can be assessed by measuring the value of the expression:{a mathematical formula} The average slack values for this experimentation are reported in Fig. 16, where the overall resource waste is shown to grow according to a roughly quadratic law for the {a mathematical formula}CROSS⁎ approach. The growth is much slower for CROSS (in fact, it is approximately constant for acyclic graphs). The amount of wasted resource capacity is an important measure of how efficiently the resources are used and in a practical setting directly translates to platform/machine costs.</paragraph><paragraph>Interestingly, the two approaches have comparable performance for small capacity values. This suggest that the time limit is not severely limiting the search effectiveness. This is a relevant remark, since we expected CROSS to be considerably slower in finding good solutions. More details are reported in the histograms from Fig. 17, that show the instance count, grouped by the time (in seconds) employed by each method to get 1% close to the final best solution. As one can see, {a mathematical formula}CROSS⁎ is indeed faster on average, but both methods manage to provide high quality schedules in seconds.</paragraph></section></section><section label="9"><section-title>Conclusions</section-title><paragraph>We have proposed a constraint approach to solve cyclic scheduling problems, based on modular arithmetic. In particular, we have devised global constraints to model temporal dependencies (the Modular Precedence Constraint, ModPC) and resource restrictions (The Global Cyclic Cumulative Constraint, GCCC). For both of them, we devised original filtering algorithms. We have also described a restricted version of our solver where we rely on a specific assumption to model the resource restrictions via traditional cumulative constraints, rather than via the GCCC. The general instantiation of our approach is referred to as CROSS, while its restricted version as {a mathematical formula}CROSS⁎.</paragraph><paragraph>The approach has been tested extensively on industrial as well as synthetically generated instances. Our method was able to return solutions very close to the known optimum or to a lower bound in a matter of seconds (or a fraction of second), on problems of practical size. Depending on the considered benchmarks, our approach either outperformed existing heuristic or unfolding-based techniques, or provided a very good compromise between solution quality and time. All this makes our method one of the best available solvers for cyclic scheduling problems with cumulative resources.</paragraph><paragraph>Future research directions include improving the effectiveness of the proof of optimality, which is currently a weak point of the method. A second, very interesting, research topic concerns the design of a more effective search strategy for the unrestricted CROSS approach. Since the start times assigned by our Random &amp; Restart strategy depend on the period upper bound {a mathematical formula}λ¯, they tend to lead to solutions with period quite close to {a mathematical formula}λ¯ itself, thus making the optimization process slower. This could be addressed by exploiting ideas from the Precedence Constrain Posting technique [29], [34], developed for non-cyclic scheduling problems.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>We thank Maria Ayala and Christian Artigues for the useful discussions and the industrial benchmarks. The work described in this publication was supported by the SMECY Project, JTI ARTEMIS, Grant agreement No.: 100230.</paragraph></acknowledgements><references><reference label="[1]"><authors>C. Artigues,S. Demassey,E. Néron</authors><title>Resource-constrained Project Scheduling</title><host>Control Systems, Robotics and Manufacturing Series (2010)Wiley</host></reference><reference label="[2]">M. Ayala, C. Artigues, On integer linear programming formulations for the resource-constrained modulo scheduling problem, Technical Report 10393, LAAS-CNRS, Toulouse, France, 2010.</reference><reference label="[3]"><authors>P. Baptiste,C. Le Pape,W. Nuijten</authors><title>Constrains-Based Scheduling: Applying Constraint Programming to Scheduling</title><host>(2001)Springer</host></reference><reference label="[4]"><authors>Nicolas Beldiceanu,Mats Carlsson,Sven Thiel</authors><title>Sweep synchronization as a global propagation mechanism</title><host>Comput. Oper. Res.33 (10)(October 2006) pp.2835-2851</host></reference><reference label="[5]"><authors>S.S. Bhattacharyya,S. Sriram</authors><title>Embedded Multiprocessors – Scheduling and Synchronization (Signal Processing and Communications)</title><host>2nd edition(2009)CRC Press</host></reference><reference label="[6]"><authors>F. Blachot,B. Dupont de Dinechin,G. Huard</authors><title>SCAN: A heuristic for near-optimal software pipelining</title><host>Proc. of Euro-Par 2006, vol. 4128(2006) pp.289-298</host></reference><reference label="[7]"><authors>A. Bonfietti,L. Benini,M. Lombardi,M. Milano</authors><title>An efficient and complete approach for throughput-maximal SDF allocation and scheduling on multi-core platforms</title><host>Proc. of DATE(2010) pp.897-902</host></reference><reference label="[8]"><authors>Peter Brucker,Thomas Kampmeyer</authors><title>Tabu search algorithms for cyclic machine scheduling problems</title><host>J. Sched. (1994)(2005) pp.303-322</host></reference><reference label="[9]"><authors>Peter Brucker,Thomas Kampmeyer</authors><title>A general model for cyclic machine scheduling problems</title><host>Discrete Appl. Math.156 (13)(July 2008) pp.2561-2572</host></reference><reference label="[10]"><authors>H. Chen,C. Chu,J.-M. Proth</authors><title>Cyclic scheduling of a hoist with time window constraints</title><host>IEEE Trans. Robot. Autom.14 (1)(Feb. 1998) pp.144-152</host></reference><reference label="[11]"><authors>J.M. Codina,J. Llosa,A. González</authors><title>A comparative study of modulo scheduling techniques</title><host>Proc. of ICS ʼ02(2002) pp.97-106</host></reference><reference label="[12]"><authors>A. Dasdan</authors><title>Experimental analysis of the fastest optimum cycle ratio and mean algorithms</title><host>ACM Trans. Des. Autom. Electron. Syst.9 (4)(October 2004) pp.385-418</host></reference><reference label="[13]"><authors>D.L. Draper,A.K. Jonsson,D.P. Clements,D.E. Joslin</authors><title>Cyclic scheduling</title><host>Proc. of IJCAI(1999)Morgan Kaufmann Publishers Inc. pp.1016-1021</host></reference><reference label="[14]"><authors>B. Dupont de Dinechin</authors><title>From machine scheduling to VLIW instruction scheduling</title><host>ST J. Res.1 (2)(2004) pp.1-35</host></reference><reference label="[15]"><authors>A.E. Eichenberger,E.S. Davidson</authors><title>Efficient formulation for optimal modulo schedulers</title><host>ACM SIGPLAN Not.32 (5)(1997) pp.194-205</host></reference><reference label="[16]"><authors>L. Georgiadis,A.V. Goldberg,R. Tarjan,R.F. Werneck</authors><title>An experimental study of minimum mean cycle algorithms</title><host>Proc. of ALENEX(2009) pp.1-13</host></reference><reference label="[17]"><authors>A. Ghamarian,M. Geilen,T. Basten,B. Theelen,M.R. Mousavi,S. Stuijk</authors><title>Liveness and boundedness of synchronous data flow graphs</title><host>Proc. of FMCAD(2006) pp.68-75</host></reference><reference label="[18]"><authors>M. Hagog,A. Zaks</authors><title>Swing modulo scheduling for gcc</title><host>Proc. of the 2004 GCC Developersʼ Summit(2004) pp.55-</host></reference><reference label="[19]"><authors>C. Hanen</authors><title>Study of a NP-hard cyclic scheduling problem: The recurrent job-shop</title><host>Eur. J. Oper. Res.72 (1)(1994) pp.82-101</host></reference><reference label="[20]"><authors>R.A. Huff</authors><title>Lifetime-sensitive modulo scheduling</title><host>Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation, vol. 28(June 1993)ACM pp.258-267</host></reference><reference label="[21]"><authors>K. Ito,K.K. Parhi</authors><title>Determining the minimum iteration period of an algorithm</title><host>J. VLSI Signal Process.244 (3)(1995) pp.229-244</host></reference><reference label="[22]"><authors>M. Kudlur,S. Mahlke</authors><title>Orchestrating the execution of stream programs on multicore platforms</title><host>Proc. of PLDI, vol. 43(May 2008) pp.114-124</host></reference><reference label="[23]"><authors>C. Le Pape,P. Couronné</authors><title>Time-versus-capacity compromises in project scheduling</title><host>Proc. of the 13th Workshop of the UK Planning Special Interest Group(1994)</host></reference><reference label="[24]"><authors>E.A. Lee,D.G. Messerschmitt</authors><title>Static scheduling of synchronous data flow programs for digital signal processing</title><host>IEEE Trans. Comput.36 (1)(January 1987) pp.24-35</host></reference><reference label="[25]"><authors>E. Levner,V. Kats,D. Alcaide López de Pablo,T.C.E. Cheng</authors><title>Complexity of cyclic scheduling problems: A state-of-the-art survey</title><host>Comput. Ind. Eng.59 (2)(September 2010) pp.352-361</host></reference><reference label="[26]"><authors>J. Llosa,A. Gonzalez,E. Ayguade,M. Valero</authors><title>Swing modulo scheduling: A lifetime-sensitive approach</title><host>Proc. of PACT(1996)IEEE Computer Society pp.80-87</host></reference><reference label="[27]"><authors>J. Llosa,A. Gonzalez,E. Ayguade,M. Valero,J. Eckhardt</authors><title>Lifetime-sensitive modulo scheduling in a production environment</title><host>IEEE Trans. Comput.50 (3)(2001) pp.234-249</host></reference><reference label="[28]"><authors>M. Lombardi,A. Bonfietti,M. Milano,L. Benini</authors><title>Precedence constraint posting for cyclic scheduling problems</title><host>Proc. of CPAIOR(2011) pp.137-153</host></reference><reference label="[29]"><authors>M. Lombardi,M. Milano</authors><title>A min-flow algorithm for minimal critical set detection in resource constrained project scheduling</title><host>Artif. Intell.182–183 (2012) pp.58-67</host></reference><reference label="[30]"><authors>S.T. McCormick,M.L. Pinedo</authors><title>Transient behavior in a flexible assembly system</title><host>Int. J. Flex. Manuf. Syst.3 (1990)(1991) pp.27-44</host></reference><reference label="[31]"><authors>S.T. McCormick,U.S. Rao</authors><title>Some complexity results in cyclic scheduling</title><host>Math. Comput. Model.20 (2)(July 1994) pp.107-122</host></reference><reference label="[32]"><authors>K.K. Parhi,D.G. Messerschmitt</authors><title>Rate-optimal fully-static multiprocessor scheduling of data-flow signal processing programs</title><host>Proc. of IEEE International Symposium on Circuits and Systems, vol. 217(1989)IEEE pp.1923-1928</host></reference><reference label="[33]"><authors>K.K. Parhi,D.G. Messerschmitt</authors><title>Static rate-optimal scheduling of iterative data-flow programs via optimum unfolding</title><host>IEEE Trans. Comput.40 (2)(1991) pp.178-195</host></reference><reference label="[34]"><authors>N. Policella,A. Cesta,A. Oddi,S.F. Smith</authors><title>From precedence constraint posting to partial order schedules: A csp approach to robust scheduling</title><host>AI Commun.20 (3)(2007) pp.163-180</host></reference><reference label="[35]"><authors>R.B. Rau</authors><title>Iterative modulo scheduling: An algorithm for software pipelining loops</title><host>Proc. of the 27th Annual International Symposium on Microarchitecture(1994)ACM pp.63-74</host></reference></references><footnote><note-para label="1">Heuristic-based Modulo Scheduling is a family of Software Pipelining techniques that produce effective schedules with a relatively small compilation time. These Modulo Scheduling techniques take as input the application to be scheduled represented by its data dependence graph and a description of the architecture and produce a schedule for the application.</note-para><note-para label="2">The method can take advantage of the use of bidirectional scheduling method as the period λ is fixed.</note-para><note-para label="3">As stated in Section 2.3.1 the iteration bound IB is a valid lower bound for the modulus.</note-para><note-para label="4">For ensuring the termination of the algorithm the period λ is considered as integer value.</note-para><note-para label="5">The Trigger and the Coherence algorithms have an asymptotic complexity of {a mathematical formula}O(n⋅logn), where n is the number of the activities.</note-para><note-para label="6">In case {a mathematical formula}di is not integer, then {a mathematical formula}di⁎=di−λ⋅⌊diλ⌋.</note-para><note-para label="7">Note that the list corresponds to the set of all feasible start times coinciding with the modular end times of the scheduled activities. These instants correspond to the start times of all the feasible schedulability windows of the considered activity (see Section 6.3.1 for further details).</note-para><note-para label="8">In a preliminary experimentation the solver was found to typically provide the best solution in less than a second. Hence a 300 s time limit should be widely sufficient for the method to converge to best solution it can find.</note-para><note-para label="9">For both {a mathematical formula}CROSS⁎ and SMS, the quality gap is computed with the following formulation: {a mathematical formula}100⁎(Sol−ILP_Opt)/ILP_Opt, where {a mathematical formula}ILP_Opt⩽Sol.</note-para></footnote></root>