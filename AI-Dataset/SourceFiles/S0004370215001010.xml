<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370215001010</url><title>Relational linear programming</title><authors>Kristian Kersting,Martin Mladenov,Pavel Tokmakov</authors><abstract>We propose relational linear programming, a simple framework for combining linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logic program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages such as AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems, in particular over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines.</abstract><keywords>Machine learning;Optimization;Relational logic;Statistical relational learning;Linear programming;Symmetry;(Fractional) automorphism;Color-refinement;Lifted probabilistic inference</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>Modern social and technological trends result in an enormous increase in the amount of accessible data, with a significant portion of the resources being interrelated in a complex way and having inherent uncertainty. Such data, which we may refer to as relational data, arises for instance in social network and media mining, natural language processing, open information extraction, the web, bioinformatics, and robotics, among others. Making this data amenable to computing machinery typically yields substantial social and/or business value. Therefore, it is not surprising that probabilistic logical languages{sup:1} are currently provoking much new AI research with tremendous theoretical and practical implications. By combining aspects of logic and probabilities—a dream of AI dating back to at least the late 1980's when Nils Nilsson introduced the term probabilistic logics [5]—they help to effectively manage both complex interactions and uncertainty in the data.</paragraph><paragraph>However, instead of looking at AI through the glasses of probabilities over possible worlds, we may also approach it using optimization. That is, we have a preference relation, i.e., some objective function over possible worlds, and we want a best possible world according to the preference. Consider for example a typical data analyst solving a machine learning problem for a given dataset. She selects a model for the underlying phenomenon to be learned (choosing a learning bias), formats the raw data according to the chosen model, tunes the model parameters by minimizing some objective function induced by the data and the model assumptions, and may iterate the last step as part of model selection and validation. This is an instance of the declarative “Model + Solver” paradigm that was and is prevalent in AI [6], natural language processing [7], machine learning [8], and data mining [9]: instead of outlining how a solution should be computed, we specify what the problem is in terms some high-level modeling language and solve it using general solvers.</paragraph><paragraph>Unfortunately, however, today's solvers for mathematical programs typically require that the program is presented in solver-readable form and/or offer only some very restricted modeling environment. For example, a solver may require that a set of linear constraints be presented as a system of linear inequalities {a mathematical formula}Ax≤b. This can create severe difficulties for the user. First of all, the process of turning the intuition that defines the model “on paper” into a solver-readable form could be quite cumbersome. Consider the following example from graph isomorphism [10]. Given two graphs G and H, the LP formulation introduces a variable for every possible partial function mapping k vertices of G to k vertices in H. It is not a trivial task to come up with a convenient linear indexing of the variables, let alone expressing the resulting inequalities {a mathematical formula}Ax≤b. It requires the user to produce and maintain complicated matrix generation code, which can be tedious and error-prone. Moreover, the reusability of such specialized code is limited, as relatively minor modifications of the equations could require large modifications of the code (for example, the user decides to switch from having variables over sets of vertices to variables over tuples of vertices). Ideally, one would like to separate the rules that generate the problem from the problem instance itself. Finally, solver-readable forms are inherently propositional. By design they cannot model domains with a variable number of individuals and relations among them without enumerating all of them. As already mentioned, however, many AI tasks and domains are best modeled in terms of individuals and relations. Agents must deal with heterogeneous information of all types. Even more important, they must often build models before they know what individuals are in the domain and, therefore, before they know what variables exist. Hence modeling should facilitate the formulation of abstract, general knowledge.</paragraph><paragraph>To overcome these downsides and triggered by the success of probabilistic logical languages, we show that optimization is liftable to the relational level too. Specifically, we lift linear programs—the most tractable, best understood, and widely used in practice fragment of mathematical programs—by introducing relational linear programs (RLPs). They are declarative LP templates defined through the logical concepts of individuals, relations, and quantified variables, and allow the user to express LP objectives and constraints for a varying number of individuals without enumerating them. For instance, the following code{a mathematical formula} encodes the conservation of flows for all vertices of a graph that are not source or target. Together with a logical knowledge base (LogKB) referring to the individuals and relations (effectively a logical program consisting of logical facts and rules), it induces a ground LP that can be solved using any LP solver. However, since RLPs consist of templates that are instantiated several times to construct a ground linear model, they are also likely to induce ground models that exhibit symmetries, and we will demonstrate how to detect and exploit them. As our main technical contribution, we introduce lifted linear programming (LLP). It detects symmetries in a linear program in quasilinear time and eliminates them by reparametrizing the original linear program into a smaller one sharing optimal solutions that can be computed using any LP solver.</paragraph><paragraph>Both contributions together result in relational linear programming, best summarized as{a mathematical formula} The user describes a relational problem in a high-level, relational LP modeling language and—given a logical knowledge base (LogKB) encoding some individuals or rather data—the system automatically compiles a symmetry-reduced LP that in turn can be solved using any off-the-shelf LP solver.</paragraph><paragraph>Our empirical illustrations on several AI tasks such as computing optimal value-functions of Markov decision processes [11], approximate inference within Markov logic networks [12] using LP relaxations, and collective classification [13] demonstrate that relational linear programming can</paragraph><list><list-item label="•">ease the process of turning the “modeler's form”, i.e., the form in which the modeler understands a problem or actually a class of problems (since we can now deal with a varying number of individuals and relations among them), into a solver-readable form, and</list-item><list-item label="•">considerably reduce the time spent to compute solutions.</list-item></list><paragraph> The present paper is a significant extension of a previously published conference paper [14]. It provides a much more concise development of LLP and the first coherent view on relational linear programming as a novel and promising way for scaling AI by developing and showcasing the first modeling language for LPs based on logic programming. One of the advantages of the language is the closeness of its syntax to the mathematical notation of LP problems while supporting language elements from logic programming such as individuals, relations, and quantified variables. This allows for a very concise and readable relational definition of linear optimization problems in a general, declarative fashion; the (relational) algebraic formulation of a model does not contain any hints how to process it.</paragraph><paragraph>We proceed as follows. We start off by reviewing linear programming and existing LP template languages in Section 2. Then, in Section 3, we introduce relational linear programming, both the syntax and the semantics. Afterwards, Section 4 shows how to detect and exploit symmetries in linear programs. Section 5 illustrates relational linear programming empirically on several examples from machine learning and AI. Before concluding, we finally touch upon related work and directions for future work.</paragraph></section><section label="2"><section-title>Linear programming</section-title><paragraph>A linear program (LP) [15] is an optimization problem that can be expressed in the following general form:{a mathematical formula}{a mathematical formula}{a mathematical formula} where {a mathematical formula}A∈Rm×n and {a mathematical formula}G∈Rp×n are matrices, b, c and h are real vectors of dimension {a mathematical formula}m,n, and p respectively, and {a mathematical formula}&lt;⋅,⋅&gt; denotes the scalar product of two vectors. Note that the equality constraints can be reformulated as inequality constraints to yield an LP in a form containing only inequalities,{a mathematical formula} which we represent by the triplet {a mathematical formula}L=(A,b,c).</paragraph><paragraph>LPs have found a wide application in the fields of operations research, where they are applied to problems like multicommodity flow and optimal resource allocation, and combinatorial optimization, where they provide the basis for many approximation algorithms for hard problems such as TSP. They have also found their way to machine learning and AI. For instance, they have been used for classification [16], [17], [18], for structured prediction [19], as subroutines in collective classification approaches [20], [21], for efficient approximate MAP inference (see e.g. [22]) for finding the optimal policy of Markov decision problems [23], [24], for inverse reinforcement learning [25], and clustering and dimensionality reduction [26], [27].</paragraph><paragraph>While LP models often look intuitive on paper, applying them in practice presents a challenge. The main issue is the following gap: The solver-readable form of an LP—the L-triplet representation as shown above—is not the form that is natural for users. Furthermore, the matrix A is typically sparse, i.e., having mostly 0-entries. Consequently, modeling any real world problem in the solver-friendly L-form can be quite error prone and time consuming. This might explain why LPs are typically specified in a more declarative, abstract way that we will discuss next.</paragraph><section label="2.1"><section-title>Declarative modeling languages for linear programs</section-title><paragraph>Linear programs are often presented in a declarative, abstract way, which separates the general structure of a problem at hand from the actual instance. As an example, consider maximizing the total flow in a network [28]. The problem is, given a finite directed graph {a mathematical formula}G(V,E) in which every edge {a mathematical formula}(u,v)∈E has a non-negative capacity {a mathematical formula}c(u,v), and two vertices s and t called source and target, to maximize a function {a mathematical formula}f:V×V→R, called flow. In order to be a valid flow, f must be subjected to the following constraints: {a mathematical formula}f(u,v)≤c(u,v),f(u,v)≥0, and{a mathematical formula} The latter constraint asserts that incoming flow equals to outgoing flow for internal vertices, that is, no “goods” are created or destroyed in nodes that are not the source or the sink. To build the flow LP for a given network, we first introduce variables to represent flow over each edge of the network. Then, we formulate the capacity constraints and conservation constraints depending on the network structure. Finally, the objective function of the LP is the yields the value of the total flow in the network.</paragraph><paragraph>As one can see, the general definition of the flow LP can be separated from the specification of the network at hand and, in turn, be applied to different networks. Such a declarative, abstract modeling is realized using algebraic modeling languages (see Section 6 for references). They simplify LP definitions by allowing one to use algebraic notation instead of matrices to define the objective and the constraints through parameters whose domains are defined in a separate file, thus enabling model/instance separation.</paragraph><paragraph>Let us have a look at the L-form. Starting from (1), algebraic modeling languages typically make the involved arithmetic expressions explicit:{a mathematical formula} where the sets P and K, as well as the corresponding non-zero entries of vectors c, b and matrix A are defined in a separate file. This simplified representation is then automatically translated to the matrix format and fed to a solver that the user prefers.</paragraph><paragraph>To code the LP in this “set form”, several mathematical programming modeling languages have been proposed. According to NEOS solver statistics,{sup:2} AMPL is the most popular one. We only briefly review the basic AMPL concepts. For more details, we refer to [29], [30].</paragraph><paragraph>Based on the “set form”, an LP can be written in AMPL as shown in Fig. 1. In principle, an AMPL program consists of one objective (a line starting with a {an inline-figure} or {an inline-figure} keyword), and a number of ground or indexed constraints (lines starting with a {an inline-figure} keyword). If a constraint is indexed (i.e. the constraint in the example above is indexed by the set K), a ground constraint is generated for every combination of values of the indexing variables (in the example above, there is just one index variable in the constraint, hence a ground constraint is generated for every value in K). The keyword {an inline-figure} declares a set name, whose members are provided in a separate file. The keyword {an inline-figure} declares a parameter, which may be a single scalar value or a collection of values indexed by a set. Subscripts in algebraic notation are written in square brackets as in b[i] instead of {a mathematical formula}bi. The values to be determined by the solver are defined by the {an inline-figure} keyword. The typical ∑ symbol is replaced by the {an inline-figure} keyword. The key element of the AMPL system are the so-called indexing expressions:{a mathematical formula} In addition to being part of variable/parameter declaration, indexing expressions serve both as limits for sums and as indices for constraints. Finally, comments in AMPL start with the symbol {an inline-figure}.</paragraph><paragraph>To illustrate AMPL, Fig. 2 shows the flow problems formulated in AMPL. The program starts with a definition of all sets, parameters and variables that appear in it. They are then used to define the objective and the constraints from the network flow problem, in particular the third one. The first two constraints are incorporated into the variable definition. As one can see, AMPL allows one to write down the problem description in a declarative way. It frees the user from engineering instance specific LPs while capturing the general properties of the problem class at hand.</paragraph><paragraph>Unfortunately, AMPL does not provide logically parametrized definitions for the arithmetic expressions and for the index sets. It is difficult to use the logical formula {a mathematical formula}ϕ(X)≡∃Ysource(S)∧edge(S,Y)∧edge(Y,X)∧¬source(X) to compactly code the set of all nodes that have a distance of two from the source node within a network. Relational linear programs (RLPs), which we will introduce now, feature exactly this. They allows us to keep AMPL's benefits that make it the number one choice for optimization experts and at the same time enable logical constructions that are established and effective tools for dealing with relational problems.</paragraph></section></section><section label="3"><section-title>Relational linear programs</section-title><paragraph>The main idea is to parameterize AMPL's arithmetic expressions by logical variables and to replace AMPL's indexing expression by queries to a logical knowledge base. Before showing how to do this, let us briefly review logic programming. For more details we refer to [2], [31], [32].</paragraph><section label="3.1"><section-title>Logic programs</section-title><paragraph>A logic program is a set of clauses constructed using four types of symbols: constants, variables, functors, and predicates. Say we want to model the smoking habits of people. In addition to the usual “flat” data about attributes of people like age, education and smoking habits, we have access to the social network among the people, cf. Fig. 3. Formally speaking, we have that {a mathematical formula}attribute/2 and {a mathematical formula}friends/2 are predicates (with their arity, i.e., number of arguments listed explicitly). The symbols anna, bob, edward, frank, gary, helen, iris are constants and X, and Y are variables. All constants and variables are also terms. In addition, one can also have structured terms such as the functor{a mathematical formula}s(X) of arity 1, which contains the function symbols and the term X. Atoms are predicate symbols followed by the necessary number of terms, e.g., {a mathematical formula}friends(bob,anna), {a mathematical formula}nat(s(X)), {a mathematical formula}attribute(X,passive), etc. Literals are atoms {a mathematical formula}nat(s(X)) (positive literal) and their negations {a mathematical formula}notnat(s(X)) (negative literals). We are now able to define the key concept of a clause. They are formulas of the form{a mathematical formula} where A (the head) and the {a mathematical formula}Bj (the body) are literals and all variables are understood to be universally quantified. For instance, the clause {a mathematical formula}c≡attribute(X,passive):−friends(X,Y),attribute(Y,smokes) can be read as X has attribute passive if X and Y are friends and Y has the attribute smokes. Clauses with an empty body are facts. A logic program consists of a finite set of clauses. The set of variables in a term, atom, conjunction or clause E, is denoted as {a mathematical formula}Var(E), e.g., {a mathematical formula}Var(c)={X,Y}. A term, atom or clause E is ground when there is no variable occurring in E, i.e. {a mathematical formula}Var(E)=∅.</paragraph><paragraph>A substitution{a mathematical formula}θ={V1/t1,…,Vn/tn}, e.g. {a mathematical formula}{Y/anna}, is an assignment of terms {a mathematical formula}ti to variables {a mathematical formula}Vi. Applying a substitution θ to a term, atom or clause e yields the instantiated term, atom, or clause eθ where all occurrences of the variables {a mathematical formula}Vi are simultaneously replaced by {a mathematical formula}ti, e.g. {a mathematical formula}c{Y/anna} is {a mathematical formula}attribute(X,passive):−friends(X,anna),attribute(anna,smokes). The Herbrand base of a logic program P, denoted as {a mathematical formula}hb(P), is the set of all ground atoms constructed with the predicate, constant and function symbols in the alphabet of P. A Herbrand interpretation for a logic program P is a subset of {a mathematical formula}hb(P). A Herbrand interpretation I is a model of a clause c if and only if for all substitutions θ such that {a mathematical formula}body(c)θ⊆I holds, it also holds that {a mathematical formula}head(c)θ∈I. A clause c (logic program P) entails another clause {a mathematical formula}c′ (logic program {a mathematical formula}P′), denoted as {a mathematical formula}c⊨c′ ({a mathematical formula}P⊨P′), if and only if, each model of c (P) is also a model of {a mathematical formula}c′ ({a mathematical formula}P′). The least Herbrand model{a mathematical formula}LH(P), which constitutes the semantics of the logic program P, consists of all facts {a mathematical formula}f∈hb(P) such that P logically entails f, i.e. {a mathematical formula}P⊨f.</paragraph><paragraph>A queryq is of the form {a mathematical formula}B1,…,Bm where the {a mathematical formula}Bj are literals and all variables are understood to be existentially quantified. Given a logic program P, a correct answer to the query q is a substitution θ such as that qθ is entailed by P. That is, qθ is true in {a mathematical formula}LH(P). The answer substitution θ is often computed using SLD-resolution. Finally, the answer set of a q is the set of all correct answers to q.</paragraph><paragraph>Logic programming is especially convenient for representing relational data such as our social network from Fig. 3. All one needs is the binary predicate friends/2 to encode the edges in the social graph as well as the predicates {a mathematical formula}attribute(X,Attr) to code the attributes (name, age, etc.) of the people in the social network.</paragraph></section><section label="3.2"><section-title>Parametrizing linear programs using logic programs</section-title><paragraph>Since our language can be seen as a logic programming variant of AMPL, we introduce its syntax in a contrast to the AMPL syntax.</paragraph><paragraph>A first important thing to notice is that AMPL mimics arithmetic notation in its syntax as much as possible. It operates on sets, intersections of sets and arithmetic expressions indexed with these sets. Our language for relational linear programming effectively replaces these constructs with logical predicates, clauses, and queries to define the three main parts of an RLP: the objective template, the constraints template, and a logical knowledge base.</paragraph><paragraph>To illustrate this, let us reconsider the network flow problem as running example. An RLP for the flow example is shown in Fig. 4. It directly codes the flow constraints, concisely captures the essence of flow problems, and illustrates nicely that linear programming can be viewed as being highly relational in nature. Let us now discuss this program line by line.</paragraph><section label="3.2.1"><section-title>Logically parametrized algebraic expressions</section-title><paragraph>LP predicates define logically parametrized sets of variables and parameters in the LP. In our flow running example, flow/2 captures for instance the flows between nodes. Sets that are explicitly defining domains in AMPL are discarded and parameter/variable domains are defined implicitly. In contrast to logic, (ground) LP atoms can take any numeric value, not just true or false. For instance the flow between the nodes is captured by flow/2, and the specific flow between node f and t could take the value 3.7, i.e., flow(f, t) = 3.7. Logically parameterized LP variables, par-vars for short, are values to be determined by the solver, and we follow AMPL's notation in lines 1–2:{a mathematical formula} The rest of the program specifies the objective (line 4) and the constraints (lines 5–17) of the flow problem. For this, we use logically parameterized algebraic expressions, or par-expressions in short.</paragraph><paragraph>Par-expressions are expressions of finite length of the form{a mathematical formula} Here {a mathematical formula}ψi are numeric constants, atoms or par-expressions, and the {a mathematical formula}opj are arithmetic operators ‘+’, ‘−’, or ‘⋅’. The term {a mathematical formula}{an inline-figure}{X{an inline-figure}ϕ}, which is optional, essentially implements the AMPL aggregation sum but now indexed over the variables X that are a subset of the logical variables appearing in the logical query ϕ and in the LP atoms appearing in the {a mathematical formula}ψi's; let {a mathematical formula}atoms(ψ) denote this set. That is, the AMPL indexing expression {j in P} for the sum is turned into an indexing with respect to X. For those readers that a familiar with Prolog, essentially, one can think of this as calling the Prolog meta-predicate{a mathematical formula} treating {a mathematical formula}atoms(ψ) as a conjunction over the involved atoms. This will produce the set P of all substitutions of the variables X (with any duplicate removed) such that the query {a mathematical formula}ϕ∧atoms(ψ) is satisfied. In case, we are interested in multi-sets, i.e., to express counts, one may also use {a mathematical formula}findall/3. This could be expressed using {a mathematical formula}{an inline-figure}〈ϕ〉 instead of {a mathematical formula}{an inline-figure}{ϕ}. The {an inline-figure} aggregation and the involved par-expression is then evaluated over the resulting multidimensional index P. If no {an inline-figure} is provided, this will just be logical indexing for the evaluation of the par-expression {a mathematical formula}ψ1op1ψ2op2…opn−1ψn.</paragraph><paragraph>Now, an objective is a par-expression without free logical variables. In our flow running example, the objective{sup:3} reads e.g.{a mathematical formula} This says that we want to maximize the outflows for all source nodes. Note that we assume no free logical variables to avoid producing multiple and conflicting objectives.{sup:4}</paragraph><paragraph>With par-expressions at hand, we can introduce par-equalities{sup:5} and par-inequalities for specifying constraints. They are finite-length expression of the form{a mathematical formula} where {a mathematical formula}ϕ1 is a par-expression and op denotes one of the operators ‘=’, ‘&lt;’, ‘&gt;’, ‘&lt;=’, and ‘&gt;=’. Without loss of generality, we can assume the right hand side to be 0; if not we just move it to the left hand side by subtraction. We assume that all par-(in)equalities are all-quantified, which bounds all free variables not already bound by a {an inline-figure} statement. This is indicated by{a mathematical formula} where X denotes all the free variables of the query ϕ and {a mathematical formula}atoms(ψ). For each tuple in this indexed set, we then evaluate the par-(in)equality ψ. This way, constraints may lead to several ground instances.</paragraph><paragraph>In our flow running example, the constraints for the auxiliary LP predicate outflow/2 is{a mathematical formula} Since the logical variable Y is bound by the summation for outflow/1 and the logical variable X by the all-quantification, this says that there will be one equality expression as constraint per node summing over all flows of the outgoing edges. Likewise we can define the auxiliary LP predicate inflow/2 summing over all ingoing edges:{a mathematical formula} The remaining constraints are defined in a similar fashion:{a mathematical formula}</paragraph><paragraph>Already this simple flow example illustrates the power of RLPs. Since indexing expressions are logical queries, we can naturally express things that either look cumbersome in AMPL or even go beyond its capabilities. For instance the concept of internal edge (line 2 in Fig. 2), which is explicitly represented by a lengthy expression with sets intersections and differences in AMPL, is compactly represented using the logical query vertex(X), not source(X), not target(X) in the par-equations involving in-/outflow (line 12 in Fig. 4). As the reader may have noticed, several predicates such as vertex/1 and source/1 are not defined in the flow RLP. In such cases, we assume the predicate to be defined in a logical knowledge base LogKB that we will discuss next.</paragraph></section><section label="3.2.2"><section-title>The logical knowledge base</section-title><paragraph>Every predicate not defined to be a par-var is assumed to be a logically parameterized LP parameter, par-param for short, defined in a (possibly external{sup:6}) logical knowledge base LogKB. We here use Prolog but assume that each query from the RLP produces a finite set of answers, i.e., ground substitutions of its logical variables rendering it to be true.</paragraph><paragraph>Fig. 5 illustrates an instance for the maximal flow problem. It can be expressed using the following LogKB:{a mathematical formula} where cap(s, a) = 4 is short-hand notation for cap(s,a,4) and cap(X, Y) for cap(X,Y,_), where we use an anonymized variable{sup:7} ‘_’. The predicates edge/2 and vertex/1 are logical predicates taking the values 1 or 0 (corresponding to true and false) within the RLP. For instance edge(X,Y) is true for {X/s,Y/a}, {X/a,Y/s}, {X/s,Y/b}, {X/b,Y/s}, … within the LogKB and, hence, they take the value 1 in the RLP. Finally, indeed, querying for a vertex could result in a multi-set as answer, since the definition of vertex/1 tests for a vertex as the source and the target of an edge. However, recall that a logical indexing statement {...} removes any duplicates first.</paragraph><paragraph>Putting everything together, a relational mathematical program can be defined as follows.</paragraph><paragraph label="Definition 1">A relational mathematical program consists of (1) a finite set of par-var declarations of predicates, (2) one par-expression to define the objective, and (3) a finite set of par-(in)equalities to define the constraints. Everything not explicitly defined is assumed to be a par-param defined in an external LogKB.</paragraph><paragraph>Finally, relational linear programs (RLPs) are relational mathematical programs where all involved par-expressions are affine. Affine par-expressions can be written (after potentially simplifying them) in the form {a mathematical formula}∑i=1nϕi⋅ψi where the {a mathematical formula}ϕis are known quantities (LP parameters) and the {a mathematical formula}ψis are LP atoms.</paragraph><paragraph>Let us now turn towards the semantics of relational linear programs.</paragraph></section><section label="3.2.3"><section-title>The semantics and grounding</section-title><paragraph>Together with a logical knowledge base LogKB, a relational linear program (RLP) induces a linear program (LP) as long as the logical queries in the RLP have finite answer sets:</paragraph><paragraph label="Theorem 2">An RLP together with a LogKB (such that the logical queries in the RLP have finite answer sets) induces a linear program (LP).</paragraph><paragraph label="Proof">Since there is only one objective and a finite set of constraints, it is sufficient to show that each of them induces only a finite set of ground instances of finite length. We will do so by induction over the depth of their syntax tree. Let us consider a par-equality constraint; all other cases follow the same principles:{a mathematical formula} Assume {a mathematical formula}n=1. There are three cases. The statement is trivially true if {a mathematical formula}ψ1 is a constant (although this case may not really make sense for LPs). If {a mathematical formula}ψ1 is a parameter atom, its logical variables are bound by the logical query ϕ or by the LogKB. By assumption there are only finitely many groundings. Finally, if {a mathematical formula}ψ1 is an LP atom, its logical variables are bound by the logical query ϕ. Again, the answer set of ϕ is assumed to be finite. In both ‘atomic’ cases, this will generate a finite set of ground equality constraints; one for each answer of the free variables of {a mathematical formula}ψ1 unified with ϕ.From this, by induction, it follows that any combination of {a mathematical formula}n&gt;1 many LP atoms and constants using arithmetic operators induces only a finite set of ground instances.What remains are {an inline-figure} statements. If we prove them to be ‘finite’, the theorem would follow. So assume that the par-equation involves {an inline-figure} statements. The first step we do is turning the par equation into a kind of prenex normal form. In logic, a formula is in prenex normal form if it is written as a string of quantifiers followed by a quantifier-free part. For the par-equation at hand, this means that one can be write it as a string of a single {an inline-figure} statement, followed by a string of (zero or more) {an inline-figure} statements followed by a {an inline-figure}-free part, which is correspondingly simplified respectively rewritten. Actually, the block of {an inline-figure} statements can be merged after suitable renaming of the involved logical variables. Hence, without loss of generality, we can assume a single {a mathematical formula}{an inline-figure}{Y{an inline-figure}ϕ′}{ψ} string for some par-expression ψ of length n, which does not include any other {an inline-figure} statement.Now, by our induction base, ψ itself would result in a finite set of ground arithmetic expressions as long as its free variables have been bound. This, however, is the case since by definition all free variables of ψ are bound by the LogKB, by the query {a mathematical formula}ϕ′ of the {an inline-figure} statement or by the logical query ϕ of the leading {an inline-figure} statement. Moreover, again by assumption, the answer set of the leading {an inline-figure} statement is finite. For each of its answers, we connect the resulting ground arithmetic expressions for ψ (each of them are of finite length) by “+”-operators. Doing so captures the semantics of the {an inline-figure} statement. This produces a finite set of arithmetic expressions of finite lengths, which proves the theorem.  □</paragraph><paragraph>The proof of Theorem 2 is constructive and can be turned into a general algorithm for grounding the RLP. We turn all par-(in)equalities and the objective into prenex normal form. Viewing them essentially as logical formulas, we can now ground them inside-out with respect to the {an inline-figure} and {an inline-figure} statements. This can be done using any Prolog engine implementing a meta-interpreter calling e.g. findall and setof in the internal loop. A simple version of this is summarized in Algorithm 1.</paragraph><paragraph>In some cases, as also in some of our experiments, it can be more efficient to use a relational database management system (RDBMS) for grounding a RLP in a forward-chaining way, treating the queries in par-expressions as SQL queries. Niu, Ré, Doan and Shavlik [33] have demonstrated this to be beneficial for inference in Markov logic networks (MLNs) [12]. We essentially follow the same strategy for grounding RLPs. PostgreSQL was used in our experiments, which allows to perform arithmetic computations and string concatenations inside an SQL query, so we are able to get sums of LP variables with corresponding coefficients directly from a query. As a result, the only post processing needed is the concatenation of these strings. Our grounding implementation takes comparable time to Niu et al.'s TUFFY system for MLNs with comparable number of ground predicates to be generated, which is a state-of-the-art performance on this task at the moment.</paragraph></section><section label="3.2.4"><section-title>Relational linear programming and simplifications</section-title><paragraph>To summarize what we have so far, relational linear programming works as summarized in Algorithm 2. We encode the general problem structure of the optimization problem at hand as a relational linear program (line 1). Then we specify a problem instance using an external logical knowledge base LogKB (line 2). Finally, we ground the relational linear program for the problem instance using Algorithm 1 (line 3).</paragraph><paragraph>For illustration, reconsider the flow instance as depicted in Fig. 5. Together with the relational flow LP in Fig. 4, it induces the following ground linear program in AMPL notation (for the sake of readability, only some of the groundings are shown):{a mathematical formula} This illustrates another benefit of relational linear programming. While formulating LPs in canonical form as input to LP solvers requires explicit value enumeration for the A matrix and the b and c vectors, relational linear programming avoids this explicit value enumeration, exploiting the existence of domain objects, relations over these objects, and the ability to express objectives and constraints using quantification. We can just change the LogKB to induce a different flow LP; we do not have to touch the “LP logic” anymore. However, we can make relational linear programming even more concise. Although auxiliary LP predicates such as {a mathematical formula}inflow/1 and {a mathematical formula}inflow/1 indeed increase the readability of relational LPs, they may also unnecessarily blow up the induced LP. Both definitions could have been directly “substituted” in the conservation of flow constraint:{a mathematical formula} This avoids creating the 2n auxiliary LP variables and constraints, where n is the number of nodes in the graph, but also sacrifices the readability of the RLP. To keep the balance between readability and size complexity of the induced LP, we will therefore sometimes make use of what we call “inline definitions”. Inline definitions appear before the objective and do not involve {an inline-figure} statements. There is just a single atom on the left hand side of an equality followed by a par-expression. The idea is that they tell the “compiler” to substitute the right hand side of the definition inline in the objective and constraints by performing inline expansion, i.e., by inserting the right hand side code there by pattern matching thereby saving the overhead of using auxiliary LP variables. Furthermore, we will often drop the {a mathematical formula}X{an inline-figure} parts if X consists of all variables of the query anyhow. Finally, we will often drop the brackets as long as the scope of logical variables is clear and make the all-quantification using {an inline-figure} statements only implicitly. That is, we drop the {an inline-figure} statements; a {a mathematical formula}{ψ} statement corresponds to a {an inline-figure} statement with all variables appearing the query ψ used for indexing. All simplifications are illustrated in the flow RLP shown in Fig. 6.</paragraph><paragraph>However, relational linear programming is not just about concise modeling. One can also efficiently detect and exploit symmetries within the induced LPs and, in turn, speed up solving them. How to do this will be shown next.</paragraph></section></section></section><section label="4"><section-title>Exploiting symmetries for dimension reduction of LPs</section-title><paragraph>As we have already mentioned in the introduction, one of the features of many relational models is that they can produce model instances with a lot of symmetries. These symmetries in turn can be exploited to perform inference at a “lifted” level, i.e., at the level of groups of variables. For probabilistic relational models, this lifted inference can yield dramatic speed-ups, since one reasons about the groups of indistinguishable variables as a whole, instead of treating them individually. Triggered by this success, we will now show that linear programming is liftable, too. To this end, we start off with a high-level discussion of our approach, connecting it to lifted probabilistic inference, followed by the technical results necessary to support our approach.</paragraph><section label="4.1"><section-title>Symmetry detection and compression</section-title><paragraph>In developing our compression method, we are motivated by a conceptual paradigm that has been successful for other inference algorithms such as lifted belief propagation (BP) [34], [35], [36], a message-passing algorithm for approximate inference in Markov random fields (MRFs). From a high-level perspective, one can summarize this paradigm as follows (the more precise definitions will be given later in this section). We start off with a standard inference algorithm and look for conditions (in terms of the specific model being solved) that would make a set of variables behave identically. That is, we look for features of the model that apriori guarantee that a set of variables would maintain equal values throughout the solution process, or at least in the final solution. This identical behavior is what we refer to as symmetry, while the variables subjected to it are referred to as indistinguishable. To take advantage of the discovered symmetry, we finally replace the sets of indistinguishable variables by a representative (also called lifted, or super-) variable. We then reformulate the model carefully (in general, it may also be necessary to modify the inference algorithm as well) so that a solution of the original model can be read off from the reformulated one. If the reformulated model is of smaller size, there is a good chance to solve the problem at hand faster.</paragraph><paragraph>Intuitively, to increase the chance of lower end-to-end solution times, the method for detecting symmetries should be considerably faster than the actual solver; otherwise we may end up spending more time detecting symmetries than we gain from compressing the model. In the following we will present such an efficient approach, more specifically, a quasilinear time approach for lifting linear programs. For the technical discussion we first introduce our notion of symmetry: equitable partitions of (the variables and constraints of) a linear program and their algebraic representations, fractional automorphisms. To justify that equitable partitions are indeed a form of symmetry in the sense above, we prove that LPs admitting nontrivial equitable partitions also admit optimal solutions where all variables belonging to an equivalence class are equal. We then show how to reparametrize an LP given an equitable partition by replacing a set of indistinguishable variables by a single supervariable representing their sum. Unlike lifted BP, where the compressed model is not an MRF anymore, here we actually get a compressed LP, the solutions of which can be transfered back to solutions of the original LP. The compressed LP can be solved by any off-the-shelf LP solver. Finally, we will discuss symmetry detection, i.e., the computation of equitable partitions of LPs.</paragraph><paragraph>The steps outlined above (detecting symmetry, reparametrizing the LP, solving the reparametrized LP and recovering the original solution) constitute Lifted Linear Programming. While we will make use of mostly combinatorial and algebraic concepts along the way, these operations have a geometrical interpretation as well, as illustrated in Fig. 7. Essentially, we identify a lower-dimensional subspace that intersects the feasible region in such a way that an optimal solution is contained within that intersection. To arrive at a smaller LP, we project onto that subspace, and transfer back by computing the inverse image under this projection.</paragraph><paragraph>We now dive into the technical discussion of Lifted Linear Programming.</paragraph></section><section label="4.2"><section-title>Equitable partitions and fractional automorphisms</section-title><paragraph>Let {a mathematical formula}L=(A,b,c) be an LP with {a mathematical formula}A∈Rm×n, that is, with m constraints and n variables. In the following, we aim to partition the variables and constraints into mutually-exclusive classes, which are indistinguishable. Thus, we define a partition of the LP to be the set {a mathematical formula}P={P1,…,Pp;Q1,…,Qq}, where the sets {a mathematical formula}∪i=1pPi={1,…,n}, {a mathematical formula}Pi∩Pj=∅, partition the variables, and the sets {a mathematical formula}∪i=1qQi={1,…,m}, {a mathematical formula}Qj∩Qj=∅, partition the constraints of the LP into (equivalence) classes. Hence, we also require that {a mathematical formula}Pi∩Qj=∅ for all appropriate {a mathematical formula}i,j.</paragraph><paragraph>We say that a partition {a mathematical formula}P={P1,…,Pp;Q1,…,Qq} of {a mathematical formula}L=(A,b,c) is equitable if the following conditions hold.</paragraph><list><list-item label="i.">For any two variables {a mathematical formula}i,j in the same class P, {a mathematical formula}ci=cj. For any two constraints {a mathematical formula}i,j in the same class Q, {a mathematical formula}bi=bj;</list-item><list-item label="ii.">For any two variables {a mathematical formula}i,j in the same class P, and for any constraint class Q and real number c:{a mathematical formula} Analogously, for any two constraints {a mathematical formula}i,j in the same class Q, and for any constraint class P and real number c:{a mathematical formula}</list-item></list><paragraph> In other words, if we fix any class of constraints Q, then the number of constraints in Q where a variable {a mathematical formula}i∈P participates with a coefficient of c should be equal for all other {a mathematical formula}j∈P. The same should hold for any two equivalent constraints and any class of variables.</paragraph><paragraph>It is clear that every LP admits at least one equitable partition, namely the discrete (or trivial) one, where every constraint and variable is in its own singleton class. Whether the LP admits coarser ones depends on its structure. In general, we would like to partition LPs as coarsely as possible in order to gain the highest potential speed ups. How to do that efficiently is discussed later on in Section 4.4. Moreover, equitable partitions of LPs generalize equitable partitions for graphs [37]. To make the connection explicit, we consider an equitable partition of a matrix M to be an equitable partition of the LP {a mathematical formula}(M,0,0), whereas an equitable partition of a bipartite graph G is the equitable partition of its bipartite adjacency matrix.</paragraph><paragraph>Let us illustrate equitable partitions using the following didactic LP{a mathematical formula} together with the LogKB (recall that logical atoms are assumed to evaluate to 0 and 1 within an RLP):{a mathematical formula} If we ground this RLP and convert it to dual form (as in (1)), we obtain the following linear program {a mathematical formula}L0=(A,b,c):{a mathematical formula}{a mathematical formula} where for brevity we have substituted {a mathematical formula}p(x), {a mathematical formula}p(y), {a mathematical formula}p(z) by {a mathematical formula}x,y,z respectively. We claim now that the partition {a mathematical formula}P0={{1,2},{3};{1}{2,3}{4}}—meaning that x is equivalent to y but not to z and the second and third constraint are equivalent, but not the first and the fourth—is an equitable partition of {a mathematical formula}L0. Let us verify that. First, we have that {a mathematical formula}c1=c2≠c3 and {a mathematical formula}b2=b3≠b1≠b4 (condition (i)). We now need to verify condition (ii). For the sake of illustration we will do this only for the class {a mathematical formula}P={1,2}. Consider {a mathematical formula}Q={1}. Since Q is a singleton, condition (ii) reduces to whether {a mathematical formula}A11=A12=c, which happens to be the case in our example, as we have only {a mathematical formula}c=1. The same holds for {a mathematical formula}Q={4}. For {a mathematical formula}Q={2,3}, we need to check the condition for {a mathematical formula}c=0 and {a mathematical formula}c=−1. We have{a mathematical formula} as well as{a mathematical formula} One can easily complete the counting argument for all other pairs of classes.</paragraph><paragraph>So far, we have introduced equitable partitions of linear programs. In order to show that LP variables grouped together by an equitable partition assume equal values in some optimal solution of the LP, we have to introduce one more bit of mathematical machinery, which makes the interplay between equitable partitions apparent: the notion of fractional automorphism due to Ramana, Scheinerman and Ullmann [38].{sup:8}</paragraph><paragraph>Let M be an {a mathematical formula}m×n real matrix. A fractional automorphism of M is a pair of doubly stochastic (meaning that the entries are non-negative, and every row and column sum to one) matrices {a mathematical formula}XP,XQ such that{a mathematical formula} The following theorem establishes the correspondence between equitable partitions and fractional automorphisms.</paragraph><paragraph label="Theorem 3">(See[38], [39].) Let M be a rectangular real matrix. Then:</paragraph><list><list-item label="i)">if{a mathematical formula}P={P1,…,Pp;Q1,…,Qq}is an equitable partition of M (i.e., of the LP{a mathematical formula}(M,0,0)), then the matrices{a mathematical formula}XP,XQ, having entries{a mathematical formula}is a fractional automorphism of M.</list-item><list-item label="ii)">conversely, let{a mathematical formula}XP,XQbe a fractional automorphism of the matrix M. Then the partition{a mathematical formula}P, where columns{a mathematical formula}i,jbelong to the same{a mathematical formula}P∈Pif and only if at least one of{a mathematical formula}(XP)ijand{a mathematical formula}(XP)jiis greater than 0, respectively rows{a mathematical formula}i,jbelong to a class Q if{a mathematical formula}(XQ)ijor{a mathematical formula}(XQ)jiis greater than 0, is an equitable partition of M.</list-item></list><paragraph>In particular, we require part i) of Theorem 3 since encoding equitable partitions of LPs as fractional automorphisms will provide us with insights into the geometrical aspects of lifting and will be an essential tool for proving its soundness. Before we continue with our review of the mathematical background for lifted linear programming, we make one additional observation.</paragraph><paragraph label="Proposition 4">Let{a mathematical formula}Pbe an equitable partition of{a mathematical formula}(A,b,c)and{a mathematical formula}XP,XQbe the matrices according to(3). Then,{a mathematical formula}cTXP=cTand{a mathematical formula}XQb=b.</paragraph><paragraph label="Proof">The proof follows directly from the fact that we group together elements of an LP only if their corresponding c or b entries are equal.  □</paragraph><paragraph>Finally, we note that any matrix/graph partition (equitable or not) can be represented by a doubly stochastic matrix using (3). However, keep in mind that the resulting matrix, which is called partition matrix, will not be a fractional automorphism unless the partition is equitable. In any case, partition matrices have a useful property that will later on allow us to reduce the number of constraints and variables of a linear program. More precisely:</paragraph><paragraph label="Proposition 5">(See[40].) Let X be a doubly stochastic matrix produced by some partition{a mathematical formula}Paccording to(3). Then{a mathematical formula}X=B˜B˜Twith{a mathematical formula}</paragraph><paragraph>Before starting to develop lifted linear programming, let us illustrate fractional automorphisms and partition matrices. Reconsider the equitable partition {a mathematical formula}P0={{1,2},{3};{1}{2,3}{4}} of the LP {a mathematical formula}L0 in our running example. The fractional automorphism of {a mathematical formula}L0 induced by (3) is{a mathematical formula} and{a mathematical formula} Moreover, Proposition 4 holds since{a mathematical formula} It is easily verified that {a mathematical formula}cTXP=cT and {a mathematical formula}XQb=b hold as well.</paragraph></section><section label="4.3"><section-title>Lifted linear programming</section-title><paragraph>We are now ready to establish lifted linear programs. We split the argument in two parts: first, we will show that if an LP L admits an optimal solution {a mathematical formula}x⁎, then it also admits a solution {a mathematical formula}XPx⁎. Note that if {a mathematical formula}XP is a doubly stochastic matrix, every entry in {a mathematical formula}XPx⁎ is the average value of all the entries in its equivalence class. Hence, as claimed, variables that are indistinguishable according to the equitable partition behave identically. Now, if we add the constraint {a mathematical formula}∃y∈Rn:x=XPy, in other words {a mathematical formula}x∈span(XP), to the linear program, we will not cut away the optimum. The second claim is that instead of adding {a mathematical formula}∃y∈Rn:x=XPy, we can achieve this restriction by projecting the entire LP into the span of {a mathematical formula}XP. Unless {a mathematical formula}P is the discrete partition with all singleton classes, {a mathematical formula}XP will not be full rank, thus the resulting LP will have fewer variables. One can verify that the rank of {a mathematical formula}XP is the number of P-classes of {a mathematical formula}P. So, we project to a low-dimensional space, solve the LP there, and then recover the high-dimensional solution via simple matrix multiplication. This idea is exactly what was illustrated in Fig. 7.</paragraph><paragraph>Let us now state the main result on lifted linear programming:</paragraph><paragraph label="Theorem 6">Let{a mathematical formula}L=(A,b,c)be a linear program and{a mathematical formula}(XP,XQ)be a fractional automorphism of L. Then, it holds that if x is a feasible in L, then{a mathematical formula}XPxis feasible as well and both have the same objective value. As a consequence, if{a mathematical formula}x⁎is an optimal solution,{a mathematical formula}XPx⁎is optimal as well.</paragraph><paragraph label="Proof">Let x be feasible in {a mathematical formula}L=(A,b,c), i.e. {a mathematical formula}Ax≤b. Observe that left multiplication of the system by a doubly stochastic matrix preserves the direction of inequalities. More precisely{a mathematical formula} for any doubly stochastic{sup:9}S. Now, we left-multiply the system by {a mathematical formula}XQ{a mathematical formula} since {a mathematical formula}XQA=AXP and {a mathematical formula}XQb=b. This proves the first part of the Theorem. Finally, observe that {a mathematical formula}cT(XPx)=cTx as {a mathematical formula}cTXP=cT. This proves the second part of the theorem.  □</paragraph><paragraph>We have thus shown that if we add the constraint {a mathematical formula}x∈span(XP) to L, we can still find a solution of the same quality as in the original program. How does this help reducing the dimensionality of the LP?</paragraph><paragraph>To answer this, we observe that the constraint {a mathematical formula}x∈span(XP) can be implemented implicitly, through reparametrization. That is, instead of adding it to {a mathematical formula}L=(A,b,c) explicitly, we take the LP {a mathematical formula}L′=(AXP,b,XPTc). Now, recall that {a mathematical formula}XP was generated by an equitable partition, and it can be factorized as {a mathematical formula}XP=B˜PB˜PT where {a mathematical formula}B˜P is the normalized incidence matrix of {a mathematical formula}{P1,…,Pp}⊂U as in (4). Note that the span of {a mathematical formula}XP=B˜PB˜PT is equivalent (in the vector space isomorphism sense) to the column space of {a mathematical formula}B˜P. That is, every {a mathematical formula}x∈Rn with {a mathematical formula}x∈span(XP) can be expressed as {a mathematical formula}x=B˜Py for some {a mathematical formula}y∈Rp and conversely {a mathematical formula}B˜Py∈span(XP) for all {a mathematical formula}y∈Rp. Hence, we can replace {a mathematical formula}L′=(AXP,b,XPTc) with the equivalent {a mathematical formula}L″=(AB˜P,b,B˜PTc). Since this is now a problem in {a mathematical formula}p≤n variables, i.e., of (potentially) reduced dimension, a speed-up of solving the original LP is possible. Finally, by the above, if {a mathematical formula}y⁎ is an optimal solution of {a mathematical formula}L″, {a mathematical formula}B˜Py is an optimum solution of L.</paragraph><paragraph>Moreover, please observe that after compressing all equivalent variables, the equivalent constraints (i.e., the Q-classes of the partition) become redundant. That is, the rows of {a mathematical formula}AB˜P whose indices were grouped together by the equitable partition of the original LP become identical vectors. Thus, one can achieve additional compression by retaining only one constraint per constraint equivalence class.</paragraph><paragraph>Overall, this proves that the lifted linear programming approach as summarized in Algorithm 3 is sound. Given an LP, we find an equitable partition (line 1). Since the number of classes directly determines the size of the lifted LP, we seek to compute the coarsest such partition; how to do this is the topic of the next subsection. Then, we read off the characteristic matrix as per (4) (line 2). Finally, we solve the lifted LP (line 3) and “unlift” the lifted solution to a solution of the original LP (line 4). Applying this lifted linear programming to LPs induced by RLPs, we can revise relational linear programming as summarized in Algorithm 4.</paragraph><paragraph>Before showing how to compute the coarsest equitable partition, we illustrate how the lifted LP looks like for our running example {a mathematical formula}L0. First, we compute {a mathematical formula}AB˜P as{a mathematical formula} As noted above, the equivalent constraints 2 and 3 have become redundant, so we drop them. If we recompute the c and b vector accordingly, our lifted LP becomes{a mathematical formula}{a mathematical formula} having replaced {a mathematical formula}x+y by the supervariable {a mathematical formula}x.</paragraph></section><section label="4.4"><section-title>Computing fractional automorphisms</section-title><paragraph>So far, we have developed a method for speeding up solving LPs given an equitable partition of their variables and constraints. However, we left open the question of how to find such partitions. Now, we will make up for this. That is, we will now deal with the computational aspect of lifting.</paragraph><paragraph>In graph theory, equitable partitions are well-studied generalizations of orbit partitions [37]—partitions induced by the action of a (sub)group of automorphisms of a graph. That is, an orbit partition is an equitable partition, but not vice versa. While computing the orbit partition of a graph is a hard (GI-complete, to be precise) problem, computing the coarsest equitable partition (CEP) of a graph can be done using an algorithm called color refinement (also known as naive vertex classification). It is a very simple, yet extremely useful algorithmic routine for graph isomorphism testing. It classifies the vertices by iteratively refining a coloring—the colors encode the (equivalence) classes—of the vertices as follows. Initially, all vertices have the same color. Then in each step of the iteration, two vertices that currently have the same color get different colors if for some color c they have a different number of neighbors of color c. The process stops if no further refinement is achieved, resulting in a stable coloring of the graph. If one carefully chooses the order in which the resulting classes are refined, Berkholz, Bonsma and Grohe [41] have shown that color refinement can be realized in time {a mathematical formula}O(|E|+|V|)log⁡(|V|), i.e. quasilinear in the size of the graph with nodes V and edges E. Automorphism group computation tools such as Nauty and Saucy make use of this fact and employ the coarsest equitable partition as a heuristic in orbit computation. As a result, there are highly efficient (both theoretically and empirically) implementations of color refinement.</paragraph><paragraph>To compute the coarsest equitable partition of an LP, we make use of these tools by converting the LP CEP problem to a colored graph CEP problem. Hence, we need a graphical representation of {a mathematical formula}L=(A,b,c), which we call the coefficient graph of L (see Fig. 8), {a mathematical formula}GL. To construct {a mathematical formula}GL, we add a vertex to {a mathematical formula}GL for every of the m constraints and n variables of L. Then, we connect a constraint vertex i and variable vertex j if and only if{a mathematical formula}Aij≠0. Furthermore, we assign colors to the edges {a mathematical formula}{i,j} in such a way that {a mathematical formula}color({i,j})=color({u,v})⇔Aij=Auv. Finally, to ensure that c and b are preserved by any automorphism we find, we color the vertices in a similar manner, i.e., for row vertices {a mathematical formula}i,j{a mathematical formula}color(i)=color(j)⇔bi=bj and {a mathematical formula}color(u)=color(v)⇔cu=cv for column vertices. We must also choose the colors in a way that no pair of row and column vertices share the same color; this is always possible.</paragraph><paragraph>One can verify that an equitable partition of {a mathematical formula}GL yields an equitable partition of L. From the complexity of color refinement and the fact that our graphical construction grows only linearly with the size of the LP, the following theorem holds:</paragraph><paragraph label="Theorem 7">The coarsest (i.e., the one yielding the most compression) equitable partition of an LP is computable in quasilinear time in terms of the number of variables and the size of the constraints, i.e., of the non-zero entries of A.</paragraph><paragraph>Before illustrating relational linear programming empirically, we would like to provide some additional remarks on lifted linear programming.</paragraph></section><section label="4.5"><section-title>Discussion of lifted linear programming</section-title><paragraph>First, indeed lifted linear programming can be applied to any LP, not just those generated by RLP. At first sight, one may consider them to be orthogonal. However, this is not the case. One should rather think about lifted linear programming as the “assembly language” of (relational) linear programming, since RLPs can be grounded and solved as ordinary LPs. Going beyond that, the relational specification of an LP can be used to compute equitable partitions faster, ideally even without grounding. Such an approach was demonstrated to be beneficial by Apsel, Kersting and Mladenov [42] for relational MAP inference. Under certain restrictions on the language, this could be generalized to RLPs. Another example for language-induced symmetries are the renaming groups due to Bui, Huynh and Riedel [43]. While these approaches make heavy use of relational representations, they are still based on the basic fact that the underlying ground problem is liftable. This is the perspective we choose to adapt for RLPs in the present work. The interaction between languages and equitable partitions is a very promising avenue for future research.</paragraph><paragraph>Second, recall that our compression method works with any equitable partition, not only the coarsest one (which happens to be efficiently computable). As mentioned, an equitable partition of a graph—the orbit partition—can be constructed out of its automorphism group, i.e., the set of all possible ways in which we can rename the vertices and get the same graph back. The orbit partition groups two vertices whenever there exists an automorphism that maps one to the other. Applying this partitioning method to coefficient graphs of linear programs and using the corresponding fractional automorphism is equivalent to previous theoretical well-known results in solving linear programs under symmetry (see e.g. [44] and references therein). Note that there are major benefits for using the color refinement partition instead of the orbit partition for linear programs:</paragraph><list><list-item label="•">The color refinement partition is at least as coarse as the orbit partition as shown in [37]. To illustrate this, consider the so-called Frucht graph as shown in Fig. 9. Suppose we turn this graph into a linear program by introducing constraint nodes along the edges and coloring everything with the same color. The Frucht graph has two extreme properties with respect to equitable partitions: 1) it is asymmetric, meaning that the orbit partition is trivial having one vertex per equivalence class; 2) it is regular (every vertex has degree 3); as one can easily verify, in this case the coarsest equitable partition consists of a single class! Due to these two properties, the orbit partition yields no compression on the Frucht graph, whereas the coarsest equitable resp. color refinement partition produces an LP with a single variable.</list-item><list-item label="•">The color refinement partition can be computed in quasilinear time, yet current tools for orbit partition enumeration may have significantly worse running times: although computing orbit-partitions may indeed be practical in a number of cases, computing them is a GI-complete problem. Thus, by using color refinement we achieve strict gains in both compression and efficiency compared to using orbits.</list-item></list><paragraph> Finally, the color refinement algorithm is essentially the lifting algorithm of lifted BP as formulated by Kersting, Ahmadi and Natarajan [35]. Yet, there are remarkable differences between lifted BP and lifted LPs. For example, after lifting, a lifted MRF is no longer an MRF in the classical sense since one needs a special data structure to keep track of counts of variables and factors. Hence, one also has to introduce a modified message-passing algorithm. In contrast, a lifted LP is a linear program and no specialized solver is necessary. Moreover, even though the solution found by lifted BP is identical to the one of BP, it need not be a true solution of the inference task (computing the single-node marginals) since BP approximates the task. Thus, the exactness of lifted BP for inference is limited by that of BP. On the other hand, a lifted LP always recovers an exact optimal solution of the corresponding LP. No approximation is involved.</paragraph><paragraph>Let us now turn towards illustrating empirically relational linear programming using several AI tasks.</paragraph></section></section><section label="5"><section-title>Illustrations of relational linear programming</section-title><paragraph>Our intention here is to investigate empirically the viability of the ideas and concepts of relational linear programming through the following questions:</paragraph><list><list-item label="(Q1)">Can important AI tasks be encoded in a concise and readable relational way using RLPs?</list-item><list-item label="(Q2)">Are there (R)LPs that can be solved more efficiently using lifting?</list-item><list-item label="(Q3)">Does relational linear programming enable a programming approach to AI tasks facilitating the construction of more sophisticated models from simpler ones by adding par-constraints?</list-item><list-item label="(Q4)">If lifted linear programming is beneficial, can the benefits be observed for different LP solvers or are they bound to a particular solver?</list-item><list-item label="(Q5)">Is the numerical accuracy of the solver unaffected by lifting?</list-item></list><paragraph> If all question can be answered affirmatively, relational linear programming has the potential to make linear models faster to write and easier to understand, to reduce the development time and cost to encourage experimentation, and in turn to reduce the level of expertise necessary to build AI applications. Consequently, our primary focus is not to achieve the best performance by using advanced models. Instead we will focus on basic models.</paragraph><paragraph>We have implemented a prototype system of relational linear programming, and illustrate the relational modeling of several AI tasks: computing the value function of Markov decision processes, performing MAP inference in Markov logic networks via an LP relaxation and performing collective transductive classification using LP support vector machines.</paragraph><section label="5.1"><section-title>Lifted linear programming for solving Markov decision processes</section-title><paragraph>Our first application for illustrating relational linear programing is the computation of the value function of Markov Decision Problems (MDPs). The LP formulation of this task is as follows [45]:{a mathematical formula} where {a mathematical formula}vi is the value of state i from the set of all states {a mathematical formula}ΩS, {a mathematical formula}cik is the reward that the agent receives when carrying out action {a mathematical formula}k∈K, and {a mathematical formula}pijk is the probability of transferring from state i to state j by taking action k. γ is a discounting factor. The corresponding RLP is given in Fig. 10. Since it abstracts away the states and rewards—they are defined in the LogKB—it extracts the essence of computing value functions of MDPs. Given a LogKB, a ground LP is automatically created instead of coding the LP by hand for each problem instance again and again as in vanilla linear programming. This answers question (Q1) affirmatively.</paragraph><paragraph>The MDP instance that we used is the well-known Gridworld (see e.g. [46]). The gridworld problem consists of an agent navigating within a grid of {a mathematical formula}n×n states. Every state has an associated reward {a mathematical formula}R(s). Typically there is one or several states with high rewards (considered the goals) whereas the other states have zero or negative associated rewards. We considered an instance of gridworld with a single goal state in the upper-right corner with a reward of 100. The reward of all other states was set to −1. The scheme for the LogKB is as follows:{a mathematical formula} We solved the resulting set of LPs using CVXOPT on a 3.2 GHz Core i7, 32 GB RAM workstation running Ubuntu 12. As summarized in Fig. 11(a), the MDPS-LPs can be compiled to about half of the original size. Furthermore, Fig. 11(b) shows that already this compression leads to improved running time. In both figures, we broke down the measured total time for solving the LP into the time spent on lifting and solving respectively.</paragraph><paragraph>We then introduced additional symmetries by putting a goal in every corner of the grid. As expected, this gave more room for compression, which further improved efficiency as reflected in Figs. 11(c) and 11(d).</paragraph><paragraph>These results affirmatively answer question (Q2) and strongly support that (Q1) can be answered affirmatively as well. However, the examples that we have considered so far are quite sparse in their structure. Thus, one might wonder whether the demonstrated benefit is achieved only because we are solving a sparse problem in dense form. To address this we converted the MDP problem to a sparse representation for our further experiments. As one can see in Fig. 11(e) lifting still resulted in an improvement of size as well as running time. Therefore, we can conclude that lifting an LP is beneficial regardless of whether the problem is sparse or dense, thus one might view symmetry as a dimension orthogonal to sparsity. Remarkably, the results follow closely what has been achieved with MDP-specific symmetry finding and model minimization approaches [47], [48], [49].</paragraph></section><section label="5.2"><section-title>Programming MAP-LP inference in Markov logic networks (MLNs)</section-title><paragraph>MLNs [12] are a prominent probabilistic relational model using weighted logical rules such as {a mathematical formula}0.75smokes(X)⇒cancer(X). Given a set of constants, an MLN induces a Markov random field (MRF) with a node for each ground atom and a clique for every ground formula. We here focus on MAP (maximum a posteriori) inference where we want to find a most likely joint assignment to all the random variables. A common approach to approximate MAP inference in MRFs is based on LPs [22]. Let us now briefly review this approach.</paragraph><paragraph>Suppose we are presented with a propositional MRF with binary random variables {a mathematical formula}X={x1,…,xn} and factors {a mathematical formula}F={(θf,xf)}f, where each {a mathematical formula}θf is a function (having no 0-values) over a subset of random variables {a mathematical formula}xf⊆X. As LP variables, we introduce variable beliefs {a mathematical formula}μi over the states of each random variable {a mathematical formula}xi (e.g. {a mathematical formula}μi(xi=0), {a mathematical formula}μi(xi=1) are LP variables) and joint beliefs {a mathematical formula}μf over all joint configuration of each subset {a mathematical formula}xf (e.g. {a mathematical formula}μf(xi=0,xj=1,xk=0), etc.). The essence of the MAP-LP approach is to constrain the joint beliefs to be consistent with the variable beliefs under marginalization, e.g., {a mathematical formula}μf(xi=0,xj=0)+μf(xi=0,xj=1)=μi(xi=0). More precisely, the MAP-LP is defined as follows{a mathematical formula} and if we were to solve this LP as an integer LP, the variable beliefs would give us the exact MAP assignment. However, without integrality constraints, this is an approximation.</paragraph><paragraph>Using RLPs, we can compactly encode the MAP-LP for MLNs. To see this, consider the friends-and-smokers MLN [12] consisting of two rules. The first rule says that smoking can cause cancer {a mathematical formula}0.75smokes(X)⇒cancer(X), and the second implies that if two people are friends then they are likely to have the same smoking habits {a mathematical formula}0.75friends(X,Y)∧smokes(X)⇒smokes(Y). Since both MLNs and RLPs are based on logic programming, we expect to be able to define the MAP-LP in a first-order fashion, incorporating the MLN semantics directly into (6). In order to understand how this works, let us discuss some key features. From the MLN semantics we know the following: (1) we can generate the set of random variables by grounding out {a mathematical formula}smokes(X), {a mathematical formula}cancer(X), and {a mathematical formula}friends(X,Y) for every person in the domain. (2) We can generate the set of factors by grounding out the above two rules. Every ground rule is a factor over the ground atoms involved in it. In the friends-and-smokers example, we have pairwise and ternary rules (involving 2 resp. 3 ground atoms).</paragraph><paragraph>Thus, the set of MAP-LP variables is parametrized in terms of the predicates and clauses of the MLN, and we directly get a set of logically parameterized LP variables. They are: atom beliefs, {a mathematical formula}m(smokes(X)=t), {a mathematical formula}m(smokes(X)=f); pairwise factor beliefs,{a mathematical formula}{a mathematical formula} induced by the rule {a mathematical formula}smokes(X)⇒cancer(X); and finally ternary beliefs like {a mathematical formula}m(friends(X,Y)=f,smokes(X)=t,smokes(Y)=f) induced by the third rule (we will not list all joint configurations here). It is now easy to see that the marginalization constraints are logically parametrized as well. For example, we have constraints such as{a mathematical formula} Finally, the objective weight of a joint belief variable follows from the MLN semantics as well: it is the weight of the rule if the joint configuration satisfies the rule and 0 otherwise. E.g., the weight of {a mathematical formula}m(smokes(anna)=t,cancer(anna)=f) is 0 because this truth assignment is not a model of {a mathematical formula}smokes(anna)⇒cancer(anna). In contrast, the beliefs over the remaining three configurations have a weight of 0.75. Consequently, one can encode the objective weights in a predicate w given in the LogKB.</paragraph><paragraph>The resulting RLP is shown in Fig. 12. If we were to ground out this RLP, we would get exactly the LP from (6) applied to the ground MRF induced by the friends-and-smokers MLN. Note how we have abstracted away the names of the predicates. As a result, this RLP can be used with any ternary MLN. Changes in evidence, constants, or MLN rules are confined entirely to the LogKB, which for the friends-and-smokers MLN looks as follows:{a mathematical formula} Here, w represents the objective value assigned to the joint belief of the atoms in the first two (resp. three) arguments having the values of the latter two (resp. three) arguments. Please keep our short-hand notation in mind: w(smokes(X), cancer(X), t, f) = 0 stands for w(smokes(X), cancer(X), t, f, 0).</paragraph><paragraph>With the RLP in Fig. 12, we have seamlessly merged together MLN and MAP-LP semantics in one compact RLP and retained the freedom to change the MLN rules, domain constants and evidence easily within the RLP itself. This illustrates the modeling power of RLP in relational domains and presents a strong argument that (Q1) can be answered affirmatively.</paragraph><paragraph>Let us now turn towards investigating (Q2). As shown in previous works, inference in graphical models can be dramatically sped-up using lifted inference. Thus, it is natural to expect that the symmetries in graphical models, which can be exploited by standard lifted inference techniques, will also be reflected in the corresponding MAP-(R)LP. To verify whether this is indeed the case we induced MRFs of varying size from a friends-and-smokers MLN by varying the number of people from 50 to 300.</paragraph><paragraph>The results of the experiments are summarized in Figs. 13(a) and (b). As Fig. 13(a) shows, the number of LP variables is significantly reduced. Not only is the linear program reduced, but due to the fact that the lifting is carried out only once, we also measure a considerable decrease in running time as depicted in Fig. 13(b). Note that the time for the lifted experiment includes the time needed to compile the LP. This affirmatively answers (Q2).</paragraph></section><section label="5.3"><section-title>Programming collective classification using LP-SVM</section-title><paragraph>Networks have become ubiquitous, and often we are interested in how objects in these networks influence each other. Consequently, collective classification has received a lot of attention [50], [51], [52], [53], [12], [13]. It refers to the task of jointly classifying a set of inter-related objects. It exploits the fact that inter-related objects often share a lot of similarities. For example, in citation networks there are dependencies among the topics of a papers' references, and in social networks people who are in close contact tend to have similar interests. Using these dependencies allows collective classification methods to outperform methods that assume object independence [50].</paragraph><paragraph>Despite being successful, most of the research in collective classification so far has focused on generative models, at least as part of the column generation approach to solving quadratic program formulations of the collective classification task. We here illustrate that relational linear programming could provide a first step towards a principled large-margin approach. Specifically, we introduce a transductive{sup:10} collective SVM based on RLPs. In essence, to classify unlabeled objects, we will not only consider their attributes, but also their relationship to the labeled ones as part of the model.</paragraph><paragraph>We start off by reviewing the vanilla LP approach to SVMs and then show how RLPs can be used to program a transductive collective classifier.</paragraph><paragraph>Support vector machines (SVMs) [54] are the most widely used model for discriminative classification at the moment. The hypothesis space of an SVM is the space of affine models represented by coefficients (weights) of a vector orthogonal to a hyperplane, and an intercept. In the soft-margin version of SVM, training strives to strike a balance between width of the margin between the examples and the separating hyperplane, and the number of examples that fall within or on the wrong side of the margin (misclassifications). Maximization of the margin is achieved by penalizing squared Euclidean norm of the weight vector and traditionally posed as a quadratic optimization problem (QP).</paragraph><paragraph>Zhou et al. [16] have shown that the same problem can be modeled as an LP when replacing the squared Euclidean norm by the infinity norm without incurring a major loss in generalization performance. The LP suggested by Zhou et al. is as follows, and we refer to [16] for more details:{a mathematical formula}{a mathematical formula}{a mathematical formula}{a mathematical formula} This LP-SVM can readily be applied to classify papers in the Cora dataset [55]. The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of a corresponding word from a dictionary. The dictionary consists of 1433 unique words. We turned this problem into a binary classification problem by taking the largest of the 7 classes as a positive class and merging the other 6 into a negative class.</paragraph><paragraph>The RLP in Fig. 14 encodes the vanilla LP-SVM. In this non-collective setting, we ignored the citation information and classified documents based on 0/1 word features using the following LogKB:{a mathematical formula} Here the arguments of attribute/2 are the indices of a document and a word present in a document respectively. Words that are not present in a document, i.e., have value zero in the original dataset, are not specified. Again, this answers (Q1) affirmatively, since the RLP stays fixed for different datasets and only the LogKB changes.</paragraph><paragraph>We now show how to transform this vanilla (R)LP-SVM model into a transductive collective one (TC-RLP-SVM) by making only a slight modification to the RLP.</paragraph><paragraph>Indeed, there are a number of ways to do this, and exploring them is an interesting avenue for future work. Since our primary focus is not to achieve the best performance but to illustrate the ease of the relational mathematical programming approach, we chose the following, rather basic approach. We add constraints which favor that unlabeled instances have the same label as their labeled neighbors. To account for contradicting examples, we introduce slack variables for these constraints and add them to the objective with a separate penalty parameter. This results in the TC-RLP-SVM model shown in Fig. 15. Here, the new predicate pred/2 denotes the predicted label for unlabeled instances. The LogKB gets two new predicates:{a mathematical formula} The cite/2 predicate encodes citation information, and the query/1 predicate marks unlabeled instances whose labels are to be inferred. We notice that the parameters in the objective play a different role in the TC-RLP-SVM. In the vanilla case a parameter has to be carefully chosen in the training phase, but then prediction is done using the learned weight vector only. In the transductive setting the linear model, which is at the heart of RLP-SVM, plays a role of a medium between labeled and unlabeled instances. The weights are tuned for every new problem instance (recall the point of transductive inference is not to learn an intermediate predictive model, but to do inference directly). In this case the objective parameters become more important. The optimal value of the parameters depends on the data and hence they have to be tuned during the transductive inference phase as well using for instance cross-validation.</paragraph><paragraph>To investigate the benefit of the relational mathematical programming approach to collective inference, we compared the performance of the vanilla RLP-SVM to the TC-RLP-SVM—the same RLP-SVM with few additional relational constraints—on the task of paper topic classification using the Cora dataset. The experiment protocol was as follows. We first randomly split the dataset into a training set A, a validation set C, and a test set B in proportion {a mathematical formula}70/15/15. The validation set was used to select the parameters of the TC-RLP-SVM in a 5-fold cross-validation fashion. That is, we split the validation set into 5 subsets {a mathematical formula}Ci of equal size. On these sets we selected the parameter using a grid search for each {a mathematical formula}Ci on an {a mathematical formula}A∪(C∖Ci) labeled and {a mathematical formula}B∪Ci unlabeled examples, computing the prediction error on {a mathematical formula}Ci and averaging it over all {a mathematical formula}Cis. We then evaluated the selected parameters on the test set B whose labels were never revealed in training. We repeated this experiment 5 times (one for each {a mathematical formula}Ci) for the TC-RLP-SVM. For consistency, we followed the same protocol with RLP-SVM, except that the set {a mathematical formula}B∪Ci did not appear during training at all as the RLP-SVM has no use for unlabeled examples. That is, we selected parameters by training on {a mathematical formula}A∪(C∖Ci) and evaluating on {a mathematical formula}Ci. The selected parameters were then evaluated on the test set B. The results are summarized in Fig. 16(a). As one can see, the vanilla RLP-SVM achieved a prediction error of {a mathematical formula}16±1%. The TC-RLP-SVM achieved {a mathematical formula}7.5±1%. A paired t-test ({a mathematical formula}p=0.05) revealed that the difference in mean is significant.</paragraph><paragraph>Although best performance was not our goal, the performance is quite encouraging. Consequently, we conducted a more detailed study of the TC-RLP-SVM performance. We applied three state-of-the-art collective classification approaches to the Cora dataset, evaluated them on exactly the same splits of the data end compared their performance to that of our model.</paragraph><paragraph>The first two methods are part of NetKit [56]—a toolkit for classification in networked data. Among the many available options we chose the two methods that performed best in the original study [56]. The simplest classifier available in the toolkit, the weight-voted relational neighbor classifier (wvRN), turned out to also be one of the two best in combination with relaxation labeling as a collective inference procedure. In its basic setting (which is the one being used in the experiments to follow) it simply predicts a probability of each label for an instance as an average probability of this label for all the neighbors of the instance in the network:{a mathematical formula} where {a mathematical formula}Ni is a set of neighbors of a node {a mathematical formula}vi and Z a normalization constant. The second model we compared to is the link-based classifier [57] (nLB). This classifier creates a feature vector for a node by aggregating the labels of neighboring nodes, and then uses logistic regression to build a discriminative model based on these feature vectors. The learned model is then applied to estimate {a mathematical formula}P(xi=c|Ni). Various aggregation methods can be used in this setting, including existence, the mode, and value counts. The latter method with normalized value counts has been shown to have the best performance and is thus used in our study. It is also combined with relaxation labeling for collective inference. For a fair comparisons to the TC-RLP-SVM, both methods are also allowed to use instance-level information (word vectors) by training a logistic regression model on the observed instances and using its predictions as priors for the unobserved ones. Finally, a simple network-only MLN was used as the last baseline. It consists of a single rule{a mathematical formula} with an additional restriction that every document can have only one category. For probabilistic inference we used Tuffy [33].</paragraph><paragraph>The experiment protocol was follows: we had 4 settings in which we kept 20, 40, 60 and 80% of the labels in the training set. For each setting 10 random splits were generated, with labeled instances selected uniformly at random. Each method was then evaluated on the same random splits. We report average errors for each setting and method. To once again illustrate the benefits of using relational information, the Vanilla (LP-)SVM, which uses instance-level information only, is included into the evaluation as well. The results are summarized in Fig. 16(b).</paragraph><paragraph>The performances of the NetKit methods correspond to those originally reported by Macskassy and Provost [56]. The nLB classifier does not outperform the simpler wvRN even with 80% of the labeled examples. The MLN classifier, which ignores the instance-level information, shows a very similar performance to the wvRN but is consistently a little bit worse. Note that already with 40% of the labeled examples, all the classifiers, except for the Vanilla (LP-)SVM, achieve accuracies of at least 85%. In fact, even the highest performance achieved by the Vanilla (LP-)SVM (83% accuracy) is significantly lower. This confirms that utilizing relational information can significantly boost the classification performance. However, one can also see that the Vanilla (LP-)SVM can significantly be improved by just adding few additional constraints. More precisely, the TC-(LP-)SVM outperforms the Vanilla (LP-)SVM in all cases and catches up quickly with the NetKit methods. The slightly lower performance in general and in particular for lower percent of observed labels is likely due to the fact that we used a very basic model that does not propagate label information through the citation network.</paragraph><paragraph>In any case, the empirical results clearly show that collective inference can be tackled using RLPs and, hence, provide another affirmative answer to (Q1). That the performance gain was achieved by simply reprogramming the Vanilla (LP-)SVM highlights the power of relational linear programming and answers (Q3) affirmatively. Keep in mind that our goal was not to obtain a novel and better collective model, but just to demonstrate the possibilities opened up by relational linear programming.</paragraph></section><section label="5.4"><section-title>Lifted LP support vector machines</section-title><paragraph>To close the loop, we finally investigated the liftability of RLP-SVMs. Although not surprising from a theoretical perspective—we have already shown that any LP that contains symmetries is liftable—this constitutes the very first symmetry-aware (LP-)SVM solver and is an encouraging sign that lifting goes beyond probabilistic inference: lifted statistical machine learning in general is not insurmountable.</paragraph><paragraph>To investigate symmetries in relational SVM classification, we solved the LPs produced by the parameter selection phase of TC-SVM across different observed label percentages in a lifted fashion. More precisely, following the experimental protocol of the last experiment, we started with four cases: {a mathematical formula}20%, {a mathematical formula}40%, {a mathematical formula}60% and {a mathematical formula}80% observed labels. For each case, we limited ourselves to evaluating 4 regularization parameter pairs ((C(1), C(2)) in line 13 of the RLP in Fig. 15, also referred to as ({a mathematical formula}c1,c2) for brevity). These four pairs consist of the optimal parameters, and 2×, 10× and 1000× the optimal parameters. The motivation behind this choice is to represent different parameter scales during the grid search. As in the previous experiment, for each parameter evaluation we did a 5-fold cross-validation. Finally, to encompass a wider variety of training sets, we repeated this process across 10 random {a mathematical formula}A/B/C splits. Thus, our dataset consists of {a mathematical formula}4×4×5×10=800 LPs in total.</paragraph><paragraph>We first computed the compression ratio as the ratio of the size (number of constraints and variables) of the lifted LP and the size of the ground LP for each of the LPs. Fig. 17(a) breaks down the ratios by percentage of observed labels. As one can see, even with the constraints induced by the random subgraphs of Cora corresponding to the random label deletions, a non-negligible amount of symmetry is present in the problem. Moreover, on average, the compression rate increases as more labels are introduced. On first sight this seems to be counterintuitive since adding information is typically considered to break symmetries. However, since the RLP employs information only between pairs of observed and unobserved instances, the lower the number of such pairs, the more symmetry is there actually. That is, as we increase the percent of observed labels, the number of such pairs is likely to decrease.</paragraph><paragraph>Next, we solved all LPs using Gurobi (version 6.0), the most popular LP solver in 2012 according to NEOS statistics{sup:11} with its default settings. We used the same machine as in the MDP experiments. Fig. 17(b) summarizes the net gain per LP of lifted solving (including the time for symmetry detection and compression) as opposed to solving the ground model across the different parameter pairs. One can see that even with Gurobi's sophisticated pre-solving heuristics, lifting yields a considerable speed-up, although there are a few LPs where solving the lifted model was slower. Across all LPs, however, lifting the LPs gained us a total of 1290.83 seconds compared to the ground solver. Fig. 17(c) shows a scatterplot of the lifted vs. ground solving time when solving with Gurobi, together with the fitted regression model. As can be seen, for this particular set of LPs, the correlation coefficient between lifted and ground time is on the order of the amount of symmetry present in the problem. These results not only provide an affirmative answer to (Q2) but also present strong evidence that speed-ups can indeed be observed across different solvers, even in state-of-the-art commercial packages. Hence, (Q4) can also be answered affirmatively.</paragraph><paragraph>Finally, to investigate (Q5), we recorded the relative difference of the quality of the lifted and the ground solutions, i.e.{a mathematical formula} in all experiments involving lifting. For all experiments with CVXOPT, we ran the solver with a convergence threshold of {a mathematical formula}10−7. For the MDP experiments, we observed Δ's between {a mathematical formula}10−6 and {a mathematical formula}10−9. For MAP, the Δ's were in the range {a mathematical formula}10−7 to {a mathematical formula}10−8. With Gurobi, in all experiments Δ was in the range {a mathematical formula}10−13 to {a mathematical formula}10−14. Gurobi automatically set its convergence threshold to {a mathematical formula}10−8. While one can imagine the existence of pathological cases where lifting influences the numerical performance of the solver, this was not observed in our experiments. This provides an affirmative answer to question (Q5).</paragraph><paragraph>Taking all results together, the experimental illustrations clearly show that all five questions (Q1)–(Q5) can be answered affirmatively.</paragraph></section></section><section label="6"><section-title>Related work</section-title><paragraph>Relational linear programming as introduced here is related to two lines of research, namely rich languages for mathematical programming and exploiting symmetries for AI tasks.</paragraph><section label="6.1"><section-title>Languages for mathematical programming</section-title><paragraph>Several expressive modeling languages for mathematical programming have been proposed. Examples of popular ones are AMPL [29], [30], GAMS [58], AIMMS [59], and Xpress-Mosel [60], and more can be found in general surveys [61], [62], [63]. These modeling languages are mixtures of declarative and imperative programming styles using sets of objects to index multidimensional parameters and LP variables. Employing essentially “for-in” statements, this indexing is used to define objectives and constraints in an abstract model, which separates the declaration of a model from the data used to generate a specific model instance. Although this indeed has much in common with relational linear programming, there are also fundamental differences. Consider e.g. AMPL. As stated by Fourer, Gay, and Kernighan [30, page 122], the “current version of AMPL does not support a full-fledged ‘logical’ type of parameter that would stand for only the values true and false”. Instead it promotes the use parameters of type binary together with logical operators such as “if-then-else” and in particular “for-all” and “exists”. However, this requires one to also encode what is false, and these operators are intended to work with parameters, not LP variables as featured in relational linear programming. To do this in AMPL, one would have to introduce some binary LP variables with corresponding constraints that encode the “exists” and “for-all” operators. This, however, again blows up the model since we would need to also represent what is false. In contrast, by unification across different queries, relational linear programming connects different “sets” of LP variables encoded as atoms using logic programming proof techniques.</paragraph><paragraph>As an alternative, since index sets and data tables are closely related to the attributes and relations of relational database systems, there have also been proposals for “feeding” a linear program directly from relational database systems [64], [65], [66] as well as AMPL's interface with relational databases to define sets and parameters. However, this takes the “logic” out of the LP modeling language. Moreover, logic programming—which allows one to use e.g. complex terms and to define recursive predicates, where we do not know how often we have to union two “tables”—was not considered and the resulting approaches do not provide a syntax close to the mathematical notation of linear and logic programs.</paragraph><paragraph>This also holds for Roth and Yih [67], who presented an abstract ILP model for global inference in natural language processing (NLP) tasks. The idea is to use an object-oriented entity-relationship model in the background and to index the sum-statements in ILPs over the members of object and relation classes. This is closer in spirit to relational linear programming but e.g. unification and negation are not supported when defining constraints. Generally, motivated by the already mentioned “Model + Solver” paradigm, NLP witnesses a growing need for relational mathematical modeling languages [68], [69], [70], [71], [72]. Consider e.g. the subsequent work on global inference by Clarke and Lapata [69]. They quantify constraints using statements such as “{a mathematical formula}∀i,j,k:xj∧xk conjoined by xi” saying if two head words are conjoined in the source sentence, then add some algebraic constraints over variables indexed by {a mathematical formula}i,j, and k. Although akin to a relational specification, no general relational modeling language for (I)LPs has been presented. The approaches are rather ad-hoc systems that “ground” the specific constraints by looping over some specialized code in some high-level programming language such as C/C++. For solving relational Markov Decision Processes, Sanner and Boutilier [24] used approximate linear programming with relational constraints represented via case statements. This approach scaled to problems of previously prohibitive size by avoiding grounding and is indeed close in spirit to relational programming. However, Sanner and Boutilier did not introduce a relational modeling language for arbitrary LPs.</paragraph><paragraph>Thus, following Klabjan, Fourer and Ma [73, unpublished], one can argue that there is a need for a mathematical programming solution with built-in language constructs that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming. Klabjan et al. show that the basic functionalities and requirements of mathematical programming languages such as AMPL can be realized in any Datalog-like logic programming language. This is probably the closest in spirit to relational linear programming. However, there are significant differences. First, RLPs directly extend the syntax of mathematical programming languages with logic programming concepts and, hence, stay close to the mathematical notation of LPs. Moreover, Datalog is not expressive enough to capture the entire AI spectrum of problems we would like to tackle using RLPs. For instance, it disallows complex terms as arguments of predicates—e.g., {a mathematical formula}p(1,2) is admissible but not {a mathematical formula}p(f(1),2)—making it hard to work with complex data structures such as graphs, imposing certain stratification restrictions on the use of negation and recursion, and only allowing range restricted variables, i.e. each variable in the head of a rule must also appear in a not negated clause in the premise of this rule. Consider dealing with LP problems on graphs. Indeed, with Datalog one could encode this using predicates encoding the nodes and edges of the graph. However, there are several other ways to encode graphs using complex terms, and which form is the best certainly depends on the LP application at hand. For instance, if the number of nodes is important and not provided by the user (the graph is generated within the program) then the simple predicate-based representation would require meta-predicates to compute the number of nodes. In a list-based representation, however, we can just compute the length of the list. Moreover, complex terms allow—as illustrated in the present paper—for a concise specification of MAP-LP inference in Markov logic networks. Hence, we advocate Prolog for similar reasons as Eisner and Filardo do in their AI language DYNA [74].</paragraph><paragraph>Recently, Mattingley and Boyd [75] have introduced CVXGEN, a software tool that takes a high level description of a convex optimization problem family, and automatically generates custom C code that compiles into a reliable, high speed solver for the problem family. CVXGEN features indexed parameters and variables but no logical way for declaring the objective and the constraints while referring to them. Generally, there is a tendency to embed mathematical programming languages into imperative high-level programming language such as Python [76], [77], Java, C/C++ and Matlab, among others.{sup:12} In particular Diamond, Chu and Boyd's CVXPY [77] enables an object-oriented approach to constructing optimization problems and notes that such an object-oriented approach is simpler and more flexible than the traditional method of constructing problems by embedding information in matrices. In contrast, the goal of relational linear programming is to put logic programming into optimization. The same holds for Rizzolo and Roth's Learning Bayes Java (LBJ) [78], which grew out of the above mentioned research on global inference in NLP. LBJ combines ideas from optimization, first order logic, and object-oriented programming for compiling complex models into ILPs. In particular, LBJ provides a convenient syntax for specifying the interactions between Java functions as arbitrary first-order-logical formulas. That is, it does not aim at putting logic programming into optimization. Moreover, the only two predicates in the constraints are equality and inequality, although their arguments may be arbitrary Java expressions. Finally, Gordon, Hong, and Dudík [79], [80] developed first-order programming (FOP) that combines the strength of mixed-integer LPs and first-order logic. In contrast to the present paper, however, they focused on first-order logical reasoning and not on specifying arbitrary linear programs in a relational way.</paragraph></section><section label="6.2"><section-title>Exploiting symmetries for solving AI tasks</section-title><paragraph>Another distinguishing feature of relational linear programming compared to all the modeling approaches mentioned so far is that none of them has considered how to detect and exploit symmetries in arbitrary LPs. Indeed, there are symmetry-breaking approaches for (mixed–)integer programming (MIPs) [81] that are also featured by commercial solvers such as CPLEX [82] and GUROBI [83]. The dominant paradigm is to add symmetry breaking inequalities, similarly to what has been done for SAT and CSP [84]. Alternatively one can prune the search space to eliminate symmetric solutions (see e.g. [81] for a survey). In contrast, lifted linear programming reduces the dimensionality of the LP at hand; the LP is compressed. To do so, one takes advantage of convexity and projects the LP into the fixed space of its symmetry group [44]. The projections we investigated in the present paper are similar in spirit. Until recently, however, discussions were mostly concentrated on the case where the symmetry group of the LP consists of permutations [85]. In such cases the problem of computing the symmetry group of the LP can be reduced to computing the colored automorphisms of a “coefficient” graph connected with the linear program [86], [81]. Moreover, the reduction of the LP in this case essentially consists of mapping variables to their orbits. Our approach subsumes this method, as we replace the orbits with a coarser equivalence relation which, in contrast to the orbits, is computable in quasilinear time. Going beyond permutations, Bödi and Herr [44] extend the scope of symmetry, showing that any invertible linear map, which preserves the feasible region and objective of the LP, may be used to speed-up solving. While this setting offers more compression, the symmetry detection problem becomes even more difficult.</paragraph><paragraph>Finally, lifted linear programming as introduced here has been proven already beneficial. Lifted (I)LP-MAP inference approaches for (relational) graphical models based on (relaxed) graph automorphisms and variants have been explored in several ways [43], [87], [88], [89], [42], which go beyond the scope of the present paper.</paragraph></section></section><section label="7"><section-title>Future work</section-title><paragraph>The indent of our paper has been to introduce and explore the basic idea and concepts of relational linear programming. Consequently, there is significant additional work to be done. More work is needed to extended the language presented with the concepts of modules and name spaces, allowing one to build libraries of relational programs, as well as combining it with Mattingley and Boyd's [75] CVXGEN to automatically generate custom C code that compiles into a reliable, high speed solver for the problem family at hand. The framework should also be extended to other mathematical programs such as (mixed) integer LPs, quadratic programs, and semi-definite programs, among others. Column generation and cutting plane approaches to lifted linear programming should also be explored. Very recently, e.g., Lu and Boutilier [90] presented a value-directed compression technique for propositional assignment LP. They dynamically segment the individuals into blocks using a form of column generation, constructing groups of individuals who can provably be treated identically in the optimal assignment solution. Together with the declarative nature of the resulting relational mathematical programming approach to AI, one should investigate program analysis approaches to automate problem decomposition at a lifted level. If symmetries could be detected and exploited efficiently in other mathematical programs, too, this would put general symmetry-aware machine learning and AI even more into reach.</paragraph><paragraph>In general, we have only started to explore the interaction of lifting and solving linear programs, and mathematical programs in general. Since lifting changes the geometry of the linear program, it could have an impact on the various heuristics employed in modern solvers for efficient solving linear programs. This interaction should be explored and it should be investigated whether it may even cancel the benefits of lifting. Ground and lifted linear programs may also require different convergence thresholds in order to maintain the same quality of solutions. While we did not observe any surprising behavior in our experiments, exploring these issues is an important and interesting avenue for future work.</paragraph><paragraph>The most attractive immediate avenue, however, is to explore relational linear programming within other AI and machine learning tasks. First of all, the novel collective classification approach should be rigorously evaluated and compared to other approaches on a number of other benchmark datasets. Other attractive avenues are the exploration of the symmetry-aware SVMs outlined in the present paper within other learning setting, relational dimensionality reduction via LP-SVMs [91], novel relational boosting approaches via linear programs [92], and developing relational and lifted solvers for computing optimal Stackelberg strategies in two-player normal-form game [93], among others. Lifting should also be explored within recent probabilistic CSP proposals. For instance, Sraswat et al. [94] very recently sketched the probabilistic constraint programming language C10. It is based on the concurrent constraint programming framework and implemented on top of X10, a language for scale-out computation. Lifting may scale C10 even more. Along similar lines, one should explore lifting consensus optimization [95].</paragraph><paragraph>One should also push the programming view on relational machine learning tasks. As an example consider kernels for classifying graphs. Graph classification is a very important task in bioinformatics [96], natural language processing [97] and many other fields. Kernelized SVM is often the method of choice. The inner products in SVM can be replaced by functions (kernels) that effectively represent inner products in higher dimensional space. This inner product view on one hand makes SVM a non-linear classifier but also allows one to deal with structured objects such as graphs e.g. using convolution kernels [98]. Convolution kernels introduced the idea that kernels could be built to work with discrete data structures interactively from kernels for smaller composite parts. RLPs suggests to view them as a programming task. Within LogKB we define the parts and a generalized sum over products—a generalized convolution—is realized within RLP. Similarly, many other graph kernels known could be realized. Walks [99], cyclic patterns [100] and shortest-paths [101] are examples of substructures considered so far. All these concepts are naturally representable as a logic programs in Prolog. E.g., {a mathematical formula}shortestPath(A,B,G,Paths) computes in Path the shortest path between nodes A and B in graph G. One can program the shortest path and use it to define a convolution kernel in an RLP SVM as follows:{a mathematical formula} where simple_k is any kernel on vertices and lengths. In this way, relational mathematical programming suggests a novel programming view on graph kernels that integrates the kernel programming with the mathematical program into one declarative model.</paragraph></section><section label="8"><section-title>Conclusion</section-title><paragraph>We have introduced relational linear programming, a simple framework combining linear and logic programming. Its main building blocks are relational linear programs (RLPs). They are compact LP templates defining the objective and the constraints through the logical concepts of individuals, relations, and quantified variables. This contrasts with mainstream LP template languages such as AMPL, which mixes imperative and linear programming, and allows a more intuitive representation of optimization problems over relational domains where we have to reason about a varying number of objects and relations among them, without enumerating them. Inference in RLPs is performed by lifted linear programming. That is, symmetries within the ground linear program are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf linear program solver. This significantly extends the scope of lifted inference since it paves the way for lifted LP solvers for linear assignment, allocation and many other AI task that can be solved using LPs. Empirical results on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using relational LP support vector machines illustrated the promise of relational linear programming.</paragraph><section-title>Acknowledgements</section-title></section></content><acknowledgements><paragraph>The authors would like to thank the anonymous reviewers for their valuable feedback. K.K. and M.M. would also like to thank Babak Ahmadi for his fruitful collaboration on the earlier version of lifted linear programming as well as Martin Grohe and Aziz Erkal Selman for their fruitful collaboration on color-refinement. P.T. was with the CS Department of the University of Bonn, Germany, when working on relational linear programming. K.K. and M.M. were with Fraunhofer IAIS when working on the earlier version of lifted linear programming. The research presented was partly supported by the Fraunhofer ATTRACT fellowship STREAM, by the EC under contract number FP-248258-First-MM, by the German-Israeli Foundation for Scientific Research and Development, 1180-218.6/2011, and by the German Science Foundation (DFG), KE 1686/2-1, as part of the Coordination Project SPP 1527.</paragraph></acknowledgements><references><reference label="[1]"><host>L. GetoorB. TaskarIntroduction to Statistical Relational Learning(2007)MIT PressCambridge, MA</host></reference><reference label="[2]"><authors>L. De Raedt</authors><title>Logical and Relational Learning</title><host>(2008)Springer</host></reference><reference label="[3]"><host>L. De RaedtP. FrasconiK. KerstingS.H. MuggletonProbabilistic Inductive Logic Programming(2008)Springer</host></reference><reference label="[4]"><authors>L.D. Raedt,K. Kersting</authors><title>Statistical relational learning</title><host>G.W.C. SammutEncyclopedia of Machine Learning(2010)SpringerHeidelberg pp.916-924</host></reference><reference label="[5]"><authors>N. Nilsson</authors><title>Probabilistic logic</title><host>Artif. Intell.28 (1)(1986) pp.71-87</host></reference><reference label="[6]"><authors>H. Geffner</authors><title>Artificial intelligence: from programs to solvers</title><host>AI Commun.27 (1)(2014) pp.45-51</host></reference><reference label="[7]"><authors>A. Rush,M. Collins</authors><title>A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing</title><host>J. Artif. Intell. Res.45 (2012) pp.305-362</host></reference><reference label="[8]"><host>S. SraS. NowozinS. WrightOptimization for Machine Learning(2011)MIT Press</host></reference><reference label="[9]"><authors>T. Guns,S. Nijssen,L. De Raedt</authors><title>Itemset mining: a constraint programming perspective</title><host>Artif. Intell.175 (12–13)(2011) pp.1951-1983</host></reference><reference label="[10]"><authors>A. Atserias,E. Maneva</authors><title>Sherali–Adams relaxations and indistinguishability in counting logics</title><host>SIAM J. Comput.42 (1)(2013) pp.112-137</host></reference><reference label="[11]"><authors>M. Littman,T. Dean,L. Pack Kaelbling</authors><title>On the complexity of solving Markov decision problems</title><host>Proceedings of the 11th Annual Conference on Uncertainty in Artificial IntelligenceUAI-95(1995) pp.394-402</host></reference><reference label="[12]"><authors>M. Richardson,P. Domingos</authors><title>Markov logic networks</title><host>Mach. Learn.62 (1–2)(2006) pp.107-136</host></reference><reference label="[13]"><authors>P. Sen,G. Namata,M. Bilgic,L. Getoor,B. Gallagher,T. Eliassi-Rad</authors><title>Collective classification in network data</title><host>AI Mag.29 (3)(2008) pp.93-106</host></reference><reference label="[14]"><authors>M. Mladenov,B. Ahmadi,K. Kersting</authors><title>Lifted linear programming</title><host>Proceedings of the 15th International Conference on Artificial Intelligence and StatisticsAISTATSJ. Mach. Learn. Res. Workshop Conf. Proc.vol. 22 (2012) pp.788-797</host></reference><reference label="[15]"><authors>G. Dantzig,M. Thapa</authors><title>Linear Programming 2: Theory and Extensions</title><host>(2003)Springer</host></reference><reference label="[16]"><authors>W. Zhou,L. Zhang,L. Jiao</authors><title>Linear programming support vector machines</title><host>Pattern Recognit.35 (12)(2002) pp.2927-2936</host></reference><reference label="[17]"><authors>A. Demiriz,K.P. Bennett,J. Shawe-Taylor</authors><title>Linear programming boosting via column generation</title><host>Mach. Learn.46 (1–3)(2002) pp.225-254</host></reference><reference label="[18]"><authors>K. Ataman,W. Street,Y. Zhang</authors><title>Learning to rank by maximizing auc with linear programming</title><host>Proceedings of the International Joint Conference on Neural NetworksIJCNN(2006) pp.123-129</host></reference><reference label="[19]"><authors>Z. Wang,J. Shawe-Taylor</authors><title>Large-margin structured prediction via linear programming</title><host>Proceedings of the 12th International Conference on Artificial Intelligence and StatisticsAISTATS(2009) pp.599-606</host></reference><reference label="[20]"><authors>T. Klein,U. Brefeld,T. Scheffer</authors><title>Exact and approximate inference for annotating graphs with structural SVMs</title><host>Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, Part 1ECML PKDD-08(2008) pp.611-623</host></reference><reference label="[21]"><authors>M. Torkamani,D. Lowd</authors><title>Convex adversarial collective classification</title><host>Proceedings of the 30th International Conference on Machine LearningICML(2013) pp.642-650</host></reference><reference label="[22]"><authors>M.J. Wainwright,M.I. Jordan</authors><title>Graphical models, exponential families, and variational inference</title><host>Found. Trends Mach. Learn.1 (1–2)(2008) pp.1-305</host></reference><reference label="[23]"><authors>U. Syed,M. Bowling,R.E. Schapire</authors><title>Apprenticeship learning using linear programming</title><host>Proceedings of the 25th International Conference on Machine LearningICML(2008)ACM pp.1032-1039</host></reference><reference label="[24]"><authors>S. Sanner,C. Boutilier</authors><title>Practical solution techniques for first-order MDPs</title><host>Artif. Intell.173 (5–6)(2009) pp.748-788</host></reference><reference label="[25]"><authors>A.Y. Ng,S.J. Russell</authors><title>Algorithms for inverse reinforcement learning</title><host>Proceedings of the 17th International Conference on Machine LearningICML(2000) pp.663-670</host></reference><reference label="[26]"><authors>N. Komodakis,N. Paragios,G. Tziritas</authors><title>Clustering via LP-based stabilities</title><host>Proceedings of the 21st Conference on Neural Information Processing SystemsNIPS(2008) pp.865-872</host></reference><reference label="[27]"><authors>M. Sandler</authors><title>On the use of linear programming for unsupervised text classification</title><host>Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining(2005)ACM pp.256-264</host></reference><reference label="[28]"><authors>R.K. Ahuja,T.L. Magnanti,J.B. Orlin</authors><title>Network Flows: Theory, Algorithms, and Applications</title><host>(1993)Prentice Hall</host></reference><reference label="[29]"><authors>R. Fourer,D.M. Gay,B.W. Kernighan</authors><title>AMPL: A Mathematical Programming Language</title><host>(1993)The Scientific PressSan Francisco, CA</host></reference><reference label="[30]"><authors>R. Fourer,D. Gay,B. Kernighan</authors><title>AMPL: A Modeling Language for Mathematical Programming</title><host>second edition(2002)Duxbury Press/Brooks/Cole Publishing Company</host><host>http://ampl.com/resources/the-ampl-book/</host></reference><reference label="[31]"><authors>J. Lloyd</authors><title>Foundations of Logic Programming</title><host>(1987)Springer-VerlagBerlin</host></reference><reference label="[32]"><authors>P. Flach</authors><title>Simply Logical – Intelligent Reasoning by Example</title><host>Wiley Professional Computing (1994)Wiley</host></reference><reference label="[33]"><authors>F. Niu,C. Ré,A. Doan,J. Shavlik</authors><title>Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS</title><host>Proc. VLDB Endow.4 (6)(2011) pp.373-384</host></reference><reference label="[34]"><authors>P. Singla,P. Domingos</authors><title>Lifted first-order belief propagation</title><host>Proceedings of the 23rd AAAI Conference on Artificial IntelligenceAAAI, Chicago, IL, USA(2008) pp.1094-1099</host></reference><reference label="[35]"><authors>K. Kersting,B. Ahmadi,S. Natarajan</authors><title>Counting belief propagation</title><host>Proceedings of the 25th Conference on Uncertainty in Artificial IntelligenceUAI(2009)</host></reference><reference label="[36]"><authors>B. Ahmadi,K. Kersting,M. Mladenov,S. Natarajan</authors><title>Exploiting symmetries for scaling loopy belief propagation and relational training</title><host>Mach. Learn.92 (2013) pp.91-132</host></reference><reference label="[37]"><authors>C. Godsil,G. Royle</authors><title>Algebraic Graph Theory</title><host>(2001)Springer</host></reference><reference label="[38]"><authors>M. Ramana,E. Scheinerman,D. Ullman</authors><title>Fractional isomorphism of graphs</title><host>Discrete Math.132 (1994) pp.247-265</host></reference><reference label="[39]"><authors>M. Grohe,K. Kersting,M. Mladenov,E. Selman</authors><title>Dimension reduction via colour refinement</title><host>Proceedings of the 22th Annual European SymposiumESA-14(2014) pp.505-516</host></reference><reference label="[40]"><authors>C. Godsil</authors><title>Compact graphs and equitable partitions</title><host>Linear Algebra Appl.255 (1997) pp.259-266</host></reference><reference label="[41]"><authors>C. Berkholz,P. Bonsma,M. Grohe</authors><title>Tight lower and upper bounds for the complexity of canonical colour refinement</title><host>Proceedings of the 21st Annual European Symposium on AlgorithmsESA(2013) pp.145-156</host></reference><reference label="[42]"><authors>U. Apsel,K. Kersting,M. Mladenov</authors><title>Lifting relational MAP-LPs using cluster signatures</title><host>Proceedings of the 28th AAAI Conference on Artificial IntelligenceAAAI(2014)</host></reference><reference label="[43]"><authors>H. Bui,T. Huynh,S. Riedel</authors><title>Automorphism groups of graphical models and lifted variational inference</title><host>Proceedings of the 29th Conference on Uncertainty in Artificial IntelligenceUAI(2013)</host></reference><reference label="[44]"><authors>R. Bödi,K. Herr,M. Joswig</authors><title>Algorithms for highly symmetric linear and integer programs</title><host>Math. Program., Ser. A137 (1–2)(2013) pp.65-90</host></reference><reference label="[45]"><authors>M. Littman,T. Dean,L.P. Kaelbling</authors><title>On the complexity of solving Markov decision problems</title><host>Proceedings of the 11th International Conference on Uncertainty in Artificial IntelligenceUAI(1995) pp.394-402</host></reference><reference label="[46]"><authors>R. Sutton,A. Barto</authors><title>Reinforcement Learning: An Introduction</title><host>(1998)The MIT Press</host></reference><reference label="[47]"><authors>S. Narayanamurthy,B. Ravindran</authors><title>On the hardness of finding symmetries in Markov decision processes</title><host>Proceedings of the 25th International Conference on Machine LearningICML(2008) pp.688-695</host></reference><reference label="[48]">B. Ravindran,A. BartoSymmetries and model minimization in Markov decision processesTech. Rep. 01-43<host>(2001)University of MassachusettsAmherst, MA, USA</host></reference><reference label="[49]"><authors>T. Dean,R. Givan</authors><title>Model minimization in Markov decision processes</title><host>Proceedings of the 14th National Conference on Artificial IntelligenceAAAI(1997) pp.106-111</host></reference><reference label="[50]"><authors>S. Chakrabarti,B. Dom,P. Indyk</authors><title>Enhanced hypertext categorization using hyperlinks</title><host>Proceedings of ACM SIGMOD International Conference on Management of DataSIGMOD(1998) pp.307-318</host></reference><reference label="[51]"><authors>J. Neville,D. Jensen</authors><title>Iterative classification in relational data</title><host>Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational Data(2000) pp.13-20</host></reference><reference label="[52]"><authors>J. Neville,D. Jensen</authors><title>Collective classification with relational dependency networks</title><host>Proceedings of the 2nd International Workshop on Multi-Relational Data Mining(2003) pp.77-91</host></reference><reference label="[53]"><authors>J. Neville,D. Jensen</authors><title>Relational dependency networks</title><host>J. Mach. Learn. Res.8 (2007) pp.653-692</host></reference><reference label="[54]">V.N. VapnikStatistical Learning TheoryAdaptive and Learning Systems for Signal Processing, Communications and Control Series (1998)John Wiley &amp; SonsNew YorkA Wiley-Interscience Publication</reference><reference label="[55]"><authors>P. Sen,G.M. Namata,M. Bilgic,L. Getoor,B. Gallagher,T. Eliassi-Rad</authors><title>Collective classification in network data</title><host>AI Mag.29 (3)(2008) pp.93-106</host></reference><reference label="[56]"><authors>S.A. Macskassy,F. Provost</authors><title>Classification in networked data: a toolkit and a univariate case study</title><host>J. Mach. Learn. Res.8 (2007) pp.935-983</host></reference><reference label="[57]"><authors>Q. Lu,L. Getoor</authors><title>Link-based classification</title><host>Proceedings of the 20th International Conference on Machine LearningICML(2003) pp.496-503</host></reference><reference label="[58]"><authors>A. Brooke,D. Kendrick,A. Meeraus</authors><title>GAMS: A User's Guide</title><host>(1992)The Scientific PressRedwood City, CA</host></reference><reference label="[59]">J. Bisschop, P. Lindberg, AIMMS the Modeling System, Paragon Decision Technology, 1993.</reference><reference label="[60]"><authors>T. Ciriani,Y. Colombani,S. Heipcke</authors><title>Embedding optimisation algorithms with mosel</title><host>4OR1 (2)(2003) pp.155-167</host></reference><reference label="[61]"><authors>C. Kuip</authors><title>Algebraic languages for mathematical programming</title><host>Eur. J. Oper. Res.67 (1993) pp.25-51</host></reference><reference label="[62]"><authors>E. Fragniere,J. Gondzio</authors><title>Optimization modeling languages</title><host>P. PardalosM. ResendeHandbook of Applied Optimization(2002)Oxford University PressNew York pp.993-1007</host></reference><reference label="[63]"><host>S. WallaceW. ZiembaApplications of Stochastic Programming(2005)SIAMPhiladelphia</host></reference><reference label="[64]"><authors>G. Mitra,C. Luca,S. Moody,B. Kristjanssonl</authors><title>Sets and indices in linear programming modelling and their integration with relational data models</title><host>Comput. Optim. Appl.4 (1995) pp.263-283</host></reference><reference label="[65]"><authors>A. Atamtürk,E. Johnson,J. Linderoth,M. Savelsbergh</authors><title>A relational modeling system for linear and integer programming</title><host>Oper. Res.48 (6)(2000) pp.846-857</host></reference><reference label="[66]"><authors>R. Farrell,T. Maness</authors><title>A relational database approach to a linear programming-based decision support system for production planning in secondary wood product manufacturing</title><host>Decis. Support Syst.40 (2)(2005) pp.183-196</host></reference><reference label="[67]"><authors>W. Yih,D. Roth</authors><title>Global inference for entity and relation identification via a linear programming formulation</title><host>L. GetoorB. TaskarAn Introduction to Statistical Relational Learning(2007)MIT Press</host></reference><reference label="[68]"><authors>S. Riedel,J. Clarke</authors><title>Incremental integer linear programming for non-projective dependency parsing</title><host>Proceedings of the Conference on Empirical Methods in Natural Language ProcessingEMNLP(2006) pp.129-137</host></reference><reference label="[69]"><authors>J. Clarke,M. Lapata</authors><title>Global inference for sentence compression: an integer linear programming approach</title><host>J. Artif. Intell. Res.31 (2008) pp.399-429</host></reference><reference label="[70]"><authors>A. Martins,N. Smith,E. Xing</authors><title>Concise integer linear programming formulations for dependency parsing</title><host>Proceedings of the 47th Annual Meeting of the Association for Computational LinguisticsACL(2009) pp.342-350</host></reference><reference label="[71]"><authors>S. Riedel,D. Smith,A. McCallum</authors><title>Parse, price and cut—delayed column and row generation for graph based parsers</title><host>Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language LearningEMNLP-CoNLL(2012) pp.732-743</host></reference><reference label="[72]"><authors>X. Cheng,D. Roth</authors><title>Relational inference for wikification</title><host>Proceedings of the Conference on Empirical Methods in Natural Language ProcessingEMNLP(2013) pp.1787-1796</host></reference><reference label="[73]">D. Kabjan,R. Fourer,J. MaAlgebraic modeling in a deductive database languagehttp://dynresmanagement.com/uploads/3/3/2/9/3329212/datalog_modeling.pdf(2009)presented at the 11th INFORMS Computing Society (ICS) Conference 2009, but not published</reference><reference label="[74]">J. Eisner,N. FilardoDyna: extending datalog for modern AIO. de MoorG. GottlobT. FurcheA. SellersDatalog Reloaded – 1st International WorkshopDatalog 2010, Oxford, UK, March 16–19, 2010Lect. Notes Comput. Sci.vol. 6702 (2011)Springer pp.181-220Revised Selected Papers</reference><reference label="[75]"><authors>J. Mattingley,S. Boyd</authors><title>CVXGEN: a code generator for embedded convex optimization</title><host>Optim. Eng.12 (1)(2012) pp.1-27</host></reference><reference label="[76]"><authors>W. Hart,J.-P. Watson,D. Woodruff</authors><title>Pyomo: modeling and solving mathematical programs in Python</title><host>Math. Program. Comput.3 (2011) pp.219-260</host></reference><reference label="[77]">S. Diamond,E. Chu,S. BoydCVXPY: a Python-embedded modeling language for convex optimizationversion 0.2<host>http://cvxpy.org/(May 2014)</host></reference><reference label="[78]"><authors>N. Rizzolo,D. Roth</authors><title>Modeling discriminative global inference</title><host>Proceedings of the IEEE International Conference on Semantic ComputingICSC(2007) pp.597-604</host></reference><reference label="[79]"><authors>G. Gordon,S. Hong,M. Dudík</authors><title>First-order mixed integer linear programming</title><host>Proceedings of the 25th Conference on Uncertainty in Artificial IntelligenceUAI(2009) pp.213-222</host></reference><reference label="[80]"><authors>E. Zawadzki,G. Gordon,A. Platzer</authors><title>An instantiation-based theorem prover for first-order programming</title><host>Proceedings of the 14th International Conference on Artificial Intelligence and StatisticsAISTATSvol. 15 (2011) pp.855-863</host></reference><reference label="[81]"><authors>F. Margot</authors><title>Symmetry in integer linear programming</title><host>M. JüngerT. LieblingD. NaddefG. NemhauserW. PulleyblankG. ReineltG. RinaldiL. Wolsey50 Years of Integer Programming 1958–2008: From the Early Years to the State-of-the-Art(2010)Springer pp.1-40</host></reference><reference label="[82]"><authors>T. Achterberg,R. Wunderling</authors><title>Mixed integer programming: analysing 12 years of progress</title><host>M. JüngerG. ReineltFacets of Combinatorial Optimization: Festschrift for Martin Grötschel(2002)Springer pp.449-481</host></reference><reference label="[83]">Gurobi Optimization, Inc., Gurobi optimizer reference manual, http://www.gurobi.com, 2014.</reference><reference label="[84]"><authors>M. Sellmann,P. Van Hentenryck</authors><title>Structural symmetry breaking</title><host>Proceedings of 19th International Joint Conference on Artificial IntelligenceIJCAI(2005) pp.298-303</host></reference><reference label="[85]"><authors>R. Bödi,T. Grundhöfer,K. Herr</authors><title>Symmetries of linear programs</title><host>Note Mat.30 (1)(2010) pp.129-132</host></reference><reference label="[86]">T. Berthold, M. Pfetsch, Detecting orbitopal symmetries, 2009.</reference><reference label="[87]"><authors>J. Noessner,M. Niepert,H. Stuckenschmidt Rockit</authors><title>Exploiting parallelism and symmetry for map inference in statistical relational models</title><host>Proceedings of the 27th AAAI Conference on Artificial IntelligenceAAAI(2013)</host></reference><reference label="[88]"><authors>M. Mladenov,A. Globerson,K. Kersting</authors><title>Efficient lifting of MAP LP relaxations using k-locality</title><host>Proceedings of the 17th Int. Conf. on Artificial Intelligence and StatisticsAISTATSJ. Mach. Learn. Res. Workshop Conf. Proc.vol. 33 (2014)</host></reference><reference label="[89]"><authors>M. Mladenov,A. Globerson,K. Kersting</authors><title>Lifted message passing as reparametrization of graphical models</title><host>Proceedings of the 30th Int. Conf. on Uncertainty in Artificial IntelligenceUAI(2014)</host></reference><reference label="[90]"><authors>T. Lu,C. Boutilier</authors><title>Value-directed compression of large-scale assignment problems</title><host>Proceedings of the 29th AAAI Conference on Artificial IntelligenceAAAI(2015)</host></reference><reference label="[91]"><authors>J. Bi,K. Bennett,M. Embrechts,C. Breneman,M. Song</authors><title>Dimensionality reduction via sparse support vector machines</title><host>J. Mach. Learn. Res.3 (2003) pp.1229-1243</host></reference><reference label="[92]"><authors>A. Demiriz,K. Bennett,J. Shawe-Taylor</authors><title>Linear programming boosting via column generation</title><host>Mach. Learn.46 (1–3)(2002) pp.225-254</host></reference><reference label="[93]"><authors>V. Conitzer,T. Sandholm</authors><title>Computing the optimal strategy to commit to</title><host>Proceedings 7th ACM Conference on Electronic CommerceEC(2006) pp.82-90</host></reference><reference label="[94]"><authors>V. Saraswat,V. Gupta,R. Jagadeesan,P. Panangaden,D. Precup,F. Rossi,P. Sen</authors><title>Probabilistic constraint programming</title><host>Working Notes of the 2014 NIPS Workshop on Probabilistic Programming(2014)</host></reference><reference label="[95]"><authors>S. Bach,M. Broecheler,L. Getoor,D. O'leary</authors><title>Scaling MPE inference for constrained continuous Markov random fields with consensus optimization</title><host>Proceedings of the International Conference on Neural Information Processing SystemsNIPS(2012) pp.2654-2662</host></reference><reference label="[96]"><authors>A. Airola,S. Pyysalo,J. Björne,T. Pahikkala,F. Ginter,T. Salakoski</authors><title>All-paths graph kernel for protein–protein interaction extraction with evaluation of cross-corpus learning</title><host>BMC Bioinform.9 (Suppl. 11)(2008) pp.S2-</host></reference><reference label="[97]"><authors>J. Suzuki,T. Hirao,Y. Sasaki,E. Maeda</authors><title>Hierarchical directed acyclic graph kernel: methods for structured natural language data</title><host>Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsACL(2003) pp.32-39</host></reference><reference label="[98]">D. HausslerConvolution kernels on discrete structuresTech. rep., UCSC-CRL-99-10<host>(1999)UC Santa Cruz</host></reference><reference label="[99]"><authors>T. Gärtner</authors><title>Exponential and geometric kernels for graphs</title><host>Working Notes of the NIPS Workshop on Unreal Data: Principles of Modeling Nonvectorial Data(2002) pp.49-58</host></reference><reference label="[100]"><authors>T. Horváth,T. Gärtner,S. Wrobel</authors><title>Cyclic pattern kernels for predictive graph mining</title><host>Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningKDD(2004) pp.158-167</host></reference><reference label="[101]"><authors>K.M. Borgwardt,H.-P. Kriegel</authors><title>Shortest-path kernels on graphs</title><host>Proceedings of the 5th IEEE International Conference on Data MiningICDM(2005)</host></reference></references><footnote><note-para label="1">We refer to e.g. [1], [2], [3], [4] and references therein for overviews.</note-para><note-para label="2">http://www.neos-server.org/neos/report.html; accessed on April 19, 2014.</note-para><note-para label="3">For the sake of simplicity, we here assume that there are exactly one source and one sink vertex. If one wants to enforce this, we could simply add this as a logical constraint within the selection query, resulting in an empty objective if there are several source and sink notes. In AMPL, one would use additional check statements to express such restrictions, which cannot be expressed using simple inequalities.</note-para><note-para label="4">Investigating free variables in the objective that in turn are bound by a leading {an inline-figure} statement is an attractive avenue for future work as it allows one to produce several LPs with shared constraints and to embed relational linear programs back into logic programming. Doing so goes beyond the scope of the present paper.</note-para><note-para label="5">We will sometimes also call them par-equations.</note-para><note-para label="6">Depending on the use case, some predicates may be defined as part of the LP, while others may be defined in an external database.</note-para><note-para label="7">They can also be used withing the LP part of RLPs.</note-para><note-para label="8">Note that our definition is slightly modified from the original one in [38], since we are interested in rectangular matrices (equivalent to weighted bipartite graphs in [38]).</note-para><note-para label="9">To see why this is the case, consider the real numbers {a mathematical formula}a1≤b1 and {a mathematical formula}a2≤b2. For any positive {a mathematical formula}s1,s2, we have {a mathematical formula}s1a1+s2a2≤s1b1+s2b2. We generalize this by induction to any finite number of variables and apply to {a mathematical formula}S(Ax)i=∑jSij(Ax)j≤∑jSijbj=(Sb)i, as S is positive and {a mathematical formula}(Ax)j≤bj by assumption.</note-para><note-para label="10">Transductive inference is a mode of direct reasoning from observed to unobserved instances without an inductive hypothesis finding problem [54].</note-para><note-para label="11">http://zverovich.net/2013/01/01/neos-statistics-for-2012.html. In 2013, it is still the far more popular than CPLEX although non-linear solvers were more in focus, see http://zverovich.net/2014/01/02/neos-statistics-for-2013.html. Both webpages were queried on Nov. 29, 2014.</note-para><note-para label="12">All major solver suites such as GUROBI and CPLEX feature APIs for this. We refer to the corresponding manuals.</note-para></footnote></root>