<?xml version="1.0" encoding="UTF-8"?><root><url>https://www.sciencedirect.com/science/article/pii//S0004370216301540</url><title>A unified framework of active transfer learning for cross-system recommendation</title><authors>Lili Zhao,Sinno Jialin Pan,Qiang Yang</authors><abstract>In the past decade, artificial intelligence (AI) techniques have been successfully applied to recommender systems employed in many e-commerce companies, such as Amazon, eBay, Netflix, etc., which aim to provide personalized recommendations on products or services. Among various AI-based recommendation techniques, collaborative filtering has proven to be one of the most promising methods. However, most collaborative-filtering-based recommender systems, especially the newly launched ones, have trouble making accurate recommendations for users. This is caused by the data sparsity issue in recommender systems, where little existing rating information is available. To address this issue, one of the most effective practices is applying transfer learning techniques by leveraging relatively rich collaborative data knowledge from related systems, which have been well running. Previous transfer learning models for recommender systems often assume that a sufficient set of entity correspondences (either user or item) across the target and auxiliary systems (a.k.a. source systems) is given in advance. This assumption does not hold in many real-world scenarios where entity correspondences across systems are usually unknown, and the cost of identifying them can be expensive. In this paper, we propose a new transfer learning framework for recommender systems, which relaxes the above assumption to facilitate flexible knowledge transfer across different systems with low cost by using an active learning principle to construct entity correspondences across systems. Specifically, for the purpose of maximizing knowledge transfer, we first iteratively select entities in the target system based on some criterion to query their correspondences in the source system. We then plug the actively constructed entity correspondences into a general transferred collaborative-filtering model to improve recommendation quality. Based on the framework, we propose three solutions by specifying three state-of-the-art collaborative filtering methods, namely Maximum-Margin Matrix Factorization, Regularized Low-rank Matrix Factorization, and Probabilistic Matrix Factorization. We perform extensive experiments on two real-world datasets to verify the effectiveness of our proposed framework and the three specified solutions for cross-system recommendation.</abstract><keywords>Transfer learning;Active learning;Recommender systems</keywords><content><section label="1"><section-title>Introduction</section-title><paragraph>With the development and explosion of Web 1.0 and 2.0 technologies, recommender systems have been part of the Internet in the past two decades to provide recommendations on items, e.g., products or services. The goal of recommender systems is to suggest personalized items that match the interests of each specific user [1].{sup:1} To understand users' interests, a typical method is to ask users to fill their personal information and answer some predefined questions, and then summarize the information to build specific profiles for each user manually. With the built user profiles, one can recommend relevant items to each user. Instead of manually conducting surveys to build users profiles and generating recommended item list, modern recommender systems have been adopting various artificial intelligence (AI) techniques to learn users profiles, predict users' intensions, and recommend items of interest automatically. In general, commonly used AI techniques for recommender systems include collaborative filtering (CF), content-based filtering, case-based reasoning, constraint satisfaction with a domain-dependent knowledge base, etc. [2], [3]. Among these AI techniques, CF, especially matrix factorization, has proven to be one of the most promising methods, and been successfully used in many commercial recommender systems.</paragraph><paragraph>The goal of CF-based recommender systems is to generate item recommendations for a user by utilizing the observed preferences of other users whose historical behaviors are correlated with those of the target user. However, most CF-based recommender systems perform poorly when little collaborative information, e.g., historical ratings on items, is available. This is referred to as the data sparsity problem, which is one of the most common and major challenging problems in many newly launched recommender systems. To address the data sparsity problem, transfer learning has been proposed by exploiting auxiliary collaborative data from some related recommender system(s). In a nutshell, transfer learning is a promising paradigm of machine learning, which aims to adapt a predictive model across different domains or tasks with little additional human supervision by extracting and transferring common knowledge across the source and the target domains or tasks [4], [5]. To make a success of transfer learning, discovering commonality between the source and the target domains or tasks is crucial. A motivation behind applying transfer learning to recommender systems is that many users are active in multiple social medias (e.g., Twitter, Facebook, etc.), or purchasing products from various e-commercial websites (e.g., Amazon, Taobao, etc.), thus, a source CF model built with rich collaborative data can be compressed by identifying a same set of users across different websites as a prior to assist the training of a more precise CF model for the target recommender system [6], [7]. This approach is also known as cross-system CF.</paragraph><paragraph>Previous transfer-learning approaches to cross-system CF can be classified into two categories: 1) CF methods with cross-system entity correspondences, and 2) CF methods without cross-system entity correspondences. In the former category, Mehta and Hofmann [8] and Pan et al.  [9], [10], respectively, proposed to embed the cross-system entity correspondences as constraints to jointly learn the CF models for the source and the target recommender systems with an aim to improve the performance of the target CF system. Although these approaches have shown promising results, they require the existence of fully or sufficient entity-correspondence mappings, i.e., user correspondence or item correspondence, across different systems. This strong prerequisite is often difficult to satisfy in most real-world scenarios, as some specific users or items in one system may be missing in other systems. For example, user populations of Twitter and Facebook services are overlapping, but not identical, as is the case with Amazon and eBay. In addition, even though there may exist potential entity-correspondence mappings between different systems, they may be expensive or time-consuming to be recognized as users may use different names, or an item may be named differently in different online commercial systems. As illustrated in Fig. 1, we have two movie recommender systems A and B. In general, different movies may share a same name. For instance, the movie “The Island” can be referred to as a American science fiction/thriller film released in 2005, or a Russian biographical film about a fictional 20th century Eastern Orthodox monk released in 2006. In this case, for the movie “The Island” in system A, we are not sure which version of the movie “The Island” in system B is its correspondence. Therefore, to identify whether two movies are corresponding to each other, one may need to compare their meta data or even need to watch the trailers if the meta information is missing, which can be very time-consuming.</paragraph><paragraph>In the latter category of approaches where no assumption is made on pre-existing cross-system mappings, researches have been focused on capturing the group-level behaviors of users for knowledge transfer. For example, Li et al.  [6] proposed a codebook-based-transfer (CBT) method for cross-system CF. The main assumption of CBT is that specific users or items may be different across systems, but the groups of them, e.g., based on interests, ages, etc., should behave similarly. Therefore, CBT aims to first generate a set of cluster-level user-item rating patterns from the source system, which is referred to as a codebook. The codebook can then be used as a prior for learning a CF model for the target system. Li et al.  [7] further proposed a probabilistic model for cross-system CF that shares a similar motivation with CBT. Compared to the approaches in the former category, which make use of cross-system entity correspondences as a bridge, however, these approaches are less effective for knowledge transfer across recommender systems.</paragraph><paragraph>In this paper, we assume that the cross-system entity correspondences are unknown in general, but that the mappings can be identified with cost. Based on this assumption, we propose a unified framework to actively construct entity-correspondence mappings across recommender systems, where a flexible transfer learning approach with partial entity-correspondence mappings between systems and a strategy for actively constructing cross-system correspondences are integrated. To be specific, the proposed framework consists of two main components:</paragraph><list><list-item label="•">an active learning approach to constructing entity correspondences across systems in an iterative manner, and</list-item><list-item label="•">an extended matrix factorization approach to cross-system CF that flexibly leverages partial entity-correspondence mappings as a bridge for knowledge transfer.</list-item></list><paragraph>The proposed framework is general, where various extended matrix factorization methods in a transfer learning manner can be integrated. In this paper, we offer three specific solutions by extending and plugging three well-known CF methods, namely Maximum-Margin Matrix Factorization (MMMF), Regularized Low-rank Matrix Factorization (RLMF), and Probabilistic Matrix Factorization (PMF) into the framework. Specifically, we extend the three popular CF methods in the transfer learning manner such that they can flexibly transfer knowledge across domains with partial cross-domain entity correspondences, and propose specific active learning strategies on top of the extended CF methods to actively construct entity correspondences across domains. Note that matrix factorization based CF is a rapidly advancing research area, where several new techniques are proposed every year. According to a recent comparative study of CF algorithms [11], various state-of-the-art matrix factorization methods are comparable to each other in general. Some methods work better on relatively smaller datasets, while others work better on larger datasets, in terms of user or item size. Some methods work better when historical ratings of items by users are sparse, while others work better when there are sufficient historical ratings. In this work, our focus is not discussing which matrix factorization method can be adapted into our framework to achieve the best performance for knowledge transfer across different recommender systems, but providing a general active transfer learning framework, where researchers can extend their favor matrix factorization methods for different applications and datasets.</paragraph><paragraph>Compared to our previous work [12], the contributions of this paper are summarized as follows.</paragraph><list><list-item label="1.">We generalize the MMMF-based active transfer learning method proposed in [12] to a unified framework.</list-item><list-item label="2.">Based on the framework, we further specify two more solutions based on other well-known CF methods, namely RLMF and PMF.</list-item><list-item label="3.">We conduct extensive experiments to verify the effectiveness of the proposed active transfer learning framework for cross-system CF.</list-item></list><paragraph>The rest of this paper is organized as follows. In the following section, we start by reviewing some related work. In Section 3, we introduce the notations and preliminaries used in this paper. In Section 4, we first describe the proposed framework at a high level, and then present three specific solutions in detail. After that we show experiments that are conducted on two real-world datasets to verify the effectiveness of the proposed framework and the three specific solutions in Section 5. Finally, we conclude our work in Section 6.</paragraph></section><section label="2"><section-title>Related work</section-title><paragraph>Recommender systems emerged as an independent research area in the mid-1990s [13], [14], and attracted more and more attention from both academia and industry since the Netflix competition{sup:2} held between 2006 and 2009. Besides products/services recommendation for e-commerce, recommender systems have been employed in many other application areas, such as configuration for products design [15], requirement engineering [16], music recommendation [17], tag recommendation in the social web [18], etc. Among various techniques for recommender systems, AI-based methods, specially CF methods through matrix factorization, have proven to be effective and promising [19], [11].</paragraph><paragraph>Besides the work mentioned in Section 1, there is some other related work on applying transfer learning to cross-system CF. For instance, Pan et al.  [20] proposed an approach known as TIF (Transfer by Integrative Factorization) to integrate the auxiliary uncertain ratings, which are distributions of ratings instead of exact point-wise scores, as constraints into the target matrix factorization problem. Cao et al.  [21] and Zhang et al.  [22], respectively, extended the Collective Matrix Factorization (CMF) [23] method to solve multi-domain CF problems in a multi-task learning manner. In their work, a CF model is learned for each domain by exploiting the correlations or similarities among multiple domains. Tang et al.  [24] applied collective learning techniques for multi-domain Web search with implicit feedbacks. However, most of the existing methods require that the entities, either users or items, across different systems to be identical, and the correspondences between domains are given in advance. As we discussed in the previous section, this assumption is impractical in many real-world scenarios.</paragraph><paragraph>Our work is also related to some previous work on active learning for CF [25], [26], [27], [28], [29], which aims to solve the sparsity problem in CF by actively querying users to give ratings on selected items. A common assumption behind this direction of work is that the users queried by an active learner are always able to provide ratings on the selected items in the system. However, in many real-world scenarios, this assumption is hard to satisfy because users may only be familiar with some items in the system, and may fail to provide ratings on the actively selected items. For example, a user may only watched a few movies, and is not able to provide accurate ratings on other movies that he/she has not watched. Alternatively, in this paper, instead of actively asking users to give ratings on selected items, we propose to actively construct cross-system entity correspondence mappings as a bridge for knowledge transfer across recommender systems to solve the data sparsity problem in the target recommender system.</paragraph><paragraph>Another related research area is to develop a unified framework for active learning and transfer learning. Previous work in this research direction is focused on how to actively query instances in the target domain for labels in order to learn a target predictive model by leveraging source domain knowledge [30], [31], [32], [33], [34], [35], [36]. Existing methods can be classified into three categories: 1) performing transfer and active learning once, respectively, in a pipeline [37], [30], [31], 2) performing transfer and active learning alternatingly [32], [33], [34], [36], and 3) integrating transfer and active learning into a unified optimization problem [35]. Most of them are focused on classification or regression problems. Different from previous work, in this paper, our study on active transfer learning is focused on addressing the data sparsity problem for CF instead of traditional classification or regression problems. Moreover, the proposed active learning strategy aims to construct entity correspondences between systems instead of querying a “class label” for an instance. Therefore, existing methods on combining active learning and transfer learning cannot be directly applied to our problem.</paragraph></section><section label="3"><section-title>Notations &amp; preliminaries</section-title><paragraph>Denote by {a mathematical formula}D the target CF task, which is associated with an extremely sparse preference matrix {a mathematical formula}X(d)∈Rmd×nd, where {a mathematical formula}md is the number of users and {a mathematical formula}nd is the number of items. Each entry {a mathematical formula}xuv(d) of {a mathematical formula}X(d) corresponds to user u's preference on item v. If {a mathematical formula}xuv(d)≠0, it means for user u, the preference on item v is observed, otherwise unobserved. Let {a mathematical formula}I(d) be the set of all observed {a mathematical formula}(u,v) pairs of {a mathematical formula}X(d). The goal is to predict users' unobserved preferences based on a few historically observed preferences. For rating-based recommender systems, preferences are represented by numerical values (e.g., {a mathematical formula}[1,2,...,5], or one star through five stars). In cross-system CF, besides {a mathematical formula}D, suppose we have a source CF task {a mathematical formula}S which is associated with a relatively dense preference matrix {a mathematical formula}X(s)∈Rms×ns, where {a mathematical formula}ms is the number of users and {a mathematical formula}ns is the number of items. Similarly, let {a mathematical formula}I(s) be the set of all observed {a mathematical formula}(u,v) pairs of {a mathematical formula}X(s). Furthermore, we assume that cross-system entity correspondences are unknown, but can be identified with cost. Given a budget in terms of the maximum number of cross-system entity correspondences to be constructed, our goal is to 1) actively construct entity correspondences across the source and the target systems, and 2) make use of them for knowledge transfer from the source task {a mathematical formula}S to the target task {a mathematical formula}D. In the sequel, we denote by {a mathematical formula}X⁎,i the ith column of the matrix X, and superscript {sup:⊤} the transpose of a vector or matrix, and use the words “domain” and “system” interchangeably.</paragraph><section label="3.1"><section-title>Matrix factorization for collaborative filtering</section-title><paragraph>Matrix factorization [38], [39], [19] is one family of state-of-the-art algorithms in CF [40]. In matrix factorization for CF, given a sparse matrix X, one can model the users and items using low-rank factor matrices {a mathematical formula}U∈Rk×m and {a mathematical formula}V∈Rk×n, respectively, where the uth user and the vth item are represented by {a mathematical formula}U⁎u and {a mathematical formula}V⁎v, i.e., the uth and vth column of U and V, respectively. The objective of most matrix factorization methods for CF can be summarized in a general minimization problem as follows,{a mathematical formula} where {a mathematical formula}ℓ(⋅) is a loss function of factorization on the target rating matrix X, and Θ is a set of parameters. The second term in the objective is a regularization term on the low-rank factor matrices of users and items, and {a mathematical formula}λ≥0 is a trade-off parameter.</paragraph><paragraph>Different forms of the loss function {a mathematical formula}ℓ(⋅) lead to different approaches. Some popular loss functions include the Hinge loss with the form {a mathematical formula}h(z)=(1−z)+=max⁡(0,1−z), the negative-log-posterior loss {a mathematical formula}ℓ=−ln⁡p(U,V|X) or equivalently the sum-of-squared-errors loss {a mathematical formula}ℓ=∑u,v(xuv−U⁎u⊤V⁎v)2. In the following sections, we review three popular matrix factorization methods namely Maximum-Margin Matrix Factorization (MMMF) [41], Regularized Low-rank Matrix Factorization (RLMF) [19], and Probabilistic Matrix Factorization (PMF) [42], which will be extended and plugged in our proposed framework for cross-system CF.</paragraph><section label="3.1.1"><section-title>Maximum-Margin Matrix Factorization</section-title><paragraph>MMMF [41] aims to learn a fully observed matrix {a mathematical formula}Y∈Rm×n to approximate the target preference matrix {a mathematical formula}X∈Rm×n by maximizing the predictive margin and minimizing the trace norm of Y. Specially, in binary preference predictions, {a mathematical formula}xuv∈{−1,+1}, where {a mathematical formula}xuv=+1 denotes that the user u likes the item v, while {a mathematical formula}xuv=−1 denotes dislike. By considering hard-margin matrix factorization, the goal of MMMF is to find a minimum trace norm matrix Y that matches the observed preferences with a margin of one, i.e., {a mathematical formula}yuvxuv≥1 for all {a mathematical formula}(u,v)∈I, where {a mathematical formula}I is the set of observed {a mathematical formula}(u,v) pairs of X. By introducing slack variables {a mathematical formula}ξuv≥0, the hard-margin constraint can be relaxed by requiring {a mathematical formula}yuvxuv≥1−ξuv for all {a mathematical formula}(u,v)∈I, and minimizing a trade-off between the trace norm and the slack. This results in the following objective,{a mathematical formula} where {a mathematical formula}h(z)=(1−z)+=max⁡(0,1−z) is the Hinge loss, {a mathematical formula}‖⋅‖Σ denotes the trace norm, and {a mathematical formula}λ≥0 is a trade-off parameter. As proposed in [38], the objective (2) can be extended to ordinal rating predictions, and solved efficiently. To be specific, suppose {a mathematical formula}xuv∈{1,2,...,R}, one can use {a mathematical formula}R−1 thresholds {a mathematical formula}θ1,θ2,...,θR−1 to relate the predicted real-valued {a mathematical formula}yuv to the discrete-valued {a mathematical formula}xuv by requiring{a mathematical formula} where {a mathematical formula}θ0=−∞ and {a mathematical formula}θR=∞. When adding slack in this case, not only the violation of the two immediate constraints {a mathematical formula}θxuv−1+1≤yuv and {a mathematical formula}yuv≤θxuv−1, but also the violation of all other implied threshold constraints {a mathematical formula}θr+1≤yuv for {a mathematical formula}r&lt;xuv and {a mathematical formula}yuv≤θr−1 for {a mathematical formula}r≥xuv should be penalized. The goal by doing this is to emphasize the cost of crossing multiple rating-boundaries and yield a loss function that upper bounds the mean-absolute-error (MAE). By further supposing that Y can be decomposed as {a mathematical formula}Y=U⊤V, where {a mathematical formula}U∈Rk×m and {a mathematical formula}V∈Rk×n. The objective of MMMF for ordinal rating predictions can be written as follows,{a mathematical formula} where {a mathematical formula}Tuvr=+1 for {a mathematical formula}r≥xuv, while {a mathematical formula}Tuvr=−1 for {a mathematical formula}r&lt;xuv, and {a mathematical formula}‖⋅‖F denotes the Frobenius norm. The thresholds {a mathematical formula}Θ={θur}'s can be learned together with U and V from the data. Note that the thresholds here are user-specific, i.e., for a same user u, the values of the corresponding thresholds {a mathematical formula}θur's are the same, while for different users u's, the values of the corresponding thresholds {a mathematical formula}θur's can be different. The alternating minimization approach can be applied to solve the optimization problem [38], [19]: iteratively keep two of U, V and Θ fixed and optimize over the other using gradient-descent approaches, then switch and repeat.</paragraph></section><section label="3.1.2"><section-title>Regularized Low-Rank Matrix Factorization</section-title><paragraph>RLMF [19] is a matrix factorization approach based on regularized Singular Value Decomposition (SVD) on sparse matrices. The objective of RLMF is to solve the following minimization problem,{a mathematical formula} where {a mathematical formula}r¯ is the observed overall averaged rating, {a mathematical formula}bu and {a mathematical formula}bv indicate the bias of user u and item v, respectively. The second term of the objective consists of a set of regularization terms on U, V, {a mathematical formula}bu and {a mathematical formula}bv, respectively, and λ is a trade-off parameter to control the impact of the regularization terms. A local minimum of the objective (4) can be obtained by performing gradient descent on the objective with respect to U, V, {a mathematical formula}bu and {a mathematical formula}bv, alternatingly.</paragraph></section><section label="3.1.3"><section-title>Probabilistic Matrix Factorization</section-title><paragraph>PMF [42] adopts a probabilistic model with Gaussian observation noise, and aims to maximize the following conditional distribution over the observed ratings,{a mathematical formula} where {a mathematical formula}N(xuv|U⁎u⊤V⁎v,σ2) is a probability density function of a Gaussian distribution with the mean {a mathematical formula}μ=U⁎u⊤V⁎v and the variance {a mathematical formula}σ2, and {a mathematical formula}Iuv is the indicator function that is equal to 1 if the user u rates the item v, i.e., {a mathematical formula}(u,v)∈I, and equal to 0 otherwise. To bound predictions within the range of valid rating values, the logistic function {a mathematical formula}g(x)=1/(1+exp(−x)) is post-performed on the dot product between the user- and item-specific latent vectors, and the function {a mathematical formula}t(x)=(x−1)/(R−1) is used to map the ratings {a mathematical formula}{1,2,...,R} to the interval {a mathematical formula}[0,1] so that the range of valid rating values matches the range of predictions. This results in the following conditional distribution:{a mathematical formula} Moreover, usually, zero-mean spherical Gaussian priors are placed on the user and item latent vectors, respectively,{a mathematical formula}</paragraph><paragraph>In this paper, we adopt the constraint version of PMF proposed in [42], which introduces a new latent similarity constraint matrix {a mathematical formula}H∈Rk×n on users. As a result, the latent vector of the user u can be represented by{a mathematical formula} where {a mathematical formula}Iuh is the indicator function that is equal to 1 if the user u rates the item h, and equal to 0 otherwise, {a mathematical formula}H⁎h captures the effect of a user having rated a particular item v on the prior mean of the user's latent vector, and {a mathematical formula}Y⁎u can be seen as the offset added to the mean of the prior distribution to get the latent vector {a mathematical formula}U⁎u for the user u. By plugging (8) into (6), we obtain a new conditional distribution over the observed ratings as follows,{a mathematical formula} where the Gaussian prior on U in (7) is replaced by the one on Y, and an additional zero-mean spherical Gaussian prior is placed on the latent similarity constraint matrix H:{a mathematical formula} Based on the conditional distribution in (9), it can be proven that maximizing the log-posterior {a mathematical formula}ln⁡p(Y,V,H|X,σ2,σY2,σV2,σH2) over the user and item factor matrices with the priors is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms formulated as follows,{a mathematical formula} where {a mathematical formula}λY=σ2/σY2, {a mathematical formula}λV=σ2/σV2 and {a mathematical formula}λH=σ2/σH2 are trade-off parameters to balance the effect among the approximation error term and the regularization terms. A local minimum of the objective function (11) can be found by performing gradient decent on the objective respect to Y, V and H, alternatingly.</paragraph></section></section></section><section label="4"><section-title>Active transfer learning for cross-system collaborative filtering</section-title><section label="4.1"><section-title>A unified framework</section-title><paragraph>The overall general framework on active transfer learning for cross-system CF is described in Algorithm 1. To begin with, we apply a base matrix factorization method f, which will be specified in particular solutions, on the target collaborative data to learn a CF model for initialization. After that, we iteratively perform the following three steps:</paragraph><list><list-item label="•">We choose K entities based on an entity selection function {a mathematical formula}ActiveLearn(), which will be specified in particular solutions as well.</list-item><list-item label="•">We query their correspondences in the source system.</list-item><list-item label="•">We then apply the extended matrix factorization method {a mathematical formula}fTL in the transfer learning manner on both the source and target collaborative data to learn an updated CF model. Note that the entity selection function {a mathematical formula}ActiveLearn() is built on top of the base method f at the initial step or the extended method {a mathematical formula}fTL at each iteration.</list-item></list><paragraph>In the rest of this section, we first describe the idea of the extended matrix factorization method in the transfer learning manner {a mathematical formula}fTL by assuming a set of cross-system correspondences be constructed as input, and then present the high-level idea on how to design {a mathematical formula}ActiveLearn() to actively select entities for querying cross-system correspondences.</paragraph><section label="4.1.1">High-level idea on {a mathematical formula}fTL<paragraph>Denote by {a mathematical formula}UC(s) and {a mathematical formula}VC(s) the factor sub-matrices of {a mathematical formula}U(s) and {a mathematical formula}V(s) for the entities in the source system, whose indices are in {a mathematical formula}C, respectively. Similarly, denote by {a mathematical formula}UC(d) and {a mathematical formula}VC(d) the factor sub-matrices for the entities in the target system, whose indices are in {a mathematical formula}C, respectively. Here {a mathematical formula}C denotes the unified indices of the constructed corresponding entities, which can be either users or items, between the source and target systems. The general objective of the extended matrix factorization method {a mathematical formula}fTL with partial entity correspondences for cross-system CF can be written as follows,{a mathematical formula} where the last term is a regularization term that aims to use {a mathematical formula}UC(s) and {a mathematical formula}VC(s) as priors to learn more precise {a mathematical formula}UC(d) and {a mathematical formula}VC(d), which can be expanded to obtain more precise {a mathematical formula}U(d) and {a mathematical formula}V(d), respectively. The associated {a mathematical formula}λC≥0 is a trade-off parameter to control the impact of the regularization term.</paragraph><paragraph>Intuitively, a simple way to define the regularization term is to enforce the target factor sub-matrices {a mathematical formula}UC(d) and {a mathematical formula}VC(d) in target system to be the same as the source factor sub-matrices {a mathematical formula}UC(s) and {a mathematical formula}VC(s), respectively, i.e.,{a mathematical formula} where {a mathematical formula}WC(d)=[UC(d)VC(d)], {a mathematical formula}WC(s)=[UC(s)VC(s)] with {a mathematical formula}UC(d),UC(s)∈Rk×n1, {a mathematical formula}VC(d),VC(s)∈Rk×n2, and {a mathematical formula}n1+n2=|C|. The regularization term defined in (13) is based on an “identical” assumption on the factor sub-matrices {a mathematical formula}UC and {a mathematical formula}VC: the source and the target systems should share the same factor sub-matrices {a mathematical formula}UC and {a mathematical formula}VC, i.e., {a mathematical formula}UC(s)=UC(d)=UC and {a mathematical formula}VC(s)=VC(d)=VC. This assumption is similar to that used in CMF, and may be too restricted to satisfy in practice.</paragraph><paragraph>Alternatively, we propose to use the similarities between entities estimated in the source system as priors to constrain the similarities between the corresponding entities in the target system. The motivation is that if two entities in the source system are similar to each other, then their correspondences tend to be similar to each other in the target system as well. Therefore, we propose to use the following regularization term on the factor sub-matrices,{a mathematical formula} where {a mathematical formula}tr(⋅) denotes the trace of a matrix, and {a mathematical formula}LC(s)=[LU(s)00LV(s)], with {a mathematical formula}LU(s)=DU(s)−AU(s) and {a mathematical formula}LV(s)=DV(s)−AV(s), where {a mathematical formula}AU(s)=UC(s)⊤UC(s) and {a mathematical formula}AV(s)=VC(s)⊤VC(s) are the similarity matrices of the corresponding users and items in the source system, respectively. The diagonal matrices {a mathematical formula}DU(s) and {a mathematical formula}DV(s) are defined as {a mathematical formula}[DU(s)]ii=∑j[AU(s)]ij and {a mathematical formula}[DV(s)]ii=∑j[AV(s)]ij, respectively. The matrices {a mathematical formula}WC(s) and {a mathematical formula}WC(d) are as the same as defined in (13), and the matrices {a mathematical formula}LU(s) and {a mathematical formula}LV(s) are known as the Laplacian matrices [43]. Note that a similar regularization term has been proposed in [44]. However, their work is focused on utilizing relational information for single-domain CF, and the Laplacian matrix is constructed using links between entities in a single domain.</paragraph></section><section label="4.1.2">High-level idea on {a mathematical formula}ActiveLearn()<paragraph>Based on the extended matrix factorization method introduced above, intuitively, at each iteration, we should select entities (either items or users) in the target system, whose predictions by the current CF model are of most uncertainty, to query their correspondences in the source system. In this way, knowledge transferred from the source system can improve the prediction accuracy on the most uncertain target-system entities, and thus improve the overall prediction accuracy for the target system. Similar ideas have been widely used in many active learning approaches to various applications [45]. However, in the context of recommender systems, users' ratings on items typically follow a power-law distribution, which is also known as the long tail problem. Specifically, regarding items, the long tail is composed of a small number of popular items with lots of users' ratings, and the rest are located in the heavy tail, which are not sold well and only have few users' ratings. Similarly, regarding users, the long tail is composed of a small number of active users who give a lots of ratings on items, and the rest are located in the heavy tail, who are inactive to give items ratings. It has been shown that matrix-factorization-based CF methods usually fail to make confident predictions on the items (or users) for a specific user (or item), whose historical ratings are rare. Therefore, the items (or users) of the most uncertain predictions in the target recommender system tend to be in the tail with high probabilities. Furthermore, since we assume the source and the target recommender systems be similar, if the items (or users) are in the tail in the target system, then their correspondences tend to be in the tail in the source system as well. This implies that the predictions on the corresponding entities in the source system may not be precise either, resulting in limited knowledge transferred through the extended matrix factorization method. Therefore, besides focusing on prediction uncertainty, we need to take the long tail issue into consideration when designing an active entity selection strategy for building cross-system correspondences.</paragraph><paragraph>At a high level, we propose an active entity selection strategy for the target domain as follow. For simplicity in description, in this section and the subsequent sections, we only describe how to actively select users from the target system for querying corresponding users in the source system. Procedure on actively selecting items for correspondences construction is similar.</paragraph><list><list-item label="•">We first denote by {a mathematical formula}δ(d)(u,v) an certainty measure of the prediction of a matrix-factorization-based CF model on a user-item pair {a mathematical formula}(u,v) in the target system. The larger is the value, the more certain or confident is the prediction.</list-item><list-item label="•">With {a mathematical formula}δ(d)(u,v), we then define an entity-level certainty measure on a user, {a mathematical formula}δu(d), as follows,{a mathematical formula} where {a mathematical formula}Iu(d) denotes the item set of observed ratings given by user u in the target system, while {a mathematical formula}Iˆu(d) denotes the item set of unobserved ratings for user u in the target system. On the right hand side of the equation, the first term is the average of the certainty of predictions on the user-item pairs for user u, whose rating are observed, and the second term is the average of the certainty of predictions on the user-item pairs for user u, whose ratings are unobserved. The tradeoff parameter {a mathematical formula}η∈[0,1] is to balance the impact of the two terms to the overall certainty of the predictions on user u. In this paper, we simply set {a mathematical formula}η=0.5.</list-item><list-item label="•">At each round or iteration, in order to select K source-system users, we first select {a mathematical formula}K1 (&lt;K) users who are of the least certainty (i.e., the most uncertainty) based on {a mathematical formula}δu(d) to construct {a mathematical formula}C2, i.e., selecting {a mathematical formula}K1 users whose corresponding {a mathematical formula}δu(d)'s are of smallest values. After that for the rest users {a mathematical formula}{ui}'s, we select {a mathematical formula}K−K1 users with largest scores according to the following function to construct {a mathematical formula}C2,{a mathematical formula} where {a mathematical formula}sim(ui,uj)=|Iui(d)∩Iuj(d)|max(|Iui(d)|,|Iuj(d)|) is the correlation measure between the users {a mathematical formula}ui and {a mathematical formula}uj based on their rating behaviors. Finally, we set {a mathematical formula}C=C1∩C2, which is the set of K users to be selected. The motivation behind the scoring function (16) is that we aim to select users for constructing {a mathematical formula}C2, who are 1) informative, i.e., with large values of {a mathematical formula}{δui(d)}'s, and thus supposed to be “active” instead of being in the tail, and 2) of strong correlation to the pre-selected most uncertain users in {a mathematical formula}C1, i.e., with large values of {a mathematical formula}∑uj∈C1sim(ui,uj), and thus supposed to be helpful for generating reasonable recommendations based on the intrinsic assumption in CF.</list-item></list><paragraph> In the following sections, we introduce three particular solutions by equipping different base matrix factorization methods and their transfer learning extensions in the framework, and present their specific active correspondences construction approaches in detail.</paragraph></section></section><section label="4.2"><section-title>A solution equipped with Maximum-Margin Matrix Factorization</section-title><paragraph>In this section, we first present a particular solution based on MMMF. We start by introducing an extended MMMF method in the transfer learning manner with partial entity correspondences between the source and the target systems, and then present an approach to actively selecting entities to construct correspondences between systems.</paragraph><section label="4.2.1"><section-title>MMMF with partial entity correspondence</section-title><paragraph>By plugging the objective of MMMF (3) and the regularization term of cross-system entity correspondences (14) into the framework (12), we obtain the optimization problem of the extended MMMF method in the transfer learning manner with partial entity correspondences between systems as follows,{a mathematical formula} where {a mathematical formula}WC(d)=[UC(d)VC(d)], and {a mathematical formula}WC(s)=[UC(s)VC(s)]. In the sequel, we denote by {a mathematical formula}MMMFTL the optimization problem (17). For comparison on the impact of different regularization terms to transfer learning, we denote by {a mathematical formula}MMMFCMF the optimization problem by replacing the last regularization term in (17) by the CMF regularization term (13) as follows,{a mathematical formula}</paragraph></section><section label="4.2.2"><section-title>Actively constructing entity correspondences through MMMF</section-title><paragraph>As MMMF is a margin-based matrix factorization method, in this section, we present a margin-based approach for actively constructing cross-system entity correspondences. To implement the active entity selection strategy introduced in Section 4.1.2, we need to specify the certainty measure of the prediction on a user-item pair, {a mathematical formula}δ(d)(u,v), and a user, {a mathematical formula}δu(d), based on the extended MMMF method. A common motivation behind most margin-based active learning approaches [46], [47], [48] is that given a margin-based model, the margin of an example denotes certainty of the prediction on the example. The larger the margin is for an example, the higher the certainty is for its prediction. In the following, we start by defining a margin on a user-item pair.</paragraph><paragraph>Margins on user-item pairs. Suppose that MMMF (3) or {a mathematical formula}MMMFTL(17) has been applied to the collaborative data in the target system to learn the factor matrices {a mathematical formula}U(d) and {a mathematical formula}V(d). The margins of a prediction with respect to the thresholds {a mathematical formula}θ0,θ1,...,θR ({a mathematical formula}θ0=−∞ and {a mathematical formula}θR=+∞) in MMMF are illustrated in Fig. 2. Intuitively, given a user-item pair {a mathematical formula}(u,v) in the target domain, we expect the predicted rating by MMMF, {a mathematical formula}U(d)⁎,u⊤V⁎,v(d), to be in the correct interval {a mathematical formula}(θxuv−1,θxuv], and to be far from the boundaries (thresholds). Therefore, the margin of a user-item pair {a mathematical formula}(u,v) can be defined as{a mathematical formula} Similar to other margin-based active learning methods, here we assume that, for a user-item pair {a mathematical formula}(u,v) whose rating is not observed, the prediction of the current CF model is correct, i.e., {a mathematical formula}xu,v(d)=U(d)⁎,u⊤V⁎,v(d). Based on the above definition, for each user-item pair {a mathematical formula}(u,v), there are {a mathematical formula}R−1 margins. For instance, as shown in Fig. 2, for the circle point that denotes the predicted rating of a user-item pair {a mathematical formula}(u,v), the associated {a mathematical formula}R−1 margins denoted by {a mathematical formula}ρ1(u,v), {a mathematical formula}ρ2(u,v), ..., and {a mathematical formula}ρR−1(u,v) are the distances between the point and the {a mathematical formula}R−1 thresholds {a mathematical formula}θ1, {a mathematical formula}θ2, ..., and {a mathematical formula}θR−1, respectively. Among the {a mathematical formula}R−1 margins, the margins to the left (lower) and right (upper) boundaries of the correct interval are of the most importance, which are denoted by {a mathematical formula}ρL(d)(u,v) and {a mathematical formula}ρU(d)(u,v), respectively. For the circle point shown in Fig. 2, {a mathematical formula}ρL(d)(u,v)=ρ2(d)(u,v) and {a mathematical formula}ρU(d)(u,v)=ρ3(d)(u,v). Intuitively, for a user-item pair {a mathematical formula}(u,v), when the predicted rating is in the middle of the correct interval, i.e., {a mathematical formula}ρL(d)(u,v)=ρU(d)(u,v), the confidence of the prediction is the highest, because it is farthest from the two boundaries of the correct interval. Therefore, we define {a mathematical formula}δ(d)(u,v) as the normalized margin of a user-item pair {a mathematical formula}(u,v) as follows,{a mathematical formula} Note that {a mathematical formula}δ(d)(u,v)∈[b,1], where {a mathematical formula}b=min⁡(1−ρL(d)ρL(d)+ρU(d),1−ρU(d)ρL(d)+ρU(d)). When {a mathematical formula}ρL(d)(u,v)=ρU(d)(u,v), the margin obtains its maximum, i.e., {a mathematical formula}δ(d)(u,v)=1, and when {a mathematical formula}ρL(d)(u,v)=0 or {a mathematical formula}ρU(d)(u,v)=0, the margin obtains its minimum, i.e., {a mathematical formula}δ(d)(u,v)=b.</paragraph><paragraph>Margins on entities. With the normalized margin or certainty measure on each user-item pair defined in (20), we are able to define the overall margin or certainty measure on a user, {a mathematical formula}δu(d), using (15). With the margin-based {a mathematical formula}δu(d), we can implement the active entity selection strategy introduced in Section 4.1.2 with the extended MMMF method. In the sequel, we denote by {a mathematical formula}MGhy this active entity selection approach.</paragraph></section></section><section label="4.3"><section-title>A solution equipped with Regularized Low-Rank Matrix Factorization</section-title><paragraph>In this section, we present a second particular solution with RLMF. We start by introducing an extended RLMF method in the transfer learning manner with flexible entity correspondences between the source and the target systems, and then present an active entity selection approach to constructing cross-system correspondences based on the extended RLMF.</paragraph><section label="4.3.1"><section-title>RLMF with partial entity correspondence</section-title><paragraph>By plugging the objective of RLMF (4) and the regularization term of cross-system entity correspondences (14) into the framework (12), we obtain the optimization problem of the extended RLMF method in the transfer learning manner with partial entity correspondences between systems as follows,{a mathematical formula} In the sequel, we denote by {a mathematical formula}RLMFTL the optimization problem (21). For comparison on the impact of different regularization terms to transfer learning, we denote by {a mathematical formula}RLMFCMF the optimization problem by replacing the last regularization term in (21) with the CMF regularization term (13) as follows,{a mathematical formula}</paragraph></section><section label="4.3.2"><section-title>Actively constructing entity correspondences through RLMF</section-title><paragraph>Different from the margin-based approach to active correspondences construction, here, we present an error-based approach with the extended RLMF method for actively constructing entity correspondences. This approach does not aim to measure how much the model is likely to change, but how much its generalization error is likely to be reduced. The idea is to iteratively build new correspondences, with which the expected generalization error of the current CF model for the target system can be reduced to the utmost extent.</paragraph><paragraph>Expected errors on user-item pairs. Suppose that RLMF (4) or {a mathematical formula}RLMFTL(21) is fed with collaborative data in the target system to learn a CF model that predicts {a mathematical formula}yuv for each user-item pair. Given {a mathematical formula}bu, {a mathematical formula}bv, U, V, we can then write the expected error of the CF model as follows:{a mathematical formula} where {a mathematical formula}e(⋅) is some loss function that measures the degree of disagreement in difference between the true ratings X and the model's predictions Y. The proposed error-based active learning approach thus aims to select a set of queries {a mathematical formula}Q at each iteration to construct K more correspondences between the source and the target system in addition to the existing set of correspondences such that the resulting new CF model obtains lower generalization error than any other set of queries {a mathematical formula}Q′ of K correspondences construction, i.e.,{a mathematical formula} where {a mathematical formula}C is the set of cross-system correspondences used in the current CF model, which can be empty. In this paper, we adopt the sum-of-squared-errors loss {a mathematical formula}e(X,Y)=∑u,v(xuv(d)−yuv(d))2. Therefore, the error on each user-item pair {a mathematical formula}(u,v) can be calculated as:{a mathematical formula} where {a mathematical formula}yuv(d) = {a mathematical formula}xuv(d) if {a mathematical formula}xuv(d) is observed, otherwise,{a mathematical formula} where {a mathematical formula}r∈{1,...,R}. In this way, the uncertainty of a prediction on an instance can be measured by the expected error of the predictive model on the instance, the larger is the expected error, the more uncertain is the prediction. However, as defined in Section 4.1.2, {a mathematical formula}δ(d)(u,v) is a “certainty” measure, which is supposed to be larger when the corresponding prediction is more certain. Therefore, here, we define {a mathematical formula}δ(d)(u,v) as the negative of the expected error on a user-item pair {a mathematical formula}(u,v):{a mathematical formula}</paragraph><paragraph>Expected errors on entities. With the expected-error-based {a mathematical formula}δ(d)(u,v) on a user-item pair defined in (25), we can define the overall uncertainty measure on a user, {a mathematical formula}δu(d), using (15), and implement the active entity selection strategy introduced in Section 4.1.2 with the extended RLMF method. In the sequel, we denote by {a mathematical formula}EEhy this active entity selection approach.</paragraph></section></section><section label="4.4"><section-title>A solution equipped with Probabilistic Matrix Factorization</section-title><paragraph>In this section, we present the third solution based on PMF. We start by introducing an extended PMF method in the transfer learning manner with partial entity correspondences between the source and the target systems, and then present an active entity correspondences construction approach accordingly.</paragraph><section label="4.4.1"><section-title>PMF with partial entity correspondence</section-title><paragraph>By plugging the objective of PMF (11) and the regularization term of cross-system entity correspondences (14) into the framework (12), we obtain the optimization problem of the extended PMF method in the transfer learning manner with partial entity correspondences between systems as follows,{a mathematical formula} where{a mathematical formula} In the sequel, we denote by {a mathematical formula}PMFTL the optimization problem (26). For comparison on the impact of different regularization terms to transfer learning, we denote by {a mathematical formula}PMFCMF the optimization problem by replacing the last regularization term in (26) by the CMF regularization term (13) as follows,{a mathematical formula}</paragraph></section><section label="4.4.2"><section-title>Actively constructing entity correspondences through PMF</section-title><paragraph>With the probabilities on predictions generated by PMF, we present an entropy-based method for actively constructing entity correspondences across domains, which attempts to sequentially minimize the expected entropy of the predictions.</paragraph><paragraph>Entropy on user-item pairs. Suppose that PMF (11) or {a mathematical formula}PMFTL(26) is performed on the collaborative data in the target system to learn a CF model. Given a user-item pair {a mathematical formula}(u,v), the entropy of the prediction {a mathematical formula}yuv given by the current model can be defined as,{a mathematical formula} where {a mathematical formula}z(d)(u,v)=N(yuv(d)|g(U⁎u(d)⊤V⁎v(d)),σ2), and {a mathematical formula}U⁎u(d)=Y⁎u(d)+∑h=1nIuhH⁎h(d)∑w=1nIuh. If {a mathematical formula}xuv(d) is observed, {a mathematical formula}yuv(d)=xuv(d), where {a mathematical formula}xuv(d) has been transformed to {a mathematical formula}[0,1] through {a mathematical formula}t(x)=x−1R−1, otherwise, {a mathematical formula}yuv(d)=t(ruv(d)), where{a mathematical formula}</paragraph><paragraph>In this way, the uncertainty of a prediction on an instance can be measured the entropy of the prediction, the larger is the entropy, the more uncertain is the prediction. Therefore, similar to error-based certainty measure, we define {a mathematical formula}δ(d)(u,v) as the negative of the entropy of the prediction on a user-item pair {a mathematical formula}(u,v):{a mathematical formula}Entropy on entities. With the entropy-based {a mathematical formula}δ(d)(u,v) on a user-item pair defined in (29), we can define the overall uncertainty measure on a user, {a mathematical formula}δu(d), using (15), and implement the active entity selection strategy introduced in Section 4.1.2 with the extended PMF method. In the sequel, we denote by {a mathematical formula}EShy this active entity selection approach.</paragraph></section></section></section><section label="5"><section-title>Experiments</section-title><section label="5.1"><section-title>Datasets and experimental setting</section-title><paragraph>We evaluate our proposed framework on two datasets: Netflix{sup:3} and Douban.{sup:4} The Netflix dataset contains more than 100 millions ratings given by more than {a mathematical formula}480,000 users on {a mathematical formula}17,770 movies with ratings in {a mathematical formula}{1,2,3,4,5}. And Douban is a popular recommendation website in China, which has over 100 millions users. It mainly provides three recommendation services, including movies, books and music with rating scale in {a mathematical formula}{1,2,3,4,5} as well.</paragraph><paragraph>For the Netflix dataset, we filter out movies with less than 5 ratings for our experiments. The dataset is partitioned into two parts along two disjoint sets of users with a same set of movies. One part consists of ratings given by {a mathematical formula}50% users with {a mathematical formula}1.2% rating density, which serves as the source domain. The remaining users are considered as the target domain with {a mathematical formula}0.7% rating density. For the Douban dataset, we crawl a set consisting of 12,000 users and 100,000 items with only movies and books. Users with less than 10 ratings are discarded. There remain 270,000 ratings on 3,500 books, and 1,400,000 ratings on 8,000 movies, given by {a mathematical formula}11,000 users. The density of the ratings on books and movies are {a mathematical formula}0.6% and {a mathematical formula}1.5%, respectively. We consider movie ratings as the source domain and book ratings as the target domain. In this task, all users are shared but items are disjoint. Furthermore, since there are about {a mathematical formula}6,000 movies shared by Netflix and Douban, we extract ratings on the shared movies from Netflix and Douban, respectively, and obtain {a mathematical formula}490,000 ratings given by {a mathematical formula}120,000 users from Douban with rating density {a mathematical formula}0.7%, and {a mathematical formula}1,600,000 ratings given by {a mathematical formula}10,000 users from Netflix with density {a mathematical formula}2.6%. We consider ratings on Netflix as the source domain and those on Douban as the target domain. In total, we construct three cross-system CF tasks, and denote by Task 1: Netflix → Netflix, Task 2: DoubanMovie → DoubanBook and Task 3: Netflix → DoubanMovie, respectively.</paragraph><paragraph>In the experiments, for each time, we split each target domain data into a training set of {a mathematical formula}80% preference entries and a test set of {a mathematical formula}20% preference entries, and report the average results of 10 random times. The parameters of the model, i.e., the number of latent factors k and the number of iterations T, are tuned on some hand-out data of Task 1: Netflix → Netflix, and fixed to all experiments.{sup:5} Here, {a mathematical formula}T=10, and {a mathematical formula}k=5. In all experiments, we set {a mathematical formula}K1=⌊K2⌋, and the regularizer weight {a mathematical formula}λC=0.5. For evaluation criterion, we use Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) defined as,{a mathematical formula} where {a mathematical formula}xuv and {a mathematical formula}xˆuv are the true and predicted ratings, respectively, and {a mathematical formula}|I| is the number of test entries. The smaller is the value, the better is the performance.</paragraph></section><section label="5.2"><section-title>Overall comparison results</section-title><paragraph>In the first experiment, we qualitatively show the effectiveness of our proposed active transfer learning framework for cross-system CF compared with the following baselines:</paragraph><list><list-item label="•">NoTransf without correspondences (NoTransf (w/o corr.)): to apply state-of-the-art CF models on the target-domain collaborative data directly without either active learning or transfer learning. In this paper, regarding state-of-the-art CF models for comparison, we use MMMF, RLMF, and PMF as described in Section 3.1.</list-item><list-item label="•">NoTransf with actively constructed correspondences (NoTransf ({a mathematical formula}x% corr)): to first apply active learning to construct cross-domain entity correspondences ({a mathematical formula}x% of all the available cross-system correspondences), and then align the source and target domain data to generate a unified item-user matrix. Finally, we apply state-of-the-art CF models on the unified matrix for recommendations.</list-item><list-item label="•">CBT: to apply the codebook-based-transfer (CBT) method on the source and target domain data for recommendations. As mentioned in Section 1, CBT does not require any entity correspondence to be constructed.</list-item><list-item label="•">{a mathematical formula}MTL with full correspondences ({a mathematical formula}MTL (100% corr.)): to apply the proposed transfer learning approaches on the source and target domain data with full entity correspondences for recommendations, where {a mathematical formula}M represents RLMF, MMMF and PMF accordingly. Note that these methods, which assume all entity correspondences be available, can be considered as an upper bound of the proposed active transfer learning methods.</list-item></list><paragraph>The overall comparison results on the three cross-domain tasks are shown in Table 1, Table 2. For the active learning approaches, we use {a mathematical formula}MGhy, {a mathematical formula}EEhy and {a mathematical formula}EShy as proposed in Section 4.1.2 with the extended matrix factorization methods {a mathematical formula}MMMFTL, {a mathematical formula}RLMFTL, and {a mathematical formula}PMFTL, respectively. As can be observed from the rows labeled with “NoTransf (w/o corr.)” in the table, applying state-of-the-art CF models on the extremely sparse target domain data directly is not able to obtain precise recommendation results in terms of RMSE or MAE. The results of rows labeled with “NoTransf (0.1% corr.)” in the table suggest that aligning all the source and target data to a unified item-user matrix and then performing state-of-the-art CF models on it cannot help to boost the recommendation performance, but may even hurt the performance compared to that of applying CF models on the target domain data only. This is because the alignment makes the matrix to be factorized larger but still very sparse, resulting in a more difficult learning task. From the table we can also observe that the transfer learning method CBT performs better than the NoTransf methods. However, our proposed active transfer learning methods {a mathematical formula}RLMFTL, {a mathematical formula}MMMFTL and {a mathematical formula}PMFTL with only {a mathematical formula}0.1% entity correspondences achieve better performance than CBT in terms of RMSE and MAE (around 2.2%, 4.9%, and 8.1% improvement in terms of RMSE, and 2.0%, 6.7%, and 8.6% improvement in terms of MAE over Task 1, Task 2, and Task 3, respectively). This verifies the conclusion that making use of cross-system entity correspondences as a bridge is useful for knowledge transfer across recommender systems. Among the three proposed methods, {a mathematical formula}MMMFTL performs best on the three tasks. Finally, by considering the performance of active strategies with full entity correspondences as the knowledge-transfer upper bound, and the performance of base models as the lower bound, our proposed active transfer learning methods with only {a mathematical formula}0.1% entity correspondences to be cross-labeled can achieve around {a mathematical formula}34.13%, {a mathematical formula}61.41% and {a mathematical formula}79.28% in RMSE or {a mathematical formula}46.09%, {a mathematical formula}68.91% and {a mathematical formula}82.66% in MAE knowledge transfer ratio as defined in (30) on average over Task 1, Task 2, and Task 3, respectively.{a mathematical formula}</paragraph></section><section label="5.3"><section-title>Experiments on different active learning strategies</section-title><paragraph>In the second experiment, we aim to verify the performance of our proposed active transfer learning framework plugging with different entity selection strategies. Here, we use {a mathematical formula}MTL as the base transfer learning approach to cross-domain CF. Regarding entity selection strategies, besides the proposed approaches, {a mathematical formula}MGhy with {a mathematical formula}MMMFTL, {a mathematical formula}EEhy with {a mathematical formula}RLMFTL, and {a mathematical formula}EShy with {a mathematical formula}PMFTL, we also conduct comparison experiments on the following strategies:</paragraph><list><list-item label="•">Random: at each iteration, to select K entities randomly in the target domain to query their correspondences in the source domain.</list-item><list-item label="•">Many: at each iteration, to select K entities with most historical ratings, i.e., the users who give most ratings on items or the items which attract most users to give ratings, in the target domain to query their correspondences in the source domain.</list-item><list-item label="•">Few: at each iteration, to select K entities with fewest historical ratings, i.e., the users who give fewest ratings on items or the items which attract fewest users to give ratings, in the target domain to query their correspondences in the source domain.</list-item><list-item label="•">{a mathematical formula}MGmin, {a mathematical formula}EEmin, {a mathematical formula}ESmin: at each iteration, to select K entities whose predicted ratings are of the most uncertainty based on {a mathematical formula}δu(d) or {a mathematical formula}δv(d) by using {a mathematical formula}MMMFTL, {a mathematical formula}RLMFTL, and {a mathematical formula}PMFTL, respectively, in the target domain to query their correspondences in the source domain.</list-item><list-item label="•">{a mathematical formula}MGmax, {a mathematical formula}EEmax, {a mathematical formula}ESmax: at each iteration, to select K entities whose predicted ratings are of the most certainty based on {a mathematical formula}δu(d) or {a mathematical formula}δv(d) by using {a mathematical formula}MMMFTL, {a mathematical formula}RLMFTL, and {a mathematical formula}PMFTL, respectively, in the target domain to query their correspondences in the source domain.</list-item></list><paragraph>Fig. 3 shows the results of {a mathematical formula}MTL with different entity selection strategies under varying proportions of entity correspondences to be constructed. From the figure, we can observe that the active approaches based on the entity-level margin, error and entropy (i.e., {a mathematical formula}Amin, {a mathematical formula}Amax, and {a mathematical formula}Ahy, where {a mathematical formula}A represents MG, EE, ES, respectively) perform much better than other approaches. In addition, compared with {a mathematical formula}Amin and {a mathematical formula}Amax, the proposed {a mathematical formula}Ahy can avoid selecting long-tail users in the source domain for knowledge transfer, thus performs best.</paragraph></section><section label="5.4"><section-title>Experiments on different cross-domain regularization terms</section-title><paragraph>As mentioned in Section 4, the regularization term {a mathematical formula}R(UC(d),VC(d),UC(s),VC(s)) in (12) for cross-system knowledge transfer can be substituted by different forms, e.g., (13) or (14), which results in different transfer learning approaches, {a mathematical formula}MCMF or {a mathematical formula}MTL accordingly. Therefore, in the third experiment, we use {a mathematical formula}Ahy as the entity selection strategy, and compare the performance of {a mathematical formula}MTL and {a mathematical formula}MCMF in terms of RMSE. As can be seen from Fig. 4, the proposed {a mathematical formula}MTL outperforms its corresponding {a mathematical formula}MCMF consistently on the three cross-system tasks under varying proportions of the labeled entity correspondences. This implies that using similarities between entities from the source domain data as priors is more safe and useful for knowledge transfer across recommender systems than using the factor matrices factorized from the source domain data as priors directly.</paragraph></section><section label="5.5"><section-title>Computational time analysis</section-title><paragraph>For the last experiment, we study computational time of the proposed three active transfer learning approaches. The computer used for running the computational time comparison experiments is equipped with 1.4 GHz Intel Core i5, 8 GB memory and 512 GB SSD. Comparison results under varying proportions of entity correspondences to be constructed on Task 1 are shown in Fig. 5. Note that when the proportion of entity correspondences equals to 0, the proposed active transfer learning approaches, {a mathematical formula}MMMFTL, {a mathematical formula}RLMFTL, and {a mathematical formula}PMFTL, are reduced to the NoTransf methods without correspondences, i.e., MMMF, RLMF, and PMF using the target domain data only, respectively. As the matrix factorization on the source system data can be pre-trained, the reported computational time does not include the time on generating {a mathematical formula}U(s) and {a mathematical formula}V(s) for the source system. Furthermore, we also ignore the time on manually labeling cross-system entity correspondences. From the figure, we can find that as {a mathematical formula}MTL aims to exploit cross-system entity correspondences to transfer knowledge from the source system to the target system, the computational time increases when the number of the constructed cross-system entity correspondences increases. However, when the proportion of the constructed entity correspondences is not larger than 20%, the computational time of {a mathematical formula}MTL is very close to its corresponding NoTransf method without correspondences, respectively. From the figure, we can also observe that among the three active transfer learning approaches, {a mathematical formula}MMMFTL's computation cost is most expensive, while {a mathematical formula}PMFTL is the most computationally efficient. In fact, the computational time of the active transfer learning approach depends on its base matrix factorization method. In practice, parallel or distributed matrix factorization techniques can be adopted to significantly boost the computational efficiency of the proposed solutions [49], [50], [51], [52], [53]. However, it is beyond the scope of this work.</paragraph><paragraph>Together with the results shown in Table 1, Table 2, we may conclude that among the three proposed active transfer learning approaches, if prediction accuracy is of the most priority, then {a mathematical formula}MMMFTL is the best choice. If computational efficiency is of the most priority, then {a mathematical formula}PMFTL is the best choice. {a mathematical formula}RLMFTL can be considered as a trade-off solution. However, it should be emphasized again that the focus of this work is not discussing which matrix factorization method can be adapted into our framework to achieve the best performance for knowledge transfer across different recommender systems, but providing a general active transfer learning framework, where researchers can extend their favor matrix factorization methods for different applications and datasets.</paragraph></section></section><section label="6"><section-title>Conclusions</section-title><paragraph>In this paper, we present a novel framework on active transfer learning for cross-system recommendations. In the proposed framework, we 1) extend previous transfer learning approaches to CF in a flexible entity corresponding manner, and 2) propose an entity selection strategy to actively construct entity correspondences across different recommender systems. In particular, we develop three specific solutions based on the framework. Our experimental results show that compared with the transfer learning method which requires full entity correspondences, our proposed framework can achieve around {a mathematical formula}34.13%, {a mathematical formula}61.41% and {a mathematical formula}79.28% in RMSE or {a mathematical formula}46.09%, {a mathematical formula}68.91% and {a mathematical formula}82.66% in MAE knowledge-transfer ratio, while only requires {a mathematical formula}0.1% of the entities to have correspondences. For future work, we are planning to apply the proposed framework to other applications, such as cross-system link prediction in social networks.</paragraph><section-title>Acknowledgement</section-title></section></content><acknowledgements><paragraph>Lili Zhao and Qiang Yang thank the support of China National 973 project2014CB340304 and Hong Kong CERG projects 16211214, 16209715 and 16244616. Sinno J. Pan thanks the support of the NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020.</paragraph></acknowledgements><references><reference label="[1]"><authors>F.J. Martin,J. Donaldson,A. Ashenfelter,M. Torrens,R. Hangartner</authors><title>The big promise of recommender systems</title><host>AI Mag.32 (3)(2011) pp.19-27</host></reference><reference label="[2]"><authors>R.D. Burke,A. Felfernig,M.H. Göker</authors><title>Recommender systems: an overview</title><host>AI Mag.32 (3)(2011) pp.13-18</host></reference><reference label="[3]"><authors>D. Jannach,M. Zanker,A. Felfernig,G. Friedrich</authors><title>Recommender Systems – an Introduction</title><host>(2010)Cambridge University Press</host></reference><reference label="[4]"><authors>S.J. Pan,Q. Yang</authors><title>A survey on transfer learning</title><host>IEEE Trans. Knowl. Data Eng.22 (10)(2010) pp.1345-1359</host></reference><reference label="[5]"><authors>S.J. Pan</authors><title>Transfer learning</title><host>C.C. AggarwalData Classification: Algorithms and Applications(2014)CRC Press pp.537-570</host></reference><reference label="[6]"><authors>B. Li,Q. Yang,X. Xue</authors><title>Can movies and books collaborate?: cross-domain collaborative filtering for sparsity reduction</title><host>IJCAI(2009) pp.2052-2057</host></reference><reference label="[7]"><authors>B. Li,Q. Yang,X. Xue</authors><title>Transfer learning for collaborative filtering via a rating-matrix generative model</title><host>ICML(2009) pp.617-624</host></reference><reference label="[8]"><authors>B. Mehta,T. Hofmann</authors><title>Cross system personalization and collaborative filtering by learning manifold alignments</title><host>KI(2006) pp.244-259</host></reference><reference label="[9]"><authors>W. Pan,E.W. Xiang,N.N. Liu,Q. Yang</authors><title>Transfer learning in collaborative filtering for sparsity reduction</title><host>AAAI(2010)</host></reference><reference label="[10]"><authors>W. Pan,Q. Yang</authors><title>Transfer learning in heterogeneous collaborative filtering domains</title><host>Artif. Intell.197 (2013) pp.39-55</host></reference><reference label="[11]">J. Lee,M. Sun,G. LebanonA comparative study of collaborative filtering algorithmsCoRR<host>arXiv:1205.3193</host></reference><reference label="[12]"><authors>L. Zhao,S.J. Pan,E.W. Xiang,E. Zhong,Z. Lu,Q. Yang</authors><title>Active transfer learning for cross-system recommendation</title><host>AAAI(2013)</host></reference><reference label="[13]"><authors>D. Goldberg,D. Nichols,B.M. Oki,D. Terry</authors><title>Using collaborative filtering to weave an information tapestry</title><host>Commun. ACM35 (12)(1992) pp.61-70</host></reference><reference label="[14]"><authors>J.S. Breese,D. Heckerman,C. Kadie</authors><title>Empirical analysis of predictive algorithms for collaborative filtering</title><host>UAI(1998) pp.43-52</host></reference><reference label="[15]"><authors>A.A. Falkner,A. Felfernig,A. Haag</authors><title>Recommendation technologies for configurable products</title><host>AI Mag.32 (3)(2011) pp.99-108</host></reference><reference label="[16]"><authors>B. Mobasher,J. Cleland-Huang</authors><title>Recommender systems in requirements engineering</title><host>AI Mag.32 (3)(2011) pp.81-89</host></reference><reference label="[17]"><authors>Ò. Celma,P. Lamere</authors><title>If you like radiohead, you might like this article</title><host>AI Mag.32 (3)(2011) pp.57-66</host></reference><reference label="[18]"><authors>R.D. Burke,J. Gemmell,A. Hotho,R. Jäschke</authors><title>Recommendation in the social web</title><host>AI Mag.32 (3)(2011) pp.46-56</host></reference><reference label="[19]"><authors>Y. Koren,R. Bell,C. Volinsky</authors><title>Matrix factorization techniques for recommender systems</title><host>Computer42 (2009) pp.30-37</host></reference><reference label="[20]"><authors>W. Pan,E.W. Xiang,Q. Yang</authors><title>Transfer learning in collaborative filtering with uncertain ratings</title><host>AAAI(2012)</host></reference><reference label="[21]"><authors>B. Cao,N.N. Liu,Q. Yang</authors><title>Transfer learning for collective link prediction in multiple heterogenous domains</title><host>ICML(2010) pp.159-166</host></reference><reference label="[22]"><authors>Y. Zhang,B. Cao,D.-Y. Yeung</authors><title>Multi-domain collaborative filtering</title><host>UAI(2010) pp.725-732</host></reference><reference label="[23]"><authors>A.P. Singh,G.J. Gordon</authors><title>Relational learning via collective matrix factorization</title><host>KDD(2008) pp.650-658</host></reference><reference label="[24]"><authors>J. Tang,J. Yan,L. Ji,M. Zhang,S. Guo,N. Liu,X. Wang,Z. Chen</authors><title>Collaborative users' brand preference mining across multiple domains from implicit feedbacks</title><host>AAAI(2011)</host></reference><reference label="[25]"><authors>L. Shi,Y. Zhao,J. Tang</authors><title>Batch mode active learning for networked data</title><host>ACM Trans. Intell. Syst. Technol.3 (2)(2012) pp.33:1-33:25</host></reference><reference label="[26]"><authors>C.E. Mello,M.-A. Aufaure,G. Zimbrao</authors><title>Active learning driven by rating impact analysis</title><host>RecSys(2010) pp.341-344</host></reference><reference label="[27]"><authors>I. Rish,G. Tesauro</authors><title>Active collaborative prediction with maximum margin matrix factorization</title><host>ISAIM(2008)</host></reference><reference label="[28]"><authors>R. Jin,L. Si</authors><title>A Bayesian approach toward active learning for collaborative filtering</title><host>UAI(2004) pp.278-285</host></reference><reference label="[29]"><authors>C. Boutilier,R.S. Zemel,B. Marlin</authors><title>Active collaborative filtering</title><host>UAI(2003) pp.98-106</host></reference><reference label="[30]"><authors>S. Raj,J. Ghosh,M.M. Crawford</authors><title>An active learning approach to knowledge transfer for hyperspectral data analysis</title><host>IGARSS(2006) pp.541-544</host></reference><reference label="[31]"><authors>Y.S. Chan,H.T. Ng</authors><title>Domain adaptation with active learning for word sense disambiguation</title><host>ACL(2007)</host></reference><reference label="[32]"><authors>X. Shi,W. Fan,J. Ren</authors><title>Actively transfer domain knowledge</title><host>ECML/PKDDvol. 2 (2008) pp.342-357</host></reference><reference label="[33]"><authors>P. Rai,A. Saha,H. DauméIII,S. Venkatasubramanian</authors><title>Domain adaptation meets active learning</title><host>NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing(2010) pp.27-32</host></reference><reference label="[34]"><authors>A. Saha,P. Rai,H. DauméIII,S. Venkatasubramanian,S.L. DuVall</authors><title>Active supervised domain adaptation</title><host>ECML/PKDDvol. 3 (2011) pp.97-112</host></reference><reference label="[35]"><authors>R. Chattopadhyay,W. Fan,I. Davidson,S. Panchanathan,J. Ye</authors><title>Joint transfer and batch-mode active learning</title><host>ICML(2013) pp.253-261</host></reference><reference label="[36]"><authors>X. Wang,T.-K. Huang,J. Schneider</authors><title>Active transfer learning under model shift</title><host>ICML(2014) pp.1305-1313</host></reference><reference label="[37]"><authors>X. Liao,Y. Xue,L. Carin</authors><title>Logistic regression with an auxiliary data source</title><host>ICML(2005) pp.505-512</host></reference><reference label="[38]"><authors>J.D.M. Rennie,N. Srebro</authors><title>Fast maximum margin matrix factorization for collaborative prediction</title><host>ICML(2005) pp.713-719</host></reference><reference label="[39]"><authors>A. Paterek</authors><title>Improving regularized singular value decomposition for collaborative filtering</title><host>KDD Cup and Workshop(2007)</host></reference><reference label="[40]"><authors>R.M. Bell,Y. Koren</authors><title>Lessons from the Netflix prize challenge</title><host>SIGKDD Explor.9 (2)(2007) pp.75-79</host></reference><reference label="[41]"><authors>N. Srebro,J.D.M. Rennie,T.S. Jaakkola</authors><title>Maximum-margin matrix factorization</title><host>NIPSvol. 17 (2005) pp.1329-1336</host></reference><reference label="[42]"><authors>R. Salakhutdinov,A. Mnih</authors><title>Probabilistic matrix factorization</title><host>NIPS(2007)</host></reference><reference label="[43]"><authors>M. Belkin,P. Niyogi</authors><title>Laplacian eigenmaps and spectral techniques for embedding and clustering</title><host>NIPS(2001) pp.585-591</host></reference><reference label="[44]"><authors>W.-J. Li,D.-Y. Yeung</authors><title>Relation regularized matrix factorization</title><host>IJCAI(2009) pp.1126-1131</host></reference><reference label="[45]">B. SettlesActive Learning Literature SurveyTechnical report<host>(2009)University of Wisconsin – Madison</host></reference><reference label="[46]"><authors>S. Tong,D. Koller</authors><title>Support vector machine active learning with applications to text classification</title><host>J. Mach. Learn. Res.2 (2002) pp.45-66</host></reference><reference label="[47]"><authors>D. Roth,K. Small</authors><title>Margin-based active learning for structured output spaces</title><host>ECML(2006) pp.413-424</host></reference><reference label="[48]"><authors>M. Balcan,A.Z. Broder,T. Zhang</authors><title>Margin based active learning</title><host>COLT(2007) pp.35-50</host></reference><reference label="[49]"><authors>H.-F. Yu,C.-J. Hsieh,S. Si,I. Dhillon</authors><title>Scalable coordinate descent approaches to parallel matrix factorization for recommender systems</title><host>ICDM(2012) pp.765-774</host></reference><reference label="[50]"><authors>H.-F. Yu,C.-J. Hsieh,S. Si,I.S. Dhillon</authors><title>Parallel matrix factorization for recommender systems</title><host>Knowl. Inf. Syst.41 (3)(2014) pp.793-819</host></reference><reference label="[51]"><authors>R. Gemulla,E. Nijkamp,P.J. Haas,Y. Sismanis</authors><title>Large-scale matrix factorization with distributed stochastic gradient descent</title><host>KDD(2011) pp.69-77</host></reference><reference label="[52]"><authors>S. Ahn,A. Korattikara,N. Liu,S. Rajan,M. Welling</authors><title>Large-scale distributed Bayesian matrix factorization using stochastic gradient MCMC</title><host>KDD(2015) pp.9-18</host></reference><reference label="[53]"><authors>M. Li,Z. Liu,A.J. Smola,Y. Wang</authors><title>Difacto: distributed factorization machines</title><host>WSDM(2016) pp.377-386</host></reference></references><footnote><note-para label="1">In a broad definition, any software system that provides suggestions on items to purchase, to subscribe, to use, or to invest can be regarded as a recommender system. In this sense, computational advertising, query suggestion, etc., can be also seen as examples of recommendations.</note-para><note-para label="2">http://www.netflixprize.com.</note-para><note-para label="3">http://www.netflix.com.</note-para><note-para label="4">http://www.douban.com.</note-para><note-para label="5">Suppose that total budget is ρ, which is the total number of correspondences to be constructed, we set the number of correspondences actively constructed in each iteration as {a mathematical formula}K=ρ/T.</note-para></footnote></root>