<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:232">
</head>
<body bgcolor="white">
<a href="#0" id="0">It then performs a full quadratic expansion of these l linear features (using the original, unnormalized features, and then normalizing the resulting quadratic features again to have mean zero and standard deviation one).</a>
<a href="#1" id="1">This observation is significant, since most past work employed ridge regression with large amounts of data (e.g., in SATzilla[119]), only measuring its performance in what turns out to be a favourable condition for it.</a>
<a href="#2" id="2">While all these features are polynomial-time computable, we note that some of them can be computationally expensive for very large instances (e.g., taking cubic time).</a>
<a href="#3" id="3">As an implementation detail, to avoid potentially large outlying predictions above the known maximal runtime of {a mathematical formula}κmax=300 seconds, we ensure that the mean imputed value does not exceed {a mathematical formula}κmax.</a>
<a href="#4" id="4">Generating hard benchmarks.</a>
<a href="#5" id="5">These instances contain an average of 1380 variables and 8042 clauses, with respective standard deviations of 3164 and {a mathematical formula}13434, and respective maxima of {a mathematical formula}19000 variables and 79 800 clauses.</a>
<a href="#6" id="6">Second, our new clause learning features (91–108) are based on statistics gathered in 2-second runs of Zchaff_rand[80].</a>
<a href="#7" id="7">The complexity of building a regression tree depends on how balanced it is.</a>
<a href="#8" id="8">In some models (ridge regression and neural networks), this mechanism leads us to ignore missing features, since their weight is multiplied by zero.</a>
<a href="#9" id="9">For the industrial SAT solver SPEAR[3], we used the same parameter configuration space as in previous work [49].</a>
</body>
</html>