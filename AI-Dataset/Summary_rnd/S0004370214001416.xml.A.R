<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:359">
</head>
<body bgcolor="white">
<a href="#0" id="0">Since we use an approximation to represent a value function and obtain an estimate iteratively, the question of algorithm convergence is twofold.</a>
<a href="#1" id="1">One primary difference between PRMs and RRT/EST methods is that PRMs were designed to learn {a mathematical formula}Cfree topology once and then use this knowledge to solve multiple planning queries, where RRTs/ESTs expand from a single start and/or goal position for a single planning query.</a>
<a href="#2" id="2">The observed maximum displacement is between {a mathematical formula}4∘ and {a mathematical formula}5∘ higher experimentally than in simulation.</a>
<a href="#3" id="3">Minimal residual oscillations trajectory generation</a>
<a href="#4" id="4">Smaller energy per frequency value and narrower frequency range correspond to less load displacement.</a>
<a href="#5" id="5">The UAV needs to pass a doorway, change altitude, and navigate between the tables, shelves, and counters.</a>
<a href="#6" id="6">Both learned load trajectories follow the same profile with three distinct peaks around 0.5 seconds, 2.2 seconds, and 3.1 seconds into the flight, followed by rapid swing control and reduction to under {a mathematical formula}5∘.</a>
<a href="#7" id="7">We call k the candidate action set size constant.</a>
<a href="#8" id="8">We test the proposed methodology in Section 4 both in simulation and experimentally on a physical system (Fig. 1a).</a>
<a href="#9" id="9">We show the policy's viability in the expanded state and action spaces in simulation in Section 4.2.1.</a>
<a href="#10" id="10">These observations lead to several practical properties of the induced greedy policy that we will verify empirically:</a>
<a href="#11" id="11">For given start and goal coordinates, the planner calculates a collision-free path between them.</a>
<a href="#12" id="12">Line 12 in Algorithm 2 creates a trajectory segment that follows the PRM calculated path between adjacent nodes {a mathematical formula}(wi,wi+1).</a>
<a href="#13" id="13">AI researchers can use the approaches and theory presented in this article to solve other constraint-balancing problems for non-linear systems using RL in continuous state and very large action spaces.</a>
<a href="#14" id="14">The resulting swing predictions (Fig. 5b) shows that in the last 0.3 seconds of the trajectory, the cubic trajectory exhibits a swing of {a mathematical formula}10∘, while the DP trajectory ends with a swing of less than {a mathematical formula}5∘.</a>
<a href="#15" id="15">The algorithm starts with an arbitrary initial state.</a>
<a href="#16" id="16">The distinction between discrete and continuous action spaces is important because it is a basis for aerial cargo delivery that is both collision-free and with bounded load displacement.</a>
</body>
</html>