<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:306">
</head>
<body bgcolor="white">
<a href="#0" id="0">The datasets used for evaluation were a new video (different from the training video) showing the sandwich making scenario and a pancake making video.</a>
<a href="#1" id="1">Another study that addressed the problem of human intentions was presented by Gehrig et al. [39], where their framework combined motion, activity, and intention recognition by humans using visual information obtained from a monocular camera combined with the knowledge domain.</a>
<a href="#2" id="2">In order to define meaningful relationships between activities and objects, we use the semantic rules obtained as described in the previous Section 4.1.</a>
<a href="#3" id="3">Online recognition by the iCub</a>
<a href="#4" id="4">These mechanisms help to understand the meaning of the recognized task and allow the system to be more flexible and adaptable to new situations.</a>
<a href="#5" id="5">In order to answer the first question, we performed the following experiment.</a>
<a href="#6" id="6">Thus, using the tree obtained, we can determine six hypotheses {a mathematical formula}(Hsandwich), which represent semantic rules that describes the basic human activities.</a>
<a href="#7" id="7">Therefore, it is necessary to design a method that can process the perceived goal-relevant information to make inferences about the goal of the demonstrator.</a>
<a href="#8" id="8">These findings are presented in the following sections.</a>
<a href="#9" id="9">In the field of robotics, especially human–robot interaction, it is important to provide robots with decision-making capabilities in order to increase their adaptability and flexibility in different environments.</a>
<a href="#10" id="10">The plan indicated the motion primitives that the robot had to execute in order to achieve a similar goal to the human.</a>
<a href="#11" id="11">For example, Pancake_1 is an instance of the Class Pancake (see Fig. 6), which is defined in the semantic map and it inherits the motion–object relationship described in the queries (4) and (5).</a>
<a href="#12" id="12">This system allows the generation of novel motion patterns, but its use is limited to the joint angles when creating the proto-symbol space.</a>
<a href="#13" id="13">These simple motions can be recognized in different scenarios but they cannot define an activity by themselves.</a>
</body>
</html>