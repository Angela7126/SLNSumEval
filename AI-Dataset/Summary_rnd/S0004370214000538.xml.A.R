<html>
<head>
<meta name="TextLength" content="SENT_NUM:17, WORD_NUM:372">
</head>
<body bgcolor="white">
<a href="#0" id="0">However, as this information might not be accurate either, it is not trivial how to efficiently handle it in practice.</a>
<a href="#1" id="1">{a mathematical formula}E[G(1−ε)B(A⁎)]≤∑j=1Nxj+μj.</a>
<a href="#2" id="2">We first introduce the bounded MAB model (Section 3.1).</a>
<a href="#3" id="3">Second, both ε-first approaches sample each worker fewer times, leading to less accurate quality estimates.</a>
<a href="#4" id="4">Third, experts often demand widely varying prices for their services.</a>
<a href="#5" id="5">(8) (i.e., the problem we have to solve within the exploitation phase and that uses the estimated {a mathematical formula}μˆi values).</a>
<a href="#6" id="6">To address these challenges, we turn to the field of multi-armed bandits (MABs), a class of problems dealing with decision-making under uncertainty [1].</a>
<a href="#7" id="7">Now, we reduce the task assignment problem in the exploitation phase to a bounded knapsack problem as follows.</a>
<a href="#8" id="8">Another potential application of our work is cloud computing, where services are potentially unreliable or vary in their quality, and where the maximum number of jobs on one service is restricted either by a fixed deadline or by user quotas.</a>
<a href="#9" id="9">Results</a>
<a href="#10" id="10">In addition, the left hand side is the optimal solution of the integer bounded knapsack problem.</a>
<a href="#11" id="11">Thus, we also vary the budget B from $5000 to {a mathematical formula}$20,000, to analyse the performance of the algorithms (for consistency fixing the set of candidates to those that charge at most $50 per hour).</a>
<a href="#12" id="12">This sub-linear theoretical bound necessarily implies that our algorithm has the zero-regret property, a key measure of efficiency within the MAB literature.</a>
<a href="#13" id="13">Let {a mathematical formula}μi denote the mean value of the rewards that the agent receives from pulling arm i. Within our model, the agent's goal is to maximise the sum of rewards it earns from pulling the arms of the machine, with respect to the budget B. However, the agent has no initial knowledge of the {a mathematical formula}μi of each arm i, so it must learn these values in order to choose a policy that maximises its sum of rewards.</a>
<a href="#14" id="14">To achieve this, we use the advertised cost of a worker, {a mathematical formula}ci, and determine its mean quality as {a mathematical formula}μi=D⋅ci, where D is a random variable representing the worker's quality-cost density.</a>
<a href="#15" id="15">{a mathematical formula}E[G(1−ε)B(Agreedy)]≥∑j=1Nxˆjμj−1.</a>
<a href="#16" id="16">Another interesting set of jobs is those with large budgets, as they present long-term investments that require careful task allocation.</a>
</body>
</html>