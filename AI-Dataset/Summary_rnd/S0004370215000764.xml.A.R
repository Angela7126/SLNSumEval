<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:251">
</head>
<body bgcolor="white">
<a href="#0" id="0">For the agent that explored using texplore-vanir, the agent learned to hit the cymbal in just one trial out of 5.</a>
<a href="#1" id="1">Next, we compare versions of texplore-vanir using one or both of its intrinsic rewards combined with the external rewards.</a>
<a href="#2" id="2">The decision trees are learned using an implementation of Quinlan's C4.5 algorithm [33].</a>
<a href="#3" id="3">This reward will drive the agent to the state-actions where its models have not yet converged to a single hypothesis of the world's dynamics.</a>
<a href="#4" id="4">Section 5 presents experiments showing that texplore-vanir: 1) learns a model more efficiently than other methods; 2) explores in a developing, curious way; and 3) can use its learned model later to perform tasks specified by a reward function.</a>
<a href="#5" id="5">texplore-vanir receives significantly more reward than the other algorithms ({a mathematical formula}p<0.001), followed by r-max.</a>
<a href="#6" id="6">Picking up the key is easier than pressing the lock, as the lock requires the agent to have already picked up the key and not yet unlocked the door.</a>
<a href="#7" id="7">For the microphone task, the reward was equal to the power of the signal received on the microphone.</a>
<a href="#8" id="8">Fig. 1 shows an example decision tree predicting the relative change in the x variable of the agent in the given gridworld domain.</a>
<a href="#9" id="9">In addition, it shows that the agent can use the intrinsic rewards in conjunction with external rewards to learn a task faster than if using external rewards alone.</a>
<a href="#10" id="10">For example, the competence progress rewards used by r-iac are intended to be used in complex high-dimensional domains where learning is slow.</a>
</body>
</html>