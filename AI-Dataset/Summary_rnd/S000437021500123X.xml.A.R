<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:215">
</head>
<body bgcolor="white">
<a href="#0" id="0">Continuously operating competences run constantly and push information to the belief layer, causing asynchronous updates to the relational map.</a>
<a href="#1" id="1">After obtaining the initial relational map, Dora is given a goal description that a magazine must exist and the robot must know where it is, written:{sup:7} (exists(?o-object)(and(=(label?o)magazine)(K(position?o)))) Planning with hypotheses</a>
<a href="#2" id="2">The reason lies in the fact that state variables either take a specific value{sup:21} or the unknown value ⊥.</a>
<a href="#3" id="3">The verification plan for one of the two runs can be seen in Fig. 5.</a>
<a href="#4" id="4">Two knowledge-level predicates [33] model the robot's epistemic state over time: a knowledge predicate and an assumptive predicate.</a>
<a href="#5" id="5">New instance assumptions will also automatically become available to the planner as new places and objects are added to the relational map.</a>
<a href="#6" id="6">After creating an explanation, Dora autonomously creates a new goal, via the mechanisms of the goal management system [14], so as to verify the hypotheses made.</a>
<a href="#7" id="7">We can force the planner to return multiple explanations.</a>
<a href="#8" id="8">Experiment 1—explore: continual planning</a>
<a href="#9" id="9">In Fig. 4 (three minutes into the run) Dora has found a meeting room, created a set of visual fixations, and is executing a POMDP plan that sequences these.</a>
<a href="#10" id="10">This experiment shows the ability to switch to decision-theoretic sessions, and to use instance assumptions derived from default knowledge, to plan in open worlds.</a>
</body>
</html>