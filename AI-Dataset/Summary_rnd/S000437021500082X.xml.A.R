<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:184">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this way, this neural network learns the relationship between the consequences of actions in both domains.</a>
<a href="#1" id="1">This is done by using the following update rule:{a mathematical formula} where s is the current state; a is the action performed in s; r is the reward received; {a mathematical formula}s′ is the new state obtained by executing action a in state s; α is the learning rate ({a mathematical formula}α=1/(1+visits(s,a))), and γ is a discount factor ({a mathematical formula}0≤γ<1).</a>
<a href="#2" id="2">But, as the case base is used as an heuristic, the SARSA(λ) algorithm learns that the actions suggested by the heuristics are misleading, and that they should not be executed.</a>
<a href="#3" id="3">In the second stage of L3-Q, the actions between the two domains are connected.</a>
<a href="#4" id="4">In the present experiment we modified the 2D Mountain Car simulator to test the results of using Q-Learning, Q(λ), SARSA, and SARSA(λ) algorithms.</a>
<a href="#5" id="5">Select N cases by random sampling the action-state set.</a>
<a href="#6" id="6">In contrast to previous work, which were mostly based on model-free methods, Taylor et al. [26] propose a transfer-learning method for a model-based reinforcement learning algorithm for continuous state space.</a>
</body>
</html>