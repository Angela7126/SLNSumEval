<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:188">
</head>
<body bgcolor="white">
<a href="#0" id="0">Direct LfD tries to approximate the mapping from states to actions; however, in reality, the teacher's policy is not necessarily deterministic or stationary.</a>
<a href="#1" id="1">We have used two different feature selection algorithms for AfD.</a>
<a href="#2" id="2">Pong serves as a proving ground for demonstrating the correctness of AfD, using just 23 episodes as training data.</a>
<a href="#3" id="3">An algorithm with a similar goal to ours is the CST algorithm [31].</a>
<a href="#4" id="4">This boundary discriminator works as the stopping criteria of the algorithm.</a>
<a href="#5" id="5">Given an MDP M and a set of human demonstrations H for a task to be learned, ADA finds a policy in three conceptual steps and an optional fourth step:</a>
<a href="#6" id="6">The simpler domain, PandaSequential, still has 64{sup:6} possible states, for a total of about 343 billion Q-values.</a>
<a href="#7" id="7">If a library of candidate abstractions is provided, it can use a different one for each skill and therefore find, like our ADA algorithm, skill-specific decompositions in a lower-dimensional state space where it is easier to learn them.</a>
<a href="#8" id="8">Fitted Q-learning</a>
<a href="#9" id="9">Fig. 6(a) shows the average discounted rewards vs. number of training episodes for fitted Q-learning, with and without the AfD abstraction (AfD+FQ and fitted Q).</a>
</body>
</html>