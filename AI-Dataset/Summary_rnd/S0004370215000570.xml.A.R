<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:194">
</head>
<body bgcolor="white">
<a href="#0" id="0">The robot tries to move the second cup always first, because the occlusion on the first cup makes the initial grasp success of the second cup higher.</a>
<a href="#1" id="1">In POMDPs with discrete variables, the state space size grows exponentially w.r.t the number of state variables.</a>
<a href="#2" id="2">We use two kinds of belief updates.</a>
<a href="#3" id="3">In this problem, the robot can gain more information of attributes by removing occlusions and gain information about the object specific grasp probability through successful and failed grasps.</a>
<a href="#4" id="4">However, the results suggest that many problems require a complex policy to gain high reward.</a>
<a href="#5" id="5">Second, we propose a new POMDP method which is applicable to manipulation planning.</a>
<a href="#6" id="6">Fig. 4 shows the average total reward over 100 simulation runs for each of the ten different cup configurations shown in Fig. 3.</a>
<a href="#7" id="7">In addition to occlusion specific grasp probabilities, our model also includes automatically adapting object specific grasp probabilities.</a>
<a href="#8" id="8">{a mathematical formula}R(s,a) yields the real-valued reward for executing action a in state s and O denotes the observation probabilities {a mathematical formula}P(o|s′,a), where o is the observation made by the agent, when action a was executed and the world moved to the state {a mathematical formula}s′.</a>
</body>
</html>