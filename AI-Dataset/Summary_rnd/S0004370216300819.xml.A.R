<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:231">
</head>
<body bgcolor="white">
<a href="#0" id="0">While defaults could be learned, that falls outside of the scope of this article.</a>
<a href="#1" id="1">For a determinization to have this property, it must be optimistic, which we define as follows.</a>
<a href="#2" id="2">We designed two types of changes: (1) the current optimal path becomes worse, and (2) the value of the current path does not change, but a better solution opens somewhere else.</a>
<a href="#3" id="3">An important variable is the time step, which is grounded up to a constant n, the maximum time step that will be considered.</a>
<a href="#4" id="4">The figure also shows the {a mathematical formula}95% confidence intervals, plotted every 5 episodes to avoid cluttering the image.</a>
<a href="#5" id="5">The robot does not have any physical means to open doors.</a>
<a href="#6" id="6">Monotonic formalism</a>
<a href="#7" id="7">This section gives an overview of previous work in the area of combining the model-based lookahead of planning with the hindsight view of learning.</a>
<a href="#8" id="8">Therefore, in order to learn the new policy the Ïµ-greedy strategy should go repeatedly against the optimal policy for 8 times in a row, which is very unlikely.</a>
<a href="#9" id="9">Optimism in the face of uncertainty is a principle which has affected exploration methods for a long time [4], and it is not surprising that it can be applied to our framework as well.</a>
<a href="#10" id="10">This adaptive behavior allows both agents to avoid being stuck in this domain, where doors cannot be opened.</a>
<a href="#11" id="11">We used the simulator Gazebo,{sup:2} and the Robot Operating System (ROS){sup:3}.</a>
</body>
</html>