<html>
<head>
<meta name="TextLength" content="SENT_NUM:14, WORD_NUM:328">
</head>
<body bgcolor="white">
<a href="#0" id="0">The 33 participants were from the local government, and from the following NGOs YABI, WWF and WCS.</a>
<a href="#1" id="1">Noticing that Letchford et al. [54] may still lead to a large number of queries, particularly given that number of pure strategies may grow exponentially, Blum et al. [14] design an algorithm that learns an ϵ-optimal strategy for the defender with a certain probability by asking a significantly lower number of queries.</a>
<a href="#2" id="2">Challenges and remedies of online repeated measures experiments</a>
<a href="#3" id="3">A second major innovation in SHARP is the adaptive nature of the adversary and addressing the issue of attack surface exposure.</a>
<a href="#4" id="4">(11) to check if the participants may have been considering animal densities only and ignoring the effect of distance while playing the game, thus not paying attention to the effective reward.</a>
<a href="#5" id="5">These participants were then allowed to proceed towards playing the first round of the game.</a>
<a href="#6" id="6">18(a)–18(d).</a>
<a href="#7" id="7">In a repeated SSG model, the defender periodically deploys new patrol strategies (in “rounds” of the game) and the adversaries observe these strategies and act accordingly.</a>
<a href="#8" id="8">One of our key findings, based on experiments with the PSU function is that the curve representing human weights for probability is S-shaped in nature, and not inverse S-shaped as prospect theory suggests.</a>
<a href="#9" id="9">For our experiments, we varied d based on Eqn.</a>
<a href="#10" id="10">Figs.</a>
<a href="#11" id="11">Although a perfectly rational adversary would choose to attack the target with the highest expected utility, more recent work has focused on modeling boundedly rational adversaries in SSGs [62], [80], [40], [81], [34], [22], thus developing models, some of which are discussed in Sections 2.2, 2.3 and 2.4.</a>
<a href="#12" id="12">SHARP without discounting but with Attractiveness (see Fig. 6(b)) generates a comparatively more robust strategy than SHARP with discounting but without Attractiveness (Fig. 6(a)) due to its adaptive utility function and similarity learning mechanism.</a>
<a href="#13" id="13">Again, this contradicts with optimality.So {a mathematical formula}S¯2=∅ and the coverage probabilities of the optimal strategy are chosen only from {a mathematical formula}0,1, i.e., the optimal defender strategy is a pure strategy.</a>
</body>
</html>