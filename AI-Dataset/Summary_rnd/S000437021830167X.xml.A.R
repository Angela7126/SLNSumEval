<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:158">
</head>
<body bgcolor="white">
<a href="#0" id="0">Without any restriction, there are {a mathematical formula}2n−1 possible parent sets, since every subset of {a mathematical formula}X∖{Xi} is a candidate.</a>
<a href="#1" id="1">For instance, we can choose {a mathematical formula}Π′=∅ and so they become entropies of single variables, which can be precomputed efficiently in total time {a mathematical formula}O(N⋅n).</a>
<a href="#2" id="2">Column 6 has the average time to run the algorithms (there is actually no significant difference between the algorithms' times in our experiments).</a>
<a href="#3" id="3">Consider the problem of learning the structure of a Bayesian Network from a complete data set of {a mathematical formula}N≥2 instances {a mathematical formula}D={D1,...,DN}.</a>
<a href="#4" id="4">It is immediate that {a mathematical formula}N⋅min⁡{H(X|Π⁎);H(Y|Π⁎)}≤(1−|ΩY|)Pen(X|Π⁎)⇒N⋅min⁡{H(X|Π″);H(Y|Π″)}≤(1−|ΩY|)Pen(X|Π″), since {a mathematical formula}Π⁎⊂Π″ and hence {a mathematical formula}−Pen(X|Π″)>−Pen(X|Π⁎).</a>
<a href="#5" id="5">This paper presents new non-trivial pruning rules to be used with the Bayesian Information Criterion (BIC) score for learning the structure of Bayesian networks.</a>
<a href="#6" id="6">Then the parent set{a mathematical formula}Π=Π⁎∪{Y}and all its supersets can be safely ignored when building the list of candidate parents sets for X.</a>
</body>
</html>