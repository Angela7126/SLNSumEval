<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:237">
</head>
<body bgcolor="white">
<a href="#0" id="0">We will explain the ideas underlying our factorization method with the help of Fig. 2.</a>
<a href="#1" id="1">Bayesian probabilistic tensor factorization (BPTF) [3] is especially promising because of its efficient sampling of large-scale datasets and simple parameter settings.</a>
<a href="#2" id="2">Moreover, semantic biases derived from item classes improve accuracy more than those from tag classes, as indicated by the results of BPTFI and BPTFT.</a>
<a href="#3" id="3">Generate {a mathematical formula}ΛU, {a mathematical formula}ΛV, and {a mathematical formula}ΛT∼W(Λ|W0,ν0), where {a mathematical formula}ΛU, {a mathematical formula}ΛV, and {a mathematical formula}ΛT are the precision matrices (a precision matrix is the inverse of a covariance matrix) for Gaussians.</a>
<a href="#4" id="4">NMTF shares the biases from such popular tags with other tags via their food categories.</a>
<a href="#5" id="5">{sup:10} Objects like the above artists in DBPedia/Freebase can have multiple classes.</a>
<a href="#6" id="6">It suits many applications (i.e. movie rating/tagging systems, restaurant rating/review systems, and music listening/tagging systems) that manage various items since the growing knowledge graph means that applications increasingly have semantic knowledge underlying objects.</a>
<a href="#7" id="7">Because the first term is much larger than the rest, the computation time is almost the same as that of BPTF.</a>
<a href="#8" id="8">Different from NMTF, GCTF can use the KL divergence, IS divergence, and Euclidean distance for optimization model (the Euclidean distance is usually used for rating prediction, as is the Gaussian distribution), however, it is worse than SSTF (as described in Section 5.3, GCTF achieves the highest accuracy when it uses the IS distance).</a>
<a href="#9" id="9">The parameter settings are easy.</a>
</body>
</html>