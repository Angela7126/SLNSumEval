<html>
<head>
<meta name="TextLength" content="SENT_NUM:9, WORD_NUM:181">
</head>
<body bgcolor="white">
<a href="#0" id="0">We use the Viterbi algorithm to obtain the optimal tag sequence.</a>
<a href="#1" id="1">We check the sizes of the added meta features and the speed in the testing stage.</a>
<a href="#2" id="2">Here, the results of the parsers with different settings are shown in Table 6, where CORE refers to the first type of meta features, WithPOS refers to the second, and WithWORD refers to the third.</a>
<a href="#3" id="3">An important reason is that discriminative models accommodate rich features without constraints such as probabilistic independence assumptions between features.</a>
<a href="#4" id="4">We create a standard data split: sections 0–18 is used for training, sections 19–21 for development, and sections 22–24 for testing.</a>
<a href="#5" id="5">We obtain an absolute improvement of 2.07 points (UAS).</a>
<a href="#6" id="6">Meta feature templates</a>
<a href="#7" id="7">The number of the possible values of {a mathematical formula}Pfb is at most {a mathematical formula}6×N(TB), where {a mathematical formula}N(TB) refers to the number of the base feature templates that is small.</a>
<a href="#8" id="8">Recently, word embeddings [15] have also been used as less-sparse word features to improve part-of-speech tagging [16], [17], named entity recognition [18], [19], and dependency parsing [20], leading to similar improvements compared with Brown clusters.</a>
</body>
</html>