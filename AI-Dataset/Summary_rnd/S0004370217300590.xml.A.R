<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:191">
</head>
<body bgcolor="white">
<a href="#0" id="0">Our approach was motivated similarly but has important differences.</a>
<a href="#1" id="1">We choose the Frobenius norm here because it leads to a convex optimization, which can be computationally more feasible than if we choose kernel alignment score in [10].</a>
<a href="#2" id="2">The last section concludes the paper.</a>
<a href="#3" id="3">Empirically, it correlates nicely with the prediction accuracy on the test samples, as will be reported in Section 6.1.</a>
<a href="#4" id="4">We also examine the alignment score (15) used to choose the hyper-parameter λ in Fig. 1.</a>
<a href="#5" id="5">Our task is to learn (the inverse of) a new dictionary kernel, denoted by S, subject to the following considerations:</a>
<a href="#6" id="6">Most algorithms can learn the {a mathematical formula}n×n low-rank kernel{sup:2} matrix on labeled and unlabeled samples{sup:3} in the form of {a mathematical formula}K=GG⊤, which can then be fed into an SVM for classification.</a>
<a href="#7" id="7">In the future, we will consider learning a sparse dictionary kernel in our method.</a>
<a href="#8" id="8">In Section 5 we discuss related work.</a>
<a href="#9" id="9">The parameter m specifies the number of landmark points in the Nyström method.</a>
<a href="#10" id="10">Empirical setup for multiple kernel learning</a>
<a href="#11" id="11">First, it has a flexible, generative structure that allows us to generalize computed low-rank factorizations to arbitrary new unseen samples.</a>
</body>
</html>