<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:238">
</head>
<body bgcolor="white">
<a href="#0" id="0">Markov Decision Processes have become the standard model for probabilistic planning.</a>
<a href="#1" id="1">However, when applied to many practical problems, the estimates of transition probabilities are inaccurate.</a>
<a href="#2" id="2">This may be due to conflicting elicitations from experts or insufficient state transition information.</a>
<a href="#3" id="3">The Markov Decision Process with Imprecise Transition Probabilities (MDP-IPs) was introduced to obtain a robust policy where there is uncertainty in the transition.</a>
<a href="#4" id="4">Although it has been proposed a symbolic dynamic programming algorithm for MDP-IPs (called SPUDD-IP) that can solve problems up to 22 state variables, in practice, solving MDP-IP problems is time-consuming.</a>
<a href="#5" id="5">In this paper we propose efficient algorithms for a more general class of MDP-IPs, called Stochastic Shortest Path MDP-IPs (SSP MDP-IPs) that use initial state information to solve complex problems by focusing on reachable states.</a>
<a href="#6" id="6">The (L)RTDP-IP algorithm, a (Labeled) Real Time Dynamic Programming algorithm for SSP MDP-IPs, is proposed together with three different methods for sampling the next state.</a>
<a href="#7" id="7">It is shown here that the convergence of (L)RTDP-IP can be obtained by using any of these three methods, although the Bellman backups for this class of problems prescribe a minimax optimization.</a>
<a href="#8" id="8">As far as we are aware, this is the first asynchronous algorithm for SSP MDP-IPs given in terms of a general set of probability constraints that requires non-linear optimization over imprecise probabilities in the Bellman backup.</a>
<a href="#9" id="9">Our results show up to three orders of magnitude speedup for (L)RTDP-IP when compared with the SPUDD-IP algorithm.</a>
</body>
</html>