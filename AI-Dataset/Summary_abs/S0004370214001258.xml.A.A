<html>
<head>
<meta name="TextLength" content="SENT_NUM:5, WORD_NUM:171">
</head>
<body bgcolor="white">
<a href="#0" id="0">Inferring the most probable explanation to a set of variables, given a partial observation of the remaining variables, is one of the canonical computational problems in Bayesian networks, with widespread applications in AI and beyond.</a>
<a href="#1" id="1">This problem, known as MAP, is computationally intractable (NP-hard) and remains so even when only an approximate solution is sought.</a>
<a href="#2" id="2">We propose a heuristic formulation of the MAP problem, denoted as Inference to the Most Frugal Explanation (MFE), based on the observation that many intermediate variables (that are neither observed nor to be explained) are irrelevant with respect to the outcome of the explanatory process.</a>
<a href="#3" id="3">An explanation based on few samples (often even a singleton sample) from these irrelevant variables is typically almost as good as an explanation based on (the computationally costly) marginalization over these variables.</a>
<a href="#4" id="4">We show that while MFE is computationally intractable in general (as is MAP), it can be tractably approximated under plausible situational constraints, and its inferences are fairly robust with respect to which intermediate variables are considered to be relevant.</a>
</body>
</html>