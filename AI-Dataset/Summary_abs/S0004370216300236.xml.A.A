<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:257">
</head>
<body bgcolor="white">
<a href="#0" id="0">There is a long history in game theory on the topic of Bayesian or “rational” learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players.</a>
<a href="#1" id="1">This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours.</a>
<a href="#2" id="2">The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents.</a>
<a href="#3" id="3">The game theory literature studies this idea primarily in the context of equilibrium attainment.</a>
<a href="#4" id="4">In contrast, many AI applications have a focus on task completion and payoff maximisation.</a>
<a href="#5" id="5">With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types.</a>
<a href="#6" id="6">We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct.</a>
<a href="#7" id="7">Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects.</a>
<a href="#8" id="8">Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types.</a>
<a href="#9" id="9">Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.</a>
</body>
</html>