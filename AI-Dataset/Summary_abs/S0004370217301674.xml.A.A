<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:163">
</head>
<body bgcolor="white">
<a href="#0" id="0">A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved.</a>
<a href="#1" id="1">When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal.</a>
<a href="#2" id="2">In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient.</a>
<a href="#3" id="3">These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots.</a>
<a href="#4" id="4">The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources.</a>
<a href="#5" id="5">DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts.</a>
<a href="#6" id="6">Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.</a>
</body>
</html>