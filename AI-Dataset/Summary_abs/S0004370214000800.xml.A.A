<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:213">
</head>
<body bgcolor="white">
<a href="#0" id="0">In this paper, we propose a novel machine learning framework called “Online Transfer Learning” (OTL), which aims to attack an online learning task on a target domain by transferring knowledge from some source domain.</a>
<a href="#1" id="1">We do not assume data in the target domain follows the same distribution as that in the source domain, and the motivation of our work is to enhance a supervised online learning task on a target domain by exploiting the existing knowledge that had been learnt from training data in source domains.</a>
<a href="#2" id="2">OTL is in general very challenging since data in both source and target domains not only can be different in their class distributions, but also can be diverse in their feature representations.</a>
<a href="#3" id="3">As a first attempt to this new research problem, we investigate two different settings of OTL: (i) OTL on homogeneous domains of common feature space, and (ii) OTL across heterogeneous domains of different feature spaces.</a>
<a href="#4" id="4">For each setting, we propose effective OTL algorithms to solve online classification tasks, and show some theoretical bounds of the algorithms.</a>
<a href="#5" id="5">In addition, we also apply the OTL technique to attack the challenging online learning tasks with concept-drifting data streams.</a>
<a href="#6" id="6">Finally, we conduct extensive empirical studies on a comprehensive testbed, in which encouraging results validate the efficacy of our techniques.</a>
</body>
</html>