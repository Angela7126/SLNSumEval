<html>
<head>
<meta name="TextLength" content="SENT_NUM:7, WORD_NUM:187">
</head>
<body bgcolor="white">
<a href="#0" id="0">This paper proposes a unified approach to learning from constraints, which integrates the ability of classical machine learning techniques to learn from continuous feature-based representations with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning.</a>
<a href="#1" id="1">Learning tasks are modeled in the general framework of multi-objective optimization, where a set of constraints must be satisfied in addition to the traditional smoothness regularization term.</a>
<a href="#2" id="2">The constraints translate First Order Logic formulas, which can express learning-from-example supervisions and general prior knowledge about the environment by using fuzzy logic.</a>
<a href="#3" id="3">By enforcing the constraints also on the test set, this paper presents a natural extension of the framework to perform collective classification.</a>
<a href="#4" id="4">Interestingly, the theory holds for both the case of data represented by feature vectors and the case of data simply expressed by pattern identifiers, thus extending classic kernel machines and graph regularization, respectively.</a>
<a href="#5" id="5">This paper also proposes a probabilistic interpretation of the proposed learning scheme, and highlights intriguing connections with probabilistic approaches like Markov Logic Networks.</a>
<a href="#6" id="6">Experimental results on classic benchmarks provide clear evidence of the remarkable improvements that are obtained with respect to related approaches.</a>
</body>
</html>