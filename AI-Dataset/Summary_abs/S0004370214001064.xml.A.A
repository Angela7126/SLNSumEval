<html>
<head>
<meta name="TextLength" content="SENT_NUM:11, WORD_NUM:198">
</head>
<body bgcolor="white">
<a href="#0" id="0">We introduce kLog, a novel approach to statistical relational learning.</a>
<a href="#1" id="1">Unlike standard approaches, kLog does not represent a probability distribution directly.</a>
<a href="#2" id="2">It is rather a language to perform kernel-based learning on expressive logical and relational representations.</a>
<a href="#3" id="3">kLog allows users to specify learning problems declaratively.</a>
<a href="#4" id="4">It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases.</a>
<a href="#5" id="5">Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph â€” in particular, a grounded entity/relationship diagram.</a>
<a href="#6" id="6">Subsequently, a choice of graph kernel defines the feature space.</a>
<a href="#7" id="7">kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems.</a>
<a href="#8" id="8">The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification.</a>
<a href="#9" id="9">We also report about empirical comparisons, showing that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy.</a>
<a href="#10" id="10">kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials.</a>
</body>
</html>