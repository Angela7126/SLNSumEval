<html>
<head>
<meta name="TextLength" content="SENT_NUM:12, WORD_NUM:259">
</head>
<body bgcolor="white">
<a href="#0" id="0">In robotics, lower-level controllers are typically used to make the robot solve a specific task in a fixed context.</a>
<a href="#1" id="1">For example, the lower-level controller can encode a hitting movement while the context defines the target coordinates to hit.</a>
<a href="#2" id="2">However, in many learning problems the context may change between task executions.</a>
<a href="#3" id="3">To adapt the policy to a new context, we utilize a hierarchical approach by learning an upper-level policy that generalizes the lower-level controllers to new contexts.</a>
<a href="#4" id="4">A common approach to learn such upper-level policies is to use policy search.</a>
<a href="#5" id="5">However, the majority of current contextual policy search approaches are model-free and require a high number of interactions with the robot and its environment.</a>
<a href="#6" id="6">Model-based approaches are known to significantly reduce the amount of robot experiments, however, current model-based techniques cannot be applied straightforwardly to the problem of learning contextual upper-level policies.</a>
<a href="#7" id="7">They rely on specific parametrizations of the policy and the reward function, which are often unrealistic in the contextual policy search formulation.</a>
<a href="#8" id="8">In this paper, we propose a novel model-based contextual policy search algorithm that is able to generalize lower-level controllers, and is data-efficient.</a>
<a href="#9" id="9">Our approach is based on learned probabilistic forward models and information theoretic policy search.</a>
<a href="#10" id="10">Unlike current algorithms, our method does not require any assumption on the parametrization of the policy or the reward function.</a>
<a href="#11" id="11">We show on complex simulated robotic tasks and in a real robot experiment that the proposed learning framework speeds up the learning process by up to two orders of magnitude in comparison to existing methods, while learning high quality policies.</a>
</body>
</html>