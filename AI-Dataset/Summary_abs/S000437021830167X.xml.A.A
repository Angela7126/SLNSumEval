<html>
<head>
<meta name="TextLength" content="SENT_NUM:6, WORD_NUM:147">
</head>
<body bgcolor="white">
<a href="#0" id="0">For decomposable score-based structure learning of Bayesian networks, existing approaches first compute a collection of candidate parent sets for each variable and then optimize over this collection by choosing one parent set for each variable without creating directed cycles while maximizing the total score.</a>
<a href="#1" id="1">We target the task of constructing the collection of candidate parent sets when the score of choice is the Bayesian Information Criterion (BIC).</a>
<a href="#2" id="2">We provide new non-trivial results that can be used to prune the search space of candidate parent sets of each node.</a>
<a href="#3" id="3">We analyze how these new results relate to previous ideas in the literature both theoretically and empirically.</a>
<a href="#4" id="4">We show in experiments with UCI data sets that gains can be significant.</a>
<a href="#5" id="5">Since the new pruning rules are easy to implement and have low computational costs, they can be promptly integrated into all state-of-the-art methods for structure learning of Bayesian networks.</a>
</body>
</html>