<html>
<head>
<meta name="TextLength" content="SENT_NUM:10, WORD_NUM:188">
</head>
<body bgcolor="white">
<a href="#0" id="0">Low-rank matrix decomposition and kernel learning are two useful techniques in building advanced learning systems.</a>
<a href="#1" id="1">Low-rank decomposition can greatly reduce the computational cost of manipulating large kernel matrices.</a>
<a href="#2" id="2">However, existing approaches are mostly unsupervised and do not incorporate side information such as class labels, making the decomposition less effective for a specific learning task.</a>
<a href="#3" id="3">On the other hand, kernel learning techniques aim at constructing kernel matrices whose structure is well aligned with the learning target, which improves the generalization performance of kernel methods.</a>
<a href="#4" id="4">However, most kernel learning approaches are computationally very expensive.</a>
<a href="#5" id="5">To obtain the advantages of both techniques and address their limitations, in this paper we propose a novel kernel low-rank decomposition formulation called the generalized Nystr√∂m method.</a>
<a href="#6" id="6">Our approach inherits the linear time and space complexity via matrix decomposition, while at the same time fully exploits (partial) label information in computing task-dependent decomposition.</a>
<a href="#7" id="7">In addition, the resultant low-rank factors can generalize to arbitrary new samples, rendering great flexibility in inductive learning scenarios.</a>
<a href="#8" id="8">We further extend the algorithm to a multiple kernel learning setup.</a>
<a href="#9" id="9">The experimental results on semi-supervised classification demonstrate the usefulness of the proposed method.</a>
</body>
</html>